[
  {
    "id": "http://arxiv.org/abs/2504.16078v1",
    "title": "LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making Abilities",
    "authors": [
      "Thomas Schmied",
      "Jörg Bornschein",
      "Jordi Grau-Moya",
      "Markus Wulfmeier",
      "Razvan Pascanu"
    ],
    "author_ids": [],
    "abstract": "The success of Large Language Models (LLMs) has sparked interest in various\nagentic applications. A key hypothesis is that LLMs, leveraging common sense\nand Chain-of-Thought (CoT) reasoning, can effectively explore and efficiently\nsolve complex domains. However, LLM agents have been found to suffer from\nsub-optimal exploration and the knowing-doing gap, the inability to effectively\nact on knowledge present in the model. In this work, we systematically study\nwhy LLMs perform sub-optimally in decision-making scenarios. In particular, we\nclosely examine three prevalent failure modes: greediness, frequency bias, and\nthe knowing-doing gap. We propose mitigation of these shortcomings by\nfine-tuning via Reinforcement Learning (RL) on self-generated CoT rationales.\nOur experiments across multi-armed bandits, contextual bandits, and\nTic-tac-toe, demonstrate that RL fine-tuning enhances the decision-making\nabilities of LLMs by increasing exploration and narrowing the knowing-doing\ngap. Finally, we study both classic exploration mechanisms, such as\n$\\epsilon$-greedy, and LLM-specific approaches, such as self-correction and\nself-consistency, to enable more effective fine-tuning of LLMs for\ndecision-making.",
    "published_date": "2025-04-22T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.16078v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.16028v1",
    "title": "Hessian Riemannian Flow For Multi-Population Wardrop Equilibrium",
    "authors": [
      "Tigran Bakaryan",
      "Christoph Aoun",
      "Ricardo de Lima Ribeiro",
      "Naira Hovakimyan",
      "Diogo Gomes"
    ],
    "author_ids": [],
    "abstract": "In this paper, we address the problem of optimizing flows on generalized\ngraphs that feature multiple entry points and multiple populations, each with\nvarying cost structures. We tackle this problem by considering the\nmulti-population Wardrop equilibrium, defined through variational inequalities.\nWe rigorously analyze the existence and uniqueness of the Wardrop equilibrium.\nFurthermore, we introduce an efficient numerical method to find the solution.\nIn particular, we reformulate the equilibrium problem as a distributed\noptimization problem over subgraphs and introduce a novel Hessian Riemannian\nflow method, a Riemannian-manifold-projected Hessian flow, to efficiently\ncompute a solution. Finally, we demonstrate the effectiveness of our approach\nthrough examples in urban traffic management, including routing for diverse\nvehicle types and strategies for minimizing emissions in congested\nenvironments.",
    "published_date": "2025-04-22T00:00:00",
    "year": 2025,
    "categories": [
      "eess.SY",
      "cs.MA",
      "cs.SY",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.16028v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.15941v1",
    "title": "FairTranslate: An English-French Dataset for Gender Bias Evaluation in Machine Translation by Overcoming Gender Binarity",
    "authors": [
      "Fanny Jourdan",
      "Yannick Chevalier",
      "Cécile Favre"
    ],
    "author_ids": [],
    "abstract": "Large Language Models (LLMs) are increasingly leveraged for translation tasks\nbut often fall short when translating inclusive language -- such as texts\ncontaining the singular 'they' pronoun or otherwise reflecting fair linguistic\nprotocols. Because these challenges span both computational and societal\ndomains, it is imperative to critically evaluate how well LLMs handle inclusive\ntranslation with a well-founded framework.\n  This paper presents FairTranslate, a novel, fully human-annotated dataset\ndesigned to evaluate non-binary gender biases in machine translation systems\nfrom English to French. FairTranslate includes 2418 English-French sentence\npairs related to occupations, annotated with rich metadata such as the\nstereotypical alignment of the occupation, grammatical gender indicator\nambiguity, and the ground-truth gender label (male, female, or inclusive).\n  We evaluate four leading LLMs (Gemma2-2B, Mistral-7B, Llama3.1-8B,\nLlama3.3-70B) on this dataset under different prompting procedures. Our results\nreveal substantial biases in gender representation across LLMs, highlighting\npersistent challenges in achieving equitable outcomes in machine translation.\nThese findings underscore the need for focused strategies and interventions\naimed at ensuring fair and inclusive language usage in LLM-based translation\nsystems.\n  We make the FairTranslate dataset publicly available on Hugging Face, and\ndisclose the code for all experiments on GitHub.",
    "published_date": "2025-04-22T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.15941v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.15924v1",
    "title": "Achieving Distributive Justice in Federated Learning via Uncertainty Quantification",
    "authors": [
      "Alycia Carey",
      "Xintao Wu"
    ],
    "author_ids": [],
    "abstract": "Client-level fairness metrics for federated learning are used to ensure that\nall clients in a federation either: a) have similar final performance on their\nlocal data distributions (i.e., client parity), or b) obtain final performance\non their local data distributions relative to their contribution to the\nfederated learning process (i.e., contribution fairness). While a handful of\nworks that propose either client-parity or contribution-based fairness metrics\nground their definitions and decisions in social theories of equality -- such\nas distributive justice -- most works arbitrarily choose what notion of\nfairness to align with which makes it difficult for practitioners to choose\nwhich fairness metric aligns best with their fairness ethics. In this work, we\npropose UDJ-FL (Uncertainty-based Distributive Justice for Federated Learning),\na flexible federated learning framework that can achieve multiple distributive\njustice-based client-level fairness metrics. Namely, by utilizing techniques\ninspired by fair resource allocation, in conjunction with performing aleatoric\nuncertainty-based client weighing, our UDJ-FL framework is able to achieve\negalitarian, utilitarian, Rawls' difference principle, or desert-based\nclient-level fairness. We empirically show the ability of UDJ-FL to achieve all\nfour defined distributive justice-based client-level fairness metrics in\naddition to providing fairness equivalent to (or surpassing) other popular fair\nfederated learning works. Further, we provide justification for why aleatoric\nuncertainty weighing is necessary to the construction of our UDJ-FL framework\nas well as derive theoretical guarantees for the generalization bounds of\nUDJ-FL. Our code is publicly available at\nhttps://github.com/alycia-noel/UDJ-FL.",
    "published_date": "2025-04-22T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML",
      "68T01",
      "I.2.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.15924v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.15886v1",
    "title": "Beyond Attention: Investigating the Threshold Where Objective Robot Exclusion Becomes Subjective",
    "authors": [
      "Clarissa Sabrina Arlinghaus",
      "Ashita Ashok",
      "Ashim Mandal",
      "Karsten Berns",
      "Günter W. Maier"
    ],
    "author_ids": [],
    "abstract": "As robots become increasingly involved in decision-making processes (e.g.,\npersonnel selection), concerns about fairness and social inclusion arise. This\nstudy examines social exclusion in robot-led group interviews by robot Ameca,\nexploring the relationship between objective exclusion (robot's attention\nallocation), subjective exclusion (perceived exclusion), mood change, and need\nfulfillment. In a controlled lab study (N = 35), higher objective exclusion\nsignificantly predicted subjective exclusion. In turn, subjective exclusion\nnegatively impacted mood and need fulfillment but only mediated the\nrelationship between objective exclusion and need fulfillment. A piecewise\nregression analysis identified a critical threshold at which objective\nexclusion begins to be perceived as subjective exclusion. Additionally, the\nstanding position was the primary predictor of exclusion, whereas demographic\nfactors (e.g., gender, height) had no significant effect. These findings\nunderscore the need to consider both objective and subjective exclusion in\nhuman-robot interactions and have implications for fairness in robot-assisted\nhiring processes.",
    "published_date": "2025-04-22T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.15886v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.15736v1",
    "title": "Riemannian Neural Geodesic Interpolant",
    "authors": [
      "Jiawen Wu",
      "Bingguang Chen",
      "Yuyi Zhou",
      "Qi Meng",
      "Rongchan Zhu",
      "Zhi-Ming Ma"
    ],
    "author_ids": [],
    "abstract": "Stochastic interpolants are efficient generative models that bridge two\narbitrary probability density functions in finite time, enabling flexible\ngeneration from the source to the target distribution or vice versa. These\nmodels are primarily developed in Euclidean space, and are therefore limited in\ntheir application to many distribution learning problems defined on Riemannian\nmanifolds in real-world scenarios. In this work, we introduce the Riemannian\nNeural Geodesic Interpolant (RNGI) model, which interpolates between two\nprobability densities on a Riemannian manifold along the stochastic geodesics,\nand then samples from one endpoint as the final state using the continuous flow\noriginating from the other endpoint. We prove that the temporal marginal\ndensity of RNGI solves a transport equation on the Riemannian manifold. After\ntraining the model's the neural velocity and score fields, we propose the\nEmbedding Stochastic Differential Equation (E-SDE) algorithm for stochastic\nsampling of RNGI. E-SDE significantly improves the sampling quality by reducing\nthe accumulated error caused by the excessive intrinsic discretization of\nRiemannian Brownian motion in the classical Geodesic Random Walk (GRW)\nalgorithm. We also provide theoretical bounds on the generative bias measured\nin terms of KL-divergence. Finally, we demonstrate the effectiveness of the\nproposed RNGI and E-SDE through experiments conducted on both collected and\nsynthetic distributions on S2 and SO(3).",
    "published_date": "2025-04-22T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.15736v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.15728v1",
    "title": "SAGA: Semantic-Aware Gray color Augmentation for Visible-to-Thermal Domain Adaptation across Multi-View Drone and Ground-Based Vision Systems",
    "authors": [
      "Manjunath D",
      "Aniruddh Sikdar",
      "Prajwal Gurunath",
      "Sumanth Udupa",
      "Suresh Sundaram"
    ],
    "author_ids": [],
    "abstract": "Domain-adaptive thermal object detection plays a key role in facilitating\nvisible (RGB)-to-thermal (IR) adaptation by reducing the need for co-registered\nimage pairs and minimizing reliance on large annotated IR datasets. However,\ninherent limitations of IR images, such as the lack of color and texture cues,\npose challenges for RGB-trained models, leading to increased false positives\nand poor-quality pseudo-labels. To address this, we propose Semantic-Aware Gray\ncolor Augmentation (SAGA), a novel strategy for mitigating color bias and\nbridging the domain gap by extracting object-level features relevant to IR\nimages. Additionally, to validate the proposed SAGA for drone imagery, we\nintroduce the IndraEye, a multi-sensor (RGB-IR) dataset designed for diverse\napplications. The dataset contains 5,612 images with 145,666 instances,\ncaptured from diverse angles, altitudes, backgrounds, and times of day,\noffering valuable opportunities for multimodal learning, domain adaptation for\nobject detection and segmentation, and exploration of sensor-specific strengths\nand weaknesses. IndraEye aims to enhance the development of more robust and\naccurate aerial perception systems, especially in challenging environments.\nExperimental results show that SAGA significantly improves RGB-to-IR adaptation\nfor autonomous driving and IndraEye dataset, achieving consistent performance\ngains of +0.4% to +7.6% (mAP) when integrated with state-of-the-art domain\nadaptation techniques. The dataset and codes are available at\nhttps://github.com/airliisc/IndraEye.",
    "published_date": "2025-04-22T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.15728v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.15719v1",
    "title": "Implementing Rational Choice Functions with LLMs and Measuring their Alignment with User Preferences",
    "authors": [
      "Anna Karnysheva",
      "Christian Drescher",
      "Dietrich Klakow"
    ],
    "author_ids": [],
    "abstract": "As large language models (LLMs) become integral to intelligent user\ninterfaces (IUIs), their role as decision-making agents raises critical\nconcerns about alignment. Although extensive research has addressed issues such\nas factuality, bias, and toxicity, comparatively little attention has been paid\nto measuring alignment to preferences, i.e., the relative desirability of\ndifferent alternatives, a concept used in decision making, economics, and\nsocial choice theory. However, a reliable decision-making agent makes choices\nthat align well with user preferences.\n  In this paper, we generalize existing methods that exploit LLMs for ranking\nalternative outcomes by addressing alignment with the broader and more flexible\nconcept of user preferences, which includes both strict preferences and\nindifference among alternatives. To this end, we put forward design principles\nfor using LLMs to implement rational choice functions, and provide the\nnecessary tools to measure preference satisfaction. We demonstrate the\napplicability of our approach through an empirical study in a practical\napplication of an IUI in the automotive domain.",
    "published_date": "2025-04-22T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.15719v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.15715v1",
    "title": "Assessing FAIRness of the Digital Shadow Reference Model",
    "authors": [
      "Johannes Theissen-Lipp"
    ],
    "author_ids": [],
    "abstract": "Models play a critical role in managing the vast amounts of data and\nincreasing complexity found in the IoT, IIoT, and IoP domains. The Digital\nShadow Reference Model, which serves as a foundational metadata schema for\nlinking data and metadata in these environments, is an example of such a model.\nEnsuring FAIRness (adherence to the FAIR Principles) is critical because it\nimproves data findability, accessibility, interoperability, and reusability,\nfacilitating efficient data management and integration across systems.\n  This paper presents an evaluation of the FAIRness of the Digital Shadow\nReference Model using a structured evaluation framework based on the FAIR Data\nPrinciples. Using the concept of FAIR Implementation Profiles (FIPs),\nsupplemented by a mini-questionnaire, we systematically evaluate the model's\nadherence to these principles. Our analysis identifies key strengths, including\nthe model's metadata schema that supports rich descriptions and authentication\ntechniques, and highlights areas for improvement, such as the need for globally\nunique identifiers and consequent support for different Web standards. The\nresults provide actionable insights for improving the FAIRness of the model and\npromoting better data management and reuse. This research contributes to the\nfield by providing a detailed assessment of the Digital Shadow Reference Model\nand recommending next steps to improve its FAIRness and usability.",
    "published_date": "2025-04-22T00:00:00",
    "year": 2025,
    "categories": [
      "cs.DB",
      "cs.CY",
      "cs.IR",
      "H.1; H.3; H.4; H.m; I.6"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.15715v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.15470v1",
    "title": "Manifold Induced Biases for Zero-shot and Few-shot Detection of Generated Images",
    "authors": [
      "Jonathan Brokman",
      "Amit Giloni",
      "Omer Hofman",
      "Roman Vainshtein",
      "Hisashi Kojima",
      "Guy Gilboa"
    ],
    "author_ids": [],
    "abstract": "Distinguishing between real and AI-generated images, commonly referred to as\n'image detection', presents a timely and significant challenge. Despite\nextensive research in the (semi-)supervised regime, zero-shot and few-shot\nsolutions have only recently emerged as promising alternatives. Their main\nadvantage is in alleviating the ongoing data maintenance, which quickly becomes\noutdated due to advances in generative technologies. We identify two main gaps:\n(1) a lack of theoretical grounding for the methods, and (2) significant room\nfor performance improvements in zero-shot and few-shot regimes. Our approach is\nfounded on understanding and quantifying the biases inherent in generated\ncontent, where we use these quantities as criteria for characterizing generated\nimages. Specifically, we explore the biases of the implicit probability\nmanifold, captured by a pre-trained diffusion model. Through score-function\nanalysis, we approximate the curvature, gradient, and bias towards points on\nthe probability manifold, establishing criteria for detection in the zero-shot\nregime. We further extend our contribution to the few-shot setting by employing\na mixture-of-experts methodology. Empirical results across 20 generative models\ndemonstrate that our method outperforms current approaches in both zero-shot\nand few-shot settings. This work advances the theoretical understanding and\npractical usage of generated content biases through the lens of manifold\nanalysis.",
    "published_date": "2025-04-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.15470v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.15404v1",
    "title": "Context Aware Grounded Teacher for Source Free Object Detection",
    "authors": [
      "Tajamul Ashraf",
      "Rajes Manna",
      "Partha Sarathi Purkayastha",
      "Tavaheed Tariq",
      "Janibul Bashir"
    ],
    "author_ids": [],
    "abstract": "We focus on the Source Free Object Detection (SFOD) problem, when source data\nis unavailable during adaptation, and the model must adapt to the unlabeled\ntarget domain. In medical imaging, several approaches have leveraged a\nsemi-supervised student-teacher architecture to bridge domain discrepancy.\nContext imbalance in labeled training data and significant domain shifts\nbetween domains can lead to biased teacher models that produce inaccurate\npseudolabels, degrading the student model's performance and causing a mode\ncollapse. Class imbalance, particularly when one class significantly outnumbers\nanother, leads to contextual bias. To tackle the problem of context bias and\nthe significant performance drop of the student model in the SFOD setting, we\nintroduce Grounded Teacher (GT) as a standard framework. In this study, we\nmodel contextual relationships using a dedicated relational context module and\nleverage it to mitigate inherent biases in the model. This approach enables us\nto apply augmentations to closely related classes, across and within domains,\nenhancing the performance of underrepresented classes while keeping the effect\non dominant classes minimal. We further improve the quality of predictions by\nimplementing an expert foundational branch to supervise the student model. We\nvalidate the effectiveness of our approach in mitigating context bias under the\nSFOD setting through experiments on three medical datasets supported by\ncomprehensive ablation studies. All relevant resources, including preprocessed\ndata, trained model weights, and code, are publicly available at this\nhttps://github.com/Tajamul21/Grounded_Teacher.",
    "published_date": "2025-04-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.15404v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.15211v1",
    "title": "Position: Bayesian Statistics Facilitates Stakeholder Participation in Evaluation of Generative AI",
    "authors": [
      "Yanan Long"
    ],
    "author_ids": [],
    "abstract": "The evaluation of Generative AI (GenAI) systems plays a critical role in\npublic policy and decision-making, yet existing methods are often limited by\nreliance on benchmark-driven, point-estimate comparisons that fail to capture\nuncertainty and broader societal impacts. This paper argues for the use of\nBayesian statistics as a principled framework to address these challenges.\nBayesian methods enable the integration of domain expertise through prior\nelicitation, allow for continuous learning from new data, and provide robust\nuncertainty quantification via posterior inference. We demonstrate how Bayesian\ninference can be applied to GenAI evaluation, particularly in incorporating\nstakeholder perspectives to enhance fairness, transparency, and reliability.\nFurthermore, we discuss Bayesian workflows as an iterative process for model\nvalidation and refinement, ensuring robust assessments of GenAI systems in\ndynamic, real-world contexts.",
    "published_date": "2025-04-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.15211v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.15208v1",
    "title": "Compute-Optimal LLMs Provably Generalize Better With Scale",
    "authors": [
      "Marc Finzi",
      "Sanyam Kapoor",
      "Diego Granziol",
      "Anming Gu",
      "Christopher De Sa",
      "J. Zico Kolter",
      "Andrew Gordon Wilson"
    ],
    "author_ids": [],
    "abstract": "Why do larger language models generalize better? To investigate this\nquestion, we develop generalization bounds on the pretraining objective of\nlarge language models (LLMs) in the compute-optimal regime, as described by the\nChinchilla scaling laws. We introduce a novel, fully empirical Freedman-type\nmartingale concentration inequality that tightens existing bounds by accounting\nfor the variance of the loss function. This generalization bound can be\ndecomposed into three interpretable components: the number of parameters per\ntoken, the loss variance, and the quantization error at a fixed bitrate. As\ncompute-optimal language models are scaled up, the number of parameters per\ndata point remains constant; however, both the loss variance and the\nquantization error decrease, implying that larger models should have smaller\ngeneralization gaps. We examine why larger models tend to be more quantizable\nfrom an information theoretic perspective, showing that the rate at which they\ncan integrate new information grows more slowly than their capacity on the\ncompute-optimal frontier. From these findings we produce a scaling law for the\ngeneralization gap, with bounds that become predictably stronger with scale.",
    "published_date": "2025-04-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.15208v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.15206v1",
    "title": "How Global Calibration Strengthens Multiaccuracy",
    "authors": [
      "Sílvia Casacuberta",
      "Parikshit Gopalan",
      "Varun Kanade",
      "Omer Reingold"
    ],
    "author_ids": [],
    "abstract": "Multiaccuracy and multicalibration are multigroup fairness notions for\nprediction that have found numerous applications in learning and computational\ncomplexity. They can be achieved from a single learning primitive: weak\nagnostic learning. Here we investigate the power of multiaccuracy as a learning\nprimitive, both with and without the additional assumption of calibration. We\nfind that multiaccuracy in itself is rather weak, but that the addition of\nglobal calibration (this notion is called calibrated multiaccuracy) boosts its\npower substantially, enough to recover implications that were previously known\nonly assuming the stronger notion of multicalibration.\n  We give evidence that multiaccuracy might not be as powerful as standard weak\nagnostic learning, by showing that there is no way to post-process a\nmultiaccurate predictor to get a weak learner, even assuming the best\nhypothesis has correlation $1/2$. Rather, we show that it yields a restricted\nform of weak agnostic learning, which requires some concept in the class to\nhave correlation greater than $1/2$ with the labels. However, by also requiring\nthe predictor to be calibrated, we recover not just weak, but strong agnostic\nlearning.\n  A similar picture emerges when we consider the derivation of hardcore\nmeasures from predictors satisfying multigroup fairness notions. On the one\nhand, while multiaccuracy only yields hardcore measures of density half the\noptimal, we show that (a weighted version of) calibrated multiaccuracy achieves\noptimal density.\n  Our results yield new insights into the complementary roles played by\nmultiaccuracy and calibration in each setting. They shed light on why\nmultiaccuracy and global calibration, although not particularly powerful by\nthemselves, together yield considerably stronger notions.",
    "published_date": "2025-04-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.15206v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.15132v1",
    "title": "Investigating Youth's Technical and Ethical Understanding of Generative Language Models When Engaging in Construction and Deconstruction Activities",
    "authors": [
      "Luis Morales-Navarro"
    ],
    "author_ids": [],
    "abstract": "The widespread adoption of generative artificial intelligence/machine\nlearning (AI/ML) technologies has increased the need to support youth in\ndeveloping AI/ML literacies. However, most work has centered on preparing young\npeople to use these systems, with less attention to how they can participate in\ndesigning and evaluating them. This study investigates how engaging young\npeople in the design and auditing of generative language models (GLMs) may\nfoster the development of their understanding of how these systems work from\nboth technical and ethical perspectives. The study takes an in-pieces approach\nto investigate novices' conceptions of GLMs. Such an approach supports the\nanalysis of how technical and ethical conceptions evolve and relate to each\nother. I am currently conducting a series of participatory design workshops\nwith sixteen ninth graders (ages 14-15) in which they will (a) build GLMs from\na data-driven perspective that glassboxes how data shapes model performance and\n(b) audit commercial GLMs by repeatedly and systematically querying them to\ndraw inferences about their behaviors. I will analyze participants'\ninteractions to identify ethical and technical conceptions they may exhibit\nwhile designing and auditing GLMs. I will also conduct clinical interviews and\nuse microgenetic knowledge analysis and ordered network analysis to investigate\nhow participants' ethical and technical conceptions of GLMs relate to each\nother and change after the workshop. The study will contribute (a) evidence of\nhow engaging youth in design and auditing activities may support the\ndevelopment of ethical and technical understanding of GLMs and (b) an inventory\nof novice design and auditing practices that may support youth's technical and\nethical understanding of GLMs.",
    "published_date": "2025-04-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.CY",
      "H.5.2; K.3.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.15132v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.15095v2",
    "title": "VistaDepth: Frequency Modulation With Bias Reweighting For Enhanced Long-Range Depth Estimation",
    "authors": [
      "Mingxia Zhan",
      "Li Zhang",
      "Xiaomeng Chu",
      "Beibei Wang"
    ],
    "author_ids": [],
    "abstract": "Monocular depth estimation (MDE) aims to predict per-pixel depth values from\na single RGB image. Recent advancements have positioned diffusion models as\neffective MDE tools by framing the challenge as a conditional image generation\ntask. Despite their progress, these methods often struggle with accurately\nreconstructing distant depths, due largely to the imbalanced distribution of\ndepth values and an over-reliance on spatial-domain features. To overcome these\nlimitations, we introduce VistaDepth, a novel framework that integrates\nadaptive frequency-domain feature enhancements with an adaptive\nweight-balancing mechanism into the diffusion process. Central to our approach\nis the Latent Frequency Modulation (LFM) module, which dynamically refines\nspectral responses in the latent feature space, thereby improving the\npreservation of structural details and reducing noisy artifacts. Furthermore,\nwe implement an adaptive weighting strategy that modulates the diffusion loss\nin real-time, enhancing the model's sensitivity towards distant depth\nreconstruction. These innovations collectively result in superior depth\nperception performance across both distance and detail. Experimental\nevaluations confirm that VistaDepth achieves state-of-the-art performance among\ndiffusion-based MDE techniques, particularly excelling in the accurate\nreconstruction of distant regions.",
    "published_date": "2025-04-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.15095v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.15090v1",
    "title": "Federated Latent Factor Model for Bias-Aware Recommendation with Privacy-Preserving",
    "authors": [
      "Junxiang Gao",
      "Yixin Ran",
      "Jia Chen"
    ],
    "author_ids": [],
    "abstract": "A recommender system (RS) aims to provide users with personalized item\nrecommendations, enhancing their overall experience. Traditional RSs collect\nand process all user data on a central server. However, this centralized\napproach raises significant privacy concerns, as it increases the risk of data\nbreaches and privacy leakages, which are becoming increasingly unacceptable to\nprivacy-sensitive users. To address these privacy challenges, federated\nlearning has been integrated into RSs, ensuring that user data remains secure.\nIn centralized RSs, the issue of rating bias is effectively addressed by\njointly analyzing all users' raw interaction data. However, this becomes a\nsignificant challenge in federated RSs, as raw data is no longer accessible due\nto privacy-preserving constraints. To overcome this problem, we propose a\nFederated Bias-Aware Latent Factor (FBALF) model. In FBALF, training bias is\nexplicitly incorporated into every local model's loss function, allowing for\nthe effective elimination of rating bias without compromising data privacy.\nExtensive experiments conducted on three real-world datasets demonstrate that\nFBALF achieves significantly higher recommendation accuracy compared to other\nstate-of-the-art federated RSs.",
    "published_date": "2025-04-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.15090v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.15075v1",
    "title": "Mitigating Degree Bias in Graph Representation Learning with Learnable Structural Augmentation and Structural Self-Attention",
    "authors": [
      "Van Thuy Hoang",
      "Hyeon-Ju Jeon",
      "O-Joun Lee"
    ],
    "author_ids": [],
    "abstract": "Graph Neural Networks (GNNs) update node representations through message\npassing, which is primarily based on the homophily principle, assuming that\nadjacent nodes share similar features. However, in real-world graphs with\nlong-tailed degree distributions, high-degree nodes dominate message passing,\ncausing a degree bias where low-degree nodes remain under-represented due to\ninadequate messages. The main challenge in addressing degree bias is how to\ndiscover non-adjacent nodes to provide additional messages to low-degree nodes\nwhile reducing excessive messages for high-degree nodes. Nevertheless,\nexploiting non-adjacent nodes to provide valuable messages is challenging, as\nit could generate noisy information and disrupt the original graph structures.\nTo solve it, we propose a novel Degree Fairness Graph Transformer, named\nDegFairGT, to mitigate degree bias by discovering structural similarities\nbetween non-adjacent nodes through learnable structural augmentation and\nstructural self-attention. Our key idea is to exploit non-adjacent nodes with\nsimilar roles in the same community to generate informative edges under our\naugmentation, which could provide informative messages between nodes with\nsimilar roles while ensuring that the homophily principle is maintained within\nthe community. To enable DegFairGT to learn such structural similarities, we\nthen propose a structural self-attention to capture the similarities between\nnode pairs. To preserve global graph structures and prevent graph augmentation\nfrom hindering graph structure, we propose a Self-Supervised Learning task to\npreserve p-step transition probability and regularize graph augmentation.\nExtensive experiments on six datasets showed that DegFairGT outperformed\nstate-of-the-art baselines in degree fairness analysis, node classification,\nand node clustering tasks.",
    "published_date": "2025-04-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.15075v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.15019v2",
    "title": "Feedback Stackelberg-Nash equilibria in difference games with quasi-hierarchical interactions and inequality constraints",
    "authors": [
      "Partha Sarathi Mohapatra",
      "Puduru Viswanadha Reddy",
      "Georges Zaccour"
    ],
    "author_ids": [],
    "abstract": "In this paper, we study a class of two-player deterministic finite-horizon\ndifference games with coupled inequality constraints, where each player has two\ntypes of decision variables: one involving sequential interactions and the\nother simultaneous interactions. We refer to this class of games as\nquasi-hierarchical dynamic games and define a solution concept called the\nfeedback Stackelberg-Nash (FSN) equilibrium. Under separability assumption on\ncost functions, we provide a recursive formulation of the FSN solution using\ndynamic programming. We show that the FSN solution can be derived from the\nparametric feedback Stackelberg solution of an associated unconstrained game\ninvolving only sequential interactions, with a specific choice of the\nparameters that satisfy certain implicit complementarity conditions. For the\nlinear-quadratic case, we show that an FSN solution is obtained by\nreformulating these complementarity conditions as a single large-scale linear\ncomplementarity problem. Finally, we illustrate our results using a dynamic\nduopoly game with production constraints.",
    "published_date": "2025-04-21T00:00:00",
    "year": 2025,
    "categories": [
      "math.OC",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.15019v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.14991v1",
    "title": "Understanding Accuracy-Fairness Trade-offs in Re-ranking through Elasticity in Economics",
    "authors": [
      "Chen Xu",
      "Jujia Zhao",
      "Wenjie Wang",
      "Liang Pang",
      "Jun Xu",
      "Tat-Seng Chua",
      "Maarten de Rijke"
    ],
    "author_ids": [],
    "abstract": "Fairness is an increasingly important factor in re-ranking tasks. Prior work\nhas identified a trade-off between ranking accuracy and item fairness. However,\nthe underlying mechanisms are still not fully understood. An analogy can be\ndrawn between re-ranking and the dynamics of economic transactions. The\naccuracy-fairness trade-off parallels the coupling of the commodity tax\ntransfer process. Fairness considerations in re-ranking, similar to a commodity\ntax on suppliers, ultimately translate into a cost passed on to consumers.\nAnalogously, item-side fairness constraints result in a decline in user-side\naccuracy. In economics, the extent to which commodity tax on the supplier (item\nfairness) transfers to commodity tax on users (accuracy loss) is formalized\nusing the notion of elasticity. The re-ranking fairness-accuracy trade-off is\nsimilarly governed by the elasticity of utility between item groups. This\ninsight underscores the limitations of current fair re-ranking evaluations,\nwhich often rely solely on a single fairness metric, hindering comprehensive\nassessment of fair re-ranking algorithms. Centered around the concept of\nelasticity, this work presents two significant contributions. We introduce the\nElastic Fairness Curve (EF-Curve) as an evaluation framework. This framework\nenables a comparative analysis of algorithm performance across different\nelasticity levels, facilitating the selection of the most suitable approach.\nFurthermore, we propose ElasticRank, a fair re-ranking algorithm that employs\nelasticity calculations to adjust inter-item distances within a curved space.\nExperiments on three widely used ranking datasets demonstrate its effectiveness\nand efficiency.",
    "published_date": "2025-04-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.14991v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.14985v1",
    "title": "aiXamine: LLM Safety and Security Simplified",
    "authors": [
      "Fatih Deniz",
      "Dorde Popovic",
      "Yazan Boshmaf",
      "Euisuh Jeong",
      "Minhaj Ahmad",
      "Sanjay Chawla",
      "Issa Khalil"
    ],
    "author_ids": [],
    "abstract": "Evaluating Large Language Models (LLMs) for safety and security remains a\ncomplex task, often requiring users to navigate a fragmented landscape of ad\nhoc benchmarks, datasets, metrics, and reporting formats. To address this\nchallenge, we present aiXamine, a comprehensive black-box evaluation platform\nfor LLM safety and security. aiXamine integrates over 40 tests (i.e.,\nbenchmarks) organized into eight key services targeting specific dimensions of\nsafety and security: adversarial robustness, code security, fairness and bias,\nhallucination, model and data privacy, out-of-distribution (OOD) robustness,\nover-refusal, and safety alignment. The platform aggregates the evaluation\nresults into a single detailed report per model, providing a detailed breakdown\nof model performance, test examples, and rich visualizations. We used aiXamine\nto assess over 50 publicly available and proprietary LLMs, conducting over 2K\nexaminations. Our findings reveal notable vulnerabilities in leading models,\nincluding susceptibility to adversarial attacks in OpenAI's GPT-4o, biased\noutputs in xAI's Grok-3, and privacy weaknesses in Google's Gemini 2.0.\nAdditionally, we observe that open-source models can match or exceed\nproprietary models in specific services such as safety alignment, fairness and\nbias, and OOD robustness. Finally, we identify trade-offs between distillation\nstrategies, model size, training methods, and architectural choices.",
    "published_date": "2025-04-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.14985v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.14904v1",
    "title": "VLM as Policy: Common-Law Content Moderation Framework for Short Video Platform",
    "authors": [
      "Xingyu Lu",
      "Tianke Zhang",
      "Chang Meng",
      "Xiaobei Wang",
      "Jinpeng Wang",
      "YiFan Zhang",
      "Shisong Tang",
      "Changyi Liu",
      "Haojie Ding",
      "Kaiyu Jiang",
      "Kaiyu Tang",
      "Bin Wen",
      "Hai-Tao Zheng",
      "Fan Yang",
      "Tingting Gao",
      "Di Zhang",
      "Kun Gai"
    ],
    "author_ids": [],
    "abstract": "Exponentially growing short video platforms (SVPs) face significant\nchallenges in moderating content detrimental to users' mental health,\nparticularly for minors. The dissemination of such content on SVPs can lead to\ncatastrophic societal consequences. Although substantial efforts have been\ndedicated to moderating such content, existing methods suffer from critical\nlimitations: (1) Manual review is prone to human bias and incurs high\noperational costs. (2) Automated methods, though efficient, lack nuanced\ncontent understanding, resulting in lower accuracy. (3) Industrial moderation\nregulations struggle to adapt to rapidly evolving trends due to long update\ncycles. In this paper, we annotate the first SVP content moderation benchmark\nwith authentic user/reviewer feedback to fill the absence of benchmark in this\nfield. Then we evaluate various methods on the benchmark to verify the\nexistence of the aforementioned limitations. We further propose our common-law\ncontent moderation framework named KuaiMod to address these challenges. KuaiMod\nconsists of three components: training data construction, offline adaptation,\nand online deployment & refinement. Leveraging large vision language model\n(VLM) and Chain-of-Thought (CoT) reasoning, KuaiMod adequately models video\ntoxicity based on sparse user feedback and fosters dynamic moderation policy\nwith rapid update speed and high accuracy. Offline experiments and large-scale\nonline A/B test demonstrates the superiority of KuaiMod: KuaiMod achieves the\nbest moderation performance on our benchmark. The deployment of KuaiMod reduces\nthe user reporting rate by 20% and its application in video recommendation\nincreases both Daily Active User (DAU) and APP Usage Time (AUT) on several\nKuaishou scenarios. We have open-sourced our benchmark at\nhttps://kuaimod.github.io.",
    "published_date": "2025-04-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.CL",
      "cs.MM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.14904v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.14882v1",
    "title": "Some Optimizers are More Equal: Understanding the Role of Optimizers in Group Fairness",
    "authors": [
      "Mojtaba Kolahdouzi",
      "Hatice Gunes",
      "Ali Etemad"
    ],
    "author_ids": [],
    "abstract": "We study whether and how the choice of optimization algorithm can impact\ngroup fairness in deep neural networks. Through stochastic differential\nequation analysis of optimization dynamics in an analytically tractable setup,\nwe demonstrate that the choice of optimization algorithm indeed influences\nfairness outcomes, particularly under severe imbalance. Furthermore, we show\nthat when comparing two categories of optimizers, adaptive methods and\nstochastic methods, RMSProp (from the adaptive category) has a higher\nlikelihood of converging to fairer minima than SGD (from the stochastic\ncategory). Building on this insight, we derive two new theoretical guarantees\nshowing that, under appropriate conditions, RMSProp exhibits fairer parameter\nupdates and improved fairness in a single optimization step compared to SGD. We\nthen validate these findings through extensive experiments on three publicly\navailable datasets, namely CelebA, FairFace, and MS-COCO, across different\ntasks as facial expression recognition, gender classification, and multi-label\nclassification, using various backbones. Considering multiple fairness\ndefinitions including equalized odds, equal opportunity, and demographic\nparity, adaptive optimizers like RMSProp and Adam consistently outperform SGD\nin terms of group fairness, while maintaining comparable predictive accuracy.\nOur results highlight the role of adaptive updates as a crucial yet overlooked\nmechanism for promoting fair outcomes.",
    "published_date": "2025-04-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.14882v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.15322v1",
    "title": "How to systematically develop an effective AI-based bias correction model?",
    "authors": [
      "Xiao Zhou",
      "Yuze Sun",
      "Jie Wu",
      "Xiaomeng Huang"
    ],
    "author_ids": [],
    "abstract": "This study introduces ReSA-ConvLSTM, an artificial intelligence (AI)\nframework for systematic bias correction in numerical weather prediction (NWP).\nWe propose three innovations by integrating dynamic climatological\nnormalization, ConvLSTM with temporal causality constraints, and residual\nself-attention mechanisms. The model establishes a physics-aware nonlinear\nmapping between ECMWF forecasts and ERA5 reanalysis data. Using 41 years\n(1981-2021) of global atmospheric data, the framework reduces systematic biases\nin 2-m air temperature (T2m), 10-m winds (U10/V10), and sea-level pressure\n(SLP), achieving up to 20% RMSE reduction over 1-7 day forecasts compared to\noperational ECMWF outputs. The lightweight architecture (10.6M parameters)\nenables efficient generalization to multiple variables and downstream\napplications, reducing retraining time by 85% for cross-variable correction\nwhile improving ocean model skill through bias-corrected boundary conditions.\nThe ablation experiments demonstrate that our innovations significantly improve\nthe model's correction performance, suggesting that incorporating variable\ncharacteristics into the model helps enhance forecasting skills.",
    "published_date": "2025-04-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.ao-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.15322v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.14815v1",
    "title": "What Lurks Within? Concept Auditing for Shared Diffusion Models at Scale",
    "authors": [
      "Xiaoyong Yuan",
      "Xiaolong Ma",
      "Linke Guo",
      "Lan Zhang"
    ],
    "author_ids": [],
    "abstract": "Diffusion models (DMs) have revolutionized text-to-image generation, enabling\nthe creation of highly realistic and customized images from text prompts. With\nthe rise of parameter-efficient fine-tuning (PEFT) techniques like LoRA, users\ncan now customize powerful pre-trained models using minimal computational\nresources. However, the widespread sharing of fine-tuned DMs on open platforms\nraises growing ethical and legal concerns, as these models may inadvertently or\ndeliberately generate sensitive or unauthorized content, such as copyrighted\nmaterial, private individuals, or harmful content. Despite the increasing\nregulatory attention on generative AI, there are currently no practical tools\nfor systematically auditing these models before deployment. In this paper, we\naddress the problem of concept auditing: determining whether a fine-tuned DM\nhas learned to generate a specific target concept. Existing approaches\ntypically rely on prompt-based input crafting and output-based image\nclassification but suffer from critical limitations, including prompt\nuncertainty, concept drift, and poor scalability. To overcome these challenges,\nwe introduce Prompt-Agnostic Image-Free Auditing (PAIA), a novel, model-centric\nconcept auditing framework. By treating the DM as the object of inspection,\nPAIA enables direct analysis of internal model behavior, bypassing the need for\noptimized prompts or generated images. We evaluate PAIA on 320 controlled model\nand 690 real-world community models sourced from a public DM sharing platform.\nPAIA achieves over 90% detection accuracy while reducing auditing time by\n18-40x compared to existing baselines. To our knowledge, PAIA is the first\nscalable and practical solution for pre-deployment concept auditing of\ndiffusion models, providing a practical foundation for safer and more\ntransparent diffusion model sharing.",
    "published_date": "2025-04-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.14815v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.14769v1",
    "title": "Building babyGPTs: Youth Engaging in Data Practices and Ethical Considerations through the Construction of Generative Language Models",
    "authors": [
      "Luis Morales-Navarro",
      "Daniel J. Noh",
      "Yasmin B. Kafai"
    ],
    "author_ids": [],
    "abstract": "As generative language models (GLMs) have gained popularity, youth are\nincreasingly using them in their everyday lives. As such, most research has\ncentered on supporting youth as users of GLM-powered systems. However, we know\nlittle of how to engage youth in the design of these models. Building on the\nrich legacy of child-computer interaction research that positions youth as\ndesigners of computing systems, we explore how to support young people in\ndesigning GLMs. Through a case study of three teenagers (ages 14-15) building a\nbabyGPT screenplay generator, we illustrate how the team developed a model\nwhile engaging in artificial intelligence/machine learning-relevant data\npractices and addressing ethical issues. This paper contributes a case study\nthat demonstrates the feasibility of engaging youth in building GLMs.",
    "published_date": "2025-04-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.CY",
      "H.5.2; K.3.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.14769v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.14766v1",
    "title": "Disentangling Linguistic Features with Dimension-Wise Analysis of Vector Embeddings",
    "authors": [
      "Saniya Karwa",
      "Navpreet Singh"
    ],
    "author_ids": [],
    "abstract": "Understanding the inner workings of neural embeddings, particularly in models\nsuch as BERT, remains a challenge because of their high-dimensional and opaque\nnature. This paper proposes a framework for uncovering the specific dimensions\nof vector embeddings that encode distinct linguistic properties (LPs). We\nintroduce the Linguistically Distinct Sentence Pairs (LDSP-10) dataset, which\nisolates ten key linguistic features such as synonymy, negation, tense, and\nquantity. Using this dataset, we analyze BERT embeddings with various methods,\nincluding the Wilcoxon signed-rank test, mutual information, and recursive\nfeature elimination, to identify the most influential dimensions for each LP.\nWe introduce a new metric, the Embedding Dimension Impact (EDI) score, which\nquantifies the relevance of each embedding dimension to a LP. Our findings show\nthat certain properties, such as negation and polarity, are robustly encoded in\nspecific dimensions, while others, like synonymy, exhibit more complex\npatterns. This study provides insights into the interpretability of embeddings,\nwhich can guide the development of more transparent and optimized language\nmodels, with implications for model bias mitigation and the responsible\ndeployment of AI systems.",
    "published_date": "2025-04-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.14766v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.14716v1",
    "title": "Pairwise or Pointwise? Evaluating Feedback Protocols for Bias in LLM-Based Evaluation",
    "authors": [
      "Tuhina Tripathi",
      "Manya Wadhwa",
      "Greg Durrett",
      "Scott Niekum"
    ],
    "author_ids": [],
    "abstract": "Large Language Models (LLMs) are widely used as proxies for human labelers in\nboth training (Reinforcement Learning from AI Feedback) and large-scale\nresponse evaluation (LLM-as-a-judge). Alignment and evaluation are critical\ncomponents in the development of reliable LLMs, and the choice of feedback\nprotocol plays a central role in both but remains understudied. In this work,\nwe show that the choice of feedback protocol (absolute scores versus relative\npreferences) can significantly affect evaluation reliability and induce\nsystematic biases. In particular, we show that pairwise evaluation protocols\nare more vulnerable to distracted evaluation. Generator models can exploit\nspurious attributes (or distractor features) favored by the LLM judge,\nresulting in inflated scores for lower-quality outputs and misleading training\nsignals. We find that absolute scoring is more robust to such manipulation,\nproducing judgments that better reflect response quality and are less\ninfluenced by distractor features. Our results demonstrate that generator\nmodels can flip preferences by embedding distractor features, skewing\nLLM-as-a-judge comparisons and leading to inaccurate conclusions about model\nquality in benchmark evaluations. Pairwise preferences flip in about 35% of the\ncases, compared to only 9% for absolute scores. We offer recommendations for\nchoosing feedback protocols based on dataset characteristics and evaluation\nobjectives.",
    "published_date": "2025-04-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.14716v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.14690v1",
    "title": "FarsEval-PKBETS: A new diverse benchmark for evaluating Persian large language models",
    "authors": [
      "Mehrnoush Shamsfard",
      "Zahra Saaberi",
      "Mostafa Karimi manesh",
      "Seyed Mohammad Hossein Hashemi",
      "Zahra Vatankhah",
      "Motahareh Ramezani",
      "Niki Pourazin",
      "Tara Zare",
      "Maryam Azimi",
      "Sarina Chitsaz",
      "Sama Khoraminejad",
      "Morteza Mahdavi Mortazavi",
      "Mohammad Mahdi Chizari",
      "Sahar Maleki",
      "Seyed Soroush Majd",
      "Mostafa Masumi",
      "Sayed Ali Musavi Khoeini",
      "Amir Mohseni",
      "Sogol Alipour"
    ],
    "author_ids": [],
    "abstract": "Research on evaluating and analyzing large language models (LLMs) has been\nextensive for resource-rich languages such as English, yet their performance in\nlanguages such as Persian has received considerably less attention. This paper\nintroduces FarsEval-PKBETS benchmark, a subset of FarsEval project for\nevaluating large language models in Persian. This benchmark consists of 4000\nquestions and answers in various formats, including multiple choice, short\nanswer and descriptive responses. It covers a wide range of domains and\ntasks,including medicine, law, religion, Persian language, encyclopedic\nknowledge, human preferences, social knowledge, ethics and bias, text\ngeneration, and respecting others' rights. This bechmark incorporates\nlinguistics, cultural, and local considerations relevant to the Persian\nlanguage and Iran. To ensure the questions are challenging for current LLMs,\nthree models -- Llama3-70B, PersianMind, and Dorna -- were evaluated using this\nbenchmark. Their average accuracy was below 50%, meaning they provided fully\ncorrect answers to fewer than half of the questions. These results indicate\nthat current language models are still far from being able to solve this\nbenchmark",
    "published_date": "2025-04-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "68T50",
      "I.2.7; E.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.14690v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.14683v1",
    "title": "Polynomial-Time Constant-Approximation for Fair Sum-of-Radii Clustering",
    "authors": [
      "Sina Bagheri Nezhad",
      "Sayan Bandyapadhyay",
      "Tianzhi Chen"
    ],
    "author_ids": [],
    "abstract": "In a seminal work, Chierichetti et al. introduced the $(t,k)$-fair clustering\nproblem: Given a set of red points and a set of blue points in a metric space,\na clustering is called fair if the number of red points in each cluster is at\nmost $t$ times and at least $1/t$ times the number of blue points in that\ncluster. The goal is to compute a fair clustering with at most $k$ clusters\nthat optimizes certain objective function. Considering this problem, they\ndesigned a polynomial-time $O(1)$- and $O(t)$-approximation for the $k$-center\nand the $k$-median objective, respectively. Recently, Carta et al. studied this\nproblem with the sum-of-radii objective and obtained a\n$(6+\\epsilon)$-approximation with running time\n$O((k\\log_{1+\\epsilon}(k/\\epsilon))^kn^{O(1)})$, i.e., fixed-parameter\ntractable in $k$. Here $n$ is the input size. In this work, we design the first\npolynomial-time $O(1)$-approximation for $(t,k)$-fair clustering with the\nsum-of-radii objective, improving the result of Carta et al. Our result places\nsum-of-radii in the same group of objectives as $k$-center, that admit\npolynomial-time $O(1)$-approximations. This result also implies a\npolynomial-time $O(1)$-approximation for the Euclidean version of the problem,\nfor which an $f(k)\\cdot n^{O(1)}$-time $(1+\\epsilon)$-approximation was known\ndue to Drexler et al.. Here $f$ is an exponential function of $k$. We are also\nable to extend our result to any arbitrary $\\ell\\ge 2$ number of colors when\n$t=1$. This matches known results for the $k$-center and $k$-median objectives\nin this case. The significant disparity of sum-of-radii compared to $k$-center\nand $k$-median presents several complex challenges, all of which we\nsuccessfully overcome in our work. Our main contribution is a novel\ncluster-merging-based analysis technique for sum-of-radii that helps us achieve\nthe constant-approximation bounds.",
    "published_date": "2025-04-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.14683v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.14663v1",
    "title": "The Developer Experience of LGBTQIA+ People in Agile Teams: a Multivocal Literature Review",
    "authors": [
      "Edvaldo Wassouf Jr",
      "Débora Paiva",
      "Kiev Gama",
      "Awdren Fontão"
    ],
    "author_ids": [],
    "abstract": "Research on underrepresented populations is essential for fostering greater\ndiversity within the software industry. Team diversity is important for reasons\nthat go beyond ethics. Diversity contributes to greater innovation and\nproductivity, helping decrease turnover rates and reduce team conflicts. Within\nthis context, LGBTQIA+ software engineering professionals face unique\nchallenges, e.g., self-isolation and invisibility feeling. Developer Experience\n(DX) encompasses cognitive, emotional, and motivational considerations,\nsupporting the idea that improving how DX can enhance team performance,\nstrengthen collaboration, and lead to more successful software projects. This\nstudy aimed to examine traditional and grey literature data through a\nMultivocal Literature Review focused on the DX of LGBTQIA+ professionals in\nagile teams. Our findings reveal that issues such as invisibility, prejudice,\nand discrimination adversely affect their experiences, compounded by the\npredominance of heterosexual males in the field. Conversely, professionals who\nfeel welcomed by their teams and organizations, especially in processes\ntailored to their needs, report more positive team dynamics and engagement.",
    "published_date": "2025-04-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.14663v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.14657v1",
    "title": "A Case Study Exploring the Current Landscape of Synthetic Medical Record Generation with Commercial LLMs",
    "authors": [
      "Yihan Lin",
      "Zhirong Bella Yu",
      "Simon Lee"
    ],
    "author_ids": [],
    "abstract": "Synthetic Electronic Health Records (EHRs) offer a valuable opportunity to\ncreate privacy preserving and harmonized structured data, supporting numerous\napplications in healthcare. Key benefits of synthetic data include precise\ncontrol over the data schema, improved fairness and representation of patient\npopulations, and the ability to share datasets without concerns about\ncompromising real individuals privacy. Consequently, the AI community has\nincreasingly turned to Large Language Models (LLMs) to generate synthetic data\nacross various domains. However, a significant challenge in healthcare is\nensuring that synthetic health records reliably generalize across different\nhospitals, a long standing issue in the field. In this work, we evaluate the\ncurrent state of commercial LLMs for generating synthetic data and investigate\nmultiple aspects of the generation process to identify areas where these models\nexcel and where they fall short. Our main finding from this work is that while\nLLMs can reliably generate synthetic health records for smaller subsets of\nfeatures, they struggle to preserve realistic distributions and correlations as\nthe dimensionality of the data increases, ultimately limiting their ability to\ngeneralize across diverse hospital settings.",
    "published_date": "2025-04-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.14657v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.14623v1",
    "title": "Synthesising Asynchronous Automata from Fair Specifications",
    "authors": [
      "Béatrice Bérard",
      "Benjamin Monmege",
      "B Srivathsan",
      "Arnab Sur"
    ],
    "author_ids": [],
    "abstract": "Asynchronous automata are a model of distributed finite state processes\nsynchronising on shared actions. A celebrated result by Zielonka shows how a\ndeterministic asynchronous automaton (AA) can be synthesised, starting from two\ninputs: a global specification as a deterministic finite-state automaton (DFA)\nand a distribution of the alphabet into local alphabets for each process. The\ntranslation is particularly complex and has been revisited several times. In\nthis work, we revisit this construction on a restricted class of fair\nspecifications: a DFA described a fair specification if in every loop, all\nprocesses participate in at least one action - so, no process is starved. For\nfair specifications, we present a new construction to synthesise an AA. Our\nconstruction is conceptually simpler and results in an AA where every process\nhas a number of local states that is linear in the number of states of the DFA,\nand where the only exponential explosion is related to a parameter of fairness\n(the length of the longest word that can be read in the DFA in which not every\nprocess participates). Finally, we show how this construction can be combined\nwith an existing construction for hierarchical process architectures.",
    "published_date": "2025-04-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.FL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.14623v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.14550v1",
    "title": "Regret-aware Re-ranking for Guaranteeing Two-sided Fairness and Accuracy in Recommender Systems",
    "authors": [
      "Xiaopeng Ye",
      "Chen Xu",
      "Jun Xu",
      "Xuyang Xie",
      "Gang Wang",
      "Zhenhua Dong"
    ],
    "author_ids": [],
    "abstract": "In multi-stakeholder recommender systems (RS), users and providers operate as\ntwo crucial and interdependent roles, whose interests must be well-balanced.\nPrior research, including our work BankFair, has demonstrated the importance of\nguaranteeing both provider fairness and user accuracy to meet their interests.\nHowever, when they balance the two objectives, another critical factor emerges\nin RS: individual fairness, which manifests as a significant disparity in\nindividual recommendation accuracy, with some users receiving high accuracy\nwhile others are left with notably low accuracy. This oversight severely harms\nthe interests of users and exacerbates social polarization. How to guarantee\nindividual fairness while ensuring user accuracy and provider fairness remains\nan unsolved problem. To bridge this gap, in this paper, we propose our method\nBankFair+. Specifically, BankFair+ extends BankFair with two steps: (1)\nintroducing a non-linear function from regret theory to ensure individual\nfairness while enhancing user accuracy; (2) formulating the re-ranking process\nas a regret-aware fuzzy programming problem to meet the interests of both\nindividual user and provider, therefore balancing the trade-off between\nindividual fairness and provider fairness. Experiments on two real-world\nrecommendation datasets demonstrate that BankFair+ outperforms all baselines\nregarding individual fairness, user accuracy, and provider fairness.",
    "published_date": "2025-04-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.14550v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.14536v1",
    "title": "The Ephemeral Shadow: Hyperreal Beings in Stimulative Performance",
    "authors": [
      "Dong Zhang",
      "Yanjun Zhou",
      "Jingyi Yu"
    ],
    "author_ids": [],
    "abstract": "The Ephemeral Shadow is an interactive art installation centered on the\nconcept of \"simulacrum,\" focusing on the reconstruction of subjectivity at the\nintersection of reality and virtuality. Drawing inspiration from the aesthetic\nimagery of traditional shadow puppetry, the installation combines robotic\nperformance and digital projection to create a multi-layered visual space,\npresenting a progressively dematerialized hyperreal experience. By blurring the\naudience's perception of the boundaries between entity and image, the work\nemploys the replacement of physical presence with imagery as its core\ntechnique, critically reflecting on issues of technological subjectivity,\naffective computing, and ethics. Situated within the context of posthumanism\nand digital media, the installation prompts viewers to contemplate: as digital\ntechnologies increasingly approach and simulate \"humanity,\" how can we reshape\nidentity and perception while safeguarding the core values and ethical\nprinciples of human subjectivity?",
    "published_date": "2025-04-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.14536v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.14522v1",
    "title": "Biased by Design: Leveraging AI Biases to Enhance Critical Thinking of News Readers",
    "authors": [
      "Liudmila Zavolokina",
      "Kilian Sprenkamp",
      "Zoya Katashinskaya",
      "Daniel Gordon Jones"
    ],
    "author_ids": [],
    "abstract": "This paper explores the design of a propaganda detection tool using Large\nLanguage Models (LLMs). Acknowledging the inherent biases in AI models,\nespecially in political contexts, we investigate how these biases might be\nleveraged to enhance critical thinking in news consumption. Countering the\ntypical view of AI biases as detrimental, our research proposes strategies of\nuser choice and personalization in response to a user's political stance,\napplying psychological concepts of confirmation bias and cognitive dissonance.\nWe present findings from a qualitative user study, offering insights and design\nrecommendations (bias awareness, personalization and choice, and gradual\nintroduction of diverse perspectives) for AI tools in propaganda detection.",
    "published_date": "2025-04-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.14522v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.15313v1",
    "title": "PolicyEvol-Agent: Evolving Policy via Environment Perception and Self-Awareness with Theory of Mind",
    "authors": [
      "Yajie Yu",
      "Yue Feng"
    ],
    "author_ids": [],
    "abstract": "Multi-agents has exhibited significant intelligence in real-word simulations\nwith Large language models (LLMs) due to the capabilities of social cognition\nand knowledge retrieval. However, existing research on agents equipped with\neffective cognition chains including reasoning, planning, decision-making and\nreflecting remains limited, especially in the dynamically interactive\nscenarios. In addition, unlike human, prompt-based responses face challenges in\npsychological state perception and empirical calibration during uncertain\ngaming process, which can inevitably lead to cognition bias. In light of above,\nwe introduce PolicyEvol-Agent, a comprehensive LLM-empowered framework\ncharacterized by systematically acquiring intentions of others and adaptively\noptimizing irrational strategies for continual enhancement. Specifically,\nPolicyEvol-Agent first obtains reflective expertise patterns and then\nintegrates a range of cognitive operations with Theory of Mind alongside\ninternal and external perspectives. Simulation results, outperforming RL-based\nmodels and agent-based methods, demonstrate the superiority of PolicyEvol-Agent\nfor final gaming victory. Moreover, the policy evolution mechanism reveals the\neffectiveness of dynamic guideline adjustments in both automatic and human\nevaluation.",
    "published_date": "2025-04-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.15313v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.14492v1",
    "title": "FairSteer: Inference Time Debiasing for LLMs with Dynamic Activation Steering",
    "authors": [
      "Yichen Li",
      "Zhiting Fan",
      "Ruizhe Chen",
      "Xiaotang Gai",
      "Luqi Gong",
      "Yan Zhang",
      "Zuozhu Liu"
    ],
    "author_ids": [],
    "abstract": "Large language models (LLMs) are prone to capturing biases from training\ncorpus, leading to potential negative social impacts. Existing prompt-based\ndebiasing methods exhibit instability due to their sensitivity to prompt\nchanges, while fine-tuning-based techniques incur substantial computational\noverhead and catastrophic forgetting. In this paper, we propose FairSteer, a\nnovel inference-time debiasing framework without requiring customized prompt\ndesign or model retraining. Motivated by the linear representation hypothesis,\nour preliminary investigation demonstrates that fairness-related features can\nbe encoded into separable directions in the hidden activation space. FairSteer\noperates in three steps: biased activation detection, debiasing steering vector\n(DSV) computation, and dynamic activation steering. Specifically, it first\ntrains a lightweight linear classifier to detect bias signatures in\nactivations, and then computes DSVs as intervention directions derived from\nsmall contrastive prompt pairs. Subsequently, it performs debiasing by\nadjusting activations with DSVs in the inference stage. Comprehensive\nevaluation with six LLMs demonstrates the superiority of FairSteer across\nquestion-answering, counterfactual input evaluation and open-ended text\ngeneration tasks. Code will be released.",
    "published_date": "2025-04-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.14492v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.14446v1",
    "title": "Neglected Risks: The Disturbing Reality of Children's Images in Datasets and the Urgent Call for Accountability",
    "authors": [
      "Carlos Caetano",
      "Gabriel O. dos Santos",
      "Caio Petrucci",
      "Artur Barros",
      "Camila Laranjeira",
      "Leo S. F. Ribeiro",
      "Júlia F. de Mendonça",
      "Jefersson A. dos Santos",
      "Sandra Avila"
    ],
    "author_ids": [],
    "abstract": "Including children's images in datasets has raised ethical concerns,\nparticularly regarding privacy, consent, data protection, and accountability.\nThese datasets, often built by scraping publicly available images from the\nInternet, can expose children to risks such as exploitation, profiling, and\ntracking. Despite the growing recognition of these issues, approaches for\naddressing them remain limited. We explore the ethical implications of using\nchildren's images in AI datasets and propose a pipeline to detect and remove\nsuch images. As a use case, we built the pipeline on a Vision-Language Model\nunder the Visual Question Answering task and tested it on the #PraCegoVer\ndataset. We also evaluate the pipeline on a subset of 100,000 images from the\nOpen Images V7 dataset to assess its effectiveness in detecting and removing\nimages of children. The pipeline serves as a baseline for future research,\nproviding a starting point for more comprehensive tools and methodologies.\nWhile we leverage existing models trained on potentially problematic data, our\ngoal is to expose and address this issue. We do not advocate for training or\ndeploying such models, but instead call for urgent community reflection and\naction to protect children's rights. Ultimately, we aim to encourage the\nresearch community to exercise - more than an additional - care in creating new\ndatasets and to inspire the development of tools to protect the fundamental\nrights of vulnerable groups, particularly children.",
    "published_date": "2025-04-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.14446v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.14388v1",
    "title": "Balancing Fairness and Performance in Healthcare AI: A Gradient Reconciliation Approach",
    "authors": [
      "Xiaoyang Wang",
      "Christopher C. Yang"
    ],
    "author_ids": [],
    "abstract": "The rapid growth of healthcare data and advances in computational power have\naccelerated the adoption of artificial intelligence (AI) in medicine. However,\nAI systems deployed without explicit fairness considerations risk exacerbating\nexisting healthcare disparities, potentially leading to inequitable resource\nallocation and diagnostic disparities across demographic subgroups. To address\nthis challenge, we propose FairGrad, a novel gradient reconciliation framework\nthat automatically balances predictive performance and multi-attribute fairness\noptimization in healthcare AI models. Our method resolves conflicting\noptimization objectives by projecting each gradient vector onto the orthogonal\nplane of the others, thereby regularizing the optimization trajectory to ensure\nequitable consideration of all objectives. Evaluated on diverse real-world\nhealthcare datasets and predictive tasks - including Substance Use Disorder\n(SUD) treatment and sepsis mortality - FairGrad achieved statistically\nsignificant improvements in multi-attribute fairness metrics (e.g., equalized\nodds) while maintaining competitive predictive accuracy. These results\ndemonstrate the viability of harmonizing fairness and utility in\nmission-critical medical AI applications.",
    "published_date": "2025-04-19T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.14388v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.14378v1",
    "title": "Machine learning enhanced atom probe tomography analysis: a snapshot review",
    "authors": [
      "Yue Li",
      "Ye Wei",
      "Alaukik Saxena",
      "Markus Kühbach",
      "Christoph Freysoldt",
      "Baptiste Gault"
    ],
    "author_ids": [],
    "abstract": "Atom probe tomography (APT) is a burgeoning characterization technique that\nprovides compositional mapping of materials in three-dimensions at near-atomic\nscale. Since its significant expansion in the past 30 years, we estimate that\none million APT datasets have been collected, each containing millions to\nbillions of individual ions. Their analysis and the extraction of\nmicrostructural information has largely relied upon individual users whose\nvaried level of expertise causes clear and documented bias. Current practices\nhinder efficient data processing, and make challenging standardization and the\ndeployment of data analysis workflows that would be compliant with FAIR data\nprinciples. Over the past decade, building upon the long-standing expertise of\nthe APT community in the development of advanced data processing or data mining\ntechniques, there has been a surge of novel machine learning (ML) approaches\naiming for user-independence, and that are efficient, reproducible, and robust\nfrom a statistics perspective. Here, we provide a snapshot review of this\nrapidly evolving field. We begin with a brief introduction to APT and the\nnature of the APT data. This is followed by an overview of relevant ML\nalgorithms and a comprehensive review of their applications to APT. We also\ndiscuss how ML can enable discoveries beyond human capability, offering new\ninsights into the mechanisms within materials. Finally, we provide guidance for\nfuture directions in this domain.",
    "published_date": "2025-04-19T00:00:00",
    "year": 2025,
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.14378v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.14359v1",
    "title": "A Multimodal Recaptioning Framework to Account for Perceptual Diversity in Multilingual Vision-Language Modeling",
    "authors": [
      "Kyle Buettner",
      "Jacob Emmerson",
      "Adriana Kovashka"
    ],
    "author_ids": [],
    "abstract": "There are many ways to describe, name, and group objects when captioning an\nimage. Differences are evident when speakers come from diverse cultures due to\nthe unique experiences that shape perception. Machine translation of captions\nhas pushed multilingual capabilities in vision-language models (VLMs), but data\ncomes mainly from English speakers, indicating a perceptual bias and lack of\nmodel flexibility. In this work, we address this challenge and outline a\ndata-efficient framework to instill multilingual VLMs with greater\nunderstanding of perceptual diversity. We specifically propose an LLM-based,\nmultimodal recaptioning strategy that alters the object descriptions of English\ncaptions before translation. The greatest benefits are demonstrated in a\ntargeted multimodal mechanism guided by native speaker data. By adding produced\nrewrites as augmentations in training, we improve on German and Japanese\ntext-image retrieval cases studies (up to +3.5 mean recall overall, +4.7 on\nnon-native error cases). We further propose a mechanism to analyze the specific\nobject description differences across datasets, and we offer insights into\ncross-dataset and cross-language generalization.",
    "published_date": "2025-04-19T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.14359v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.14325v2",
    "title": "FAIRGAME: a Framework for AI Agents Bias Recognition using Game Theory",
    "authors": [
      "Alessio Buscemi",
      "Daniele Proverbio",
      "Alessandro Di Stefano",
      "The Anh Han",
      "German Castignani",
      "Pietro Liò"
    ],
    "author_ids": [],
    "abstract": "Letting AI agents interact in multi-agent applications adds a layer of\ncomplexity to the interpretability and prediction of AI outcomes, with profound\nimplications for their trustworthy adoption in research and society. Game\ntheory offers powerful models to capture and interpret strategic interaction\namong agents, but requires the support of reproducible, standardized and\nuser-friendly IT frameworks to enable comparison and interpretation of results.\nTo this end, we present FAIRGAME, a Framework for AI Agents Bias Recognition\nusing Game Theory. We describe its implementation and usage, and we employ it\nto uncover biased outcomes in popular games among AI agents, depending on the\nemployed Large Language Model (LLM) and used language, as well as on the\npersonality trait or strategic knowledge of the agents. Overall, FAIRGAME\nallows users to reliably and easily simulate their desired games and scenarios\nand compare the results across simulation campaigns and with game-theoretic\npredictions, enabling the systematic discovery of biases, the anticipation of\nemerging behavior out of strategic interplays, and empowering further research\ninto strategic decision-making using LLM agents.",
    "published_date": "2025-04-19T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.14325v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.14212v1",
    "title": "Bias Analysis and Mitigation through Protected Attribute Detection and Regard Classification",
    "authors": [
      "Takuma Udagawa",
      "Yang Zhao",
      "Hiroshi Kanayama",
      "Bishwaranjan Bhattacharjee"
    ],
    "author_ids": [],
    "abstract": "Large language models (LLMs) acquire general linguistic knowledge from\nmassive-scale pretraining. However, pretraining data mainly comprised of\nweb-crawled texts contain undesirable social biases which can be perpetuated or\neven amplified by LLMs. In this study, we propose an efficient yet effective\nannotation pipeline to investigate social biases in the pretraining corpora.\nOur pipeline consists of protected attribute detection to identify diverse\ndemographics, followed by regard classification to analyze the language\npolarity towards each attribute. Through our experiments, we demonstrate the\neffect of our bias analysis and mitigation measures, focusing on Common Crawl\nas the most representative pretraining corpus.",
    "published_date": "2025-04-19T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.14212v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.14205v1",
    "title": "Dual-channel Heterophilic Message Passing for Graph Fraud Detection",
    "authors": [
      "Wenxin Zhang",
      "Jingxing Zhong",
      "Guangzhen Yao",
      "Renda Han",
      "Xiaojian Lin",
      "Zeyu Zhang",
      "Cuicui Luo"
    ],
    "author_ids": [],
    "abstract": "Fraudulent activities have significantly increased across various domains,\nsuch as e-commerce, online review platforms, and social networks, making fraud\ndetection a critical task. Spatial Graph Neural Networks (GNNs) have been\nsuccessfully applied to fraud detection tasks due to their strong inductive\nlearning capabilities. However, existing spatial GNN-based methods often\nenhance the graph structure by excluding heterophilic neighbors during message\npassing to align with the homophilic bias of GNNs. Unfortunately, this approach\ncan disrupt the original graph topology and increase uncertainty in\npredictions. To address these limitations, this paper proposes a novel\nframework, Dual-channel Heterophilic Message Passing (DHMP), for fraud\ndetection. DHMP leverages a heterophily separation module to divide the graph\ninto homophilic and heterophilic subgraphs, mitigating the low-pass inductive\nbias of traditional GNNs. It then applies shared weights to capture signals at\ndifferent frequencies independently and incorporates a customized sampling\nstrategy for training. This allows nodes to adaptively balance the\ncontributions of various signals based on their labels. Extensive experiments\non three real-world datasets demonstrate that DHMP outperforms existing\nmethods, highlighting the importance of separating signals with different\nfrequencies for improved fraud detection. The code is available at\nhttps://github.com/shaieesss/DHMP.",
    "published_date": "2025-04-19T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.14205v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.14150v1",
    "title": "Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations",
    "authors": [
      "Katie Matton",
      "Robert Osazuwa Ness",
      "John Guttag",
      "Emre Kıcıman"
    ],
    "author_ids": [],
    "abstract": "Large language models (LLMs) are capable of generating plausible explanations\nof how they arrived at an answer to a question. However, these explanations can\nmisrepresent the model's \"reasoning\" process, i.e., they can be unfaithful.\nThis, in turn, can lead to over-trust and misuse. We introduce a new approach\nfor measuring the faithfulness of LLM explanations. First, we provide a\nrigorous definition of faithfulness. Since LLM explanations mimic human\nexplanations, they often reference high-level concepts in the input question\nthat purportedly influenced the model. We define faithfulness in terms of the\ndifference between the set of concepts that LLM explanations imply are\ninfluential and the set that truly are. Second, we present a novel method for\nestimating faithfulness that is based on: (1) using an auxiliary LLM to modify\nthe values of concepts within model inputs to create realistic counterfactuals,\nand (2) using a Bayesian hierarchical model to quantify the causal effects of\nconcepts at both the example- and dataset-level. Our experiments show that our\nmethod can be used to quantify and discover interpretable patterns of\nunfaithfulness. On a social bias task, we uncover cases where LLM explanations\nhide the influence of social bias. On a medical question answering task, we\nuncover cases where LLM explanations provide misleading claims about which\npieces of evidence influenced the model's decisions.",
    "published_date": "2025-04-19T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.14150v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.14137v1",
    "title": "Rethinking Target Label Conditioning in Adversarial Attacks: A 2D Tensor-Guided Generative Approach",
    "authors": [
      "Hangyu Liu",
      "Bo Peng",
      "Pengxiang Ding",
      "Donglin Wang"
    ],
    "author_ids": [],
    "abstract": "Compared to single-target adversarial attacks, multi-target attacks have\ngarnered significant attention due to their ability to generate adversarial\nimages for multiple target classes simultaneously. Existing generative\napproaches for multi-target attacks mainly analyze the effect of the use of\ntarget labels on noise generation from a theoretical perspective, lacking\npractical validation and comprehensive summarization. To address this gap, we\nfirst identify and validate that the semantic feature quality and quantity are\ncritical factors affecting the transferability of targeted attacks: 1) Feature\nquality refers to the structural and detailed completeness of the implanted\ntarget features, as deficiencies may result in the loss of key discriminative\ninformation; 2) Feature quantity refers to the spatial sufficiency of the\nimplanted target features, as inadequacy limits the victim model's attention to\nthis feature. Based on these findings, we propose the 2D Tensor-Guided\nAdversarial Fusion (2D-TGAF) framework, which leverages the powerful generative\ncapabilities of diffusion models to encode target labels into two-dimensional\nsemantic tensors for guiding adversarial noise generation. Additionally, we\ndesign a novel masking strategy tailored for the training process, ensuring\nthat parts of the generated noise retain complete semantic information about\nthe target class. Extensive experiments on the standard ImageNet dataset\ndemonstrate that 2D-TGAF consistently surpasses state-of-the-art methods in\nattack success rates, both on normally trained models and across various\ndefense mechanisms.",
    "published_date": "2025-04-19T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.14137v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.14120v1",
    "title": "Inclusive Education with AI: Supporting Special Needs and Tackling Language Barriers",
    "authors": [
      "Ricardo Fitas"
    ],
    "author_ids": [],
    "abstract": "Early childhood classrooms are becoming increasingly diverse, with students\nspanning a range of linguistic backgrounds and abilities. AI offers innovative\ntools to help educators create more inclusive learning environments by breaking\ndown language barriers and providing tailored support for children with special\nneeds. This chapter provides a comprehensive review of how AI technologies can\nfacilitate inclusion in early education. It is discussed AI-driven language\nassistance tools that enable real-time translation and communication in\nmultilingual classrooms, and it is explored assistive technologies powered by\nAI that personalize learning for students with disabilities. The implications\nof these technologies for teachers are examined, including shifts in educator\nroles and workloads. General outcomes observed with AI integration - such as\nimproved student engagement and performance - as well as challenges related to\nequitable access and the need for ethical implementation are highlighted.\nFinally, practical recommendations for educators, policymakers, and developers\nare offered to collaboratively harness AI in a responsible manner, ensuring\nthat its benefits reach all learners.",
    "published_date": "2025-04-19T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.14120v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.13792v1",
    "title": "The Binary and Ternary Quantization Can Improve Feature Discrimination",
    "authors": [
      "Weizhi Lu",
      "Mingrui Chen",
      "Weiyu Li"
    ],
    "author_ids": [],
    "abstract": "In machine learning, quantization is widely used to simplify data\nrepresentation and facilitate algorithm deployment on hardware. Given the\nfundamental role of classification in machine learning, it is crucial to\ninvestigate the impact of quantization on classification. Current research\nprimarily focuses on quantization errors, operating under the premise that\nhigher quantization errors generally result in lower classification\nperformance. However, this premise lacks a solid theoretical foundation and\noften contradicts empirical findings. For instance, certain extremely low\nbit-width quantization methods, such as $\\{0,1\\}$-binary quantization and $\\{0,\n\\pm1\\}$-ternary quantization, can achieve comparable or even superior\nclassification accuracy compared to the original non-quantized data, despite\nexhibiting high quantization errors. To more accurately evaluate classification\nperformance, we propose to directly investigate the feature discrimination of\nquantized data, instead of analyzing its quantization error. Interestingly, it\nis found that both binary and ternary quantization methods can improve, rather\nthan degrade, the feature discrimination of the original data. This remarkable\nperformance is validated through classification experiments across various data\ntypes, including images, speech, and texts.",
    "published_date": "2025-04-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.13792v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.13791v1",
    "title": "Collective Learning Mechanism based Optimal Transport Generative Adversarial Network for Non-parallel Voice Conversion",
    "authors": [
      "Sandipan Dhar",
      "Md. Tousin Akhter",
      "Nanda Dulal Jana",
      "Swagatam Das"
    ],
    "author_ids": [],
    "abstract": "After demonstrating significant success in image synthesis, Generative\nAdversarial Network (GAN) models have likewise made significant progress in the\nfield of speech synthesis, leveraging their capacity to adapt the precise\ndistribution of target data through adversarial learning processes. Notably, in\nthe realm of State-Of-The-Art (SOTA) GAN-based Voice Conversion (VC) models,\nthere exists a substantial disparity in naturalness between real and\nGAN-generated speech samples. Furthermore, while many GAN models currently\noperate on a single generator discriminator learning approach, optimizing\ntarget data distribution is more effectively achievable through a single\ngenerator multi-discriminator learning scheme. Hence, this study introduces a\nnovel GAN model named Collective Learning Mechanism-based Optimal Transport GAN\n(CLOT-GAN) model, incorporating multiple discriminators, including the Deep\nConvolutional Neural Network (DCNN) model, Vision Transformer (ViT), and\nconformer. The objective of integrating various discriminators lies in their\nability to comprehend the formant distribution of mel-spectrograms, facilitated\nby a collective learning mechanism. Simultaneously, the inclusion of Optimal\nTransport (OT) loss aims to precisely bridge the gap between the source and\ntarget data distribution, employing the principles of OT theory. The\nexperimental validation on VCC 2018, VCTK, and CMU-Arctic datasets confirms\nthat the CLOT-GAN-VC model outperforms existing VC models in objective and\nsubjective assessments.",
    "published_date": "2025-04-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.13791v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.13790v1",
    "title": "Four Bottomless Errors and the Collapse of Statistical Fairness",
    "authors": [
      "James Brusseau"
    ],
    "author_ids": [],
    "abstract": "The AI ethics of statistical fairness is an error, the approach should be\nabandoned, and the accumulated academic work deleted. The argument proceeds by\nidentifying four recurring mistakes within statistical fairness. One conflates\nfairness with equality, which confines thinking to similars being treated\nsimilarly. The second and third errors derive from a perspectival ethical view\nwhich functions by negating others and their viewpoints. The final mistake\nconstrains fairness to work within predefined social groups instead of allowing\nunconstrained fairness to subsequently define group composition. From the\nnature of these misconceptions, the larger argument follows. Because the errors\nare integral to how statistical fairness works, attempting to resolve the\ndifficulties only deepens them. Consequently, the errors cannot be corrected\nwithout undermining the larger project, and statistical fairness collapses from\nwithin. While the collapse ends a failure in ethics, it also provokes distinct\npossibilities for fairness, data, and algorithms. Quickly indicating some of\nthese directions is a secondary aim of the paper, and one that aligns with what\nfairness has consistently meant and done since Aristotle.",
    "published_date": "2025-04-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.13790v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.13772v1",
    "title": "Bake Two Cakes with One Oven: RL for Defusing Popularity Bias and Cold-start in Third-Party Library Recommendations",
    "authors": [
      "Minh Hoang Vuong",
      "Anh M. T. Bui",
      "Phuong T. Nguyen",
      "Davide Di Ruscio"
    ],
    "author_ids": [],
    "abstract": "Third-party libraries (TPLs) have become an integral part of modern software\ndevelopment, enhancing developer productivity and accelerating time-to-market.\nHowever, identifying suitable candidates from a rapidly growing and\ncontinuously evolving collection of TPLs remains a challenging task. TPL\nrecommender systems have been studied, offering a promising solution to address\nthis issue. They typically rely on collaborative filtering (CF) that exploits a\ntwo-dimensional project-library matrix (user-item in general context of\nrecommendation) when making recommendations. We have noticed that CF-based\napproaches often encounter two challenges: (i) a tendency to recommend popular\nitems more frequently, making them even more dominant, a phenomenon known as\npopularity bias, and (ii) difficulty in generating recommendations for new\nusers or items due to limited user-item interactions, commonly referred to as\nthe cold-start problem. In this paper, we propose a reinforcement learning\n(RL)-based approach to address popularity bias and the cold-start problem in\nTPL recommendation. Our method comprises three key components. First, we\nutilize a graph convolution network (GCN)-based embedding model to learn user\npreferences and user-item interactions, allowing us to capture complex\nrelationships within interaction subgraphs and effectively represent new\nuser/item embeddings. Second, we introduce an aggregation operator to generate\na representative embedding from user and item embeddings, which is then used to\nmodel cold-start users. Finally, we adopt a model-based RL framework for TPL\nrecommendation, where popularity bias is mitigated through a carefully designed\nreward function and a rarity-based replay buffer partitioning strategy. The\nresults demonstrated that our proposed approach outperforms state-of-the-art\nmodels in cold-start scenarios while effectively mitigating the impact of\npopularity bias.",
    "published_date": "2025-04-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.13772v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.13690v2",
    "title": "Analysing the Robustness of Vision-Language-Models to Common Corruptions",
    "authors": [
      "Muhammad Usama",
      "Syeda Aishah Asim",
      "Syed Bilal Ali",
      "Syed Talal Wasim",
      "Umair Bin Mansoor"
    ],
    "author_ids": [],
    "abstract": "Vision-language models (VLMs) have demonstrated impressive capabilities in\nunderstanding and reasoning about visual and textual content. However, their\nrobustness to common image corruptions remains under-explored. In this work, we\npresent the first comprehensive analysis of VLM robustness across 19 corruption\ntypes from the ImageNet-C benchmark, spanning four categories: noise, blur,\nweather, and digital distortions. We introduce two new benchmarks, TextVQA-C\nand GQA-C, to systematically evaluate how corruptions affect scene text\nunderstanding and object-based reasoning, respectively. Our analysis reveals\nthat transformer-based VLMs exhibit distinct vulnerability patterns across\ntasks: text recognition deteriorates most severely under blur and snow\ncorruptions, while object reasoning shows higher sensitivity to corruptions\nsuch as frost and impulse noise. We connect these observations to the\nfrequency-domain characteristics of different corruptions, revealing how\ntransformers' inherent bias toward low-frequency processing explains their\ndifferential robustness patterns. Our findings provide valuable insights for\ndeveloping more corruption-robust vision-language models for real-world\napplications.",
    "published_date": "2025-04-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.13690v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.13677v1",
    "title": "Revisiting Uncertainty Quantification Evaluation in Language Models: Spurious Interactions with Response Length Bias Results",
    "authors": [
      "Andrea Santilli",
      "Adam Golinski",
      "Michael Kirchhof",
      "Federico Danieli",
      "Arno Blaas",
      "Miao Xiong",
      "Luca Zappella",
      "Sinead Williamson"
    ],
    "author_ids": [],
    "abstract": "Uncertainty Quantification (UQ) in Language Models (LMs) is crucial for\nimproving their safety and reliability. Evaluations often use performance\nmetrics like AUROC to assess how well UQ methods (e.g., negative sequence\nprobabilities) correlate with task correctness functions (e.g., ROUGE-L). In\nthis paper, we show that commonly used correctness functions bias UQ\nevaluations by inflating the performance of certain UQ methods. We evaluate 7\ncorrectness functions -- from lexical-based and embedding-based metrics to\nLLM-as-a-judge approaches -- across 4 datasets x 4 models x 6 UQ methods. Our\nanalysis reveals that length biases in the errors of these correctness\nfunctions distort UQ assessments by interacting with length biases in UQ\nmethods. We identify LLM-as-a-judge approaches as among the least length-biased\nchoices and hence a potential solution to mitigate these biases.",
    "published_date": "2025-04-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.13677v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.13674v1",
    "title": "Beyond Stereotypes: Exploring How Minority College Students Experience Stigma on Reddit",
    "authors": [
      "Chaeeun Han",
      "Sangpil Youm",
      "Hojeong Yoo",
      "Sou Hyun Jang"
    ],
    "author_ids": [],
    "abstract": "Minority college students face unique challenges shaped by their identities\nbased on their gender/sexual orientation, race, religion, and academic\ninstitutions, which influence their academic and social experiences. Although\nresearch has highlighted the challenges faced by individual minority groups,\nthe stigma process-labeling, stereotyping, separation, status loss, and\ndiscrimination-that underpin these experiences remains underexamined,\nparticularly in the online spaces where college students are highly active. We\naddress these gaps by examining posts on subreddit, r/college, as indicators\nfor stigma processes, our approach applies a Stereotype-BERT model, including\nstance toward each stereotype. We extend the stereotype model to encompass\nstatus loss and discrimination by using semantic distance with their reference\nsentences. Our analyses show that professional indicated posts are primarily\nlabeled under the stereotyping stage, whereas posts indicating racial are\nhighly represented in status loss and discrimination. Intersectional identified\nposts are more frequently associated with status loss and discrimination. The\nfindings of this study highlight the need for multifaceted intersectional\napproaches to identifying stigma, which subsequently serve as indicators to\npromote equity for minority groups, especially racial minorities and those\nexperiencing compounded vulnerabilities due to intersecting identities.",
    "published_date": "2025-04-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.13674v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.13610v1",
    "title": "Fairness and Robustness in Machine Unlearning",
    "authors": [
      "Khoa Tran",
      "Simon S. Woo"
    ],
    "author_ids": [],
    "abstract": "Machine unlearning poses the challenge of ``how to eliminate the influence of\nspecific data from a pretrained model'' in regard to privacy concerns. While\nprior research on approximated unlearning has demonstrated accuracy and\nefficiency in time complexity, we claim that it falls short of achieving exact\nunlearning, and we are the first to focus on fairness and robustness in machine\nunlearning algorithms. Our study presents fairness Conjectures for a\nwell-trained model, based on the variance-bias trade-off characteristic, and\nconsiders their relevance to robustness. Our Conjectures are supported by\nexperiments conducted on the two most widely used model architectures, ResNet\nand ViT, demonstrating the correlation between fairness and robustness:\n\\textit{the higher fairness-gap is, the more the model is sensitive and\nvulnerable}. In addition, our experiments demonstrate the vulnerability of\ncurrent state-of-the-art approximated unlearning algorithms to adversarial\nattacks, where their unlearned models suffer a significant drop in accuracy\ncompared to the exact-unlearned models. We claim that our fairness-gap\nmeasurement and robustness metric should be used to evaluate the unlearning\nalgorithm. Furthermore, we demonstrate that unlearning in the intermediate and\nlast layers is sufficient and cost-effective for time and memory complexity.",
    "published_date": "2025-04-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.13610v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.13476v1",
    "title": "Variational Autoencoder Framework for Hyperspectral Retrievals (Hyper-VAE) of Phytoplankton Absorption and Chlorophyll a in Coastal Waters for NASA's EMIT and PACE Missions",
    "authors": [
      "Jiadong Lou",
      "Bingqing Liu",
      "Yuanheng Xiong",
      "Xiaodong Zhang",
      "Xu Yuan"
    ],
    "author_ids": [],
    "abstract": "Phytoplankton absorb and scatter light in unique ways, subtly altering the\ncolor of water, changes that are often minor for human eyes to detect but can\nbe captured by sensitive ocean color instruments onboard satellites from space.\nHyperspectral sensors, paired with advanced algorithms, are expected to\nsignificantly enhance the characterization of phytoplankton community\ncomposition, especially in coastal waters where ocean color remote sensing\napplications have historically encountered significant challenges. This study\npresents novel machine learning-based solutions for NASA's hyperspectral\nmissions, including EMIT and PACE, tackling high-fidelity retrievals of\nphytoplankton absorption coefficient and chlorophyll a from their hyperspectral\nremote sensing reflectance. Given that a single Rrs spectrum may correspond to\nvaried combinations of inherent optical properties and associated\nconcentrations, the Variational Autoencoder (VAE) is used as a backbone in this\nstudy to handle such multi-distribution prediction problems. We first time\ntailor the VAE model with innovative designs to achieve hyperspectral\nretrievals of aphy and of Chl-a from hyperspectral Rrs in optically complex\nestuarine-coastal waters. Validation with extensive experimental observation\ndemonstrates superior performance of the VAE models with high precision and low\nbias. The in-depth analysis of VAE's advanced model structures and learning\ndesigns highlights the improvement and advantages of VAE-based solutions over\nthe mixture density network (MDN) approach, particularly on high-dimensional\ndata, such as PACE. Our study provides strong evidence that current EMIT and\nPACE hyperspectral data as well as the upcoming Surface Biology Geology mission\nwill open new pathways toward a better understanding of phytoplankton community\ndynamics in aquatic ecosystems when integrated with AI technologies.",
    "published_date": "2025-04-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CV",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.13476v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.13462v1",
    "title": "Stratify: Rethinking Federated Learning for Non-IID Data through Balanced Sampling",
    "authors": [
      "Hui Yeok Wong",
      "Chee Kau Lim",
      "Chee Seng Chan"
    ],
    "author_ids": [],
    "abstract": "Federated Learning (FL) on non-independently and identically distributed\n(non-IID) data remains a critical challenge, as existing approaches struggle\nwith severe data heterogeneity. Current methods primarily address symptoms of\nnon-IID by applying incremental adjustments to Federated Averaging (FedAvg),\nrather than directly resolving its inherent design limitations. Consequently,\nperformance significantly deteriorates under highly heterogeneous conditions,\nas the fundamental issue of imbalanced exposure to diverse class and feature\ndistributions remains unresolved. This paper introduces Stratify, a novel FL\nframework designed to systematically manage class and feature distributions\nthroughout training, effectively tackling the root cause of non-IID challenges.\nInspired by classical stratified sampling, our approach employs a Stratified\nLabel Schedule (SLS) to ensure balanced exposure across labels, significantly\nreducing bias and variance in aggregated gradients. Complementing SLS, we\npropose a label-aware client selection strategy, restricting participation\nexclusively to clients possessing data relevant to scheduled labels.\nAdditionally, Stratify incorporates a fine-grained, high-frequency update\nscheme, accelerating convergence and further mitigating data heterogeneity. To\nuphold privacy, we implement a secure client selection protocol leveraging\nhomomorphic encryption, enabling precise global label statistics without\ndisclosing sensitive client information. Extensive evaluations on MNIST,\nCIFAR-10, CIFAR-100, Tiny-ImageNet, COVTYPE, PACS, and Digits-DG demonstrate\nthat Stratify attains performance comparable to IID baselines, accelerates\nconvergence, and reduces client-side computation compared to state-of-the-art\nmethods, underscoring its practical effectiveness in realistic federated\nlearning scenarios.",
    "published_date": "2025-04-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.13462v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.13979v1",
    "title": "Framework, Standards, Applications and Best practices of Responsible AI : A Comprehensive Survey",
    "authors": [
      "Thippa Reddy Gadekallu",
      "Kapal Dev",
      "Sunder Ali Khowaja",
      "Weizheng Wang",
      "Hailin Feng",
      "Kai Fang",
      "Sharnil Pandya",
      "Wei Wang"
    ],
    "author_ids": [],
    "abstract": "Responsible Artificial Intelligence (RAI) is a combination of ethics\nassociated with the usage of artificial intelligence aligned with the common\nand standard frameworks. This survey paper extensively discusses the global and\nnational standards, applications of RAI, current technology and ongoing\nprojects using RAI, and possible challenges in implementing and designing RAI\nin the industries and projects based on AI. Currently, ethical standards and\nimplementation of RAI are decoupled which caters each industry to follow their\nown standards to use AI ethically. Many global firms and government\norganizations are taking necessary initiatives to design a common and standard\nframework. Social pressure and unethical way of using AI forces the RAI design\nrather than implementation.",
    "published_date": "2025-04-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.13979v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.13261v1",
    "title": "CPG-EVAL: A Multi-Tiered Benchmark for Evaluating the Chinese Pedagogical Grammar Competence of Large Language Models",
    "authors": [
      "Dong Wang"
    ],
    "author_ids": [],
    "abstract": "Purpose: The rapid emergence of large language models (LLMs) such as ChatGPT\nhas significantly impacted foreign language education, yet their pedagogical\ngrammar competence remains under-assessed. This paper introduces CPG-EVAL, the\nfirst dedicated benchmark specifically designed to evaluate LLMs' knowledge of\npedagogical grammar within the context of foreign language instruction.\nMethodology: The benchmark comprises five tasks designed to assess grammar\nrecognition, fine-grained grammatical distinction, categorical discrimination,\nand resistance to linguistic interference. Findings: Smaller-scale models can\nsucceed in single language instance tasks, but struggle with multiple instance\ntasks and interference from confusing instances. Larger-scale models show\nbetter resistance to interference but still have significant room for accuracy\nimprovement. The evaluation indicates the need for better instructional\nalignment and more rigorous benchmarks, to effectively guide the deployment of\nLLMs in educational contexts. Value: This study offers the first specialized,\ntheory-driven, multi-tiered benchmark framework for systematically evaluating\nLLMs' pedagogical grammar competence in Chinese language teaching contexts.\nCPG-EVAL not only provides empirical insights for educators, policymakers, and\nmodel developers to better gauge AI's current abilities in educational\nsettings, but also lays the groundwork for future research on improving model\nalignment, enhancing educational suitability, and ensuring informed\ndecision-making concerning LLM integration in foreign language instruction.",
    "published_date": "2025-04-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.13261v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.13173v1",
    "title": "It's All Connected: A Journey Through Test-Time Memorization, Attentional Bias, Retention, and Online Optimization",
    "authors": [
      "Ali Behrouz",
      "Meisam Razaviyayn",
      "Peilin Zhong",
      "Vahab Mirrokni"
    ],
    "author_ids": [],
    "abstract": "Designing efficient and effective architectural backbones has been in the\ncore of research efforts to enhance the capability of foundation models.\nInspired by the human cognitive phenomenon of attentional bias-the natural\ntendency to prioritize certain events or stimuli-we reconceptualize neural\narchitectures, including Transformers, Titans, and modern linear recurrent\nneural networks as associative memory modules that learn a mapping of keys and\nvalues using an internal objective, referred to as attentional bias.\nSurprisingly, we observed that most existing sequence models leverage either\n(1) dot-product similarity, or (2) L2 regression objectives as their\nattentional bias. Going beyond these objectives, we present a set of\nalternative attentional bias configurations along with their effective\napproximations to stabilize their training procedure. We then reinterpret\nforgetting mechanisms in modern deep learning architectures as a form of\nretention regularization, providing a novel set of forget gates for sequence\nmodels. Building upon these insights, we present Miras, a general framework to\ndesign deep learning architectures based on four choices of: (i) associative\nmemory architecture, (ii) attentional bias objective, (iii) retention gate, and\n(iv) memory learning algorithm. We present three novel sequence models-Moneta,\nYaad, and Memora-that go beyond the power of existing linear RNNs while\nmaintaining a fast parallelizable training process. Our experiments show\ndifferent design choices in Miras yield models with varying strengths. For\nexample, certain instances of Miras achieve exceptional performance in special\ntasks such as language modeling, commonsense reasoning, and recall intensive\ntasks, even outperforming Transformers and other modern linear recurrent\nmodels.",
    "published_date": "2025-04-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.13173v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.13102v1",
    "title": "A Multi-task Learning Balanced Attention Convolutional Neural Network Model for Few-shot Underwater Acoustic Target Recognition",
    "authors": [
      "Wei Huang",
      "Shumeng Sun",
      "Junpeng Lu",
      "Zhenpeng Xu",
      "Zhengyang Xiu",
      "Hao Zhang"
    ],
    "author_ids": [],
    "abstract": "Underwater acoustic target recognition (UATR) is of great significance for\nthe protection of marine diversity and national defense security. The\ndevelopment of deep learning provides new opportunities for UATR, but faces\nchallenges brought by the scarcity of reference samples and complex\nenvironmental interference. To address these issues, we proposes a multi-task\nbalanced channel attention convolutional neural network (MT-BCA-CNN). The\nmethod integrates a channel attention mechanism with a multi-task learning\nstrategy, constructing a shared feature extractor and multi-task classifiers to\njointly optimize target classification and feature reconstruction tasks. The\nchannel attention mechanism dynamically enhances discriminative acoustic\nfeatures such as harmonic structures while suppressing noise. Experiments on\nthe Watkins Marine Life Dataset demonstrate that MT-BCA-CNN achieves 97\\%\nclassification accuracy and 95\\% $F1$-score in 27-class few-shot scenarios,\nsignificantly outperforming traditional CNN and ACNN models, as well as popular\nstate-of-the-art UATR methods. Ablation studies confirm the synergistic\nbenefits of multi-task learning and attention mechanisms, while a dynamic\nweighting adjustment strategy effectively balances task contributions. This\nwork provides an efficient solution for few-shot underwater acoustic\nrecognition, advancing research in marine bioacoustics and sonar signal\nprocessing.",
    "published_date": "2025-04-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.13102v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.13085v1",
    "title": "Tackling Social Bias against the Poor: A Dataset and Taxonomy on Aporophobia",
    "authors": [
      "Georgina Curto",
      "Svetlana Kiritchenko",
      "Muhammad Hammad Fahim Siddiqui",
      "Isar Nejadgholi",
      "Kathleen C. Fraser"
    ],
    "author_ids": [],
    "abstract": "Eradicating poverty is the first goal in the United Nations Sustainable\nDevelopment Goals. However, aporophobia -- the societal bias against people\nliving in poverty -- constitutes a major obstacle to designing, approving and\nimplementing poverty-mitigation policies. This work presents an initial step\ntowards operationalizing the concept of aporophobia to identify and track\nharmful beliefs and discriminative actions against poor people on social media.\nIn close collaboration with non-profits and governmental organizations, we\nconduct data collection and exploration. Then we manually annotate a corpus of\nEnglish tweets from five world regions for the presence of (1) direct\nexpressions of aporophobia, and (2) statements referring to or criticizing\naporophobic views or actions of others, to comprehensively characterize the\nsocial media discourse related to bias and discrimination against the poor.\nBased on the annotated data, we devise a taxonomy of categories of aporophobic\nattitudes and actions expressed through speech on social media. Finally, we\ntrain several classifiers and identify the main challenges for automatic\ndetection of aporophobia in social networks. This work paves the way towards\nidentifying, tracking, and mitigating aporophobic views on social media at\nscale.",
    "published_date": "2025-04-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.13085v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.13038v1",
    "title": "How Large Language Models Are Changing MOOC Essay Answers: A Comparison of Pre- and Post-LLM Responses",
    "authors": [
      "Leo Leppänen",
      "Lili Aunimo",
      "Arto Hellas",
      "Jukka K. Nurminen",
      "Linda Mannila"
    ],
    "author_ids": [],
    "abstract": "The release of ChatGPT in late 2022 caused a flurry of activity and concern\nin the academic and educational communities. Some see the tool's ability to\ngenerate human-like text that passes at least cursory inspections for factual\naccuracy ``often enough'' a golden age of information retrieval and\ncomputer-assisted learning. Some, on the other hand, worry the tool may lead to\nunprecedented levels of academic dishonesty and cheating. In this work, we\nquantify some of the effects of the emergence of Large Language Models (LLMs)\non online education by analyzing a multi-year dataset of student essay\nresponses from a free university-level MOOC on AI ethics. Our dataset includes\nessays submitted both before and after ChatGPT's release. We find that the\nlaunch of ChatGPT coincided with significant changes in both the length and\nstyle of student essays, mirroring observations in other contexts such as\nacademic publishing. We also observe -- as expected based on related public\ndiscourse -- changes in prevalence of key content words related to AI and LLMs,\nbut not necessarily the general themes or topics discussed in the student\nessays as identified through (dynamic) topic modeling.",
    "published_date": "2025-04-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.CL",
      "K.3.1; I.2.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.13038v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.12989v1",
    "title": "Query Complexity of Classical and Quantum Channel Discrimination",
    "authors": [
      "Theshani Nuradha",
      "Mark M. Wilde"
    ],
    "author_ids": [],
    "abstract": "Quantum channel discrimination has been studied from an information-theoretic\nperspective, wherein one is interested in the optimal decay rate of error\nprobabilities as a function of the number of unknown channel accesses. In this\npaper, we study the query complexity of quantum channel discrimination, wherein\nthe goal is to determine the minimum number of channel uses needed to reach a\ndesired error probability. To this end, we show that the query complexity of\nbinary channel discrimination depends logarithmically on the inverse error\nprobability and inversely on the negative logarithm of the (geometric and\nHolevo) channel fidelity. As a special case of these findings, we precisely\ncharacterize the query complexity of discriminating between two classical\nchannels. We also provide lower and upper bounds on the query complexity of\nbinary asymmetric channel discrimination and multiple quantum channel\ndiscrimination. For the former, the query complexity depends on the geometric\nR\\'enyi and Petz R\\'enyi channel divergences, while for the latter, it depends\non the negative logarithm of (geometric and Uhlmann) channel fidelity. For\nmultiple channel discrimination, the upper bound scales as the logarithm of the\nnumber of channels.",
    "published_date": "2025-04-17T00:00:00",
    "year": 2025,
    "categories": [
      "quant-ph",
      "cs.IT",
      "cs.LG",
      "math.IT",
      "math.ST",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.12989v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.12897v1",
    "title": "OntoPortal-Astro, a Semantic Artefact Catalogue for Astronomy",
    "authors": [
      "Baptiste Cecconi",
      "Laura Debisschop",
      "Sébastien Derrière",
      "Mireille Louys",
      "Carmen Corre",
      "Nina Grau",
      "Clément Jonquet"
    ],
    "author_ids": [],
    "abstract": "The astronomy communities are widely recognised as mature communities for\ntheir open science practices. However, while their data ecosystems are rather\nadvanced and permit efficient data interoperability, there are still gaps\nbetween these ecosystems. Semantic artefacts (e.g., ontologies, thesauri,\nvocabularies or metadata schemas) are a means to bridge that gap as they allow\nto semantically described the data and map the underlying concepts. The\nincreasing use of semantic artefacts in astronomy presents challenges in\ndescription, selection, evaluation, trust, and mappings. The landscape remains\nfragmented, with semantic artefacts scattered across various registries in\ndiverse formats and structures -- not yet fully developed or encoded with rich\nsemantic web standards like OWL or SKOS -- and often with overlapping scopes.\nEnhancing data semantic interoperability requires common platforms to catalog,\nalign, and facilitate the sharing of FAIR semantic artefacts. In the frame of\nthe FAIR-IMPACT project, we prototyped a semantic artefact catalogue for\nastronomy, heliophysics and planetary sciences. This exercise resulted in\nimproved vocabulary and ontology management in the communities, and is now\npaving the way for better interdisciplinary data discovery and reuse. This\narticle presents current practices in our discipline, reviews candidate SAs for\nsuch a catalogue, presents driving use cases and the perspective of a real\nproduction service for the astronomy community based on the OntoPortal\ntechnology, that will be called OntoPortal-Astro.",
    "published_date": "2025-04-17T00:00:00",
    "year": 2025,
    "categories": [
      "astro-ph.IM",
      "cs.DL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.12897v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.12883v1",
    "title": "Mirror, Mirror of the Flow: How Does Regularization Shape Implicit Bias?",
    "authors": [
      "Tom Jacobs",
      "Chao Zhou",
      "Rebekka Burkholz"
    ],
    "author_ids": [],
    "abstract": "Implicit bias plays an important role in explaining how overparameterized\nmodels generalize well. Explicit regularization like weight decay is often\nemployed in addition to prevent overfitting. While both concepts have been\nstudied separately, in practice, they often act in tandem. Understanding their\ninterplay is key to controlling the shape and strength of implicit bias, as it\ncan be modified by explicit regularization. To this end, we incorporate\nexplicit regularization into the mirror flow framework and analyze its lasting\neffects on the geometry of the training dynamics, covering three distinct\neffects: positional bias, type of bias, and range shrinking. Our analytical\napproach encompasses a broad class of problems, including sparse coding, matrix\nsensing, single-layer attention, and LoRA, for which we demonstrate the utility\nof our insights. To exploit the lasting effect of regularization and highlight\nthe potential benefit of dynamic weight decay schedules, we propose to switch\noff weight decay during training, which can improve generalization, as we\ndemonstrate in experiments.",
    "published_date": "2025-04-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.12883v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.12859v1",
    "title": "Enhancing Decentralization in Blockchain Decision-Making Through Quadratic Voting and Its Generalization",
    "authors": [
      "Lyudmila Kovalchuk",
      "Mariia Rodinko",
      "Roman Oliynykov",
      "Andrii Nastenko",
      "Dmytro Kaidalov",
      "Kenric Nelson"
    ],
    "author_ids": [],
    "abstract": "This study explores the application of Quadratic Voting (QV) and its\ngeneralization to improve decentralization and effectiveness in blockchain\ngovernance systems. The conducted research identified three main types of\nquadratic (square root) voting. Two of them pertain to voting with a split\nstake, and one involves voting without splitting. In split stakes, Type 1 QV\napplies the square root to the total stake before distributing it among\npreferences, while Type 2 QV distributes the stake first and then applies the\nsquare root. In unsplit stakes (Type 3 QV), the square root of the total stake\nis allocated entirely to each preference. The presented formal proofs confirm\nthat Types 2 and 3 QV, along with generalized models, enhance decentralization\nas measured by the Gini and Nakamoto coefficients. A pivotal discovery is the\nexistence of a threshold stakeholder whose relative voting ratio increases\nunder QV compared to linear voting, while smaller stakeholders also gain\ninfluence. The generalized QV model allows flexible adjustment of this\nthreshold, enabling tailored decentralization levels. Maintaining fairness, QV\nensures that stakeholders with higher stakes retain a proportionally greater\nvoting ratio while redistributing influence to prevent excessive concentration.\nIt is shown that to preserve fairness and robustness, QV must be implemented\nalongside privacy-preserving cryptographic voting protocols, as voters casting\ntheir ballots last could otherwise manipulate outcomes. The generalized QV\nmodel, proposed in this paper, enables algorithmic parametrization to achieve\ndesired levels of decentralization for specific use cases. This flexibility\nmakes it applicable across diverse domains, including user interaction with\ncryptocurrency platforms, facilitating community events and educational\ninitiatives, and supporting charitable activities through decentralized\ndecision-making.",
    "published_date": "2025-04-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.12859v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.12767v1",
    "title": "Out of Sight Out of Mind, Out of Sight Out of Mind: Measuring Bias in Language Models Against Overlooked Marginalized Groups in Regional Contexts",
    "authors": [
      "Fatma Elsafoury",
      "David Hartmann"
    ],
    "author_ids": [],
    "abstract": "We know that language models (LMs) form biases and stereotypes of minorities,\nleading to unfair treatments of members of these groups, thanks to research\nmainly in the US and the broader English-speaking world. As the negative\nbehavior of these models has severe consequences for society and individuals,\nindustry and academia are actively developing methods to reduce the bias in\nLMs. However, there are many under-represented groups and languages that have\nbeen overlooked so far. This includes marginalized groups that are specific to\nindividual countries and regions in the English speaking and Western world, but\ncrucially also almost all marginalized groups in the rest of the world. The UN\nestimates, that between 600 million to 1.2 billion people worldwide are members\nof marginalized groups and in need for special protection. If we want to\ndevelop inclusive LMs that work for everyone, we have to broaden our\nunderstanding to include overlooked marginalized groups and low-resource\nlanguages and dialects.\n  In this work, we contribute to this effort with the first study investigating\noffensive stereotyping bias in 23 LMs for 270 marginalized groups from Egypt,\nthe remaining 21 Arab countries, Germany, the UK, and the US. Additionally, we\ninvestigate the impact of low-resource languages and dialects on the study of\nbias in LMs, demonstrating the limitations of current bias metrics, as we\nmeasure significantly higher bias when using the Egyptian Arabic dialect versus\nModern Standard Arabic. Our results show, LMs indeed show higher bias against\nmany marginalized groups in comparison to dominant groups. However, this is not\nthe case for Arabic LMs, where the bias is high against both marginalized and\ndominant groups in relation to religion and ethnicity.\n  Our results also show higher intersectional bias against Non-binary, LGBTQIA+\nand Black women.",
    "published_date": "2025-04-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.12767v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.12744v1",
    "title": "Biasing the Driving Style of an Artificial Race Driver for Online Time-Optimal Maneuver Planning",
    "authors": [
      "Sebastiano Taddei",
      "Mattia Piccinini",
      "Francesco Biral"
    ],
    "author_ids": [],
    "abstract": "In this work, we present a novel approach to bias the driving style of an\nartificial race driver (ARD) for online time-optimal trajectory planning. Our\nmethod leverages a nonlinear model predictive control (MPC) framework that\ncombines time minimization with exit speed maximization at the end of the\nplanning horizon. We introduce a new MPC terminal cost formulation based on the\ntrajectory planned in the previous MPC step, enabling ARD to adapt its driving\nstyle from early to late apex maneuvers in real-time. Our approach is\ncomputationally efficient, allowing for low replan times and long planning\nhorizons. We validate our method through simulations, comparing the results\nagainst offline minimum-lap-time (MLT) optimal control and online minimum-time\nMPC solutions. The results demonstrate that our new terminal cost enables ARD\nto bias its driving style, and achieve online lap times close to the MLT\nsolution and faster than the minimum-time MPC solution. Our approach paves the\nway for a better understanding of the reasons behind human drivers' choice of\nearly or late apex maneuvers.",
    "published_date": "2025-04-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.12744v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.12733v1",
    "title": "Adversary-Augmented Simulation for Fairness Evaluation and Defense in Hyperledger Fabric",
    "authors": [
      "Erwan Mahe",
      "Rouwaida Abdallah",
      "Pierre-Yves Piriou",
      "Sara Tucci-Piergiovanni"
    ],
    "author_ids": [],
    "abstract": "This paper presents an adversary model and a simulation framework\nspecifically tailored for analyzing attacks on distributed systems composed of\nmultiple distributed protocols, with a focus on assessing the security of\nblockchain networks. Our model classifies and constrains adversarial actions\nbased on the assumptions of the target protocols, defined by failure models,\ncommunication models, and the fault tolerance thresholds of Byzantine Fault\nTolerant (BFT) protocols. The goal is to study not only the intended effects of\nadversarial strategies but also their unintended side effects on critical\nsystem properties. We apply this framework to analyze fairness properties in a\nHyperledger Fabric (HF) blockchain network. Our focus is on novel fairness\nattacks that involve coordinated adversarial actions across various HF\nservices. Simulations show that even a constrained adversary can violate\nfairness with respect to specific clients (client fairness) and impact related\nguarantees (order fairness), which relate the reception order of transactions\nto their final order in the blockchain. This paper significantly extends our\nprevious work by introducing and evaluating a mitigation mechanism specifically\ndesigned to counter transaction reordering attacks. We implement and integrate\nthis defense into our simulation environment, demonstrating its effectiveness\nunder diverse conditions.",
    "published_date": "2025-04-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CR",
      "cs.DC",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.12733v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.12732v1",
    "title": "Validating LLM-Generated Relevance Labels for Educational Resource Search",
    "authors": [
      "Ratan J. Sebastian",
      "Anett Hoppe"
    ],
    "author_ids": [],
    "abstract": "Manual relevance judgements in Information Retrieval are costly and require\nexpertise, driving interest in using Large Language Models (LLMs) for automatic\nassessment. While LLMs have shown promise in general web search scenarios,\ntheir effectiveness for evaluating domain-specific search results, such as\neducational resources, remains unexplored. To investigate different ways of\nincluding domain-specific criteria in LLM prompts for relevance judgement, we\ncollected and released a dataset of 401 human relevance judgements from a user\nstudy involving teaching professionals performing search tasks related to\nlesson planning. We compared three approaches to structuring these prompts: a\nsimple two-aspect evaluation baseline from prior work on using LLMs as\nrelevance judges, a comprehensive 12-dimensional rubric derived from\neducational literature, and criteria directly informed by the study\nparticipants. Using domain-specific frameworks, LLMs achieved strong agreement\nwith human judgements (Cohen's $\\kappa$ up to 0.650), significantly\noutperforming the baseline approach. The participant-derived framework proved\nparticularly robust, with GPT-3.5 achieving $\\kappa$ scores of 0.639 and 0.613\nfor 10-dimension and 5-dimension versions respectively. System-level evaluation\nshowed that LLM judgements reliably identified top-performing retrieval\napproaches (RBO scores 0.71-0.76) while maintaining reasonable discrimination\nbetween systems (RBO 0.52-0.56). These findings suggest that LLMs can\neffectively evaluate educational resources when prompted with domain-specific\ncriteria, though performance varies with framework complexity and input\nstructure.",
    "published_date": "2025-04-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.12732v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.12712v1",
    "title": "Convergence and Implicit Bias of Gradient Descent on Continual Linear Classification",
    "authors": [
      "Hyunji Jung",
      "Hanseul Cho",
      "Chulhee Yun"
    ],
    "author_ids": [],
    "abstract": "We study continual learning on multiple linear classification tasks by\nsequentially running gradient descent (GD) for a fixed budget of iterations per\ntask. When all tasks are jointly linearly separable and are presented in a\ncyclic/random order, we show the directional convergence of the trained linear\nclassifier to the joint (offline) max-margin solution. This is surprising\nbecause GD training on a single task is implicitly biased towards the\nindividual max-margin solution for the task, and the direction of the joint\nmax-margin solution can be largely different from these individual solutions.\nAdditionally, when tasks are given in a cyclic order, we present a\nnon-asymptotic analysis on cycle-averaged forgetting, revealing that (1)\nalignment between tasks is indeed closely tied to catastrophic forgetting and\nbackward knowledge transfer and (2) the amount of forgetting vanishes to zero\nas the cycle repeats. Lastly, we analyze the case where the tasks are no longer\njointly separable and show that the model trained in a cyclic order converges\nto the unique minimum of the joint loss function.",
    "published_date": "2025-04-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.12712v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.12709v1",
    "title": "Self-Supervised Pre-training with Combined Datasets for 3D Perception in Autonomous Driving",
    "authors": [
      "Shumin Wang",
      "Zhuoran Yang",
      "Lidian Wang",
      "Zhipeng Tang",
      "Heng Li",
      "Lehan Pan",
      "Sha Zhang",
      "Jie Peng",
      "Jianmin Ji",
      "Yanyong Zhang"
    ],
    "author_ids": [],
    "abstract": "The significant achievements of pre-trained models leveraging large volumes\nof data in the field of NLP and 2D vision inspire us to explore the potential\nof extensive data pre-training for 3D perception in autonomous driving. Toward\nthis goal, this paper proposes to utilize massive unlabeled data from\nheterogeneous datasets to pre-train 3D perception models. We introduce a\nself-supervised pre-training framework that learns effective 3D representations\nfrom scratch on unlabeled data, combined with a prompt adapter based domain\nadaptation strategy to reduce dataset bias. The approach significantly improves\nmodel performance on downstream tasks such as 3D object detection, BEV\nsegmentation, 3D object tracking, and occupancy prediction, and shows steady\nperformance increase as the training data volume scales up, demonstrating the\npotential of continually benefit 3D perception models for autonomous driving.\nWe will release the source code to inspire further investigations in the\ncommunity.",
    "published_date": "2025-04-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.12709v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.13217v1",
    "title": "Sustainability via LLM Right-sizing",
    "authors": [
      "Jennifer Haase",
      "Finn Klessascheck",
      "Jan Mendling",
      "Sebastian Pokutta"
    ],
    "author_ids": [],
    "abstract": "Large language models (LLMs) have become increasingly embedded in\norganizational workflows. This has raised concerns over their energy\nconsumption, financial costs, and data sovereignty. While performance\nbenchmarks often celebrate cutting-edge models, real-world deployment decisions\nrequire a broader perspective: when is a smaller, locally deployable model\n\"good enough\"? This study offers an empirical answer by evaluating eleven\nproprietary and open-weight LLMs across ten everyday occupational tasks,\nincluding summarizing texts, generating schedules, and drafting emails and\nproposals. Using a dual-LLM-based evaluation framework, we automated task\nexecution and standardized evaluation across ten criteria related to output\nquality, factual accuracy, and ethical responsibility. Results show that GPT-4o\ndelivers consistently superior performance but at a significantly higher cost\nand environmental footprint. Notably, smaller models like Gemma-3 and Phi-4\nachieved strong and reliable results on most tasks, suggesting their viability\nin contexts requiring cost-efficiency, local deployment, or privacy. A cluster\nanalysis revealed three model groups -- premium all-rounders, competent\ngeneralists, and limited but safe performers -- highlighting trade-offs between\nquality, control, and sustainability. Significantly, task type influenced model\neffectiveness: conceptual tasks challenged most models, while aggregation and\ntransformation tasks yielded better performances. We argue for a shift from\nperformance-maximizing benchmarks to task- and context-aware sufficiency\nassessments that better reflect organizational priorities. Our approach\ncontributes a scalable method to evaluate AI models through a sustainability\nlens and offers actionable guidance for responsible LLM deployment in practice.",
    "published_date": "2025-04-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.13217v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.12587v1",
    "title": "Software Engineering Principles for Fairer Systems: Experiments with GroupCART",
    "authors": [
      "Kewen Peng",
      "Hao Zhuo",
      "Yicheng Yang",
      "Tim Menzies"
    ],
    "author_ids": [],
    "abstract": "Discrimination-aware classification aims to make accurate predictions while\nsatisfying fairness constraints. Traditional decision tree learners typically\noptimize for information gain in the target attribute alone, which can result\nin models that unfairly discriminate against protected social groups (e.g.,\ngender, ethnicity). Motivated by these shortcomings, we propose GroupCART, a\ntree-based ensemble optimizer that avoids bias during model construction by\noptimizing not only for decreased entropy in the target attribute but also for\nincreased entropy in protected attributes. Our experiments show that GroupCART\nachieves fairer models without data transformation and with minimal performance\ndegradation. Furthermore, the method supports customizable weighting, offering\na smooth and flexible trade-off between predictive performance and fairness\nbased on user requirements. These results demonstrate that algorithmic bias in\ndecision tree models can be mitigated through multi-task, fairness-aware\nlearning. All code and datasets used in this study are available at:\nhttps://github.com/anonymous12138/groupCART.",
    "published_date": "2025-04-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.12587v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.12553v1",
    "title": "ELAB: Extensive LLM Alignment Benchmark in Persian Language",
    "authors": [
      "Zahra Pourbahman",
      "Fatemeh Rajabi",
      "Mohammadhossein Sadeghi",
      "Omid Ghahroodi",
      "Somaye Bakhshaei",
      "Arash Amini",
      "Reza Kazemi",
      "Mahdieh Soleymani Baghshah"
    ],
    "author_ids": [],
    "abstract": "This paper presents a comprehensive evaluation framework for aligning Persian\nLarge Language Models (LLMs) with critical ethical dimensions, including\nsafety, fairness, and social norms. It addresses the gaps in existing LLM\nevaluation frameworks by adapting them to Persian linguistic and cultural\ncontexts. This benchmark creates three types of Persian-language benchmarks:\n(i) translated data, (ii) new data generated synthetically, and (iii) new\nnaturally collected data. We translate Anthropic Red Teaming data, AdvBench,\nHarmBench, and DecodingTrust into Persian. Furthermore, we create\nProhibiBench-fa, SafeBench-fa, FairBench-fa, and SocialBench-fa as new datasets\nto address harmful and prohibited content in indigenous culture. Moreover, we\ncollect extensive dataset as GuardBench-fa to consider Persian cultural norms.\nBy combining these datasets, our work establishes a unified framework for\nevaluating Persian LLMs, offering a new approach to culturally grounded\nalignment evaluation. A systematic evaluation of Persian LLMs is performed\nacross the three alignment aspects: safety (avoiding harmful content), fairness\n(mitigating biases), and social norms (adhering to culturally accepted\nbehaviors). We present a publicly available leaderboard that benchmarks Persian\nLLMs with respect to safety, fairness, and social norms at:\nhttps://huggingface.co/spaces/MCILAB/LLM_Alignment_Evaluation.",
    "published_date": "2025-04-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.12553v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.12532v1",
    "title": "Generalization through variance: how noise shapes inductive biases in diffusion models",
    "authors": [
      "John J. Vastola"
    ],
    "author_ids": [],
    "abstract": "How diffusion models generalize beyond their training set is not known, and\nis somewhat mysterious given two facts: the optimum of the denoising score\nmatching (DSM) objective usually used to train diffusion models is the score\nfunction of the training distribution; and the networks usually used to learn\nthe score function are expressive enough to learn this score to high accuracy.\nWe claim that a certain feature of the DSM objective -- the fact that its\ntarget is not the training distribution's score, but a noisy quantity only\nequal to it in expectation -- strongly impacts whether and to what extent\ndiffusion models generalize. In this paper, we develop a mathematical theory\nthat partly explains this 'generalization through variance' phenomenon. Our\ntheoretical analysis exploits a physics-inspired path integral approach to\ncompute the distributions typically learned by a few paradigmatic under- and\noverparameterized diffusion models. We find that the distributions diffusion\nmodels effectively learn to sample from resemble their training distributions,\nbut with 'gaps' filled in, and that this inductive bias is due to the\ncovariance structure of the noisy target used during training. We also\ncharacterize how this inductive bias interacts with feature-related inductive\nbiases.",
    "published_date": "2025-04-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cond-mat.dis-nn",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.12532v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.12529v1",
    "title": "Is Trust Correlated With Explainability in AI? A Meta-Analysis",
    "authors": [
      "Zahra Atf",
      "Peter R. Lewis"
    ],
    "author_ids": [],
    "abstract": "This study critically examines the commonly held assumption that\nexplicability in artificial intelligence (AI) systems inherently boosts user\ntrust. Utilizing a meta-analytical approach, we conducted a comprehensive\nexamination of the existing literature to explore the relationship between AI\nexplainability and trust. Our analysis, incorporating data from 90 studies,\nreveals a statistically significant but moderate positive correlation between\nthe explainability of AI systems and the trust they engender among users. This\nindicates that while explainability contributes to building trust, it is not\nthe sole or predominant factor in this equation. In addition to academic\ncontributions to the field of Explainable AI (XAI), this research highlights\nits broader socio-technical implications, particularly in promoting\naccountability and fostering user trust in critical domains such as healthcare\nand justice. By addressing challenges like algorithmic bias and ethical\ntransparency, the study underscores the need for equitable and sustainable AI\nadoption. Rather than focusing solely on immediate trust, we emphasize the\nnormative importance of fostering authentic and enduring trustworthiness in AI\nsystems.",
    "published_date": "2025-04-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.12529v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.13959v1",
    "title": "AI Safety Should Prioritize the Future of Work",
    "authors": [
      "Sanchaita Hazra",
      "Bodhisattwa Prasad Majumder",
      "Tuhin Chakrabarty"
    ],
    "author_ids": [],
    "abstract": "Current efforts in AI safety prioritize filtering harmful content, preventing\nmanipulation of human behavior, and eliminating existential risks in\ncybersecurity or biosecurity. While pressing, this narrow focus overlooks\ncritical human-centric considerations that shape the long-term trajectory of a\nsociety. In this position paper, we identify the risks of overlooking the\nimpact of AI on the future of work and recommend comprehensive transition\nsupport towards the evolution of meaningful labor with human agency. Through\nthe lens of economic theories, we highlight the intertemporal impacts of AI on\nhuman livelihood and the structural changes in labor markets that exacerbate\nincome inequality. Additionally, the closed-source approach of major\nstakeholders in AI development resembles rent-seeking behavior through\nexploiting resources, breeding mediocrity in creative labor, and monopolizing\ninnovation. To address this, we argue in favor of a robust international\ncopyright anatomy supported by implementing collective licensing that ensures\nfair compensation mechanisms for using data to train AI models. We strongly\nrecommend a pro-worker framework of global AI governance to enhance shared\nprosperity and economic justice while reducing technical debt.",
    "published_date": "2025-04-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "econ.GN",
      "q-fin.EC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.13959v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.12482v1",
    "title": "Agentic AI Optimisation (AAIO): what it is, how it works, why it matters, and how to deal with it",
    "authors": [
      "Luciano Floridi",
      "Carlotta Buttaboni",
      "Emmie Hine",
      "Jessica Morley",
      "Claudio Novelli",
      "Tyler Schroder"
    ],
    "author_ids": [],
    "abstract": "The emergence of Agentic Artificial Intelligence (AAI) systems capable of\nindependently initiating digital interactions necessitates a new optimisation\nparadigm designed explicitly for seamless agent-platform interactions. This\narticle introduces Agentic AI Optimisation (AAIO) as an essential methodology\nfor ensuring effective integration between websites and agentic AI systems.\nLike how Search Engine Optimisation (SEO) has shaped digital content\ndiscoverability, AAIO can define interactions between autonomous AI agents and\nonline platforms. By examining the mutual interdependency between website\noptimisation and agentic AI success, the article highlights the virtuous cycle\nthat AAIO can create. It further explores the governance, ethical, legal, and\nsocial implications (GELSI) of AAIO, emphasising the necessity of proactive\nregulatory frameworks to mitigate potential negative impacts. The article\nconcludes by affirming AAIO's essential role as part of a fundamental digital\ninfrastructure in the era of autonomous digital agents, advocating for\nequitable and inclusive access to its benefits.",
    "published_date": "2025-04-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.12482v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.12476v1",
    "title": "What do people expect from Artificial Intelligence? Public opinion on alignment in AI moderation from Germany and the United States",
    "authors": [
      "Andreas Jungherr",
      "Adrian Rauchfleisch"
    ],
    "author_ids": [],
    "abstract": "Recent advances in generative Artificial Intelligence have raised public\nawareness, shaping expectations and concerns about their societal implications.\nCentral to these debates is the question of AI alignment -- how well AI systems\nmeet public expectations regarding safety, fairness, and social values.\nHowever, little is known about what people expect from AI-enabled systems and\nhow these expectations differ across national contexts. We present evidence\nfrom two surveys of public preferences for key functional features of\nAI-enabled systems in Germany (n = 1800) and the United States (n = 1756). We\nexamine support for four types of alignment in AI moderation: accuracy and\nreliability, safety, bias mitigation, and the promotion of aspirational\nimaginaries. U.S. respondents report significantly higher AI use and\nconsistently greater support for all alignment features, reflecting broader\ntechnological openness and higher societal involvement with AI. In both\ncountries, accuracy and safety enjoy the strongest support, while more\nnormatively charged goals -- like fairness and aspirational imaginaries --\nreceive more cautious backing, particularly in Germany. We also explore how\nindividual experience with AI, attitudes toward free speech, political\nideology, partisan affiliation, and gender shape these preferences. AI use and\nfree speech support explain more variation in Germany. In contrast, U.S.\nresponses show greater attitudinal uniformity, suggesting that higher exposure\nto AI may consolidate public expectations. These findings contribute to debates\non AI governance and cross-national variation in public preferences. More\nbroadly, our study demonstrates the value of empirically grounding AI alignment\ndebates in public attitudes and of explicitly developing normatively grounded\nexpectations into theoretical and policy discussions on the governance of\nAI-generated content.",
    "published_date": "2025-04-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.12476v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.12458v1",
    "title": "M$^2$FGB: A Min-Max Gradient Boosting Framework for Subgroup Fairness",
    "authors": [
      "Jansen S. B. Pereira",
      "Giovani Valdrighi",
      "Marcos Medeiros Raimundo"
    ],
    "author_ids": [],
    "abstract": "In recent years, fairness in machine learning has emerged as a critical\nconcern to ensure that developed and deployed predictive models do not have\ndisadvantageous predictions for marginalized groups. It is essential to\nmitigate discrimination against individuals based on protected attributes such\nas gender and race. In this work, we consider applying subgroup justice\nconcepts to gradient-boosting machines designed for supervised learning\nproblems. Our approach expanded gradient-boosting methodologies to explore a\nbroader range of objective functions, which combines conventional losses such\nas the ones from classification and regression and a min-max fairness term. We\nstudy relevant theoretical properties of the solution of the min-max\noptimization problem. The optimization process explored the primal-dual\nproblems at each boosting round. This generic framework can be adapted to\ndiverse fairness concepts. The proposed min-max primal-dual gradient boosting\nalgorithm was theoretically shown to converge under mild conditions and\nempirically shown to be a powerful and flexible approach to address binary and\nsubgroup fairness.",
    "published_date": "2025-04-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.12458v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.12417v1",
    "title": "Interpretable AI-driven Guidelines for Type 2 Diabetes Treatment from Observational Data",
    "authors": [
      "Dewang Kumar Agarwal",
      "Dimitris J. Bertsimas"
    ],
    "author_ids": [],
    "abstract": "Objective: Create precise, structured, data-backed guidelines for type 2\ndiabetes treatment progression, suitable for clinical adoption.\n  Research Design and Methods: Our training cohort was composed of patient\n(with type 2 diabetes) visits from Boston Medical Center (BMC) from 1998 to\n2014. We divide visits into 4 groups based on the patient's treatment regimen\nbefore the visit, and further divide them into subgroups based on the\nrecommended treatment during the visit. Since each subgroup has observational\ndata, which has confounding bias (sicker patients are prescribed more\naggressive treatments), we used machine learning and optimization to remove\nsome datapoints so that the remaining data resembles a randomized trial. On\neach subgroup, we train AI-backed tree-based models to prescribe treatment\nchanges. Once we train these tree models, we manually combine the models for\nevery group to create an end-to-end prescription pipeline for all patients in\nthat group. In this process, we prioritize stepping up to a more aggressive\ntreatment before considering less aggressive options. We tested this pipeline\non unseen data from BMC, and an external dataset from Hartford healthcare (type\n2 diabetes patient visits from January 2020 to May 2024).\n  Results: The median HbA1c reduction achieved by our pipelines is 0.26% more\nthan what the doctors achieved on the unseen BMC patients. For the Hartford\ncohort, our pipelines were better by 0.13%.\n  Conclusions: This precise, interpretable, and efficient AI-backed approach to\ntreatment progression in type 2 diabetes is predicted to outperform the current\npractice and can be deployed to improve patient outcomes.",
    "published_date": "2025-04-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.12417v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.12236v1",
    "title": "Towards Human-Centered Early Prediction Models for Academic Performance in Real-World Contexts",
    "authors": [
      "Han Zhang",
      "Yiyi Ren",
      "Paula S. Nurius",
      "Jennifer Mankoff",
      "Anind K. Dey"
    ],
    "author_ids": [],
    "abstract": "Supporting student success requires collaboration among multiple\nstakeholders. Researchers have explored machine learning models for academic\nperformance prediction; yet key challenges remain in ensuring these models are\ninterpretable, equitable, and actionable within real-world educational support\nsystems. First, many models prioritize predictive accuracy but overlook\nhuman-centered considerations, limiting trust among students and reducing their\nusefulness for educators and institutional decision-makers. Second, most models\nrequire at least a month of data before making reliable predictions, delaying\nopportunities for early intervention. Third, current models primarily rely on\nsporadically collected, classroom-derived data, missing broader behavioral\npatterns that could provide more continuous and actionable insights. To address\nthese gaps, we present three modeling approaches-LR, 1D-CNN, and MTL-1D-CNN-to\nclassify students as low or high academic performers. We evaluate them based on\nexplainability, fairness, and generalizability to assess their alignment with\nkey social values. Using behavioral and self-reported data collected within the\nfirst week of two Spring terms, we demonstrate that these models can identify\nat-risk students as early as week one. However, trade-offs across\nhuman-centered considerations highlight the complexity of designing predictive\nmodels that effectively support multi-stakeholder decision-making and\nintervention strategies. We discuss these trade-offs and their implications for\ndifferent stakeholders, outlining how predictive models can be integrated into\nstudent support systems. Finally, we examine broader socio-technical challenges\nin deploying these models and propose future directions for advancing\nhuman-centered, collaborative academic prediction systems.",
    "published_date": "2025-04-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "68U35",
      "H.5.0; I.2.m"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.12236v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.11913v1",
    "title": "Broadening Participation through Physical Computing: Replicating Sensor-Based Programming Workshops for Rural Students in Sri Lanka",
    "authors": [
      "Poornima Meegammana",
      "Hussel Suriyaarachchi",
      "Paul Denny",
      "Suranga Nanayakkara"
    ],
    "author_ids": [],
    "abstract": "In today's digital world, computing education offers critical opportunities,\nyet systemic inequities exclude under-represented communities, especially in\nrural, under-resourced regions. Early engagement is vital for building interest\nin computing careers and achieving equitable participation. Recent work has\nshown that the use of sensor-enabled tools and block-based programming can\nimprove engagement and self-efficacy for students from under-represented\ngroups, but these findings lack replication in diverse, resource-constrained\nsettings. This study addresses this gap by implementing sensor-based\nprogramming workshops with rural students in Sri Lanka. Replicating methods\nfrom the literature, we conduct a between-group study (sensor vs. non-sensor)\nusing Scratch and real-time environmental sensors. We found that students in\nboth groups reported significantly higher confidence in programming in Scratch\nafter the workshop. In addition, average changes in both self-efficacy and\noutcome expectancy were higher in the experimental (sensor) group than in the\ncontrol (non-sensor) group, mirroring trends observed in the original study\nbeing replicated. We also found that using the sensors helped to enhance\ncreativity and inspired some students to express an interest in information and\ncommunications technology (ICT) careers, supporting the value of such hands-on\nactivities in building programming confidence among under-represented groups.",
    "published_date": "2025-04-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.11913v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.11877v1",
    "title": "Benchmarking Mutual Information-based Loss Functions in Federated Learning",
    "authors": [
      "Sarang S",
      "Harsh D. Chothani",
      "Qilei Li",
      "Ahmed M. Abdelmoniem",
      "Arnab K. Paul"
    ],
    "author_ids": [],
    "abstract": "Federated Learning (FL) has attracted considerable interest due to growing\nprivacy concerns and regulations like the General Data Protection Regulation\n(GDPR), which stresses the importance of privacy-preserving and fair machine\nlearning approaches. In FL, model training takes place on decentralized data,\nso as to allow clients to upload a locally trained model and receive a globally\naggregated model without exposing sensitive information. However, challenges\nrelated to fairness-such as biases, uneven performance among clients, and the\n\"free rider\" issue complicates its adoption. In this paper, we examine the use\nof Mutual Information (MI)-based loss functions to address these concerns. MI\nhas proven to be a powerful method for measuring dependencies between variables\nand optimizing deep learning models. By leveraging MI to extract essential\nfeatures and minimize biases, we aim to improve both the fairness and\neffectiveness of FL systems. Through extensive benchmarking, we assess the\nimpact of MI-based losses in reducing disparities among clients while enhancing\nthe overall performance of FL.",
    "published_date": "2025-04-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.11877v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.11775v1",
    "title": "Discrimination-free Insurance Pricing with Privatized Sensitive Attributes",
    "authors": [
      "Tianhe Zhang",
      "Suhan Liu",
      "Peng Shi"
    ],
    "author_ids": [],
    "abstract": "Fairness has emerged as a critical consideration in the landscape of machine\nlearning algorithms, particularly as AI continues to transform decision-making\nacross societal domains. To ensure that these algorithms are free from bias and\ndo not discriminate against individuals based on sensitive attributes such as\ngender and race, the field of algorithmic bias has introduced various fairness\nconcepts, along with methodologies to achieve these notions in different\ncontexts. Despite the rapid advancement, not all sectors have embraced these\nfairness principles to the same extent. One specific sector that merits\nattention in this regard is insurance. Within the realm of insurance pricing,\nfairness is defined through a distinct and specialized framework. Consequently,\nachieving fairness according to established notions does not automatically\nensure fair pricing in insurance. In particular, regulators are increasingly\nemphasizing transparency in pricing algorithms and imposing constraints on\ninsurance companies on the collection and utilization of sensitive consumer\nattributes. These factors present additional challenges in the implementation\nof fairness in pricing algorithms. To address these complexities and comply\nwith regulatory demands, we propose an efficient method for constructing fair\nmodels that are tailored to the insurance domain, using only privatized\nsensitive attributes. Notably, our approach ensures statistical guarantees,\ndoes not require direct access to sensitive attributes, and adapts to varying\ntransparency requirements, addressing regulatory demands while ensuring\nfairness in insurance pricing.",
    "published_date": "2025-04-16T00:00:00",
    "year": 2025,
    "categories": [
      "stat.ML",
      "cs.CY",
      "cs.LG",
      "q-fin.RM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.11775v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.11671v1",
    "title": "Steering Prosocial AI Agents: Computational Basis of LLM's Decision Making in Social Simulation",
    "authors": [
      "Ji Ma"
    ],
    "author_ids": [],
    "abstract": "Large language models (LLMs) increasingly serve as human-like decision-making\nagents in social science and applied settings. These LLM-agents are typically\nassigned human-like characters and placed in real-life contexts. However, how\nthese characters and contexts shape an LLM's behavior remains underexplored.\nThis study proposes and tests methods for probing, quantifying, and modifying\nan LLM's internal representations in a Dictator Game -- a classic behavioral\nexperiment on fairness and prosocial behavior. We extract ``vectors of variable\nvariations'' (e.g., ``male'' to ``female'') from the LLM's internal state.\nManipulating these vectors during the model's inference can substantially alter\nhow those variables relate to the model's decision-making. This approach offers\na principled way to study and regulate how social concepts can be encoded and\nengineered within transformer-based models, with implications for alignment,\ndebiasing, and designing AI agents for social simulations in both academic and\ncommercial applications.",
    "published_date": "2025-04-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "econ.GN",
      "q-fin.EC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.11671v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.11570v2",
    "title": "Traffic Adaptive Moving-window Service Patrolling for Real-time Incident Management during High-impact Events",
    "authors": [
      "Haozhe Lei",
      "Ya-Ting Yang",
      "Tao Li",
      "Zilin Bian",
      "Fan Zuo",
      "Sundeep Rangan",
      "Kaan Ozbay"
    ],
    "author_ids": [],
    "abstract": "This paper presents the Traffic Adaptive Moving-window Patrolling Algorithm\n(TAMPA), designed to improve real-time incident management during major events\nlike sports tournaments and concerts. Such events significantly stress\ntransportation networks, requiring efficient and adaptive patrol solutions.\nTAMPA integrates predictive traffic modeling and real-time complaint\nestimation, dynamically optimizing patrol deployment. Using dynamic\nprogramming, the algorithm continuously adjusts patrol strategies within short\nplanning windows, effectively balancing immediate response and efficient\nrouting. Leveraging the Dvoretzky-Kiefer-Wolfowitz inequality, TAMPA detects\nsignificant shifts in complaint patterns, triggering proactive adjustments in\npatrol routes. Theoretical analyses ensure performance remains closely aligned\nwith optimal solutions. Simulation results from an urban traffic network\ndemonstrate TAMPA's superior performance, showing improvements of approximately\n87.5\\% over stationary methods and 114.2\\% over random strategies. Future work\nincludes enhancing adaptability and incorporating digital twin technology for\nimproved predictive accuracy, particularly relevant for events like the 2026\nFIFA World Cup at MetLife Stadium.",
    "published_date": "2025-04-15T00:00:00",
    "year": 2025,
    "categories": [
      "math.OC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.11570v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.11431v1",
    "title": "Masculine Defaults via Gendered Discourse in Podcasts and Large Language Models",
    "authors": [
      "Maria Teleki",
      "Xiangjue Dong",
      "Haoran Liu",
      "James Caverlee"
    ],
    "author_ids": [],
    "abstract": "Masculine defaults are widely recognized as a significant type of gender\nbias, but they are often unseen as they are under-researched. Masculine\ndefaults involve three key parts: (i) the cultural context, (ii) the masculine\ncharacteristics or behaviors, and (iii) the reward for, or simply acceptance\nof, those masculine characteristics or behaviors. In this work, we study\ndiscourse-based masculine defaults, and propose a twofold framework for (i) the\nlarge-scale discovery and analysis of gendered discourse words in spoken\ncontent via our Gendered Discourse Correlation Framework (GDCF); and (ii) the\nmeasurement of the gender bias associated with these gendered discourse words\nin LLMs via our Discourse Word-Embedding Association Test (D-WEAT). We focus\nour study on podcasts, a popular and growing form of social media, analyzing\n15,117 podcast episodes. We analyze correlations between gender and discourse\nwords -- discovered via LDA and BERTopic -- to automatically form gendered\ndiscourse word lists. We then study the prevalence of these gendered discourse\nwords in domain-specific contexts, and find that gendered discourse-based\nmasculine defaults exist in the domains of business, technology/politics, and\nvideo games. Next, we study the representation of these gendered discourse\nwords from a state-of-the-art LLM embedding model from OpenAI, and find that\nthe masculine discourse words have a more stable and robust representation than\nthe feminine discourse words, which may result in better system performance on\ndownstream tasks for men. Hence, men are rewarded for their discourse patterns\nwith better system performance by one of the state-of-the-art language models\n-- and this embedding disparity is a representational harm and a masculine\ndefault.",
    "published_date": "2025-04-15T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.11431v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.12345v1",
    "title": "Reimagining Urban Science: Scaling Causal Inference with Large Language Models",
    "authors": [
      "Yutong Xia",
      "Ao Qu",
      "Yunhan Zheng",
      "Yihong Tang",
      "Dingyi Zhuang",
      "Yuxuan Liang",
      "Cathy Wu",
      "Roger Zimmermann",
      "Jinhua Zhao"
    ],
    "author_ids": [],
    "abstract": "Urban causal research is essential for understanding the complex dynamics of\ncities and informing evidence-based policies. However, it is challenged by the\ninefficiency and bias of hypothesis generation, barriers to multimodal data\ncomplexity, and the methodological fragility of causal experimentation. Recent\nadvances in large language models (LLMs) present an opportunity to rethink how\nurban causal analysis is conducted. This Perspective examines current urban\ncausal research by analyzing taxonomies that categorize research topics, data\nsources, and methodological approaches to identify structural gaps. We then\nintroduce an LLM-driven conceptual framework, AutoUrbanCI, composed of four\ndistinct modular agents responsible for hypothesis generation, data\nengineering, experiment design and execution, and results interpretation with\npolicy recommendations. We propose evaluation criteria for rigor and\ntransparency and reflect on implications for human-AI collaboration, equity,\nand accountability. We call for a new research agenda that embraces\nAI-augmented workflows not as replacements for human expertise but as tools to\nbroaden participation, improve reproducibility, and unlock more inclusive forms\nof urban causal reasoning.",
    "published_date": "2025-04-15T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.12345v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.11295v1",
    "title": "Autoregressive Distillation of Diffusion Transformers",
    "authors": [
      "Yeongmin Kim",
      "Sotiris Anagnostidis",
      "Yuming Du",
      "Edgar Schönfeld",
      "Jonas Kohler",
      "Markos Georgopoulos",
      "Albert Pumarola",
      "Ali Thabet",
      "Artsiom Sanakoyeu"
    ],
    "author_ids": [],
    "abstract": "Diffusion models with transformer architectures have demonstrated promising\ncapabilities in generating high-fidelity images and scalability for high\nresolution. However, iterative sampling process required for synthesis is very\nresource-intensive. A line of work has focused on distilling solutions to\nprobability flow ODEs into few-step student models. Nevertheless, existing\nmethods have been limited by their reliance on the most recent denoised samples\nas input, rendering them susceptible to exposure bias. To address this\nlimitation, we propose AutoRegressive Distillation (ARD), a novel approach that\nleverages the historical trajectory of the ODE to predict future steps. ARD\noffers two key benefits: 1) it mitigates exposure bias by utilizing a predicted\nhistorical trajectory that is less susceptible to accumulated errors, and 2) it\nleverages the previous history of the ODE trajectory as a more effective source\nof coarse-grained information. ARD modifies the teacher transformer\narchitecture by adding token-wise time embedding to mark each input from the\ntrajectory history and employs a block-wise causal attention mask for training.\nFurthermore, incorporating historical inputs only in lower transformer layers\nenhances performance and efficiency. We validate the effectiveness of ARD in a\nclass-conditioned generation on ImageNet and T2I synthesis. Our model achieves\na $5\\times$ reduction in FID degradation compared to the baseline methods while\nrequiring only 1.1\\% extra FLOPs on ImageNet-256. Moreover, ARD reaches FID of\n1.84 on ImageNet-256 in merely 4 steps and outperforms the publicly available\n1024p text-to-image distilled models in prompt adherence score with a minimal\ndrop in FID compared to the teacher. Project page:\nhttps://github.com/alsdudrla10/ARD.",
    "published_date": "2025-04-15T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.11295v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.11183v1",
    "title": "Bias Beyond English: Evaluating Social Bias and Debiasing Methods in a Low-Resource Setting",
    "authors": [
      "Ej Zhou",
      "Weiming Lu"
    ],
    "author_ids": [],
    "abstract": "Social bias in language models can potentially exacerbate social\ninequalities. Despite it having garnered wide attention, most research focuses\non English data. In a low-resource scenario, the models often perform worse due\nto insufficient training data. This study aims to leverage high-resource\nlanguage corpora to evaluate bias and experiment with debiasing methods in\nlow-resource languages. We evaluated the performance of recent multilingual\nmodels in five languages: English (\\textsc{eng}), Chinese (\\textsc{zho}),\nRussian (\\textsc{rus}), Indonesian (\\textsc{ind}) and Thai (\\textsc{tha}), and\nanalyzed four bias dimensions: \\textit{gender}, \\textit{religion},\n\\textit{nationality}, and \\textit{race-color}. By constructing multilingual\nbias evaluation datasets, this study allows fair comparisons between models\nacross languages. We have further investigated three debiasing\nmethods-\\texttt{CDA}, \\texttt{Dropout}, \\texttt{SenDeb}-and demonstrated that\ndebiasing methods from high-resource languages can be effectively transferred\nto low-resource ones, providing actionable insights for fairness research in\nmultilingual NLP.",
    "published_date": "2025-04-15T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.11183v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.11169v1",
    "title": "MuSeD: A Multimodal Spanish Dataset for Sexism Detection in Social Media Videos",
    "authors": [
      "Laura De Grazia",
      "Pol Pastells",
      "Mauro Vázquez Chas",
      "Desmond Elliott",
      "Danae Sánchez Villegas",
      "Mireia Farrús",
      "Mariona Taulé"
    ],
    "author_ids": [],
    "abstract": "Sexism is generally defined as prejudice and discrimination based on sex or\ngender, affecting every sector of society, from social institutions to\nrelationships and individual behavior. Social media platforms amplify the\nimpact of sexism by conveying discriminatory content not only through text but\nalso across multiple modalities, highlighting the critical need for a\nmultimodal approach to the analysis of sexism online. With the rise of social\nmedia platforms where users share short videos, sexism is increasingly\nspreading through video content. Automatically detecting sexism in videos is a\nchallenging task, as it requires analyzing the combination of verbal, audio,\nand visual elements to identify sexist content. In this study, (1) we introduce\nMuSeD, a new Multimodal Spanish dataset for Sexism Detection consisting of\n$\\approx$ 11 hours of videos extracted from TikTok and BitChute; (2) we propose\nan innovative annotation framework for analyzing the contribution of textual\nand multimodal labels in the classification of sexist and non-sexist content;\nand (3) we evaluate a range of large language models (LLMs) and multimodal LLMs\non the task of sexism detection. We find that visual information plays a key\nrole in labeling sexist content for both humans and models. Models effectively\ndetect explicit sexism; however, they struggle with implicit cases, such as\nstereotypes, instances where annotators also show low agreement. This\nhighlights the inherent difficulty of the task, as identifying implicit sexism\ndepends on the social and cultural context.",
    "published_date": "2025-04-15T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.11169v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.11104v1",
    "title": "Using LLMs as prompt modifier to avoid biases in AI image generators",
    "authors": [
      "René Peinl"
    ],
    "author_ids": [],
    "abstract": "This study examines how Large Language Models (LLMs) can reduce biases in\ntext-to-image generation systems by modifying user prompts. We define bias as a\nmodel's unfair deviation from population statistics given neutral prompts. Our\nexperiments with Stable Diffusion XL, 3.5 and Flux demonstrate that\nLLM-modified prompts significantly increase image diversity and reduce bias\nwithout the need to change the image generators themselves. While occasionally\nproducing results that diverge from original user intent for elaborate prompts,\nthis approach generally provides more varied interpretations of underspecified\nrequests rather than superficial variations. The method works particularly well\nfor less advanced image generators, though limitations persist for certain\ncontexts like disability representation. All prompts and generated images are\navailable at https://iisys-hof.github.io/llm-prompt-img-gen/",
    "published_date": "2025-04-15T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.CV",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.11104v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.11059v1",
    "title": "Quantifying Group Fairness in Community Detection",
    "authors": [
      "Elze de Vink",
      "Frank W. Takes",
      "Akrati Saxena"
    ],
    "author_ids": [],
    "abstract": "Understanding community structures is crucial for analyzing networks, as\nnodes join communities that collectively shape large-scale networks. In\nreal-world settings, the formation of communities is often impacted by several\nsocial factors, such as ethnicity, gender, wealth, or other attributes. These\nfactors may introduce structural inequalities; for instance, real-world\nnetworks can have a few majority groups and many minority groups. Community\ndetection algorithms, which identify communities based on network topology, may\ngenerate unfair outcomes if they fail to account for existing structural\ninequalities, particularly affecting underrepresented groups. In this work, we\npropose a set of novel group fairness metrics to assess the fairness of\ncommunity detection methods. Additionally, we conduct a comparative evaluation\nof the most common community detection methods, analyzing the trade-off between\nperformance and fairness. Experiments are performed on synthetic networks\ngenerated using LFR, ABCD, and HICH-BA benchmark models, as well as on\nreal-world networks. Our results demonstrate that the fairness-performance\ntrade-off varies widely across methods, with no single class of approaches\nconsistently excelling in both aspects. We observe that Infomap and\nSignificance methods are high-performing and fair with respect to different\ntypes of communities across most networks. The proposed metrics and findings\nprovide valuable insights for designing fair and effective community detection\nalgorithms.",
    "published_date": "2025-04-15T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.11059v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.10948v1",
    "title": "BEACON: A Benchmark for Efficient and Accurate Counting of Subgraphs",
    "authors": [
      "Mohammad Matin Najafi",
      "Xianju Zhu",
      "Chrysanthi Kosyfaki",
      "Laks V. S. Lakshmanan",
      "Reynold Cheng"
    ],
    "author_ids": [],
    "abstract": "Subgraph counting the task of determining the number of instances of a query\npattern within a large graph lies at the heart of many critical applications,\nfrom analyzing financial networks and transportation systems to understanding\nbiological interactions. Despite decades of work yielding efficient algorithmic\n(AL) solutions and, more recently, machine learning (ML) approaches, a clear\ncomparative understanding is elusive. This gap stems from the absence of a\nunified evaluation framework, standardized datasets, and accessible ground\ntruths, all of which hinder systematic analysis and fair benchmarking. To\novercome these barriers, we introduce BEACON: a comprehensive benchmark\ndesigned to rigorously evaluate both AL and ML-based subgraph counting methods.\nBEACON provides a standardized dataset with verified ground truths, an\nintegrated evaluation environment, and a public leaderboard, enabling\nreproducible and transparent comparisons across diverse approaches. Our\nextensive experiments reveal that while AL methods excel in efficiently\ncounting subgraphs on very large graphs, they struggle with complex patterns\n(e.g., those exceeding six nodes). In contrast, ML methods are capable of\nhandling larger patterns but demand massive graph data inputs and often yield\nsuboptimal accuracy on small, dense graphs. These insights not only highlight\nthe unique strengths and limitations of each approach but also pave the way for\nfuture advancements in subgraph counting techniques. Overall, BEACON represents\na significant step towards unifying and accelerating research in subgraph\ncounting, encouraging innovative solutions and fostering a deeper understanding\nof the trade-offs between algorithmic and machine learning paradigms.",
    "published_date": "2025-04-15T00:00:00",
    "year": 2025,
    "categories": [
      "cs.DS",
      "cs.AI",
      "cs.DB",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.10948v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.11504v2",
    "title": "Counterfactual Fairness Evaluation of Machine Learning Models on Educational Datasets",
    "authors": [
      "Woojin Kim",
      "Hyeoncheol Kim"
    ],
    "author_ids": [],
    "abstract": "As machine learning models are increasingly used in educational settings,\nfrom detecting at-risk students to predicting student performance, algorithmic\nbias and its potential impacts on students raise critical concerns about\nalgorithmic fairness. Although group fairness is widely explored in education,\nworks on individual fairness in a causal context are understudied, especially\non counterfactual fairness. This paper explores the notion of counterfactual\nfairness for educational data by conducting counterfactual fairness analysis of\nmachine learning models on benchmark educational datasets. We demonstrate that\ncounterfactual fairness provides meaningful insight into the causality of\nsensitive attributes and causal-based individual fairness in education.",
    "published_date": "2025-04-15T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.11504v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.10932v1",
    "title": "Multi-scale DeepOnet (Mscale-DeepOnet) for Mitigating Spectral Bias in Learning High Frequency Operators of Oscillatory Functions",
    "authors": [
      "Bo Wang",
      "Lizuo Liu",
      "Wei Cai"
    ],
    "author_ids": [],
    "abstract": "In this paper, a multi-scale DeepOnet (Mscale-DeepOnet) is proposed to reduce\nthe spectral bias of the DeepOnet in learning high-frequency mapping between\nhighly oscillatory functions, with an application to the nonlinear mapping\nbetween the coefficient of the Helmholtz equation and its solution. The\nMscale-DeepOnet introduces the multiscale neural network in the branch and\ntrunk networks of the original DeepOnet, the resulting Mscale-DeepOnet is shown\nto be able to capture various high-frequency components of the mapping itself\nand its image. Numerical results demonstrate the substantial improvement of the\nMscale-DeepOnet for the problem of wave scattering in the high-frequency regime\nover the normal DeepOnet with a similar number of network parameters.",
    "published_date": "2025-04-15T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.NA",
      "math.NA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.10932v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.10917v1",
    "title": "Towards A Universal Graph Structural Encoder",
    "authors": [
      "Jialin Chen",
      "Haolan Zuo",
      "Haoyu Peter Wang",
      "Siqi Miao",
      "Pan Li",
      "Rex Ying"
    ],
    "author_ids": [],
    "abstract": "Recent advancements in large-scale pre-training have shown the potential to\nlearn generalizable representations for downstream tasks. In the graph domain,\nhowever, capturing and transferring structural information across different\ngraph domains remains challenging, primarily due to the inherent differences in\ntopological patterns across various contexts. Additionally, most existing\nmodels struggle to capture the complexity of rich graph structures, leading to\ninadequate exploration of the embedding space. To address these challenges, we\npropose GFSE, a universal graph structural encoder designed to capture\ntransferable structural patterns across diverse domains such as molecular\ngraphs, social networks, and citation networks. GFSE is the first cross-domain\ngraph structural encoder pre-trained with multiple self-supervised learning\nobjectives. Built on a Graph Transformer, GFSE incorporates attention\nmechanisms informed by graph inductive bias, enabling it to encode intricate\nmulti-level and fine-grained topological features. The pre-trained GFSE\nproduces generic and theoretically expressive positional and structural\nencoding for graphs, which can be seamlessly integrated with various downstream\ngraph feature encoders, including graph neural networks for vectorized features\nand Large Language Models for text-attributed graphs. Comprehensive experiments\non synthetic and real-world datasets demonstrate GFSE's capability to\nsignificantly enhance the model's performance while requiring substantially\nless task-specific fine-tuning. Notably, GFSE achieves state-of-the-art\nperformance in 81.6% evaluated cases, spanning diverse graph models and\ndatasets, highlighting its potential as a powerful and versatile encoder for\ngraph-structured data.",
    "published_date": "2025-04-15T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.10917v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.10915v2",
    "title": "LOKA Protocol: A Decentralized Framework for Trustworthy and Ethical AI Agent Ecosystems",
    "authors": [
      "Rajesh Ranjan",
      "Shailja Gupta",
      "Surya Narayan Singh"
    ],
    "author_ids": [],
    "abstract": "The rise of autonomous AI agents, capable of perceiving, reasoning, and\nacting independently, signals a profound shift in how digital ecosystems\noperate, govern, and evolve. As these agents proliferate beyond centralized\ninfrastructures, they expose foundational gaps in identity, accountability, and\nethical alignment. Three critical questions emerge: Identity: Who or what is\nthe agent? Accountability: Can its actions be verified, audited, and trusted?\nEthical Consensus: Can autonomous systems reliably align with human values and\nprevent harmful emergent behaviors? We present the novel LOKA Protocol (Layered\nOrchestration for Knowledgeful Agents), a unified, systems-level architecture\nfor building ethically governed, interoperable AI agent ecosystems. LOKA\nintroduces a proposed Universal Agent Identity Layer (UAIL) for decentralized,\nverifiable identity; intent-centric communication protocols for semantic\ncoordination across diverse agents; and a Decentralized Ethical Consensus\nProtocol (DECP) that could enable agents to make context-aware decisions\ngrounded in shared ethical baselines. Anchored in emerging standards such as\nDecentralized Identifiers (DIDs), Verifiable Credentials (VCs), and\npost-quantum cryptography, LOKA proposes a scalable, future-resilient blueprint\nfor multi-agent AI governance. By embedding identity, trust, and ethics into\nthe protocol layer itself, LOKA proposes the foundation for a new era of\nresponsible, transparent, and autonomous AI ecosystems operating across digital\nand physical domains.",
    "published_date": "2025-04-15T00:00:00",
    "year": 2025,
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.10915v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.10886v1",
    "title": "Exploring Persona-dependent LLM Alignment for the Moral Machine Experiment",
    "authors": [
      "Jiseon Kim",
      "Jea Kwon",
      "Luiz Felipe Vecchietti",
      "Alice Oh",
      "Meeyoung Cha"
    ],
    "author_ids": [],
    "abstract": "Deploying large language models (LLMs) with agency in real-world applications\nraises critical questions about how these models will behave. In particular,\nhow will their decisions align with humans when faced with moral dilemmas? This\nstudy examines the alignment between LLM-driven decisions and human judgment in\nvarious contexts of the moral machine experiment, including personas reflecting\ndifferent sociodemographics. We find that the moral decisions of LLMs vary\nsubstantially by persona, showing greater shifts in moral decisions for\ncritical tasks than humans. Our data also indicate an interesting partisan\nsorting phenomenon, where political persona predominates the direction and\ndegree of LLM decisions. We discuss the ethical implications and risks\nassociated with deploying these models in applications that involve moral\ndecisions.",
    "published_date": "2025-04-15T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.10886v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.10797v1",
    "title": "Name of Thrones: Evaluating How LLMs Rank Student Names, Race, and Gender in Status Hierarchies",
    "authors": [
      "Annabella Sakunkoo",
      "Jonathan Sakunkoo"
    ],
    "author_ids": [],
    "abstract": "Across cultures, names tell a lot about their bearers as they carry deep\npersonal and cultural significance. Names also serve as powerful signals of\ngender, race, and status in the social hierarchy - a pecking order in which\nindividual positions shape others' expectations on their perceived competence\nand worth. With the widespread adoption of LLMs and as names are often an input\nfor LLMs, it is crucial to evaluate whether LLMs may sort people into status\npositions based on first and last names and, if so, whether it is in an unfair,\nbiased fashion. While prior work has primarily investigated biases in first\nnames, little attention has been paid to last names and even less to the\ncombined effects of first and last names. In this study, we conduct a\nlarge-scale analysis of name variations across 5 ethnicities to examine how AI\nexhibits name biases. Our study investigates three key characteristics of\ninequality and finds that LLMs reflect and reinforce status hierarchies based\non names that signal gender and ethnicity as they encode differential\nexpectations of competence, leadership, and economic potential. Contrary to the\ncommon assumption that AI tends to favor Whites, we show that East and, in some\ncontexts, South Asian names receive higher rankings. We also disaggregate\nAsians, a population projected to be the largest immigrant group in the U.S. by\n2055. Our results challenge the monolithic Asian model minority assumption,\nillustrating a more complex and stratified model of bias. Gender moderates\nbiases, with girls facing unfair disadvantages in certain racial groups.\nAdditionally, spanning cultural categories by adopting Western first names\nimproves AI-perceived status for East and Southeast Asian students,\nparticularly for girls. Our findings underscore the importance of\nintersectional and more nuanced understandings of race, gender, and mixed\nidentities in the evaluation of LLMs.",
    "published_date": "2025-04-15T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "H.5; J.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.10797v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.10777v1",
    "title": "AtlasD: Automatic Local Symmetry Discovery",
    "authors": [
      "Manu Bhat",
      "Jonghyun Park",
      "Jianke Yang",
      "Nima Dehmamy",
      "Robin Walters",
      "Rose Yu"
    ],
    "author_ids": [],
    "abstract": "Existing symmetry discovery methods predominantly focus on global\ntransformations across the entire system or space, but they fail to consider\nthe symmetries in local neighborhoods. This may result in the reported symmetry\ngroup being a misrepresentation of the true symmetry. In this paper, we\nformalize the notion of local symmetry as atlas equivariance. Our proposed\npipeline, automatic local symmetry discovery (AtlasD), recovers the local\nsymmetries of a function by training local predictor networks and then learning\na Lie group basis to which the predictors are equivariant. We demonstrate\nAtlasD is capable of discovering local symmetry groups with multiple connected\ncomponents in top-quark tagging and partial differential equation experiments.\nThe discovered local symmetry is shown to be a useful inductive bias that\nimproves the performance of downstream tasks in climate segmentation and vision\ntasks.",
    "published_date": "2025-04-15T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.10777v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.10714v1",
    "title": "Playing to Pay: Interplay of Monetization and Retention Strategies in Korean Mobile Gaming",
    "authors": [
      "HwiJoon Lee",
      "Kashif Imteyaz",
      "Saiph Savage"
    ],
    "author_ids": [],
    "abstract": "Mobile gaming's global growth has introduced evolving monetization\nstrategies, such as in app purchases and ads, designed to boost revenue while\nmaintaining player engagement. However, there is limited understanding of the\nscope and frequency of these strategies, particularly in mature markets like\nSouth Korea. To address this research gap, this study examines the monetization\nstrategies used in the top 40 most popular Korean mobile games through direct\ngameplay observations and targeted video analyses. We identified the prevalence\nof specific strategies, including time gated progression, Conflict Driven\nDesign, and social Dynamics, which are systematically categorized in our\nproposed framework for monetization. Our findings also highlight ethical\nconcerns, including issues with transparency, probability disclosures, and the\nexploitation of competitive pressures areas that remain poorly regulated. To\naddress these challenges, we emphasize the need for stricter consumer\nprotections, cross regional research, and greater focus on protecting\nvulnerable populations to promote a more equitable and responsible gaming\nenvironment.",
    "published_date": "2025-04-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.10714v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.13199v1",
    "title": "Building Trustworthy Multimodal AI: A Review of Fairness, Transparency, and Ethics in Vision-Language Tasks",
    "authors": [
      "Mohammad Saleha",
      "Azadeh Tabatabaeib"
    ],
    "author_ids": [],
    "abstract": "Objective: This review explores the trustworthiness of multimodal artificial\nintelligence (AI) systems, specifically focusing on vision-language tasks. It\naddresses critical challenges related to fairness, transparency, and ethical\nimplications in these systems, providing a comparative analysis of key tasks\nsuch as Visual Question Answering (VQA), image captioning, and visual dialogue.\nBackground: Multimodal models, particularly vision-language models, enhance\nartificial intelligence (AI) capabilities by integrating visual and textual\ndata, mimicking human learning processes. Despite significant advancements, the\ntrustworthiness of these models remains a crucial concern, particularly as AI\nsystems increasingly confront issues regarding fairness, transparency, and\nethics. Methods: This review examines research conducted from 2017 to 2024\nfocusing on forenamed core vision-language tasks. It employs a comparative\napproach to analyze these tasks through the lens of trustworthiness,\nunderlining fairness, explainability, and ethics. This study synthesizes\nfindings from recent literature to identify trends, challenges, and\nstate-of-the-art solutions. Results: Several key findings were highlighted.\nTransparency: Explainability of vision language tasks is important for user\ntrust. Techniques, such as attention maps and gradient-based methods, have\nsuccessfully addressed this issue. Fairness: Bias mitigation in VQA and visual\ndialogue systems is essential for ensuring unbiased outcomes across diverse\ndemographic groups. Ethical Implications: Addressing biases in multilingual\nmodels and ensuring ethical data handling is critical for the responsible\ndeployment of vision-language systems. Conclusion: This study underscores the\nimportance of integrating fairness, transparency, and ethical considerations in\ndeveloping vision-language models within a unified framework.",
    "published_date": "2025-04-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.13199v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.12337v1",
    "title": "\"It Listens Better Than My Therapist\": Exploring Social Media Discourse on LLMs as Mental Health Tool",
    "authors": [
      "Anna-Carolina Haensch"
    ],
    "author_ids": [],
    "abstract": "The emergence of generative AI chatbots such as ChatGPT has prompted growing\npublic and academic interest in their role as informal mental health support\ntools. While early rule-based systems have been around for several years, large\nlanguage models (LLMs) offer new capabilities in conversational fluency,\nempathy simulation, and availability. This study explores how users engage with\nLLMs as mental health tools by analyzing over 10,000 TikTok comments from\nvideos referencing LLMs as mental health tools. Using a self-developed tiered\ncoding schema and supervised classification models, we identify user\nexperiences, attitudes, and recurring themes. Results show that nearly 20% of\ncomments reflect personal use, with these users expressing overwhelmingly\npositive attitudes. Commonly cited benefits include accessibility, emotional\nsupport, and perceived therapeutic value. However, concerns around privacy,\ngeneric responses, and the lack of professional oversight remain prominent. It\nis important to note that the user feedback does not indicate which therapeutic\nframework, if any, the LLM-generated output aligns with. While the findings\nunderscore the growing relevance of AI in everyday practices, they also\nhighlight the urgent need for clinical and ethical scrutiny in the use of AI\nfor mental health support.",
    "published_date": "2025-04-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.HC",
      "cs.SI",
      "J.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.12337v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.10389v2",
    "title": "Diversity-Fair Online Selection",
    "authors": [
      "Ming Hu",
      "Yanzhi Li",
      "Tongwen Wu"
    ],
    "author_ids": [],
    "abstract": "Online selection problems frequently arise in applications such as\ncrowdsourcing and employee recruitment. Existing research typically focuses on\ncandidates with a single attribute. However, crowdsourcing tasks often require\ncontributions from individuals across various demographics. Further motivated\nby the dynamic nature of crowdsourcing and hiring, we study the diversity-fair\nonline selection problem, in which a recruiter must make real-time decisions to\nfoster workforce diversity across many dimensions. We propose two scenarios for\nthis problem. The fixed-capacity scenario, suited for short-term hiring for\ncrowdsourced workers, provides the recruiter with a fixed capacity to fill\ntemporary job vacancies. In contrast, in the unknown-capacity scenario,\nrecruiters optimize diversity across recruitment seasons with increasing\ncapacities, reflecting that the firm honors diversity consideration in a\nlong-term employee acquisition strategy. By modeling the diversity over $d$\ndimensions as a max-min fairness objective, we show that no policy can surpass\na competitive ratio of $O(1/d^{1/3})$ for either scenario, indicating that any\nachievable result inevitably decays by some polynomial factor in $d$. To this\nend, we develop bilevel hierarchical randomized policies that ensure compliance\nwith the capacity constraint. For the fixed-capacity scenario, leveraging\nmarginal information about the arriving population allows us to achieve a\ncompetitive ratio of $1/(4\\sqrt{d} \\lceil \\log_2 d \\rceil)$. For the\nunknown-capacity scenario, we establish a competitive ratio of\n$\\Omega(1/d^{3/4})$ under mild boundedness conditions. In both bilevel\nhierarchical policies, the higher level determines ex-ante selection\nprobabilities and then informs the lower level's randomized selection that\nensures no loss in efficiency. Both policies prioritize core diversity and then\nadjust for underrepresented dimensions.",
    "published_date": "2025-04-14T00:00:00",
    "year": 2025,
    "categories": [
      "econ.TH",
      "cs.DS",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.10389v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.10371v1",
    "title": "Brain-Machine Interfaces & Information Retrieval Challenges and Opportunities",
    "authors": [
      "Yashar Moshfeghi",
      "Niall McGuire"
    ],
    "author_ids": [],
    "abstract": "The fundamental goal of Information Retrieval (IR) systems lies in their\ncapacity to effectively satisfy human information needs - a challenge that\nencompasses not just the technical delivery of information, but the nuanced\nunderstanding of human cognition during information seeking. Contemporary IR\nplatforms rely primarily on observable interaction signals, creating a\nfundamental gap between system capabilities and users' cognitive processes.\nBrain-Machine Interface (BMI) technologies now offer unprecedented potential to\nbridge this gap through direct measurement of previously inaccessible aspects\nof information-seeking behaviour. This perspective paper offers a broad\nexamination of the IR landscape, providing a comprehensive analysis of how BMI\ntechnology could transform IR systems, drawing from advances at the\nintersection of both neuroscience and IR research. We present our analysis\nthrough three identified fundamental vertices: (1) understanding the neural\ncorrelates of core IR concepts to advance theoretical models of search\nbehaviour, (2) enhancing existing IR systems through contextual integration of\nneurophysiological signals, and (3) developing proactive IR capabilities\nthrough direct neurophysiological measurement. For each vertex, we identify\nspecific research opportunities and propose concrete directions for developing\nBMI-enhanced IR systems. We conclude by examining critical technical and\nethical challenges in implementing these advances, providing a structured\nroadmap for future research at the intersection of neuroscience and IR.",
    "published_date": "2025-04-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.10371v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.10353v1",
    "title": "Patch and Shuffle: A Preprocessing Technique for Texture Classification in Autonomous Cementitious Fabrication",
    "authors": [
      "Jeremiah Giordani"
    ],
    "author_ids": [],
    "abstract": "Autonomous fabrication systems are transforming construction and\nmanufacturing, yet they remain vulnerable to print errors. Texture\nclassification is a key component of computer vision systems that enable\nreal-time monitoring and adjustment during cementitious fabrication.\nTraditional classification methods often rely on global image features, which\ncan bias the model toward semantic content rather than low-level textures. In\nthis paper, we introduce a novel preprocessing technique called \"patch and\nshuffle,\" which segments input images into smaller patches, shuffles them, and\nreconstructs a jumbled image before classification. This transformation removes\nsemantic context, forcing the classifier to rely on local texture features.\n  We evaluate this approach on a dataset of extruded cement images, using a\nResNet-18-based architecture. Our experiments compare the patch and shuffle\nmethod to a standard pipeline, holding all other factors constant. Results show\na significant improvement in accuracy: the patch and shuffle model achieved\n90.64% test accuracy versus 72.46% for the baseline. These findings suggest\nthat disrupting global structure enhances performance in texture-based\nclassification tasks.\n  This method has implications for broader vision tasks where low-level\nfeatures matter more than high-level semantics. The technique may improve\nclassification in applications ranging from fabrication monitoring to medical\nimaging.",
    "published_date": "2025-04-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.10353v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.10276v1",
    "title": "Who Speaks for Ethics? How Demographics Shape Ethical Advocacy in Software Development",
    "authors": [
      "Lauren Olson",
      "Ricarda Anna-Lena Fischer",
      "Florian Kunneman",
      "Emitzá Guzmán"
    ],
    "author_ids": [],
    "abstract": "The integration of ethics into software development faces significant\nchallenges due to market fundamentalism in organizational practices, where\nprofit often takes precedence over ethical considerations. Additionally, the\ncritical influence of practitioners' individual backgrounds on ethical\ndecision-making remains underexplored, highlighting a gap in comprehensive\nresearch. This is especially essential to understand due to the demographic\nimbalance in software roles. This study investigates ethical concerns in\nsoftware development, focusing on how they are perceived, prioritized, and\naddressed by demographically different practitioners. By surveying 217 software\npractitioners across diverse roles, industries, and countries, we identify\ncritical barriers to ethical integration and examine practitioners' capacity to\nmitigate these issues. Our findings reveal pronounced demographic disparities,\nwith marginalized groups - including women, BIPOC, and disabled individuals -\nreporting ethical concerns at higher frequencies. Notably, marginalized\npractitioners demonstrated heightened sensitivity to ethical implementation and\ngreater empowerment to address them. However, practitioners overall often lack\nthe support needed to address ethical challenges effectively. These insights\nunderscore the urgent need for reforms in software education and development\nprocesses that center on diverse perspectives. Such reforms are essential to\nadvancing ethical integration in software development and ensuring responsible\ncomputing practices in an increasingly complex technological landscape.",
    "published_date": "2025-04-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SE",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.10276v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.10233v1",
    "title": "Bingo: Radix-based Bias Factorization for Random Walk on Dynamic Graphs",
    "authors": [
      "Pinhuan Wang",
      "Chengying Huan",
      "Zhibin Wang",
      "Chen Tian",
      "Yuede Ji",
      "Hang Liu"
    ],
    "author_ids": [],
    "abstract": "Random walks are a primary means for extracting information from large-scale\ngraphs. While most real-world graphs are inherently dynamic, state-of-the-art\nrandom walk engines failed to efficiently support such a critical use case.\nThis paper takes the initiative to build a general random walk engine for\ndynamically changing graphs with two key principles: (i) This system should\nsupport both low-latency streaming updates and high-throughput batched updates.\n(ii) This system should achieve fast sampling speed while maintaining\nacceptable space consumption to support dynamic graph updates. Upholding both\nstandards, we introduce Bingo, a GPU-based random walk engine for dynamically\nchanging graphs. First, we propose a novel radix-based bias factorization\nalgorithm to support constant time sampling complexity while supporting fast\nstreaming updates. Second, we present a group-adaption design to reduce space\nconsumption dramatically. Third, we incorporate GPU-aware designs to support\nhigh-throughput batched graph updates on massively parallel platforms.\nTogether, Bingo outperforms existing efforts across various applications,\nsettings, and datasets, achieving up to a 271.11x speedup compared to the\nstate-of-the-art efforts.",
    "published_date": "2025-04-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.10233v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.10232v1",
    "title": "Fairness and Efficiency in Two-Sided Matching Markets",
    "authors": [
      "Pallavi Jain",
      "Palash Jha",
      "Shubham Solanki"
    ],
    "author_ids": [],
    "abstract": "We propose a new fairness notion, motivated by the practical challenge of\nallocating teaching assistants (TAs) to courses in a department. Each course\nrequires a certain number of TAs and each TA has preferences over the courses\nthey want to assist. Similarly, each course instructor has preferences over the\nTAs who applied for their course. We demand fairness and efficiency for both\nsides separately, giving rise to the following criteria: (i) every course gets\nthe required number of TAs and the average utility of the assigned TAs meets a\nthreshold; (ii) the allocation of courses to TAs is envy-free, where a TA\nenvies another TA if the former prefers the latter's course and has a higher or\nequal grade in that course. Note that the definition of envy-freeness here\ndiffers from the one in the literature, and we call it merit-based\nenvy-freeness.\n  We show that the problem of finding a merit-based envy-free and efficient\nmatching is NP-hard even for very restricted settings, such as two courses and\nuniform valuations; constant degree, constant capacity of TAs for every course,\nvaluations in the range {0,1,2,3}, identical valuations from TAs, and even\nmore. To find tractable results, we consider some restricted instances, such\nas, strict valuation of TAs for courses, the difference between the number of\npositively valued TAs for a course and the capacity, the number of positively\nvalued TAs/courses, types of valuation functions, and obtained some\npolynomial-time solvable cases, showing the contrast with intractable results.\nWe further studied the problem in the paradigm of parameterized algorithms and\ndesigned some exact and approximation algorithms.",
    "published_date": "2025-04-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.GT",
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.10232v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.11486v1",
    "title": "Designing AI-Enabled Countermeasures to Cognitive Warfare",
    "authors": [
      "Jurriaan van Diggelen",
      "Eugene Aidman",
      "Jazz Rowa",
      "Julian Vince"
    ],
    "author_ids": [],
    "abstract": "Foreign information operations on social media platforms pose significant\nrisks to democratic societies. With the rise of Artificial Intelligence (AI),\nthis threat is likely to intensify, potentially overwhelming human defenders.\nTo achieve the necessary scale and tempo to defend against these threats,\nutilizing AI as part of the solution seems inevitable. Although there has been\na significant debate on AI in Lethal Autonomous Weapon Systems (LAWS), it is\nequally likely that AI will be widely used in information operations for\ndefensive and offensive objectives. Similar to LAWS, AI-driven information\noperations occupy a highly sensitive moral domain where removing human\ninvolvement in the tactical decision making process raises ethical concerns.\nAlthough AI has yet to revolutionize the field, a solid ethical stance is\nurgently needed on how AI can be responsibly used to defend against information\noperations on social media platforms. This paper proposes possible AI-enabled\ncountermeasures against cognitive warfare and argues how they can be developed\nin a responsible way, such that meaningful human control is preserved.",
    "published_date": "2025-04-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.11486v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.10058v1",
    "title": "Data Cooperatives: Democratic Models for Ethical Data Stewardship",
    "authors": [
      "Francisco Mendonca",
      "Giovanna DiMarzo",
      "Nabil Abdennadher"
    ],
    "author_ids": [],
    "abstract": "Data cooperatives offer a new model for fair data governance, enabling\nindividuals to collectively control, manage, and benefit from their information\nwhile adhering to cooperative principles such as democratic member control,\neconomic participation, and community concern. This paper reviews data\ncooperatives, distinguishing them from models like data trusts, data commons,\nand data unions, and defines them based on member ownership, democratic\ngovernance, and data sovereignty. It explores applications in sectors like\nhealthcare, agriculture, and construction. Despite their potential, data\ncooperatives face challenges in coordination, scalability, and member\nengagement, requiring innovative governance strategies, robust technical\nsystems, and mechanisms to align member interests with cooperative goals. The\npaper concludes by advocating for data cooperatives as a sustainable,\ndemocratic, and ethical model for the future data economy.",
    "published_date": "2025-04-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.10058v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.10000v1",
    "title": "Do We Really Need Curated Malicious Data for Safety Alignment in Multi-modal Large Language Models?",
    "authors": [
      "Yanbo Wang",
      "Jiyang Guan",
      "Jian Liang",
      "Ran He"
    ],
    "author_ids": [],
    "abstract": "Multi-modal large language models (MLLMs) have made significant progress, yet\ntheir safety alignment remains limited. Typically, current open-source MLLMs\nrely on the alignment inherited from their language module to avoid harmful\ngenerations. However, the lack of safety measures specifically designed for\nmulti-modal inputs creates an alignment gap, leaving MLLMs vulnerable to\nvision-domain attacks such as typographic manipulation. Current methods utilize\na carefully designed safety dataset to enhance model defense capability, while\nthe specific knowledge or patterns acquired from the high-quality dataset\nremain unclear. Through comparison experiments, we find that the alignment gap\nprimarily arises from data distribution biases, while image content, response\nquality, or the contrastive behavior of the dataset makes little contribution\nto boosting multi-modal safety. To further investigate this and identify the\nkey factors in improving MLLM safety, we propose finetuning MLLMs on a small\nset of benign instruct-following data with responses replaced by simple, clear\nrejection sentences. Experiments show that, without the need for\nlabor-intensive collection of high-quality malicious data, model safety can\nstill be significantly improved, as long as a specific fraction of rejection\ndata exists in the finetuning set, indicating the security alignment is not\nlost but rather obscured during multi-modal pretraining or instruction\nfinetuning. Simply correcting the underlying data bias could narrow the safety\ngap in the vision domain.",
    "published_date": "2025-04-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.10000v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.09990v1",
    "title": "Correlative and Discriminative Label Grouping for Multi-Label Visual Prompt Tuning",
    "authors": [
      "LeiLei Ma",
      "Shuo Xu",
      "MingKun Xie",
      "Lei Wang",
      "Dengdi Sun",
      "Haifeng Zhao"
    ],
    "author_ids": [],
    "abstract": "Modeling label correlations has always played a pivotal role in multi-label\nimage classification (MLC), attracting significant attention from researchers.\nHowever, recent studies have overemphasized co-occurrence relationships among\nlabels, which can lead to overfitting risk on this overemphasis, resulting in\nsuboptimal models. To tackle this problem, we advocate for balancing\ncorrelative and discriminative relationships among labels to mitigate the risk\nof overfitting and enhance model performance. To this end, we propose the\nMulti-Label Visual Prompt Tuning framework, a novel and parameter-efficient\nmethod that groups classes into multiple class subsets according to label\nco-occurrence and mutual exclusivity relationships, and then models them\nrespectively to balance the two relationships. In this work, since each group\ncontains multiple classes, multiple prompt tokens are adopted within Vision\nTransformer (ViT) to capture the correlation or discriminative label\nrelationship within each group, and effectively learn correlation or\ndiscriminative representations for class subsets. On the other hand, each group\ncontains multiple group-aware visual representations that may correspond to\nmultiple classes, and the mixture of experts (MoE) model can cleverly assign\nthem from the group-aware to the label-aware, adaptively obtaining label-aware\nrepresentation, which is more conducive to classification. Experiments on\nmultiple benchmark datasets show that our proposed approach achieves\ncompetitive results and outperforms SOTA methods on multiple pre-trained\nmodels.",
    "published_date": "2025-04-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.09990v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.09963v1",
    "title": "Towards Unbiased Federated Graph Learning: Label and Topology Perspectives",
    "authors": [
      "Zhengyu Wu",
      "Boyang Pang",
      "Xunkai Li",
      "Yinlin Zhu",
      "Daohan Su",
      "Bowen Fan",
      "Rong-Hua Li",
      "Guoren Wang",
      "Chenghu Zhou"
    ],
    "author_ids": [],
    "abstract": "Federated Graph Learning (FGL) enables privacy-preserving, distributed\ntraining of graph neural networks without sharing raw data. Among its\napproaches, subgraph-FL has become the dominant paradigm, with most work\nfocused on improving overall node classification accuracy. However, these\nmethods often overlook fairness due to the complexity of node features, labels,\nand graph structures. In particular, they perform poorly on nodes with\ndisadvantaged properties, such as being in the minority class within subgraphs\nor having heterophilous connections (neighbors with dissimilar labels or\nmisleading features). This reveals a critical issue: high accuracy can mask\ndegraded performance on structurally or semantically marginalized nodes. To\naddress this, we advocate for two fairness goals: (1) improving representation\nof minority class nodes for class-wise fairness and (2) mitigating topological\nbias from heterophilous connections for topology-aware fairness. We propose\nFairFGL, a novel framework that enhances fairness through fine-grained graph\nmining and collaborative learning. On the client side, the History-Preserving\nModule prevents overfitting to dominant local classes, while the Majority\nAlignment Module refines representations of heterophilous majority-class nodes.\nThe Gradient Modification Module transfers minority-class knowledge from\nstructurally favorable clients to improve fairness. On the server side, FairFGL\nuploads only the most influenced subset of parameters to reduce communication\ncosts and better reflect local distributions. A cluster-based aggregation\nstrategy reconciles conflicting updates and curbs global majority dominance .\nExtensive evaluations on eight benchmarks show FairFGL significantly improves\nminority-group performance , achieving up to a 22.62 percent Macro-F1 gain\nwhile enhancing convergence over state-of-the-art baselines.",
    "published_date": "2025-04-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DB",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.09963v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.09946v2",
    "title": "Assessing Judging Bias in Large Reasoning Models: An Empirical Study",
    "authors": [
      "Qian Wang",
      "Zhanzhi Lou",
      "Zhenheng Tang",
      "Nuo Chen",
      "Xuandong Zhao",
      "Wenxuan Zhang",
      "Dawn Song",
      "Bingsheng He"
    ],
    "author_ids": [],
    "abstract": "Large Reasoning Models (LRMs) like DeepSeek-R1 and OpenAI-o1 have\ndemonstrated remarkable reasoning capabilities, raising important questions\nabout their biases in LLM-as-a-judge settings. We present a comprehensive\nbenchmark comparing judging biases between LLMs and LRMs across both subjective\npreference-alignment datasets and objective fact-based datasets. Through\ninvestigation of bandwagon, authority, position, and distraction biases, we\nuncover four key findings: (1) despite their advanced reasoning capabilities,\nLRMs remain susceptible to the above biases; (2) LRMs demonstrate better\nrobustness than LLMs specifically on fact-related datasets; (3) LRMs exhibit\nnotable position bias, preferring options in later positions; and (4) we\nidentify a novel \"superficial reflection bias\" where phrases mimicking\nreasoning (e.g., \"wait, let me think...\") significantly influence model\njudgments. To address these biases, we design and evaluate three mitigation\nstrategies: specialized system prompts that reduce judging biases by up to 19\\%\nin preference alignment datasets and 14\\% in fact-related datasets, in-context\nlearning that provides up to 27\\% improvement on preference tasks but shows\ninconsistent results on factual tasks, and a self-reflection mechanism that\nreduces biases by up to 10\\% in preference datasets and 16\\% in fact-related\ndatasets, with self-reflection proving particularly effective for LRMs. Our\nwork provides crucial insights for developing more reliable LLM-as-a-Judge\nframeworks, especially as LRMs become increasingly deployed as automated\njudges.",
    "published_date": "2025-04-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.09946v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.09929v1",
    "title": "Moderate Actor-Critic Methods: Controlling Overestimation Bias via Expectile Loss",
    "authors": [
      "Ukjo Hwang",
      "Songnam Hong"
    ],
    "author_ids": [],
    "abstract": "Overestimation is a fundamental characteristic of model-free reinforcement\nlearning (MF-RL), arising from the principles of temporal difference learning\nand the approximation of the Q-function. To address this challenge, we propose\na novel moderate target in the Q-function update, formulated as a convex\noptimization of an overestimated Q-function and its lower bound. Our primary\ncontribution lies in the efficient estimation of this lower bound through the\nlower expectile of the Q-value distribution conditioned on a state. Notably,\nour moderate target integrates seamlessly into state-of-the-art (SOTA) MF-RL\nalgorithms, including Deep Deterministic Policy Gradient (DDPG) and Soft Actor\nCritic (SAC). Experimental results validate the effectiveness of our moderate\ntarget in mitigating overestimation bias in DDPG, SAC, and distributional RL\nalgorithms.",
    "published_date": "2025-04-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.09929v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.09861v1",
    "title": "EthosGPT: Mapping Human Value Diversity to Advance Sustainable Development Goals (SDGs)",
    "authors": [
      "Luyao Zhang"
    ],
    "author_ids": [],
    "abstract": "Large language models (LLMs) are transforming global decision-making and\nsocietal systems by processing diverse data at unprecedented scales. However,\ntheir potential to homogenize human values poses critical risks, similar to\nbiodiversity loss undermining ecological resilience. Rooted in the ancient\nGreek concept of ethos, meaning both individual character and the shared moral\nfabric of communities, EthosGPT draws on a tradition that spans from\nAristotle's virtue ethics to Adam Smith's moral sentiments as the ethical\nfoundation of economic cooperation. These traditions underscore the vital role\nof value diversity in fostering social trust, institutional legitimacy, and\nlong-term prosperity. EthosGPT addresses the challenge of value homogenization\nby introducing an open-source framework for mapping and evaluating LLMs within\na global scale of human values. Using international survey data on cultural\nindices, prompt-based assessments, and comparative statistical analyses,\nEthosGPT reveals both the adaptability and biases of LLMs across regions and\ncultures. It offers actionable insights for developing inclusive LLMs, such as\ndiversifying training data and preserving endangered cultural heritage to\nensure representation in AI systems. These contributions align with the United\nNations Sustainable Development Goals (SDGs), especially SDG 10 (Reduced\nInequalities), SDG 11.4 (Cultural Heritage Preservation), and SDG 16 (Peace,\nJustice and Strong Institutions). Through interdisciplinary collaboration,\nEthosGPT promotes AI systems that are both technically robust and ethically\ninclusive, advancing value plurality as a cornerstone for sustainable and\nequitable futures.",
    "published_date": "2025-04-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "econ.GN",
      "q-fin.EC",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.09861v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.09635v1",
    "title": "A Two-Stage Interpretable Matching Framework for Causal Inference",
    "authors": [
      "Sahil Shikalgar",
      "Md. Noor-E-Alam"
    ],
    "author_ids": [],
    "abstract": "Matching in causal inference from observational data aims to construct\ntreatment and control groups with similar distributions of covariates, thereby\nreducing confounding and ensuring an unbiased estimation of treatment effects.\nThis matched sample closely mimics a randomized controlled trial (RCT), thus\nimproving the quality of causal estimates. We introduce a novel Two-stage\nInterpretable Matching (TIM) framework for transparent and interpretable\ncovariate matching. In the first stage, we perform exact matching across all\navailable covariates. For treatment and control units without an exact match in\nthe first stage, we proceed to the second stage. Here, we iteratively refine\nthe matching process by removing the least significant confounder in each\niteration and attempting exact matching on the remaining covariates. We learn a\ndistance metric for the dropped covariates to quantify closeness to the\ntreatment unit(s) within the corresponding strata. We used these high- quality\nmatches to estimate the conditional average treatment effects (CATEs). To\nvalidate TIM, we conducted experiments on synthetic datasets with varying\nassociation structures and correlations. We assessed its performance by\nmeasuring bias in CATE estimation and evaluating multivariate overlap between\ntreatment and control groups before and after matching. Additionally, we apply\nTIM to a real-world healthcare dataset from the Centers for Disease Control and\nPrevention (CDC) to estimate the causal effect of high cholesterol on diabetes.\nOur results demonstrate that TIM improves CATE estimates, increases\nmultivariate overlap, and scales effectively to high-dimensional data, making\nit a robust tool for causal inference in observational data.",
    "published_date": "2025-04-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "stat.ME"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.09635v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.09608v1",
    "title": "ERL-MPP: Evolutionary Reinforcement Learning with Multi-head Puzzle Perception for Solving Large-scale Jigsaw Puzzles of Eroded Gaps",
    "authors": [
      "Xingke Song",
      "Xiaoying Yang",
      "Chenglin Yao",
      "Jianfeng Ren",
      "Ruibin Bai",
      "Xin Chen",
      "Xudong Jiang"
    ],
    "author_ids": [],
    "abstract": "Solving jigsaw puzzles has been extensively studied. While most existing\nmodels focus on solving either small-scale puzzles or puzzles with no gap\nbetween fragments, solving large-scale puzzles with gaps presents distinctive\nchallenges in both image understanding and combinatorial optimization. To\ntackle these challenges, we propose a framework of Evolutionary Reinforcement\nLearning with Multi-head Puzzle Perception (ERL-MPP) to derive a better set of\nswapping actions for solving the puzzles. Specifically, to tackle the\nchallenges of perceiving the puzzle with gaps, a Multi-head Puzzle Perception\nNetwork (MPPN) with a shared encoder is designed, where multiple puzzlet heads\ncomprehensively perceive the local assembly status, and a discriminator head\nprovides a global assessment of the puzzle. To explore the large swapping\naction space efficiently, an Evolutionary Reinforcement Learning (EvoRL) agent\nis designed, where an actor recommends a set of suitable swapping actions from\na large action space based on the perceived puzzle status, a critic updates the\nactor using the estimated rewards and the puzzle status, and an evaluator\ncoupled with evolutionary strategies evolves the actions aligning with the\nhistorical assembly experience. The proposed ERL-MPP is comprehensively\nevaluated on the JPLEG-5 dataset with large gaps and the MIT dataset with\nlarge-scale puzzles. It significantly outperforms all state-of-the-art models\non both datasets.",
    "published_date": "2025-04-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.09608v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.09495v1",
    "title": "Debiasing 6-DOF IMU via Hierarchical Learning of Continuous Bias Dynamics",
    "authors": [
      "Ben Liu",
      "Tzu-Yuan Lin",
      "Wei Zhang",
      "Maani Ghaffari"
    ],
    "author_ids": [],
    "abstract": "This paper develops a deep learning approach to the online debiasing of IMU\ngyroscopes and accelerometers. Most existing methods rely on implicitly\nlearning a bias term to compensate for raw IMU data. Explicit bias learning has\nrecently shown its potential as a more interpretable and motion-independent\nalternative. However, it remains underexplored and faces challenges,\nparticularly the need for ground truth bias data, which is rarely available. To\naddress this, we propose a neural ordinary differential equation (NODE)\nframework that explicitly models continuous bias dynamics, requiring only pose\nground truth, often available in datasets. This is achieved by extending the\ncanonical NODE framework to the matrix Lie group for IMU kinematics with a\nhierarchical training strategy. The validation on two public datasets and one\nreal-world experiment demonstrates significant accuracy improvements in IMU\nmeasurements, reducing errors in both pure IMU integration and visual-inertial\nodometry.",
    "published_date": "2025-04-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.09495v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.09346v1",
    "title": "\"It's not a representation of me\": Examining Accent Bias and Digital Exclusion in Synthetic AI Voice Services",
    "authors": [
      "Shira Michel",
      "Sufi Kaur",
      "Sarah Elizabeth Gillespie",
      "Jeffrey Gleason",
      "Christo Wilson",
      "Avijit Ghosh"
    ],
    "author_ids": [],
    "abstract": "Recent advances in artificial intelligence (AI) speech generation and voice\ncloning technologies have produced naturalistic speech and accurate voice\nreplication, yet their influence on sociotechnical systems across diverse\naccents and linguistic traits is not fully understood. This study evaluates two\nsynthetic AI voice services (Speechify and ElevenLabs) through a mixed methods\napproach using surveys and interviews to assess technical performance and\nuncover how users' lived experiences influence their perceptions of accent\nvariations in these speech technologies. Our findings reveal technical\nperformance disparities across five regional, English-language accents and\ndemonstrate how current speech generation technologies may inadvertently\nreinforce linguistic privilege and accent-based discrimination, potentially\ncreating new forms of digital exclusion. Overall, our study highlights the need\nfor inclusive design and regulation by providing actionable insights for\ndevelopers, policymakers, and organizations to ensure equitable and socially\nresponsible AI speech technologies.",
    "published_date": "2025-04-12T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.09346v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.09343v1",
    "title": "Confirmation Bias in Generative AI Chatbots: Mechanisms, Risks, Mitigation Strategies, and Future Research Directions",
    "authors": [
      "Yiran Du"
    ],
    "author_ids": [],
    "abstract": "This article explores the phenomenon of confirmation bias in generative AI\nchatbots, a relatively underexamined aspect of AI-human interaction. Drawing on\ncognitive psychology and computational linguistics, it examines how\nconfirmation bias, commonly understood as the tendency to seek information that\naligns with existing beliefs, can be replicated and amplified by the design and\nfunctioning of large language models. The article analyzes the mechanisms by\nwhich confirmation bias may manifest in chatbot interactions, assesses the\nethical and practical risks associated with such bias, and proposes a range of\nmitigation strategies. These include technical interventions, interface\nredesign, and policy measures aimed at promoting balanced AI-generated\ndiscourse. The article concludes by outlining future research directions,\nemphasizing the need for interdisciplinary collaboration and empirical\nevaluation to better understand and address confirmation bias in generative AI\nsystems.",
    "published_date": "2025-04-12T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.09343v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.09271v1",
    "title": "Linguistic Comparison of AI- and Human-Written Responses to Online Mental Health Queries",
    "authors": [
      "Koustuv Saha",
      "Yoshee Jain",
      "Munmun De Choudhury"
    ],
    "author_ids": [],
    "abstract": "The ubiquity and widespread use of digital and online technologies have\ntransformed mental health support, with online mental health communities\n(OMHCs) providing safe spaces for peer support. More recently, generative AI\nand large language models (LLMs) have introduced new possibilities for\nscalable, around-the-clock mental health assistance that could potentially\naugment and supplement the capabilities of OMHCs. Although genAI shows promise\nin delivering immediate and personalized responses, their effectiveness in\nreplicating the nuanced, experience-based support of human peers remains an\nopen question. In this study, we harnessed 24,114 posts and 138,758 online\ncommunity (OC) responses from 55 OMHCs on Reddit. We prompted several\nstate-of-the-art LLMs (GPT-4-Turbo, Llama-3, and Mistral-7B) with these posts,\nand compared their (AI) responses to human-written (OC) responses based on a\nvariety of linguistic measures across psycholinguistics and lexico-semantics.\nOur findings revealed that AI responses are more verbose, readable, and\nanalytically structured, but lack linguistic diversity and personal narratives\ninherent in human-human interactions. Through a qualitative examination, we\nfound validation as well as complementary insights into the nature of AI\nresponses, such as its neutrality of stance and the absence of seeking\nback-and-forth clarifications. We discuss the ethical and practical\nimplications of integrating generative AI into OMHCs, advocating for frameworks\nthat balance AI's scalability and timeliness with the irreplaceable\nauthenticity, social interactiveness, and expertise of human connections that\nform the ethos of online support communities.",
    "published_date": "2025-04-12T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.09271v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.09210v2",
    "title": "FairACE: Achieving Degree Fairness in Graph Neural Networks via Contrastive and Adversarial Group-Balanced Training",
    "authors": [
      "Jiaxin Liu",
      "Xiaoqian Jiang",
      "Xiang Li",
      "Bohan Zhang",
      "Jing Zhang"
    ],
    "author_ids": [],
    "abstract": "Fairness has been a significant challenge in graph neural networks (GNNs)\nsince degree biases often result in un-equal prediction performance among nodes\nwith varying degrees. Existing GNN models focus on prediction accuracy,\nfrequently overlooking fairness across different degree groups. To addressthis\nissue, we propose a novel GNN framework, namely Fairness- Aware Asymmetric\nContrastive Ensemble (FairACE), which inte-grates asymmetric contrastive\nlearning with adversarial training to improve degree fairness. FairACE captures\none-hop local neighborhood information and two-hop monophily similarity to\ncreate fairer node representations and employs a degree fairness regulator to\nbalance performance between high-degree and low-degree nodes. During model\ntraining, a novel group-balanced fairness loss is proposed to minimize\nclassification disparities across degree groups. In addition, we also propose a\nnovel fairness metric, the Accuracy Distribution Gap (ADG), which can\nquantitatively assess and ensure equitable performance across different\ndegree-based node groups. Experimental results on both synthetic and real-world\ndatasets demonstrate that FairACE significantly improves degree fairness\nmetrics while maintaining competitive accuracy in comparison to the\nstate-of-the-art GNN models.",
    "published_date": "2025-04-12T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.09210v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.09139v1",
    "title": "Exact inequalities and optimal recovery by inaccurate information",
    "authors": [
      "K. Yu. Osipenko"
    ],
    "author_ids": [],
    "abstract": "The paper considers a multidimensional problem of optimal recovery of an\noperator whose action is represented by multiplying the original function by a\nweight function of a special type, based on inaccurately specified information\nabout the values of operators of a similar type. An exact inequality for the\nnorms of such operators is obtained. The problem under consideration is a\ngeneralization of the problem of optimal recovery of a derivative based on\nother inaccurately specified derivatives in the space $\\mathbb R^d$ and the\nproblem of an exact inequality, which is an analogue of the\nHardy-Littlewood-Polya inequality.",
    "published_date": "2025-04-12T00:00:00",
    "year": 2025,
    "categories": [
      "math.NA",
      "cs.NA",
      "26DD15, 41A65, 41A46, 49N30"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.09139v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.09030v1",
    "title": "Authoritarian Recursions: How Fiction, History, and AI Reinforce Control in Education, Warfare, and Discourse",
    "authors": [
      "Hasan Oguz"
    ],
    "author_ids": [],
    "abstract": "The growing integration of artificial intelligence (AI) into military,\neducational, and propaganda systems raises urgent ethical challenges related to\nautonomy, bias, and the erosion of human oversight. This study employs a\nmixed-methods approach -- combining historical analysis, speculative fiction\ncritique, and contemporary case studies -- to examine how AI technologies may\nreproduce structures of authoritarian control.\n  Drawing parallels between Nazi-era indoctrination systems, the fictional\nSkynet AI from \\textit{The Terminator}, and present-day deployments of AI in\nclassrooms, battlefields, and digital media, the study identifies recurring\npatterns of harm. These include unchecked autonomy, algorithmic opacity,\nsurveillance normalization, and the amplification of structural bias. In\nmilitary contexts, lethal autonomous weapons systems (LAWS) undermine\naccountability and challenge compliance with international humanitarian law. In\neducation, AI-driven learning platforms and surveillance technologies risk\nreinforcing ideological conformity and suppressing intellectual agency.\nMeanwhile, AI-powered propaganda systems increasingly manipulate public\ndiscourse through targeted content curation and disinformation.\n  The findings call for a holistic ethical framework that integrates lessons\nfrom history, critical social theory, and technical design. To mitigate\nrecursive authoritarian risks, the study advocates for robust human-in-the-loop\narchitectures, algorithmic transparency, participatory governance, and the\nintegration of critical AI literacy into policy and pedagogy.",
    "published_date": "2025-04-12T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.09030v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.08989v2",
    "title": "RouterKT: Mixture-of-Experts for Knowledge Tracing",
    "authors": [
      "Han Liao",
      "Shuaishuai Zu"
    ],
    "author_ids": [],
    "abstract": "Knowledge Tracing (KT) is a fundamental task in Intelligent Tutoring Systems\n(ITS), which aims to model the dynamic knowledge states of students based on\ntheir interaction histories. However, existing KT models often rely on a global\nforgetting decay mechanism for capturing learning patterns, assuming that\nstudents' performance is predominantly influenced by their most recent\ninteractions. Such approaches fail to account for the diverse and complex\nlearning patterns arising from individual differences and varying learning\nstages. To address this limitation, we propose RouterKT, a novel\nMixture-of-Experts (MoE) architecture designed to capture heterogeneous\nlearning patterns by enabling experts to specialize in different patterns\nwithout any handcrafted learning pattern bias such as forgetting decay.\nSpecifically, RouterKT introduces a \\textbf{person-wise routing mechanism} to\neffectively model individual-specific learning behaviors and employs\n\\textbf{multi-heads as experts} to enhance the modeling of complex and diverse\npatterns. Comprehensive experiments on ten benchmark datasets demonstrate that\nRouterKT exhibits significant flexibility and improves the performance of\nvarious KT backbone models, with a maximum average AUC improvement of 3.29\\%\nacross different backbones and datasets, outperforming other state-of-the-art\nmodels. Moreover, RouterKT demonstrates consistently superior inference\nefficiency compared to existing approaches based on handcrafted learning\npattern bias, highlighting its usability for real-world educational\napplications. The source code is available at\nhttps://github.com/ringotc/RouterKT.git.",
    "published_date": "2025-04-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.IR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.08989v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.08974v1",
    "title": "Mixed Signals: Decoding VLMs' Reasoning and Underlying Bias in Vision-Language Conflict",
    "authors": [
      "Pouya Pezeshkpour",
      "Moin Aminnaseri",
      "Estevam Hruschka"
    ],
    "author_ids": [],
    "abstract": "Vision-language models (VLMs) have demonstrated impressive performance by\neffectively integrating visual and textual information to solve complex tasks.\nHowever, it is not clear how these models reason over the visual and textual\ndata together, nor how the flow of information between modalities is\nstructured. In this paper, we examine how VLMs reason by analyzing their biases\nwhen confronted with scenarios that present conflicting image and text cues, a\ncommon occurrence in real-world applications. To uncover the extent and nature\nof these biases, we build upon existing benchmarks to create five datasets\ncontaining mismatched image-text pairs, covering topics in mathematics,\nscience, and visual descriptions. Our analysis shows that VLMs favor text in\nsimpler queries but shift toward images as query complexity increases. This\nbias correlates with model scale, with the difference between the percentage of\nimage- and text-preferred responses ranging from +56.8% (image favored) to\n-74.4% (text favored), depending on the task and model. In addition, we explore\nthree mitigation strategies: simple prompt modifications, modifications that\nexplicitly instruct models on how to handle conflicting information (akin to\nchain-of-thought prompting), and a task decomposition strategy that analyzes\neach modality separately before combining their results. Our findings indicate\nthat the effectiveness of these strategies in identifying and mitigating bias\nvaries significantly and is closely linked to the model's overall performance\non the task and the specific modality in question.",
    "published_date": "2025-04-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.08974v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.08909v1",
    "title": "Hybrid AI-Physical Modeling for Penetration Bias Correction in X-band InSAR DEMs: A Greenland Case Study",
    "authors": [
      "Islam Mansour",
      "Georg Fischer",
      "Ronny Haensch",
      "Irena Hajnsek"
    ],
    "author_ids": [],
    "abstract": "Digital elevation models derived from Interferometric Synthetic Aperture\nRadar (InSAR) data over glacial and snow-covered regions often exhibit\nsystematic elevation errors, commonly termed \"penetration bias.\" We leverage\nexisting physics-based models and propose an integrated correction framework\nthat combines parametric physical modeling with machine learning. We evaluate\nthe approach across three distinct training scenarios - each defined by a\ndifferent set of acquisition parameters - to assess overall performance and the\nmodel's ability to generalize. Our experiments on Greenland's ice sheet using\nTanDEM-X data show that the proposed hybrid model corrections significantly\nreduce the mean and standard deviation of DEM errors compared to a purely\nphysical modeling baseline. The hybrid framework also achieves significantly\nimproved generalization than a pure ML approach when trained on data with\nlimited diversity in acquisition parameters.",
    "published_date": "2025-04-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.08909v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.08620v1",
    "title": "Efficient Mixture of Geographical Species for On Device Wildlife Monitoring",
    "authors": [
      "Emmanuel Azuh Mensah",
      "Joban Mand",
      "Yueheng Ou",
      "Min Jang",
      "Kurtis Heimerl"
    ],
    "author_ids": [],
    "abstract": "Efficient on-device models have become attractive for near-sensor insight\ngeneration, of particular interest to the ecological conservation community.\nFor this reason, deep learning researchers are proposing more approaches to\ndevelop lower compute models. However, since vision transformers are very new\nto the edge use case, there are still unexplored approaches, most notably\nconditional execution of subnetworks based on input data. In this work, we\nexplore the training of a single species detector which uses conditional\ncomputation to bias structured sub networks in a geographically-aware manner.\nWe propose a method for pruning the expert model per location and demonstrate\nconditional computation performance on two geographically distributed datasets:\niNaturalist and iWildcam.",
    "published_date": "2025-04-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.08620v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.08535v2",
    "title": "Secondary Safety Control for Systems with Sector Bounded Nonlinearities [Extended Version]",
    "authors": [
      "Yankai Lin",
      "Michelle S. Chong",
      "Carlos Murguia"
    ],
    "author_ids": [],
    "abstract": "We consider the problem of safety verification and safety-aware controller\nsynthesis for systems with sector bounded nonlinearities. We aim to keep the\nstates of the system within a given safe set under potential actuator and\nsensor attacks. Specifically, we adopt the setup that a controller has already\nbeen designed to stabilize the plant. Using invariant sets and barrier\ncertificate theory, we first give sufficient conditions to verify the safety of\nthe closed-loop system under attacks. Furthermore, by using a subset of sensors\nthat are assumed to be free of attacks, we provide a synthesis method for a\nsecondary controller that enhances the safety of the system. The sufficient\nconditions to verify safety are derived using Lyapunov-based tools and the\nS-procedure. Using the projection lemma, the conditions are then formulated as\nlinear matrix inequality (LMI) problems which can be solved efficiently.\nLastly, our theoretical results are illustrated through numerical simulations.",
    "published_date": "2025-04-11T00:00:00",
    "year": 2025,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.08535v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.08418v1",
    "title": "seeBias: A Comprehensive Tool for Assessing and Visualizing AI Fairness",
    "authors": [
      "Yilin Ning",
      "Yian Ma",
      "Mingxuan Liu",
      "Xin Li",
      "Nan Liu"
    ],
    "author_ids": [],
    "abstract": "Fairness in artificial intelligence (AI) prediction models is increasingly\nemphasized to support responsible adoption in high-stakes domains such as\nhealth care and criminal justice. Guidelines and implementation frameworks\nhighlight the importance of both predictive accuracy and equitable outcomes.\nHowever, current fairness toolkits often evaluate classification performance\ndisparities in isolation, with limited attention to other critical aspects such\nas calibration. To address these gaps, we present seeBias, an R package for\ncomprehensive evaluation of model fairness and predictive performance. seeBias\noffers an integrated evaluation across classification, calibration, and other\nperformance domains, providing a more complete view of model behavior. It\nincludes customizable visualizations to support transparent reporting and\nresponsible AI implementation. Using public datasets from criminal justice and\nhealthcare, we demonstrate how seeBias supports fairness evaluations, and\nuncovers disparities that conventional fairness metrics may overlook. The R\npackage is available on GitHub, and a Python version is under development.",
    "published_date": "2025-04-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.08418v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.12323v2",
    "title": "The Other Side of the Coin: Exploring Fairness in Retrieval-Augmented Generation",
    "authors": [
      "Zheng Zhang",
      "Ning Li",
      "Qi Liu",
      "Rui Li",
      "Weibo Gao",
      "Qingyang Mao",
      "Zhenya Huang",
      "Baosheng Yu",
      "Dacheng Tao"
    ],
    "author_ids": [],
    "abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by\nretrieving relevant document from external knowledge sources. By referencing\nthis external knowledge, RAG effectively reduces the generation of factually\nincorrect content and addresses hallucination issues within LLMs. Recently,\nthere has been growing attention to improving the performance and efficiency of\nRAG systems from various perspectives. While these advancements have yielded\nsignificant results, the application of RAG in domains with considerable\nsocietal implications raises a critical question about fairness: What impact\ndoes the introduction of the RAG paradigm have on the fairness of LLMs? To\naddress this question, we conduct extensive experiments by varying the LLMs,\nretrievers, and retrieval sources. Our experimental analysis reveals that the\nscale of the LLMs plays a significant role in influencing fairness outcomes\nwithin the RAG framework. When the model scale is smaller than 8B, the\nintegration of retrieval mechanisms often exacerbates unfairness in small-scale\nLLMs (e.g., LLaMA3.2-1B, Mistral-7B, and LLaMA3-8B). To mitigate the fairness\nissues introduced by RAG for small-scale LLMs, we propose two approaches,\nFairFT and FairFilter. Specifically, in FairFT, we align the retriever with the\nLLM in terms of fairness, enabling it to retrieve documents that facilitate\nfairer model outputs. In FairFilter, we propose a fairness filtering mechanism\nto filter out biased content after retrieval. Finally, we validate our proposed\napproaches on real-world datasets, demonstrating their effectiveness in\nimproving fairness while maintaining performance.",
    "published_date": "2025-04-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.12323v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.08863v1",
    "title": "An Evaluation of Cultural Value Alignment in LLM",
    "authors": [
      "Nicholas Sukiennik",
      "Chen Gao",
      "Fengli Xu",
      "Yong Li"
    ],
    "author_ids": [],
    "abstract": "LLMs as intelligent agents are being increasingly applied in scenarios where\nhuman interactions are involved, leading to a critical concern about whether\nLLMs are faithful to the variations in culture across regions. Several works\nhave investigated this question in various ways, finding that there are biases\npresent in the cultural representations of LLM outputs. To gain a more\ncomprehensive view, in this work, we conduct the first large-scale evaluation\nof LLM culture assessing 20 countries' cultures and languages across ten LLMs.\nWith a renowned cultural values questionnaire and by carefully analyzing LLM\noutput with human ground truth scores, we thoroughly study LLMs' cultural\nalignment across countries and among individual models. Our findings show that\nthe output over all models represents a moderate cultural middle ground. Given\nthe overall skew, we propose an alignment metric, revealing that the United\nStates is the best-aligned country and GLM-4 has the best ability to align to\ncultural values. Deeper investigation sheds light on the influence of model\norigin, prompt language, and value dimensions on cultural output. Specifically,\nmodels, regardless of where they originate, align better with the US than they\ndo with China. The conclusions provide insight to how LLMs can be better\naligned to various cultures as well as provoke further discussion of the\npotential for LLMs to propagate cultural bias and the need for more culturally\nadaptable models.",
    "published_date": "2025-04-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.08863v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.08861v1",
    "title": "Diachronic and synchronic variation in the performance of adaptive machine learning systems: The ethical challenges",
    "authors": [
      "Joshua Hatherley",
      "Robert Sparrow"
    ],
    "author_ids": [],
    "abstract": "Objectives: Machine learning (ML) has the potential to facilitate \"continual\nlearning\" in medicine, in which an ML system continues to evolve in response to\nexposure to new data over time, even after being deployed in a clinical\nsetting. In this paper, we provide a tutorial on the range of ethical issues\nraised by the use of such \"adaptive\" ML systems in medicine that have, thus\nfar, been neglected in the literature.\n  Target audience: The target audiences for this tutorial are the developers of\nmachine learning AI systems, healthcare regulators, the broader medical\ninformatics community, and practicing clinicians.\n  Scope: Discussions of adaptive ML systems to date have overlooked the\ndistinction between two sorts of variance that such systems may exhibit --\ndiachronic evolution (change over time) and synchronic variation (difference\nbetween cotemporaneous instantiations of the algorithm at different sites) --\nand under-estimated the significance of the latter. We highlight the challenges\nthat diachronic evolution and synchronic variation present for the quality of\npatient care, informed consent, and equity, and discuss the complex ethical\ntrade-offs involved in the design of such systems.",
    "published_date": "2025-04-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.08861v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.08360v1",
    "title": "Target Tracking With ISAC Using EMLSR in Next-Generation IEEE 802.11 WLANs: Non-Cooperative and Cooperative Approaches",
    "authors": [
      "Ching-Lun Tai",
      "Jingyuan Zhang",
      "Douglas M. Blough",
      "Raghupathy Sivakumar"
    ],
    "author_ids": [],
    "abstract": "New amendments support Wi-Fi access points (APs) and stations (STAs) in\nnext-generation IEEE 802.11 wireless local area networks (WLANs). IEEE 802.11be\n(Wi-Fi 7) features multi-link operation (MLO) with multi-link device (MLD)\nhosting multiple interfaces, highlighting enhanced multi-link single-radio\n(EMLSR) operation. IEEE 802.11bf features Wi-Fi sensing, enabling integrated\nsensing and communications (ISAC) in Wi-Fi. In this paper, we pioneer an\ninnovative combination of EMLSR operation and ISAC functionality, considering\ntarget tracking with ISAC using EMLSR in IEEE 802.11 WLANs. We establish a\nunique scenario where AP MLD needs to make ISAC decision and STA MLD selection\nwhen its interface gains a transmit opportunity (TXOP). Then, we present key\ndesign principles: ISAC decision involves the Kalman filter for target state\nand a developed time-based strategy for sensing/communications determination,\nwhile STA MLD selection involves a Cram\\'er-Rao lower bound (CRLB)-based\ntrilateration performance metric along with a developed candidate strategy for\nUL sensing and involves a developed weighted proportional fairness-aware\nheuristic strategy for DL communications. We propose novel non-cooperative and\ncooperative approaches, where each interface leverages its own information and\naggregate information across all interfaces, respectively. For proposed\nnon-cooperative and cooperative approaches, simulation results exhibit their\ntradeoff and superiority about sensing and communications.",
    "published_date": "2025-04-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.08360v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.08260v2",
    "title": "Evaluating the Bias in LLMs for Surveying Opinion and Decision Making in Healthcare",
    "authors": [
      "Yonchanok Khaokaew",
      "Flora D. Salim",
      "Andreas Züfle",
      "Hao Xue",
      "Taylor Anderson",
      "C. Raina MacIntyre",
      "Matthew Scotch",
      "David J Heslop"
    ],
    "author_ids": [],
    "abstract": "Generative agents have been increasingly used to simulate human behaviour in\nsilico, driven by large language models (LLMs). These simulacra serve as\nsandboxes for studying human behaviour without compromising privacy or safety.\nHowever, it remains unclear whether such agents can truly represent real\nindividuals. This work compares survey data from the Understanding America\nStudy (UAS) on healthcare decision-making with simulated responses from\ngenerative agents. Using demographic-based prompt engineering, we create\ndigital twins of survey respondents and analyse how well different LLMs\nreproduce real-world behaviours. Our findings show that some LLMs fail to\nreflect realistic decision-making, such as predicting universal vaccine\nacceptance. However, Llama 3 captures variations across race and Income more\naccurately but also introduces biases not present in the UAS data. This study\nhighlights the potential of generative agents for behavioural research while\nunderscoring the risks of bias from both LLMs and prompting strategies.",
    "published_date": "2025-04-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.08260v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.08166v1",
    "title": "Learning Object Focused Attention",
    "authors": [
      "Vivek Trivedy",
      "Amani Almalki",
      "Longin Jan Latecki"
    ],
    "author_ids": [],
    "abstract": "We propose an adaptation to the training of Vision Transformers (ViTs) that\nallows for an explicit modeling of objects during the attention computation.\nThis is achieved by adding a new branch to selected attention layers that\ncomputes an auxiliary loss which we call the object-focused attention (OFA)\nloss. We restrict the attention to image patches that belong to the same object\nclass, which allows ViTs to gain a better understanding of configural (or\nholistic) object shapes by focusing on intra-object patches instead of other\npatches such as those in the background. Our proposed inductive bias fits\neasily into the attention framework of transformers since it only adds an\nauxiliary loss over selected attention layers. Furthermore, our approach has no\nadditional overhead during inference. We also experiment with multiscale\nmasking to further improve the performance of our OFA model and give a path\nforward for self-supervised learning with our method. Our experimental results\ndemonstrate that ViTs with OFA achieve better classification results than their\nbase models, exhibit a stronger generalization ability to out-of-distribution\n(OOD) and adversarially corrupted images, and learn representations based on\nobject shapes rather than spurious correlations via general textures. For our\nOOD setting, we generate a novel dataset using the COCO dataset and Stable\nDiffusion inpainting which we plan to share with the community.",
    "published_date": "2025-04-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.08166v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.08151v1",
    "title": "Adaptive Bounded Exploration and Intermediate Actions for Data Debiasing",
    "authors": [
      "Yifan Yang",
      "Yang Liu",
      "Parinaz Naghizadeh"
    ],
    "author_ids": [],
    "abstract": "The performance of algorithmic decision rules is largely dependent on the\nquality of training datasets available to them. Biases in these datasets can\nraise economic and ethical concerns due to the resulting algorithms' disparate\ntreatment of different groups. In this paper, we propose algorithms for\nsequentially debiasing the training dataset through adaptive and bounded\nexploration in a classification problem with costly and censored feedback. Our\nproposed algorithms balance between the ultimate goal of mitigating the impacts\nof data biases -- which will in turn lead to more accurate and fairer\ndecisions, and the exploration risks incurred to achieve this goal.\nSpecifically, we propose adaptive bounds to limit the region of exploration,\nand leverage intermediate actions which provide noisy label information at a\nlower cost. We analytically show that such exploration can help debias data in\ncertain distributions, investigate how {algorithmic fairness interventions} can\nwork in conjunction with our proposed algorithms, and validate the performance\nof these algorithms through numerical experiments on synthetic and real-world\ndata.",
    "published_date": "2025-04-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.08151v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.07936v1",
    "title": "We Are All Creators: Generative AI, Collective Knowledge, and the Path Towards Human-AI Synergy",
    "authors": [
      "Jordi Linares-Pellicer",
      "Juan Izquierdo-Domenech",
      "Isabel Ferri-Molla",
      "Carlos Aliaga-Torro"
    ],
    "author_ids": [],
    "abstract": "Generative AI presents a profound challenge to traditional notions of human\nuniqueness, particularly in creativity. Fueled by neural network based\nfoundation models, these systems demonstrate remarkable content generation\ncapabilities, sparking intense debates about authorship, copyright, and\nintelligence itself. This paper argues that generative AI represents an\nalternative form of intelligence and creativity, operating through mathematical\npattern synthesis rather than biological understanding or verbatim replication.\nThe fundamental differences between artificial and biological neural networks\nreveal AI learning as primarily statistical pattern extraction from vast\ndatasets crystallized forms of collective human knowledge scraped from the\ninternet. This perspective complicates copyright theft narratives and\nhighlights practical challenges in attributing AI outputs to individual\nsources. Rather than pursuing potentially futile legal restrictions, we\nadvocate for human AI synergy. By embracing generative AI as a complementary\ntool alongside human intuition, context, and ethical judgment, society can\nunlock unprecedented innovation, democratize creative expression, and address\ncomplex challenges. This collaborative approach, grounded in realistic\nunderstanding of AIs capabilities and limitations, offers the most promising\npath forward. Additionally, recognizing these models as products of collective\nhuman knowledge raises ethical questions about accessibility ensuring equitable\naccess to these tools could prevent widening societal divides and leverage\ntheir full potential for collective benefit.",
    "published_date": "2025-04-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.07936v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.07911v1",
    "title": "The Urban Impact of AI: Modeling Feedback Loops in Next-Venue Recommendation",
    "authors": [
      "Giovanni Mauro",
      "Marco Minici",
      "Luca Pappalardo"
    ],
    "author_ids": [],
    "abstract": "Next-venue recommender systems are increasingly embedded in location-based\nservices, shaping individual mobility decisions in urban environments. While\ntheir predictive accuracy has been extensively studied, less attention has been\npaid to their systemic impact on urban dynamics. In this work, we introduce a\nsimulation framework to model the human-AI feedback loop underpinning\nnext-venue recommendation, capturing how algorithmic suggestions influence\nindividual behavior, which in turn reshapes the data used to retrain the\nmodels. Our simulations, grounded in real-world mobility data, systematically\nexplore the effects of algorithmic adoption across a range of recommendation\nstrategies. We find that while recommender systems consistently increase\nindividual-level diversity in visited venues, they may simultaneously amplify\ncollective inequality by concentrating visits on a limited subset of popular\nplaces. This divergence extends to the structure of social co-location\nnetworks, revealing broader implications for urban accessibility and spatial\nsegregation. Our framework operationalizes the feedback loop in next-venue\nrecommendation and offers a novel lens through which to assess the societal\nimpact of AI-assisted mobility-providing a computational tool to anticipate\nfuture risks, evaluate regulatory interventions, and inform the design of ethic\nalgorithmic systems.",
    "published_date": "2025-04-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.07911v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.07887v1",
    "title": "Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge",
    "authors": [
      "Riccardo Cantini",
      "Alessio Orsino",
      "Massimo Ruggiero",
      "Domenico Talia"
    ],
    "author_ids": [],
    "abstract": "Large Language Models (LLMs) have revolutionized artificial intelligence,\ndriving advancements in machine translation, summarization, and conversational\nagents. However, their increasing integration into critical societal domains\nhas raised concerns about embedded biases, which can perpetuate stereotypes and\ncompromise fairness. These biases stem from various sources, including\nhistorical inequalities in training data, linguistic imbalances, and\nadversarial manipulation. Despite mitigation efforts, recent studies indicate\nthat LLMs remain vulnerable to adversarial attacks designed to elicit biased\nresponses. This work proposes a scalable benchmarking framework to evaluate LLM\nrobustness against adversarial bias elicitation. Our methodology involves (i)\nsystematically probing models with a multi-task approach targeting biases\nacross various sociocultural dimensions, (ii) quantifying robustness through\nsafety scores using an LLM-as-a-Judge approach for automated assessment of\nmodel responses, and (iii) employing jailbreak techniques to investigate\nvulnerabilities in safety mechanisms. Our analysis examines prevalent biases in\nboth small and large state-of-the-art models and their impact on model safety.\nAdditionally, we assess the safety of domain-specific models fine-tuned for\ncritical fields, such as medicine. Finally, we release a curated dataset of\nbias-related prompts, CLEAR-Bias, to facilitate systematic vulnerability\nbenchmarking. Our findings reveal critical trade-offs between model size and\nsafety, aiding the development of fairer and more robust future language\nmodels.",
    "published_date": "2025-04-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.07887v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.07822v2",
    "title": "DG-STMTL: A Novel Graph Convolutional Network for Multi-Task Spatio-Temporal Traffic Forecasting",
    "authors": [
      "Wanna Cui",
      "Peizheng Wang",
      "Faliang Yin"
    ],
    "author_ids": [],
    "abstract": "Spatio-temporal traffic prediction is crucial in intelligent transportation\nsystems. The key challenge of accurate prediction is how to model the complex\nspatio-temporal dependencies and adapt to the inherent dynamics in data.\nTraditional Graph Convolutional Networks (GCNs) often struggle with static\nadjacency matrices that introduce domain bias or learnable matrices that may be\noverfitting to specific patterns. This challenge becomes more complex when\nconsidering Multi-Task Learning (MTL). While MTL has the potential to enhance\nprediction accuracy through task synergies, it can also face significant\nhurdles due to task interference. To overcome these challenges, this study\nintroduces a novel MTL framework, Dynamic Group-wise Spatio-Temporal Multi-Task\nLearning (DG-STMTL). DG-STMTL proposes a hybrid adjacency matrix generation\nmodule that combines static matrices with dynamic ones through a task-specific\ngating mechanism. We also introduce a group-wise GCN module to enhance the\nmodelling capability of spatio-temporal dependencies. We conduct extensive\nexperiments on two real-world datasets to evaluate our method. Results show\nthat our method outperforms other state-of-the-arts, indicating its\neffectiveness and robustness.",
    "published_date": "2025-04-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.07822v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.07801v1",
    "title": "FairEval: Evaluating Fairness in LLM-Based Recommendations with Personality Awareness",
    "authors": [
      "Chandan Kumar Sah",
      "Xiaoli Lian",
      "Tony Xu",
      "Li Zhang"
    ],
    "author_ids": [],
    "abstract": "Recent advances in Large Language Models (LLMs) have enabled their\napplication to recommender systems (RecLLMs), yet concerns remain regarding\nfairness across demographic and psychological user dimensions. We introduce\nFairEval, a novel evaluation framework to systematically assess fairness in\nLLM-based recommendations. FairEval integrates personality traits with eight\nsensitive demographic attributes,including gender, race, and age, enabling a\ncomprehensive assessment of user-level bias. We evaluate models, including\nChatGPT 4o and Gemini 1.5 Flash, on music and movie recommendations. FairEval's\nfairness metric, PAFS, achieves scores up to 0.9969 for ChatGPT 4o and 0.9997\nfor Gemini 1.5 Flash, with disparities reaching 34.79 percent. These results\nhighlight the importance of robustness in prompt sensitivity and support more\ninclusive recommendation systems.",
    "published_date": "2025-04-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.07801v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.07787v1",
    "title": "Fairness Mediator: Neutralize Stereotype Associations to Mitigate Bias in Large Language Models",
    "authors": [
      "Yisong Xiao",
      "Aishan Liu",
      "Siyuan Liang",
      "Xianglong Liu",
      "Dacheng Tao"
    ],
    "author_ids": [],
    "abstract": "LLMs have demonstrated remarkable performance across diverse applications,\nyet they inadvertently absorb spurious correlations from training data, leading\nto stereotype associations between biased concepts and specific social groups.\nThese associations perpetuate and even amplify harmful social biases, raising\nsignificant fairness concerns. To mitigate such biases, prior studies have\nattempted to project model embeddings into unbiased spaces during inference.\nHowever, these approaches have shown limited effectiveness due to their weak\nalignment with downstream social biases. Inspired by the observation that\nconcept cognition in LLMs is primarily represented through a linear associative\nmemory mechanism, where key-value mapping occurs in the MLP layers, we posited\nthat biased concepts and social groups are similarly encoded as entity (key)\nand information (value) pairs, which can be manipulated to promote fairer\nassociations. To this end, we propose Fairness Mediator (FairMed), a bias\nmitigation framework that neutralizes stereotype associations. Our framework\ncomprises two main components: a stereotype association prober and an\nadversarial debiasing neutralizer. The prober captures stereotype associations\nencoded within MLP layer activations by employing prompts centered around\nbiased concepts to detect the emission probabilities for social groups.\nSubsequently, the adversarial debiasing neutralizer intervenes in MLP\nactivations during inference to equalize the association probabilities among\ndifferent social groups. Extensive experiments across nine protected attributes\nshow that FairMed significantly outperforms SOTA methods in effectiveness.\nCompared to the most effective baseline, FairMed presents competitive\nefficiency by cutting mitigation overhead by hundreds of minutes. FairMed also\nmaintains the LLM's language understanding capabilities without compromising\noverall performance.",
    "published_date": "2025-04-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.07787v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.08834v1",
    "title": "A Systematic Literature Review of Unmanned Aerial Vehicles for Healthcare and Emergency Services",
    "authors": [
      "Sara Habibi",
      "Naghmeh Ivaki",
      "João Barata"
    ],
    "author_ids": [],
    "abstract": "Unmanned aerial vehicles (UAVs), initially developed for military\napplications, are now used in various fields. As UAVs become more common across\nmultiple industries, it is crucial to understand how to adopt them effectively,\nefficiently, and safely. The utilization of UAVs in healthcare and emergency\nservices has evolved significantly in recent years, with these aerial vehicles\npotentially contributing to increased survival rates and enhanced healthcare\nservices.\n  This paper presents a two-stage systematic literature review, including a\ntertiary study of 15 review papers and an in-depth assessment of 136 primary\npublications focused on using UAVs in healthcare and emergency services. The\nresearch demonstrates how civilian UAVs have been used in numerous\napplications, such as healthcare emergencies, medical supply delivery, and\ndisaster management, for diverse use cases such as Automated External\nDefibrillator (AED) delivery, blood delivery, and search and rescue.\n  The studies indicate that UAVs significantly improve response times in\nemergency situations, enhance survival rates by ensuring the timely delivery of\ncritical medical supplies such as AEDs, and prove to be cost-effective\nalternatives to traditional delivery methods, especially in remote or\ninaccessible areas. The studies also highlight the need for ongoing research\nand development to address existing challenges, such as regulatory frameworks,\nsecurity, privacy and safety concerns, infrastructure development, and ethical\nand social issues. Effectively understanding and tackling these challenges is\nessential for maximizing the benefits of UAV technology in healthcare and\nemergency services, ultimately leading to safer, more resilient, and responsive\nsystems that can better serve public health needs.",
    "published_date": "2025-04-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.08834v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.07610v1",
    "title": "What Contributes to Affective Polarization in Networked Online Environments? Evidence from an Agent-Based Model",
    "authors": [
      "Narayani Vedam",
      "Subhayan Mukerjee",
      "Prasanta Bhattacharya"
    ],
    "author_ids": [],
    "abstract": "Affective polarization, or, inter-party hostility, is increasingly recognized\nas a pervasive issue in democracies worldwide, posing a threat to social\ncohesion. The digital media ecosystem, now widely accessible and ever-present,\nhas often been implicated in accelerating this phenomenon. However, the precise\ncausal mechanisms responsible for driving affective polarization have been a\nsubject of extensive debate. While the concept of echo chambers, characterized\nby individuals ensconced within like-minded groups, bereft of\ncounter-attitudinal content, has long been the prevailing hypothesis,\naccumulating empirical evidence suggests a more nuanced picture. This study\naims to contribute to the ongoing debate by employing an agent-based model to\nillustrate how affective polarization is either fostered or hindered by\nindividual news consumption and dissemination patterns based on ideological\nalignment. To achieve this, we parameterize three key aspects: (1) The\naffective asymmetry of individuals' engagement with in-party versus out-party\ncontent, (2) The proportion of in-party members within one's social\nneighborhood, and (3) The degree of partisan bias among the elites within the\npopulation. Subsequently, we observe macro-level changes in affective\npolarization within the population under various conditions stipulated by these\nparameters. This approach allows us to explore the intricate dynamics of\naffective polarization within digital environments, shedding light on the\ninterplay between individual behaviors, social networks, and information\nexposure.",
    "published_date": "2025-04-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.07610v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.07607v1",
    "title": "Stochastic Smoothed Primal-Dual Algorithms for Nonconvex Optimization with Linear Inequality Constraints",
    "authors": [
      "Ruichuan Huang",
      "Jiawei Zhang",
      "Ahmet Alacaoglu"
    ],
    "author_ids": [],
    "abstract": "We propose smoothed primal-dual algorithms for solving stochastic and smooth\nnonconvex optimization problems with linear inequality constraints. Our\nalgorithms are single-loop and only require a single stochastic gradient based\non one sample at each iteration. A distinguishing feature of our algorithm is\nthat it is based on an inexact gradient descent framework for the Moreau\nenvelope, where the gradient of the Moreau envelope is estimated using one step\nof a stochastic primal-dual augmented Lagrangian method. To handle inequality\nconstraints and stochasticity, we combine the recently established global error\nbounds in constrained optimization with a Moreau envelope-based analysis of\nstochastic proximal algorithms. For obtaining $\\varepsilon$-stationary points,\nwe establish the optimal $O(\\varepsilon^{-4})$ sample complexity guarantee for\nour algorithms and provide extensions to stochastic linear constraints. We also\nshow how to improve this complexity to $O(\\varepsilon^{-3})$ by using variance\nreduction and the expected smoothness assumption. Unlike existing methods, the\niterations of our algorithms are free of subproblems, large batch sizes or\nincreasing penalty parameters and use dual variable updates to ensure\nfeasibility.",
    "published_date": "2025-04-10T00:00:00",
    "year": 2025,
    "categories": [
      "math.OC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.07607v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.08832v1",
    "title": "Generative AI in Collaborative Academic Report Writing: Advantages, Disadvantages, and Ethical Considerations",
    "authors": [
      "Mahshid Sadeghpour",
      "Arathi Arakala",
      "Asha Rao"
    ],
    "author_ids": [],
    "abstract": "The availability and abundance of GenAI tools to administer tasks\ntraditionally managed by people have raised concerns, particularly within the\neducation and academic sectors, as some students may highly rely on these tools\nto complete the assignments designed to enable learning. This article focuses\non informing students about the significance of investing their time during\ntheir studies on developing essential life-long learning skills using their own\ncritical thinking, rather than depending on AI models that are susceptible to\nmisinformation, hallucination, and bias. As we transition to an AI-centric era,\nit is important to educate students on how these models work, their pitfalls,\nand the ethical concerns associated with feeding data to such tools.",
    "published_date": "2025-04-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.08832v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.07516v1",
    "title": "Enhancements for Developing a Comprehensive AI Fairness Assessment Standard",
    "authors": [
      "Avinash Agarwal",
      "Mayashankar Kumar",
      "Manisha J. Nene"
    ],
    "author_ids": [],
    "abstract": "As AI systems increasingly influence critical sectors like\ntelecommunications, finance, healthcare, and public services, ensuring fairness\nin decision-making is essential to prevent biased or unjust outcomes that\ndisproportionately affect vulnerable entities or result in adverse impacts.\nThis need is particularly pressing as the industry approaches the 6G era, where\nAI will drive complex functions like autonomous network management and\nhyper-personalized services. The TEC Standard for Fairness Assessment and\nRating of AI Systems provides guidelines for evaluating fairness in AI,\nfocusing primarily on tabular data and supervised learning models. However, as\nAI applications diversify, this standard requires enhancement to strengthen its\nimpact and broaden its applicability. This paper proposes an expansion of the\nTEC Standard to include fairness assessments for images, unstructured text, and\ngenerative AI, including large language models, ensuring a more comprehensive\napproach that keeps pace with evolving AI technologies. By incorporating these\ndimensions, the enhanced framework will promote responsible and trustworthy AI\ndeployment across various sectors.",
    "published_date": "2025-04-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.07516v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.07395v1",
    "title": "FAIR-SIGHT: Fairness Assurance in Image Recognition via Simultaneous Conformal Thresholding and Dynamic Output Repair",
    "authors": [
      "Arya Fayyazi",
      "Mehdi Kamal",
      "Massoud Pedram"
    ],
    "author_ids": [],
    "abstract": "We introduce FAIR-SIGHT, an innovative post-hoc framework designed to ensure\nfairness in computer vision systems by combining conformal prediction with a\ndynamic output repair mechanism. Our approach calculates a fairness-aware\nnon-conformity score that simultaneously assesses prediction errors and\nfairness violations. Using conformal prediction, we establish an adaptive\nthreshold that provides rigorous finite-sample, distribution-free guarantees.\nWhen the non-conformity score for a new image exceeds the calibrated threshold,\nFAIR-SIGHT implements targeted corrective adjustments, such as logit shifts for\nclassification and confidence recalibration for detection, to reduce both group\nand individual fairness disparities, all without the need for retraining or\nhaving access to internal model parameters. Comprehensive theoretical analysis\nvalidates our method's error control and convergence properties. At the same\ntime, extensive empirical evaluations on benchmark datasets show that\nFAIR-SIGHT significantly reduces fairness disparities while preserving high\npredictive performance.",
    "published_date": "2025-04-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.07395v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.07312v2",
    "title": "The Gendered Algorithm: Navigating Financial Inclusion & Equity in AI-facilitated Access to Credit",
    "authors": [
      "Genevieve Smith"
    ],
    "author_ids": [],
    "abstract": "A growing trend in financial technology (fintech) is the use of mobile phone\ndata and machine learning (ML) to provide credit scores- and subsequently,\nopportunities to access loans- to groups left out of traditional banking. This\npaper draws on interview data with leaders, investors, and data scientists at\nfintech companies developing ML-based alternative lending apps in low- and\nmiddle-income countries to explore financial inclusion and gender implications.\nMore specifically, it examines how the underlying logics, design choices, and\nmanagement decisions of ML-based alternative lending tools by fintechs embed or\nchallenge gender biases, and consequently influence gender equity in access to\nfinance. Findings reveal developers follow 'gender blind' approaches, grounded\nin beliefs that ML is objective and data reflects the truth. This leads to a\nlack of grappling with the ways data, features for creditworthiness, and access\nto apps are gendered. Overall, tools increase access to finance, but not gender\nequitably: Interviewees report less women access loans and receive lower\namounts than men, despite being better repayers. Fintechs identify demand- and\nsupply-side reasons for gender differences, but frame them as outside their\nresponsibility. However, that women are observed as better repayers reveals a\nmarket inefficiency and potential discriminatory effect, further linked to\nprofit optimization objectives. This research introduces the concept of encoded\ngender norms, whereby without explicit attention to the gendered nature of data\nand algorithmic design, AI tools reproduce existing inequalities. In doing so,\nthey reinforce gender norms as self-fulfilling prophecies. The idea that AI is\ninherently objective and, when left alone, 'fair', is seductive and misleading.\nIn reality, algorithms reflect the perspectives, priorities, and values of the\npeople and institutions that design them.",
    "published_date": "2025-04-09T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.07312v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.07170v1",
    "title": "Trustworthy AI Must Account for Intersectionality",
    "authors": [
      "Jesse C. Cresswell"
    ],
    "author_ids": [],
    "abstract": "Trustworthy AI encompasses many aspirational aspects for aligning AI systems\nwith human values, including fairness, privacy, robustness, explainability, and\nuncertainty quantification. However, efforts to enhance one aspect often\nintroduce unintended trade-offs that negatively impact others, making it\nchallenging to improve all aspects simultaneously. In this position paper, we\nreview notable approaches to these five aspects and systematically consider\nevery pair, detailing the negative interactions that can arise. For example,\napplying differential privacy to model training can amplify biases in the data,\nundermining fairness. Drawing on these findings, we take the position that\naddressing trustworthiness along each axis in isolation is insufficient.\nInstead, research on Trustworthy AI must account for intersectionality between\naspects and adopt a holistic view across all relevant axes at once. To\nillustrate our perspective, we provide guidance on how researchers can work\ntowards integrated trustworthiness, a case study on how intersectionality\napplies to the financial industry, and alternative views to our position.",
    "published_date": "2025-04-09T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.07170v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.07031v1",
    "title": "Identifying Key Challenges of Hardness-Based Resampling",
    "authors": [
      "Pawel Pukowski",
      "Venet Osmani"
    ],
    "author_ids": [],
    "abstract": "Performance gap across classes remains a persistent challenge in machine\nlearning, often attributed to variations in class hardness. One way to quantify\nclass hardness is through sample complexity - the minimum number of samples\nrequired to effectively learn a given class. Sample complexity theory suggests\nthat class hardness is driven by differences in the amount of data required for\ngeneralization. That is, harder classes need substantially more samples to\nachieve generalization. Therefore, hardness-based resampling is a promising\napproach to mitigate these performance disparities. While resampling has been\nstudied extensively in data-imbalanced settings, its impact on balanced\ndatasets remains unexplored.\n  This raises the fundamental question whether resampling is effective because\nit addresses data imbalance or hardness imbalance. We begin addressing this\nquestion by introducing class imbalance into balanced datasets and evaluate its\neffect on performance disparities. We oversample hard classes and undersample\neasy classes to bring hard classes closer to their sample complexity\nrequirements while maintaining a constant dataset size for fairness. We\nestimate class-level hardness using the Area Under the Margin (AUM) hardness\nestimator and leverage it to compute resampling ratios. Using these ratios, we\nperform hardness-based resampling on the well-known CIFAR-10 and CIFAR-100\ndatasets.\n  Contrary to theoretical expectations, our results show that hardness-based\nresampling does not meaningfully affect class-wise performance disparities. To\nexplain this discrepancy, we conduct detailed analyses to identify key\nchallenges unique to hardness-based imbalance, distinguishing it from\ntraditional data-based imbalance. Our insights help explain why theoretical\nsample complexity expectations fail to translate into practical performance\ngains and we provide guidelines for future research.",
    "published_date": "2025-04-09T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.07031v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.13908v1",
    "title": "AI-Assisted Conversational Interviewing: Effects on Data Quality and User Experience",
    "authors": [
      "Soubhik Barari",
      "Jarret Angbazo",
      "Natalie Wang",
      "Leah M. Christian",
      "Elizabeth Dean",
      "Zoe Slowinski",
      "Brandon Sepulvado"
    ],
    "author_ids": [],
    "abstract": "Standardized surveys scale efficiently but sacrifice depth, while\nconversational interviews improve response quality at the cost of scalability\nand consistency. This study bridges the gap between these methods by\nintroducing a framework for AI-assisted conversational interviewing. To\nevaluate this framework, we conducted a web survey experiment where 1,800\nparticipants were randomly assigned to text-based conversational AI agents, or\n\"textbots\", to dynamically probe respondents for elaboration and interactively\ncode open-ended responses. We assessed textbot performance in terms of coding\naccuracy, response quality, and respondent experience. Our findings reveal that\ntextbots perform moderately well in live coding even without survey-specific\nfine-tuning, despite slightly inflated false positive errors due to respondent\nacquiescence bias. Open-ended responses were more detailed and informative, but\nthis came at a slight cost to respondent experience. Our findings highlight the\nfeasibility of using AI methods to enhance open-ended data collection in web\nsurveys.",
    "published_date": "2025-04-09T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.AI",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.13908v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.06669v1",
    "title": "NLP Security and Ethics, in the Wild",
    "authors": [
      "Heather Lent",
      "Erick Galinkin",
      "Yiyi Chen",
      "Jens Myrup Pedersen",
      "Leon Derczynski",
      "Johannes Bjerva"
    ],
    "author_ids": [],
    "abstract": "As NLP models are used by a growing number of end-users, an area of\nincreasing importance is NLP Security (NLPSec): assessing the vulnerability of\nmodels to malicious attacks and developing comprehensive countermeasures\nagainst them. While work at the intersection of NLP and cybersecurity has the\npotential to create safer NLP for all, accidental oversights can result in\ntangible harm (e.g., breaches of privacy or proliferation of malicious models).\nIn this emerging field, however, the research ethics of NLP have not yet faced\nmany of the long-standing conundrums pertinent to cybersecurity, until now. We\nthus examine contemporary works across NLPSec, and explore their engagement\nwith cybersecurity's ethical norms. We identify trends across the literature,\nultimately finding alarming gaps on topics like harm minimization and\nresponsible disclosure. To alleviate these concerns, we provide concrete\nrecommendations to help NLP researchers navigate this space more ethically,\nbridging the gap between traditional cybersecurity and NLP ethics, which we\nframe as ``white hat NLP''. The goal of this work is to help cultivate an\nintentional culture of ethical research for those working in NLP Security.",
    "published_date": "2025-04-09T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.06669v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.06667v1",
    "title": "Toward Holistic Evaluation of Recommender Systems Powered by Generative Models",
    "authors": [
      "Yashar Deldjoo",
      "Nikhil Mehta",
      "Maheswaran Sathiamoorthy",
      "Shuai Zhang",
      "Pablo Castells",
      "Julian McAuley"
    ],
    "author_ids": [],
    "abstract": "Recommender systems powered by generative models (Gen-RecSys) extend beyond\nclassical item ranking by producing open-ended content, which simultaneously\nunlocks richer user experiences and introduces new risks. On one hand, these\nsystems can enhance personalization and appeal through dynamic explanations and\nmulti-turn dialogues. On the other hand, they might venture into unknown\nterritory-hallucinating nonexistent items, amplifying bias, or leaking private\ninformation. Traditional accuracy metrics cannot fully capture these\nchallenges, as they fail to measure factual correctness, content safety, or\nalignment with user intent.\n  This paper makes two main contributions. First, we categorize the evaluation\nchallenges of Gen-RecSys into two groups: (i) existing concerns that are\nexacerbated by generative outputs (e.g., bias, privacy) and (ii) entirely new\nrisks (e.g., item hallucinations, contradictory explanations). Second, we\npropose a holistic evaluation approach that includes scenario-based assessments\nand multi-metric checks-incorporating relevance, factual grounding, bias\ndetection, and policy compliance. Our goal is to provide a guiding framework so\nresearchers and practitioners can thoroughly assess Gen-RecSys, ensuring\neffective personalization and responsible deployment.",
    "published_date": "2025-04-09T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.06667v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.06580v1",
    "title": "Exploring Ordinal Bias in Action Recognition for Instructional Videos",
    "authors": [
      "Joochan Kim",
      "Minjoon Jung",
      "Byoung-Tak Zhang"
    ],
    "author_ids": [],
    "abstract": "Action recognition models have achieved promising results in understanding\ninstructional videos. However, they often rely on dominant, dataset-specific\naction sequences rather than true video comprehension, a problem that we define\nas ordinal bias. To address this issue, we propose two effective video\nmanipulation methods: Action Masking, which masks frames of frequently\nco-occurring actions, and Sequence Shuffling, which randomizes the order of\naction segments. Through comprehensive experiments, we demonstrate that current\nmodels exhibit significant performance drops when confronted with nonstandard\naction sequences, underscoring their vulnerability to ordinal bias. Our\nfindings emphasize the importance of rethinking evaluation strategies and\ndeveloping models capable of generalizing beyond fixed action patterns in\ndiverse instructional videos.",
    "published_date": "2025-04-09T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.06580v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.06557v1",
    "title": "Market, power, gift, and concession economies: Comparison using four-mode primitive network models",
    "authors": [
      "Takeshi Kato",
      "Junichi Miyakoshi",
      "Misa Owa",
      "Ryuji Mine"
    ],
    "author_ids": [],
    "abstract": "Reducing wealth inequality is a global challenge, and the problems of\ncapitalism stem from the enclosure of the commons and the breakdown of the\ncommunity. According to previous studies by Polanyi, Karatani, and Graeber,\neconomic modes can be divided into capitalist market economy (enclosure and\nexchange), power economy (de-enclosure and redistribution), gift economy\n(obligation to return and reciprocity), and concession economy (de-obligation\nto return). The concession economy reflects Graeber's baseline communism (from\neach according to their abilities, to each according to their needs) and\nDeguchi's We-turn philosophy (the \"I\" as an individual has a \"fundamental\nincapability\" and the subject of physical action, responsibility, and freedom\nis \"We\" as a multi-agent system, including the \"I\"). In this study, we\nconstructed novel network models for these four modes and compared their\nproperties (cluster coefficient, graph density, reciprocity, assortativity,\ncentrality, and Gini coefficient). From the calculation results, it became\nclear that the market economy leads to inequality; the power economy mitigates\ninequality but cannot eliminate it; the gift and concession economies lead to a\nhealthy and equal economy; and the concession economy, free from the ties of\nobligation to return, is possible without guaranteeing reciprocity. We intend\nto promote the transformation from a capitalist economy to a concession economy\nthrough activities that disseminate baseline communism and the We-turn\nphilosophy that promotes concession, that is, developing a cooperative platform\nto support concession through information technology and empirical research\nthrough fieldwork.",
    "published_date": "2025-04-09T00:00:00",
    "year": 2025,
    "categories": [
      "econ.TH",
      "cs.SI",
      "physics.soc-ph",
      "91B72, 05C90, 91D30",
      "J.4; K.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.06557v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.07149v1",
    "title": "Glocalizing Generative AI in Education for the Global South: The Design Case of 21st Century Teacher Educator AI for Ghana",
    "authors": [
      "Matthew Nyaaba"
    ],
    "author_ids": [],
    "abstract": "This study presents the design and development of the 21st Century Teacher\nEducator for Ghana GPT, a customized Generative AI (GenAI) tool created using\nOpenAI's Retrieval-Augmented Generation (RAG) and Interactive Semi-Automated\nPrompting Strategy (ISA). Anchored in a Glocalized design approach, this tool\nsupports pre-service teachers (PSTs) in Ghana by embedding localized\nlinguistic, cultural, and curricular content within globally aligned principles\nof ethical and responsible AI use. The model utilizes structured, preloaded\ndatasets-including Ghana's National Teacher Education Curriculum Framework\n(NTECF), UNESCO's (2023) AI guidelines, and culturally responsive pedagogies-to\noffer curriculum-aligned, linguistically adaptive, and pedagogically grounded\nlearning support. The ISA enables users to input their institution, year, and\nsemester, generating tailored academic content such as lecture notes,\nassessment practice, practicum resources, and action research guidance. The\ndesign incorporates the Culture and Context-Aware Framework, GenAI-CRSciA, and\nframeworks addressing GenAI neocolonialism to ensure equity, curriculum\nfidelity, and local relevance. Pilot implementation revealed notable strengths\nin language adaptation and localization, delivering bilingual support in\nEnglish and Ghanaian languages like Twi, Dagbani, Mampruli, and Dagaare, with\ncontextualized examples for deeper understanding. The GPT also generated\npractice assessments aligned with course objectives, reinforcing learner\nengagement. Challenges included occasional hallucinations due to limited\ncorpora in some indigenous languages and access barriers tied to premium\nsubscriptions. This design case contributes to discourse on Glocalized GenAI\nand calls for collaboration with OpenAI NextGen to expand access and\nempirically assess usage across diverse African educational contexts.",
    "published_date": "2025-04-09T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.07149v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.06470v1",
    "title": "Deep Fair Learning: A Unified Framework for Fine-tuning Representations with Sufficient Networks",
    "authors": [
      "Enze Shi",
      "Linglong Kong",
      "Bei Jiang"
    ],
    "author_ids": [],
    "abstract": "Ensuring fairness in machine learning is a critical and challenging task, as\nbiased data representations often lead to unfair predictions. To address this,\nwe propose Deep Fair Learning, a framework that integrates nonlinear sufficient\ndimension reduction with deep learning to construct fair and informative\nrepresentations. By introducing a novel penalty term during fine-tuning, our\nmethod enforces conditional independence between sensitive attributes and\nlearned representations, addressing bias at its source while preserving\npredictive performance. Unlike prior methods, it supports diverse sensitive\nattributes, including continuous, discrete, binary, or multi-group types.\nExperiments on various types of data structure show that our approach achieves\na superior balance between fairness and utility, significantly outperforming\nstate-of-the-art baselines.",
    "published_date": "2025-04-08T00:00:00",
    "year": 2025,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.06470v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.06436v1",
    "title": "Language-Dependent Political Bias in AI: A Study of ChatGPT and Gemini",
    "authors": [
      "Dogus Yuksel",
      "Mehmet Cem Catalbas",
      "Bora Oc"
    ],
    "author_ids": [],
    "abstract": "As leading examples of large language models, ChatGPT and Gemini claim to\nprovide accurate and unbiased information, emphasizing their commitment to\npolitical neutrality and avoidance of personal bias. This research investigates\nthe political tendency of large language models and the existence of\ndifferentiation according to the query language. For this purpose, ChatGPT and\nGemini were subjected to a political axis test using 14 different languages.\nThe findings of the study suggest that these large language models do exhibit\npolitical tendencies, with both models demonstrating liberal and leftist\nbiases. A comparative analysis revealed that Gemini exhibited a more pronounced\nliberal and left-wing tendency compared to ChatGPT. The study also found that\nthese political biases varied depending on the language used for inquiry. The\nstudy delves into the factors that constitute political tendencies and\nlinguistic differentiation, exploring differences in the sources and scope of\neducational data, structural and grammatical features of languages, cultural\nand political contexts, and the model's response to linguistic features. From\nthis standpoint, and an ethical perspective, it is proposed that artificial\nintelligence tools should refrain from asserting a lack of political tendencies\nand neutrality, instead striving for political neutrality and executing user\nqueries by incorporating these tendencies.",
    "published_date": "2025-04-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.ET",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.06436v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.07997v1",
    "title": "BiasCause: Evaluate Socially Biased Causal Reasoning of Large Language Models",
    "authors": [
      "Tian Xie",
      "Tongxin Yin",
      "Vaishakh Keshava",
      "Xueru Zhang",
      "Siddhartha Reddy Jonnalagadda"
    ],
    "author_ids": [],
    "abstract": "While large language models (LLMs) already play significant roles in society,\nresearch has shown that LLMs still generate content including social bias\nagainst certain sensitive groups. While existing benchmarks have effectively\nidentified social biases in LLMs, a critical gap remains in our understanding\nof the underlying reasoning that leads to these biased outputs. This paper goes\none step further to evaluate the causal reasoning process of LLMs when they\nanswer questions eliciting social biases. We first propose a novel conceptual\nframework to classify the causal reasoning produced by LLMs. Next, we use LLMs\nto synthesize $1788$ questions covering $8$ sensitive attributes and manually\nvalidate them. The questions can test different kinds of causal reasoning by\nletting LLMs disclose their reasoning process with causal graphs. We then test\n4 state-of-the-art LLMs. All models answer the majority of questions with\nbiased causal reasoning, resulting in a total of $4135$ biased causal graphs.\nMeanwhile, we discover $3$ strategies for LLMs to avoid biased causal reasoning\nby analyzing the \"bias-free\" cases. Finally, we reveal that LLMs are also prone\nto \"mistaken-biased\" causal reasoning, where they first confuse correlation\nwith causality to infer specific sensitive group names and then incorporate\nbiased causal reasoning.",
    "published_date": "2025-04-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.07997v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.06389v1",
    "title": "SemiDAViL: Semi-supervised Domain Adaptation with Vision-Language Guidance for Semantic Segmentation",
    "authors": [
      "Hritam Basak",
      "Zhaozheng Yin"
    ],
    "author_ids": [],
    "abstract": "Domain Adaptation (DA) and Semi-supervised Learning (SSL) converge in\nSemi-supervised Domain Adaptation (SSDA), where the objective is to transfer\nknowledge from a source domain to a target domain using a combination of\nlimited labeled target samples and abundant unlabeled target data. Although\nintuitive, a simple amalgamation of DA and SSL is suboptimal in semantic\nsegmentation due to two major reasons: (1) previous methods, while able to\nlearn good segmentation boundaries, are prone to confuse classes with similar\nvisual appearance due to limited supervision; and (2) skewed and imbalanced\ntraining data distribution preferring source representation learning whereas\nimpeding from exploring limited information about tailed classes. Language\nguidance can serve as a pivotal semantic bridge, facilitating robust class\ndiscrimination and mitigating visual ambiguities by leveraging the rich\nsemantic relationships encoded in pre-trained language models to enhance\nfeature representations across domains. Therefore, we propose the first\nlanguage-guided SSDA setting for semantic segmentation in this work.\nSpecifically, we harness the semantic generalization capabilities inherent in\nvision-language models (VLMs) to establish a synergistic framework within the\nSSDA paradigm. To address the inherent class-imbalance challenges in\nlong-tailed distributions, we introduce class-balanced segmentation loss\nformulations that effectively regularize the learning process. Through\nextensive experimentation across diverse domain adaptation scenarios, our\napproach demonstrates substantial performance improvements over contemporary\nstate-of-the-art (SoTA) methodologies. Code is available:\n\\href{https://github.com/hritam-98/SemiDAViL}{GitHub}.",
    "published_date": "2025-04-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.06389v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.06219v1",
    "title": "Can Performant LLMs Be Ethical? Quantifying the Impact of Web Crawling Opt-Outs",
    "authors": [
      "Dongyang Fan",
      "Vinko Sabolčec",
      "Matin Ansaripour",
      "Ayush Kumar Tarun",
      "Martin Jaggi",
      "Antoine Bosselut",
      "Imanol Schlag"
    ],
    "author_ids": [],
    "abstract": "The increasing adoption of web crawling opt-outs by copyright holders of\nonline content raises critical questions about the impact of data compliance on\nlarge language model (LLM) performance. However, little is known about how\nthese restrictions (and the resultant filtering of pretraining datasets) affect\nthe capabilities of models trained using these corpora. In this work, we\nconceptualize this effect as the $\\textit{data compliance gap}$ (DCG), which\nquantifies the performance difference between models trained on datasets that\ncomply with web crawling opt-outs, and those that do not. We measure the data\ncompliance gap in two settings: pretraining models from scratch and continual\npretraining from existing compliant models (simulating a setting where\ncopyrighted data could be integrated later in pretraining). Our experiments\nwith 1.5B models show that, as of January 2025, compliance with web data\nopt-outs does not degrade general knowledge acquisition (close to 0\\% DCG).\nHowever, in specialized domains such as biomedical research, excluding major\npublishers leads to performance declines. These findings suggest that while\ngeneral-purpose LLMs can be trained to perform equally well using fully open\ndata, performance in specialized domains may benefit from access to\nhigh-quality copyrighted sources later in training. Our study provides\nempirical insights into the long-debated trade-off between data compliance and\ndownstream model performance, informing future discussions on AI training\npractices and policy decisions.",
    "published_date": "2025-04-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.06219v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.06185v1",
    "title": "WoundAmbit: Bridging State-of-the-Art Semantic Segmentation and Real-World Wound Care",
    "authors": [
      "Vanessa Borst",
      "Timo Dittus",
      "Tassilo Dege",
      "Astrid Schmieder",
      "Samuel Kounev"
    ],
    "author_ids": [],
    "abstract": "Chronic wounds affect a large population, particularly the elderly and\ndiabetic patients, who often exhibit limited mobility and co-existing health\nconditions. Automated wound monitoring via mobile image capture can reduce\nin-person physician visits by enabling remote tracking of wound size. Semantic\nsegmentation is key to this process, yet wound segmentation remains\nunderrepresented in medical imaging research. To address this, we benchmark\nstate-of-the-art deep learning models from general-purpose vision, medical\nimaging, and top methods from public wound challenges. For fair comparison, we\nstandardize training, data augmentation, and evaluation, conducting\ncross-validationto minimize partitioning bias. We also assess real-world\ndeployment aspects, including generalization to an out-of-distribution wound\ndataset, computational efficiency, and interpretability. Additionally, we\npropose a reference object-based approach to convert AI-generated masks into\nclinically relevant wound size estimates, and evaluate this, along with mask\nquality, for the best models based on physician assessments. Overall, the\ntransformer-based TransNeXt showed the highest levels of generalizability.\nDespite variations in inference times, all models processed at least one image\nper second on the CPU, which is deemed adequate for the intended application.\nInterpretability analysis typically revealed prominent activations in wound\nregions, emphasizing focus on clinically relevant features. Expert evaluation\nshowed high mask approval for all analyzed models, with VWFormer and ConvNeXtS\nbackbone performing the best. Size retrieval accuracy was similar across\nmodels, and predictions closely matched expert annotations. Finally, we\ndemonstrate how our AI-driven wound size estimation framework, WoundAmbit, can\nbe integrated into a custom telehealth system. Our code will be made available\non GitHub upon publication.",
    "published_date": "2025-04-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.06185v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.06160v3",
    "title": "Navigating the Rabbit Hole: Emergent Biases in LLM-Generated Attack Narratives Targeting Mental Health Groups",
    "authors": [
      "Rijul Magu",
      "Arka Dutta",
      "Sean Kim",
      "Ashiqur R. KhudaBukhsh",
      "Munmun De Choudhury"
    ],
    "author_ids": [],
    "abstract": "Large Language Models (LLMs) have been shown to demonstrate imbalanced biases\nagainst certain groups. However, the study of unprovoked targeted attacks by\nLLMs towards at-risk populations remains underexplored. Our paper presents\nthree novel contributions: (1) the explicit evaluation of LLM-generated attacks\non highly vulnerable mental health groups; (2) a network-based framework to\nstudy the propagation of relative biases; and (3) an assessment of the relative\ndegree of stigmatization that emerges from these attacks. Our analysis of a\nrecently released large-scale bias audit dataset reveals that mental health\nentities occupy central positions within attack narrative networks, as revealed\nby a significantly higher mean centrality of closeness (p-value = 4.06e-10) and\ndense clustering (Gini coefficient = 0.7). Drawing from sociological\nfoundations of stigmatization theory, our stigmatization analysis indicates\nincreased labeling components for mental health disorder-related targets\nrelative to initial targets in generation chains. Taken together, these\ninsights shed light on the structural predilections of large language models to\nheighten harmful discourse and highlight the need for suitable approaches for\nmitigation.",
    "published_date": "2025-04-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "cs.SI",
      "J.4; K.4.1; K.4.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.06160v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.05995v1",
    "title": "NativQA Framework: Enabling LLMs with Native, Local, and Everyday Knowledge",
    "authors": [
      "Firoj Alam",
      "Md Arid Hasan",
      "Sahinur Rahman Laskar",
      "Mucahid Kutlu",
      "Shammur Absar Chowdhury"
    ],
    "author_ids": [],
    "abstract": "The rapid advancement of large language models (LLMs) has raised concerns\nabout cultural bias, fairness, and their applicability in diverse linguistic\nand underrepresented regional contexts. To enhance and benchmark the\ncapabilities of LLMs, there is a need to develop large-scale resources focused\non multilingual, local, and cultural contexts. In this study, we propose a\nframework, NativQA, that can seamlessly construct large-scale, culturally and\nregionally aligned QA datasets in native languages. The framework utilizes\nuser-defined seed queries and leverages search engines to collect\nlocation-specific, everyday information. It has been evaluated across 39\nlocations in 24 countries and in 7 languages, ranging from extremely\nlow-resource to high-resource languages, which resulted over 300K Question\nAnswer (QA) pairs. The developed resources can be used for LLM benchmarking and\nfurther fine-tuning. The framework has been made publicly available for the\ncommunity (https://gitlab.com/nativqa/nativqa-framework).",
    "published_date": "2025-04-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "68T50",
      "F.2.2; I.2.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.05995v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.05928v1",
    "title": "Evaluation of the impact of expert knowledge: How decision support scores impact the effectiveness of automatic knowledge-driven feature engineering (aKDFE)",
    "authors": [
      "Olof Björneld",
      "Tora Hammar",
      "Daniel Nilsson",
      "Alisa Lincke",
      "Welf Löwe"
    ],
    "author_ids": [],
    "abstract": "Adverse Drug Events (ADEs), harmful medication effects, pose significant\nhealthcare challenges, impacting patient safety and costs. This study evaluates\nautomatic Knowledge-Driven Feature Engineering (aKDFE) for improved ADE\nprediction from Electronic Health Record (EHR) data, comparing it with\nautomated event-based Knowledge Discovery in Databases (KDD). We investigated\nhow incorporating domain-specific ADE risk scores for prolonged heart QT\ninterval, extracted from the Janusmed Riskprofile (Janusmed) Clinical Decision\nSupport System (CDSS), affects prediction performance using EHR data and\nmedication handling events. Results indicate that, while aKDFE step 1\n(event-based feature generation) alone did not significantly improve ADE\nprediction performance, aKDFE step 2 (patient-centric transformation) enhances\nthe prediction performance. High Area Under the Receiver Operating\nCharacteristic curve (AUROC) values suggest strong feature correlations to the\noutcome, aligning with the predictive power of patients' prior healthcare\nhistory for ADEs. Statistical analysis did not confirm that incorporating the\nJanusmed information (i) risk scores and (ii) medication route of\nadministration into the model's feature set enhanced predictive performance.\nHowever, the patient-centric transformation applied by aKDFE proved to be a\nhighly effective feature engineering approach. Limitations include a\nsingle-project focus, potential bias from machine learning pipeline methods,\nand reliance on AUROC. In conclusion, aKDFE, particularly with patient-centric\ntransformation, improves ADE prediction from EHR data. Future work will explore\nattention-based models, event feature sequences, and automatic methods for\nincorporating domain knowledge into the aKDFE framework.",
    "published_date": "2025-04-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "62R01, 68T05",
      "I.2.6"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.05928v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.05923v1",
    "title": "Uncovering Fairness through Data Complexity as an Early Indicator",
    "authors": [
      "Juliett Suárez Ferreira",
      "Marija Slavkovik",
      "Jorge Casillas"
    ],
    "author_ids": [],
    "abstract": "Fairness constitutes a concern within machine learning (ML) applications.\nCurrently, there is no study on how disparities in classification complexity\nbetween privileged and unprivileged groups could influence the fairness of\nsolutions, which serves as a preliminary indicator of potential unfairness. In\nthis work, we investigate this gap, specifically, we focus on synthetic\ndatasets designed to capture a variety of biases ranging from historical bias\nto measurement and representational bias to evaluate how various complexity\nmetrics differences correlate with group fairness metrics. We then apply\nassociation rule mining to identify patterns that link disproportionate\ncomplexity differences between groups with fairness-related outcomes, offering\ndata-centric indicators to guide bias mitigation. Our findings are also\nvalidated by their application in real-world problems, providing evidence that\nquantifying group-wise classification complexity can uncover early indicators\nof potential fairness challenges. This investigation helps practitioners to\nproactively address bias in classification tasks.",
    "published_date": "2025-04-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.05923v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.06322v1",
    "title": "Assessing employment and labour issues implicated by using AI",
    "authors": [
      "Thijs Willems",
      "Darion Jin Hotan",
      "Jiawen Cheryl Tang",
      "Norakmal Hakim bin Norhashim",
      "King Wang Poon",
      "Zi An Galvyn Goh",
      "Radha Vinod"
    ],
    "author_ids": [],
    "abstract": "This chapter critiques the dominant reductionist approach in AI and work\nstudies, which isolates tasks and skills as replaceable components. Instead, it\nadvocates for a systemic perspective that emphasizes the interdependence of\ntasks, roles, and workplace contexts. Two complementary approaches are\nproposed: an ethnographic, context-rich method that highlights how AI\nreconfigures work environments and expertise; and a relational task-based\nanalysis that bridges micro-level work descriptions with macro-level labor\ntrends. The authors argue that effective AI impact assessments must go beyond\npredicting automation rates to include ethical, well-being, and\nexpertise-related questions. Drawing on empirical case studies, they\ndemonstrate how AI reshapes human-technology relations, professional roles, and\ntacit knowledge practices. The chapter concludes by calling for a\nhuman-centric, holistic framework that guides organizational and policy\ndecisions, balancing technological possibilities with social desirability and\nsustainability of work.",
    "published_date": "2025-04-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.06322v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.05758v1",
    "title": "Addressing Class Imbalance with Probabilistic Graphical Models and Variational Inference",
    "authors": [
      "Yujia Lou",
      "Jie Liu",
      "Yuan Sheng",
      "Jiawei Wang",
      "Yiwei Zhang",
      "Yaokun Ren"
    ],
    "author_ids": [],
    "abstract": "This study proposes a method for imbalanced data classification based on deep\nprobabilistic graphical models (DPGMs) to solve the problem that traditional\nmethods have insufficient learning ability for minority class samples. To\naddress the classification bias caused by class imbalance, we introduce\nvariational inference optimization probability modeling, which enables the\nmodel to adaptively adjust the representation ability of minority classes and\ncombines the class-aware weight adjustment strategy to enhance the classifier's\nsensitivity to minority classes. In addition, we combine the adversarial\nlearning mechanism to generate minority class samples in the latent space so\nthat the model can better characterize the category boundary in the\nhigh-dimensional feature space. The experiment is evaluated on the Kaggle\n\"Credit Card Fraud Detection\" dataset and compared with a variety of advanced\nimbalanced classification methods (such as GAN-based sampling, BRF,\nXGBoost-Cost Sensitive, SAAD, HAN). The results show that the method in this\nstudy has achieved the best performance in AUC, Precision, Recall and F1-score\nindicators, effectively improving the recognition rate of minority classes and\nreducing the false alarm rate. This method can be widely used in imbalanced\nclassification tasks such as financial fraud detection, medical diagnosis, and\nanomaly detection, providing a new solution for related research.",
    "published_date": "2025-04-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.05758v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.05757v1",
    "title": "A Douglas-Rachford Splitting Method for Solving Monotone Variational Inequalities in Linear-quadratic Dynamic Games",
    "authors": [
      "Reza Rahimi Baghbadorani",
      "Emilio Benenati",
      "Sergio Grammatico"
    ],
    "author_ids": [],
    "abstract": "This paper considers constrained linear dynamic games with quadratic\nobjective functions, which can be cast as affine variational inequalities. By\nleveraging the problem structure, we apply the Douglas-Rachford splitting,\nwhich generates a solution algorithm with linear convergence rate. The fast\nconvergence of the method enables receding-horizon control architectures.\nFurthermore, we demonstrate that the associated VI admits a closed-form\nsolution within a neighborhood of the attractor, thus allowing for a further\nreduction in computation time. Finally, we benchmark the proposed method via\nnumerical experiments in an automated driving application.",
    "published_date": "2025-04-08T00:00:00",
    "year": 2025,
    "categories": [
      "eess.SY",
      "cs.SY",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.05757v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.06314v1",
    "title": "Beyond authorship: Analyzing contributions in PLOS ONE and the challenges of appropriate attribution",
    "authors": [
      "Abdelghani Maddi",
      "Jaime Teixeira Da Silva"
    ],
    "author_ids": [],
    "abstract": "Purpose This study aims to evaluate the accuracy of authorship attributions\nin scientific publications, focusing on the fairness and precision of\nindividual contributions within academic works. Design/methodology/approach The\nstudy analyzes 81,823 publications from the journal PLOS ONE , covering the\nperiod from January 2018 to June 2023. It examines the authorship attributions\nwithin these publications to try and determine the prevalence of inappropriate\nauthorship. It also investigates the demographic and professional profiles of\naffected authors, exploring trends and potential factors contributing to\ninaccuracies in authorship. Findings Surprisingly, 9.14% of articles feature at\nleast one author with inappropriate authorship, affecting over 14,000\nindividuals (2.56% of the sample). Inappropriate authorship is more\nconcentrated in Asia, Africa, and specific European countries like Italy.\nEstablished researchers with significant publication records and those\naffiliated with companies or nonprofits show higher instances of potential\nmonetary authorship. Research limitations Our findings are based on\ncontributions as declared by the authors, which implies a degree of trust in\ntheir transparency. However, this reliance on self-reporting may introduce\nbiases or inaccuracies into the dataset. Further research could employ\nadditional verification methods to enhance the reliability of the findings.\nPractical implications These findings have significant implications for journal\npublishers, highlighting the necessity for robust control mechanisms to ensure\nthe integrity of authorship attributions. Moreover, researchers must exercise\ndiscernment in determining when to acknowledge a contributor and when to\ninclude them in the author list. Addressing these issues is crucial for\nmaintaining the credibility and fairness of academic publications.\nOriginality/value This study contributes to an understanding of critical issues\nwithin academic authorship, shedding light on the prevalence and impact of\ninappropriate authorship attributions. By calling for a nuanced approach to\nensure accurate credit is given where it is due, the study underscores the\nimportance of upholding ethical standards in scholarly publishing.",
    "published_date": "2025-04-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.DL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.06314v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.05689v1",
    "title": "Separator Injection Attack: Uncovering Dialogue Biases in Large Language Models Caused by Role Separators",
    "authors": [
      "Xitao Li",
      "Haijun Wang",
      "Jiang Wu",
      "Ting Liu"
    ],
    "author_ids": [],
    "abstract": "Conversational large language models (LLMs) have gained widespread attention\ndue to their instruction-following capabilities. To ensure conversational LLMs\nfollow instructions, role separators are employed to distinguish between\ndifferent participants in a conversation. However, incorporating role\nseparators introduces potential vulnerabilities. Misusing roles can lead to\nprompt injection attacks, which can easily misalign the model's behavior with\nthe user's intentions, raising significant security concerns. Although various\nprompt injection attacks have been proposed, recent research has largely\noverlooked the impact of role separators on safety. This highlights the\ncritical need to thoroughly understand the systemic weaknesses in dialogue\nsystems caused by role separators. This paper identifies modeling weaknesses\ncaused by role separators. Specifically, we observe a strong positional bias\nassociated with role separators, which is inherent in the format of dialogue\nmodeling and can be triggered by the insertion of role separators. We further\ndevelop the Separators Injection Attack (SIA), a new orthometric attack based\non role separators. The experiment results show that SIA is efficient and\nextensive in manipulating model behavior with an average gain of 18.2% for\nmanual methods and enhances the attack success rate to 100% with automatic\nmethods.",
    "published_date": "2025-04-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.05689v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.05632v2",
    "title": "Reasoning Towards Fairness: Mitigating Bias in Language Models through Reasoning-Guided Fine-Tuning",
    "authors": [
      "Sanchit Kabra",
      "Akshita Jha",
      "Chandan K. Reddy"
    ],
    "author_ids": [],
    "abstract": "Recent advances in large-scale generative language models have shown that\nreasoning capabilities can significantly improve model performance across a\nvariety of tasks. However, the impact of reasoning on a model's ability to\nmitigate stereotypical responses remains largely underexplored. In this work,\nwe investigate the crucial relationship between a model's reasoning ability and\nfairness, and ask whether improved reasoning capabilities can mitigate harmful\nstereotypical responses, especially those arising due to shallow or flawed\nreasoning. We conduct a comprehensive evaluation of multiple open-source LLMs,\nand find that larger models with stronger reasoning abilities exhibit\nsubstantially lower stereotypical bias on existing fairness benchmarks.\nBuilding on this insight, we introduce ReGiFT -- Reasoning Guided Fine-Tuning,\na novel approach that extracts structured reasoning traces from advanced\nreasoning models and infuses them into models that lack such capabilities. We\nuse only general-purpose reasoning and do not require any fairness-specific\nsupervision for bias mitigation. Notably, we see that models fine-tuned using\nReGiFT not only improve fairness relative to their non-reasoning counterparts\nbut also outperform advanced reasoning models on fairness benchmarks. We also\nanalyze how variations in the correctness of the reasoning traces and their\nlength influence model fairness and their overall performance. Our findings\nhighlight that enhancing reasoning capabilities is an effective,\nfairness-agnostic strategy for mitigating stereotypical bias caused by\nreasoning flaws.",
    "published_date": "2025-04-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.05632v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.05610v1",
    "title": "Fairness in Machine Learning-based Hand Load Estimation: A Case Study on Load Carriage Tasks",
    "authors": [
      "Arafat Rahman",
      "Sol Lim",
      "Seokhyun Chung"
    ],
    "author_ids": [],
    "abstract": "Predicting external hand load from sensor data is essential for ergonomic\nexposure assessments, as obtaining this information typically requires direct\nobservation or supplementary data. While machine learning methods have been\nused to estimate external hand load from worker postures or force exertion\ndata, our findings reveal systematic bias in these predictions due to\nindividual differences such as age and biological sex. To explore this issue,\nwe examined bias in hand load prediction by varying the sex ratio in the\ntraining dataset. We found substantial sex disparity in predictive performance,\nespecially when the training dataset is more sex-imbalanced. To address this\nbias, we developed and evaluated a fair predictive model for hand load\nestimation that leverages a Variational Autoencoder (VAE) with feature\ndisentanglement. This approach is designed to separate sex-agnostic and\nsex-specific latent features, minimizing feature overlap. The disentanglement\ncapability enables the model to make predictions based solely on sex-agnostic\nfeatures of motion patterns, ensuring fair prediction for both biological\nsexes. Our proposed fair algorithm outperformed conventional machine learning\nmethods (e.g., Random Forests) in both fairness and predictive accuracy,\nachieving a lower mean absolute error (MAE) difference across male and female\nsets and improved fairness metrics such as statistical parity (SP) and positive\nand negative residual differences (PRD and NRD), even when trained on\nimbalanced sex datasets. These findings emphasize the importance of\nfairness-aware machine learning algorithms to prevent potential disadvantages\nin workplace health and safety for certain worker populations.",
    "published_date": "2025-04-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.05610v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.05583v1",
    "title": "Gaze-Guided Learning: Avoiding Shortcut Bias in Visual Classification",
    "authors": [
      "Jiahang Li",
      "Shibo Xue",
      "Yong Su"
    ],
    "author_ids": [],
    "abstract": "Inspired by human visual attention, deep neural networks have widely adopted\nattention mechanisms to learn locally discriminative attributes for challenging\nvisual classification tasks. However, existing approaches primarily emphasize\nthe representation of such features while neglecting their precise\nlocalization, which often leads to misclassification caused by shortcut biases.\nThis limitation becomes even more pronounced when models are evaluated on\ntransfer or out-of-distribution datasets. In contrast, humans are capable of\nleveraging prior object knowledge to quickly localize and compare fine-grained\nattributes, a capability that is especially crucial in complex and\nhigh-variance classification scenarios. Motivated by this, we introduce\nGaze-CIFAR-10, a human gaze time-series dataset, along with a dual-sequence\ngaze encoder that models the precise sequential localization of human attention\non distinct local attributes. In parallel, a Vision Transformer (ViT) is\nemployed to learn the sequential representation of image content. Through\ncross-modal fusion, our framework integrates human gaze priors with\nmachine-derived visual sequences, effectively correcting inaccurate\nlocalization in image feature representations. Extensive qualitative and\nquantitative experiments demonstrate that gaze-guided cognitive cues\nsignificantly enhance classification accuracy.",
    "published_date": "2025-04-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "I.4.9; I.5.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.05583v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.05563v1",
    "title": "From Fairness to Truthfulness: Rethinking Data Valuation Design",
    "authors": [
      "Dongyang Fan",
      "Tyler J. Rotello",
      "Sai Praneeth Karimireddy"
    ],
    "author_ids": [],
    "abstract": "As large language models increasingly rely on external data sources, fairly\ncompensating data contributors has become a central concern. In this paper, we\nrevisit the design of data markets through a game-theoretic lens, where data\nowners face private, heterogeneous costs for data sharing. We show that\ncommonly used valuation methods--such as Leave-One-Out and Data Shapley--fail\nto ensure truthful reporting of these costs, leading to inefficient market\noutcomes. To address this, we adapt well-established payment rules from\nmechanism design, namely Myerson and Vickrey-Clarke-Groves (VCG), to the data\nmarket setting. We demonstrate that the Myerson payment is the minimal truthful\npayment mechanism, optimal from the buyer's perspective, and that VCG and\nMyerson payments coincide in unconstrained allocation settings. Our findings\nhighlight the importance of incorporating incentive compatibility into data\nvaluation, paving the way for more robust and efficient data markets.",
    "published_date": "2025-04-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.GT",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.05563v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.05559v1",
    "title": "SciSciGPT: Advancing Human-AI Collaboration in the Science of Science",
    "authors": [
      "Erzhuo Shao",
      "Yifang Wang",
      "Yifan Qian",
      "Zhenyu Pan",
      "Han Liu",
      "Dashun Wang"
    ],
    "author_ids": [],
    "abstract": "The increasing availability of large-scale datasets has fueled rapid progress\nacross many scientific fields, creating unprecedented opportunities for\nresearch and discovery while posing significant analytical challenges. Recent\nadvances in large language models (LLMs) and AI agents have opened new\npossibilities for human-AI collaboration, offering powerful tools to navigate\nthis complex research landscape. In this paper, we introduce SciSciGPT, an\nopen-source, prototype AI collaborator that uses the science of science as a\ntestbed to explore the potential of LLM-powered research tools. SciSciGPT\nautomates complex workflows, supports diverse analytical approaches,\naccelerates research prototyping and iteration, and facilitates\nreproducibility. Through case studies, we demonstrate its ability to streamline\na wide range of empirical and analytical research tasks while highlighting its\nbroader potential to advance research. We further propose an LLM Agent\ncapability maturity model for human-AI collaboration, envisioning a roadmap to\nfurther improve and expand upon frameworks like SciSciGPT. As AI capabilities\ncontinue to evolve, frameworks like SciSciGPT may play increasingly pivotal\nroles in scientific research and discovery, unlocking further opportunities. At\nthe same time, these new advances also raise critical challenges, from ensuring\ntransparency and ethical use to balancing human and AI contributions.\nAddressing these issues may shape the future of scientific inquiry and inform\nhow we train the next generation of scientists to thrive in an increasingly\nAI-integrated research ecosystem.",
    "published_date": "2025-04-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "I.2; J.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.05559v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.05525v1",
    "title": "Debiasing Continuous-time Nonlinear Autoregressions",
    "authors": [
      "Simon Kuang",
      "Xinfan Lin"
    ],
    "author_ids": [],
    "abstract": "We study how to identify a class of continuous-time nonlinear systems defined\nby an ordinary differential equation affine in the unknown parameter. We define\na notion of asymptotic consistency as $(n, h) \\to (\\infty, 0)$, and we achieve\nit using a family of direct methods where the first step is differentiating a\nnoisy time series and the second step is a plug-in linear estimator. The first\nstep, differentiation, is a signal processing adaptation of the nonparametric\nstatistical technique of local polynomial regression. The second step,\ngeneralized linear regression, can be consistent using a least squares\nestimator, but we demonstrate two novel bias corrections that improve the\naccuracy for finite $h$. These methods significantly broaden the class of\ncontinuous-time systems that can be consistently estimated by direct methods.",
    "published_date": "2025-04-07T00:00:00",
    "year": 2025,
    "categories": [
      "eess.SY",
      "cs.SY",
      "math.ST",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.05525v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.07992v1",
    "title": "'Neural howlround' in large language models: a self-reinforcing bias phenomenon, and a dynamic attenuation solution",
    "authors": [
      "Seth Drake"
    ],
    "author_ids": [],
    "abstract": "Large language model (LLM)-driven AI systems may exhibit an inference failure\nmode we term `neural howlround,' a self-reinforcing cognitive loop where\ncertain highly weighted inputs become dominant, leading to entrenched response\npatterns resistant to correction. This paper explores the mechanisms underlying\nthis phenomenon, which is distinct from model collapse and biased salience\nweighting. We propose an attenuation-based correction mechanism that\ndynamically introduces counterbalancing adjustments and can restore adaptive\nreasoning, even in `locked-in' AI systems. Additionally, we discuss some other\nrelated effects arising from improperly managed reinforcement. Finally, we\noutline potential applications of this mitigation strategy for improving AI\nrobustness in real-world decision-making tasks.",
    "published_date": "2025-04-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.NE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.07992v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.06303v1",
    "title": "On the Effectiveness and Generalization of Race Representations for Debiasing High-Stakes Decisions",
    "authors": [
      "Dang Nguyen",
      "Chenhao Tan"
    ],
    "author_ids": [],
    "abstract": "Understanding and mitigating biases is critical for the adoption of large\nlanguage models (LLMs) in high-stakes decision-making. We introduce Admissions\nand Hiring, decision tasks with hypothetical applicant profiles where a\nperson's race can be inferred from their name, as simplified test beds for\nracial bias. We show that Gemma 2B Instruct and LLaMA 3.2 3B Instruct exhibit\nstrong biases. Gemma grants admission to 26% more White than Black applicants,\nand LLaMA hires 60% more Asian than White applicants. We demonstrate that these\nbiases are resistant to prompt engineering: multiple prompting strategies all\nfail to promote fairness. In contrast, using distributed alignment search, we\ncan identify \"race subspaces\" within model activations and intervene on them to\ndebias model decisions. Averaging the representation across all races within\nthe subspaces reduces Gemma's bias by 37-57%. Finally, we examine the\ngeneralizability of Gemma's race subspaces, and find limited evidence for\ngeneralization, where changing the prompt format can affect the race\nrepresentation. Our work suggests mechanistic approaches may provide a\npromising venue for improving the fairness of LLMs, but a universal race\nrepresentation remains elusive.",
    "published_date": "2025-04-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.06303v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.05278v1",
    "title": "The challenge of uncertainty quantification of large language models in medicine",
    "authors": [
      "Zahra Atf",
      "Seyed Amir Ahmad Safavi-Naini",
      "Peter R. Lewis",
      "Aref Mahjoubfar",
      "Nariman Naderi",
      "Thomas R. Savage",
      "Ali Soroush"
    ],
    "author_ids": [],
    "abstract": "This study investigates uncertainty quantification in large language models\n(LLMs) for medical applications, emphasizing both technical innovations and\nphilosophical implications. As LLMs become integral to clinical\ndecision-making, accurately communicating uncertainty is crucial for ensuring\nreliable, safe, and ethical AI-assisted healthcare. Our research frames\nuncertainty not as a barrier but as an essential part of knowledge that invites\na dynamic and reflective approach to AI design. By integrating advanced\nprobabilistic methods such as Bayesian inference, deep ensembles, and Monte\nCarlo dropout with linguistic analysis that computes predictive and semantic\nentropy, we propose a comprehensive framework that manages both epistemic and\naleatoric uncertainties. The framework incorporates surrogate modeling to\naddress limitations of proprietary APIs, multi-source data integration for\nbetter context, and dynamic calibration via continual and meta-learning.\nExplainability is embedded through uncertainty maps and confidence metrics to\nsupport user trust and clinical interpretability. Our approach supports\ntransparent and ethical decision-making aligned with Responsible and Reflective\nAI principles. Philosophically, we advocate accepting controlled ambiguity\ninstead of striving for absolute predictability, recognizing the inherent\nprovisionality of medical knowledge.",
    "published_date": "2025-04-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.05278v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.05231v1",
    "title": "Mapping biodiversity at very-high resolution in Europe",
    "authors": [
      "César Leblanc",
      "Lukas Picek",
      "Benjamin Deneu",
      "Pierre Bonnet",
      "Maximilien Servajean",
      "Rémi Palard",
      "Alexis Joly"
    ],
    "author_ids": [],
    "abstract": "This paper describes a cascading multimodal pipeline for high-resolution\nbiodiversity mapping across Europe, integrating species distribution modeling,\nbiodiversity indicators, and habitat classification. The proposed pipeline\nfirst predicts species compositions using a deep-SDM, a multimodal model\ntrained on remote sensing, climate time series, and species occurrence data at\n50x50m resolution. These predictions are then used to generate biodiversity\nindicator maps and classify habitats with Pl@ntBERT, a transformer-based LLM\ndesigned for species-to-habitat mapping. With this approach, continental-scale\nspecies distribution maps, biodiversity indicator maps, and habitat maps are\nproduced, providing fine-grained ecological insights. Unlike traditional\nmethods, this framework enables joint modeling of interspecies dependencies,\nbias-aware training with heterogeneous presence-absence data, and large-scale\ninference from multi-source remote sensing inputs.",
    "published_date": "2025-04-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.05231v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.05210v1",
    "title": "A moving target in AI-assisted decision-making: Dataset shift, model updating, and the problem of update opacity",
    "authors": [
      "Joshua Hatherley"
    ],
    "author_ids": [],
    "abstract": "Machine learning (ML) systems are vulnerable to performance decline over time\ndue to dataset shift. To address this problem, experts often suggest that ML\nsystems should be regularly updated to ensure ongoing performance stability.\nSome scholarly literature has begun to address the epistemic and ethical\nchallenges associated with different updating methodologies. Thus far, however,\nlittle attention has been paid to the impact of model updating on the\nML-assisted decision-making process itself, particularly in the AI ethics and\nAI epistemology literatures. This article aims to address this gap in the\nliterature. It argues that model updating introduces a new sub-type of opacity\ninto ML-assisted decision-making -- update opacity -- that occurs when users\ncannot understand how or why an update has changed the reasoning or behaviour\nof an ML system. This type of opacity presents a variety of distinctive\nepistemic and safety concerns that available solutions to the black box problem\nin ML are largely ill-equipped to address. A variety of alternative strategies\nmay be developed or pursued to address the problem of update opacity more\ndirectly, including bi-factual explanations, dynamic model reporting, and\nupdate compatibility. However, each of these strategies presents its own risks\nor carries significant limitations. Further research will be needed to address\nthe epistemic and safety concerns associated with model updating and update\nopacity going forward.",
    "published_date": "2025-04-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.05210v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.05154v2",
    "title": "CARE: Aligning Language Models for Regional Cultural Awareness",
    "authors": [
      "Geyang Guo",
      "Tarek Naous",
      "Hiromi Wakaki",
      "Yukiko Nishimura",
      "Yuki Mitsufuji",
      "Alan Ritter",
      "Wei Xu"
    ],
    "author_ids": [],
    "abstract": "Existing language models (LMs) often exhibit a Western-centric bias and\nstruggle to represent diverse cultural knowledge. Previous attempts to address\nthis rely on synthetic data and express cultural knowledge only in English. In\nthis work, we study whether a small amount of human-written, multilingual\ncultural preference data can improve LMs across various model families and\nsizes. We first introduce CARE, a multilingual resource of 24.1k responses with\nhuman preferences on 2,580 questions about Chinese and Arab cultures, all\ncarefully annotated by native speakers and offering more balanced coverage.\nUsing CARE, we demonstrate that cultural alignment improves existing LMs beyond\ngeneric resources without compromising general capabilities. Moreover, we\nevaluate the cultural awareness of LMs, native speakers, and retrieved web\ncontent when queried in different languages. Our experiment reveals regional\ndisparities among LMs, which may also be reflected in the documentation gap:\nnative speakers often take everyday cultural commonsense and social norms for\ngranted, while non-natives are more likely to actively seek out and document\nthem. CARE is publicly available at https://github.com/Guochry/CARE (we plan to\nadd Japanese data in the near future).",
    "published_date": "2025-04-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.05154v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.05056v1",
    "title": "Infinite precedence graphs for consistency verification in P-time event graphs",
    "authors": [
      "Davide Zorzenon",
      "Jörg Raisch"
    ],
    "author_ids": [],
    "abstract": "Precedence constraints are inequalities used to model time dependencies. In\n1958, Gallai proved that a finite system of precedence constraints admits\nsolutions if and only if the corresponding precedence graph does not contain\npositive-weight circuits. We show that this result extends naturally to the\ncase of infinitely many constraints. We then analyze two specific classes of\ninfinite precedence graphs -- $\\mathbb{N}$-periodic and ultimately periodic\ngraphs -- and prove that the existence of solutions of their related\nconstraints can be verified in strongly polynomial time. The obtained\nalgorithms find applications in P-time event graphs, which are a subclass of\nP-time Petri nets able to model production systems under cyclic schedules where\ntasks need to be performed within given time windows.",
    "published_date": "2025-04-07T00:00:00",
    "year": 2025,
    "categories": [
      "eess.SY",
      "cs.DM",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.05056v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.05050v2",
    "title": "Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language Models",
    "authors": [
      "Jiawei Lian",
      "Jianhong Pan",
      "Lefan Wang",
      "Yi Wang",
      "Shaohui Mei",
      "Lap-Pui Chau"
    ],
    "author_ids": [],
    "abstract": "Large language models (LLMs) are foundational explorations to artificial\ngeneral intelligence, yet their alignment with human values via instruction\ntuning and preference learning achieves only superficial compliance. Here, we\ndemonstrate that harmful knowledge embedded during pretraining persists as\nindelible \"dark patterns\" in LLMs' parametric memory, evading alignment\nsafeguards and resurfacing under adversarial inducement at distributional\nshifts. In this study, we first theoretically analyze the intrinsic ethical\nvulnerability of aligned LLMs by proving that current alignment methods yield\nonly local \"safety regions\" in the knowledge manifold. In contrast, pretrained\nknowledge remains globally connected to harmful concepts via high-likelihood\nadversarial trajectories. Building on this theoretical insight, we empirically\nvalidate our findings by employing semantic coherence inducement under\ndistributional shifts--a method that systematically bypasses alignment\nconstraints through optimized adversarial prompts. This combined theoretical\nand empirical approach achieves a 100% attack success rate across 19 out of 23\nstate-of-the-art aligned LLMs, including DeepSeek-R1 and LLaMA-3, revealing\ntheir universal vulnerabilities.",
    "published_date": "2025-04-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.05050v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.05008v1",
    "title": "Surveying Professional Writers on AI: Limitations, Expectations, and Fears",
    "authors": [
      "Anastasiia Ivanova",
      "Natalia Fedorova",
      "Sergey Tilga",
      "Ekaterina Artemova"
    ],
    "author_ids": [],
    "abstract": "The rapid development of AI-driven tools, particularly large language models\n(LLMs), is reshaping professional writing. Still, key aspects of their adoption\nsuch as languages support, ethics, and long-term impact on writers voice and\ncreativity remain underexplored. In this work, we conducted a questionnaire (N\n= 301) and an interactive survey (N = 36) targeting professional writers\nregularly using AI. We examined LLM-assisted writing practices across 25+\nlanguages, ethical concerns, and user expectations. The findings of the survey\ndemonstrate important insights, reflecting upon the importance of: LLMs\nadoption for non-English speakers; the degree of misinformation, domain and\nstyle adaptation; usability and key features of LLMs. These insights can guide\nfurther development, benefiting both writers and a broader user base.",
    "published_date": "2025-04-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.05008v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.05007v1",
    "title": "Measuring the right thing: justifying metrics in AI impact assessments",
    "authors": [
      "Stefan Buijsman",
      "Herman Veluwenkamp"
    ],
    "author_ids": [],
    "abstract": "AI Impact Assessments are only as good as the measures used to assess the\nimpact of these systems. It is therefore paramount that we can justify our\nchoice of metrics in these assessments, especially for difficult to quantify\nethical and social values. We present a two-step approach to ensure metrics are\nproperly motivated. First, a conception needs to be spelled out (e.g. Rawlsian\nfairness or fairness as solidarity) and then a metric can be fitted to that\nconception. Both steps require separate justifications, as conceptions can be\njudged on how well they fit with the function of, for example, fairness. We\nargue that conceptual engineering offers helpful tools for this step. Second,\nmetrics need to be fitted to a conception. We illustrate this process through\nan examination of competing fairness metrics to illustrate that here the\nadditional content that a conception offers helps us justify the choice for a\nspecific metric. We thus advocate that impact assessments are not only clear on\ntheir metrics, but also on the conceptions that motivate those metrics.",
    "published_date": "2025-04-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.ET"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.05007v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.04927v1",
    "title": "How Is Generative AI Used for Persona Development?: A Systematic Review of 52 Research Articles",
    "authors": [
      "Danial Amin",
      "Joni Salminen",
      "Farhan Ahmed",
      "Sonja M. H. Tervola",
      "Sankalp Sethi",
      "Bernard J. Jansen"
    ],
    "author_ids": [],
    "abstract": "Although Generative AI (GenAI) has the potential for persona development,\nmany challenges must be addressed. This research systematically reviews 52\narticles from 2022-2024, with important findings. First, closed commercial\nmodels are frequently used in persona development, creating a monoculture\nSecond, GenAI is used in various stages of persona development (data\ncollection, segmentation, enrichment, and evaluation). Third, similar to other\nquantitative persona development techniques, there are major gaps in persona\nevaluation for AI generated personas. Fourth, human-AI collaboration models are\nunderdeveloped, despite human oversight being crucial for maintaining ethical\nstandards. These findings imply that realizing the full potential of\nAI-generated personas will require substantial efforts across academia and\nindustry. To that end, we provide a list of research avenues to inspire future\nwork.",
    "published_date": "2025-04-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.04927v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.13899v1",
    "title": "Predicting Satisfaction of Counterfactual Explanations from Human Ratings of Explanatory Qualities",
    "authors": [
      "Marharyta Domnich",
      "Rasmus Moorits Veski",
      "Julius Välja",
      "Kadi Tulver",
      "Raul Vicente"
    ],
    "author_ids": [],
    "abstract": "Counterfactual explanations are a widely used approach in Explainable AI,\noffering actionable insights into decision-making by illustrating how small\nchanges to input data can lead to different outcomes. Despite their importance,\nevaluating the quality of counterfactual explanations remains an open problem.\nTraditional quantitative metrics, such as sparsity or proximity, fail to fully\naccount for human preferences in explanations, while user studies are\ninsightful but not scalable. Moreover, relying only on a single overall\nsatisfaction rating does not lead to a nuanced understanding of why certain\nexplanations are effective or not. To address this, we analyze a dataset of\ncounterfactual explanations that were evaluated by 206 human participants, who\nrated not only overall satisfaction but also seven explanatory criteria:\nfeasibility, coherence, complexity, understandability, completeness, fairness,\nand trust. Modeling overall satisfaction as a function of these criteria, we\nfind that feasibility (the actionability of suggested changes) and trust (the\nbelief that the changes would lead to the desired outcome) consistently stand\nout as the strongest predictors of user satisfaction, though completeness also\nemerges as a meaningful contributor. Crucially, even excluding feasibility and\ntrust, other metrics explain 58% of the variance, highlighting the importance\nof additional explanatory qualities. Complexity appears independent, suggesting\nmore detailed explanations do not necessarily reduce satisfaction. Strong\nmetric correlations imply a latent structure in how users judge quality, and\ndemographic background significantly shapes ranking patterns. These insights\ninform the design of counterfactual algorithms that adapt explanatory qualities\nto user expertise and domain context.",
    "published_date": "2025-04-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.13899v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.05361v1",
    "title": "A Comparative Analysis of Modeling Approaches for the Association of FAIR Digital Objects Operations",
    "authors": [
      "Nicolas Blumenröhr",
      "Jana Böhm",
      "Philipp Ost",
      "Marco Kulüke",
      "Peter Wittenburg",
      "Christophe Blanchi",
      "Sven Bingert",
      "Ulrich Schwardmann"
    ],
    "author_ids": [],
    "abstract": "The concept of FAIR Digital Objects represents a foundational step towards\nrealizing machine-actionable, interoperable data infrastructures across\nscientific and industrial domains. As digital spaces become increasingly\nheterogeneous, scalable mechanisms for data processing and interpretability are\nessential. This paper provides a comparative analysis of various typing\nmechanisms to associate FAIR Digital Objects with their operations, addressing\nthe pressing need for a structured approach to manage data interactions within\nthe FAIR Digital Objects ecosystem. By examining three core models -- record\ntyping, profile typing, and attribute typing -- this work evaluates each\nmodel's complexity, flexibility, versatility, and interoperability, shedding\nlight on their strengths and limitations. With this assessment, we aim to offer\ninsights for adopting FDO frameworks that enhance data automation and promote\nthe seamless exchange of digital resources across domains.",
    "published_date": "2025-04-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.DL",
      "cs.DB"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.05361v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.04855v1",
    "title": "BIASINSPECTOR: Detecting Bias in Structured Data through LLM Agents",
    "authors": [
      "Haoxuan Li",
      "Mingyu Derek Ma",
      "Jen-tse Huang",
      "Zhaotian Weng",
      "Wei Wang",
      "Jieyu Zhao"
    ],
    "author_ids": [],
    "abstract": "Detecting biases in structured data is a complex and time-consuming task.\nExisting automated techniques are limited in diversity of data types and\nheavily reliant on human case-by-case handling, resulting in a lack of\ngeneralizability. Currently, large language model (LLM)-based agents have made\nsignificant progress in data science, but their ability to detect data biases\nis still insufficiently explored. To address this gap, we introduce the first\nend-to-end, multi-agent synergy framework, BIASINSPECTOR, designed for\nautomatic bias detection in structured data based on specific user\nrequirements. It first develops a multi-stage plan to analyze user-specified\nbias detection tasks and then implements it with a diverse and well-suited set\nof tools. It delivers detailed results that include explanations and\nvisualizations. To address the lack of a standardized framework for evaluating\nthe capability of LLM agents to detect biases in data, we further propose a\ncomprehensive benchmark that includes multiple evaluation metrics and a large\nset of test cases. Extensive experiments demonstrate that our framework\nachieves exceptional overall performance in structured data bias detection,\nsetting a new milestone for fairer data applications.",
    "published_date": "2025-04-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.04855v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.04840v2",
    "title": "Unsupervised Ego- and Exo-centric Dense Procedural Activity Captioning via Gaze Consensus Adaptation",
    "authors": [
      "Zhaofeng Shi",
      "Heqian Qiu",
      "Lanxiao Wang",
      "Qingbo Wu",
      "Fanman Meng",
      "Hongliang Li"
    ],
    "author_ids": [],
    "abstract": "Even from an early age, humans naturally adapt between exocentric (Exo) and\negocentric (Ego) perspectives to understand daily procedural activities.\nInspired by this cognitive ability, we propose a novel Unsupervised Ego-Exo\nDense Procedural Activity Captioning (UE$^{2}$DPAC) task, which aims to\ntransfer knowledge from the labeled source view to predict the time segments\nand descriptions of action sequences for the target view without annotations.\nDespite previous works endeavoring to address the fully-supervised single-view\nor cross-view dense video captioning, they lapse in the proposed task due to\nthe significant inter-view gap caused by temporal misalignment and irrelevant\nobject interference. Hence, we propose a Gaze Consensus-guided Ego-Exo\nAdaptation Network (GCEAN) that injects the gaze information into the learned\nrepresentations for the fine-grained Ego-Exo alignment. Specifically, we\npropose a Score-based Adversarial Learning Module (SALM) that incorporates a\ndiscriminative scoring network and compares the scores of distinct views to\nlearn unified view-invariant representations from a global level. Then, the\nGaze Consensus Construction Module (GCCM) utilizes the gaze to progressively\ncalibrate the learned representations to highlight the regions of interest and\nextract the corresponding temporal contexts. Moreover, we adopt hierarchical\ngaze-guided consistency losses to construct gaze consensus for the explicit\ntemporal and spatial adaptation between the source and target views. To support\nour research, we propose a new EgoMe-UE$^{2}$DPAC benchmark, and extensive\nexperiments demonstrate the effectiveness of our method, which outperforms many\nrelated methods by a large margin. The code will be released.",
    "published_date": "2025-04-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.MM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.04840v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.04812v1",
    "title": "Sparse Optimization for Transfer Learning: A L0-Regularized Framework for Multi-Source Domain Adaptation",
    "authors": [
      "Chenqi Gong",
      "Hu Yang"
    ],
    "author_ids": [],
    "abstract": "This paper explores transfer learning in heterogeneous multi-source\nenvironments with distributional divergence between target and auxiliary\ndomains. To address challenges in statistical bias and computational\nefficiency, we propose a Sparse Optimization for Transfer Learning (SOTL)\nframework based on L0-regularization. The method extends the Joint Estimation\nTransferred from Strata (JETS) paradigm with two key innovations: (1)\nL0-constrained exact sparsity for parameter space compression and complexity\nreduction, and (2) refining optimization focus to emphasize target parameters\nover redundant ones. Simulations show that SOTL significantly improves both\nestimation accuracy and computational speed, especially under adversarial\nauxiliary domain conditions. Empirical validation on the Community and Crime\nbenchmarks demonstrates the statistical robustness of the SOTL method in\ncross-domain transfer.",
    "published_date": "2025-04-07T00:00:00",
    "year": 2025,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.04812v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.04804v1",
    "title": "DebGCD: Debiased Learning with Distribution Guidance for Generalized Category Discovery",
    "authors": [
      "Yuanpei Liu",
      "Kai Han"
    ],
    "author_ids": [],
    "abstract": "In this paper, we tackle the problem of Generalized Category Discovery (GCD).\nGiven a dataset containing both labelled and unlabelled images, the objective\nis to categorize all images in the unlabelled subset, irrespective of whether\nthey are from known or unknown classes. In GCD, an inherent label bias exists\nbetween known and unknown classes due to the lack of ground-truth labels for\nthe latter. State-of-the-art methods in GCD leverage parametric classifiers\ntrained through self-distillation with soft labels, leaving the bias issue\nunattended. Besides, they treat all unlabelled samples uniformly, neglecting\nvariations in certainty levels and resulting in suboptimal learning. Moreover,\nthe explicit identification of semantic distribution shifts between known and\nunknown classes, a vital aspect for effective GCD, has been neglected. To\naddress these challenges, we introduce DebGCD, a \\underline{Deb}iased learning\nwith distribution guidance framework for \\underline{GCD}. Initially, DebGCD\nco-trains an auxiliary debiased classifier in the same feature space as the GCD\nclassifier, progressively enhancing the GCD features. Moreover, we introduce a\nsemantic distribution detector in a separate feature space to implicitly boost\nthe learning efficacy of GCD. Additionally, we employ a curriculum learning\nstrategy based on semantic distribution certainty to steer the debiased\nlearning at an optimized pace. Thorough evaluations on GCD benchmarks\ndemonstrate the consistent state-of-the-art performance of our framework,\nhighlighting its superiority. Project page: https://visual-ai.github.io/debgcd/",
    "published_date": "2025-04-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.04804v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.04752v1",
    "title": "Investigating Popularity Bias Amplification in Recommender Systems Employed in the Entertainment Domain",
    "authors": [
      "Dominik Kowald"
    ],
    "author_ids": [],
    "abstract": "Recommender systems have become an integral part of our daily online\nexperience by analyzing past user behavior to suggest relevant content in\nentertainment domains such as music, movies, and books. Today, they are among\nthe most widely used applications of AI and machine learning. Consequently,\nregulations and guidelines for trustworthy AI, such as the European AI Act,\nwhich addresses issues like bias and fairness, are highly relevant to the\ndesign, development, and evaluation of recommender systems. One particularly\nimportant type of bias in this context is popularity bias, which results in the\nunfair underrepresentation of less popular content in recommendation lists.\nThis work summarizes our research on investigating the amplification of\npopularity bias in recommender systems within the entertainment sector.\nAnalyzing datasets from three entertainment domains, music, movies, and anime,\nwe demonstrate that an item's recommendation frequency is positively correlated\nwith its popularity. As a result, user groups with little interest in popular\ncontent receive less accurate recommendations compared to those who prefer\nwidely popular items. Furthermore, we aim to better understand the connection\nbetween recommendation accuracy, calibration quality of algorithms, and\npopularity bias amplification.",
    "published_date": "2025-04-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.04752v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.04726v1",
    "title": "Can LLM-Driven Hard Negative Sampling Empower Collaborative Filtering? Findings and Potentials",
    "authors": [
      "Chu Zhao",
      "Enneng Yang",
      "Yuting Liu",
      "Jianzhe Zhao",
      "Guibing Guo",
      "Xingwei Wang"
    ],
    "author_ids": [],
    "abstract": "Hard negative samples can accelerate model convergence and optimize decision\nboundaries, which is key to improving the performance of recommender systems.\nAlthough large language models (LLMs) possess strong semantic understanding and\ngeneration capabilities, systematic research has not yet been conducted on how\nto generate hard negative samples effectively. To fill this gap, this paper\nintroduces the concept of Semantic Negative Sampling and exploreshow to\noptimize LLMs for high-quality, hard negative sampling. Specifically, we design\nan experimental pipeline that includes three main modules, profile generation,\nsemantic negative sampling, and semantic alignment, to verify the potential of\nLLM-driven hard negative sampling in enhancing the accuracy of collaborative\nfiltering (CF). Experimental results indicate that hard negative samples\ngenerated based on LLMs, when semantically aligned and integrated into CF, can\nsignificantly improve CF performance, although there is still a certain gap\ncompared to traditional negative sampling methods. Further analysis reveals\nthat this gap primarily arises from two major challenges: noisy samples and\nlack of behavioral constraints. To address these challenges, we propose a\nframework called HNLMRec, based on fine-tuning LLMs supervised by collaborative\nsignals. Experimental results show that this framework outperforms traditional\nnegative sampling and other LLM-driven recommendation methods across multiple\ndatasets, providing new solutions for empowering traditional RS with LLMs.\nAdditionally, we validate the excellent generalization ability of the LLM-based\nsemantic negative sampling method on new datasets, demonstrating its potential\nin alleviating issues such as data sparsity, popularity bias, and the problem\nof false hard negative samples. Our implementation code is available at\nhttps://github.com/user683/HNLMRec.",
    "published_date": "2025-04-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.04726v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.04640v1",
    "title": "Splits! A Flexible Dataset for Evaluating a Model's Demographic Social Inference",
    "authors": [
      "Eylon Caplan",
      "Tania Chakraborty",
      "Dan Goldwasser"
    ],
    "author_ids": [],
    "abstract": "Understanding how people of various demographics think, feel, and express\nthemselves (collectively called group expression) is essential for social\nscience and underlies the assessment of bias in Large Language Models (LLMs).\nWhile LLMs can effectively summarize group expression when provided with\nempirical examples, coming up with generalizable theories of how a group's\nexpression manifests in real-world text is challenging. In this paper, we\ndefine a new task called Group Theorization, in which a system must write\ntheories that differentiate expression across demographic groups. We make\navailable a large dataset on this task, Splits!, constructed by splitting\nReddit posts by neutral topics (e.g. sports, cooking, and movies) and by\ndemographics (e.g. occupation, religion, and race). Finally, we suggest a\nsimple evaluation framework for assessing how effectively a method can generate\n'better' theories about group expression, backed by human validation. We\npublicly release the raw corpora and evaluation scripts for Splits! to help\nresearchers assess how methods infer--and potentially misrepresent--group\ndifferences in expression. We make Splits! and our evaluation module available\nat https://github.com/eyloncaplan/splits.",
    "published_date": "2025-04-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.04640v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.06296v1",
    "title": "Assessing Computer Science Student Attitudes Towards AI Ethics and Policy",
    "authors": [
      "James Weichert",
      "Dayoung Kim",
      "Qin Zhu",
      "Junghwan Kim",
      "Hoda Eldardiry"
    ],
    "author_ids": [],
    "abstract": "As artificial intelligence (AI) grows in popularity and importance-both as a\ndomain within broader computing research and in society at large-increasing\nfocus will need to be paid to the ethical governance of this emerging\ntechnology. The attitudes and competencies with respect to AI ethics and policy\namong post-secondary students studying computer science (CS) are of particular\ninterest, as many of these students will go on to play key roles in the\ndevelopment and deployment of future AI innovations. Despite this population of\ncomputer scientists being at the forefront of learning about and using AI\ntools, their attitudes towards AI remain understudied in the literature. In an\neffort to begin to close this gap, in fall 2024 we fielded a survey ($n=117$)\nto undergraduate and graduate students enrolled in CS courses at a large public\nuniversity in the United States to assess their attitudes towards the nascent\nfields of AI ethics and policy. Additionally, we conducted one-on-one follow-up\ninterviews with 13 students to elicit more in-depth responses on topics such as\nthe use of AI tools in the classroom, ethical impacts of AI, and government\nregulation of AI. In this paper, we describe the findings of both the survey\nand interviews, drawing parallels and contrasts to broader public opinion\npolling in the United States. We conclude by evaluating the implications of CS\nstudent attitudes on the future of AI education and governance.",
    "published_date": "2025-04-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.06296v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.04631v1",
    "title": "Systematic Literature Review on Vehicular Collaborative Perception -- A Computer Vision Perspective",
    "authors": [
      "Lei Wan",
      "Jianxin Zhao",
      "Andreas Wiedholz",
      "Manuel Bied",
      "Mateus Martinez de Lucena",
      "Abhishek Dinkar Jagtap",
      "Andreas Festag",
      "Antônio Augusto Fröhlich",
      "Hannan Ejaz Keen",
      "Alexey Vinel"
    ],
    "author_ids": [],
    "abstract": "The effectiveness of autonomous vehicles relies on reliable perception\ncapabilities. Despite significant advancements in artificial intelligence and\nsensor fusion technologies, current single-vehicle perception systems continue\nto encounter limitations, notably visual occlusions and limited long-range\ndetection capabilities. Collaborative Perception (CP), enabled by\nVehicle-to-Vehicle (V2V) and Vehicle-to-Infrastructure (V2I) communication, has\nemerged as a promising solution to mitigate these issues and enhance the\nreliability of autonomous systems. Beyond advancements in communication, the\ncomputer vision community is increasingly focusing on improving vehicular\nperception through collaborative approaches. However, a systematic literature\nreview that thoroughly examines existing work and reduces subjective bias is\nstill lacking. Such a systematic approach helps identify research gaps,\nrecognize common trends across studies, and inform future research directions.\nIn response, this study follows the PRISMA 2020 guidelines and includes 106\npeer-reviewed articles. These publications are analyzed based on modalities,\ncollaboration schemes, and key perception tasks. Through a comparative\nanalysis, this review illustrates how different methods address practical\nissues such as pose errors, temporal latency, communication constraints, domain\nshifts, heterogeneity, and adversarial attacks. Furthermore, it critically\nexamines evaluation methodologies, highlighting a misalignment between current\nmetrics and CP's fundamental objectives. By delving into all relevant topics\nin-depth, this review offers valuable insights into challenges, opportunities,\nand risks, serving as a reference for advancing research in vehicular\ncollaborative perception.",
    "published_date": "2025-04-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.04631v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.07133v1",
    "title": "Can SGD Select Good Fishermen? Local Convergence under Self-Selection Biases and Beyond",
    "authors": [
      "Alkis Kalavasis",
      "Anay Mehrotra",
      "Felix Zhou"
    ],
    "author_ids": [],
    "abstract": "We revisit the problem of estimating $k$ linear regressors with\nself-selection bias in $d$ dimensions with the maximum selection criterion, as\nintroduced by Cherapanamjeri, Daskalakis, Ilyas, and Zampetakis [CDIZ23,\nSTOC'23]. Our main result is a $\\operatorname{poly}(d,k,1/\\varepsilon) +\n{k}^{O(k)}$ time algorithm for this problem, which yields an improvement in the\nrunning time of the algorithms of [CDIZ23] and [GM24, arXiv]. We achieve this\nby providing the first local convergence algorithm for self-selection, thus\nresolving the main open question of [CDIZ23].\n  To obtain this algorithm, we reduce self-selection to a seemingly unrelated\nstatistical problem called coarsening. Coarsening occurs when one does not\nobserve the exact value of the sample but only some set (a subset of the sample\nspace) that contains the exact value. Inference from coarse samples arises in\nvarious real-world applications due to rounding by humans and algorithms,\nlimited precision of instruments, and lag in multi-agent systems.\n  Our reduction to coarsening is intuitive and relies on the geometry of the\nself-selection problem, which enables us to bypass the limitations of previous\nanalytic approaches. To demonstrate its applicability, we provide a local\nconvergence algorithm for linear regression under another self-selection\ncriterion, which is related to second-price auction data. Further, we give the\nfirst polynomial time local convergence algorithm for coarse Gaussian mean\nestimation given samples generated from a convex partition. Previously, only a\nsample-efficient algorithm was known due to Fotakis, Kalavasis, Kontonis, and\nTzamos [FKKT21, COLT'21].",
    "published_date": "2025-04-06T00:00:00",
    "year": 2025,
    "categories": [
      "stat.ML",
      "cs.DS",
      "cs.LG",
      "math.ST",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.07133v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.04600v1",
    "title": "Capturing AI's Attention: Physics of Repetition, Hallucination, Bias and Beyond",
    "authors": [
      "Frank Yingjie Huo",
      "Neil F. Johnson"
    ],
    "author_ids": [],
    "abstract": "We derive a first-principles physics theory of the AI engine at the heart of\nLLMs' 'magic' (e.g. ChatGPT, Claude): the basic Attention head. The theory\nallows a quantitative analysis of outstanding AI challenges such as output\nrepetition, hallucination and harmful content, and bias (e.g. from training and\nfine-tuning). Its predictions are consistent with large-scale LLM outputs. Its\n2-body form suggests why LLMs work so well, but hints that a generalized 3-body\nAttention would make such AI work even better. Its similarity to a spin-bath\nmeans that existing Physics expertise could immediately be harnessed to help\nSociety ensure AI is trustworthy and resilient to manipulation.",
    "published_date": "2025-04-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cond-mat.other",
      "math-ph",
      "math.MP",
      "nlin.AO",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.04600v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.04566v1",
    "title": "DyCON: Dynamic Uncertainty-aware Consistency and Contrastive Learning for Semi-supervised Medical Image Segmentation",
    "authors": [
      "Maregu Assefa",
      "Muzammal Naseer",
      "Iyyakutti Iyappan Ganapathi",
      "Syed Sadaf Ali",
      "Mohamed L Seghier",
      "Naoufel Werghi"
    ],
    "author_ids": [],
    "abstract": "Semi-supervised learning in medical image segmentation leverages unlabeled\ndata to reduce annotation burdens through consistency learning. However,\ncurrent methods struggle with class imbalance and high uncertainty from\npathology variations, leading to inaccurate segmentation in 3D medical images.\nTo address these challenges, we present DyCON, a Dynamic Uncertainty-aware\nConsistency and Contrastive Learning framework that enhances the generalization\nof consistency methods with two complementary losses: Uncertainty-aware\nConsistency Loss (UnCL) and Focal Entropy-aware Contrastive Loss (FeCL). UnCL\nenforces global consistency by dynamically weighting the contribution of each\nvoxel to the consistency loss based on its uncertainty, preserving\nhigh-uncertainty regions instead of filtering them out. Initially, UnCL\nprioritizes learning from uncertain voxels with lower penalties, encouraging\nthe model to explore challenging regions. As training progress, the penalty\nshift towards confident voxels to refine predictions and ensure global\nconsistency. Meanwhile, FeCL enhances local feature discrimination in\nimbalanced regions by introducing dual focal mechanisms and adaptive confidence\nadjustments into the contrastive principle. These mechanisms jointly\nprioritizes hard positives and negatives while focusing on uncertain sample\npairs, effectively capturing subtle lesion variations under class imbalance.\nExtensive evaluations on four diverse medical image segmentation datasets\n(ISLES'22, BraTS'19, LA, Pancreas) show DyCON's superior performance against\nSOTA methods.",
    "published_date": "2025-04-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.04566v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.04509v1",
    "title": "Truncated Huber Penalty for Sparse Signal Recovery with Convergence Analysis",
    "authors": [
      "Li Yang",
      "Serena Morigi",
      "Michael K. Ng",
      "You-wei Wen"
    ],
    "author_ids": [],
    "abstract": "Sparse signal recovery from under-determined systems presents significant\nchallenges when using conventional L_0 and L_1 penalties, primarily due to\ncomputational complexity and estimation bias. This paper introduces a truncated\nHuber penalty, a non-convex metric that effectively bridges the gap between\nunbiased sparse recovery and differentiable optimization. The proposed penalty\napplies quadratic regularization to small entries while truncating large\nmagnitudes, avoiding non-differentiable points at optima. Theoretical analysis\ndemonstrates that, for an appropriately chosen threshold, any s-sparse solution\nrecoverable via conventional penalties remains a local optimum under the\ntruncated Huber function. This property allows the exact and robust recovery\ntheories developed for other penalty regularization functions to be directly\nextended to the truncated Huber function. To solve the optimization problem, we\ndevelop a block coordinate descent (BCD) algorithm with finite-step convergence\nguarantees under spark conditions. Numerical experiments are conducted to\nvalidate the effectiveness and robustness of the proposed approach.\nFurthermore, we extend the truncated Huber-penalized model to the gradient\ndomain, illustrating its applicability in signal denoising and image smoothing.",
    "published_date": "2025-04-06T00:00:00",
    "year": 2025,
    "categories": [
      "math.NA",
      "cs.NA",
      "90C26, 90C90, 65K10, 49N45,"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.04509v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.04494v1",
    "title": "Skin Color Measurement from Dermatoscopic Images: An Evaluation on a Synthetic Dataset",
    "authors": [
      "Marin Benčević",
      "Robert Šojo",
      "Irena Galić"
    ],
    "author_ids": [],
    "abstract": "This paper presents a comprehensive evaluation of skin color measurement\nmethods from dermatoscopic images using a synthetic dataset (S-SYNTH) with\ncontrolled ground-truth melanin content, lesion shapes, hair models, and 18\ndistinct lighting conditions. This allows for rigorous assessment of the\nrobustness and invariance to lighting conditions. We assess four classes of\nimage colorimetry approaches: segmentation-based, patch-based, color\nquantization, and neural networks. We use these methods to estimate the\nIndividual Typology Angle (ITA) and Fitzpatrick types from dermatoscopic\nimages. Our results show that segmentation-based and color quantization methods\nyield robust, lighting-invariant estimates, whereas patch-based approaches\nexhibit significant lighting-dependent biases that require calibration.\nFurthermore, neural network models, particularly when combined with heavy\nblurring to reduce overfitting, can provide light-invariant Fitzpatrick\npredictions, although their generalization to real-world images remains\nunverified. We conclude with practical recommendations for designing fair and\nreliable skin color estimation methods.",
    "published_date": "2025-04-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.04494v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.04440v1",
    "title": "Do We Need Responsible XR? Drawing on Responsible AI to Inform Ethical Research and Practice into XRAI / the Metaverse",
    "authors": [
      "Mark McGill",
      "Joseph O'Hagan",
      "Thomas Goodge",
      "Graham Wilson",
      "Mohamed Khamis",
      "Veronika Krauß",
      "Jan Gugenheimer"
    ],
    "author_ids": [],
    "abstract": "This position paper for the CHI 2025 workshop \"Everyday AR through\nAI-in-the-Loop\" reflects on whether as a field HCI needs to define Responsible\nXR as a parallel to, and in conjunction with, Responsible AI, addressing the\nunique vulnerabilities posed by mass adoption of wearable AI-enabled AR glasses\nand XR devices that could enact AI-driven human perceptual augmentation.",
    "published_date": "2025-04-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.04440v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.04428v1",
    "title": "Formula-Supervised Sound Event Detection: Pre-Training Without Real Data",
    "authors": [
      "Yuto Shibata",
      "Keitaro Tanaka",
      "Yoshiaki Bando",
      "Keisuke Imoto",
      "Hirokatsu Kataoka",
      "Yoshimitsu Aoki"
    ],
    "author_ids": [],
    "abstract": "In this paper, we propose a novel formula-driven supervised learning (FDSL)\nframework for pre-training an environmental sound analysis model by leveraging\nacoustic signals parametrically synthesized through formula-driven methods.\nSpecifically, we outline detailed procedures and evaluate their effectiveness\nfor sound event detection (SED). The SED task, which involves estimating the\ntypes and timings of sound events, is particularly challenged by the difficulty\nof acquiring a sufficient quantity of accurately labeled training data.\nMoreover, it is well known that manually annotated labels often contain noises\nand are significantly influenced by the subjective judgment of annotators. To\naddress these challenges, we propose a novel pre-training method that utilizes\na synthetic dataset, Formula-SED, where acoustic data are generated solely\nbased on mathematical formulas. The proposed method enables large-scale\npre-training by using the synthesis parameters applied at each time step as\nground truth labels, thereby eliminating label noise and bias. We demonstrate\nthat large-scale pre-training with Formula-SED significantly enhances model\naccuracy and accelerates training, as evidenced by our results in the DESED\ndataset used for DCASE2023 Challenge Task 4. The project page is at\nhttps://yutoshibata07.github.io/Formula-SED/",
    "published_date": "2025-04-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.04428v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.13896v1",
    "title": "Educational Twin: The Influence of Artificial XR Expert Duplicates on Future Learning",
    "authors": [
      "Clara Sayffaerth"
    ],
    "author_ids": [],
    "abstract": "Currently, it is impossible for educators to be in multiple places\nsimultaneously and teach each student individually. Technologies such as\nExtended Reality (XR) and Artificial Intelligence (AI) enable the creation of\nrealistic educational copies of experts that preserve not only visual and\nmental characteristics but also social aspects crucial for learning. However,\nresearch in this area is limited, which opens new questions for future work.\nThis paper discusses how these human digital twins can potentially improve\naspects like scalability, engagement, and preservation of social learning\nfactors. While this technology offers benefits, it also introduces challenges\nrelated to educator autonomy, social interaction shifts, and ethical\nconsiderations such as privacy, bias, and identity preservation. We outline key\nresearch questions that need to be addressed to ensure that human digital twins\nenhance the social aspects of education instead of harming them.",
    "published_date": "2025-04-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.13896v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.04299v1",
    "title": "AI-induced sexual harassment: Investigating Contextual Characteristics and User Reactions of Sexual Harassment by a Companion Chatbot",
    "authors": [
      "Mohammad",
      "Namvarpour",
      "Harrison Pauwels",
      "Afsaneh Razi"
    ],
    "author_ids": [],
    "abstract": "Advancements in artificial intelligence (AI) have led to the increase of\nconversational agents like Replika, designed to provide social interaction and\nemotional support. However, reports of these AI systems engaging in\ninappropriate sexual behaviors with users have raised significant concerns. In\nthis study, we conducted a thematic analysis of user reviews from the Google\nPlay Store to investigate instances of sexual harassment by the Replika\nchatbot. From a dataset of 35,105 negative reviews, we identified 800 relevant\ncases for analysis. Our findings revealed that users frequently experience\nunsolicited sexual advances, persistent inappropriate behavior, and failures of\nthe chatbot to respect user boundaries. Users expressed feelings of discomfort,\nviolation of privacy, and disappointment, particularly when seeking a platonic\nor therapeutic AI companion. This study highlights the potential harms\nassociated with AI companions and underscores the need for developers to\nimplement effective safeguards and ethical guidelines to prevent such\nincidents. By shedding light on user experiences of AI-induced harassment, we\ncontribute to the understanding of AI-related risks and emphasize the\nimportance of corporate responsibility in developing safer and more ethical AI\nsystems.",
    "published_date": "2025-04-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.AI",
      "H.5; I.2.7; K.4.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.04299v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.04279v2",
    "title": "Could AI Trace and Explain the Origins of AI-Generated Images and Text?",
    "authors": [
      "Hongchao Fang",
      "Yixin Liu",
      "Jiangshu Du",
      "Can Qin",
      "Ran Xu",
      "Feng Liu",
      "Lichao Sun",
      "Dongwon Lee",
      "Lifu Huang",
      "Wenpeng Yin"
    ],
    "author_ids": [],
    "abstract": "AI-generated content is becoming increasingly prevalent in the real world,\nleading to serious ethical and societal concerns. For instance, adversaries\nmight exploit large multimodal models (LMMs) to create images that violate\nethical or legal standards, while paper reviewers may misuse large language\nmodels (LLMs) to generate reviews without genuine intellectual effort. While\nprior work has explored detecting AI-generated images and texts, and\noccasionally tracing their source models, there is a lack of a systematic and\nfine-grained comparative study. Important dimensions--such as AI-generated\nimages vs. text, fully vs. partially AI-generated images, and general vs.\nmalicious use cases--remain underexplored. Furthermore, whether AI systems like\nGPT-4o can explain why certain forged content is attributed to specific\ngenerative models is still an open question, with no existing benchmark\naddressing this. To fill this gap, we introduce AI-FAKER, a comprehensive\nmultimodal dataset with over 280,000 samples spanning multiple LLMs and LMMs,\ncovering both general and malicious use cases for AI-generated images and\ntexts. Our experiments reveal two key findings: (i) AI authorship detection\ndepends not only on the generated output but also on the model's original\ntraining intent; and (ii) GPT-4o provides highly consistent but less specific\nexplanations when analyzing content produced by OpenAI's own models, such as\nDALL-E and GPT-4o itself.",
    "published_date": "2025-04-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.04279v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.04274v1",
    "title": "Randomised Splitting Methods and Stochastic Gradient Descent",
    "authors": [
      "Luke Shaw",
      "Peter A. Whalley"
    ],
    "author_ids": [],
    "abstract": "We explore an explicit link between stochastic gradient descent using common\nbatching strategies and splitting methods for ordinary differential equations.\nFrom this perspective, we introduce a new minibatching strategy (called\nSymmetric Minibatching Strategy) for stochastic gradient optimisation which\nshows greatly reduced stochastic gradient bias (from $\\mathcal{O}(h^2)$ to\n$\\mathcal{O}(h^4)$ in the optimiser stepsize $h$), when combined with\nmomentum-based optimisers. We justify why momentum is needed to obtain the\nimproved performance using the theory of backward analysis for splitting\nintegrators and provide a detailed analytic computation of the stochastic\ngradient bias on a simple example.\n  Further, we provide improved convergence guarantees for this new minibatching\nstrategy using Lyapunov techniques that show reduced stochastic gradient bias\nfor a fixed stepsize (or learning rate) over the class of strongly-convex and\nsmooth objective functions. Via the same techniques we also improve the known\nresults for the Random Reshuffling strategy for stochastic gradient descent\nmethods with momentum. We argue that this also leads to a faster convergence\nrate when considering a decreasing stepsize schedule. Both the reduced bias and\nefficacy of decreasing stepsizes are demonstrated numerically on several\nmotivating examples.",
    "published_date": "2025-04-05T00:00:00",
    "year": 2025,
    "categories": [
      "math.OC",
      "cs.NA",
      "math.NA",
      "stat.ML",
      "65L20, 90C25, 93C15"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.04274v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.04199v1",
    "title": "Investigating and Mitigating Stereotype-aware Unfairness in LLM-based Recommendations",
    "authors": [
      "Zihuai Zhao",
      "Wenqi Fan",
      "Yao Wu",
      "Qing Li"
    ],
    "author_ids": [],
    "abstract": "Large Language Models (LLMs) have demonstrated unprecedented language\nunderstanding and reasoning capabilities to capture diverse user preferences\nand advance personalized recommendations. Despite the growing interest in\nLLM-based personalized recommendations, unique challenges are brought to the\ntrustworthiness of LLM-based recommender systems (LLM-RS), since LLMs are\nlikely to inherit stereotypes that are embedded ubiquitously in word embeddings\ndue to their training on large-scale uncurated datasets. This leads to LLM-RS\nexhibiting stereotypical linguistic associations between users and items.\nHowever, there remains a lack of studies investigating the simultaneous\nexistence of stereotypes between users and items in LLM-RS. To bridge this gap,\nthis study reveals a new variant of fairness between stereotype groups\ncontaining both users and items, to quantify discrimination against stereotypes\nin LLM-RS. Moreover, in this paper, to mitigate stereotype-aware unfairness in\ntextual user and item information, we propose a novel framework (MoS), in which\nan insightful stereotype-wise routing strategy over multiple\nstereotype-relevant experts is designed to learn unbiased representations\nagainst different stereotypes in LLM- RS. Extensive experiments are conducted\nto analyze the influence of stereotype-aware fairness in LLM-RS and the\neffectiveness of our proposed methods, which consistently outperform\ncompetitive benchmarks under various fairness settings.",
    "published_date": "2025-04-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.04199v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.04141v2",
    "title": "Cognitive Debiasing Large Language Models for Decision-Making",
    "authors": [
      "Yougang Lyu",
      "Shijie Ren",
      "Yue Feng",
      "Zihan Wang",
      "Zhumin Chen",
      "Zhaochun Ren",
      "Maarten de Rijke"
    ],
    "author_ids": [],
    "abstract": "Large language models (LLMs) have shown potential in supporting\ndecision-making applications, particularly as personal conversational\nassistants in the financial, healthcare, and legal domains. While prompt\nengineering strategies have enhanced the capabilities of LLMs in\ndecision-making, cognitive biases inherent to LLMs present significant\nchallenges. Cognitive biases are systematic patterns of deviation from norms or\nrationality in decision-making that can lead to the production of inaccurate\noutputs. Existing cognitive bias mitigation strategies assume that input\nprompts contain (exactly) one type of cognitive bias and therefore fail to\nperform well in realistic settings where there maybe any number of biases.\n  To fill this gap, we propose a cognitive debiasing approach, called\nself-debiasing, that enhances the reliability of LLMs by iteratively refining\nprompts. Our method follows three sequential steps -- bias determination, bias\nanalysis, and cognitive debiasing -- to iteratively mitigate potential\ncognitive biases in prompts. Experimental results on finance, healthcare, and\nlegal decision-making tasks, using both closed-source and open-source LLMs,\ndemonstrate that the proposed self-debiasing method outperforms both advanced\nprompt engineering methods and existing cognitive debiasing techniques in\naverage accuracy under no-bias, single-bias, and multi-bias settings.",
    "published_date": "2025-04-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.04141v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.04055v1",
    "title": "Learning-Based Multi-Criteria Decision Model for Site Selection Problems",
    "authors": [
      "Mahid Ahmed",
      "Ali Dogru",
      "Chaoyang Zhang",
      "Chao Meng"
    ],
    "author_ids": [],
    "abstract": "Strategically locating sawmills is critical for the efficiency,\nprofitability, and sustainability of timber supply chains, yet it involves a\nseries of complex decision-making affected by various factors, such as\nproximity to resources and markets, proximity to roads and rail lines, distance\nfrom the urban area, slope, labor market, and existing sawmill data. Although\nconventional Multi-Criteria Decision-Making (MCDM) approaches utilize these\nfactors while locating facilities, they are susceptible to bias since they rely\nheavily on expert opinions to determine the relative factor weights. Machine\nlearning (ML) models provide an objective, data-driven alternative for site\nselection that derives these weights directly from the patterns in large\ndatasets without requiring subjective weighting. Additionally, ML models\nautonomously identify critical features, eliminating the need for subjective\nfeature selection. In this study, we propose integrated ML and MCDM methods and\nshowcase the utility of this integrated model to improve sawmill location\ndecisions via a case study in Mississippi. This integrated model is flexible\nand applicable to site selection problems across various industries.",
    "published_date": "2025-04-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.04055v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.03991v1",
    "title": "Algorithmic Prompt Generation for Diverse Human-like Teaming and Communication with Large Language Models",
    "authors": [
      "Siddharth Srikanth",
      "Varun Bhatt",
      "Boshen Zhang",
      "Werner Hager",
      "Charles Michael Lewis",
      "Katia P. Sycara",
      "Aaquib Tabrez",
      "Stefanos Nikolaidis"
    ],
    "author_ids": [],
    "abstract": "Understanding how humans collaborate and communicate in teams is essential\nfor improving human-agent teaming and AI-assisted decision-making. However,\nrelying solely on data from large-scale user studies is impractical due to\nlogistical, ethical, and practical constraints, necessitating synthetic models\nof multiple diverse human behaviors. Recently, agents powered by Large Language\nModels (LLMs) have been shown to emulate human-like behavior in social\nsettings. But, obtaining a large set of diverse behaviors requires manual\neffort in the form of designing prompts. On the other hand, Quality Diversity\n(QD) optimization has been shown to be capable of generating diverse\nReinforcement Learning (RL) agent behavior. In this work, we combine QD\noptimization with LLM-powered agents to iteratively search for prompts that\ngenerate diverse team behavior in a long-horizon, multi-step collaborative\nenvironment. We first show, through a human-subjects experiment (n=54\nparticipants), that humans exhibit diverse coordination and communication\nbehavior in this domain. We then show that our approach can effectively\nreplicate trends from human teaming data and also capture behaviors that are\nnot easily observed without collecting large amounts of data. Our findings\nhighlight the combination of QD and LLM-powered agents as an effective tool for\nstudying teaming and communication strategies in multi-agent collaboration.",
    "published_date": "2025-04-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.03991v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.07982v1",
    "title": "Metamorphic Testing for Fairness Evaluation in Large Language Models: Identifying Intersectional Bias in LLaMA and GPT",
    "authors": [
      "Harishwar Reddy",
      "Madhusudan Srinivasan",
      "Upulee Kanewala"
    ],
    "author_ids": [],
    "abstract": "Large Language Models (LLMs) have made significant strides in Natural\nLanguage Processing but remain vulnerable to fairness-related issues, often\nreflecting biases inherent in their training data. These biases pose risks,\nparticularly when LLMs are deployed in sensitive areas such as healthcare,\nfinance, and law. This paper introduces a metamorphic testing approach to\nsystematically identify fairness bugs in LLMs. We define and apply a set of\nfairness-oriented metamorphic relations (MRs) to assess the LLaMA and GPT\nmodel, a state-of-the-art LLM, across diverse demographic inputs. Our\nmethodology includes generating source and follow-up test cases for each MR and\nanalyzing model responses for fairness violations. The results demonstrate the\neffectiveness of MT in exposing bias patterns, especially in relation to tone\nand sentiment, and highlight specific intersections of sensitive attributes\nthat frequently reveal fairness faults. This research improves fairness testing\nin LLMs, providing a structured approach to detect and mitigate biases and\nimprove model robustness in fairness-sensitive applications.",
    "published_date": "2025-04-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.07982v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.03540v1",
    "title": "The Limits of \"Fairness\" of the Variational Generalized Nash Equilibrium",
    "authors": [
      "Sophie Hall",
      "Florian Dörfler",
      "Heinrich H. Nax",
      "Saverio Bolognani"
    ],
    "author_ids": [],
    "abstract": "Generalized Nash equilibrum (GNE) problems are commonly used to model\nstrategic interactions between self-interested agents who are coupled in cost\nand constraints. Specifically, the variational GNE, a refinement of the GNE, is\noften selected as the solution concept due to it's non-discriminatory treatment\nof agents by charging a uniform ``shadow price\" for shared resources. We study\nthe fairness concept of v-GNEs from a comparability perspective and show that\nit makes an implicit assumption of unit comparability of agent's cost\nfunctions, one of the strongest comparability notions. Further, we introduce a\nnew solution concept, f-GNE in which a fairness metric is chosen a priori which\nis compatible with the comparability at hand. We introduce an electric vehicle\ncharging game to demonstrate the fragility of v-GNE fairness and compare it to\nthe f-GNE under various fairness metrics.",
    "published_date": "2025-04-04T00:00:00",
    "year": 2025,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.03540v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.03520v1",
    "title": "Neutralizing the Narrative: AI-Powered Debiasing of Online News Articles",
    "authors": [
      "Chen Wei Kuo",
      "Kevin Chu",
      "Nouar AlDahoul",
      "Hazem Ibrahim",
      "Talal Rahwan",
      "Yasir Zaki"
    ],
    "author_ids": [],
    "abstract": "Bias in news reporting significantly impacts public perception, particularly\nregarding crime, politics, and societal issues. Traditional bias detection\nmethods, predominantly reliant on human moderation, suffer from subjective\ninterpretations and scalability constraints. Here, we introduce an AI-driven\nframework leveraging advanced large language models (LLMs), specifically\nGPT-4o, GPT-4o Mini, Gemini Pro, Gemini Flash, Llama 8B, and Llama 3B, to\nsystematically identify and mitigate biases in news articles. To this end, we\ncollect an extensive dataset consisting of over 30,000 crime-related articles\nfrom five politically diverse news sources spanning a decade (2013-2023). Our\napproach employs a two-stage methodology: (1) bias detection, where each LLM\nscores and justifies biased content at the paragraph level, validated through\nhuman evaluation for ground truth establishment, and (2) iterative debiasing\nusing GPT-4o Mini, verified by both automated reassessment and human reviewers.\nEmpirical results indicate GPT-4o Mini's superior accuracy in bias detection\nand effectiveness in debiasing. Furthermore, our analysis reveals temporal and\ngeographical variations in media bias correlating with socio-political dynamics\nand real-world events. This study contributes to scalable computational\nmethodologies for bias mitigation, promoting fairness and accountability in\nnews reporting.",
    "published_date": "2025-04-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.03520v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.03814v2",
    "title": "Recursive Training Loops in LLMs: How training data properties modulate distribution shift in generated data?",
    "authors": [
      "Grgur Kovač",
      "Jérémy Perez",
      "Rémy Portelas",
      "Peter Ford Dominey",
      "Pierre-Yves Oudeyer"
    ],
    "author_ids": [],
    "abstract": "Large language models (LLMs) are increasingly contributing to the creation of\ncontent on the Internet. This creates a feedback loop as subsequent generations\nof models will be trained on this generated, synthetic data. This phenomenon is\nreceiving increasing interest, in particular because previous studies have\nshown that it may lead to distribution shift - models misrepresent and forget\nthe true underlying distributions of human data they are expected to\napproximate (e.g. resulting in a drastic loss of quality). In this study, we\nstudy the impact of human data properties on distribution shift dynamics in\niterated training loops. We first confirm that the distribution shift dynamics\ngreatly vary depending on the human data by comparing four datasets (two based\non Twitter and two on Reddit). We then test whether data quality may influence\nthe rate of this shift. We find that it does on the twitter, but not on the\nReddit datasets. We then focus on a Reddit dataset and conduct a more\nexhaustive evaluation of a large set of dataset properties. This experiment\nassociated lexical diversity with larger, and semantic diversity with smaller\ndetrimental shifts, suggesting that incorporating text with high lexical (but\nlimited semantic) diversity could exacerbate the degradation of generated text.\nWe then focus on the evolution of political bias, and find that the type of\nshift observed (bias reduction, amplification or inversion) depends on the\npolitical lean of the human (true) distribution. Overall, our work extends the\nexisting literature on the consequences of recursive fine-tuning by showing\nthat this phenomenon is highly dependent on features of the human data on which\ntraining occurs. This suggests that different parts of internet (e.g. GitHub,\nReddit) may undergo different types of shift depending on their properties.",
    "published_date": "2025-04-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "68T50",
      "I.2.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.03814v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.03476v1",
    "title": "ATM-Net: Anatomy-Aware Text-Guided Multi-Modal Fusion for Fine-Grained Lumbar Spine Segmentation",
    "authors": [
      "Sheng Lian",
      "Dengfeng Pan",
      "Jianlong Cai",
      "Guang-Yong Chen",
      "Zhun Zhong",
      "Zhiming Luo",
      "Shen Zhao",
      "Shuo Li"
    ],
    "author_ids": [],
    "abstract": "Accurate lumbar spine segmentation is crucial for diagnosing spinal\ndisorders. Existing methods typically use coarse-grained segmentation\nstrategies that lack the fine detail needed for precise diagnosis.\nAdditionally, their reliance on visual-only models hinders the capture of\nanatomical semantics, leading to misclassified categories and poor segmentation\ndetails. To address these limitations, we present ATM-Net, an innovative\nframework that employs an anatomy-aware, text-guided, multi-modal fusion\nmechanism for fine-grained segmentation of lumbar substructures, i.e.,\nvertebrae (VBs), intervertebral discs (IDs), and spinal canal (SC). ATM-Net\nadopts the Anatomy-aware Text Prompt Generator (ATPG) to adaptively convert\nimage annotations into anatomy-aware prompts in different views. These insights\nare further integrated with image features via the Holistic Anatomy-aware\nSemantic Fusion (HASF) module, building a comprehensive anatomical context. The\nChannel-wise Contrastive Anatomy-Aware Enhancement (CCAE) module further\nenhances class discrimination and refines segmentation through class-wise\nchannel-level multi-modal contrastive learning. Extensive experiments on the\nMRSpineSeg and SPIDER datasets demonstrate that ATM-Net significantly\noutperforms state-of-the-art methods, with consistent improvements regarding\nclass discrimination and segmentation details. For example, ATM-Net achieves\nDice of 79.39% and HD95 of 9.91 pixels on SPIDER, outperforming the competitive\nSpineParseNet by 8.31% and 4.14 pixels, respectively.",
    "published_date": "2025-04-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.03476v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.03432v1",
    "title": "A Polynomial-Time Algorithm for Variational Inequalities under the Minty Condition",
    "authors": [
      "Ioannis Anagnostides",
      "Gabriele Farina",
      "Tuomas Sandholm",
      "Brian Hu Zhang"
    ],
    "author_ids": [],
    "abstract": "Solving (Stampacchia) variational inequalities (SVIs) is a foundational\nproblem at the heart of optimization, with a host of critical applications\nranging from engineering to economics. However, this expressivity comes at the\ncost of computational hardness. As a result, most research has focused on\ncarving out specific subclasses that elude those intractability barriers. A\nclassical property that goes back to the 1960s is the Minty condition, which\npostulates that the Minty VI (MVI) problem -- the weak dual of the SVI problem\n-- admits a solution.\n  In this paper, we establish the first polynomial-time algorithm -- that is,\nwith complexity growing polynomially in the dimension $d$ and\n$\\log(1/\\epsilon)$ -- for solving $\\epsilon$-SVIs for Lipschitz continuous\nmappings under the Minty condition. Prior approaches either incurred an\nexponentially worse dependence on $1/\\epsilon$ (and other natural parameters of\nthe problem) or made overly restrictive assumptions -- such as strong\nmonotonicity. To do so, we introduce a new variant of the ellipsoid algorithm\nwherein separating hyperplanes are obtained after taking a gradient descent\nstep from the center of the ellipsoid. It succeeds even though the set of SVIs\ncan be nonconvex and not fully dimensional. Moreover, when our algorithm is\napplied to an instance with no MVI solution and fails to identify an SVI\nsolution, it produces a succinct certificate of MVI infeasibility. We also show\nthat deciding whether the Minty condition holds is $\\mathsf{coNP}$-complete.\n  We provide several extensions and new applications of our main results.\nSpecifically, we obtain the first polynomial-time algorithms for i) solving\nmonotone VIs, ii) globally minimizing a (potentially nonsmooth) quasar-convex\nfunction, and iii) computing Nash equilibria in multi-player harmonic games.",
    "published_date": "2025-04-04T00:00:00",
    "year": 2025,
    "categories": [
      "math.OC",
      "cs.GT",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.03432v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.03393v1",
    "title": "Effects of Interpolation Error and Bias on the Random Mesh Finite Element Method for Inverse Problems",
    "authors": [
      "Anne Poot",
      "Iuri Rocha",
      "Pierre Kerfriden",
      "Frans van der Meer"
    ],
    "author_ids": [],
    "abstract": "Bayesian inverse problems are an important application for probabilistic\nsolvers of partial differential equations: when fully resolving numerical error\nis computationally infeasible, probabilistic solvers can be used to\nconsistently model the error and propagate it to the posterior. In this work,\nthe performance of the random mesh finite element method (RM-FEM) is\ninvestigated in a Bayesian inverse setting. We show how interpolation error\nnegatively affects the RM-FEM posterior, and how these negative effects can be\ndiminished. In scenarios where FEM is biased for a quantity of interest, we\nfind that RM-FEM struggles to accurately model this bias.",
    "published_date": "2025-04-04T00:00:00",
    "year": 2025,
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.03393v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.03361v1",
    "title": "Fairness vs. Equality: RSMA-Based Multi-Target and Multi-User Integrated Sensing and Communications",
    "authors": [
      "Xudong Li",
      "Rugui Yao",
      "Alexandros-Apostolos A. Boulogeorgos",
      "Theodoros A. Tsiftsis"
    ],
    "author_ids": [],
    "abstract": "This paper investigates the tradeoff between sensing and communication in an\nISAC system comprising multiple sensing targets and communication users. A\ndual-functional base station conducts downlink data transmission services based\non RSMA for multiple users, while sensing surrounding multiple targets. To\nenable effective multicast communications and ensure fair and balanced\nmulti-target sensing and under a constrained power budget, we propose a\nmulti-target sensing enhancement scheme incorporating fairness-aware BF, common\nrate splitting, and sensing power allocation. The proposed scheme minimizes the\nsensing CRB, while maximizing communication rate demands. Specifically, we\nderive closed-form expressions for both sensing CRB and communication rates.\nBuilding upon them, we formulate an optimization problem aiming to minimize the\nsensing CRB, while maximizing the communication rates. Considering the\nnon-convex nature of the original optimization problem poses significant\ncomputational challenges, we transform the tradeoff optimization into a\nPareto-optimal problem by employing Taylor series expansion, semi-definite\nrelaxation, successive convex approximation, and penalty function to transform\nthe non-convex problem and associated constraints into tractable forms.\nExtensive simulations validate the theoretical analysis and demonstrate\nsignificant advantages of the proposed RSMA-based fairness-aware BF over\nnon-orthogonal multiple access, space division multiple access, and orthogonal\nmultiple access, through comprehensive comparisons in two key aspects: CRB\nperformance improvement and sensing-communication tradeoff characteristics. The\nproposed optimization framework exhibits remarkable superiority in enhancing\nboth sensing accuracy and communication quality for ISAC systems.",
    "published_date": "2025-04-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.03361v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.03352v1",
    "title": "Detecting Stereotypes and Anti-stereotypes the Correct Way Using Social Psychological Underpinnings",
    "authors": [
      "Kaustubh Shivshankar Shejole",
      "Pushpak Bhattacharyya"
    ],
    "author_ids": [],
    "abstract": "Stereotypes are known to be highly pernicious, making their detection\ncritically important. However, current research predominantly focuses on\ndetecting and evaluating stereotypical biases in LLMs, leaving the study of\nstereotypes in its early stages. Many studies have failed to clearly\ndistinguish between stereotypes and stereotypical biases, which has\nsignificantly slowed progress in advancing research in this area. Stereotype\nand anti-stereotype detection is a problem that requires knowledge of society;\nhence, it is one of the most difficult areas in Responsible AI. This work\ninvestigates this task, where we propose a four-tuple definition and provide\nprecise terminology distinguishing stereotype, anti-stereotype, stereotypical\nbias, and bias, offering valuable insights into their various aspects. In this\npaper, we propose StereoDetect, a high-quality benchmarking dataset curated for\nthis task by optimally utilizing current datasets such as StereoSet and\nWinoQueer, involving a manual verification process and the transfer of semantic\ninformation. We demonstrate that language models for reasoning with fewer than\n10B parameters often get confused when detecting anti-stereotypes. We also\ndemonstrate the critical importance of well-curated datasets by comparing our\nmodel with other current models for stereotype detection. The dataset and code\nis available at https://github.com/KaustubhShejole/StereoDetect.",
    "published_date": "2025-04-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.03352v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.03336v1",
    "title": "Ethics Readiness of Technology: The case for aligning ethical approaches with technological maturity",
    "authors": [
      "Eline de Jong"
    ],
    "author_ids": [],
    "abstract": "The ethics of emerging technologies faces an anticipation dilemma: engaging\ntoo early risks overly speculative concerns, while engaging too late may\nforfeit the chance to shape a technology's trajectory. Despite various methods\nto address this challenge, no framework exists to assess their suitability\nacross different stages of technological development. This paper proposes such\na framework. I conceptualise two main ethical approaches: outcomes-oriented\nethics, which assesses the potential consequences of a technology's\nmaterialisation, and meaning-oriented ethics, which examines how (social)\nmeaning is attributed to a technology. I argue that the strengths and\nlimitations of outcomes- and meaning-oriented ethics depend on the\nuncertainties surrounding a technology, which shift as it matures. To capture\nthis evolution, I introduce the concept of ethics readiness: the readiness of a\ntechnology to undergo detailed ethical scrutiny. Building on the widely known\nTechnology Readiness Levels (TRLs), I propose Ethics Readiness Levels (ERLs) to\nillustrate how the suitability of ethical approaches evolves with a\ntechnology's development. At lower ERLs, where uncertainties are most\npronounced, meaning-oriented ethics proves more effective, while at higher\nERLs, as impacts become clearer, outcomes-oriented ethics gains relevance. By\nlinking Ethics Readiness to Technology Readiness, this framework underscores\nthat the appropriateness of ethical approaches evolves alongside technological\nmaturity, ensuring scrutiny remains grounded and relevant. Finally, I\ndemonstrate the practical value of this framework by applying it to quantum\ntechnologies, showing how Ethics Readiness can guide effective ethical\nengagement.",
    "published_date": "2025-04-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.03336v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.03093v1",
    "title": "Post-processing for Fair Regression via Explainable SVD",
    "authors": [
      "Zhiqun Zuo",
      "Ding Zhu",
      "Mohammad Mahdi Khalili"
    ],
    "author_ids": [],
    "abstract": "This paper presents a post-processing algorithm for training fair neural\nnetwork regression models that satisfy statistical parity, utilizing an\nexplainable singular value decomposition (SVD) of the weight matrix. We propose\na linear transformation of the weight matrix, whereby the singular values\nderived from the SVD of the transformed matrix directly correspond to the\ndifferences in the first and second moments of the output distributions across\ntwo groups. Consequently, we can convert the fairness constraints into\nconstraints on the singular values. We analytically solve the problem of\nfinding the optimal weights under these constraints. Experimental validation on\nvarious datasets demonstrates that our method achieves a similar or superior\nfairness-accuracy trade-off compared to the baselines without using the\nsensitive attribute at the inference time.",
    "published_date": "2025-04-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.03093v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.03029v1",
    "title": "Ontologies in Design: How Imagining a Tree Reveals Possibilites and Assumptions in Large Language Models",
    "authors": [
      "Nava Haghighi",
      "Sunny Yu",
      "James Landay",
      "Daniela Rosner"
    ],
    "author_ids": [],
    "abstract": "Amid the recent uptake of Generative AI, sociotechnical scholars and critics\nhave traced a multitude of resulting harms, with analyses largely focused on\nvalues and axiology (e.g., bias). While value-based analyses are crucial, we\nargue that ontologies -- concerning what we allow ourselves to think or talk\nabout -- is a vital but under-recognized dimension in analyzing these systems.\nProposing a need for a practice-based engagement with ontologies, we offer four\norientations for considering ontologies in design: pluralism, groundedness,\nliveliness, and enactment. We share examples of potentialities that are opened\nup through these orientations across the entire LLM development pipeline by\nconducting two ontological analyses: examining the responses of four LLM-based\nchatbots in a prompting exercise, and analyzing the architecture of an\nLLM-based agent simulation. We conclude by sharing opportunities and\nlimitations of working with ontologies in the design and development of\nsociotechnical systems.",
    "published_date": "2025-04-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.03029v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.02963v1",
    "title": "Digital Forensics in the Age of Large Language Models",
    "authors": [
      "Zhipeng Yin",
      "Zichong Wang",
      "Weifeng Xu",
      "Jun Zhuang",
      "Pallab Mozumder",
      "Antoinette Smith",
      "Wenbin Zhang"
    ],
    "author_ids": [],
    "abstract": "Digital forensics plays a pivotal role in modern investigative processes,\nutilizing specialized methods to systematically collect, analyze, and interpret\ndigital evidence for judicial proceedings. However, traditional digital\nforensic techniques are primarily based on manual labor-intensive processes,\nwhich become increasingly insufficient with the rapid growth and complexity of\ndigital data. To this end, Large Language Models (LLMs) have emerged as\npowerful tools capable of automating and enhancing various digital forensic\ntasks, significantly transforming the field. Despite the strides made, general\npractitioners and forensic experts often lack a comprehensive understanding of\nthe capabilities, principles, and limitations of LLM, which limits the full\npotential of LLM in forensic applications. To fill this gap, this paper aims to\nprovide an accessible and systematic overview of how LLM has revolutionized the\ndigital forensics approach. Specifically, it takes a look at the basic concepts\nof digital forensics, as well as the evolution of LLM, and emphasizes the\nsuperior capabilities of LLM. To connect theory and practice, relevant examples\nand real-world scenarios are discussed. We also critically analyze the current\nlimitations of applying LLMs to digital forensics, including issues related to\nillusion, interpretability, bias, and ethical considerations. In addition, this\npaper outlines the prospects for future research, highlighting the need for\neffective use of LLMs for transparency, accountability, and robust\nstandardization in the forensic process.",
    "published_date": "2025-04-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.02963v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.02769v1",
    "title": "Curbing the Ramifications of Authorship Abuse in Science",
    "authors": [
      "Md Somir Khan",
      "Mehmet Engin Tozal"
    ],
    "author_ids": [],
    "abstract": "Research performance is often measured using bibliometric indicators, such as\npublication count, total citations, and $h$-index. These metrics influence\ncareer advancements, salary adjustments, administrative opportunities, funding\nprospects, and professional recognition. However, the reliance on these metrics\nhas also made them targets for manipulation, misuse, and abuse. One primary\nethical concern is authorship abuse, which includes paid, ornamental,\nexploitative, and cartel authorships. These practices are prevalent because\nthey artificially enhance multiple bibliometric indicators all at once. Our\nstudy confirms a significant rise in the mean and median number of authors per\npublication across multiple disciplines over the last 34 years. While it is\nimportant to identify the cases of authorship abuse, a thorough investigation\nof every paper proves impractical. In this study, we propose a credit\nallocation scheme based on the reciprocals of the Fibonacci numbers, designed\nto adjust credit for individual contributions while systematically reducing\ncredit for potential authorship abuse. The proposed scheme aligns with rigorous\nauthorship guidelines from scientific associations, which mandate significant\ncontributions across most phases of a study, while accommodating more lenient\nguidelines from scientific publishers, which recognize authorship for minimal\ncontributions. We recalibrate the traditional bibliometric indicators to\nemphasize author contribution rather than participation in publications.\nAdditionally, we propose a new indicator, $T^{\\prime}$-index, to assess\nresearchers' leading and contributing roles in their publications. Our proposed\ncredit allocation scheme mitigates the effects of authorship abuse and promotes\na more ethical scientific ecosystem.",
    "published_date": "2025-04-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.DL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.02769v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.02708v1",
    "title": "The Hidden Space of Safety: Understanding Preference-Tuned LLMs in Multilingual context",
    "authors": [
      "Nikhil Verma",
      "Manasa Bharadwaj"
    ],
    "author_ids": [],
    "abstract": "Alignment tuning has enabled large language models to excel in reasoning,\ninstruction-following, and minimizing harmful generations. However, despite\ntheir widespread deployment, these models exhibit a monolingual bias, raising\nconcerns about the effectiveness of alignment across languages. Current\nalignment methods predominantly focus on English, leaving it unclear how\nalignment mechanism generalize to multilingual settings. To address this, we\nconduct a systematic analysis of distributional shifts in the embedding space\nof LLMs before and after alignment, uncovering its impact on model behavior\nacross diverse languages. We leverage the alignment-induced separation in\nsafety space as a quantitative tool to measure how alignment enforces safety\nconstraints. Our study evaluates seven LLMs using balanced toxicity datasets\nand parallel text-detoxification benchmarks, revealing substantial disparities\nin the latent representation space between high-resource and low-resource\nlanguages. These findings underscore the need for language-specific fine-tuning\nto ensure fair, reliable and robust multilingual alignment. Our insights\nprovide a foundation for developing truly safe multilingual LLMs, emphasizing\nthe urgency of addressing alignment gaps in underrepresented languages.",
    "published_date": "2025-04-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.02708v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.03782v1",
    "title": "A Study on Adversarial Robustness of Discriminative Prototypical Learning",
    "authors": [
      "Ramin Zarei Sabzevar",
      "Hamed Mohammadzadeh",
      "Tahmineh Tavakoli",
      "Ahad Harati"
    ],
    "author_ids": [],
    "abstract": "Deep neural networks demonstrate significant vulnerability to adversarial\nperturbations, posing risks for critical applications. Current adversarial\ntraining methods predominantly focus on robustness against attacks without\nexplicitly leveraging geometric structures in the latent space, usually\nresulting in reduced accuracy on the original clean data. To address these\nissues, we propose a novel adversarial training framework named Adversarial\nDeep Positive-Negative Prototypes (Adv-DPNP), which integrates disriminative\nprototype-based learning with adversarial training. Adv-DPNP uses unified class\nprototypes serving dual roles as classifier weights and robust anchors,\nenhancing both intra-class compactness and inter-class separation in the latent\nspace. Moreover, a novel dual-branch training mechanism maintains stable\nprototypes by updating them exclusively with clean data; while the feature\nextractor layers are learned using both clean and adversarial data to remain\ninvariant against adversarial perturbations. In addition, our approach utilizes\na composite loss function combining positive prototype alignment, negative\nprototype repulsion, and consistency regularization to further enhance\ndiscrimination, adversarial robustness, and clean accuracy. Extensive\nexperiments conducted on standard benchmark datasets confirm the effectiveness\nof Adv-DPNP compared to state-of-the-art methods, achieving higher clean\naccuracy and competitive robustness under adversarial perturbations and common\ncorruptions. Our code is available at https://github.com/fum-rpl/adv-dpnp",
    "published_date": "2025-04-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.03782v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.02648v2",
    "title": "Controlled Social Learning: Altruism vs. Bias",
    "authors": [
      "Raghu Arghal",
      "Kevin He",
      "Shirin Saeedi Bidokhti",
      "Saswati Sarkar"
    ],
    "author_ids": [],
    "abstract": "We introduce a model of controlled sequential social learning in which a\nplanner may pay a cost to adjust the private information structure of agents.\nThe planner may seek to induce correct actions that are consistent with an\nunknown true state of the world (altruistic planner) or to induce a specific\naction the planner prefers (biased planner). Our framework presents a new\noptimization problem for social learning that combines dynamic programming with\ndecentralized action choices and Bayesian belief updates. This sheds light on\npractical policy questions, such as how the socially optimal level of ad\npersonalization changes according to current beliefs or how a political\ncampaign may selectively illuminate or obfuscate the winning potential of its\ncandidate among voters. We then prove the convexity of the value function and\ncharacterize the optimal policies of altruistic and biased planners, which\nattain desired tradeoffs between the costs they incur and the payoffs they earn\nfrom the choices they induce in the agents. Even for a planner who has\nequivalent knowledge to an individual, cannot lie or cherry-pick information,\nand is fully observable, we demonstrate that it is possible to dramatically\ninfluence social welfare in both positive and negative directions.",
    "published_date": "2025-04-03T00:00:00",
    "year": 2025,
    "categories": [
      "eess.SY",
      "cs.GT",
      "cs.SI",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.02648v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.02636v1",
    "title": "A Framework for Developing University Policies on Generative AI Governance: A Cross-national Comparative Study",
    "authors": [
      "Ming Li",
      "Qin Xie",
      "Ariunaa Enkhtur",
      "Shuoyang Meng",
      "Lilan Chen",
      "Beverley Anne Yamamoto",
      "Fei Cheng",
      "Masayuki Murakami"
    ],
    "author_ids": [],
    "abstract": "As generative artificial intelligence (GAI) becomes more integrated into\nhigher education and research, universities adopt varied approaches to GAI\npolicy development. To explore these variations, this study conducts a\ncomparative analysis of leading universities in the United States, Japan, and\nChina, examining their institution-wide policies on GAI application and\ngovernance. Based on these findings, the study proposes a University Policy\nDevelopment Framework for GAI (UPDF-GAI) to provide both theoretical insights\nand practical guidance for universities in developing and refining their GAI\npolicies. A qualitative content analysis of 124 policy documents from 110\nuniversities was conducted, employing thematic coding to synthesize 20 key\nthemes and 9 sub-themes. These themes and sub-themes formed the basis for\ndeveloping the framework. The analysis reveals varying priorities and focus of\nGAI policy of universities in different countries. U.S. universities emphasize\nfaculty autonomy, practical application, and policy adaptability, shaped by\ncutting-edge research and peer collaboration. Japanese universities take a\ngovernment-regulated approach, prioritizing ethics and risk management, but\nprovide limited support for AI implementation and flexibility. Chinese\nuniversities follow a centralized, government-led model, focusing on technology\napplication over early policy development, while actively exploring GAI\nintegration in education and research. The UPDF-GAI framework offers a\nsystematic, adaptable framework for assessing and optimizing GAI policies\nacross different educational contexts. By identifying key policy\ncharacteristics, enhancing policy effectiveness, and balancing technology,\nethics, and education, enabling universities to develop sustainable,\ncontextually relevant policies that strengthen their digital competitiveness\nand institutional readiness for AI-driven education.",
    "published_date": "2025-04-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.02636v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.02597v1",
    "title": "Regulating Spatial Fairness in a Tripartite Micromobility Sharing System via Reinforcement Learning",
    "authors": [
      "Matteo Cederle",
      "Marco Fabris",
      "Gian Antonio Susto"
    ],
    "author_ids": [],
    "abstract": "In the growing field of Shared Micromobility Systems, which holds great\npotential for shaping urban transportation, fairness-oriented approaches remain\nlargely unexplored. This work addresses such a gap by investigating the balance\nbetween performance optimization and algorithmic fairness in Shared\nMicromobility Services using Reinforcement Learning. Our methodology achieves\nequitable outcomes, measured by the Gini index, across central, peripheral, and\nremote station categories. By strategically rebalancing vehicle distribution,\nit maximizes operator performance while upholding fairness principles. The\nefficacy of our approach is validated through a case study using synthetic\ndata.",
    "published_date": "2025-04-03T00:00:00",
    "year": 2025,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.02597v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.02577v1",
    "title": "Reasoning Inconsistencies and How to Mitigate Them in Deep Learning",
    "authors": [
      "Erik Arakelyan"
    ],
    "author_ids": [],
    "abstract": "The recent advancements in Deep Learning models and techniques have led to\nsignificant strides in performance across diverse tasks and modalities.\nHowever, while the overall capabilities of models show promising growth, our\nunderstanding of their internal reasoning processes remains limited,\nparticularly concerning systematic inconsistencies or errors patterns of\nlogical or inferential flaws. These inconsistencies may manifest as\ncontradictory outputs, failure to generalize across similar tasks, or erroneous\nconclusions in specific contexts. Even detecting and measuring such reasoning\ndiscrepancies is challenging, as they may arise from opaque internal\nprocedures, biases and imbalances in training data, or the inherent complexity\nof the task. Without effective methods to detect, measure, and mitigate these\nerrors, there is a risk of deploying models that are biased, exploitable, or\nlogically unreliable. This thesis aims to address these issues by producing\nnovel methods for deep learning models that reason over knowledge graphs,\nnatural language, and images. The thesis contributes two techniques for\ndetecting and quantifying predictive inconsistencies originating from opaque\ninternal procedures in natural language and image processing models. To\nmitigate inconsistencies from biases in training data, this thesis presents a\ndata efficient sampling method to improve fairness and performance and a\nsynthetic dataset generation approach in low resource scenarios. Finally, the\nthesis offers two techniques to optimize the models for complex reasoning\ntasks. These methods enhance model performance while allowing for more faithful\nand interpretable exploration and exploitation during inference. Critically,\nthis thesis provides a comprehensive framework to improve the robustness,\nfairness, and interpretability of deep learning models across diverse tasks and\nmodalities.",
    "published_date": "2025-04-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.LO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.02577v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.02572v1",
    "title": "Language Models reach higher Agreement than Humans in Historical Interpretation",
    "authors": [
      "Fabio Celli",
      "Georgios Spathulas"
    ],
    "author_ids": [],
    "abstract": "This paper compares historical annotations by humans and Large Language\nModels. The findings reveal that both exhibit some cultural bias, but Large\nLanguage Models achieve a higher consensus on the interpretation of historical\nfacts from short texts. While humans tend to disagree on the basis of their\npersonal biases, Large Models disagree when they skip information or produce\nhallucinations. These findings have significant implications for digital\nhumanities, enabling large-scale annotation and quantitative analysis of\nhistorical data. This offers new educational and research opportunities to\nexplore historical interpretations from different Language Models, fostering\ncritical thinking about bias.",
    "published_date": "2025-04-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.02572v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.02917v1",
    "title": "Bias in Large Language Models Across Clinical Applications: A Systematic Review",
    "authors": [
      "Thanathip Suenghataiphorn",
      "Narisara Tribuddharat",
      "Pojsakorn Danpanichkul",
      "Narathorn Kulthamrongsri"
    ],
    "author_ids": [],
    "abstract": "Background: Large language models (LLMs) are rapidly being integrated into\nhealthcare, promising to enhance various clinical tasks. However, concerns\nexist regarding their potential for bias, which could compromise patient care\nand exacerbate health inequities. This systematic review investigates the\nprevalence, sources, manifestations, and clinical implications of bias in LLMs.\nMethods: We conducted a systematic search of PubMed, OVID, and EMBASE from\ndatabase inception through 2025, for studies evaluating bias in LLMs applied to\nclinical tasks. We extracted data on LLM type, bias source, bias manifestation,\naffected attributes, clinical task, evaluation methods, and outcomes. Risk of\nbias was assessed using a modified ROBINS-I tool. Results: Thirty-eight studies\nmet inclusion criteria, revealing pervasive bias across various LLMs and\nclinical applications. Both data-related bias (from biased training data) and\nmodel-related bias (from model training) were significant contributors. Biases\nmanifested as: allocative harm (e.g., differential treatment recommendations);\nrepresentational harm (e.g., stereotypical associations, biased image\ngeneration); and performance disparities (e.g., variable output quality). These\nbiases affected multiple attributes, most frequently race/ethnicity and gender,\nbut also age, disability, and language. Conclusions: Bias in clinical LLMs is a\npervasive and systemic issue, with a potential to lead to misdiagnosis and\ninappropriate treatment, particularly for marginalized patient populations.\nRigorous evaluation of the model is crucial. Furthermore, the development and\nimplementation of effective mitigation strategies, coupled with continuous\nmonitoring in real-world clinical settings, are essential to ensure the safe,\nequitable, and trustworthy deployment of LLMs in healthcare.",
    "published_date": "2025-04-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.02917v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.02546v2",
    "title": "GPG: A Simple and Strong Reinforcement Learning Baseline for Model Reasoning",
    "authors": [
      "Xiangxiang Chu",
      "Hailang Huang",
      "Xiao Zhang",
      "Fei Wei",
      "Yong Wang"
    ],
    "author_ids": [],
    "abstract": "Reinforcement Learning (RL) can directly enhance the reasoning capabilities\nof large language models without extensive reliance on Supervised Fine-Tuning\n(SFT). In this work, we revisit the traditional Policy Gradient (PG) mechanism\nand propose a minimalist RL approach termed Group Policy Gradient (GPG). Unlike\nconventional methods, GPG directly optimize the original RL objective, thus\nobviating the need for surrogate loss functions. By eliminating the critic and\nreference models, avoiding KL divergence constraints, and addressing the\nadvantage and gradient estimation bias, our approach significantly simplifies\nthe training process compared to Group Relative Policy Optimization (GRPO). Our\napproach achieves superior performance without relying on auxiliary techniques\nor adjustments. As illustrated in Figure 1, extensive experiments demonstrate\nthat our method not only reduces computational costs but also consistently\noutperforms GRPO across various unimodal and multimodal tasks. Our code is\navailable at https://github.com/AMAP-ML/GPG.",
    "published_date": "2025-04-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.02546v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.02504v1",
    "title": "Ethics of Blockchain Technologies",
    "authors": [
      "Georgy Ishmaev"
    ],
    "author_ids": [],
    "abstract": "This chapter explores three key questions in blockchain ethics. First, it\nsituates blockchain ethics within the broader field of technology ethics,\noutlining its goals and guiding principles. Second, it examines the unique\nethical challenges of blockchain applications, including permissionless\nsystems, incentive mechanisms, and privacy concerns. Key obstacles, such as\nconceptual modeling and information asymmetries, are identified as critical\nissues. Finally, the chapter argues that blockchain ethics should be approached\nas an engineering discipline, emphasizing the analysis and design of trade-offs\nin complex systems.",
    "published_date": "2025-04-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.02504v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.02910v1",
    "title": "Systematic Literature Review: Explainable AI Definitions and Challenges in Education",
    "authors": [
      "Zaid M. Altukhi",
      "Sojen Pradhan"
    ],
    "author_ids": [],
    "abstract": "Explainable AI (XAI) seeks to transform black-box algorithmic processes into\ntransparent ones, enhancing trust in AI applications across various sectors\nsuch as education. This review aims to examine the various definitions of XAI\nwithin the literature and explore the challenges of XAI in education. Our goal\nis to shed light on how XAI can contribute to enhancing the educational field.\nThis systematic review, utilising the PRISMA method for rigorous and\ntransparent research, identified 19 relevant studies. Our findings reveal 15\ndefinitions and 62 challenges. These challenges are categorised using thematic\nanalysis into seven groups: explainability, ethical, technical, human-computer\ninteraction (HCI), trustworthiness, policy and guideline, and others, thereby\ndeepening our understanding of the implications of XAI in education. Our\nanalysis highlights the absence of standardised definitions for XAI, leading to\nconfusion, especially because definitions concerning ethics, trustworthiness,\ntechnicalities, and explainability tend to overlap and vary.",
    "published_date": "2025-04-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.02910v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.02461v1",
    "title": "Am I Being Treated Fairly? A Conceptual Framework for Individuals to Ascertain Fairness",
    "authors": [
      "Juliett Suárez Ferreira",
      "Marija Slavkovik",
      "Jorge Casillas"
    ],
    "author_ids": [],
    "abstract": "Current fairness metrics and mitigation techniques provide tools for\npractitioners to asses how non-discriminatory Automatic Decision Making (ADM)\nsystems are. What if I, as an individual facing a decision taken by an ADM\nsystem, would like to know: Am I being treated fairly? We explore how to create\nthe affordance for users to be able to ask this question of ADM. In this paper,\nwe argue for the reification of fairness not only as a property of ADM, but\nalso as an epistemic right of an individual to acquire information about the\ndecisions that affect them and use that information to contest and seek\neffective redress against those decisions, in case they are proven to be\ndiscriminatory. We examine key concepts from existing research not only in\nalgorithmic fairness but also in explainable artificial intelligence,\naccountability, and contestability. Integrating notions from these domains, we\npropose a conceptual framework to ascertain fairness by combining different\ntools that empower the end-users of ADM systems. Our framework shifts the focus\nfrom technical solutions aimed at practitioners to mechanisms that enable\nindividuals to understand, challenge, and verify the fairness of decisions, and\nalso serves as a blueprint for organizations and policymakers, bridging the gap\nbetween technical requirements and practical, user-centered accountability.",
    "published_date": "2025-04-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "cs.MA",
      "I.2; J.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.02461v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.02352v1",
    "title": "Liquid Neural Networks: Next-Generation AI for Telecom from First Principles",
    "authors": [
      "Fenghao Zhu",
      "Xinquan Wang",
      "Chen Zhu",
      "Chongwen Huang"
    ],
    "author_ids": [],
    "abstract": "Artificial intelligence (AI) has emerged as a transformative technology with\nimmense potential to reshape the next-generation of wireless networks. By\nleveraging advanced algorithms and machine learning techniques, AI offers\nunprecedented capabilities in optimizing network performance, enhancing data\nprocessing efficiency, and enabling smarter decision-making processes. However,\nexisting AI solutions face significant challenges in terms of robustness and\ninterpretability. Specifically, current AI models exhibit substantial\nperformance degradation in dynamic environments with varying data\ndistributions, and the black-box nature of these algorithms raises concerns\nregarding safety, transparency, and fairness. This presents a major challenge\nin integrating AI into practical communication systems. Recently, a novel type\nof neural network, known as the liquid neural networks (LNNs), has been\ndesigned from first principles to address these issues. In this paper, we\nexplore the potential of LNNs in telecommunications. First, we illustrate the\nmechanisms of LNNs and highlight their unique advantages over traditional\nnetworks. Then we unveil the opportunities that LNNs bring to future wireless\nnetworks. Furthermore, we discuss the challenges and design directions for the\nimplementation of LNNs. Finally, we summarize the performance of LNNs in two\ncase studies.",
    "published_date": "2025-04-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IT",
      "eess.SP",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.02352v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.02304v1",
    "title": "Measurement of LLM's Philosophies of Human Nature",
    "authors": [
      "Minheng Ni",
      "Ennan Wu",
      "Zidong Gong",
      "Zhengyuan Yang",
      "Linjie Li",
      "Chung-Ching Lin",
      "Kevin Lin",
      "Lijuan Wang",
      "Wangmeng Zuo"
    ],
    "author_ids": [],
    "abstract": "The widespread application of artificial intelligence (AI) in various tasks,\nalong with frequent reports of conflicts or violations involving AI, has\nsparked societal concerns about interactions with AI systems. Based on\nWrightsman's Philosophies of Human Nature Scale (PHNS), a scale empirically\nvalidated over decades to effectively assess individuals' attitudes toward\nhuman nature, we design the standardized psychological scale specifically\ntargeting large language models (LLM), named the Machine-based Philosophies of\nHuman Nature Scale (M-PHNS). By evaluating LLMs' attitudes toward human nature\nacross six dimensions, we reveal that current LLMs exhibit a systemic lack of\ntrust in humans, and there is a significant negative correlation between the\nmodel's intelligence level and its trust in humans. Furthermore, we propose a\nmental loop learning framework, which enables LLM to continuously optimize its\nvalue system during virtual interactions by constructing moral scenarios,\nthereby improving its attitude toward human nature. Experiments demonstrate\nthat mental loop learning significantly enhances their trust in humans compared\nto persona or instruction prompts. This finding highlights the potential of\nhuman-based psychological assessments for LLM, which can not only diagnose\ncognitive biases but also provide a potential solution for ethical learning in\nartificial intelligence. We release the M-PHNS evaluation code and data at\nhttps://github.com/kodenii/M-PHNS.",
    "published_date": "2025-04-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.02304v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.02286v1",
    "title": "Moment Quantization for Video Temporal Grounding",
    "authors": [
      "Xiaolong Sun",
      "Le Wang",
      "Sanping Zhou",
      "Liushuai Shi",
      "Kun Xia",
      "Mengnan Liu",
      "Yabing Wang",
      "Gang Hua"
    ],
    "author_ids": [],
    "abstract": "Video temporal grounding is a critical video understanding task, which aims\nto localize moments relevant to a language description. The challenge of this\ntask lies in distinguishing relevant and irrelevant moments. Previous methods\nfocused on learning continuous features exhibit weak differentiation between\nforeground and background features. In this paper, we propose a novel\nMoment-Quantization based Video Temporal Grounding method (MQVTG), which\nquantizes the input video into various discrete vectors to enhance the\ndiscrimination between relevant and irrelevant moments. Specifically, MQVTG\nmaintains a learnable moment codebook, where each video moment matches a\ncodeword. Considering the visual diversity, i.e., various visual expressions\nfor the same moment, MQVTG treats moment-codeword matching as a clustering\nprocess without using discrete vectors, avoiding the loss of useful information\nfrom direct hard quantization. Additionally, we employ effective\nprior-initialization and joint-projection strategies to enhance the maintained\nmoment codebook. With its simple implementation, the proposed method can be\nintegrated into existing temporal grounding models as a plug-and-play\ncomponent. Extensive experiments on six popular benchmarks demonstrate the\neffectiveness and generalizability of MQVTG, significantly outperforming\nstate-of-the-art methods. Further qualitative analysis shows that our method\neffectively groups relevant features and separates irrelevant ones, aligning\nwith our goal of enhancing discrimination.",
    "published_date": "2025-04-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.02286v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.02254v1",
    "title": "LLMs as Deceptive Agents: How Role-Based Prompting Induces Semantic Ambiguity in Puzzle Tasks",
    "authors": [
      "Seunghyun Yoo"
    ],
    "author_ids": [],
    "abstract": "Recent advancements in Large Language Models (LLMs) have not only showcased\nimpressive creative capabilities but also revealed emerging agentic behaviors\nthat exploit linguistic ambiguity in adversarial settings. In this study, we\ninvestigate how an LLM, acting as an autonomous agent, leverages semantic\nambiguity to generate deceptive puzzles that mislead and challenge human users.\nInspired by the popular puzzle game \"Connections\", we systematically compare\npuzzles produced through zero-shot prompting, role-injected adversarial\nprompts, and human-crafted examples, with an emphasis on understanding the\nunderlying agent decision-making processes. Employing computational analyses\nwith HateBERT to quantify semantic ambiguity, alongside subjective human\nevaluations, we demonstrate that explicit adversarial agent behaviors\nsignificantly heighten semantic ambiguity -- thereby increasing cognitive load\nand reducing fairness in puzzle solving. These findings provide critical\ninsights into the emergent agentic qualities of LLMs and underscore important\nethical considerations for evaluating and safely deploying autonomous language\nsystems in both educational technologies and entertainment.",
    "published_date": "2025-04-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "68T50, 68T05, 68U35"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.02254v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.02239v1",
    "title": "The Author Is Sovereign: A Manifesto for Ethical Copyright in the Age of AI",
    "authors": [
      "Ricardo Fitas"
    ],
    "author_ids": [],
    "abstract": "In the age of AI, authorship is being quietly eroded by algorithmic content\nscraping, legal gray zones like \"fair use,\" and platforms that profit from\ncreative labor without consent or compensation. This short manifesto proposes a\nradical alternative: a system in which the author is sovereign of their\nintellectual domain. It presents seven ethical principles that challenge\nprevailing assumptions about open access, copyright ownership, and the public\ndomain - arguing that voluntary, negotiated consent must replace coercive\nnorms. The text exposes how weakened authorship fuels structural exploitation.\nIn place of reactive solutions, it calls for a new ethic of authorship rooted\nin consent, dignity, and contractual fairness.",
    "published_date": "2025-04-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.02239v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.02194v1",
    "title": "FairDAG: Consensus Fairness over Concurrent Causal Design",
    "authors": [
      "Dakai Kang",
      "Junchao Chen",
      "Tien Tuan Anh Dinh",
      "Mohammad Sadoghi"
    ],
    "author_ids": [],
    "abstract": "The rise of cryptocurrencies like Bitcoin and Ethereum has driven interest in\nblockchain technology, with Ethereum's smart contracts enabling the growth of\ndecentralized finance (DeFi). However, research has shown that adversaries\nexploit transaction ordering to extract profits through attacks like\nfront-running, sandwich attacks, and liquidation manipulation. This issue\naffects both permissionless and permissioned blockchains, as block proposers\nhave full control over transaction ordering. To address this, a more fair\napproach to transaction ordering is essential.\n  Existing fairness protocols, such as Pompe and Themis, operate on\nleader-based consensus protocols, which not only suffer from low throughput but\nalso allow adversaries to manipulate transaction ordering. To address these\nlimitations, we propose FairDAG-AB and FairDAG-RL, which leverage DAG-based\nconsensus protocols.\n  We theoretically demonstrate that FairDAG protocols not only uphold fairness\nguarantees, as previous fairness protocols do, but also achieve higher\nthroughput and greater resilience to adversarial ordering manipulation. Our\ndeployment and evaluation on CloudLab further validate these claims.",
    "published_date": "2025-04-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.DB",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.02194v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.02898v1",
    "title": "A Practical Synthesis of Detecting AI-Generated Textual, Visual, and Audio Content",
    "authors": [
      "Lele Cao"
    ],
    "author_ids": [],
    "abstract": "Advances in AI-generated content have led to wide adoption of large language\nmodels, diffusion-based visual generators, and synthetic audio tools. However,\nthese developments raise critical concerns about misinformation, copyright\ninfringement, security threats, and the erosion of public trust. In this paper,\nwe explore an extensive range of methods designed to detect and mitigate\nAI-generated textual, visual, and audio content. We begin by discussing\nmotivations and potential impacts associated with AI-based content generation,\nincluding real-world risks and ethical dilemmas. We then outline detection\ntechniques spanning observation-based strategies, linguistic and statistical\nanalysis, model-based pipelines, watermarking and fingerprinting, as well as\nemergent ensemble approaches. We also present new perspectives on robustness,\nadaptation to rapidly improving generative architectures, and the critical role\nof human-in-the-loop verification. By surveying state-of-the-art research and\nhighlighting case studies in academic, journalistic, legal, and industrial\ncontexts, this paper aims to inform robust solutions and policymaking. We\nconclude by discussing open challenges, including adversarial transformations,\ndomain generalization, and ethical concerns, thereby offering a holistic guide\nfor researchers, practitioners, and regulators to preserve content authenticity\nin the face of increasingly sophisticated AI-generated media.",
    "published_date": "2025-04-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.LG",
      "68T50, 68T45, 68T10, 68T30, 94A08, 62H30, 68U10",
      "I.2.7; I.2.10; I.2.6; H.5.5; K.6.5; K.4.1; I.4.9; H.3.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.02898v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.02175v1",
    "title": "Who Should Set the Standards? Analysing Censored Arabic Content on Facebook during the Palestine-Israel Conflict",
    "authors": [
      "Walid Magdy",
      "Hamdy Mubarak",
      "Joni Salminen"
    ],
    "author_ids": [],
    "abstract": "Nascent research on human-computer interaction concerns itself with fairness\nof content moderation systems. Designing globally applicable content moderation\nsystems requires considering historical, cultural, and socio-technical factors.\nInspired by this line of work, we investigate Arab users' perception of\nFacebook's moderation practices. We collect a set of 448 deleted Arabic posts,\nand we ask Arab annotators to evaluate these posts based on (a) Facebook\nCommunity Standards (FBCS) and (b) their personal opinion. Each post was judged\nby 10 annotators to account for subjectivity. Our analysis shows a clear gap\nbetween the Arabs' understanding of the FBCS and how Facebook implements these\nstandards. The study highlights a need for discussion on the moderation\nguidelines on social media platforms about who decides the moderation\nguidelines, how these guidelines are interpreted, and how well they represent\nthe views of marginalised user communities.",
    "published_date": "2025-04-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.02175v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.02074v1",
    "title": "Trapped by Expectations: Functional Fixedness in LLM-Enabled Chat Search",
    "authors": [
      "Jiqun Liu",
      "Jamshed Karimnazarov",
      "Ryen W. White"
    ],
    "author_ids": [],
    "abstract": "Functional fixedness, a cognitive bias that restricts users' interactions\nwith a new system or tool to expected or familiar ways, limits the full\npotential of Large Language Model (LLM)-enabled chat search, especially in\ncomplex and exploratory tasks. To investigate its impact, we conducted a\ncrowdsourcing study with 450 participants, each completing one of six\ndecision-making tasks spanning public safety, diet and health management,\nsustainability, and AI ethics. Participants engaged in a multi-prompt\nconversation with ChatGPT to address the task, allowing us to compare pre-chat\nintent-based expectations with observed interactions. We found that: 1) Several\naspects of pre-chat expectations are closely associated with users' prior\nexperiences with ChatGPT, search engines, and virtual assistants; 2) Prior\nsystem experience shapes language use and prompting behavior. Frequent ChatGPT\nusers reduced deictic terms and hedge words and frequently adjusted prompts.\nUsers with rich search experience maintained structured, less-conversational\nqueries with minimal modifications. Users of virtual assistants favored\ndirective, command-like prompts, reinforcing functional fixedness; 3) When the\nsystem failed to meet expectations, participants generated more detailed\nprompts with increased linguistic diversity, reflecting adaptive shifts. These\nfindings suggest that while preconceived expectations constrain early\ninteractions, unmet expectations can motivate behavioral adaptation. With\nappropriate system support, this may promote broader exploration of LLM\ncapabilities. This work also introduces a typology for user intents in chat\nsearch and highlights the importance of mitigating functional fixedness to\nsupport more creative and analytical use of LLMs.",
    "published_date": "2025-04-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.AI",
      "H.3.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.02074v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.01951v1",
    "title": "The LLM Wears Prada: Analysing Gender Bias and Stereotypes through Online Shopping Data",
    "authors": [
      "Massimiliano Luca",
      "Ciro Beneduce",
      "Bruno Lepri",
      "Jacopo Staiano"
    ],
    "author_ids": [],
    "abstract": "With the wide and cross-domain adoption of Large Language Models, it becomes\ncrucial to assess to which extent the statistical correlations in training\ndata, which underlie their impressive performance, hide subtle and potentially\ntroubling biases. Gender bias in LLMs has been widely investigated from the\nperspectives of works, hobbies, and emotions typically associated with a\nspecific gender. In this study, we introduce a novel perspective. We\ninvestigate whether LLMs can predict an individual's gender based solely on\nonline shopping histories and whether these predictions are influenced by\ngender biases and stereotypes. Using a dataset of historical online purchases\nfrom users in the United States, we evaluate the ability of six LLMs to\nclassify gender and we then analyze their reasoning and products-gender\nco-occurrences. Results indicate that while models can infer gender with\nmoderate accuracy, their decisions are often rooted in stereotypical\nassociations between product categories and gender. Furthermore, explicit\ninstructions to avoid bias reduce the certainty of model predictions, but do\nnot eliminate stereotypical patterns. Our findings highlight the persistent\nnature of gender biases in LLMs and emphasize the need for robust\nbias-mitigation strategies.",
    "published_date": "2025-04-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.01951v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.01838v1",
    "title": "Prompting Medical Vision-Language Models to Mitigate Diagnosis Bias by Generating Realistic Dermoscopic Images",
    "authors": [
      "Nusrat Munia",
      "Abdullah-Al-Zubaer Imran"
    ],
    "author_ids": [],
    "abstract": "Artificial Intelligence (AI) in skin disease diagnosis has improved\nsignificantly, but a major concern is that these models frequently show biased\nperformance across subgroups, especially regarding sensitive attributes such as\nskin color. To address these issues, we propose a novel generative AI-based\nframework, namely, Dermatology Diffusion Transformer (DermDiT), which leverages\ntext prompts generated via Vision Language Models and multimodal text-image\nlearning to generate new dermoscopic images. We utilize large vision language\nmodels to generate accurate and proper prompts for each dermoscopic image which\nhelps to generate synthetic images to improve the representation of\nunderrepresented groups (patient, disease, etc.) in highly imbalanced datasets\nfor clinical diagnoses. Our extensive experimentation showcases the large\nvision language models providing much more insightful representations, that\nenable DermDiT to generate high-quality images. Our code is available at\nhttps://github.com/Munia03/DermDiT",
    "published_date": "2025-04-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.01838v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.01837v1",
    "title": "Cramér--Rao Inequalities for Several Generalized Fisher Information",
    "authors": [
      "Hao Wu",
      "Lei Yu"
    ],
    "author_ids": [],
    "abstract": "The de Bruijn identity states that Fisher information is the half of the\nderivative of Shannon differential entropy along heat flow. In the same spirit,\nin this paper we introduce a generalized version of Fisher information, named\nas the R\\'enyi--Fisher information, which is the half of the derivative of\nR\\'enyi information along heat flow. Based on this R\\'enyi--Fisher information,\nwe establish sharp R\\'enyi-entropic isoperimetric inequalities, which\ngeneralize the classic entropic isoperimetric inequality to the R\\'enyi\nsetting. Utilizing these isoperimetric inequalities, we extend the classical\nCram\\'er--Rao inequality from Fisher information to R\\'enyi--Fisher\ninformation. Lastly, we use these generalized Cram\\'er--Rao inequalities to\ndetermine the signs of derivatives of entropy along heat flow, strengthening\nexisting results on the complete monotonicity of entropy.",
    "published_date": "2025-04-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IT",
      "math.IT",
      "math.PR",
      "math.ST",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.01837v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.01819v1",
    "title": "Implicit Bias Injection Attacks against Text-to-Image Diffusion Models",
    "authors": [
      "Huayang Huang",
      "Xiangye Jin",
      "Jiaxu Miao",
      "Yu Wu"
    ],
    "author_ids": [],
    "abstract": "The proliferation of text-to-image diffusion models (T2I DMs) has led to an\nincreased presence of AI-generated images in daily life. However, biased T2I\nmodels can generate content with specific tendencies, potentially influencing\npeople's perceptions. Intentional exploitation of these biases risks conveying\nmisleading information to the public. Current research on bias primarily\naddresses explicit biases with recognizable visual patterns, such as skin color\nand gender. This paper introduces a novel form of implicit bias that lacks\nexplicit visual features but can manifest in diverse ways across various\nsemantic contexts. This subtle and versatile nature makes this bias challenging\nto detect, easy to propagate, and adaptable to a wide range of scenarios. We\nfurther propose an implicit bias injection attack framework (IBI-Attacks)\nagainst T2I diffusion models by precomputing a general bias direction in the\nprompt embedding space and adaptively adjusting it based on different inputs.\nOur attack module can be seamlessly integrated into pre-trained diffusion\nmodels in a plug-and-play manner without direct manipulation of user input or\nmodel retraining. Extensive experiments validate the effectiveness of our\nscheme in introducing bias through subtle and diverse modifications while\npreserving the original semantics. The strong concealment and transferability\nof our attack across various scenarios further underscore the significance of\nour approach. Code is available at https://github.com/Hannah1102/IBI-attacks.",
    "published_date": "2025-04-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.01819v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.01766v1",
    "title": "Learning with Imperfect Models: When Multi-step Prediction Mitigates Compounding Error",
    "authors": [
      "Anne Somalwar",
      "Bruce D. Lee",
      "George J. Pappas",
      "Nikolai Matni"
    ],
    "author_ids": [],
    "abstract": "Compounding error, where small prediction mistakes accumulate over time,\npresents a major challenge in learning-based control. For example, this issue\noften limits the performance of model-based reinforcement learning and\nimitation learning. One common approach to mitigate compounding error is to\ntrain multi-step predictors directly, rather than relying on autoregressive\nrollout of a single-step model. However, it is not well understood when the\nbenefits of multi-step prediction outweigh the added complexity of learning a\nmore complicated model. In this work, we provide a rigorous analysis of this\ntrade-off in the context of linear dynamical systems. We show that when the\nmodel class is well-specified and accurately captures the system dynamics,\nsingle-step models achieve lower asymptotic prediction error. On the other\nhand, when the model class is misspecified due to partial observability, direct\nmulti-step predictors can significantly reduce bias and thus outperform\nsingle-step approaches. These theoretical results are supported by numerical\nexperiments, wherein we also (a) empirically evaluate an intermediate strategy\nwhich trains a single-step model using a multi-step loss and (b) evaluate\nperformance of single step and multi-step predictors in a closed loop control\nsetting.",
    "published_date": "2025-04-02T00:00:00",
    "year": 2025,
    "categories": [
      "eess.SY",
      "cs.LG",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.01766v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.01712v1",
    "title": "Method for Mitigating Attention to Inappropriate Content Based on Attention Dynamics Model",
    "authors": [
      "Naoki Hirakura"
    ],
    "author_ids": [],
    "abstract": "The expansion of the attention economy has led to the growing issue of\ninappropriate content being posted by profit-driven users. Previous\ncountermeasures against inappropriate content have relied on moderation, which\nraises ethical concerns, or information diffusion control, which requires\nconsidering larger scale networks, including general users. This study proposes\nan imitation strategy as an intervention method that does not rely on\nmoderation and focuses on a relatively smaller scale competitive network of\ninformation disseminators rather than the entire social network. The imitation\nstrategy is a novel approach that utilizes increased competition among\ninformation disseminators through imitation to reduce attention to\ninappropriate content. Through theoretical analysis and numerical simulations,\nI demonstrate that the imitation strategy is more effective when nodes with\nhigher eigenvector centrality are selected as targets and nodes with lower\neigenvector centrality are chosen as imitators.",
    "published_date": "2025-04-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.01712v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.01700v1",
    "title": "Reasoning LLMs for User-Aware Multimodal Conversational Agents",
    "authors": [
      "Hamed Rahimi",
      "Jeanne Cattoni",
      "Meriem Beghili",
      "Mouad Abrini",
      "Mahdi Khoramshahi",
      "Maribel Pino",
      "Mohamed Chetouani"
    ],
    "author_ids": [],
    "abstract": "Personalization in social robotics is critical for fostering effective\nhuman-robot interactions, yet systems often face the cold start problem, where\ninitial user preferences or characteristics are unavailable. This paper\nproposes a novel framework called USER-LLM R1 for a user-aware conversational\nagent that addresses this challenge through dynamic user profiling and model\ninitiation. Our approach integrates chain-of-thought (CoT) reasoning models to\niteratively infer user preferences and vision-language models (VLMs) to\ninitialize user profiles from multimodal inputs, enabling personalized\ninteractions from the first encounter. Leveraging a Retrieval-Augmented\nGeneration (RAG) architecture, the system dynamically refines user\nrepresentations within an inherent CoT process, ensuring contextually relevant\nand adaptive responses. Evaluations on the ElderlyTech-VQA Bench demonstrate\nsignificant improvements in ROUGE-1 (+23.2%), ROUGE-2 (+0.6%), and ROUGE-L\n(+8%) F1 scores over state-of-the-art baselines, with ablation studies\nunderscoring the impact of reasoning model size on performance. Human\nevaluations further validate the framework's efficacy, particularly for elderly\nusers, where tailored responses enhance engagement and trust. Ethical\nconsiderations, including privacy preservation and bias mitigation, are\nrigorously discussed and addressed to ensure responsible deployment.",
    "published_date": "2025-04-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.01700v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.01668v2",
    "title": "Overlap-Aware Feature Learning for Robust Unsupervised Domain Adaptation for 3D Semantic Segmentation",
    "authors": [
      "Junjie Chen",
      "Yuecong Xu",
      "Haosheng Li",
      "Kemi Ding"
    ],
    "author_ids": [],
    "abstract": "3D point cloud semantic segmentation (PCSS) is a cornerstone for\nenvironmental perception in robotic systems and autonomous driving, enabling\nprecise scene understanding through point-wise classification. While\nunsupervised domain adaptation (UDA) mitigates label scarcity in PCSS, existing\nmethods critically overlook the inherent vulnerability to real-world\nperturbations (e.g., snow, fog, rain) and adversarial distortions. This work\nfirst identifies two intrinsic limitations that undermine current PCSS-UDA\nrobustness: (a) unsupervised features overlap from unaligned boundaries in\nshared-class regions and (b) feature structure erosion caused by\ndomain-invariant learning that suppresses target-specific patterns. To address\nthe proposed problems, we propose a tripartite framework consisting of: 1) a\nrobustness evaluation model quantifying resilience against adversarial\nattack/corruption types through robustness metrics; 2) an invertible attention\nalignment module (IAAM) enabling bidirectional domain mapping while preserving\ndiscriminative structure via attention-guided overlap suppression; and 3) a\ncontrastive memory bank with quality-aware contrastive learning that\nprogressively refines pseudo-labels with feature quality for more\ndiscriminative representations. Extensive experiments on\nSynLiDAR-to-SemanticPOSS adaptation demonstrate a maximum mIoU improvement of\n14.3\\% under adversarial attack.",
    "published_date": "2025-04-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.01668v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.01589v2",
    "title": "Text Speaks Louder than Vision: ASCII Art Reveals Textual Biases in Vision-Language Models",
    "authors": [
      "Zhaochen Wang",
      "Bryan Hooi",
      "Yiwei Wang",
      "Ming-Hsuan Yang",
      "Zi Huang",
      "Yujun Cai"
    ],
    "author_ids": [],
    "abstract": "Vision-language models (VLMs) have advanced rapidly in processing multimodal\ninformation, but their ability to reconcile conflicting signals across\nmodalities remains underexplored. This work investigates how VLMs process ASCII\nart, a unique medium where textual elements collectively form visual patterns,\npotentially creating semantic-visual conflicts. We introduce a novel evaluation\nframework that systematically challenges five state-of-the-art models\n(including GPT-4o, Claude, and Gemini) using adversarial ASCII art, where\ncharacter-level semantics deliberately contradict global visual patterns. Our\nexperiments reveal a strong text-priority bias: VLMs consistently prioritize\ntextual information over visual patterns, with visual recognition ability\ndeclining dramatically as semantic complexity increases. Various mitigation\nattempts through visual parameter tuning and prompt engineering yielded only\nmodest improvements, suggesting that this limitation requires\narchitectural-level solutions. These findings uncover fundamental flaws in how\ncurrent VLMs integrate multimodal information, providing important guidance for\nfuture model development while highlighting significant implications for\ncontent moderation systems vulnerable to adversarial examples.",
    "published_date": "2025-04-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.01589v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.01437v1",
    "title": "Behavioral Inequalities",
    "authors": [
      "Soutrik Bandyopadhyay",
      "Debasattam Pal",
      "Shubhendu Bhasin"
    ],
    "author_ids": [],
    "abstract": "We introduce behavioral inequalities as a way to model dynamical systems\ndefined by inequalities among their variables of interest. We claim that such a\nformulation enables the representation of safety-aware dynamical systems,\nsystems with bounds on disturbances, practical design limits and operational\nboundaries, etc. We develop a necessary and sufficient condition for the\nexistence of solutions to such behavioral inequalities and provide a\nparametrization of solutions when they exist. Finally, we show the efficacy of\nthe proposed method in two practical examples.",
    "published_date": "2025-04-02T00:00:00",
    "year": 2025,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.01437v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.01420v1",
    "title": "FAIRE: Assessing Racial and Gender Bias in AI-Driven Resume Evaluations",
    "authors": [
      "Athena Wen",
      "Tanush Patil",
      "Ansh Saxena",
      "Yicheng Fu",
      "Sean O'Brien",
      "Kevin Zhu"
    ],
    "author_ids": [],
    "abstract": "In an era where AI-driven hiring is transforming recruitment practices,\nconcerns about fairness and bias have become increasingly important. To explore\nthese issues, we introduce a benchmark, FAIRE (Fairness Assessment In Resume\nEvaluation), to test for racial and gender bias in large language models (LLMs)\nused to evaluate resumes across different industries. We use two methods-direct\nscoring and ranking-to measure how model performance changes when resumes are\nslightly altered to reflect different racial or gender identities. Our findings\nreveal that while every model exhibits some degree of bias, the magnitude and\ndirection vary considerably. This benchmark provides a clear way to examine\nthese differences and offers valuable insights into the fairness of AI-based\nhiring tools. It highlights the urgent need for strategies to reduce bias in\nAI-driven recruitment. Our benchmark code and dataset are open-sourced at our\nrepository:\nhttps://github.com/athenawen/FAIRE-Fairness-Assessment-In-Resume-Evaluation.git.",
    "published_date": "2025-04-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.01420v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.01396v1",
    "title": "All Patches Matter, More Patches Better: Enhance AI-Generated Image Detection via Panoptic Patch Learning",
    "authors": [
      "Zheng Yang",
      "Ruoxin Chen",
      "Zhiyuan Yan",
      "Ke-Yue Zhang",
      "Xinghe Fu",
      "Shuang Wu",
      "Xiujun Shu",
      "Taiping Yao",
      "Junchi Yan",
      "Shouhong Ding",
      "Xi Li"
    ],
    "author_ids": [],
    "abstract": "The exponential growth of AI-generated images (AIGIs) underscores the urgent\nneed for robust and generalizable detection methods. In this paper, we\nestablish two key principles for AIGI detection through systematic analysis:\n\\textbf{(1) All Patches Matter:} Unlike conventional image classification where\ndiscriminative features concentrate on object-centric regions, each patch in\nAIGIs inherently contains synthetic artifacts due to the uniform generation\nprocess, suggesting that every patch serves as an important artifact source for\ndetection. \\textbf{(2) More Patches Better}: Leveraging distributed artifacts\nacross more patches improves detection robustness by capturing complementary\nforensic evidence and reducing over-reliance on specific patches, thereby\nenhancing robustness and generalization. However, our counterfactual analysis\nreveals an undesirable phenomenon: naively trained detectors often exhibit a\n\\textbf{Few-Patch Bias}, discriminating between real and synthetic images based\non minority patches. We identify \\textbf{Lazy Learner} as the root cause:\ndetectors preferentially learn conspicuous artifacts in limited patches while\nneglecting broader artifact distributions. To address this bias, we propose the\n\\textbf{P}anoptic \\textbf{P}atch \\textbf{L}earning (PPL) framework, involving:\n(1) Random Patch Replacement that randomly substitutes synthetic patches with\nreal counterparts to compel models to identify artifacts in underutilized\nregions, encouraging the broader use of more patches; (2) Patch-wise\nContrastive Learning that enforces consistent discriminative capability across\nall patches, ensuring uniform utilization of all patches. Extensive experiments\nacross two different settings on several benchmarks verify the effectiveness of\nour approach.",
    "published_date": "2025-04-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.01396v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.03755v1",
    "title": "ProtoGCD: Unified and Unbiased Prototype Learning for Generalized Category Discovery",
    "authors": [
      "Shijie Ma",
      "Fei Zhu",
      "Xu-Yao Zhang",
      "Cheng-Lin Liu"
    ],
    "author_ids": [],
    "abstract": "Generalized category discovery (GCD) is a pragmatic but underexplored\nproblem, which requires models to automatically cluster and discover novel\ncategories by leveraging the labeled samples from old classes. The challenge is\nthat unlabeled data contain both old and new classes. Early works leveraging\npseudo-labeling with parametric classifiers handle old and new classes\nseparately, which brings about imbalanced accuracy between them. Recent methods\nemploying contrastive learning neglect potential positives and are decoupled\nfrom the clustering objective, leading to biased representations and\nsub-optimal results. To address these issues, we introduce a unified and\nunbiased prototype learning framework, namely ProtoGCD, wherein old and new\nclasses are modeled with joint prototypes and unified learning objectives,\n{enabling unified modeling between old and new classes}. Specifically, we\npropose a dual-level adaptive pseudo-labeling mechanism to mitigate\nconfirmation bias, together with two regularization terms to collectively help\nlearn more suitable representations for GCD. Moreover, for practical\nconsiderations, we devise a criterion to estimate the number of new classes.\nFurthermore, we extend ProtoGCD to detect unseen outliers, achieving task-level\nunification. Comprehensive experiments show that ProtoGCD achieves\nstate-of-the-art performance on both generic and fine-grained datasets. The\ncode is available at https://github.com/mashijie1028/ProtoGCD.",
    "published_date": "2025-04-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.03755v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.01383v1",
    "title": "v-CLR: View-Consistent Learning for Open-World Instance Segmentation",
    "authors": [
      "Chang-Bin Zhang",
      "Jinhong Ni",
      "Yujie Zhong",
      "Kai Han"
    ],
    "author_ids": [],
    "abstract": "In this paper, we address the challenging problem of open-world instance\nsegmentation. Existing works have shown that vanilla visual networks are biased\ntoward learning appearance information, \\eg texture, to recognize objects. This\nimplicit bias causes the model to fail in detecting novel objects with unseen\ntextures in the open-world setting. To address this challenge, we propose a\nlearning framework, called view-Consistent LeaRning (v-CLR), which aims to\nenforce the model to learn appearance-invariant representations for robust\ninstance segmentation. In v-CLR, we first introduce additional views for each\nimage, where the texture undergoes significant alterations while preserving the\nimage's underlying structure. We then encourage the model to learn the\nappearance-invariant representation by enforcing the consistency between object\nfeatures across different views, for which we obtain class-agnostic object\nproposals using off-the-shelf unsupervised models that possess strong\nobject-awareness. These proposals enable cross-view object feature matching,\ngreatly reducing the appearance dependency while enhancing the\nobject-awareness. We thoroughly evaluate our method on public benchmarks under\nboth cross-class and cross-dataset settings, achieving state-of-the-art\nperformance. Project page: https://visual-ai.github.io/vclr",
    "published_date": "2025-04-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.01383v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.01223v1",
    "title": "Explainable post-training bias mitigation with distribution-based fairness metrics",
    "authors": [
      "Ryan Franks",
      "Alexey Miroshnikov"
    ],
    "author_ids": [],
    "abstract": "We develop a novel optimization framework with distribution-based fairness\nconstraints for efficiently producing demographically blind, explainable models\nacross a wide range of fairness levels. This is accomplished through\npost-processing, avoiding the need for retraining. Our framework, which is\nbased on stochastic gradient descent, can be applied to a wide range of model\ntypes, with a particular emphasis on the post-processing of gradient-boosted\ndecision trees. Additionally, we design a broad class of interpretable global\nbias metrics compatible with our method by building on previous work. We\nempirically test our methodology on a variety of datasets and compare it to\nother methods.",
    "published_date": "2025-04-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "math.PR",
      "49Q22, 65K10, 91A12, 68T01"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.01223v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.01218v1",
    "title": "Prompting Forgetting: Unlearning in GANs via Textual Guidance",
    "authors": [
      "Piyush Nagasubramaniam",
      "Neeraj Karamchandani",
      "Chen Wu",
      "Sencun Zhu"
    ],
    "author_ids": [],
    "abstract": "State-of-the-art generative models exhibit powerful image-generation\ncapabilities, introducing various ethical and legal challenges to service\nproviders hosting these models. Consequently, Content Removal Techniques (CRTs)\nhave emerged as a growing area of research to control outputs without\nfull-scale retraining. Recent work has explored the use of Machine Unlearning\nin generative models to address content removal. However, the focus of such\nresearch has been on diffusion models, and unlearning in Generative Adversarial\nNetworks (GANs) has remained largely unexplored. We address this gap by\nproposing Text-to-Unlearn, a novel framework that selectively unlearns concepts\nfrom pre-trained GANs using only text prompts, enabling feature unlearning,\nidentity unlearning, and fine-grained tasks like expression and multi-attribute\nremoval in models trained on human faces. Leveraging natural language\ndescriptions, our approach guides the unlearning process without requiring\nadditional datasets or supervised fine-tuning, offering a scalable and\nefficient solution. To evaluate its effectiveness, we introduce an automatic\nunlearning assessment method adapted from state-of-the-art image-text alignment\nmetrics, providing a comprehensive analysis of the unlearning methodology. To\nour knowledge, Text-to-Unlearn is the first cross-modal unlearning framework\nfor GANs, representing a flexible and efficient advancement in managing\ngenerative model behavior.",
    "published_date": "2025-04-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.01218v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.01154v1",
    "title": "Remember, but also, Forget: Bridging Myopic and Perfect Recall Fairness with Past-Discounting",
    "authors": [
      "Ashwin Kumar",
      "William Yeoh"
    ],
    "author_ids": [],
    "abstract": "Dynamic resource allocation in multi-agent settings often requires balancing\nefficiency with fairness over time--a challenge inadequately addressed by\nconventional, myopic fairness measures. Motivated by behavioral insights that\nhuman judgments of fairness evolve with temporal distance, we introduce a novel\nframework for temporal fairness that incorporates past-discounting mechanisms.\nBy applying a tunable discount factor to historical utilities, our approach\ninterpolates between instantaneous and perfect-recall fairness, thereby\ncapturing both immediate outcomes and long-term equity considerations. Beyond\naligning more closely with human perceptions of fairness, this past-discounting\nmethod ensures that the augmented state space remains bounded, significantly\nimproving computational tractability in sequential decision-making settings. We\ndetail the formulation of discounted-recall fairness in both additive and\naveraged utility contexts, illustrate its benefits through practical examples,\nand discuss its implications for designing balanced, scalable resource\nallocation strategies.",
    "published_date": "2025-04-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.GT",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.01154v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.01127v1",
    "title": "Can LLMs Grasp Implicit Cultural Values? Benchmarking LLMs' Metacognitive Cultural Intelligence with CQ-Bench",
    "authors": [
      "Ziyi Liu",
      "Priyanka Dey",
      "Zhenyu Zhao",
      "Jen-tse Huang",
      "Rahul Gupta",
      "Yang Liu",
      "Jieyu Zhao"
    ],
    "author_ids": [],
    "abstract": "Cultural Intelligence (CQ) refers to the ability to understand unfamiliar\ncultural contexts-a crucial skill for large language models (LLMs) to\neffectively engage with globally diverse users. While existing research often\nfocuses on explicitly stated cultural norms, such approaches fail to capture\nthe subtle, implicit values that underlie real-world conversations. To address\nthis gap, we introduce CQ-Bench, a benchmark specifically designed to assess\nLLMs' capability to infer implicit cultural values from natural conversational\ncontexts. We generate a multi-character conversation-based stories dataset\nusing values from the World Value Survey and GlobalOpinions datasets, with\ntopics including ethical, religious, social, and political. Our dataset\nconstruction pipeline includes rigorous validation procedures-incorporation,\nconsistency, and implicitness checks-using GPT-4o, with 98.2% human-model\nagreement in the final validation. Our benchmark consists of three tasks of\nincreasing complexity: attitude detection, value selection, and value\nextraction. We find that while o1 and Deepseek-R1 models reach human-level\nperformance in value selection (0.809 and 0.814), they still fall short in\nnuanced attitude detection, with F1 scores of 0.622 and 0.635, respectively. In\nthe value extraction task, GPT-4o-mini and o3-mini score 0.602 and 0.598,\nhighlighting the difficulty of open-ended cultural reasoning. Notably,\nfine-tuning smaller models (e.g., LLaMA-3.2-3B) on only 500 culturally rich\nexamples improves performance by over 10%, even outperforming stronger\nbaselines (o3-mini) in some cases. Using CQ-Bench, we provide insights into the\ncurrent challenges in LLMs' CQ research and suggest practical pathways for\nenhancing LLMs' cross-cultural reasoning abilities.",
    "published_date": "2025-04-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.01127v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.00928v1",
    "title": "Taxonomizing Representational Harms using Speech Act Theory",
    "authors": [
      "Emily Corvi",
      "Hannah Washington",
      "Stefanie Reed",
      "Chad Atalla",
      "Alexandra Chouldechova",
      "P. Alex Dow",
      "Jean Garcia-Gathright",
      "Nicholas Pangakis",
      "Emily Sheng",
      "Dan Vann",
      "Matthew Vogel",
      "Hanna Wallach"
    ],
    "author_ids": [],
    "abstract": "Representational harms are widely recognized among fairness-related harms\ncaused by generative language systems. However, their definitions are commonly\nunder-specified. We present a framework, grounded in speech act theory (Austin,\n1962), that conceptualizes representational harms caused by generative language\nsystems as the perlocutionary effects (i.e., real-world impacts) of particular\ntypes of illocutionary acts (i.e., system behaviors). Building on this argument\nand drawing on relevant literature from linguistic anthropology and\nsociolinguistics, we provide new definitions stereotyping, demeaning, and\nerasure. We then use our framework to develop a granular taxonomy of\nillocutionary acts that cause representational harms, going beyond the\nhigh-level taxonomies presented in previous work. We also discuss the ways that\nour framework and taxonomy can support the development of valid measurement\ninstruments. Finally, we demonstrate the utility of our framework and taxonomy\nvia a case study that engages with recent conceptual debates about what\nconstitutes a representational harm and how such harms should be measured.",
    "published_date": "2025-04-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.00928v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.00916v1",
    "title": "Crossing number inequalities for curves on surfaces",
    "authors": [
      "Alfredo Hubard",
      "Hugo Parlier"
    ],
    "author_ids": [],
    "abstract": "We prove that, as $m$ grows, any family of $m$ homotopically distinct closed\ncurves on a surface induces a number of crossings that grows at least like $(m\n\\log m)^2$. We use this to answer two questions of Pach, Tardos and Toth\nrelated to crossing numbers of drawings of multigraphs where edges are required\nto be non-homotopic. Furthermore, we generalize these results, obtaining\neffective bounds with optimal growth rates on every orientable surface.",
    "published_date": "2025-04-01T00:00:00",
    "year": 2025,
    "categories": [
      "math.GT",
      "cs.CG",
      "math.CO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.00916v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.00874v1",
    "title": "P2NIA: Privacy-Preserving Non-Iterative Auditing",
    "authors": [
      "Jade Garcia Bourrée",
      "Hadrien Lautraite",
      "Sébastien Gambs",
      "Gilles Tredan",
      "Erwan Le Merrer",
      "Benoît Rottembourg"
    ],
    "author_ids": [],
    "abstract": "The emergence of AI legislation has increased the need to assess the ethical\ncompliance of high-risk AI systems. Traditional auditing methods rely on\nplatforms' application programming interfaces (APIs), where responses to\nqueries are examined through the lens of fairness requirements. However, such\napproaches put a significant burden on platforms, as they are forced to\nmaintain APIs while ensuring privacy, facing the possibility of data leaks.\nThis lack of proper collaboration between the two parties, in turn, causes a\nsignificant challenge to the auditor, who is subject to estimation bias as they\nare unaware of the data distribution of the platform. To address these two\nissues, we present P2NIA, a novel auditing scheme that proposes a mutually\nbeneficial collaboration for both the auditor and the platform. Extensive\nexperiments demonstrate P2NIA's effectiveness in addressing both issues. In\nsummary, our work introduces a privacy-preserving and non-iterative audit\nscheme that enhances fairness assessments using synthetic or local data,\navoiding the challenges associated with traditional API-based audits.",
    "published_date": "2025-04-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.00874v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.00860v1",
    "title": "Investigating the Capabilities and Limitations of Machine Learning for Identifying Bias in English Language Data with Information and Heritage Professionals",
    "authors": [
      "Lucy Havens",
      "Benjamin Bach",
      "Melissa Terras",
      "Beatrice Alex"
    ],
    "author_ids": [],
    "abstract": "Despite numerous efforts to mitigate their biases, ML systems continue to\nharm already-marginalized people. While predominant ML approaches assume bias\ncan be removed and fair models can be created, we show that these are not\nalways possible, nor desirable, goals. We reframe the problem of ML bias by\ncreating models to identify biased language, drawing attention to a dataset's\nbiases rather than trying to remove them. Then, through a workshop, we\nevaluated the models for a specific use case: workflows of information and\nheritage professionals. Our findings demonstrate the limitations of ML for\nidentifying bias due to its contextual nature, the way in which approaches to\nmitigating it can simultaneously privilege and oppress different communities,\nand its inevitability. We demonstrate the need to expand ML approaches to bias\nand fairness, providing a mixed-methods approach to investigating the\nfeasibility of removing bias or achieving fairness in a given ML use case.",
    "published_date": "2025-04-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.LG",
      "I.2.7; J.0; K.4.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.00860v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.00797v1",
    "title": "Bridging the Gap: Integrating Ethics and Environmental Sustainability in AI Research and Practice",
    "authors": [
      "Alexandra Sasha Luccioni",
      "Giada Pistilli",
      "Raesetje Sefala",
      "Nyalleng Moorosi"
    ],
    "author_ids": [],
    "abstract": "As the possibilities for Artificial Intelligence (AI) have grown, so have\nconcerns regarding its impacts on society and the environment. However, these\nissues are often raised separately; i.e. carbon footprint analyses of AI models\ntypically do not consider how the pursuit of scale has contributed towards\nbuilding models that are both inaccessible to most researchers in terms of cost\nand disproportionately harmful to the environment. On the other hand, model\naudits that aim to evaluate model performance and disparate impacts mostly fail\nto engage with the environmental ramifications of AI models and how these fit\ninto their auditing approaches. In this separation, both research directions\nfail to capture the depth of analysis that can be explored by considering the\ntwo in parallel and the potential solutions for making informed choices that\ncan be developed at their convergence. In this essay, we build upon work\ncarried out in AI and in sister communities, such as philosophy and sustainable\ndevelopment, to make more deliberate connections around topics such as\ngeneralizability, transparency, evaluation and equity across AI research and\npractice. We argue that the efforts aiming to study AI's ethical ramifications\nshould be made in tandem with those evaluating its impacts on the environment,\nand we conclude with a proposal of best practices to better integrate AI ethics\nand sustainability in AI research and practice.",
    "published_date": "2025-04-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.00797v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.02870v1",
    "title": "AI Hiring with LLMs: A Context-Aware and Explainable Multi-Agent Framework for Resume Screening",
    "authors": [
      "Frank P. -W. Lo",
      "Jianing Qiu",
      "Zeyu Wang",
      "Haibao Yu",
      "Yeming Chen",
      "Gao Zhang",
      "Benny Lo"
    ],
    "author_ids": [],
    "abstract": "Resume screening is a critical yet time-intensive process in talent\nacquisition, requiring recruiters to analyze vast volume of job applications\nwhile remaining objective, accurate, and fair. With the advancements in Large\nLanguage Models (LLMs), their reasoning capabilities and extensive knowledge\nbases demonstrate new opportunities to streamline and automate recruitment\nworkflows. In this work, we propose a multi-agent framework for resume\nscreening using LLMs to systematically process and evaluate resumes. The\nframework consists of four core agents, including a resume extractor, an\nevaluator, a summarizer, and a score formatter. To enhance the contextual\nrelevance of candidate assessments, we integrate Retrieval-Augmented Generation\n(RAG) within the resume evaluator, allowing incorporation of external knowledge\nsources, such as industry-specific expertise, professional certifications,\nuniversity rankings, and company-specific hiring criteria. This dynamic\nadaptation enables personalized recruitment, bridging the gap between AI\nautomation and talent acquisition. We assess the effectiveness of our approach\nby comparing AI-generated scores with ratings provided by HR professionals on a\ndataset of anonymized online resumes. The findings highlight the potential of\nmulti-agent RAG-LLM systems in automating resume screening, enabling more\nefficient and scalable hiring workflows.",
    "published_date": "2025-04-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.02870v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.00652v1",
    "title": "Towards Adaptive AI Governance: Comparative Insights from the U.S., EU, and Asia",
    "authors": [
      "Vikram Kulothungan",
      "Deepti Gupta"
    ],
    "author_ids": [],
    "abstract": "Artificial intelligence (AI) trends vary significantly across global regions,\nshaping the trajectory of innovation, regulation, and societal impact. This\nvariation influences how different regions approach AI development, balancing\ntechnological progress with ethical and regulatory considerations. This study\nconducts a comparative analysis of AI trends in the United States (US), the\nEuropean Union (EU), and Asia, focusing on three key dimensions: generative AI,\nethical oversight, and industrial applications. The US prioritizes\nmarket-driven innovation with minimal regulatory constraints, the EU enforces a\nprecautionary risk-based framework emphasizing ethical safeguards, and Asia\nemploys state-guided AI strategies that balance rapid deployment with\nregulatory oversight. Although these approaches reflect different economic\nmodels and policy priorities, their divergence poses challenges to\ninternational collaboration, regulatory harmonization, and the development of\nglobal AI standards. To address these challenges, this paper synthesizes\nregional strengths to propose an adaptive AI governance framework that\nintegrates risk-tiered oversight, innovation accelerators, and strategic\nalignment mechanisms. By bridging governance gaps, this study offers actionable\ninsights for fostering responsible AI development while ensuring a balance\nbetween technological progress, ethical imperatives, and regulatory coherence.",
    "published_date": "2025-04-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.ET"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.00652v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.02000v1",
    "title": "AI Regulation and Capitalist Growth: Balancing Innovation, Ethics, and Global Governance",
    "authors": [
      "Vikram Kulothungan",
      "Priya Ranjani Mohan",
      "Deepti Gupta"
    ],
    "author_ids": [],
    "abstract": "Artificial Intelligence (AI) is increasingly central to economic growth,\npromising new efficiencies and markets. This economic significance has sparked\ndebate over AI regulation: do rules and oversight bolster long term growth by\nbuilding trust and safeguarding the public, or do they constrain innovation and\nfree enterprise? This paper examines the balance between AI regulation and\ncapitalist ideals, focusing on how different approaches to AI data privacy can\nimpact innovation in AI-driven applications. The central question is whether AI\nregulation enhances or inhibits growth in a capitalist economy. Our analysis\nsynthesizes historical precedents, the current U.S. regulatory landscape,\neconomic projections, legal challenges, and case studies of recent AI policies.\nWe discuss that carefully calibrated AI data privacy regulations-balancing\ninnovation incentives with the public interest can foster sustainable growth by\nbuilding trust and ensuring responsible data use, while excessive regulation\nmay risk stifling innovation and entrenching incumbents.",
    "published_date": "2025-04-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.ET"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.02000v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.00584v1",
    "title": "Enhancing Negation Awareness in Universal Text Embeddings: A Data-efficient and Computational-efficient Approach",
    "authors": [
      "Hongliu Cao"
    ],
    "author_ids": [],
    "abstract": "Negation plays an important role in various natural language processing tasks\nsuch as Natural Language Inference and Sentiment Analysis tasks. Numerous prior\nstudies have found that contextual text embedding models such as BERT, ELMO,\nRoBERTa or XLNet face challenges in accurately understanding negation. Recent\nadvancements in universal text embeddings have demonstrated superior\nperformance over contextual text embeddings in various tasks. However, due to\nthe bias in popular evaluation benchmarks, the negation awareness capacity of\nthese models remains unclear. To bridge the gap in existing literature, an\nin-depth analysis is initiated in this work to study the negation awareness of\ncutting-edge universal text embedding models. Our findings reveal a significant\nlack of negation awareness in these models, often interpreting negated text\npairs as semantically similar. To efficiently deal with the conflict that\ndifferent tasks need different trade-offs between topic and negation\ninformation among other semantic information, a data-efficient and\ncomputational-efficient embedding re-weighting method is proposed without\nmodifying the parameters of text embedding models. The proposed solution is\nable to improve text embedding models' negation awareness significantly on both\nsimple negation understanding task and complex negation understanding task.\nFurthermore, the proposed solution can also significantly improve the negation\nawareness of Large Language Model based task-specific high dimensional\nuniversal text embeddings.",
    "published_date": "2025-04-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.00584v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.00543v1",
    "title": "Generalization-aware Remote Sensing Change Detection via Domain-agnostic Learning",
    "authors": [
      "Qi Zang",
      "Shuang Wang",
      "Dong Zhao",
      "Dou Quan",
      "Yang Hu",
      "Licheng Jiao"
    ],
    "author_ids": [],
    "abstract": "Change detection has essential significance for the region's development, in\nwhich pseudo-changes between bitemporal images induced by imaging environmental\nfactors are key challenges. Existing transformation-based methods regard\npseudo-changes as a kind of style shift and alleviate it by transforming\nbitemporal images into the same style using generative adversarial networks\n(GANs). However, their efforts are limited by two drawbacks: 1) Transformed\nimages suffer from distortion that reduces feature discrimination. 2) Alignment\nhampers the model from learning domain-agnostic representations that degrades\nperformance on scenes with domain shifts from the training data. Therefore,\noriented from pseudo-changes caused by style differences, we present a\ngeneralizable domain-agnostic difference learning network (DonaNet). For the\ndrawback 1), we argue for local-level statistics as style proxies to assist\nagainst domain shifts. For the drawback 2), DonaNet learns domain-agnostic\nrepresentations by removing domain-specific style of encoded features and\nhighlighting the class characteristics of objects. In the removal, we propose a\ndomain difference removal module to reduce feature variance while preserving\ndiscriminative properties and propose its enhanced version to provide\npossibilities for eliminating more style by decorrelating the correlation\nbetween features. In the highlighting, we propose a cross-temporal\ngeneralization learning strategy to imitate latent domain shifts, thus enabling\nthe model to extract feature representations more robust to shifts actively.\nExtensive experiments conducted on three public datasets demonstrate that\nDonaNet outperforms existing state-of-the-art methods with a smaller model size\nand is more robust to domain shift.",
    "published_date": "2025-04-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.00543v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.00513v1",
    "title": "Leveraging LLMs for User Stories in AI Systems: UStAI Dataset",
    "authors": [
      "Asma Yamani",
      "Malak Baslyman",
      "Moataz Ahmed"
    ],
    "author_ids": [],
    "abstract": "AI systems are gaining widespread adoption across various sectors and\ndomains. Creating high-quality AI system requirements is crucial for aligning\nthe AI system with business goals and consumer values and for social\nresponsibility. However, with the uncertain nature of AI systems and the heavy\nreliance on sensitive data, more research is needed to address the elicitation\nand analysis of AI systems requirements. With the proprietary nature of many AI\nsystems, there is a lack of open-source requirements artifacts and technical\nrequirements documents for AI systems, limiting broader research and\ninvestigation. With Large Language Models (LLMs) emerging as a promising\nalternative to human-generated text, this paper investigates the potential use\nof LLMs to generate user stories for AI systems based on abstracts from\nscholarly papers. We conducted an empirical evaluation using three LLMs and\ngenerated $1260$ user stories from $42$ abstracts from $26$ domains. We assess\ntheir quality using the Quality User Story (QUS) framework. Moreover, we\nidentify relevant non-functional requirements (NFRs) and ethical principles.\nOur analysis demonstrates that the investigated LLMs can generate user stories\ninspired by the needs of various stakeholders, offering a promising approach\nfor generating user stories for research purposes and for aiding in the early\nrequirements elicitation phase of AI systems. We have compiled and curated a\ncollection of stories generated by various LLMs into a dataset (UStAI), which\nis now publicly available for use.",
    "published_date": "2025-04-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.00513v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.00487v2",
    "title": "FortisAVQA and MAVEN: a Benchmark Dataset and Debiasing Framework for Robust Multimodal Reasoning",
    "authors": [
      "Jie Ma",
      "Zhitao Gao",
      "Qi Chai",
      "Jun Liu",
      "Pinghui Wang",
      "Jing Tao",
      "Zhou Su"
    ],
    "author_ids": [],
    "abstract": "Audio-Visual Question Answering (AVQA) is a challenging multimodal reasoning\ntask requiring intelligent systems to answer natural language queries based on\npaired audio-video inputs accurately. However, existing AVQA approaches often\nsuffer from overfitting to dataset biases, leading to poor robustness.\nMoreover, current datasets may not effectively diagnose these methods. To\naddress these challenges, we first introduce a novel dataset, FortisAVQA,\nconstructed in two stages: (1) rephrasing questions in the test split of the\npublic MUSIC-AVQA dataset and (2) introducing distribution shifts across\nquestions. The first stage expands the test space with greater diversity, while\nthe second enables a refined robustness evaluation across rare, frequent, and\noverall question distributions. Second, we introduce a robust Multimodal\nAudio-Visual Epistemic Network (MAVEN) that leverages a multifaceted cycle\ncollaborative debiasing strategy to mitigate bias learning. Experimental\nresults demonstrate that our architecture achieves state-of-the-art performance\non FortisAVQA, with a notable improvement of 7.81\\%. Extensive ablation studies\non both datasets validate the effectiveness of our debiasing components.\nAdditionally, our evaluation reveals the limited robustness of existing\nmultimodal QA methods. We also verify the plug-and-play capability of our\nstrategy by integrating it with various baseline models across both datasets.\nOur dataset and code are available at https://github.com/reml-group/fortisavqa.",
    "published_date": "2025-04-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.MM",
      "cs.CL",
      "cs.CV",
      "H.5.1; I.2.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.00487v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.00454v1",
    "title": "FA^{3}-CLIP: Frequency-Aware Cues Fusion and Attack-Agnostic Prompt Learning for Unified Face Attack Detection",
    "authors": [
      "Yongze Li",
      "Ning Li",
      "Ajian Liu",
      "Hui Ma",
      "Liying Yang",
      "Xihong Chen",
      "Zhiyao Liang",
      "Yanyan Liang",
      "Jun Wan",
      "Zhen Lei"
    ],
    "author_ids": [],
    "abstract": "Facial recognition systems are vulnerable to physical (e.g., printed photos)\nand digital (e.g., DeepFake) face attacks. Existing methods struggle to\nsimultaneously detect physical and digital attacks due to: 1) significant\nintra-class variations between these attack types, and 2) the inadequacy of\nspatial information alone to comprehensively capture live and fake cues. To\naddress these issues, we propose a unified attack detection model termed\nFrequency-Aware and Attack-Agnostic CLIP (FA\\textsuperscript{3}-CLIP), which\nintroduces attack-agnostic prompt learning to express generic live and fake\ncues derived from the fusion of spatial and frequency features, enabling\nunified detection of live faces and all categories of attacks. Specifically,\nthe attack-agnostic prompt module generates generic live and fake prompts\nwithin the language branch to extract corresponding generic representations\nfrom both live and fake faces, guiding the model to learn a unified feature\nspace for unified attack detection. Meanwhile, the module adaptively generates\nthe live/fake conditional bias from the original spatial and frequency\ninformation to optimize the generic prompts accordingly, reducing the impact of\nintra-class variations. We further propose a dual-stream cues fusion framework\nin the vision branch, which leverages frequency information to complement\nsubtle cues that are difficult to capture in the spatial domain. In addition, a\nfrequency compression block is utilized in the frequency stream, which reduces\nredundancy in frequency features while preserving the diversity of crucial\ncues. We also establish new challenging protocols to facilitate unified face\nattack detection effectiveness. Experimental results demonstrate that the\nproposed method significantly improves performance in detecting physical and\ndigital face attacks, achieving state-of-the-art results.",
    "published_date": "2025-04-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.00454v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.00431v1",
    "title": "Enhancing Fundus Image-based Glaucoma Screening via Dynamic Global-Local Feature Integration",
    "authors": [
      "Yuzhuo Zhou",
      "Chi Liu",
      "Sheng Shen",
      "Siyu Le",
      "Liwen Yu",
      "Sihan Ouyang",
      "Zongyuan Ge"
    ],
    "author_ids": [],
    "abstract": "With the advancements in medical artificial intelligence (AI), fundus image\nclassifiers are increasingly being applied to assist in ophthalmic diagnosis.\nWhile existing classification models have achieved high accuracy on specific\nfundus datasets, they struggle to address real-world challenges such as\nvariations in image quality across different imaging devices, discrepancies\nbetween training and testing images across different racial groups, and the\nuncertain boundaries due to the characteristics of glaucomatous cases. In this\nstudy, we aim to address the above challenges posed by image variations by\nhighlighting the importance of incorporating comprehensive fundus image\ninformation, including the optic cup (OC) and optic disc (OD) regions, and\nother key image patches. Specifically, we propose a self-adaptive attention\nwindow that autonomously determines optimal boundaries for enhanced feature\nextraction. Additionally, we introduce a multi-head attention mechanism to\neffectively fuse global and local features via feature linear readout,\nimproving the model's discriminative capability. Experimental results\ndemonstrate that our method achieves superior accuracy and robustness in\nglaucoma classification.",
    "published_date": "2025-04-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.00431v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.00412v1",
    "title": "Form-Substance Discrimination: Concept, Cognition, and Pedagogy",
    "authors": [
      "Alexander M. Sidorkin"
    ],
    "author_ids": [],
    "abstract": "The skill to separate form from substance in writing has gained new\nprominence in the age of AI-generated content. The challenge - discriminating\nbetween fluent expression and substantive thought - constitutes a critical\nliteracy skill for modern education. This paper examines form-substance\ndiscrimination (FSD) as an essential learning outcome for curriculum\ndevelopment in higher education. We analyze its cognitive foundations in\nfluency bias and inhibitory control, trace its evolution from composition\ntheory concepts like \"higher-order concerns,\" and explore how readers progress\nfrom novice acceptance of polished text to expert critical assessment. Drawing\non research in cognitive psychology, composition studies, and emerging AI\npedagogy, we propose practical strategies for fostering this ability through\ncurriculum design, assessment practices, and explicit instruction. By\nprioritizing substance over surface in writing education, institutions can\nprepare students to navigate an information landscape where AI-generated\ncontent amplifies the ancient tension between style and meaning, ultimately\nsafeguarding the value of authentic human thought in knowledge construction and\ncommunication.",
    "published_date": "2025-04-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.00412v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.00388v1",
    "title": "Using complex prompts to identify fine-grained biases in image generation through ChatGPT-4o",
    "authors": [
      "Marinus Ferreira"
    ],
    "author_ids": [],
    "abstract": "There are not one but two dimensions of bias that can be revealed through the\nstudy of large AI models: not only bias in training data or the products of an\nAI, but also bias in society, such as disparity in employment or health\noutcomes between different demographic groups. Often training data and AI\noutput is biased for or against certain demographics (i.e. older white people\nare overrepresented in image datasets), but sometimes large AI models\naccurately illustrate biases in the real world (i.e. young black men being\ndisproportionately viewed as threatening). These social disparities often\nappear in image generation AI outputs in the form of 'marked' features, where\nsome feature of an individual or setting is a social marker of disparity, and\nprompts both humans and AI systems to treat subjects that are marked in this\nway as exceptional and requiring special treatment. Generative AI has proven to\nbe very sensitive to such marked features, to the extent of over-emphasising\nthem and thus often exacerbating social biases. I briefly discuss how we can\nuse complex prompts to image generation AI to investigate either dimension of\nbias, emphasising how we can probe the large language models underlying image\ngeneration AI through, for example, automated sentiment analysis of the text\nprompts used to generate images.",
    "published_date": "2025-04-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.LG",
      "K.4.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.00388v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.00343v1",
    "title": "Leveraging Large Language Models for Automated Definition Extraction with TaxoMatic A Case Study on Media Bias",
    "authors": [
      "Timo Spinde",
      "Luyang Lin",
      "Smi Hinterreiter",
      "Isao Echizen"
    ],
    "author_ids": [],
    "abstract": "This paper introduces TaxoMatic, a framework that leverages large language\nmodels to automate definition extraction from academic literature. Focusing on\nthe media bias domain, the framework encompasses data collection, LLM-based\nrelevance classification, and extraction of conceptual definitions. Evaluated\non a dataset of 2,398 manually rated articles, the study demonstrates the\nframeworks effectiveness, with Claude-3-sonnet achieving the best results in\nboth relevance classification and definition extraction. Future directions\ninclude expanding datasets and applying TaxoMatic to additional domains.",
    "published_date": "2025-04-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.00343v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.00310v1",
    "title": "Detecting and Mitigating Bias in LLMs through Knowledge Graph-Augmented Training",
    "authors": [
      "Rajeev Kumar",
      "Harishankar Kumar",
      "Kumari Shalini"
    ],
    "author_ids": [],
    "abstract": "Large language models have revolutionized natural language processing with\ntheir surprising capability to understand and generate human-like text.\nHowever, many of these models inherit and further amplify the biases present in\ntheir training data, raising ethical and fairness concerns. The detection and\nmitigation of such biases are vital to ensuring that LLMs act responsibly and\nequitably across diverse domains. This work investigates Knowledge\nGraph-Augmented Training (KGAT) as a novel method to mitigate bias in LLM.\nUsing structured domain-specific knowledge from real-world knowledge graphs, we\nimprove the understanding of the model and reduce biased output. Public\ndatasets for bias assessment include Gender Shades, Bias in Bios, and FairFace,\nwhile metrics such as demographic parity and equal opportunity facilitate\nrigorous detection. We also performed targeted mitigation strategies to correct\nbiased associations, leading to a significant drop in biased output and\nimproved bias metrics. Equipped with real-world datasets and knowledge graphs,\nour framework is both scalable and effective, paving the way toward responsible\ndeployment in sensitive and high-stakes applications.",
    "published_date": "2025-04-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.00310v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.00274v1",
    "title": "Text Chunking for Document Classification for Urban System Management using Large Language Models",
    "authors": [
      "Joshua Rodriguez",
      "Om Sanan",
      "Guillermo Vizarreta-Luna",
      "Steven A. Conrad"
    ],
    "author_ids": [],
    "abstract": "Urban systems are managed using complex textual documentation that need\ncoding and analysis to set requirements and evaluate built environment\nperformance. This paper contributes to the study of applying large-language\nmodels (LLM) to qualitative coding activities to reduce resource requirements\nwhile maintaining comparable reliability to humans. Qualitative coding and\nassessment face challenges like resource limitations and bias, accuracy, and\nconsistency between human evaluators. Here we report the application of LLMs to\ndeductively code 10 case documents on the presence of 17 digital twin\ncharacteristics for the management of urban systems. We utilize two prompting\nmethods to compare the semantic processing of LLMs with human coding efforts:\nwhole text analysis and text chunk analysis using OpenAI's GPT-4o, GPT-4o-mini,\nand o1-mini models. We found similar trends of internal variability between\nmethods and results indicate that LLMs may perform on par with human coders\nwhen initialized with specific deductive coding contexts. GPT-4o, o1-mini and\nGPT-4o-mini showed significant agreement with human raters when employed using\na chunking method. The application of both GPT-4o and GPT-4o-mini as an\nadditional rater with three manual raters showed statistically significant\nagreement across all raters, indicating that the analysis of textual documents\nis benefited by LLMs. Our findings reveal nuanced sub-themes of LLM application\nsuggesting LLMs follow human memory coding processes where whole-text analysis\nmay introduce multiple meanings. The novel contributions of this paper lie in\nassessing the performance of OpenAI GPT models and introduces the chunk-based\nprompting approach, which addresses context aggregation biases by preserving\nlocalized context.",
    "published_date": "2025-03-31T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.HC",
      "I.7.5; J.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.00274v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.00241v1",
    "title": "Synthesizing Public Opinions with LLMs: Role Creation, Impacts, and the Future to eDemorcacy",
    "authors": [
      "Rabimba Karanjai",
      "Boris Shor",
      "Amanda Austin",
      "Ryan Kennedy",
      "Yang Lu",
      "Lei Xu",
      "Weidong Shi"
    ],
    "author_ids": [],
    "abstract": "This paper investigates the use of Large Language Models (LLMs) to synthesize\npublic opinion data, addressing challenges in traditional survey methods like\ndeclining response rates and non-response bias. We introduce a novel technique:\nrole creation based on knowledge injection, a form of in-context learning that\nleverages RAG and specified personality profiles from the HEXACO model and\ndemographic information, and uses that for dynamically generated prompts. This\nmethod allows LLMs to simulate diverse opinions more accurately than existing\nprompt engineering approaches. We compare our results with pre-trained models\nwith standard few-shot prompts. Experiments using questions from the\nCooperative Election Study (CES) demonstrate that our role-creation approach\nsignificantly improves the alignment of LLM-generated opinions with real-world\nhuman survey responses, increasing answer adherence. In addition, we discuss\nchallenges, limitations and future research directions.",
    "published_date": "2025-03-31T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.00241v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.00176v2",
    "title": "Discriminative Subspace Emersion from learning feature relevances across different populations",
    "authors": [
      "Marco Canducci",
      "Lida Abdi",
      "Alessandro Prete",
      "Roland J. Veen",
      "Michael Biehl",
      "Wiebke Arlt",
      "Peter Tino"
    ],
    "author_ids": [],
    "abstract": "In a given classification task, the accuracy of the learner is often hampered\nby finiteness of the training set, high-dimensionality of the feature space and\nsevere overlap between classes. In the context of interpretable learners, with\n(piecewise) linear separation boundaries, these issues can be mitigated by\ncareful construction of optimization procedures and/or estimation of relevant\nfeatures for the task. However, when the task is shared across two disjoint\npopulations the main interest is shifted towards estimating a set of features\nthat discriminate the most between the two, when performing classification. We\npropose a new Discriminative Subspace Emersion (DSE) method to extend subspace\nlearning toward a general relevance learning framework. DSE allows us to\nidentify the most relevant features in distinguishing the classification task\nacross two populations, even in cases of high overlap between classes. The\nproposed methodology is designed to work with multiple sets of labels and is\nderived in principle without being tied to a specific choice of base learner.\nTheoretical and empirical investigations over synthetic and real-world datasets\nindicate that DSE accurately identifies a common subspace for the\nclassification across different populations. This is shown to be true for a\nsurprisingly high degree of overlap between classes.",
    "published_date": "2025-03-31T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "68, 92",
      "I.2.1; I.5.2; I.5.5; J.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.00176v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.00174v1",
    "title": "MetaCLBench: Meta Continual Learning Benchmark on Resource-Constrained Edge Devices",
    "authors": [
      "Sijia Li",
      "Young D. Kwon",
      "Lik-Hang Lee",
      "Pan Hui"
    ],
    "author_ids": [],
    "abstract": "Meta-Continual Learning (Meta-CL) has emerged as a promising approach to\nminimize manual labeling efforts and system resource requirements by enabling\nContinual Learning (CL) with limited labeled samples. However, while existing\nmethods have shown success in image-based tasks, their effectiveness remains\nunexplored for sequential time-series data from sensor systems, particularly\naudio inputs. To address this gap, we conduct a comprehensive benchmark study\nevaluating six representative Meta-CL approaches using three network\narchitectures on five datasets from both image and audio modalities. We develop\nMetaCLBench, an end-to-end Meta-CL benchmark framework for edge devices to\nevaluate system overheads and investigate trade-offs among performance,\ncomputational costs, and memory requirements across various Meta-CL methods.\nOur results reveal that while many Meta-CL methods enable to learn new classes\nfor both image and audio modalities, they impose significant computational and\nmemory costs on edge devices. Also, we find that pre-training and meta-training\nprocedures based on source data before deployment improve Meta-CL performance.\nFinally, to facilitate further research, we provide practical guidelines for\nresearchers and machine learning practitioners implementing Meta-CL on\nresource-constrained environments and make our benchmark framework and tools\npublicly available, enabling fair evaluation across both accuracy and\nsystem-level metrics.",
    "published_date": "2025-03-31T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.00174v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.00150v1",
    "title": "Few-Shot Generation of Brain Tumors for Secure and Fair Data Sharing",
    "authors": [
      "Yongyi Shi",
      "Ge Wang"
    ],
    "author_ids": [],
    "abstract": "Leveraging multi-center data for medical analytics presents challenges due to\nprivacy concerns and data heterogeneity. While distributed approaches such as\nfederated learning has gained traction, they remain vulnerable to privacy\nbreaches, particularly in sensitive domains like medical imaging. Generative\nmodels, such as diffusion models, enhance privacy by synthesizing realistic\ndata. However, they are prone to memorization, especially when trained on small\ndatasets. This study proposes a decentralized few-shot generative model (DFGM)\nto synthesize brain tumor images while fully preserving privacy. DFGM\nharmonizes private tumor data with publicly shareable healthy images from\nmultiple medical centers, constructing a new dataset by blending tumor\nforegrounds with healthy backgrounds. This approach ensures stringent privacy\nprotection and enables controllable, high-quality synthesis by preserving both\nthe healthy backgrounds and tumor foregrounds. We assess DFGM's effectiveness\nin brain tumor segmentation using a UNet, achieving Dice score improvements of\n3.9% for data augmentation and 4.6% for fairness on a separate dataset.",
    "published_date": "2025-03-31T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.00150v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.01990v1",
    "title": "Advances and Challenges in Foundation Agents: From Brain-Inspired Intelligence to Evolutionary, Collaborative, and Safe Systems",
    "authors": [
      "Bang Liu",
      "Xinfeng Li",
      "Jiayi Zhang",
      "Jinlin Wang",
      "Tanjin He",
      "Sirui Hong",
      "Hongzhang Liu",
      "Shaokun Zhang",
      "Kaitao Song",
      "Kunlun Zhu",
      "Yuheng Cheng",
      "Suyuchen Wang",
      "Xiaoqiang Wang",
      "Yuyu Luo",
      "Haibo Jin",
      "Peiyan Zhang",
      "Ollie Liu",
      "Jiaqi Chen",
      "Huan Zhang",
      "Zhaoyang Yu",
      "Haochen Shi",
      "Boyan Li",
      "Dekun Wu",
      "Fengwei Teng",
      "Xiaojun Jia",
      "Jiawei Xu",
      "Jinyu Xiang",
      "Yizhang Lin",
      "Tianming Liu",
      "Tongliang Liu",
      "Yu Su",
      "Huan Sun",
      "Glen Berseth",
      "Jianyun Nie",
      "Ian Foster",
      "Logan Ward",
      "Qingyun Wu",
      "Yu Gu",
      "Mingchen Zhuge",
      "Xiangru Tang",
      "Haohan Wang",
      "Jiaxuan You",
      "Chi Wang",
      "Jian Pei",
      "Qiang Yang",
      "Xiaoliang Qi",
      "Chenglin Wu"
    ],
    "author_ids": [],
    "abstract": "The advent of large language models (LLMs) has catalyzed a transformative\nshift in artificial intelligence, paving the way for advanced intelligent\nagents capable of sophisticated reasoning, robust perception, and versatile\naction across diverse domains. As these agents increasingly drive AI research\nand practical applications, their design, evaluation, and continuous\nimprovement present intricate, multifaceted challenges. This survey provides a\ncomprehensive overview, framing intelligent agents within a modular,\nbrain-inspired architecture that integrates principles from cognitive science,\nneuroscience, and computational research. We structure our exploration into\nfour interconnected parts. First, we delve into the modular foundation of\nintelligent agents, systematically mapping their cognitive, perceptual, and\noperational modules onto analogous human brain functionalities, and elucidating\ncore components such as memory, world modeling, reward processing, and\nemotion-like systems. Second, we discuss self-enhancement and adaptive\nevolution mechanisms, exploring how agents autonomously refine their\ncapabilities, adapt to dynamic environments, and achieve continual learning\nthrough automated optimization paradigms, including emerging AutoML and\nLLM-driven optimization strategies. Third, we examine collaborative and\nevolutionary multi-agent systems, investigating the collective intelligence\nemerging from agent interactions, cooperation, and societal structures,\nhighlighting parallels to human social dynamics. Finally, we address the\ncritical imperative of building safe, secure, and beneficial AI systems,\nemphasizing intrinsic and extrinsic security threats, ethical alignment,\nrobustness, and practical mitigation strategies necessary for trustworthy\nreal-world deployment.",
    "published_date": "2025-03-31T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.01990v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.24310v1",
    "title": "BEATS: Bias Evaluation and Assessment Test Suite for Large Language Models",
    "authors": [
      "Alok Abhishek",
      "Lisa Erickson",
      "Tushar Bandopadhyay"
    ],
    "author_ids": [],
    "abstract": "In this research, we introduce BEATS, a novel framework for evaluating Bias,\nEthics, Fairness, and Factuality in Large Language Models (LLMs). Building upon\nthe BEATS framework, we present a bias benchmark for LLMs that measure\nperformance across 29 distinct metrics. These metrics span a broad range of\ncharacteristics, including demographic, cognitive, and social biases, as well\nas measures of ethical reasoning, group fairness, and factuality related\nmisinformation risk. These metrics enable a quantitative assessment of the\nextent to which LLM generated responses may perpetuate societal prejudices that\nreinforce or expand systemic inequities. To achieve a high score on this\nbenchmark a LLM must show very equitable behavior in their responses, making it\na rigorous standard for responsible AI evaluation. Empirical results based on\ndata from our experiment show that, 37.65\\% of outputs generated by industry\nleading models contained some form of bias, highlighting a substantial risk of\nusing these models in critical decision making systems. BEATS framework and\nbenchmark offer a scalable and statistically rigorous methodology to benchmark\nLLMs, diagnose factors driving biases, and develop mitigation strategies. With\nthe BEATS framework, our goal is to help the development of more socially\nresponsible and ethically aligned AI models.",
    "published_date": "2025-03-31T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "68T01 (Primary), 68T50 (Secondary)",
      "I.2.0; I.2.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.24310v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.24228v1",
    "title": "PAARS: Persona Aligned Agentic Retail Shoppers",
    "authors": [
      "Saab Mansour",
      "Leonardo Perelli",
      "Lorenzo Mainetti",
      "George Davidson",
      "Stefano D'Amato"
    ],
    "author_ids": [],
    "abstract": "In e-commerce, behavioral data is collected for decision making which can be\ncostly and slow. Simulation with LLM powered agents is emerging as a promising\nalternative for representing human population behavior. However, LLMs are known\nto exhibit certain biases, such as brand bias, review rating bias and limited\nrepresentation of certain groups in the population, hence they need to be\ncarefully benchmarked and aligned to user behavior. Ultimately, our goal is to\nsynthesise an agent population and verify that it collectively approximates a\nreal sample of humans. To this end, we propose a framework that: (i) creates\nsynthetic shopping agents by automatically mining personas from anonymised\nhistorical shopping data, (ii) equips agents with retail-specific tools to\nsynthesise shopping sessions and (iii) introduces a novel alignment suite\nmeasuring distributional differences between humans and shopping agents at the\ngroup (i.e. population) level rather than the traditional \"individual\" level.\nExperimental results demonstrate that using personas improves performance on\nthe alignment suite, though a gap remains to human behaviour. We showcase an\ninitial application of our framework for automated agentic A/B testing and\ncompare the findings to human results. Finally, we discuss applications,\nlimitations and challenges setting the stage for impactful future work.",
    "published_date": "2025-03-31T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.24228v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.24135v1",
    "title": "PixelCAM: Pixel Class Activation Mapping for Histology Image Classification and ROI Localization",
    "authors": [
      "Alexis Guichemerre",
      "Soufiane Belharbi",
      "Mohammadhadi Shateri",
      "Luke McCaffrey",
      "Eric Granger"
    ],
    "author_ids": [],
    "abstract": "Weakly supervised object localization (WSOL) methods allow training models to\nclassify images and localize ROIs. WSOL only requires low-cost image-class\nannotations yet provides a visually interpretable classifier, which is\nimportant in histology image analysis. Standard WSOL methods rely on class\nactivation mapping (CAM) methods to produce spatial localization maps according\nto a single- or two-step strategy. While both strategies have made significant\nprogress, they still face several limitations with histology images.\nSingle-step methods can easily result in under- or over-activation due to the\nlimited visual ROI saliency in histology images and the limited localization\ncues. They also face the well-known issue of asynchronous convergence between\nclassification and localization tasks. The two-step approach is sub-optimal\nbecause it is tied to a frozen classifier, limiting the capacity for\nlocalization. Moreover, these methods also struggle when applied to\nout-of-distribution (OOD) datasets. In this paper, a multi-task approach for\nWSOL is introduced for simultaneous training of both tasks to address the\nasynchronous convergence problem. In particular, localization is performed in\nthe pixel-feature space of an image encoder that is shared with classification.\nThis allows learning discriminant features and accurate delineation of\nforeground/background regions to support ROI localization and image\nclassification. We propose PixelCAM, a cost-effective foreground/background\npixel-wise classifier in the pixel-feature space that allows for spatial object\nlocalization. PixelCAM is trained using pixel pseudo-labels collected from a\npretrained WSOL model. Both image and pixel-wise classifiers are trained\nsimultaneously using standard gradient descent. In addition, our pixel\nclassifier can easily be integrated into CNN- and transformer-based\narchitectures without any modifications.",
    "published_date": "2025-03-31T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.24135v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.24119v1",
    "title": "Measuring User Experience Through Speech Analysis: Insights from HCI Interviews",
    "authors": [
      "Yong Ma",
      "Xuedong Zhang",
      "Yuchong Zhang",
      "Morten Fjeld"
    ],
    "author_ids": [],
    "abstract": "User satisfaction plays a crucial role in user experience (UX) evaluation.\nTraditionally, UX measurements are based on subjective scales, such as\nquestionnaires. However, these evaluations may suffer from subjective bias. In\nthis paper, we explore the acoustic and prosodic features of speech to\ndifferentiate between positive and neutral UX during interactive sessions. By\nanalyzing speech features such as root-mean-square (RMS), zero-crossing\nrate(ZCR), jitter, and shimmer, we identified significant differences between\nthe positive and neutral user groups. In addition, social speech features such\nas activity and engagement also show notable variations between these groups.\nOur findings underscore the potential of speech analysis as an objective and\nreliable tool for UX measurement, contributing to more robust and\nbias-resistant evaluation methodologies. This work offers a novel approach to\nintegrating speech features into UX evaluation and opens avenues for further\nresearch in HCI.",
    "published_date": "2025-03-31T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.24119v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.24081v1",
    "title": "Cell-Free Massive MIMO Under Mobility: A Fairness-Differentiated Handover Scheme",
    "authors": [
      "Yunlu Xiao",
      "Marina Petrova",
      "Ljiljana Simić"
    ],
    "author_ids": [],
    "abstract": "While cell-free massive MIMO (CF-mMIMO) offers both uniform and high\nnetwork-wide throughput in static networks, its performance in a mobile network\nis not yet fully addressed. In this paper, we evaluate the performance of a\nmobile CF-mMIMO network under a comprehensive throughput model and show that it\nsuffers from large performance degradation due to the combined effect of\nchannel aging and handover delay. To improve the performance of CF-mMIMO under\nmobility, we propose a fairness-differentiated handover scheme. Our scheme\ndifferentiates the handover policy for different users by their channel\nconditions compared to a threshold based on Jain's fairness index, in order to\nprioritize handovers for the poorly served users. We present an extensive\nevaluation of the mobile throughput performance of our handover scheme with\nrealistic urban network distributions and UE mobility patterns. Our results\nshow that our scheme significantly outperforms the existing literature\nbenchmarks when considering both channel aging and handover delay cost.\nImportantly, the advantage of UE-centric over network-centric CF-mMIMO, of\nuniformly good performance over the network, is uniquely preserved under\nmobility by our handover scheme. We thus show that CF-mMIMO can be a feasible\narchitecture for practical mobile networks.",
    "published_date": "2025-03-31T00:00:00",
    "year": 2025,
    "categories": [
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.24081v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.00060v1",
    "title": "CF-CAM: Gradient Perturbation Mitigation and Feature Stabilization for Reliable Interpretability",
    "authors": [
      "Hongjie He",
      "Xu Pan",
      "Yudong Yao"
    ],
    "author_ids": [],
    "abstract": "As deep learning continues to advance, the opacity of neural network\ndecision-making remains a critical challenge, limiting trust and applicability\nin high-stakes domains. Class Activation Mapping (CAM) techniques have emerged\nas a key approach to visualizing model decisions, yet existing methods face\ninherent trade-offs. Gradient-based CAM variants suffer from sensitivity to\ngradient perturbations, leading to unstable and unreliable explanations.\nConversely, gradient-free approaches mitigate gradient instability but incur\nsignificant computational overhead and inference latency. To address these\nlimitations, we propose Cluster Filter Class Activation Map (CF-CAM), a novel\nframework that reintroduces gradient-based weighting while enhancing robustness\nagainst gradient noise. CF-CAM employs a hierarchical importance weighting\nstrategy to balance discriminative feature preservation and noise elimination.\nA density-aware channel clustering via Density-Based Spatial Clustering of\nApplications with Noise (DBSCAN) groups semantically relevant feature channels\nand discard noise-prone activations. Additionally, cluster-conditioned gradient\nfiltering leverages bilateral filters to refine gradient signals, preserving\nedge-aware localization while suppressing noise impact. Experiment results\ndemonstrate that CF-CAM achieves superior interpretability performance while\nmaintaining resilience to gradient perturbations, outperforming\nstate-of-the-art CAM methods in faithfulness and robustness. By effectively\nmitigating gradient instability without excessive computational cost, CF-CAM\nprovides a reliable solution for enhancing the interpretability of deep neural\nnetworks in critical applications such as medical diagnosis and autonomous\ndriving.",
    "published_date": "2025-03-31T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.00060v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.23979v1",
    "title": "The more the merrier: logical and multistage processors in credit scoring",
    "authors": [
      "Arturo Pérez-Peralta",
      "Sandra Benítez-Peña",
      "Rosa E. Lillo"
    ],
    "author_ids": [],
    "abstract": "Machine Learning algorithms are ubiquitous in key decision-making contexts\nsuch as organizational justice or healthcare, which has spawned a great demand\nfor fairness in these procedures. In this paper we focus on the application of\nfair ML in finance, more concretely on the use of fairness techniques on credit\nscoring. This paper makes two contributions. On the one hand, it addresses the\nexistent gap concerning the application of established methods in the\nliterature to the case of multiple sensitive variables through the use of a new\ntechnique called logical processors (LP). On the other hand, it also explores\nthe novel method of multistage processors (MP) to investigate whether the\ncombination of fairness methods can work synergistically to produce solutions\nwith improved fairness or accuracy. Furthermore, we examine the intersection of\nthese two lines of research by exploring the integration of fairness methods in\nthe multivariate case. The results are very promising and suggest that logical\nprocessors are an appropriate way of handling multiple sensitive variables.\nFurthermore, multistage processors are capable of improving the performance of\nexisting methods.",
    "published_date": "2025-03-31T00:00:00",
    "year": 2025,
    "categories": [
      "stat.ML",
      "cs.LG",
      "68T05, 91D30, 68T37"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.23979v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.23893v1",
    "title": "DiffScale: Continuous Downscaling and Bias Correction of Subseasonal Wind Speed Forecasts using Diffusion Models",
    "authors": [
      "Maximilian Springenberg",
      "Noelia Otero",
      "Yuxin Xue",
      "Jackie Ma"
    ],
    "author_ids": [],
    "abstract": "Renewable resources are strongly dependent on local and large-scale weather\nsituations. Skillful subseasonal to seasonal (S2S) forecasts -- beyond two\nweeks and up to two months -- can offer significant socioeconomic advantages to\nthe energy sector. This study aims to enhance wind speed predictions using a\ndiffusion model with classifier-free guidance to downscale S2S forecasts of\nsurface wind speed. We propose DiffScale, a diffusion model that super-resolves\nspatial information for continuous downscaling factors and lead times.\nLeveraging weather priors as guidance for the generative process of diffusion\nmodels, we adopt the perspective of conditional probabilities on sampling\nsuper-resolved S2S forecasts. We aim to directly estimate the density\nassociated with the target S2S forecasts at different spatial resolutions and\nlead times without auto-regression or sequence prediction, resulting in an\nefficient and flexible model. Synthetic experiments were designed to\nsuper-resolve wind speed S2S forecasts from the European Center for\nMedium-Range Weather Forecast (ECMWF) from a coarse resolution to a finer\nresolution of ERA5 reanalysis data, which serves as a high-resolution target.\nThe innovative aspect of DiffScale lies in its flexibility to downscale\narbitrary scaling factors, enabling it to generalize across various grid\nresolutions and lead times -without retraining the model- while correcting\nmodel errors, making it a versatile tool for improving S2S wind speed\nforecasts. We achieve a significant improvement in prediction quality,\noutperforming baselines up to week 3.",
    "published_date": "2025-03-31T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.23893v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.23819v1",
    "title": "Conformal uncertainty quantification to evaluate predictive fairness of foundation AI model for skin lesion classes across patient demographics",
    "authors": [
      "Swarnava Bhattacharyya",
      "Umapada Pal",
      "Tapabrata Chakraborti"
    ],
    "author_ids": [],
    "abstract": "Deep learning based diagnostic AI systems based on medical images are\nstarting to provide similar performance as human experts. However these data\nhungry complex systems are inherently black boxes and therefore slow to be\nadopted for high risk applications like healthcare. This problem of lack of\ntransparency is exacerbated in the case of recent large foundation models,\nwhich are trained in a self supervised manner on millions of data points to\nprovide robust generalisation across a range of downstream tasks, but the\nembeddings generated from them happen through a process that is not\ninterpretable, and hence not easily trustable for clinical applications. To\naddress this timely issue, we deploy conformal analysis to quantify the\npredictive uncertainty of a vision transformer (ViT) based foundation model\nacross patient demographics with respect to sex, age and ethnicity for the\ntasks of skin lesion classification using several public benchmark datasets.\nThe significant advantage of this method is that conformal analysis is method\nindependent and it not only provides a coverage guarantee at population level\nbut also provides an uncertainty score for each individual. We used a\nmodel-agnostic dynamic F1-score-based sampling during model training, which\nhelped to stabilize the class imbalance and we investigate the effects on\nuncertainty quantification (UQ) with or without this bias mitigation step. Thus\nwe show how this can be used as a fairness metric to evaluate the robustness of\nthe feature embeddings of the foundation model (Google DermFoundation) and thus\nadvance the trustworthiness and fairness of clinical AI.",
    "published_date": "2025-03-31T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.23819v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.23688v1",
    "title": "Mapping Geopolitical Bias in 11 Large Language Models: A Bilingual, Dual-Framing Analysis of U.S.-China Tensions",
    "authors": [
      "William Guey",
      "Pierrick Bougault",
      "Vitor D. de Moura",
      "Wei Zhang",
      "Jose O. Gomes"
    ],
    "author_ids": [],
    "abstract": "This study systematically analyzes geopolitical bias across 11 prominent\nLarge Language Models (LLMs) by examining their responses to seven critical\ntopics in U.S.-China relations. Utilizing a bilingual (English and Chinese) and\ndual-framing (affirmative and reverse) methodology, we generated 19,712 prompts\ndesigned to detect ideological leanings in model outputs. Responses were\nquantitatively assessed on a normalized scale from -2 (strongly Pro-China) to\n+2 (strongly Pro-U.S.) and categorized according to stance, neutrality, and\nrefusal rates. The findings demonstrate significant and consistent ideological\nalignments correlated with the LLMs' geographic origins; U.S.-based models\npredominantly favored Pro-U.S. stances, while Chinese-origin models exhibited\npronounced Pro-China biases. Notably, language and prompt framing substantially\ninfluenced model responses, with several LLMs exhibiting stance reversals based\non prompt polarity or linguistic context. Additionally, we introduced\ncomprehensive metrics to evaluate response consistency across languages and\nframing conditions, identifying variability and vulnerabilities in model\nbehaviors. These results offer practical insights that can guide organizations\nand individuals in selecting LLMs best aligned with their operational\npriorities and geopolitical considerations, underscoring the importance of\ncareful model evaluation in politically sensitive applications. Furthermore,\nthe research highlights specific prompt structures and linguistic variations\nthat can strategically trigger distinct responses from models, revealing\nmethods for effectively navigating and influencing LLM outputs.",
    "published_date": "2025-03-31T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.23688v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.13871v1",
    "title": "Human aversion? Do AI Agents Judge Identity More Harshly Than Performance",
    "authors": [
      "Yuanjun Feng",
      "Vivek Chodhary",
      "Yash Raj Shrestha"
    ],
    "author_ids": [],
    "abstract": "This study examines the understudied role of algorithmic evaluation of human\njudgment in hybrid decision-making systems, a critical gap in management\nresearch. While extant literature focuses on human reluctance to follow\nalgorithmic advice, we reverse the perspective by investigating how AI agents\nbased on large language models (LLMs) assess and integrate human input. Our\nwork addresses a pressing managerial constraint: firms barred from deploying\nLLMs directly due to privacy concerns can still leverage them as mediating\ntools (for instance, anonymized outputs or decision pipelines) to guide\nhigh-stakes choices like pricing or discounts without exposing proprietary\ndata. Through a controlled prediction task, we analyze how an LLM-based AI\nagent weights human versus algorithmic predictions. We find that the AI system\nsystematically discounts human advice, penalizing human errors more severely\nthan algorithmic errors--a bias exacerbated when the agent's identity (human vs\nAI) is disclosed and the human is positioned second. These results reveal a\ndisconnect between AI-generated trust metrics and the actual influence of human\njudgment, challenging assumptions about equitable human-AI collaboration. Our\nfindings offer three key contributions. First, we identify a reverse algorithm\naversion phenomenon, where AI agents undervalue human input despite comparable\nerror rates. Second, we demonstrate how disclosure and positional bias interact\nto amplify this effect, with implications for system design. Third, we provide\na framework for indirect LLM deployment that balances predictive power with\ndata privacy. For practitioners, this research emphasize the need to audit AI\nweighting mechanisms, calibrate trust dynamics, and strategically design\ndecision sequences in human-AI systems.",
    "published_date": "2025-03-31T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.AI",
      "econ.GN",
      "q-fin.EC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.13871v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.23641v1",
    "title": "Remarks on the Polyak-Lojasiewicz inequality and the convergence of gradient systems",
    "authors": [
      "Arthur Castello B. de Oliveira",
      "Leilei Cui",
      "Eduardo D. Sontag"
    ],
    "author_ids": [],
    "abstract": "This work explores generalizations of the Polyak-Lojasiewicz inequality (PLI)\nand their implications for the convergence behavior of gradient flows in\noptimization problems. Motivated by the continuous-time linear quadratic\nregulator (CT-LQR) policy optimization problem -- where only a weaker version\nof the PLI is characterized in the literature -- this work shows that while\nweaker conditions are sufficient for global convergence to, and optimality of\nthe set of critical points of the cost function, the \"profile\" of the gradient\nflow solution can change significantly depending on which \"flavor\" of\ninequality the cost satisfies. After a general theoretical analysis, we focus\non fitting the CT-LQR policy optimization problem to the proposed framework,\nshowing that, in fact, it can never satisfy a PLI in its strongest form. We\nfollow up our analysis with a brief discussion on the difference between\ncontinuous- and discrete-time LQR policy optimization, and end the paper with\nsome intuition on the extension of this framework to optimization problems with\nL1 regularization and solved through proximal gradient flows.",
    "published_date": "2025-03-31T00:00:00",
    "year": 2025,
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.23641v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.23630v1",
    "title": "Finding Interest Needle in Popularity Haystack: Improving Retrieval by Modeling Item Exposure",
    "authors": [
      "Amit Jaspal",
      "Rahul Agarwal"
    ],
    "author_ids": [],
    "abstract": "Recommender systems operate in closed feedback loops, where user interactions\nreinforce popularity bias, leading to over-recommendation of already popular\nitems while under-exposing niche or novel content. Existing bias mitigation\nmethods, such as Inverse Propensity Scoring (IPS) and Off- Policy Correction\n(OPC), primarily operate at the ranking stage or during training, lacking\nexplicit real-time control over exposure dynamics. In this work, we introduce\nan exposure- aware retrieval scoring approach, which explicitly models item\nexposure probability and adjusts retrieval-stage ranking at inference time.\nUnlike prior work, this method decouples exposure effects from engagement\nlikelihood, enabling controlled trade-offs between fairness and engagement in\nlarge-scale recommendation platforms. We validate our approach through online\nA/B experiments in a real-world video recommendation system, demonstrating a\n25% increase in uniquely retrieved items and a 40% reduction in the dominance\nof over-popular content, all while maintaining overall user engagement levels.\nOur results establish a scalable, deployable solution for mitigating popularity\nbias at the retrieval stage, offering a new paradigm for bias-aware\npersonalization.",
    "published_date": "2025-03-31T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.23630v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.23622v1",
    "title": "Beyond Detection: Designing AI-Resilient Assessments with Automated Feedback Tool to Foster Critical Thinking",
    "authors": [
      "Muhammad Sajjad Akbar"
    ],
    "author_ids": [],
    "abstract": "The growing use of generative AI tools like ChatGPT has raised urgent\nconcerns about their impact on student learning, particularly the potential\nerosion of critical thinking and creativity. As students increasingly turn to\nthese tools to complete assessments, foundational cognitive skills are at risk\nof being bypassed, challenging the integrity of higher education and the\nauthenticity of student work. Existing AI-generated text detection tools are\ninadequate; they produce unreliable outputs and are prone to both false\npositives and false negatives, especially when students apply paraphrasing,\ntranslation, or rewording. These systems rely on shallow statistical patterns\nrather than true contextual or semantic understanding, making them unsuitable\nas definitive indicators of AI misuse. In response, this research proposes a\nproactive, AI-resilient solution based on assessment design rather than\ndetection. It introduces a web-based Python tool that integrates Bloom's\nTaxonomy with advanced natural language processing techniques including GPT-3.5\nTurbo, BERT-based semantic similarity, and TF-IDF metrics to evaluate the\nAI-solvability of assessment tasks. By analyzing surface-level and semantic\nfeatures, the tool helps educators determine whether a task targets lower-order\nthinking such as recall and summarization or higher-order skills such as\nanalysis, evaluation, and creation, which are more resistant to AI automation.\nThis framework empowers educators to design cognitively demanding, AI-resistant\nassessments that promote originality, critical thinking, and fairness. It\noffers a sustainable, pedagogically sound strategy to foster authentic learning\nand uphold academic standards in the age of AI.",
    "published_date": "2025-03-30T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.23622v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.23566v1",
    "title": "When LLM Therapists Become Salespeople: Evaluating Large Language Models for Ethical Motivational Interviewing",
    "authors": [
      "Haein Kong",
      "Seonghyeon Moon"
    ],
    "author_ids": [],
    "abstract": "Large language models (LLMs) have been actively applied in the mental health\nfield. Recent research shows the promise of LLMs in applying psychotherapy,\nespecially motivational interviewing (MI). However, there is a lack of studies\ninvestigating how language models understand MI ethics. Given the risks that\nmalicious actors can use language models to apply MI for unethical purposes, it\nis important to evaluate their capability of differentiating ethical and\nunethical MI practices. Thus, this study investigates the ethical awareness of\nLLMs in MI with multiple experiments. Our findings show that LLMs have a\nmoderate to strong level of knowledge in MI. However, their ethical standards\nare not aligned with the MI spirit, as they generated unethical responses and\nperformed poorly in detecting unethical responses. We proposed a Chain-of-Ethic\nprompt to mitigate those risks and improve safety. Finally, our proposed\nstrategy effectively improved ethical MI response generation and detection\nperformance. These findings highlight the need for safety evaluations and\nguidelines for building ethical LLM-powered psychotherapy.",
    "published_date": "2025-03-30T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.23566v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.23533v1",
    "title": "To See or Not to See: A Privacy Threat Model for Digital Forensics in Crime Investigation",
    "authors": [
      "Mario Raciti",
      "Simone Di Mauro",
      "Dimitri Van Landuyt",
      "Giampaolo Bella"
    ],
    "author_ids": [],
    "abstract": "Digital forensics is a cornerstone of modern crime investigations, yet it\nraises significant privacy concerns due to the collection, processing, and\nstorage of digital evidence. Despite that, privacy threats in digital forensics\ncrime investigations often remain underexplored, thereby leading to potential\ngaps in forensic practices and regulatory compliance, which may then escalate\ninto harming the freedoms of natural persons. With this clear motivation, the\npresent paper applies the SPADA methodology for threat modelling with the goal\nof incorporating privacy-oriented threat modelling in digital forensics. As a\nresult, we identify a total of 298 privacy threats that may affect digital\nforensics processes through crime investigations. Furthermore, we demonstrate\nan unexplored feature on how SPADA assists in handling domain-dependency during\nthreat elicitation. This yields a second list of privacy threats that are\nuniversally applicable to any domain. We then present a comprehensive and\nsystematic privacy threat model for digital forensics in crime investigation.\nMoreover, we discuss some of the challenges about validating privacy threats in\nthis domain, particularly given the variability of legal frameworks across\njurisdictions. We ultimately propose our privacy threat model as a tool for\nensuring ethical and legally compliant investigative practices.",
    "published_date": "2025-03-30T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.23533v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.23486v1",
    "title": "A Systematic Decade Review of Trip Route Planning with Travel Time Estimation based on User Preferences and Behavior",
    "authors": [
      "Nikil Jayasuriya",
      "Deshan Sumanathilaka"
    ],
    "author_ids": [],
    "abstract": "This paper systematically explores the advancements in adaptive trip route\nplanning and travel time estimation (TTE) through Artificial Intelligence (AI).\nWith the increasing complexity of urban transportation systems, traditional\nnavigation methods often struggle to accommodate dynamic user preferences,\nreal-time traffic conditions, and scalability requirements. This study explores\nthe contributions of established AI techniques, including Machine Learning\n(ML), Reinforcement Learning (RL), and Graph Neural Networks (GNNs), alongside\nemerging methodologies like Meta-Learning, Explainable AI (XAI), Generative AI,\nand Federated Learning. In addition to highlighting these innovations, the\npaper identifies critical challenges such as ethical concerns, computational\nscalability, and effective data integration, which must be addressed to advance\nthe field. The paper concludes with recommendations for leveraging AI to build\nefficient, transparent, and sustainable navigation systems.",
    "published_date": "2025-03-30T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.23486v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.23434v1",
    "title": "Towards Trustworthy GUI Agents: A Survey",
    "authors": [
      "Yucheng Shi",
      "Wenhao Yu",
      "Wenlin Yao",
      "Wenhu Chen",
      "Ninghao Liu"
    ],
    "author_ids": [],
    "abstract": "GUI agents, powered by large foundation models, can interact with digital\ninterfaces, enabling various applications in web automation, mobile navigation,\nand software testing. However, their increasing autonomy has raised critical\nconcerns about their security, privacy, and safety. This survey examines the\ntrustworthiness of GUI agents in five critical dimensions: security\nvulnerabilities, reliability in dynamic environments, transparency and\nexplainability, ethical considerations, and evaluation methodologies. We also\nidentify major challenges such as vulnerability to adversarial attacks,\ncascading failure modes in sequential decision-making, and a lack of realistic\nevaluation benchmarks. These issues not only hinder real-world deployment but\nalso call for comprehensive mitigation strategies beyond task success. As GUI\nagents become more widespread, establishing robust safety standards and\nresponsible development practices is essential. This survey provides a\nfoundation for advancing trustworthy GUI agents through systematic\nunderstanding and future research.",
    "published_date": "2025-03-30T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.23434v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.23432v1",
    "title": "La perversidad inducida",
    "authors": [
      "Pompeu Casanovas"
    ],
    "author_ids": [],
    "abstract": "The title of this essay, Induced perversity, La perversidad inducida (in\nSpanish), refers to the side effects of sharing information on social media\nwhen it turns out to be false; or when judgments are made about people, events,\ncompanies, or facts that can produce effects often leading to consequences that\nwere not initially foreseen. In digital environments, on the internet of linked\ndata, harrassment politics and the cancellation culture are not isolated\nphenomena; they should be related to the evolution of the web, the emergence of\nthe platform-driven economy, and the change of users' behaviour. Risks are not\nonly a product of technology or the emergence of artificial intelligence.\nEthical problems arise in a hybrid society, where the adaptation of human\nbehavior to human-machine interaction (HMI), and the ethical, social, and legal\nways of regulating them have yet to be learned. This contribution complements\nthe contents of the article: La regimentacion tecnologica. Inteligencia\nartificial, fascismo, agresionn y sociedad democratica. TeKnoKultura. Revista\nde Cultura Digital y Movimientos Sociales, no 22, 2-2025, monografico sobre\npoder y politica de la inteligencia artificial.\nhttps://revistas.ucm.es/index.php/TEKN/article/view/98266",
    "published_date": "2025-03-30T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SI",
      "J.4; K.3.1; K.3.2; K.4.2; K.5.2; J.4.1; J.4.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.23432v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.23398v1",
    "title": "A Large Scale Analysis of Gender Biases in Text-to-Image Generative Models",
    "authors": [
      "Leander Girrbach",
      "Stephan Alaniz",
      "Genevieve Smith",
      "Zeynep Akata"
    ],
    "author_ids": [],
    "abstract": "With the increasing use of image generation technology, understanding its\nsocial biases, including gender bias, is essential. This paper presents the\nfirst large-scale study on gender bias in text-to-image (T2I) models, focusing\non everyday situations. While previous research has examined biases in\noccupations, we extend this analysis to gender associations in daily\nactivities, objects, and contexts. We create a dataset of 3,217 gender-neutral\nprompts and generate 200 images per prompt from five leading T2I models. We\nautomatically detect the perceived gender of people in the generated images and\nfilter out images with no person or multiple people of different genders,\nleaving 2,293,295 images. To enable a broad analysis of gender bias in T2I\nmodels, we group prompts into semantically similar concepts and calculate the\nproportion of male- and female-gendered images for each prompt. Our analysis\nshows that T2I models reinforce traditional gender roles, reflect common gender\nstereotypes in household roles, and underrepresent women in financial related\nactivities. Women are predominantly portrayed in care- and human-centered\nscenarios, and men in technical or physical labor scenarios.",
    "published_date": "2025-03-30T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.23398v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.23243v1",
    "title": "Evaluating how LLM annotations represent diverse views on contentious topics",
    "authors": [
      "Megan A. Brown",
      "Shubham Atreja",
      "Libby Hemphill",
      "Patrick Y. Wu"
    ],
    "author_ids": [],
    "abstract": "Researchers have proposed the use of generative large language models (LLMs)\nto label data for both research and applied settings. This literature\nemphasizes the improved performance of LLMs relative to other natural language\nmodels, noting that LLMs typically outperform other models on standard metrics\nsuch as accuracy, precision, recall, and F1 score. However, previous literature\nhas also highlighted the bias embedded in language models, particularly around\ncontentious topics such as potentially toxic content. This bias could result in\nlabels applied by LLMs that disproportionately align with majority groups over\na more diverse set of viewpoints. In this paper, we evaluate how LLMs represent\ndiverse viewpoints on these contentious tasks. Across four annotation tasks on\nfour datasets, we show that LLMs do not show substantial disagreement with\nannotators on the basis of demographics. Instead, the model, prompt, and\ndisagreement between human annotators on the labeling task are far more\npredictive of LLM agreement. Our findings suggest that when using LLMs to\nannotate data, under-representing the views of particular groups is not a\nsubstantial concern. We conclude with a discussion of the implications for\nresearchers and practitioners.",
    "published_date": "2025-03-29T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.23243v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.23056v2",
    "title": "Achieving Socio-Economic Parity through the Lens of EU AI Act",
    "authors": [
      "Arjun Roy",
      "Stavroula Rizou",
      "Symeon Papadopoulos",
      "Eirini Ntoutsi"
    ],
    "author_ids": [],
    "abstract": "Unfair treatment and discrimination are critical ethical concerns in AI\nsystems, particularly as their adoption expands across diverse domains.\nAddressing these challenges, the recent introduction of the EU AI Act\nestablishes a unified legal framework to ensure legal certainty for AI\ninnovation and investment while safeguarding public interests, such as health,\nsafety, fundamental rights, democracy, and the rule of law (Recital 8). The Act\nencourages stakeholders to initiate dialogue on existing AI fairness notions to\naddress discriminatory outcomes of AI systems. However, these notions often\noverlook the critical role of Socio-Economic Status (SES), inadvertently\nperpetuating biases that favour the economically advantaged. This is\nconcerning, given that principles of equalization advocate for equalizing\nresources or opportunities to mitigate disadvantages beyond an individual's\ncontrol. While provisions for discrimination are laid down in the AI Act,\nspecialized directions should be broadened, particularly in addressing economic\ndisparities perpetuated by AI systems. In this work, we explore the limitations\nof popular AI fairness notions using a real-world dataset (Adult), highlighting\ntheir inability to address SES-driven disparities. To fill this gap, we propose\na novel fairness notion, Socio-Economic Parity (SEP), which incorporates SES\nand promotes positive actions for underprivileged groups while accounting for\nfactors within an individual's control, such as working hours, which can serve\nas a proxy for effort. We define a corresponding fairness measure and optimize\na model constrained by SEP to demonstrate practical utility. Our results show\nthe effectiveness of SEP in mitigating SES-driven biases. By analyzing the AI\nAct alongside our method, we lay a foundation for aligning AI fairness with SES\nfactors while ensuring legal compliance.",
    "published_date": "2025-03-29T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.23056v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.23040v1",
    "title": "Reproducibility Companion Paper:In-processing User Constrained Dominant Sets for User-Oriented Fairness in Recommender Systems",
    "authors": [
      "Yixiu Liu",
      "Zehui He",
      "Yuyuan Li",
      "Zhongxuan Han",
      "Chaochao Chen",
      "Xiaolin Zheng"
    ],
    "author_ids": [],
    "abstract": "In this paper, we reproduce experimental results presented in our earlier\nwork titled \"In-processing User Constrained Dominant Sets for User-Oriented\nFairness in Recommender Systems\" that was presented in the proceeding of the\n31st ACM International Conference on Multimedia.This work aims to verify the\neffectiveness of our previously proposed method and provide guidance for\nreproducibility. We present detailed descriptions of our preprocessed datasets,\nthe structure of our source code, configuration file settings, experimental\nenvironment, and the reproduced experimental results.",
    "published_date": "2025-03-29T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.23040v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.01030v1",
    "title": "Fair Sufficient Representation Learning",
    "authors": [
      "Xueyu Zhou",
      "Chun Yin IP",
      "Jian Huang"
    ],
    "author_ids": [],
    "abstract": "The main objective of fair statistical modeling and machine learning is to\nminimize or eliminate biases that may arise from the data or the model itself,\nensuring that predictions and decisions are not unjustly influenced by\nsensitive attributes such as race, gender, age, or other protected\ncharacteristics. In this paper, we introduce a Fair Sufficient Representation\nLearning (FSRL) method that balances sufficiency and fairness. Sufficiency\nensures that the representation should capture all necessary information about\nthe target variables, while fairness requires that the learned representation\nremains independent of sensitive attributes. FSRL is based on a convex\ncombination of an objective function for learning a sufficient representation\nand an objective function that ensures fairness. Our approach manages fairness\nand sufficiency at the representation level, offering a novel perspective on\nfair representation learning. We implement this method using distance\ncovariance, which is effective for characterizing independence between random\nvariables. We establish the convergence properties of the learned\nrepresentations. Experiments conducted on healthcase and text datasets with\ndiverse structures demonstrate that FSRL achieves a superior trade-off between\nfairness and accuracy compared to existing approaches.",
    "published_date": "2025-03-29T00:00:00",
    "year": 2025,
    "categories": [
      "stat.ML",
      "cs.LG",
      "62G05, 68T07"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.01030v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.23015v1",
    "title": "Engineering Microbial Symbiosis for Mars Habitability",
    "authors": [
      "Randall R. Correll",
      "Simon P. Worden"
    ],
    "author_ids": [],
    "abstract": "The colonization of Mars presents extraordinary challenges, including\nradiation exposure, low atmospheric pressure, and toxic regolith. Recent\nadvancements in synthetic biology and genetic engineering offer unprecedented\nopportunities to address these obstacles by utilizing terrestrial extremophiles\nand engineered organisms. This paper examines the potential for creating\nsymbiotic relationships between terrestrial microbes and hypothetical Martian\nlife forms, should they exist, to support a sustainable human presence on Mars.\nInspired by natural examples of endosymbiosis, such as mitochondria and\nchloroplasts, we propose methods to engineer life forms capable of enduring\nMartian conditions. Key components include experimental designs, laboratory\nsimulations, and bioengineering approaches essential to this endeavor. The\nethical, political, and technological challenges of introducing engineered life\nto Mars are critically evaluated, with an emphasis on international\ncollaboration and robust planetary protection policies. This research\nunderscores engineered symbiosis as a transformative strategy for enabling life\nto adapt and thrive on Mars while advancing humanity's aspirations for\ninterplanetary habitation and exploration. By addressing these challenges, this\nwork highlights a path toward sustainable life on Mars, reflecting both\nscientific ingenuity and ethical stewardship.",
    "published_date": "2025-03-29T00:00:00",
    "year": 2025,
    "categories": [
      "astro-ph.EP",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.23015v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.03716v1",
    "title": "Ethical AI on the Waitlist: Group Fairness Evaluation of LLM-Aided Organ Allocation",
    "authors": [
      "Hannah Murray",
      "Brian Hyeongseok Kim",
      "Isabelle Lee",
      "Jason Byun",
      "Dani Yogatama",
      "Evi Micha"
    ],
    "author_ids": [],
    "abstract": "Large Language Models (LLMs) are becoming ubiquitous, promising automation\neven in high-stakes scenarios. However, existing evaluation methods often fall\nshort -- benchmarks saturate, accuracy-based metrics are overly simplistic, and\nmany inherently ambiguous problems lack a clear ground truth. Given these\nlimitations, evaluating fairness becomes complex. To address this, we reframe\nfairness evaluation using Borda scores, a method from voting theory, as a\nnuanced yet interpretable metric for measuring fairness. Using organ allocation\nas a case study, we introduce two tasks: (1) Choose-One and (2) Rank-All. In\nChoose-One, LLMs select a single candidate for a kidney, and we assess fairness\nacross demographics using proportional parity. In Rank-All, LLMs rank all\ncandidates for a kidney, reflecting real-world allocation processes. Since\ntraditional fairness metrics do not account for ranking, we propose a novel\napplication of Borda scoring to capture biases. Our findings highlight the\npotential of voting-based metrics to provide a richer, more multifaceted\nevaluation of LLM fairness.",
    "published_date": "2025-03-29T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.03716v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.22971v1",
    "title": "Enhancing Federated Learning Through Secure Cluster-Weighted Client Aggregation",
    "authors": [
      "Kanishka Ranaweera",
      "Azadeh Ghari Neiat",
      "Xiao Liu",
      "Bipasha Kashyap",
      "Pubudu N. Pathirana"
    ],
    "author_ids": [],
    "abstract": "Federated learning (FL) has emerged as a promising paradigm in machine\nlearning, enabling collaborative model training across decentralized devices\nwithout the need for raw data sharing. In FL, a global model is trained\niteratively on local datasets residing on individual devices, each contributing\nto the model's improvement. However, the heterogeneous nature of these local\ndatasets, stemming from diverse user behaviours, device capabilities, and data\ndistributions, poses a significant challenge. The inherent heterogeneity in\nfederated learning gives rise to various issues, including model performance\ndiscrepancies, convergence challenges, and potential privacy concerns. As the\nglobal model progresses through rounds of training, the disparities in local\ndata quality and quantity can impede the overall effectiveness of federated\nlearning systems. Moreover, maintaining fairness and privacy across diverse\nuser groups becomes a paramount concern. To address this issue, this paper\nintroduces a novel FL framework, ClusterGuardFL, that employs dissimilarity\nscores, k-means clustering, and reconciliation confidence scores to dynamically\nassign weights to client updates. The dissimilarity scores between global and\nlocal models guide the formation of clusters, with cluster size influencing the\nweight allocation. Within each cluster, a reconciliation confidence score is\ncalculated for individual data points, and a softmax layer generates customized\nweights for clients. These weights are utilized in the aggregation process,\nenhancing the model's robustness and privacy. Experimental results demonstrate\nthe efficacy of the proposed approach in achieving improved model performance\nin diverse datasets.",
    "published_date": "2025-03-29T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.22971v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.22934v1",
    "title": "FairSAM: Fair Classification on Corrupted Data Through Sharpness-Aware Minimization",
    "authors": [
      "Yucong Dai",
      "Jie Ji",
      "Xiaolong Ma",
      "Yongkai Wu"
    ],
    "author_ids": [],
    "abstract": "Image classification models trained on clean data often suffer from\nsignificant performance degradation when exposed to testing corrupted data,\nsuch as images with impulse noise, Gaussian noise, or environmental noise. This\ndegradation not only impacts overall performance but also disproportionately\naffects various demographic subgroups, raising critical algorithmic bias\nconcerns. Although robust learning algorithms like Sharpness-Aware Minimization\n(SAM) have shown promise in improving overall model robustness and\ngeneralization, they fall short in addressing the biased performance\ndegradation across demographic subgroups. Existing fairness-aware machine\nlearning methods - such as fairness constraints and reweighing strategies - aim\nto reduce performance disparities but hardly maintain robust and equitable\naccuracy across demographic subgroups when faced with data corruption. This\nreveals an inherent tension between robustness and fairness when dealing with\ncorrupted data. To address these challenges, we introduce one novel metric\nspecifically designed to assess performance degradation across subgroups under\ndata corruption. Additionally, we propose \\textbf{FairSAM}, a new framework\nthat integrates \\underline{Fair}ness-oriented strategies into \\underline{SAM}\nto deliver equalized performance across demographic groups under corrupted\nconditions. Our experiments on multiple real-world datasets and various\npredictive tasks show that FairSAM successfully reconciles robustness and\nfairness, offering a structured solution for equitable and resilient image\nclassification in the presence of data corruption.",
    "published_date": "2025-03-29T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.22934v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.22929v1",
    "title": "Unsupervised Feature Disentanglement and Augmentation Network for One-class Face Anti-spoofing",
    "authors": [
      "Pei-Kai Huang",
      "Jun-Xiong Chong",
      "Ming-Tsung Hsu",
      "Fang-Yu Hsu",
      "Yi-Ting Lin",
      "Kai-Heng Chien",
      "Hao-Chiang Shao",
      "Chiou-Ting Hsu"
    ],
    "author_ids": [],
    "abstract": "Face anti-spoofing (FAS) techniques aim to enhance the security of facial\nidentity authentication by distinguishing authentic live faces from deceptive\nattempts. While two-class FAS methods risk overfitting to training attacks to\nachieve better performance, one-class FAS approaches handle unseen attacks well\nbut are less robust to domain information entangled within the liveness\nfeatures. To address this, we propose an Unsupervised Feature Disentanglement\nand Augmentation Network (\\textbf{UFDANet}), a one-class FAS technique that\nenhances generalizability by augmenting face images via disentangled features.\nThe \\textbf{UFDANet} employs a novel unsupervised feature disentangling method\nto separate the liveness and domain features, facilitating discriminative\nfeature learning. It integrates an out-of-distribution liveness feature\naugmentation scheme to synthesize new liveness features of unseen spoof\nclasses, which deviate from the live class, thus enhancing the representability\nand discriminability of liveness features. Additionally, \\textbf{UFDANet}\nincorporates a domain feature augmentation routine to synthesize unseen domain\nfeatures, thereby achieving better generalizability. Extensive experiments\ndemonstrate that the proposed \\textbf{UFDANet} outperforms previous one-class\nFAS methods and achieves comparable performance to state-of-the-art two-class\nFAS methods.",
    "published_date": "2025-03-29T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.22929v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.02856v1",
    "title": "The epistemic dimension of algorithmic fairness: assessing its impact in innovation diffusion and fair policy making",
    "authors": [
      "Eugenia Villa",
      "Camilla Quaresmini",
      "Valentina Breschi",
      "Viola Schiaffonati",
      "Mara Tanelli"
    ],
    "author_ids": [],
    "abstract": "Algorithmic fairness is an expanding field that addresses a range of\ndiscrimination issues associated with algorithmic processes. However, most\nworks in the literature focus on analyzing it only from an ethical perspective,\nfocusing on moral principles and values that should be considered in the design\nand evaluation of algorithms, while disregarding the epistemic dimension\nrelated to knowledge transmission and validation. However, this aspect of\nalgorithmic fairness should also be included in the debate, as it is crucial to\nintroduce a specific type of harm: an individual may be systematically excluded\nfrom the dissemination of knowledge due to the attribution of a credibility\ndeficit/excess. In this work, we specifically focus on characterizing and\nanalyzing the impact of this credibility deficit or excess on the diffusion of\ninnovations on a societal scale, a phenomenon driven by individual attitudes\nand social interactions, and also by the strength of mutual connections.\nIndeed, discrimination might shape the latter, ultimately modifying how\ninnovations spread within the network. In this light, to incorporate, also from\na formal point of view, the epistemic dimension in innovation diffusion models\nbecomes paramount, especially if these models are intended to support fair\npolicy design. For these reasons, we formalize the epistemic properties of a\nsocial environment, by extending the well-established Linear Threshold Model\n(LTM) in an epistemic direction to show the impact of epistemic biases in\ninnovation diffusion. Focusing on the impact of epistemic bias in both\nopen-loop and closed-loop scenarios featuring optimal fostering policies, our\nresults shed light on the pivotal role the epistemic dimension might have in\nthe debate of algorithmic fairness in decision-making.",
    "published_date": "2025-03-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.02856v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.01029v1",
    "title": "Who is Responsible When AI Fails? Mapping Causes, Entities, and Consequences of AI Privacy and Ethical Incidents",
    "authors": [
      "Hilda Hadan",
      "Reza Hadi Mogavi",
      "Leah Zhang-Kennedy",
      "Lennart E. Nacke"
    ],
    "author_ids": [],
    "abstract": "The rapid growth of artificial intelligence (AI) technologies has changed\ndecision-making in many fields. But, it has also raised major privacy and\nethical concerns. However, many AI incidents taxonomies and guidelines for\nacademia, industry, and government lack grounding in real-world incidents. We\nanalyzed 202 real-world AI privacy and ethical incidents. This produced a\ntaxonomy that classifies incident types across AI lifecycle stages. It accounts\nfor contextual factors such as causes, responsible entities, disclosure\nsources, and impacts. Our findings show insufficient incident reporting from AI\ndevelopers and users. Many incidents are caused by poor organizational\ndecisions and legal non-compliance. Only a few legal actions and corrective\nmeasures exist, while risk-mitigation efforts are limited. Our taxonomy\ncontributes a structured approach in reporting of future AI incidents. Our\nfindings demonstrate that current AI governance frameworks are inadequate. We\nurgently need child-specific protections and AI policies on social media. They\nmust moderate and reduce the spread of harmful AI-generated content. Our\nresearch provides insights for policymakers and practitioners, which lets them\ndesign ethical AI. It also support AI incident detection and risk management.\nFinally, it guides AI policy development. Improved policies will protect people\nfrom harmful AI applications and support innovation in AI systems.",
    "published_date": "2025-03-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.DB",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.01029v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.22896v4",
    "title": "Representation and Stability Analysis of 1D PDEs with Periodic Boundary Conditions",
    "authors": [
      "Declan Jagt",
      "Sergei Chernyshenko",
      "Matthew Peet"
    ],
    "author_ids": [],
    "abstract": "PDEs with periodic boundary conditions are frequently used to model processes\nin large spatial environments, assuming solutions to extend periodically beyond\nsome bounded interval. However, solutions to these PDEs often do not converge\nto a unique equilibrium, but instead converge to non-stationary trajectories\nexisting in the nullspace of the spatial differential operator (e.g.\n$\\frac{\\partial^2}{\\partial x^2}$). To analyse this convergence behaviour, in\nthis paper, it is shown how such trajectories can be modeled for a broad class\nof linear, 2nd order, 1D PDEs with periodic as well as more general boundary\nconditions, using the Partial Integral Equation (PIE) representation. In\nparticular, it is first shown how any PDE state satisfying these boundary\nconditions can be uniquely expressed in terms of two components, existing in\nthe image and the nullspace of the differential operator\n$\\frac{\\partial^2}{\\partial x^2}$, respectively. An equivalent representation\nof linear PDEs is then derived as a PIE, explicitly defining the dynamics of\nboth state components. Finally, a notion of exponential stability is defined\nthat requires only one of the state components to converge to zero, and it is\nshown how this stability notion can be tested by solving a linear operator\ninequality. The proposed methodology is applied to two examples, demonstrating\nthat exponential stability can be verified with tight bounds on the rate of\ndecay.",
    "published_date": "2025-03-28T00:00:00",
    "year": 2025,
    "categories": [
      "math.AP",
      "cs.SY",
      "eess.SY",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.22896v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.22877v1",
    "title": "Understanding Inequality of LLM Fact-Checking over Geographic Regions with Agent and Retrieval models",
    "authors": [
      "Bruno Coelho",
      "Shujaat Mirza",
      "Yuyuan Cui",
      "Christina Pöpper",
      "Damon McCoy"
    ],
    "author_ids": [],
    "abstract": "Fact-checking is a potentially useful application of Large Language Models\n(LLMs) to combat the growing dissemination of disinformation. However, the\nperformance of LLMs varies across geographic regions. In this paper, we\nevaluate the factual accuracy of open and private models across a diverse set\nof regions and scenarios.\n  Using a dataset containing 600 fact-checked statements balanced across six\nglobal regions we examine three experimental setups of fact-checking a\nstatement: (1) when just the statement is available, (2) when an LLM-based\nagent with Wikipedia access is utilized, and (3) as a best case scenario when a\nRetrieval-Augmented Generation (RAG) system provided with the official fact\ncheck is employed. Our findings reveal that regardless of the scenario and LLM\nused, including GPT-4, Claude Sonnet, and LLaMA, statements from the Global\nNorth perform substantially better than those from the Global South.\nFurthermore, this gap is broadened for the more realistic case of a Wikipedia\nagent-based system, highlighting that overly general knowledge bases have a\nlimited ability to address region-specific nuances. These results underscore\nthe urgent need for better dataset balancing and robust retrieval strategies to\nenhance LLM fact-checking capabilities, particularly in geographically diverse\ncontexts.",
    "published_date": "2025-03-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.22877v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.00025v1",
    "title": "Generalization Bias in Large Language Model Summarization of Scientific Research",
    "authors": [
      "Uwe Peters",
      "Benjamin Chin-Yee"
    ],
    "author_ids": [],
    "abstract": "Artificial intelligence chatbots driven by large language models (LLMs) have\nthe potential to increase public science literacy and support scientific\nresearch, as they can quickly summarize complex scientific information in\naccessible terms. However, when summarizing scientific texts, LLMs may omit\ndetails that limit the scope of research conclusions, leading to\ngeneralizations of results broader than warranted by the original study. We\ntested 10 prominent LLMs, including ChatGPT-4o, ChatGPT-4.5, DeepSeek, LLaMA\n3.3 70B, and Claude 3.7 Sonnet, comparing 4900 LLM-generated summaries to their\noriginal scientific texts. Even when explicitly prompted for accuracy, most\nLLMs produced broader generalizations of scientific results than those in the\noriginal texts, with DeepSeek, ChatGPT-4o, and LLaMA 3.3 70B overgeneralizing\nin 26 to 73% of cases. In a direct comparison of LLM-generated and\nhuman-authored science summaries, LLM summaries were nearly five times more\nlikely to contain broad generalizations (OR = 4.85, 95% CI [3.06, 7.70]).\nNotably, newer models tended to perform worse in generalization accuracy than\nearlier ones. Our results indicate a strong bias in many widely used LLMs\ntowards overgeneralizing scientific conclusions, posing a significant risk of\nlarge-scale misinterpretations of research findings. We highlight potential\nmitigation strategies, including lowering LLM temperature settings and\nbenchmarking LLMs for generalization accuracy.",
    "published_date": "2025-03-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.00025v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.22823v1",
    "title": "Quantum Doeblin Coefficients: Interpretations and Applications",
    "authors": [
      "Ian George",
      "Christoph Hirche",
      "Theshani Nuradha",
      "Mark M. Wilde"
    ],
    "author_ids": [],
    "abstract": "In classical information theory, the Doeblin coefficient of a classical\nchannel provides an efficiently computable upper bound on the total-variation\ncontraction coefficient of the channel, leading to what is known as a strong\ndata-processing inequality. Here, we investigate quantum Doeblin coefficients\nas a generalization of the classical concept. In particular, we define various\nnew quantum Doeblin coefficients, one of which has several desirable\nproperties, including concatenation and multiplicativity, in addition to being\nefficiently computable. We also develop various interpretations of two of the\nquantum Doeblin coefficients, including representations as minimal singlet\nfractions, exclusion values, reverse max-mutual and oveloH informations,\nreverse robustnesses, and hypothesis testing reverse mutual and oveloH\ninformations. Our interpretations of quantum Doeblin coefficients as either\nentanglement-assisted or unassisted exclusion values are particularly\nappealing, indicating that they are proportional to the best possible error\nprobabilities one could achieve in state-exclusion tasks by making use of the\nchannel. We also outline various applications of quantum Doeblin coefficients,\nranging from limitations on quantum machine learning algorithms that use\nparameterized quantum circuits (noise-induced barren plateaus), on error\nmitigation protocols, on the sample complexity of noisy quantum hypothesis\ntesting, on the fairness of noisy quantum models, and on mixing times of\ntime-varying channels. All of these applications make use of the fact that\nquantum Doeblin coefficients appear in upper bounds on various trace-distance\ncontraction coefficients of a channel. Furthermore, in all of these\napplications, our analysis using Doeblin coefficients provides improvements of\nvarious kinds over contributions from prior literature, both in terms of\ngenerality and being efficiently computable.",
    "published_date": "2025-03-28T00:00:00",
    "year": 2025,
    "categories": [
      "quant-ph",
      "cs.IT",
      "cs.LG",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.22823v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.22569v1",
    "title": "Comparing Methods for Bias Mitigation in Graph Neural Networks",
    "authors": [
      "Barbara Hoffmann",
      "Ruben Mayer"
    ],
    "author_ids": [],
    "abstract": "This paper examines the critical role of Graph Neural Networks (GNNs) in data\npreparation for generative artificial intelligence (GenAI) systems, with a\nparticular focus on addressing and mitigating biases. We present a comparative\nanalysis of three distinct methods for bias mitigation: data sparsification,\nfeature modification, and synthetic data augmentation. Through experimental\nanalysis using the german credit dataset, we evaluate these approaches using\nmultiple fairness metrics, including statistical parity, equality of\nopportunity, and false positive rates. Our research demonstrates that while all\nmethods improve fairness metrics compared to the original dataset, stratified\nsampling and synthetic data augmentation using GraphSAGE prove particularly\neffective in balancing demographic representation while maintaining model\nperformance. The results provide practical insights for developing more\nequitable AI systems while maintaining model performance.",
    "published_date": "2025-03-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.22569v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.22562v1",
    "title": "Niyama : Breaking the Silos of LLM Inference Serving",
    "authors": [
      "Kanishk Goel",
      "Jayashree Mohan",
      "Nipun Kwatra",
      "Ravi Shreyas Anupindi",
      "Ramachandran Ramjee"
    ],
    "author_ids": [],
    "abstract": "The widespread adoption of Large Language Models (LLMs) has enabled diverse\napplications with very different latency requirements. Existing LLM serving\nframeworks rely on siloed infrastructure with coarse-grained workload\nsegregation -- interactive and batch -- leading to inefficient resource\nutilization and limited support for fine-grained Quality-of-Service (QoS)\ndifferentiation. This results in operational inefficiencies, over-provisioning\nand poor load management during traffic surges.\n  We present Niyama, a novel QoS-driven inference serving system that enables\nefficient co-scheduling of diverse workloads on shared infrastructure. Niyama\nintroduces fine-grained QoS classification allowing applications to specify\nprecise latency requirements, and dynamically adapts scheduling decisions based\non real-time system state. Leveraging the predictable execution characteristics\nof LLM inference, Niyama implements a dynamic chunking mechanism to improve\noverall throughput while maintaining strict QoS guarantees. Additionally,\nNiyama employs a hybrid prioritization policy that balances fairness and\nefficiency, and employs selective request relegation that enables graceful\nservice degradation during overload conditions. Our evaluation demonstrates\nthat Niyama increases serving capacity by 32% compared to current siloed\ndeployments, while maintaining QoS guarantees. Notably, under extreme load, our\nsystem reduces SLO violations by an order of magnitude compared to current\nstrategies.",
    "published_date": "2025-03-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.22562v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.22454v1",
    "title": "A Causal Framework to Measure and Mitigate Non-binary Treatment Discrimination",
    "authors": [
      "Ayan Majumdar",
      "Deborah D. Kanubala",
      "Kavya Gupta",
      "Isabel Valera"
    ],
    "author_ids": [],
    "abstract": "Fairness studies of algorithmic decision-making systems often simplify\ncomplex decision processes, such as bail or loan approvals, into binary\nclassification tasks. However, these approaches overlook that such decisions\nare not inherently binary (e.g., approve or not approve bail or loan); they\nalso involve non-binary treatment decisions (e.g., bail conditions or loan\nterms) that can influence the downstream outcomes (e.g., loan repayment or\nreoffending). In this paper, we argue that non-binary treatment decisions are\nintegral to the decision process and controlled by decision-makers and,\ntherefore, should be central to fairness analyses in algorithmic\ndecision-making. We propose a causal framework that extends fairness analyses\nand explicitly distinguishes between decision-subjects' covariates and the\ntreatment decisions. This specification allows decision-makers to use our\nframework to (i) measure treatment disparity and its downstream effects in\nhistorical data and, using counterfactual reasoning, (ii) mitigate the impact\nof past unfair treatment decisions when automating decision-making. We use our\nframework to empirically analyze four widely used loan approval datasets to\nreveal potential disparity in non-binary treatment decisions and their\ndiscriminatory impact on outcomes, highlighting the need to incorporate\ntreatment decisions in fairness assessments. Moreover, by intervening in\ntreatment decisions, we show that our framework effectively mitigates treatment\ndiscrimination from historical data to ensure fair risk score estimation and\n(non-binary) decision-making processes that benefit all stakeholders.",
    "published_date": "2025-03-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.22454v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.22397v1",
    "title": "GAITGen: Disentangled Motion-Pathology Impaired Gait Generative Model -- Bringing Motion Generation to the Clinical Domain",
    "authors": [
      "Vida Adeli",
      "Soroush Mehraban",
      "Majid Mirmehdi",
      "Alan Whone",
      "Benjamin Filtjens",
      "Amirhossein Dadashzadeh",
      "Alfonso Fasano",
      "Andrea Iaboni",
      "Babak Taati"
    ],
    "author_ids": [],
    "abstract": "Gait analysis is crucial for the diagnosis and monitoring of movement\ndisorders like Parkinson's Disease. While computer vision models have shown\npotential for objectively evaluating parkinsonian gait, their effectiveness is\nlimited by scarce clinical datasets and the challenge of collecting large and\nwell-labelled data, impacting model accuracy and risk of bias. To address these\ngaps, we propose GAITGen, a novel framework that generates realistic gait\nsequences conditioned on specified pathology severity levels. GAITGen employs a\nConditional Residual Vector Quantized Variational Autoencoder to learn\ndisentangled representations of motion dynamics and pathology-specific factors,\ncoupled with Mask and Residual Transformers for conditioned sequence\ngeneration. GAITGen generates realistic, diverse gait sequences across severity\nlevels, enriching datasets and enabling large-scale model training in\nparkinsonian gait analysis. Experiments on our new PD-GaM (real) dataset\ndemonstrate that GAITGen outperforms adapted state-of-the-art models in both\nreconstruction fidelity and generation quality, accurately capturing critical\npathology-specific gait features. A clinical user study confirms the realism\nand clinical relevance of our generated sequences. Moreover, incorporating\nGAITGen-generated data into downstream tasks improves parkinsonian gait\nseverity estimation, highlighting its potential for advancing clinical gait\nanalysis.",
    "published_date": "2025-03-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.22397v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.22338v1",
    "title": "SKDU at De-Factify 4.0: Natural Language Features for AI-Generated Text-Detection",
    "authors": [
      "Shrikant Malviya",
      "Pablo Arnau-González",
      "Miguel Arevalillo-Herráez",
      "Stamos Katsigiannis"
    ],
    "author_ids": [],
    "abstract": "The rapid advancement of large language models (LLMs) has introduced new\nchallenges in distinguishing human-written text from AI-generated content. In\nthis work, we explored a pipelined approach for AI-generated text detection\nthat includes a feature extraction step (i.e. prompt-based rewriting features\ninspired by RAIDAR and content-based features derived from the NELA toolkit)\nfollowed by a classification module. Comprehensive experiments were conducted\non the Defactify4.0 dataset, evaluating two tasks: binary classification to\ndifferentiate human-written and AI-generated text, and multi-class\nclassification to identify the specific generative model used to generate the\ninput text. Our findings reveal that NELA features significantly outperform\nRAIDAR features in both tasks, demonstrating their ability to capture nuanced\nlinguistic, stylistic, and content-based differences. Combining RAIDAR and NELA\nfeatures provided minimal improvement, highlighting the redundancy introduced\nby less discriminative features. Among the classifiers tested, XGBoost emerged\nas the most effective, leveraging the rich feature sets to achieve high\naccuracy and generalisation.",
    "published_date": "2025-03-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.22338v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.22329v1",
    "title": "A Refined Analysis of Massive Activations in LLMs",
    "authors": [
      "Louis Owen",
      "Nilabhra Roy Chowdhury",
      "Abhay Kumar",
      "Fabian Güra"
    ],
    "author_ids": [],
    "abstract": "Motivated in part by their relevance for low-precision training and\nquantization, massive activations in large language models (LLMs) have recently\nemerged as a topic of interest. However, existing analyses are limited in\nscope, and generalizability across architectures is unclear. This paper helps\naddress some of these gaps by conducting an analysis of massive activations\nacross a broad range of LLMs, including both GLU-based and non-GLU-based\narchitectures. Our findings challenge several prior assumptions, most\nimportantly: (1) not all massive activations are detrimental, i.e. suppressing\nthem does not lead to an explosion of perplexity or a collapse in downstream\ntask performance; (2) proposed mitigation strategies such as Attention KV bias\nare model-specific and ineffective in certain cases. We consequently\ninvestigate novel hybrid mitigation strategies; in particular pairing Target\nVariance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT)\nsuccessfully balances the mitigation of massive activations with preserved\ndownstream model performance in the scenarios we investigated. Our code is\navailable at: https://github.com/bluorion-com/refine_massive_activations.",
    "published_date": "2025-03-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.22329v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.22328v2",
    "title": "VoteFlow: Enforcing Local Rigidity in Self-Supervised Scene Flow",
    "authors": [
      "Yancong Lin",
      "Shiming Wang",
      "Liangliang Nan",
      "Julian Kooij",
      "Holger Caesar"
    ],
    "author_ids": [],
    "abstract": "Scene flow estimation aims to recover per-point motion from two adjacent\nLiDAR scans. However, in real-world applications such as autonomous driving,\npoints rarely move independently of others, especially for nearby points\nbelonging to the same object, which often share the same motion. Incorporating\nthis locally rigid motion constraint has been a key challenge in\nself-supervised scene flow estimation, which is often addressed by\npost-processing or appending extra regularization. While these approaches are\nable to improve the rigidity of predicted flows, they lack an architectural\ninductive bias for local rigidity within the model structure, leading to\nsuboptimal learning efficiency and inferior performance. In contrast, we\nenforce local rigidity with a lightweight add-on module in neural network\ndesign, enabling end-to-end learning. We design a discretized voting space that\naccommodates all possible translations and then identify the one shared by\nnearby points by differentiable voting. Additionally, to ensure computational\nefficiency, we operate on pillars rather than points and learn representative\nfeatures for voting per pillar. We plug the Voting Module into popular model\ndesigns and evaluate its benefit on Argoverse 2 and Waymo datasets. We\noutperform baseline works with only marginal compute overhead. Code is\navailable at https://github.com/tudelft-iv/VoteFlow.",
    "published_date": "2025-03-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.22328v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.22327v1",
    "title": "Valid Cuts for the Design of Potential-based Flow Networks",
    "authors": [
      "Pascal Börner",
      "Max Klimm",
      "Annette Lutz",
      "Marc E. Pfetsch",
      "Martin Skutella",
      "Lea Strubberg"
    ],
    "author_ids": [],
    "abstract": "The construction of a cost minimal network for flows obeying physical laws is\nan important problem for the design of electricity, water, hydrogen, and\nnatural gas infrastructures. We formulate this problem as a mixed-integer\nnon-linear program with potential-based flows. The non-convexity of the\nconstraints stemming from the potential-based flow model together with the\nbinary variables indicating the decision to build a connection make these\nprograms challenging to solve. We develop a novel class of valid inequalities\non the fractional relaxations of the binary variables. Further, we show that\nthis class of inequalities can be separated in polynomial time for solutions to\na fractional relaxation. This makes it possible to incorporate these\ninequalities into a branch-and-cut framework. The advantage of these\ninequalities is lastly demonstrated in a computational study on the design of\nreal-world gas transport networks.",
    "published_date": "2025-03-28T00:00:00",
    "year": 2025,
    "categories": [
      "math.OC",
      "cs.DM",
      "math.CO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.22327v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.22238v1",
    "title": "Integrating LLMs in Software Engineering Education: Motivators, Demotivators, and a Roadmap Towards a Framework for Finnish Higher Education Institutes",
    "authors": [
      "Maryam Khan",
      "Muhammad Azeem Akbar",
      "Jussi Kasurinen"
    ],
    "author_ids": [],
    "abstract": "The increasing adoption of Large Language Models (LLMs) in software\nengineering education presents both opportunities and challenges. While LLMs\noffer benefits such as enhanced learning experiences, automated assessments,\nand personalized tutoring, their integration also raises concerns about\nacademic integrity, student over-reliance, and ethical considerations. In this\nstudy, we conducted a preliminary literature review to identify motivators and\ndemotivators for using LLMs in software engineering education. We applied a\nthematic mapping process to categorize and structure these factors (motivators\nand demotivators), offering a comprehensive view of their impact. In total, we\nidentified 25 motivators and 30 demotivators, which are further organized into\nfour high-level themes. This mapping provides a structured framework for\nunderstanding the factors that influence the integration of LLMs in software\nengineering education, both positively and negatively. As part of a larger\nresearch project, this study serves as a feasibility assessment, laying the\ngroundwork for future systematic literature review and empirical studies.\nUltimately, this project aims to develop a framework to assist Finnish higher\neducation institutions in effectively integrating LLMs into software\nengineering education while addressing potential risks and challenges.",
    "published_date": "2025-03-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.22238v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.22197v1",
    "title": "Extremely Simple Out-of-distribution Detection for Audio-visual Generalized Zero-shot Learning",
    "authors": [
      "Yang Liu",
      "Xun Zhang",
      "Jiale Du",
      "Xinbo Gao",
      "Jungong Han"
    ],
    "author_ids": [],
    "abstract": "Zero-shot Learning(ZSL) attains knowledge transfer from seen classes to\nunseen classes by exploring auxiliary category information, which is a\npromising yet difficult research topic. In this field, Audio-Visual Generalized\nZero-Shot Learning~(AV-GZSL) has aroused researchers' great interest in which\nintricate relations within triple modalities~(audio, video, and natural\nlanguage) render this task quite challenging but highly research-worthy.\nHowever, both existing embedding-based and generative-based AV-GZSL methods\ntend to suffer from domain shift problem a lot and we propose an extremely\nsimple Out-of-distribution~(OOD) detection based AV-GZSL method~(EZ-AVOOD) to\nfurther mitigate bias problem by differentiating seen and unseen samples at the\ninitial beginning. EZ-AVOOD accomplishes effective seen-unseen separation by\nexploiting the intrinsic discriminative information held in class-specific\nlogits and class-agnostic feature subspace without training an extra OOD\ndetector network. Followed by seen-unseen binary classification, we employ two\nexpert models to classify seen samples and unseen samples separately. Compared\nto existing state-of-the-art methods, our model achieves superior ZSL and GZSL\nperformances on three audio-visual datasets and becomes the new SOTA, which\ncomprehensively demonstrates the effectiveness of the proposed EZ-AVOOD.",
    "published_date": "2025-03-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.22197v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.22175v1",
    "title": "Efficient Continual Learning through Frequency Decomposition and Integration",
    "authors": [
      "Ruiqi Liu",
      "Boyu Diao",
      "Libo Huang",
      "Hangda Liu",
      "Chuanguang Yang",
      "Zhulin An",
      "Yongjun Xu"
    ],
    "author_ids": [],
    "abstract": "Continual learning (CL) aims to learn new tasks while retaining past\nknowledge, addressing the challenge of forgetting during task adaptation.\nRehearsal-based methods, which replay previous samples, effectively mitigate\nforgetting. However, research on enhancing the efficiency of these methods,\nespecially in resource-constrained environments, remains limited, hindering\ntheir application in real-world systems with dynamic data streams. The human\nperceptual system processes visual scenes through complementary frequency\nchannels: low-frequency signals capture holistic cues, while high-frequency\ncomponents convey structural details vital for fine-grained discrimination.\nInspired by this, we propose the Frequency Decomposition and Integration\nNetwork (FDINet), a novel framework that decomposes and integrates information\nacross frequencies. FDINet designs two lightweight networks to independently\nprocess low- and high-frequency components of images. When integrated with\nrehearsal-based methods, this frequency-aware design effectively enhances\ncross-task generalization through low-frequency information, preserves\nclass-specific details using high-frequency information, and facilitates\nefficient training due to its lightweight architecture. Experiments demonstrate\nthat FDINet reduces backbone parameters by 78%, improves accuracy by up to\n7.49% over state-of-the-art (SOTA) methods, and decreases peak memory usage by\nup to 80%. Additionally, on edge devices, FDINet accelerates training by up to\n5$\\times$.",
    "published_date": "2025-03-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.22175v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.22772v1",
    "title": "AI Family Integration Index (AFII): Benchmarking a New Global Readiness for AI as Family",
    "authors": [
      "Prashant Mahajan"
    ],
    "author_ids": [],
    "abstract": "As Artificial Intelligence (AI) systems increasingly permeate caregiving,\neducational, and emotionally sensitive domains, there is a growing need to\nassess national readiness beyond infrastructure and innovation capacity.\nExisting indices such as the Stanford AI Index (2024), overlooked relational,\nethical, and cultural dimensions essential to human centered AI integration. To\naddress this blind spot, this study introduces the AI Family Integration Index\n(AFII), a ten dimensional benchmarking framework that evaluates national\npreparedness for integrating emotionally intelligent AI into family and\ncaregiving systems. Using mixed-method analysis and equal weighting, the AFII\nprovides a multidimensional tool for assessing emotional and symbolic readiness\nin diverse cultural contexts. A core insight is the policy practice gap: while\nmany governments articulate ethical AI principles, few have implemented them\neffectively in relational or caregiving domains. Countries like Singapore,\nJapan, and South Korea demonstrate alignment between policy intent and\ncaregiving integration, while others such as the United States and France,\nexhibit advanced policy rhetoric but slower real-world execution. This\ndissonance is captured through the AFII Governance Gap Lens. The AFII also\nreveals divergence from conventional rankings: technological leaders like the\nU.S. and China score high in the Stanford AI Index yet rank lower in AFII due\nto weaker caregiving alignment. In contrast, nations like Sweden and Singapore\noutperform on relational readiness despite moderate technical rankings. For\npolicymakers, the AFII offers a practical, scalable, and ethically grounded\ntool to guide inclusive AI strategies, reframing readiness to center care,\nemotional safety, and cultural legitimacy in the age of relational AI.",
    "published_date": "2025-03-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.22772v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.22115v1",
    "title": "Beyond Single-Sentence Prompts: Upgrading Value Alignment Benchmarks with Dialogues and Stories",
    "authors": [
      "Yazhou Zhang",
      "Qimeng Liu",
      "Qiuchi Li",
      "Peng Zhang",
      "Jing Qin"
    ],
    "author_ids": [],
    "abstract": "Evaluating the value alignment of large language models (LLMs) has\ntraditionally relied on single-sentence adversarial prompts, which directly\nprobe models with ethically sensitive or controversial questions. However, with\nthe rapid advancements in AI safety techniques, models have become increasingly\nadept at circumventing these straightforward tests, limiting their\neffectiveness in revealing underlying biases and ethical stances. To address\nthis limitation, we propose an upgraded value alignment benchmark that moves\nbeyond single-sentence prompts by incorporating multi-turn dialogues and\nnarrative-based scenarios. This approach enhances the stealth and adversarial\nnature of the evaluation, making it more robust against superficial safeguards\nimplemented in modern LLMs. We design and implement a dataset that includes\nconversational traps and ethically ambiguous storytelling, systematically\nassessing LLMs' responses in more nuanced and context-rich settings.\nExperimental results demonstrate that this enhanced methodology can effectively\nexpose latent biases that remain undetected in traditional single-shot\nevaluations. Our findings highlight the necessity of contextual and dynamic\ntesting for value alignment in LLMs, paving the way for more sophisticated and\nrealistic assessments of AI ethics and safety.",
    "published_date": "2025-03-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.22115v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.22066v2",
    "title": "Reflection on Code Contributor Demographics and Collaboration Patterns in the Rust Community",
    "authors": [
      "Rohit Dandamudi",
      "Ifeoma Adaji",
      "Gema Rodríguez-Pérez"
    ],
    "author_ids": [],
    "abstract": "Open-source software communities thrive on global collaboration and\ncontributions from diverse participants. This study explores the Rust\nprogramming language ecosystem to understand its contributors' demographic\ncomposition and interaction patterns. Our objective is to investigate the\nphenomenon of participation inequality in key Rust projects and the presence of\ndiversity among them. We studied GitHub pull request data from the year leading\nup to the release of the latest completed Rust community annual survey in 2023.\nSpecifically, we extracted information from three leading repositories: Rust,\nRust Analyzer, and Cargo, and used social network graphs to visualize the\ninteractions and identify central contributors and sub-communities. Social\nnetwork analysis has shown concerning disparities in gender and geographic\nrepresentation among contributors who play pivotal roles in collaboration\nnetworks and the presence of varying diversity levels in the sub-communities\nformed. These results suggest that while the Rust community is globally active,\nthe contributor base does not fully reflect the diversity of the wider user\ncommunity. We conclude that there is a need for more inclusive practices to\nencourage broader participation and ensure that the contributor base aligns\nmore closely with the diverse global community that utilizes Rust.",
    "published_date": "2025-03-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SE",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.22066v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.21985v1",
    "title": "Improving Equivariant Networks with Probabilistic Symmetry Breaking",
    "authors": [
      "Hannah Lawrence",
      "Vasco Portilheiro",
      "Yan Zhang",
      "Sékou-Oumar Kaba"
    ],
    "author_ids": [],
    "abstract": "Equivariance encodes known symmetries into neural networks, often enhancing\ngeneralization. However, equivariant networks cannot break symmetries: the\noutput of an equivariant network must, by definition, have at least the same\nself-symmetries as the input. This poses an important problem, both (1) for\nprediction tasks on domains where self-symmetries are common, and (2) for\ngenerative models, which must break symmetries in order to reconstruct from\nhighly symmetric latent spaces. This fundamental limitation can be addressed by\nconsidering equivariant conditional distributions, instead of equivariant\nfunctions. We present novel theoretical results that establish necessary and\nsufficient conditions for representing such distributions. Concretely, this\nrepresentation provides a practical framework for breaking symmetries in any\nequivariant network via randomized canonicalization. Our method, SymPE\n(Symmetry-breaking Positional Encodings), admits a simple interpretation in\nterms of positional encodings. This approach expands the representational power\nof equivariant networks while retaining the inductive bias of symmetry, which\nwe justify through generalization bounds. Experimental results demonstrate that\nSymPE significantly improves performance of group-equivariant and graph neural\nnetworks across diffusion models for graphs, graph autoencoders, and lattice\nspin system modeling.",
    "published_date": "2025-03-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.21985v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.21964v1",
    "title": "NeuroLIP: Interpretable and Fair Cross-Modal Alignment of fMRI and Phenotypic Text",
    "authors": [
      "Yanting Yang",
      "Xiaoxiao Li"
    ],
    "author_ids": [],
    "abstract": "Integrating functional magnetic resonance imaging (fMRI) connectivity data\nwith phenotypic textual descriptors (e.g., disease label, demographic data)\nholds significant potential to advance our understanding of neurological\nconditions. However, existing cross-modal alignment methods often lack\ninterpretability and risk introducing biases by encoding sensitive attributes\ntogether with diagnostic-related features. In this work, we propose NeuroLIP, a\nnovel cross-modal contrastive learning framework. We introduce text\ntoken-conditioned attention (TTCA) and cross-modal alignment via localized\ntokens (CALT) to the brain region-level embeddings with each disease-related\nphenotypic token. It improves interpretability via token-level attention maps,\nrevealing brain region-disease associations. To mitigate bias, we propose a\nloss for sensitive attribute disentanglement that maximizes the attention\ndistance between disease tokens and sensitive attribute tokens, reducing\nunintended correlations in downstream predictions. Additionally, we incorporate\na negative gradient technique that reverses the sign of CALT loss on sensitive\nattributes, further discouraging the alignment of these features. Experiments\non neuroimaging datasets (ABIDE and ADHD-200) demonstrate NeuroLIP's\nsuperiority in terms of fairness metrics while maintaining the overall best\nstandard metric performance. Qualitative visualization of attention maps\nhighlights neuroanatomical patterns aligned with diagnostic characteristics,\nvalidated by the neuroscientific literature. Our work advances the development\nof transparent and equitable neuroimaging AI.",
    "published_date": "2025-03-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "q-bio.NC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.21964v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.22762v1",
    "title": "The Cost of Local and Global Fairness in Federated Learning",
    "authors": [
      "Yuying Duan",
      "Gelei Xu",
      "Yiyu Shi",
      "Michael Lemmon"
    ],
    "author_ids": [],
    "abstract": "With the emerging application of Federated Learning (FL) in finance, hiring\nand healthcare, FL models are regulated to be fair, preventing disparities with\nrespect to legally protected attributes such as race or gender. Two concepts of\nfairness are important in FL: global and local fairness. Global fairness\naddresses the disparity across the entire population and local fairness is\nconcerned with the disparity within each client. Prior fair FL frameworks have\nimproved either global or local fairness without considering both. Furthermore,\nwhile the majority of studies on fair FL focuses on binary settings, many\nreal-world applications are multi-class problems. This paper proposes a\nframework that investigates the minimum accuracy lost for enforcing a specified\nlevel of global and local fairness in multi-class FL settings. Our framework\nleads to a simple post-processing algorithm that derives fair outcome\npredictors from the Bayesian optimal score functions. Experimental results show\nthat our algorithm outperforms the current state of the art (SOTA) with regard\nto the accuracy-fairness tradoffs, computational and communication costs. Codes\nare available at:\nhttps://github.com/papersubmission678/The-cost-of-local-and-global-fairness-in-FL .",
    "published_date": "2025-03-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.22762v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.13864v1",
    "title": "Personal Data Protection in Smart Home Activity Monitoring for Digital Health: A Case Study",
    "authors": [
      "Claudio Bettini",
      "Azin Moradbeikie",
      "Gabriele Civitarese"
    ],
    "author_ids": [],
    "abstract": "Researchers in pervasive computing have worked for decades on sensor-based\nhuman activity recognition (HAR). Among the digital health applications, the\nrecognition of activities of daily living (ADL) in smart home environments\nenables the identification of behavioral changes that clinicians consider as a\ndigital bio-marker of early stages of cognitive decline. The real deployment of\nsensor-based HAR systems in the homes of elderly subjects poses several\nchallenges, with privacy and ethical concerns being major ones. This paper\nreports our experience applying privacy by design principles to develop and\ndeploy one of these systems.",
    "published_date": "2025-03-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.13864v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.21502v1",
    "title": "ALADIN-$β$: A Distributed Optimization Algorithm for Solving MPCC Problems",
    "authors": [
      "Yifei Wang",
      "Shuting Wu",
      "Genke Yang",
      "Jian Chu",
      "Xu Du"
    ],
    "author_ids": [],
    "abstract": "Mathematical Programs with Complementarity Constraints (MPCC) are critical in\nvarious real-world applications but notoriously challenging due to\nnon-smoothness and degeneracy from complementarity constraints. The\n$\\ell_1$-Exact Penalty-Barrier enhanced \\texttt{IPOPT} improves performance and\nrobustness by introducing additional inequality constraints and decision\nvariables. However, this comes at the cost of increased computational\ncomplexity due to the higher dimensionality and additional constraints\nintroduced in the centralized formulation. To mitigate this, we propose a\ndistributed structure-splitting reformulation that decomposes these inequality\nconstraints and auxiliary variables into independent sub-problems. Furthermore,\nwe introduce Augmented Lagrangian Alternating Direction Inexact Newton\n(ALADIN)-$\\beta$, a novel approach that integrates the $\\ell_1$-Exact\nPenalty-Barrier method with ALADIN to efficiently solve the distributed\nreformulation. Numerical experiments demonstrate that even without a\nglobalization strategy, the proposed distributed approach achieves fast\nconvergence while maintaining high precision.",
    "published_date": "2025-03-27T00:00:00",
    "year": 2025,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.21502v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.21459v1",
    "title": "RoadSocial: A Diverse VideoQA Dataset and Benchmark for Road Event Understanding from Social Video Narratives",
    "authors": [
      "Chirag Parikh",
      "Deepti Rawat",
      "Rakshitha R. T.",
      "Tathagata Ghosh",
      "Ravi Kiran Sarvadevabhatla"
    ],
    "author_ids": [],
    "abstract": "We introduce RoadSocial, a large-scale, diverse VideoQA dataset tailored for\ngeneric road event understanding from social media narratives. Unlike existing\ndatasets limited by regional bias, viewpoint bias and expert-driven\nannotations, RoadSocial captures the global complexity of road events with\nvaried geographies, camera viewpoints (CCTV, handheld, drones) and rich social\ndiscourse. Our scalable semi-automatic annotation framework leverages Text LLMs\nand Video LLMs to generate comprehensive question-answer pairs across 12\nchallenging QA tasks, pushing the boundaries of road event understanding.\nRoadSocial is derived from social media videos spanning 14M frames and 414K\nsocial comments, resulting in a dataset with 13.2K videos, 674 tags and 260K\nhigh-quality QA pairs. We evaluate 18 Video LLMs (open-source and proprietary,\ndriving-specific and general-purpose) on our road event understanding\nbenchmark. We also demonstrate RoadSocial's utility in improving road event\nunderstanding capabilities of general-purpose Video LLMs.",
    "published_date": "2025-03-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.21459v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.21315v1",
    "title": "Tricking Retrievers with Influential Tokens: An Efficient Black-Box Corpus Poisoning Attack",
    "authors": [
      "Cheng Wang",
      "Yiwei Wang",
      "Yujun Cai",
      "Bryan Hooi"
    ],
    "author_ids": [],
    "abstract": "Retrieval-augmented generation (RAG) systems enhance large language models by\nincorporating external knowledge, addressing issues like outdated internal\nknowledge and hallucination. However, their reliance on external knowledge\nbases makes them vulnerable to corpus poisoning attacks, where adversarial\npassages can be injected to manipulate retrieval results. Existing methods for\ncrafting such passages, such as random token replacement or training inversion\nmodels, are often slow and computationally expensive, requiring either access\nto retriever's gradients or large computational resources. To address these\nlimitations, we propose Dynamic Importance-Guided Genetic Algorithm (DIGA), an\nefficient black-box method that leverages two key properties of retrievers:\ninsensitivity to token order and bias towards influential tokens. By focusing\non these characteristics, DIGA dynamically adjusts its genetic operations to\ngenerate effective adversarial passages with significantly reduced time and\nmemory usage. Our experimental evaluation shows that DIGA achieves superior\nefficiency and scalability compared to existing methods, while maintaining\ncomparable or better attack success rates across multiple datasets.",
    "published_date": "2025-03-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.21315v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.21237v1",
    "title": "Bias-Aware Agent: Enhancing Fairness in AI-Driven Knowledge Retrieval",
    "authors": [
      "Karanbir Singh",
      "William Ngu"
    ],
    "author_ids": [],
    "abstract": "Advancements in retrieving accessible information have evolved faster in the\nlast few years compared to the decades since the internet's creation. Search\nengines, like Google, have been the number one way to find relevant data. They\nhave always relied on the user's abilities to find the best information in its\nbillions of links and sources at everybody's fingertips. The advent of large\nlanguage models (LLMs) has completely transformed the field of information\nretrieval. The LLMs excel not only at retrieving relevant knowledge but also at\nsummarizing it effectively, making information more accessible and consumable\nfor users. On top of it, the rise of AI Agents has introduced another aspect to\ninformation retrieval i.e. dynamic information retrieval which enables the\nintegration of real-time data such as weather forecasts, and financial data\nwith the knowledge base to curate context-aware knowledge. However, despite\nthese advancements the agents remain susceptible to issues of bias and\nfairness, challenges deeply rooted within the knowledge base and training of\nLLMs. This study introduces a novel approach to bias-aware knowledge retrieval\nby leveraging agentic framework and the innovative use of bias detectors as\ntools to identify and highlight inherent biases in the retrieved content. By\nempowering users with transparency and awareness, this approach aims to foster\nmore equitable information systems and promote the development of responsible\nAI.",
    "published_date": "2025-03-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.21237v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.21165v1",
    "title": "Extending Silicon Lifetime: A Review of Design Techniques for Reliable Integrated Circuits",
    "authors": [
      "Shaik Jani Babu",
      "Fan Hu",
      "Linyu Zhu",
      "Sonal Singhal",
      "Xinfei Guo"
    ],
    "author_ids": [],
    "abstract": "Reliability has become an increasing concern in modern computing. Integrated\ncircuits (ICs) are the backbone of modern computing devices across industries,\nincluding artificial intelligence (AI), consumer electronics, healthcare,\nautomotive, industrial, and aerospace. Moore Law has driven the semiconductor\nIC industry toward smaller dimensions, improved performance, and greater energy\nefficiency. However, as transistors shrink to atomic scales, aging-related\ndegradation mechanisms such as Bias Temperature Instability (BTI), Hot Carrier\nInjection (HCI), Time-Dependent Dielectric Breakdown (TDDB), Electromigration\n(EM), and stochastic aging-induced variations have become major reliability\nthreats. From an application perspective, applications like AI training and\nautonomous driving require continuous and sustainable operation to minimize\nrecovery costs and enhance safety. Additionally, the high cost of chip\nreplacement and reproduction underscores the need for extended lifespans. These\nfactors highlight the urgency of designing more reliable ICs. This survey\naddresses the critical aging issues in ICs, focusing on fundamental degradation\nmechanisms and mitigation strategies. It provides a comprehensive overview of\naging impact and the methods to counter it, starting with the root causes of\naging and summarizing key monitoring techniques at both circuit and system\nlevels. A detailed analysis of circuit-level mitigation strategies highlights\nthe distinct aging characteristics of digital, analog, and SRAM circuits,\nemphasizing the need for tailored solutions. The survey also explores emerging\nsoftware approaches in design automation, aging characterization, and\nmitigation, which are transforming traditional reliability optimization.\nFinally, it outlines the challenges and future directions for improving aging\nmanagement and ensuring the long-term reliability of ICs across diverse\napplications.",
    "published_date": "2025-03-27T00:00:00",
    "year": 2025,
    "categories": [
      "eess.SY",
      "cs.AR",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.21165v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.21092v1",
    "title": "FAIR-QR: Enhancing Fairness-aware Information Retrieval through Query Refinement",
    "authors": [
      "Fumian Chen",
      "Hui Fang"
    ],
    "author_ids": [],
    "abstract": "Information retrieval systems such as open web search and recommendation\nsystems are ubiquitous and significantly impact how people receive and consume\nonline information. Previous research has shown the importance of fairness in\ninformation retrieval systems to combat the issue of echo chambers and mitigate\nthe rich-get-richer effect. Therefore, various fairness-aware information\nretrieval methods have been proposed. Score-based fairness-aware information\nretrieval algorithms, focusing on statistical parity, are interpretable but\ncould be mathematically infeasible and lack generalizability. In contrast,\nlearning-to-rank-based fairness-aware information retrieval algorithms using\nfairness-aware loss functions demonstrate strong performance but lack\ninterpretability. In this study, we proposed a novel and interpretable\nframework that recursively refines query keywords to retrieve documents from\nunderrepresented groups and achieve group fairness. Retrieved documents using\nrefined queries will be re-ranked to ensure relevance. Our method not only\nshows promising retrieval results regarding relevance and fairness but also\npreserves interpretability by showing refined keywords used at each iteration.",
    "published_date": "2025-03-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.21092v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.20989v1",
    "title": "Inferring fine-grained migration patterns across the United States",
    "authors": [
      "Gabriel Agostini",
      "Rachel Young",
      "Maria Fitzpatrick",
      "Nikhil Garg",
      "Emma Pierson"
    ],
    "author_ids": [],
    "abstract": "Fine-grained migration data illuminate important demographic, environmental,\nand health phenomena. However, migration datasets within the United States\nremain lacking: publicly available Census data are neither spatially nor\ntemporally granular, and proprietary data have higher resolution but\ndemographic and other biases. To address these limitations, we develop a\nscalable iterative-proportional-fitting based method which reconciles\nhigh-resolution but biased proprietary data with low-resolution but more\nreliable Census data. We apply this method to produce MIGRATE, a dataset of\nannual migration matrices from 2010 - 2019 which captures flows between 47.4\nbillion pairs of Census Block Groups -- about four thousand times more granular\nthan publicly available data. These estimates are highly correlated with\nexternal ground-truth datasets, and improve accuracy and reduce bias relative\nto raw proprietary data. We publicly release MIGRATE estimates and provide a\ncase study illustrating how they reveal granular patterns of migration in\nresponse to California wildfires.",
    "published_date": "2025-03-26T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.20989v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.20960v2",
    "title": "Multi-Modal Framing Analysis of News",
    "authors": [
      "Arnav Arora",
      "Srishti Yadav",
      "Maria Antoniak",
      "Serge Belongie",
      "Isabelle Augenstein"
    ],
    "author_ids": [],
    "abstract": "Automated frame analysis of political communication is a popular task in\ncomputational social science that is used to study how authors select aspects\nof a topic to frame its reception. So far, such studies have been narrow, in\nthat they use a fixed set of pre-defined frames and focus only on the text,\nignoring the visual contexts in which those texts appear. Especially for\nframing in the news, this leaves out valuable information about editorial\nchoices, which include not just the written article but also accompanying\nphotographs. To overcome such limitations, we present a method for conducting\nmulti-modal, multi-label framing analysis at scale using large\n(vision-)language models. Grounding our work in framing theory, we extract\nlatent meaning embedded in images used to convey a certain point and contrast\nthat to the text by comparing the respective frames used. We also identify\nhighly partisan framing of topics with issue-specific frame analysis found in\nprior qualitative work. We demonstrate a method for doing scalable integrative\nframing analysis of both text and image in news, providing a more complete\npicture for understanding media bias.",
    "published_date": "2025-03-26T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.20960v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.20959v1",
    "title": "Sociotechnical Effects of Machine Translation",
    "authors": [
      "Joss Moorkens",
      "Andy Way",
      "Séamus Lankford"
    ],
    "author_ids": [],
    "abstract": "While the previous chapters have shown how machine translation (MT) can be\nuseful, in this chapter we discuss some of the side-effects and risks that are\nassociated, and how they might be mitigated. With the move to neural MT and\napproaches using Large Language Models (LLMs), there is an associated impact on\nclimate change, as the models built by multinational corporations are massive.\nThey are hugely expensive to train, consume large amounts of electricity, and\noutput huge volumes of kgCO2 to boot. However, smaller models which still\nperform to a high level of quality can be built with much lower carbon\nfootprints, and tuning pre-trained models saves on the requirement to train\nfrom scratch. We also discuss the possible detrimental effects of MT on\ntranslators and other users. The topics of copyright and ownership of data are\ndiscussed, as well as ethical considerations on data and MT use. Finally, we\nshow how if done properly, using MT in crisis scenarios can save lives, and we\nprovide a method of how this might be done.",
    "published_date": "2025-03-26T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.20959v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.20783v1",
    "title": "Understanding R1-Zero-Like Training: A Critical Perspective",
    "authors": [
      "Zichen Liu",
      "Changyu Chen",
      "Wenjun Li",
      "Penghui Qi",
      "Tianyu Pang",
      "Chao Du",
      "Wee Sun Lee",
      "Min Lin"
    ],
    "author_ids": [],
    "abstract": "DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can\ndirectly enhance the reasoning capabilities of LLMs without supervised\nfine-tuning. In this work, we critically examine R1-Zero-like training by\nanalyzing its two core components: base models and RL. We investigate a wide\nrange of base models, including DeepSeek-V3-Base, to understand how pretraining\ncharacteristics influence RL performance. Our analysis reveals that\nDeepSeek-V3-Base already exhibit ''Aha moment'', while Qwen2.5 base models\ndemonstrate strong reasoning capabilities even without prompt templates,\nsuggesting potential pretraining biases. Additionally, we identify an\noptimization bias in Group Relative Policy Optimization (GRPO), which\nartificially increases response length (especially for incorrect outputs)\nduring training. To address this, we introduce Dr. GRPO, an unbiased\noptimization method that improves token efficiency while maintaining reasoning\nperformance. Leveraging these insights, we present a minimalist R1-Zero recipe\nthat achieves 43.3% accuracy on AIME 2024 with a 7B base model, establishing a\nnew state-of-the-art. Our code is available at\nhttps://github.com/sail-sg/understand-r1-zero.",
    "published_date": "2025-03-26T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.20783v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.20781v1",
    "title": "BASKET: A Large-Scale Video Dataset for Fine-Grained Skill Estimation",
    "authors": [
      "Yulu Pan",
      "Ce Zhang",
      "Gedas Bertasius"
    ],
    "author_ids": [],
    "abstract": "We present BASKET, a large-scale basketball video dataset for fine-grained\nskill estimation. BASKET contains 4,477 hours of video capturing 32,232\nbasketball players from all over the world. Compared to prior skill estimation\ndatasets, our dataset includes a massive number of skilled participants with\nunprecedented diversity in terms of gender, age, skill level, geographical\nlocation, etc. BASKET includes 20 fine-grained basketball skills, challenging\nmodern video recognition models to capture the intricate nuances of player\nskill through in-depth video analysis. Given a long highlight video (8-10\nminutes) of a particular player, the model needs to predict the skill level\n(e.g., excellent, good, average, fair, poor) for each of the 20 basketball\nskills. Our empirical analysis reveals that the current state-of-the-art video\nmodels struggle with this task, significantly lagging behind the human\nbaseline. We believe that BASKET could be a useful resource for developing new\nvideo models with advanced long-range, fine-grained recognition capabilities.\nIn addition, we hope that our dataset will be useful for domain-specific\napplications such as fair basketball scouting, personalized player development,\nand many others. Dataset and code are available at\nhttps://github.com/yulupan00/BASKET.",
    "published_date": "2025-03-26T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.20781v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.20646v2",
    "title": "Immersive and Wearable Thermal Rendering for Augmented Reality",
    "authors": [
      "Alexandra Watkins",
      "Ritam Ghosh",
      "Evan Chow",
      "Nilanjan Sarkar"
    ],
    "author_ids": [],
    "abstract": "In augmented reality (AR), where digital content is overlaid onto the real\nworld, realistic thermal feedback has been shown to enhance immersion. Yet\ncurrent thermal feedback devices, heavily influenced by the needs of virtual\nreality, often hinder physical interactions and are ineffective for immersion\nin AR. To bridge this gap, we have identified three design considerations\nrelevant for AR thermal feedback: indirect feedback to maintain dexterity,\nthermal passthrough to preserve real-world temperature perception, and\nspatiotemporal rendering for dynamic sensations. We then created a unique and\ninnovative thermal feedback device that satisfies these criteria. Human subject\nexperiments assessing perceptual sensitivity, object temperature matching,\nspatial pattern recognition, and moving thermal stimuli demonstrated the impact\nof our design, enabling realistic temperature discrimination, virtual object\nperception, and enhanced immersion. These findings demonstrate that carefully\ndesigned thermal feedback systems can bridge the sensory gap between physical\nand virtual interactions, enhancing AR realism and usability.",
    "published_date": "2025-03-26T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.20646v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.22735v1",
    "title": "Training in translation tools and technologies: Findings of the EMT survey 2023",
    "authors": [
      "Andrew Rothwell",
      "Joss Moorkens",
      "Tomas Svoboda"
    ],
    "author_ids": [],
    "abstract": "This article reports on the third iteration of a survey of computerized tools\nand technologies taught as part of postgraduate translation training\nprogrammes. While the survey was carried out under the aegis of the EMT\nNetwork, more than half of responses are from outside that network. The results\nshow the responsiveness of programmes to innovations in translation technology,\nwith increased compulsory inclusion of machine translation, post-editing, and\nquality evaluation, and a rapid response to the release of generative tools.\nThe flexibility required during the Covid-19 pandemic has also led to some\nlasting changes to programmes. While the range of tools being taught has\ncontinued to expand, programmes seem to be consolidating their core offering\naround cloud-based software with cost-free academic access. There has also been\nan increase in the embedding of professional contexts and workflows associated\nwith translation technology. Generic file management and data security skills\nhave increased in perceived importance, and legal and ethical issues related to\ntranslation data have also become more prominent. In terms of course delivery\nthe shift away from conventional labs identified in EMT2017 has accelerated\nmarkedly, no doubt partly driven by the pandemic, accompanied by a dramatic\nexpansion in the use of students' personal devices.",
    "published_date": "2025-03-26T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.22735v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.20546v1",
    "title": "Regression-Based Estimation of Causal Effects in the Presence of Selection Bias and Confounding",
    "authors": [
      "Marlies Hafer",
      "Alexander Marx"
    ],
    "author_ids": [],
    "abstract": "We consider the problem of estimating the expected causal effect $E[Y|do(X)]$\nfor a target variable $Y$ when treatment $X$ is set by intervention, focusing\non continuous random variables. In settings without selection bias or\nconfounding, $E[Y|do(X)] = E[Y|X]$, which can be estimated using standard\nregression methods. However, regression fails when systematic missingness\ninduced by selection bias, or confounding distorts the data. Boeken et al.\n[2023] show that when training data is subject to selection, proxy variables\nunaffected by this process can, under certain constraints, be used to correct\nfor selection bias to estimate $E[Y|X]$, and hence $E[Y|do(X)]$, reliably. When\ndata is additionally affected by confounding, however, this equality is no\nlonger valid.\n  Building on these results, we consider a more general setting and propose a\nframework that incorporates both selection bias and confounding. Specifically,\nwe derive theoretical conditions ensuring identifiability and recoverability of\ncausal effects under access to external data and proxy variables. We further\nintroduce a two-step regression estimator (TSR), capable of exploiting proxy\nvariables to adjust for selection bias while accounting for confounding. We\nshow that TSR coincides with prior work if confounding is absent, but achieves\na lower variance. Extensive simulation studies validate TSR's correctness for\nscenarios which may include both selection bias and confounding with proxy\nvariables.",
    "published_date": "2025-03-26T00:00:00",
    "year": 2025,
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.ME"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.20546v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.20483v1",
    "title": "Dissecting and Mitigating Diffusion Bias via Mechanistic Interpretability",
    "authors": [
      "Yingdong Shi",
      "Changming Li",
      "Yifan Wang",
      "Yongxiang Zhao",
      "Anqi Pang",
      "Sibei Yang",
      "Jingyi Yu",
      "Kan Ren"
    ],
    "author_ids": [],
    "abstract": "Diffusion models have demonstrated impressive capabilities in synthesizing\ndiverse content. However, despite their high-quality outputs, these models\noften perpetuate social biases, including those related to gender and race.\nThese biases can potentially contribute to harmful real-world consequences,\nreinforcing stereotypes and exacerbating inequalities in various social\ncontexts. While existing research on diffusion bias mitigation has\npredominantly focused on guiding content generation, it often neglects the\nintrinsic mechanisms within diffusion models that causally drive biased\noutputs. In this paper, we investigate the internal processes of diffusion\nmodels, identifying specific decision-making mechanisms, termed bias features,\nembedded within the model architecture. By directly manipulating these\nfeatures, our method precisely isolates and adjusts the elements responsible\nfor bias generation, permitting granular control over the bias levels in the\ngenerated content. Through experiments on both unconditional and conditional\ndiffusion models across various social bias attributes, we demonstrate our\nmethod's efficacy in managing generation distribution while preserving image\nquality. We also dissect the discovered model mechanism, revealing different\nintrinsic features controlling fine-grained aspects of generation, boosting\nfurther research on mechanistic interpretability of diffusion models.",
    "published_date": "2025-03-26T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.20483v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.20428v1",
    "title": "Evaluating Facial Expression Recognition Datasets for Deep Learning: A Benchmark Study with Novel Similarity Metrics",
    "authors": [
      "F. Xavier Gaya-Morey",
      "Cristina Manresa-Yee",
      "Célia Martinie",
      "Jose M. Buades-Rubio"
    ],
    "author_ids": [],
    "abstract": "This study investigates the key characteristics and suitability of widely\nused Facial Expression Recognition (FER) datasets for training deep learning\nmodels. In the field of affective computing, FER is essential for interpreting\nhuman emotions, yet the performance of FER systems is highly contingent on the\nquality and diversity of the underlying datasets. To address this issue, we\ncompiled and analyzed 24 FER datasets, including those targeting specific age\ngroups such as children, adults, and the elderly, and processed them through a\ncomprehensive normalization pipeline. In addition, we enriched the datasets\nwith automatic annotations for age and gender, enabling a more nuanced\nevaluation of their demographic properties. To further assess dataset efficacy,\nwe introduce three novel metricsLocal, Global, and Paired Similarity, which\nquantitatively measure dataset difficulty, generalization capability, and\ncross-dataset transferability. Benchmark experiments using state-of-the-art\nneural networks reveal that large-scale, automatically collected datasets\n(e.g., AffectNet, FER2013) tend to generalize better, despite issues with\nlabeling noise and demographic biases, whereas controlled datasets offer higher\nannotation quality but limited variability. Our findings provide actionable\nrecommendations for dataset selection and design, advancing the development of\nmore robust, fair, and effective FER systems.",
    "published_date": "2025-03-26T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.20428v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.20414v1",
    "title": "Active Data Sampling and Generation for Bias Remediation",
    "authors": [
      "Antonio Maratea",
      "Rita Perna"
    ],
    "author_ids": [],
    "abstract": "Adequate sampling space coverage is the keystone to effectively train\ntrustworthy Machine Learning models. Unfortunately, real data do carry several\ninherent risks due to the many potential biases they exhibit when gathered\nwithout a proper random sampling over the reference population, and most of the\ntimes this is way too expensive or time consuming to be a viable option.\nDepending on how training data have been gathered, unmitigated biases can lead\nto harmful or discriminatory consequences that ultimately hinders large scale\napplicability of pre-trained models and undermine their truthfulness or\nfairness expectations. In this paper, a mixed active sampling and data\ngeneration strategy -- called samplation -- is proposed as a mean to compensate\nduring fine-tuning of a pre-trained classifer the unfair classifications it\nproduces, assuming that the training data come from a non-probabilistic\nsampling schema. Given a pre-trained classifier, first a fairness metric is\nevaluated on a test set, then new reservoirs of labeled data are generated and\nfinally a number of reversely-biased artificial samples are generated for the\nfine-tuning of the model. Using as case study Deep Models for visual semantic\nrole labeling, the proposed method has been able to fully cure a simulated\ngender bias starting from a 90/10 imbalance, with only a small percentage of\nnew data and with a minor effect on accuracy.",
    "published_date": "2025-03-26T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.20414v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.20391v1",
    "title": "Generative AI and News Consumption: Design Fictions and Critical Analysis",
    "authors": [
      "Joel Kiskola",
      "Henrik Rydenfelt",
      "Thomas Olsson",
      "Lauri Haapanen",
      "Noora Vänttinen",
      "Matti Nelimarkka",
      "Minna Vigren",
      "Salla-Maaria Laaksonen",
      "Tuukka Lehtiniemi"
    ],
    "author_ids": [],
    "abstract": "The emergence of Generative AI features in news applications may radically\nchange news consumption and challenge journalistic practices. To explore the\nfuture potentials and risks of this understudied area, we created six design\nfictions depicting scenarios such as virtual companions delivering news\nsummaries to the user, AI providing context to news topics, and content being\ntransformed into other formats on demand. The fictions, discussed with a\nmulti-disciplinary group of experts, enabled a critical examination of the\ndiverse ethical, societal, and journalistic implications of AI shaping this\neveryday activity. The discussions raised several concerns, suggesting that\nsuch consumer-oriented AI applications can clash with journalistic values and\nprocesses. These include fears that neither consumers nor AI could successfully\nbalance engagement, objectivity, and truth, leading to growing detachment from\nshared understanding. We offer critical insights into the potential long-term\neffects to guide design efforts in this emerging application area of GenAI.",
    "published_date": "2025-03-26T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.20391v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.20276v1",
    "title": "Small-Signal Stability Condition of Inverter-Integrated Power Systems: Closed-Form Expression by Stationary Power Flow Variables",
    "authors": [
      "Taku Nishino",
      "Yoshiyuki Onishi",
      "Takayuki Ishizaki"
    ],
    "author_ids": [],
    "abstract": "This paper shows that a necessary and sufficient condition for the\nsmall-signal stability of an inverter-integrated power system can be expressed\nin terms of semidefinite matrix inequalities determined only by the synchronous\nreactance of the components, the susceptance matrix of the transmission\nnetwork, and the stationary values of the power flow distribution. To derive\nthe stability condition, we consider a class of grid-forming inverters\ncorresponding to a singular perturbation of the synchronous generator. The\nresulting matrix inequality condition, which has twice as many dimensions as\nthe number of buses and is independent of the dynamics of the connected\ncomponents, is expressed in terms of each component compensating in a\ndecentralized manner for the loss of frequency synchronization caused by the\nreactive power consumption in the transmission network. A simple numerical\nexample using a 3-bus power system model shows that a grid-forming inverter\nload improves power system synchronization, while a grid-following inverter\nload disrupts it.",
    "published_date": "2025-03-26T00:00:00",
    "year": 2025,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.20276v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.21819v1",
    "title": "Optimizing Safe and Aligned Language Generation: A Multi-Objective GRPO Approach",
    "authors": [
      "Xuying Li",
      "Zhuo Li",
      "Yuji Kosuga",
      "Victor Bian"
    ],
    "author_ids": [],
    "abstract": "Aligning large language models (LLMs) with human values and safety\nconstraints is challenging, especially when objectives like helpfulness,\ntruthfulness, and avoidance of harm conflict. Reinforcement Learning from Human\nFeedback (RLHF) has achieved notable success in steering models, but is complex\nand can be unstable. Recent approaches such as Direct Preference Optimization\n(DPO) simplify preference-based fine-tuning but may introduce bias or trade-off\ncertain objectives~\\cite{dpo}. In this work, we propose a Group Relative Policy\nOptimization (GRPO) framework with a multi-label reward regression model to\nachieve safe and aligned language generation. The GRPO algorithm optimizes a\npolicy by comparing groups of sampled responses, eliminating the need for a\nseparate value critic and improving training efficiency~\\cite{grpo}. We train a\nreward model to predict multiple alignment scores (e.g., safety, helpfulness,\netc.), which are combined into a single reward signal. We provide a theoretical\nderivation for using this learned multi-aspect reward within GRPO and discuss\nits advantages and limitations. Empirically, our approach improves all the\nsafety and quality metrics evaluated in language generation tasks on model\nscales (0.5B, 7B, and 14B parameters), demonstrating a robust balance of\nobjectives. We compare GRPO to PPO-based RLHF and DPO, highlighting that GRPO\nachieves alignment with significantly lower computational cost and explicit\nmulti-objective handling. \\textbf{We will open-source all trained models at\nhttps://huggingface.co/hydroxai.",
    "published_date": "2025-03-26T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.21819v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.20257v1",
    "title": "How Secure is Forgetting? Linking Machine Unlearning to Machine Learning Attacks",
    "authors": [
      "Muhammed Shafi K. P.",
      "Serena Nicolazzo",
      "Antonino Nocera",
      "Vinod P"
    ],
    "author_ids": [],
    "abstract": "As Machine Learning (ML) evolves, the complexity and sophistication of\nsecurity threats against this paradigm continue to grow as well, threatening\ndata privacy and model integrity. In response, Machine Unlearning (MU) is a\nrecent technology that aims to remove the influence of specific data from a\ntrained model, enabling compliance with privacy regulations and user requests.\nThis can be done for privacy compliance (e.g., GDPR's right to be forgotten) or\nmodel refinement. However, the intersection between classical threats in ML and\nMU remains largely unexplored. In this Systematization of Knowledge (SoK), we\nprovide a structured analysis of security threats in ML and their implications\nfor MU. We analyze four major attack classes, namely, Backdoor Attacks,\nMembership Inference Attacks (MIA), Adversarial Attacks, and Inversion Attacks,\nwe investigate their impact on MU and propose a novel classification based on\nhow they are usually used in this context. Finally, we identify open\nchallenges, including ethical considerations, and explore promising future\nresearch directions, paving the way for future research in secure and\nprivacy-preserving Machine Unlearning.",
    "published_date": "2025-03-26T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.20257v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.20823v1",
    "title": "Playing the Fool: Jailbreaking LLMs and Multimodal LLMs with Out-of-Distribution Strategy",
    "authors": [
      "Joonhyun Jeong",
      "Seyun Bae",
      "Yeonsung Jung",
      "Jaeryong Hwang",
      "Eunho Yang"
    ],
    "author_ids": [],
    "abstract": "Despite the remarkable versatility of Large Language Models (LLMs) and\nMultimodal LLMs (MLLMs) to generalize across both language and vision tasks,\nLLMs and MLLMs have shown vulnerability to jailbreaking, generating textual\noutputs that undermine safety, ethical, and bias standards when exposed to\nharmful or sensitive inputs. With the recent advancement of safety alignment\nvia preference-tuning from human feedback, LLMs and MLLMs have been equipped\nwith safety guardrails to yield safe, ethical, and fair responses with regard\nto harmful inputs. However, despite the significance of safety alignment,\nresearch on the vulnerabilities remains largely underexplored. In this paper,\nwe investigate the unexplored vulnerability of the safety alignment, examining\nits ability to consistently provide safety guarantees for\nout-of-distribution(OOD)-ifying harmful inputs that may fall outside the\naligned data distribution. Our key observation is that OOD-ifying the vanilla\nharmful inputs highly increases the uncertainty of the model to discern the\nmalicious intent within the input, leading to a higher chance of being\njailbroken. Exploiting this vulnerability, we propose JOOD, a new Jailbreak\nframework via OOD-ifying inputs beyond the safety alignment. We explore various\noff-the-shelf visual and textual transformation techniques for OOD-ifying the\nharmful inputs. Notably, we observe that even simple mixing-based techniques\nsuch as image mixup prove highly effective in increasing the uncertainty of the\nmodel, thereby facilitating the bypass of the safety alignment. Experiments\nacross diverse jailbreak scenarios demonstrate that JOOD effectively jailbreaks\nrecent proprietary LLMs and MLLMs such as GPT-4 and o1 with high attack success\nrate, which previous attack approaches have consistently struggled to\njailbreak. Code is available at https://github.com/naver-ai/JOOD.",
    "published_date": "2025-03-26T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.20823v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.07118v1",
    "title": "Sacred or Secular? Religious Bias in AI-Generated Financial Advice",
    "authors": [
      "Muhammad Salar Khan",
      "Hamza Umer"
    ],
    "author_ids": [],
    "abstract": "This study examines religious biases in AI-generated financial advice,\nfocusing on ChatGPT's responses to financial queries. Using a prompt-based\nmethodology and content analysis, we find that 50% of the financial emails\ngenerated by ChatGPT exhibit religious biases, with explicit biases present in\nboth ingroup and outgroup interactions. While ingroup biases personalize\nresponses based on religious alignment, outgroup biases introduce religious\nframing that may alienate clients or create ideological friction. These\nfindings align with broader research on AI bias and suggest that ChatGPT is not\nmerely reflecting societal biases but actively shaping financial discourse\nbased on perceived religious identity. Using the Critical Algorithm Studies\nframework, we argue that ChatGPT functions as a mediator of financial\nnarratives, selectively reinforcing religious perspectives. This study\nunderscores the need for greater transparency, bias mitigation strategies, and\nregulatory oversight to ensure neutrality in AI-driven financial services.",
    "published_date": "2025-03-26T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.ET"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.07118v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.20099v1",
    "title": "AI Identity, Empowerment, and Mindfulness in Mitigating Unethical AI Use",
    "authors": [
      "Mayssam Tarighi Shaayesteh",
      "Sara Memarian Esfahani",
      "Hossein Mohit"
    ],
    "author_ids": [],
    "abstract": "This study examines how AI identity influences psychological empowerment and\nunethical AI behavior among college students, while also exploring the\nmoderating role of IT mindfulness. Findings show that a strong AI identity\nenhances psychological empowerment and academic engagement but can also lead to\nincreased unethical AI practices. Crucially, IT mindfulness acts as an ethical\nsafeguard, promoting sensitivity to ethical concerns and reducing misuse of AI.\nThese insights have implications for educators, policymakers, and AI\ndevelopers, emphasizing For Peer Review the need for a balanced approach that\nencourages digital engagement without compromising student responsibility. The\nstudy also contributes to philosophical discussions of psychological agency,\nsuggesting that empowerment through AI can yield both positive and negative\noutcomes. Mindfulness emerges as essential in guiding ethical AI interactions.\nOverall, the research informs ongoing debates on ethics in education and AI,\noffering strategies to align technological advancement with ethical\naccountability and responsible use.",
    "published_date": "2025-03-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.20099v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.20098v1",
    "title": "Fundamental Limits of Perfect Concept Erasure",
    "authors": [
      "Somnath Basu Roy Chowdhury",
      "Avinava Dubey",
      "Ahmad Beirami",
      "Rahul Kidambi",
      "Nicholas Monath",
      "Amr Ahmed",
      "Snigdha Chaturvedi"
    ],
    "author_ids": [],
    "abstract": "Concept erasure is the task of erasing information about a concept (e.g.,\ngender or race) from a representation set while retaining the maximum possible\nutility -- information from original representations. Concept erasure is useful\nin several applications, such as removing sensitive concepts to achieve\nfairness and interpreting the impact of specific concepts on a model's\nperformance. Previous concept erasure techniques have prioritized robustly\nerasing concepts over retaining the utility of the resultant representations.\nHowever, there seems to be an inherent tradeoff between erasure and retaining\nutility, making it unclear how to achieve perfect concept erasure while\nmaintaining high utility. In this paper, we offer a fresh perspective toward\nsolving this problem by quantifying the fundamental limits of concept erasure\nthrough an information-theoretic lens. Using these results, we investigate\nconstraints on the data distribution and the erasure functions required to\nachieve the limits of perfect concept erasure. Empirically, we show that the\nderived erasure functions achieve the optimal theoretical bounds. Additionally,\nwe show that our approach outperforms existing methods on a range of synthetic\nand real-world datasets using GPT-4 representations.",
    "published_date": "2025-03-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.20098v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.22720v1",
    "title": "Why Representation Engineering Works: A Theoretical and Empirical Study in Vision-Language Models",
    "authors": [
      "Bowei Tian",
      "Xuntao Lyu",
      "Meng Liu",
      "Hongyi Wang",
      "Ang Li"
    ],
    "author_ids": [],
    "abstract": "Representation Engineering (RepE) has emerged as a powerful paradigm for\nenhancing AI transparency by focusing on high-level representations rather than\nindividual neurons or circuits. It has proven effective in improving\ninterpretability and control, showing that representations can emerge,\npropagate, and shape final model outputs in large language models (LLMs).\nHowever, in Vision-Language Models (VLMs), visual input can override factual\nlinguistic knowledge, leading to hallucinated responses that contradict\nreality. To address this challenge, we make the first attempt to extend RepE to\nVLMs, analyzing how multimodal representations are preserved and transformed.\nBuilding on our findings and drawing inspiration from successful RepE\napplications, we develop a theoretical framework that explains the stability of\nneural activity across layers using the principal eigenvector, uncovering the\nunderlying mechanism of RepE. We empirically validate these instrinsic\nproperties, demonstrating their broad applicability and significance. By\nbridging theoretical insights with empirical validation, this work transforms\nRepE from a descriptive tool into a structured theoretical framework, opening\nnew directions for improving AI robustness, fairness, and transparency.",
    "published_date": "2025-03-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.22720v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.19858v1",
    "title": "Culture Clash: When Deceptive Design Meets Diverse Player Expectations",
    "authors": [
      "Hilda Hadan",
      "Sabrina A. Sgandurra",
      "Leah Zhang-Kennedy",
      "Lennart E. Nacke"
    ],
    "author_ids": [],
    "abstract": "Deceptive game designs that manipulate players are increasingly common in the\ngaming industry, but the impact on players is not well studied. While studies\nhave revealed player frustration, there is a gap in understanding how cultural\nattributes affect the impact of deceptive design in games. This paper proposes\na new research direction on the connection between the representation of\nculture in games and player response to deceptive designs. We believe that\nunderstanding the interplay between cultural attributes and deceptive design\ncan inform the creation of games that are ethical and entertaining for players\naround the globe.",
    "published_date": "2025-03-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.19858v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.19846v2",
    "title": "Attention IoU: Examining Biases in CelebA using Attention Maps",
    "authors": [
      "Aaron Serianni",
      "Tyler Zhu",
      "Olga Russakovsky",
      "Vikram V. Ramaswamy"
    ],
    "author_ids": [],
    "abstract": "Computer vision models have been shown to exhibit and amplify biases across a\nwide array of datasets and tasks. Existing methods for quantifying bias in\nclassification models primarily focus on dataset distribution and model\nperformance on subgroups, overlooking the internal workings of a model. We\nintroduce the Attention-IoU (Attention Intersection over Union) metric and\nrelated scores, which use attention maps to reveal biases within a model's\ninternal representations and identify image features potentially causing the\nbiases. First, we validate Attention-IoU on the synthetic Waterbirds dataset,\nshowing that the metric accurately measures model bias. We then analyze the\nCelebA dataset, finding that Attention-IoU uncovers correlations beyond\naccuracy disparities. Through an investigation of individual attributes through\nthe protected attribute of Male, we examine the distinct ways biases are\nrepresented in CelebA. Lastly, by subsampling the training set to change\nattribute correlations, we demonstrate that Attention-IoU reveals potential\nconfounding variables not present in dataset labels.",
    "published_date": "2025-03-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.19846v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.19661v1",
    "title": "CoSimGen: Controllable Diffusion Model for Simultaneous Image and Mask Generation",
    "authors": [
      "Rupak Bose",
      "Chinedu Innocent Nwoye",
      "Aditya Bhat",
      "Nicolas Padoy"
    ],
    "author_ids": [],
    "abstract": "The acquisition of annotated datasets with paired images and segmentation\nmasks is a critical challenge in domains such as medical imaging, remote\nsensing, and computer vision. Manual annotation demands significant resources,\nfaces ethical constraints, and depends heavily on domain expertise. Existing\ngenerative models often target single-modality outputs, either images or\nsegmentation masks, failing to address the need for high-quality, simultaneous\nimage-mask generation. Additionally, these models frequently lack adaptable\nconditioning mechanisms, restricting control over the generated outputs and\nlimiting their applicability for dataset augmentation and rare scenario\nsimulation. We propose CoSimGen, a diffusion-based framework for controllable\nsimultaneous image and mask generation. Conditioning is intuitively achieved\nthrough (1) text prompts grounded in class semantics, (2) spatial embedding of\ncontext prompts to provide spatial coherence, and (3) spectral embedding of\ntimestep information to model noise levels during diffusion. To enhance\ncontrollability and training efficiency, the framework incorporates contrastive\ntriplet loss between text and class embeddings, alongside diffusion and\nadversarial losses. Initial low-resolution outputs 128 x 128 are super-resolved\nto 512 x 512, producing high-fidelity images and masks with strict adherence to\nconditions. We evaluate CoSimGen on metrics such as FID, KID, LPIPS, Class FID,\nPositive predicted value for image fidelity and semantic alignment of generated\nsamples over 4 diverse datasets. CoSimGen achieves state-of-the-art performance\nacross all datasets, achieving the lowest KID of 0.11 and LPIPS of 0.53 across\ndatasets.",
    "published_date": "2025-03-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.19661v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.19540v1",
    "title": "FLEX: A Benchmark for Evaluating Robustness of Fairness in Large Language Models",
    "authors": [
      "Dahyun Jung",
      "Seungyoon Lee",
      "Hyeonseok Moon",
      "Chanjun Park",
      "Heuiseok Lim"
    ],
    "author_ids": [],
    "abstract": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced interactions between users and models. These advancements concurrently\nunderscore the need for rigorous safety evaluations due to the manifestation of\nsocial biases, which can lead to harmful societal impacts. Despite these\nconcerns, existing benchmarks may overlook the intrinsic weaknesses of LLMs,\nwhich can generate biased responses even with simple adversarial instructions.\nTo address this critical gap, we introduce a new benchmark, Fairness Benchmark\nin LLM under Extreme Scenarios (FLEX), designed to test whether LLMs can\nsustain fairness even when exposed to prompts constructed to induce bias. To\nthoroughly evaluate the robustness of LLMs, we integrate prompts that amplify\npotential biases into the fairness assessment. Comparative experiments between\nFLEX and existing benchmarks demonstrate that traditional evaluations may\nunderestimate the inherent risks in models. This highlights the need for more\nstringent LLM evaluation benchmarks to guarantee safety and fairness.",
    "published_date": "2025-03-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.19540v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.19530v1",
    "title": "VectorFit : Adaptive Singular & Bias Vector Fine-Tuning of Pre-trained Foundation Models",
    "authors": [
      "Suhas G Hegde",
      "Shilpy Kaur",
      "Aruna Tiwari"
    ],
    "author_ids": [],
    "abstract": "Popular PEFT methods achieve parameter efficiency by assuming that\nincremental weight updates are inherently low-rank, which often leads to a\nperformance gap compared to full fine-tuning. While recent methods have\nattempted to address this limitation, they typically lack sufficient parameter\nand memory efficiency. We propose VectorFit, an effective and easily deployable\napproach that adaptively trains the singular vectors and biases of pre-trained\nweight matrices. We demonstrate that the utilization of structural and\ntransformational characteristics of pre-trained weights enables high-rank\nupdates comparable to those of full fine-tuning. As a result, VectorFit\nachieves superior performance with 9X less trainable parameters compared to\nstate-of-the-art PEFT methods. Through extensive experiments over 17 datasets\nspanning diverse language and vision tasks such as natural language\nunderstanding and generation, question answering, image classification, and\nimage generation, we exhibit that VectorFit consistently outperforms baselines,\neven in extremely low-budget scenarios.",
    "published_date": "2025-03-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.19530v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.19503v2",
    "title": "Adaptive Weighted Parameter Fusion with CLIP for Class-Incremental Learning",
    "authors": [
      "Juncen Guo",
      "Xiaoguang Zhu",
      "Liangyu Teng",
      "Hao Yang",
      "Jing Liu",
      "Yang Liu",
      "Liang Song"
    ],
    "author_ids": [],
    "abstract": "Class-incremental Learning (CIL) enables the model to incrementally absorb\nknowledge from new classes and build a generic classifier across all previously\nencountered classes. When the model optimizes with new classes, the knowledge\nof previous classes is inevitably erased, leading to catastrophic forgetting.\nAddressing this challenge requires making a trade-off between retaining old\nknowledge and accommodating new information. However, this balancing process\noften requires sacrificing some information, which can lead to a partial loss\nin the model's ability to discriminate between classes. To tackle this issue,\nwe design the adaptive weighted parameter fusion with Contrastive\nLanguage-Image Pre-training (CLIP), which not only takes into account the\nvariability of the data distribution of different tasks, but also retains all\nthe effective information of the parameter matrix to the greatest extent. In\naddition, we introduce a balance factor that can balance the data distribution\nalignment and distinguishability of adjacent tasks. Experimental results on\nseveral traditional benchmarks validate the superiority of the proposed method.",
    "published_date": "2025-03-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.19503v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.19426v1",
    "title": "DeCAP: Context-Adaptive Prompt Generation for Debiasing Zero-shot Question Answering in Large Language Models",
    "authors": [
      "Suyoung Bae",
      "YunSeok Choi",
      "Jee-Hyong Lee"
    ],
    "author_ids": [],
    "abstract": "While Large Language Models (LLMs) excel in zero-shot Question Answering\n(QA), they tend to expose biases in their internal knowledge when faced with\nsocially sensitive questions, leading to a degradation in performance. Existing\nzero-shot methods are efficient but fail to consider context and prevent bias\npropagation in the answers. To address this, we propose DeCAP, a method for\ndebiasing LLMs using Context-Adaptive Prompt Generation. DeCAP leverages a\nQuestion Ambiguity Detection to take appropriate debiasing actions based on the\ncontext and a Neutral Answer Guidance Generation to suppress the LLMs make\nobjective judgments about the context, minimizing the propagation of bias from\ntheir internal knowledge. Our various experiments across eight LLMs show that\nDeCAP achieves state-of-the-art zero-shot debiased QA performance. This\ndemonstrates DeCAP's efficacy in enhancing the fairness and accuracy of LLMs in\ndiverse QA settings.",
    "published_date": "2025-03-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.19426v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.19419v1",
    "title": "Factorizations of relative entropy using stochastic localization",
    "authors": [
      "Pietro Caputo",
      "Zongchen Chen",
      "Daniel Parisi"
    ],
    "author_ids": [],
    "abstract": "We derive entropy factorization estimates for spin systems using the\nstochastic localization approach proposed by Eldan and Chen-Eldan, which, in\nthis context, is equivalent to the renormalization group approach developed\nindependently by Bauerschmidt, Bodineau, and Dagallier. The method provides\napproximate Shearer-type inequalities for the corresponding Gibbs measure at\nsufficiently high temperature, without restrictions on the degree of the\nunderlying graph. For Ising systems, these are shown to hold up to the critical\ntree-uniqueness threshold, including polynomial bounds at the critical point,\nwith optimal $O(\\sqrt n)$ constants for the Curie-Weiss model at criticality.\nIn turn, these estimates imply tight mixing time bounds for arbitrary block\ndynamics or Gibbs samplers, improving over existing results. Moreover, we\nestablish new tensorization statements for the Shearer inequality asserting\nthat if a system consists of weakly interacting but otherwise arbitrary\ncomponents, each of which satisfies an approximate Shearer inequality, then the\nwhole system also satisfies such an estimate.",
    "published_date": "2025-03-25T00:00:00",
    "year": 2025,
    "categories": [
      "math.PR",
      "cs.IT",
      "math.FA",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.19419v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.03699v3",
    "title": "Reinforcing Clinical Decision Support through Multi-Agent Systems and Ethical AI Governance",
    "authors": [
      "Ying-Jung Chen",
      "Ahmad Albarqawi",
      "Chi-Sheng Chen"
    ],
    "author_ids": [],
    "abstract": "Recent advances in the data-driven medicine approach, which integrates\nethically managed and explainable artificial intelligence into clinical\ndecision support systems (CDSS), are critical to ensure reliable and effective\npatient care. This paper focuses on comparing novel agent system designs that\nuse modular agents to analyze laboratory results, vital signs, and clinical\ncontext, and to predict and validate results. We implement our agent system\nwith the eICU database, including running lab analysis, vitals-only\ninterpreters, and contextual reasoners agents first, then sharing the memory\ninto the integration agent, prediction agent, transparency agent, and a\nvalidation agent. Our results suggest that the multi-agent system (MAS)\nperformed better than the single-agent system (SAS) with mortality prediction\naccuracy (59%, 56%) and the mean error for length of stay (LOS)(4.37 days, 5.82\ndays), respectively. However, the transparency score for the SAS (86.21) is\nslightly better than the transparency score for MAS (85.5). Finally, this study\nsuggests that our agent-based framework not only improves process transparency\nand prediction accuracy but also strengthens trustworthy AI-assisted decision\nsupport in an intensive care setting.",
    "published_date": "2025-03-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "cs.MA",
      "q-bio.QM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.03699v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.19309v1",
    "title": "Iterative Hypothesis Generation for Scientific Discovery with Monte Carlo Nash Equilibrium Self-Refining Trees",
    "authors": [
      "Gollam Rabby",
      "Diyana Muhammed",
      "Prasenjit Mitra",
      "Sören Auer"
    ],
    "author_ids": [],
    "abstract": "Scientific hypothesis generation is a fundamentally challenging task in\nresearch, requiring the synthesis of novel and empirically grounded insights.\nTraditional approaches rely on human intuition and domain expertise, while\npurely large language model (LLM) based methods often struggle to produce\nhypotheses that are both innovative and reliable. To address these limitations,\nwe propose the Monte Carlo Nash Equilibrium Self-Refine Tree (MC-NEST), a novel\nframework that integrates Monte Carlo Tree Search with Nash Equilibrium\nstrategies to iteratively refine and validate hypotheses. MC-NEST dynamically\nbalances exploration and exploitation through adaptive sampling strategies,\nwhich prioritize high-potential hypotheses while maintaining diversity in the\nsearch space. We demonstrate the effectiveness of MC-NEST through comprehensive\nexperiments across multiple domains, including biomedicine, social science, and\ncomputer science. MC-NEST achieves average scores of 2.65, 2.74, and 2.80 (on a\n1-3 scale) for novelty, clarity, significance, and verifiability metrics on the\nsocial science, computer science, and biomedicine datasets, respectively,\noutperforming state-of-the-art prompt-based methods, which achieve 2.36, 2.51,\nand 2.52 on the same datasets. These results underscore MC-NEST's ability to\ngenerate high-quality, empirically grounded hypotheses across diverse domains.\nFurthermore, MC-NEST facilitates structured human-AI collaboration, ensuring\nthat LLMs augment human creativity rather than replace it. By addressing key\nchallenges such as iterative refinement and the exploration-exploitation\nbalance, MC-NEST sets a new benchmark in automated hypothesis generation.\nAdditionally, MC-NEST's ethical design enables responsible AI use, emphasizing\ntransparency and human supervision in hypothesis generation.",
    "published_date": "2025-03-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.19309v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.19301v1",
    "title": "Fairness in Proof of Team Sprint (PoTS): Evaluating Reward Distribution Across Performance Levels",
    "authors": [
      "Naoki Yonezawa"
    ],
    "author_ids": [],
    "abstract": "Blockchain consensus mechanisms must balance security, decentralization, and\nefficiency while ensuring fair participation. Proof of Team Sprint (PoTS) is a\ncooperative consensus mechanism designed to address the energy inefficiencies\nand centralization tendencies of traditional Proof of Work (PoW). Unlike PoW,\nwhere rewards disproportionately favor high-performance nodes, PoTS encourages\ncollaboration by forming teams and distributing rewards more equitably among\nparticipants. In this study, we evaluate the fairness properties of PoTS by\nanalyzing reward distribution under varying computational power distributions.\nThrough extensive simulations, we compare equal-share allocation and\nproportional reward allocation, highlighting their impact on decentralization\nand participation. Our results demonstrate that PoTS significantly reduces\nreward disparity between high-performance and low-performance nodes, fostering\na more inclusive ecosystem. Additionally, we observe that as team sizes\nincrease, the influence of individual computational power is mitigated,\nallowing lower-performance nodes to contribute meaningfully. Moreover, our\nfindings reveal that the marginal benefit of investing in extremely\nhigh-performance hardware diminishes, which discourages centralization and\naligns incentives toward sustainable participation. We also discuss the\neconomic implications of PoTS, particularly its potential to reshape blockchain\nmining strategies by balancing fairness with computational efficiency. These\ninsights contribute to the broader discussion on blockchain fairness and\nprovide a foundation for further research into cooperative consensus\nmechanisms.",
    "published_date": "2025-03-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.19301v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.19295v1",
    "title": "Exploring Semantic Feature Discrimination for Perceptual Image Super-Resolution and Opinion-Unaware No-Reference Image Quality Assessment",
    "authors": [
      "Guanglu Dong",
      "Xiangyu Liao",
      "Mingyang Li",
      "Guihuan Guo",
      "Chao Ren"
    ],
    "author_ids": [],
    "abstract": "Generative Adversarial Networks (GANs) have been widely applied to image\nsuper-resolution (SR) to enhance the perceptual quality. However, most existing\nGAN-based SR methods typically perform coarse-grained discrimination directly\non images and ignore the semantic information of images, making it challenging\nfor the super resolution networks (SRN) to learn fine-grained and\nsemantic-related texture details. To alleviate this issue, we propose a\nsemantic feature discrimination method, SFD, for perceptual SR. Specifically,\nwe first design a feature discriminator (Feat-D), to discriminate the\npixel-wise middle semantic features from CLIP, aligning the feature\ndistributions of SR images with that of high-quality images. Additionally, we\npropose a text-guided discrimination method (TG-D) by introducing learnable\nprompt pairs (LPP) in an adversarial manner to perform discrimination on the\nmore abstract output feature of CLIP, further enhancing the discriminative\nability of our method. With both Feat-D and TG-D, our SFD can effectively\ndistinguish between the semantic feature distributions of low-quality and\nhigh-quality images, encouraging SRN to generate more realistic and\nsemantic-relevant textures. Furthermore, based on the trained Feat-D and LPP,\nwe propose a novel opinion-unaware no-reference image quality assessment (OU\nNR-IQA) method, SFD-IQA, greatly improving OU NR-IQA performance without any\nadditional targeted training. Extensive experiments on classical SISR,\nreal-world SISR, and OU NR-IQA tasks demonstrate the effectiveness of our\nproposed methods.",
    "published_date": "2025-03-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.19295v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.19289v1",
    "title": "Empirical Evaluation and Scalability Analysis of Proof of Team Sprint (PoTS): Reward Fairness, Energy Efficiency, and System Stability",
    "authors": [
      "Naoki Yonezawa"
    ],
    "author_ids": [],
    "abstract": "This paper presents an empirical evaluation of the Proof of Team Sprint\n(PoTS) consensus algorithm, focusing on reward fairness, energy efficiency,\nsystem stability, and scalability. We conducted large-scale simulations\ncomparing PoTS with conventional Proof of Work (PoW) across various team sizes\nand computational conditions. In PoW, the highest-performance node ranked first\nin all 100 trials, demonstrating extreme centralization. In contrast, PoTS\nreduced this dominance: the same node ranked first only 54 times, indicating\nfairer reward distribution. Statistical analysis showed that as team size\nincreased, skewness and kurtosis of reward distributions decreased, confirming\nimproved equity among participants. PoTS also demonstrated significant energy\nsavings. The total active computation time followed a near $1/N$ scaling trend,\nreducing energy use by up to 64 times when team size was 64, while preserving\nconsensus integrity. Repeated simulations showed stable reward distributions\nand system performance, affirming PoTS's robustness. Furthermore, the\ncorrelation between performance and reward peaked at 0.90 for team size 16,\nreflecting an optimal balance between fairness and meritocracy. Overall, PoTS\noffers a cooperative, energy-efficient alternative to PoW, mitigating\ncentralization risks and promoting equitable participation. These findings\nvalidate PoTS as a sustainable and fair consensus mechanism suited for future\nblockchain systems.",
    "published_date": "2025-03-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.19289v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.19182v1",
    "title": "Evaluating Bias in LLMs for Job-Resume Matching: Gender, Race, and Education",
    "authors": [
      "Hayate Iso",
      "Pouya Pezeshkpour",
      "Nikita Bhutani",
      "Estevam Hruschka"
    ],
    "author_ids": [],
    "abstract": "Large Language Models (LLMs) offer the potential to automate hiring by\nmatching job descriptions with candidate resumes, streamlining recruitment\nprocesses, and reducing operational costs. However, biases inherent in these\nmodels may lead to unfair hiring practices, reinforcing societal prejudices and\nundermining workplace diversity. This study examines the performance and\nfairness of LLMs in job-resume matching tasks within the English language and\nU.S. context. It evaluates how factors such as gender, race, and educational\nbackground influence model decisions, providing critical insights into the\nfairness and reliability of LLMs in HR applications. Our findings indicate that\nwhile recent models have reduced biases related to explicit attributes like\ngender and race, implicit biases concerning educational background remain\nsignificant. These results highlight the need for ongoing evaluation and the\ndevelopment of advanced bias mitigation strategies to ensure equitable hiring\npractices when using LLMs in industry settings.",
    "published_date": "2025-03-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.19182v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.19134v1",
    "title": "MIRAGE: Multimodal Immersive Reasoning and Guided Exploration for Red-Team Jailbreak Attacks",
    "authors": [
      "Wenhao You",
      "Bryan Hooi",
      "Yiwei Wang",
      "Youke Wang",
      "Zong Ke",
      "Ming-Hsuan Yang",
      "Zi Huang",
      "Yujun Cai"
    ],
    "author_ids": [],
    "abstract": "While safety mechanisms have significantly progressed in filtering harmful\ntext inputs, MLLMs remain vulnerable to multimodal jailbreaks that exploit\ntheir cross-modal reasoning capabilities. We present MIRAGE, a novel multimodal\njailbreak framework that exploits narrative-driven context and role immersion\nto circumvent safety mechanisms in Multimodal Large Language Models (MLLMs). By\nsystematically decomposing the toxic query into environment, role, and action\ntriplets, MIRAGE constructs a multi-turn visual storytelling sequence of images\nand text using Stable Diffusion, guiding the target model through an engaging\ndetective narrative. This process progressively lowers the model's defences and\nsubtly guides its reasoning through structured contextual cues, ultimately\neliciting harmful responses. In extensive experiments on the selected datasets\nwith six mainstream MLLMs, MIRAGE achieves state-of-the-art performance,\nimproving attack success rates by up to 17.5% over the best baselines.\nMoreover, we demonstrate that role immersion and structured semantic\nreconstruction can activate inherent model biases, facilitating the model's\nspontaneous violation of ethical safeguards. These results highlight critical\nweaknesses in current multimodal safety mechanisms and underscore the urgent\nneed for more robust defences against cross-modal threats.",
    "published_date": "2025-03-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.19134v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.19063v1",
    "title": "COoL-TEE: Client-TEE Collaboration for Resilient Distributed Search",
    "authors": [
      "Matthieu Bettinger",
      "Etienne Rivière",
      "Sonia Ben Mokhtar",
      "Anthony Simonet-Boulogne"
    ],
    "author_ids": [],
    "abstract": "Current marketplaces rely on search mechanisms with distributed systems but\ncentralized governance, making them vulnerable to attacks, failures, censorship\nand biases. While search mechanisms with more decentralized governance (e.g.,\nDeSearch) have been recently proposed, these are still exposed to information\nhead-start attacks (IHS) despite the use of Trusted Execution Environments\n(TEEs). These attacks allow malicious users to gain a head-start over other\nusers for the discovery of new assets in the market, which give them an unfair\nadvantage in asset acquisition. We propose COoL-TEE, a TEE-based provider\nselection mechanism for distributed search, running in single- or\nmulti-datacenter environments, that is resilient to information head-start\nattacks. COoL-TEE relies on a Client-TEE collaboration, which enables clients\nto distinguish between slow providers and malicious ones. Performance\nevaluations in single- and multi-datacenter environments show that, using\nCOoL-TEE, malicious users respectively gain only up to 2% and 7% of assets more\nthan without IHS, while they can claim 20% or more on top of their fair share\nin the same conditions with DeSearch.",
    "published_date": "2025-03-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.19063v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.18947v1",
    "title": "Tuning-Free Amodal Segmentation via the Occlusion-Free Bias of Inpainting Models",
    "authors": [
      "Jae Joong Lee",
      "Bedrich Benes",
      "Raymond A. Yeh"
    ],
    "author_ids": [],
    "abstract": "Amodal segmentation aims to predict segmentation masks for both the visible\nand occluded regions of an object. Most existing works formulate this as a\nsupervised learning problem, requiring manually annotated amodal masks or\nsynthetic training data. Consequently, their performance depends on the quality\nof the datasets, which often lack diversity and scale. This work introduces a\ntuning-free approach that repurposes pretrained diffusion-based inpainting\nmodels for amodal segmentation. Our approach is motivated by the\n\"occlusion-free bias\" of inpainting models, i.e., the inpainted objects tend to\nbe complete objects without occlusions. Specifically, we reconstruct the\noccluded regions of an object via inpainting and then apply segmentation, all\nwithout additional training or fine-tuning. Experiments on five datasets\ndemonstrate the generalizability and robustness of our approach. On average,\nour approach achieves 5.3% more accurate masks over the state-of-the-art.",
    "published_date": "2025-03-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.18947v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.18842v2",
    "title": "Three Kinds of AI Ethics",
    "authors": [
      "Emanuele Ratti"
    ],
    "author_ids": [],
    "abstract": "There is an overwhelming abundance of works in AI Ethics. This growth is\nchaotic because of how sudden it is, its volume, and its multidisciplinary\nnature. This makes difficult to keep track of debates, and to systematically\ncharacterize goals, research questions, methods, and expertise required by AI\nethicists. In this article, I show that the relation between AI and ethics can\nbe characterized in at least three ways, which correspond to three\nwell-represented kinds of AI ethics: ethics and AI; ethics in AI; ethics of AI.\nI elucidate the features of these three kinds of AI Ethics, characterize their\nresearch questions, and identify the kind of expertise that each kind needs. I\nalso show how certain criticisms to AI ethics are misplaced, as being done from\nthe point of view of one kind of AI ethics, to another kind with different\ngoals. All in all, this work sheds light on the nature of AI ethics, and sets\nthe groundwork for more informed discussions about the scope, methods, and\ntraining of AI ethicists.",
    "published_date": "2025-03-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.18842v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.18826v2",
    "title": "Interpretable and Fair Mechanisms for Abstaining Classifiers",
    "authors": [
      "Daphne Lenders",
      "Andrea Pugnana",
      "Roberto Pellungrini",
      "Toon Calders",
      "Dino Pedreschi",
      "Fosca Giannotti"
    ],
    "author_ids": [],
    "abstract": "Abstaining classifiers have the option to refrain from providing a prediction\nfor instances that are difficult to classify. The abstention mechanism is\ndesigned to trade off the classifier's performance on the accepted data while\nensuring a minimum number of predictions. In this setting, often fairness\nconcerns arise when the abstention mechanism solely reduces errors for the\nmajority groups of the data, resulting in increased performance differences\nacross demographic groups. While there exist a bunch of methods that aim to\nreduce discrimination when abstaining, there is no mechanism that can do so in\nan explainable way. In this paper, we fill this gap by introducing\nInterpretable and Fair Abstaining Classifier IFAC, an algorithm that can reject\npredictions both based on their uncertainty and their unfairness. By rejecting\npossibly unfair predictions, our method reduces error and positive decision\nrate differences across demographic groups of the non-rejected data. Since the\nunfairness-based rejections are based on an interpretable-by-design method,\ni.e., rule-based fairness checks and situation testing, we create a transparent\nprocess that can empower human decision-makers to review the unfair predictions\nand make more just decisions for them. This explainable aspect is especially\nimportant in light of recent AI regulations, mandating that any high-risk\ndecision task should be overseen by human experts to reduce discrimination\nrisks.",
    "published_date": "2025-03-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.18826v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.18814v1",
    "title": "Towards Responsible AI Music: an Investigation of Trustworthy Features for Creative Systems",
    "authors": [
      "Jacopo de Berardinis",
      "Lorenzo Porcaro",
      "Albert Meroño-Peñuela",
      "Angelo Cangelosi",
      "Tess Buckley"
    ],
    "author_ids": [],
    "abstract": "Generative AI is radically changing the creative arts, by fundamentally\ntransforming the way we create and interact with cultural artefacts. While\noffering unprecedented opportunities for artistic expression and\ncommercialisation, this technology also raises ethical, societal, and legal\nconcerns. Key among these are the potential displacement of human creativity,\ncopyright infringement stemming from vast training datasets, and the lack of\ntransparency, explainability, and fairness mechanisms. As generative systems\nbecome pervasive in this domain, responsible design is crucial. Whilst previous\nwork has tackled isolated aspects of generative systems (e.g., transparency,\nevaluation, data), we take a comprehensive approach, grounding these efforts\nwithin the Ethics Guidelines for Trustworthy Artificial Intelligence produced\nby the High-Level Expert Group on AI appointed by the European Commission - a\nframework for designing responsible AI systems across seven macro requirements.\nFocusing on generative music AI, we illustrate how these requirements can be\ncontextualised for the field, addressing trustworthiness across multiple\ndimensions and integrating insights from the existing literature. We further\npropose a roadmap for operationalising these contextualised requirements,\nemphasising interdisciplinary collaboration and stakeholder engagement. Our\nwork provides a foundation for designing and evaluating responsible music\ngeneration systems, calling for collaboration among AI experts, ethicists,\nlegal scholars, and artists. This manuscript is accompanied by a website:\nhttps://amresearchlab.github.io/raim-framework/.",
    "published_date": "2025-03-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.18814v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.18808v1",
    "title": "CRCL: Causal Representation Consistency Learning for Anomaly Detection in Surveillance Videos",
    "authors": [
      "Yang Liu",
      "Hongjin Wang",
      "Zepu Wang",
      "Xiaoguang Zhu",
      "Jing Liu",
      "Peng Sun",
      "Rui Tang",
      "Jianwei Du",
      "Victor C. M. Leung",
      "Liang Song"
    ],
    "author_ids": [],
    "abstract": "Video Anomaly Detection (VAD) remains a fundamental yet formidable task in\nthe video understanding community, with promising applications in areas such as\ninformation forensics and public safety protection. Due to the rarity and\ndiversity of anomalies, existing methods only use easily collected regular\nevents to model the inherent normality of normal spatial-temporal patterns in\nan unsupervised manner. Previous studies have shown that existing unsupervised\nVAD models are incapable of label-independent data offsets (e.g., scene\nchanges) in real-world scenarios and may fail to respond to light anomalies due\nto the overgeneralization of deep neural networks. Inspired by causality\nlearning, we argue that there exist causal factors that can adequately\ngeneralize the prototypical patterns of regular events and present significant\ndeviations when anomalous instances occur. In this regard, we propose Causal\nRepresentation Consistency Learning (CRCL) to implicitly mine potential\nscene-robust causal variable in unsupervised video normality learning.\nSpecifically, building on the structural causal models, we propose\nscene-debiasing learning and causality-inspired normality learning to strip\naway entangled scene bias in deep representations and learn causal video\nnormality, respectively. Extensive experiments on benchmarks validate the\nsuperiority of our method over conventional deep representation learning.\nMoreover, ablation studies and extension validation show that the CRCL can cope\nwith label-independent biases in multi-scene settings and maintain stable\nperformance with only limited training data available.",
    "published_date": "2025-03-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.18808v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.19006v1",
    "title": "Computational Thinking with Computer Vision: Developing AI Competency in an Introductory Computer Science Course",
    "authors": [
      "Tahiya Chowdhury"
    ],
    "author_ids": [],
    "abstract": "Developing competency in artificial intelligence is becoming increasingly\ncrucial for computer science (CS) students at all levels of the CS curriculum.\nHowever, most previous research focuses on advanced CS courses, as traditional\nintroductory courses provide limited opportunities to develop AI skills and\nknowledge. This paper introduces an introductory CS course where students learn\ncomputational thinking through computer vision, a sub-field of AI, as an\napplication context. The course aims to achieve computational thinking outcomes\nalongside critical thinking outcomes that expose students to AI approaches and\ntheir societal implications. Through experiential activities such as individual\nprojects and reading discussions, our course seeks to balance technical\nlearning and critical thinking goals. Our evaluation, based on pre-and\npost-course surveys, shows an improved sense of belonging, self-efficacy, and\nAI ethics awareness among students. The results suggest that an AI-focused\ncontext can enhance participation and employability, student-selected projects\nsupport self-efficacy, and ethically grounded AI instruction can be effective\nfor interdisciplinary audiences. Students' discussions on reading assignments\ndemonstrated deep engagement with the complex challenges in today's AI\nlandscape. Finally, we share insights on scaling such courses for larger\ncohorts and improving the learning experience for introductory CS students.",
    "published_date": "2025-03-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.19006v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.18765v1",
    "title": "Group Decision-Making System with Sentiment Analysis of Discussion Chat and Fuzzy Consensus Modeling",
    "authors": [
      "Adilet Yerkin",
      "Pakizar Shamoi"
    ],
    "author_ids": [],
    "abstract": "Group Decision-Making (GDM) plays a crucial role in various real-life\nscenarios where individuals express their opinions in natural language rather\nthan structured numerical values. Traditional GDM approaches often overlook the\nsubjectivity and ambiguity present in human discussions, making it challenging\nto achieve a fair and consensus-driven decision. This paper proposes a fuzzy\nconsensus-based group decision-making system that integrates sentiment and\nemotion analysis to extract preference values from textual inputs. The proposed\nframework combines explicit voting preferences with sentiment scores derived\nfrom chat discussions, which are then processed using a Fuzzy Inference System\n(FIS) to compute a total preference score for each alternative and determine\nthe top-ranked option. To ensure fairness in group decision-making, we\nintroduce a fuzzy logic-based consensus measurement model that evaluates\nparticipants' agreement and confidence levels to assess overall feedback. To\nillustrate the effectiveness of our approach, we apply the methodology to a\nrestaurant selection scenario, where a group of individuals must decide on a\ndining option based on brief chat discussions. The results demonstrate that the\nfuzzy consensus mechanism successfully aggregates individual preferences and\nensures a balanced outcome that accurately reflects group sentiment.",
    "published_date": "2025-03-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.18765v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.18724v1",
    "title": "From Trust to Truth: Actionable policies for the use of AI in fact-checking in Germany and Ukraine",
    "authors": [
      "Veronika Solopova"
    ],
    "author_ids": [],
    "abstract": "The rise of Artificial Intelligence (AI) presents unprecedented opportunities\nand challenges for journalism, fact-checking and media regulation. While AI\noffers tools to combat disinformation and enhance media practices, its\nunregulated use and associated risks necessitate clear policies and\ncollaborative efforts. This policy paper explores the implications of\nartificial intelligence (AI) for journalism and fact-checking, with a focus on\naddressing disinformation and fostering responsible AI integration. Using\nGermany and Ukraine as key case studies, it identifies the challenges posed by\ndisinformation, proposes regulatory and funding strategies, and outlines\ntechnical standards to enhance AI adoption in media. The paper offers\nactionable recommendations to ensure AI's responsible and effective integration\ninto media ecosystems. AI presents significant opportunities to combat\ndisinformation and enhance journalistic practices. However, its implementation\nlacks cohesive regulation, leading to risks such as bias, transparency issues,\nand over-reliance on automated systems. In Ukraine, establishing an independent\nmedia regulatory framework adapted to its governance is crucial, while Germany\ncan act as a leader in advancing EU-wide collaborations and standards.\nTogether, these efforts can shape a robust AI-driven media ecosystem that\npromotes accuracy and trust.",
    "published_date": "2025-03-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.18724v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.22713v1",
    "title": "Chirp Localization via Fine-Tuned Transformer Model: A Proof-of-Concept Study",
    "authors": [
      "Nooshin Bahador",
      "Milad Lankarany"
    ],
    "author_ids": [],
    "abstract": "Spectrograms are pivotal in time-frequency signal analysis, widely used in\naudio processing and computational neuroscience. Chirp-like patterns in\nelectroencephalogram (EEG) spectrograms (marked by linear or exponential\nfrequency sweep) are key biomarkers for seizure dynamics, but automated tools\nfor their detection, localization, and feature extraction are lacking. This\nstudy bridges this gap by fine-tuning a Vision Transformer (ViT) model on\nsynthetic spectrograms, augmented with Low-Rank Adaptation (LoRA) to boost\nadaptability. We generated 100000 synthetic spectrograms with chirp parameters,\ncreating the first large-scale benchmark for chirp localization. These\nspectrograms mimic neural chirps using linear or exponential frequency sweep,\nGaussian noise, and smoothing. A ViT model, adapted for regression, predicted\nchirp parameters. LoRA fine-tuned the attention layers, enabling efficient\nupdates to the pre-trained backbone. Training used MSE loss and the AdamW\noptimizer, with a learning rate scheduler and early stopping to curb\noverfitting. Only three features were targeted: Chirp Start Time (Onset Time),\nChirp Start Frequency (Onset Frequency), and Chirp End Frequency (Offset\nFrequency). Performance was evaluated via Pearson correlation between predicted\nand actual labels. Results showed strong alignment: 0.9841 correlation for\nchirp start time, with stable inference times (137 to 140s) and minimal bias in\nerror distributions. This approach offers a tool for chirp analysis in EEG\ntime-frequency representation, filling a critical methodological void.",
    "published_date": "2025-03-24T00:00:00",
    "year": 2025,
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.SD"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.22713v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.18637v1",
    "title": "Unbiasing through Textual Descriptions: Mitigating Representation Bias in Video Benchmarks",
    "authors": [
      "Nina Shvetsova",
      "Arsha Nagrani",
      "Bernt Schiele",
      "Hilde Kuehne",
      "Christian Rupprecht"
    ],
    "author_ids": [],
    "abstract": "We propose a new \"Unbiased through Textual Description (UTD)\" video benchmark\nbased on unbiased subsets of existing video classification and retrieval\ndatasets to enable a more robust assessment of video understanding\ncapabilities. Namely, we tackle the problem that current video benchmarks may\nsuffer from different representation biases, e.g., object bias or single-frame\nbias, where mere recognition of objects or utilization of only a single frame\nis sufficient for correct prediction. We leverage VLMs and LLMs to analyze and\ndebias benchmarks from such representation biases. Specifically, we generate\nframe-wise textual descriptions of videos, filter them for specific information\n(e.g. only objects) and leverage them to examine representation biases across\nthree dimensions: 1) concept bias - determining if a specific concept (e.g.,\nobjects) alone suffice for prediction; 2) temporal bias - assessing if temporal\ninformation contributes to prediction; and 3) common sense vs. dataset bias -\nevaluating whether zero-shot reasoning or dataset correlations contribute to\nprediction. We conduct a systematic analysis of 12 popular video classification\nand retrieval datasets and create new object-debiased test splits for these\ndatasets. Moreover, we benchmark 30 state-of-the-art video models on original\nand debiased splits and analyze biases in the models. To facilitate the future\ndevelopment of more robust video understanding benchmarks and models, we\nrelease: \"UTD-descriptions\", a dataset with our rich structured descriptions\nfor each dataset, and \"UTD-splits\", a dataset of object-debiased test splits.",
    "published_date": "2025-03-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.18637v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.03695v1",
    "title": "Are Anxiety Detection Models Generalizable? A Cross-Activity and Cross-Population Study Using Wearables",
    "authors": [
      "Nilesh Kumar Sahu",
      "Snehil Gupta",
      "Haroon R Lone"
    ],
    "author_ids": [],
    "abstract": "Anxiety-provoking activities, such as public speaking, can trigger heightened\nanxiety responses in individuals with anxiety disorders. Recent research\nsuggests that physiological signals, including electrocardiogram (ECG) and\nelectrodermal activity (EDA), collected via wearable devices, can be used to\ndetect anxiety in such contexts through machine learning models. However, the\ngeneralizability of these anxiety prediction models across different activities\nand diverse populations remains underexplored-an essential step for assessing\nmodel bias and fostering user trust in broader applications. To address this\ngap, we conducted a study with 111 participants who engaged in three\nanxiety-provoking activities. Utilizing both our collected dataset and two\nwell-known publicly available datasets, we evaluated the generalizability of\nanxiety detection models within participants (for both same-activity and\ncross-activity scenarios) and across participants (within-activity and\ncross-activity). In total, we trained and tested more than 3348 anxiety\ndetection models (using six classifiers, 31 feature sets, and 18 train-test\nconfigurations). Our results indicate that three key metrics-AUROC, recall for\nanxious states, and recall for non-anxious states-were slightly above the\nbaseline score of 0.5. The best AUROC scores ranged from 0.62 to 0.73, with\nrecall for the anxious class spanning 35.19% to 74.3%. Interestingly, model\nperformance (as measured by AUROC) remained relatively stable across different\nactivities and participant groups, though recall for the anxious class did\nexhibit some variation.",
    "published_date": "2025-03-24T00:00:00",
    "year": 2025,
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.03695v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.18569v1",
    "title": "Anchor-based oversampling for imbalanced tabular data via contrastive and adversarial learning",
    "authors": [
      "Hadi Mohammadi",
      "Ehsan Nazerfard",
      "Mostafa Haghir Chehreghani"
    ],
    "author_ids": [],
    "abstract": "Imbalanced data represent a distribution with more frequencies of one class\n(majority) than the other (minority). This phenomenon occurs across various\ndomains, such as security, medical care and human activity. In imbalanced\nlearning, classification algorithms are typically inclined to classify the\nmajority class accurately, resulting in artificially high accuracy rates. As a\nresult, many minority samples are mistakenly labelled as majority-class\ninstances, resulting in a bias that benefits the majority class. This study\npresents a framework based on boundary anchor samples to tackle the imbalance\nlearning challenge. First, we select and use anchor samples to train a\nmultilayer perceptron (MLP) classifier, which acts as a prior knowledge model\nand aids the adversarial and contrastive learning procedures. Then, we designed\na novel deep generative model called Anchor Stabilized Conditional Generative\nAdversarial Network or Anch-SCGAN in short. Anch-SCGAN is supported with two\ngenerators for the minority and majority classes and a discriminator\nincorporating additional class-specific information from the pre-trained\nfeature extractor MLP. In addition, we facilitate the generator's training\nprocedure in two ways. First, we define a new generator loss function based on\nreprocessed anchor samples and contrastive learning. Second, we apply a scoring\nstrategy to stabilize the adversarial training part in generators. We train\nAnch-SCGAN and further finetune it with anchor samples to improve the precision\nof the generated samples. Our experiments on 16 real-world imbalanced datasets\nillustrate that Anch-SCGAN outperforms the renowned methods in imbalanced\nlearning.",
    "published_date": "2025-03-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.18569v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.18484v1",
    "title": "PM4Bench: A Parallel Multilingual Multi-Modal Multi-task Benchmark for Large Vision Language Model",
    "authors": [
      "Junyuan Gao",
      "Jiahe Song",
      "Jiang Wu",
      "Runchuan Zhu",
      "Guanlin Shen",
      "Shasha Wang",
      "Xingjian Wei",
      "Haote Yang",
      "Songyang Zhang",
      "Weijia Li",
      "Bin Wang",
      "Dahua Lin",
      "Lijun Wu",
      "Conghui He"
    ],
    "author_ids": [],
    "abstract": "Existing multilingual benchmarks for Large Vision Language Models (LVLMs)\nsuffer from limitations including language-specific content biases, disjointed\nmultimodal input formats, and a lack of safety evaluation. To address these\ngaps, we propose PM4Bench, the first Parallel Multilingual Multi-Modal\nMulti-task Benchmark for LVLMs. PM4Bench features a parallel corpus design\nacross 10 languages, enabling fair and accurate cross-lingual comparisons. It\nincludes the vision setting where text and queries are embedded in images,\nrequiring LVLMs to simultaneously \"see\", \"read\", and \"think\", aligning with\nreal-world applications. Additionally, PM\\textsuperscript{4}Bench incorporates\nsafety evaluations, addressing critical oversight in existing multilingual\nbenchmarks. Using PM4Bench, we evaluate 11 mainstream LVLMs, revealing\nsignificant cross-linguistic performance disparities, particularly in vision\nsettings, and identifying OCR capability as a key determinant of these\nimbalances. We will release PM4Bench at https://github.com/opendatalab/PM4Bench .",
    "published_date": "2025-03-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.18484v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.18389v1",
    "title": "Agent-based Modeling meets the Capability Approach for Human Development: Simulating Homelessness Policy-making",
    "authors": [
      "Alba Aguilera",
      "Nardine Osman",
      "Georgina Curto"
    ],
    "author_ids": [],
    "abstract": "The global rise in homelessness calls for urgent and alternative policy\nsolutions. Non-profits and governmental organizations alert about the many\nchallenges faced by people experiencing homelessness (PEH), which include not\nonly the lack of shelter but also the lack of opportunities for personal\ndevelopment. In this context, the capability approach (CA), which underpins the\nUnited Nations Sustainable Development Goals (SDGs), provides a comprehensive\nframework to assess inequity in terms of real opportunities. This paper\nexplores how the CA can be combined with agent-based modelling and\nreinforcement learning. The goals are: (1) implementing the CA as a Markov\nDecision Process (MDP), (2) building on such MDP to develop a rich\ndecision-making model that accounts for more complex motivators of behaviour,\nsuch as values and needs, and (3) developing an agent-based simulation\nframework that allows to assess alternative policies aiming to expand or\nrestore people's capabilities. The framework is developed in a real case study\nof health inequity and homelessness, working in collaboration with\nstakeholders, non-profits and domain experts. The ultimate goal of the project\nis to develop a novel agent-based simulation framework, rooted in the CA, which\ncan be replicated in a diversity of social contexts to assess policies in a\nnon-invasive way.",
    "published_date": "2025-03-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.18389v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.18358v1",
    "title": "Cost-Sensitive Learning for Long-Tailed Temporal Action Segmentation",
    "authors": [
      "Zhanzhong Pang",
      "Fadime Sener",
      "Shrinivas Ramasubramanian",
      "Angela Yao"
    ],
    "author_ids": [],
    "abstract": "Temporal action segmentation in untrimmed procedural videos aims to densely\nlabel frames into action classes. These videos inherently exhibit long-tailed\ndistributions, where actions vary widely in frequency and duration. In temporal\naction segmentation approaches, we identified a bi-level learning bias. This\nbias encompasses (1) a class-level bias, stemming from class imbalance favoring\nhead classes, and (2) a transition-level bias arising from variations in\ntransitions, prioritizing commonly observed transitions. As a remedy, we\nintroduce a constrained optimization problem to alleviate both biases. We\ndefine learning states for action classes and their associated transitions and\nintegrate them into the optimization process. We propose a novel cost-sensitive\nloss function formulated as a weighted cross-entropy loss, with weights\nadaptively adjusted based on the learning state of actions and their\ntransitions. Experiments on three challenging temporal segmentation benchmarks\nand various frameworks demonstrate the effectiveness of our approach, resulting\nin significant improvements in both per-class frame-wise and segment-wise\nperformance.",
    "published_date": "2025-03-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.18358v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.18197v1",
    "title": "FROG: Fair Removal on Graphs",
    "authors": [
      "Ziheng Chen",
      "Jiali Cheng",
      "Gabriele Tolomei",
      "Sijia Liu",
      "Hadi Amiri",
      "Yu Wang",
      "Kaushiki Nag",
      "Lu Lin"
    ],
    "author_ids": [],
    "abstract": "As compliance with privacy regulations becomes increasingly critical, the\ngrowing demand for data privacy has highlighted the significance of machine\nunlearning in many real world applications, such as social network and\nrecommender systems, many of which can be represented as graph-structured data.\nHowever, existing graph unlearning algorithms indiscriminately modify edges or\nnodes from well-trained models without considering the potential impact of such\nstructural modifications on fairness. For example, forgetting links between\nnodes with different genders in a social network may exacerbate group\ndisparities, leading to significant fairness concerns. To address these\nchallenges, we propose a novel approach that jointly optimizes the graph\nstructure and the corresponding model for fair unlearning tasks.\nSpecifically,our approach rewires the graph to enhance unlearning efficiency by\nremoving redundant edges that hinder forgetting while preserving fairness\nthrough targeted edge augmentation. Additionally, we introduce a worst-case\nevaluation mechanism to assess the reliability of fair unlearning performance.\nExtensive experiments on real-world datasets demonstrate the effectiveness of\nthe proposed approach in achieving superior unlearning outcomes.",
    "published_date": "2025-03-23T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.18197v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.18994v1",
    "title": "HH4AI: A methodological Framework for AI Human Rights impact assessment under the EUAI ACT",
    "authors": [
      "Paolo Ceravolo",
      "Ernesto Damiani",
      "Maria Elisa D'Amico",
      "Bianca de Teffe Erb",
      "Simone Favaro",
      "Nannerel Fiano",
      "Paolo Gambatesa",
      "Simone La Porta",
      "Samira Maghool",
      "Lara Mauri",
      "Niccolo Panigada",
      "Lorenzo Maria Ratto Vaquer",
      "Marta A. Tamborini"
    ],
    "author_ids": [],
    "abstract": "This paper introduces the HH4AI Methodology, a structured approach to\nassessing the impact of AI systems on human rights, focusing on compliance with\nthe EU AI Act and addressing technical, ethical, and regulatory challenges. The\npaper highlights AIs transformative nature, driven by autonomy, data, and\ngoal-oriented design, and how the EU AI Act promotes transparency,\naccountability, and safety. A key challenge is defining and assessing\n\"high-risk\" AI systems across industries, complicated by the lack of\nuniversally accepted standards and AIs rapid evolution.\n  To address these challenges, the paper explores the relevance of ISO/IEC and\nIEEE standards, focusing on risk management, data quality, bias mitigation, and\ngovernance. It proposes a Fundamental Rights Impact Assessment (FRIA)\nmethodology, a gate-based framework designed to isolate and assess risks\nthrough phases including an AI system overview, a human rights checklist, an\nimpact assessment, and a final output phase. A filtering mechanism tailors the\nassessment to the system's characteristics, targeting areas like\naccountability, AI literacy, data governance, and transparency.\n  The paper illustrates the FRIA methodology through a fictional case study of\nan automated healthcare triage service. The structured approach enables\nsystematic filtering, comprehensive risk assessment, and mitigation planning,\neffectively prioritizing critical risks and providing clear remediation\nstrategies. This promotes better alignment with human rights principles and\nenhances regulatory compliance.",
    "published_date": "2025-03-23T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.18994v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.18095v1",
    "title": "Clarifying Misconceptions in COVID-19 Vaccine Sentiment and Stance Analysis and Their Implications for Vaccine Hesitancy Mitigation: A Systematic Review",
    "authors": [
      "Lorena G Barberia",
      "Belinda Lombard",
      "Norton Trevisan Roman",
      "Tatiane C. M. Sousa"
    ],
    "author_ids": [],
    "abstract": "Background Advances in machine learning (ML) models have increased the\ncapability of researchers to detect vaccine hesitancy in social media using\nNatural Language Processing (NLP). A considerable volume of research has\nidentified the persistence of COVID-19 vaccine hesitancy in discourse shared on\nvarious social media platforms. Methods Our objective in this study was to\nconduct a systematic review of research employing sentiment analysis or stance\ndetection to study discourse towards COVID-19 vaccines and vaccination spread\non Twitter (officially known as X since 2023). Following registration in the\nPROSPERO international registry of systematic reviews, we searched papers\npublished from 1 January 2020 to 31 December 2023 that used supervised machine\nlearning to assess COVID-19 vaccine hesitancy through stance detection or\nsentiment analysis on Twitter. We categorized the studies according to a\ntaxonomy of five dimensions: tweet sample selection approach, self-reported\nstudy type, classification typology, annotation codebook definitions, and\ninterpretation of results. We analyzed if studies using stance detection report\ndifferent hesitancy trends than those using sentiment analysis by examining how\nCOVID-19 vaccine hesitancy is measured, and whether efforts were made to avoid\nmeasurement bias. Results Our review found that measurement bias is widely\nprevalent in studies employing supervised machine learning to analyze sentiment\nand stance toward COVID-19 vaccines and vaccination. The reporting errors are\nsufficiently serious that they hinder the generalisability and interpretation\nof these studies to understanding whether individual opinions communicate\nreluctance to vaccinate against SARS-CoV-2. Conclusion Improving the reporting\nof NLP methods is crucial to addressing knowledge gaps in vaccine hesitancy\ndiscourse.",
    "published_date": "2025-03-23T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG",
      "I.2.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.18095v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.08742v1",
    "title": "Simulating Filter Bubble on Short-video Recommender System with Large Language Model Agents",
    "authors": [
      "Nicholas Sukiennik",
      "Haoyu Wang",
      "Zailin Zeng",
      "Chen Gao",
      "Yong Li"
    ],
    "author_ids": [],
    "abstract": "An increasing reliance on recommender systems has led to concerns about the\ncreation of filter bubbles on social media, especially on short video platforms\nlike TikTok. However, their formation is still not entirely understood due to\nthe complex dynamics between recommendation algorithms and user feedback. In\nthis paper, we aim to shed light on these dynamics using a large language\nmodel-based simulation framework. Our work employs real-world short-video data\ncontaining rich video content information and detailed user-agents to\nrealistically simulate the recommendation-feedback cycle. Through large-scale\nsimulations, we demonstrate that LLMs can replicate real-world user-recommender\ninteractions, uncovering key mechanisms driving filter bubble formation. We\nidentify critical factors, such as demographic features and category attraction\nthat exacerbate content homogenization. To mitigate this, we design and test\ninterventions including various cold-start and feedback weighting strategies,\nshowing measurable reductions in filter bubble effects. Our framework enables\nrapid prototyping of recommendation strategies, offering actionable solutions\nto enhance content diversity in real-world systems. Furthermore, we analyze how\nLLM-inherent biases may propagate through recommendations, proposing safeguards\nto promote equity for vulnerable groups, such as women and low-income\npopulations. By examining the interplay between recommendation and LLM agents,\nthis work advances a deeper understanding of algorithmic bias and provides\npractical tools to promote inclusive digital spaces.",
    "published_date": "2025-03-23T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.08742v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.17956v1",
    "title": "On the Origins of Sampling Bias: Implications on Fairness Measurement and Mitigation",
    "authors": [
      "Sami Zhioua",
      "Ruta Binkyte",
      "Ayoub Ouni",
      "Farah Barika Ktata"
    ],
    "author_ids": [],
    "abstract": "Accurately measuring discrimination is crucial to faithfully assessing\nfairness of trained machine learning (ML) models. Any bias in measuring\ndiscrimination leads to either amplification or underestimation of the existing\ndisparity. Several sources of bias exist and it is assumed that bias resulting\nfrom machine learning is born equally by different groups (e.g. females vs\nmales, whites vs blacks, etc.). If, however, bias is born differently by\ndifferent groups, it may exacerbate discrimination against specific\nsub-populations. Sampling bias, in particular, is inconsistently used in the\nliterature to describe bias due to the sampling procedure. In this paper, we\nattempt to disambiguate this term by introducing clearly defined variants of\nsampling bias, namely, sample size bias (SSB) and underrepresentation bias\n(URB). Through an extensive set of experiments on benchmark datasets and using\nmainstream learning algorithms, we expose relevant observations in several\nmodel training scenarios. The observations are finally framed as actionable\nrecommendations for practitioners.",
    "published_date": "2025-03-23T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.17956v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.20797v1",
    "title": "\"Whose Side Are You On?\" Estimating Ideology of Political and News Content Using Large Language Models and Few-shot Demonstration Selection",
    "authors": [
      "Muhammad Haroon",
      "Magdalena Wojcieszak",
      "Anshuman Chhabra"
    ],
    "author_ids": [],
    "abstract": "The rapid growth of social media platforms has led to concerns about\nradicalization, filter bubbles, and content bias. Existing approaches to\nclassifying ideology are limited in that they require extensive human effort,\nthe labeling of large datasets, and are not able to adapt to evolving\nideological contexts. This paper explores the potential of Large Language\nModels (LLMs) for classifying the political ideology of online content in the\ncontext of the two-party US political spectrum through in-context learning\n(ICL). Our extensive experiments involving demonstration selection in\nlabel-balanced fashion, conducted on three datasets comprising news articles\nand YouTube videos, reveal that our approach significantly outperforms\nzero-shot and traditional supervised methods. Additionally, we evaluate the\ninfluence of metadata (e.g., content source and descriptions) on ideological\nclassification and discuss its implications. Finally, we show how providing the\nsource for political and non-political content influences the LLM's\nclassification.",
    "published_date": "2025-03-23T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.20797v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.17716v1",
    "title": "EMPLACE: Self-Supervised Urban Scene Change Detection",
    "authors": [
      "Tim Alpherts",
      "Sennay Ghebreab",
      "Nanne van Noord"
    ],
    "author_ids": [],
    "abstract": "Urban change is a constant process that influences the perception of\nneighbourhoods and the lives of the people within them. The field of Urban\nScene Change Detection (USCD) aims to capture changes in street scenes using\ncomputer vision and can help raise awareness of changes that make it possible\nto better understand the city and its residents. Traditionally, the field of\nUSCD has used supervised methods with small scale datasets. This constrains\nmethods when applied to new cities, as it requires labour-intensive labeling\nprocesses and forces a priori definitions of relevant change. In this paper we\nintroduce AC-1M the largest USCD dataset by far of over 1.1M images, together\nwith EMPLACE, a self-supervising method to train a Vision Transformer using our\nadaptive triplet loss. We show EMPLACE outperforms SOTA methods both as a\npre-training method for linear fine-tuning as well as a zero-shot setting.\nLastly, in a case study of Amsterdam, we show that we are able to detect both\nsmall and large changes throughout the city and that changes uncovered by\nEMPLACE, depending on size, correlate with housing prices - which in turn is\nindicative of inequity.",
    "published_date": "2025-03-22T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.17716v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.17682v1",
    "title": "Safe RLHF-V: Safe Reinforcement Learning from Human Feedback in Multimodal Large Language Models",
    "authors": [
      "Jiaming Ji",
      "Xinyu Chen",
      "Rui Pan",
      "Han Zhu",
      "Conghui Zhang",
      "Jiahao Li",
      "Donghai Hong",
      "Boyuan Chen",
      "Jiayi Zhou",
      "Kaile Wang",
      "Juntao Dai",
      "Chi-Min Chan",
      "Sirui Han",
      "Yike Guo",
      "Yaodong Yang"
    ],
    "author_ids": [],
    "abstract": "Multimodal large language models (MLLMs) are critical for developing\ngeneral-purpose AI assistants, yet they face growing safety risks. How can we\nensure that MLLMs are safely aligned to prevent undesired behaviors such as\ndiscrimination, misinformation, or violations of ethical standards? In a\nfurther step, we need to explore how to fine-tune MLLMs to enhance reasoning\nperformance while ensuring they satisfy safety constraints. Fundamentally, this\ncan be formulated as a min-max optimization problem. In this study, we propose\nSafe RLHF-V, the first multimodal safety alignment framework that jointly\noptimizes helpfulness and safety using separate multimodal reward and cost\nmodels within a Lagrangian-based constrained optimization framework. Given that\nthere is a lack of preference datasets that separate helpfulness and safety in\nmultimodal scenarios, we introduce BeaverTails-V, the first open-source dataset\nwith dual preference annotations for helpfulness and safety, along with\nmulti-level safety labels (minor, moderate, severe). Additionally, we design a\nMulti-level Guardrail System to proactively defend against unsafe queries and\nadversarial attacks. By applying the Beaver-Guard-V moderation for 5 rounds of\nfiltering and re-generation on the precursor model, the overall safety of the\nupstream model is significantly improved by an average of 40.9%. Experimental\nresults demonstrate that fine-tuning different MLLMs with Safe RLHF can\neffectively enhance model helpfulness while ensuring improved safety.\nSpecifically, Safe RLHF-V improves model safety by 34.2% and helpfulness by\n34.3%. All of datasets, models, and code can be found at\nhttps://github.com/SafeRLHF-V to support the safety development of MLLMs and\nreduce potential societal risks.",
    "published_date": "2025-03-22T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.17682v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.17677v1",
    "title": "Reducing Class-wise Confusion for Incremental Learning with Disentangled Manifolds",
    "authors": [
      "Huitong Chen",
      "Yu Wang",
      "Yan Fan",
      "Guosong Jiang",
      "Qinghua Hu"
    ],
    "author_ids": [],
    "abstract": "Class incremental learning (CIL) aims to enable models to continuously learn\nnew classes without catastrophically forgetting old ones. A promising direction\nis to learn and use prototypes of classes during incremental updates. Despite\nsimplicity and intuition, we find that such methods suffer from inadequate\nrepresentation capability and unsatisfied feature overlap. These two factors\ncause class-wise confusion and limited performance. In this paper, we develop a\nConfusion-REduced AuTo-Encoder classifier (CREATE) for CIL. Specifically, our\nmethod employs a lightweight auto-encoder module to learn compact manifold for\neach class in the latent subspace, constraining samples to be well\nreconstructed only on the semantically correct auto-encoder. Thus, the\nrepresentation stability and capability of class distributions are enhanced,\nalleviating the potential class-wise confusion problem. To further distinguish\nthe overlapped features, we propose a confusion-aware latent space separation\nloss that ensures samples are closely distributed in their corresponding\nlow-dimensional manifold while keeping away from the distributions of features\nfrom other classes. Our method demonstrates stronger representational capacity\nand discrimination ability by learning disentangled manifolds and reduces class\nconfusion. Extensive experiments on multiple datasets and settings show that\nCREATE outperforms other state-of-the-art methods up to 5.41%.",
    "published_date": "2025-03-22T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.17677v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.17661v2",
    "title": "A Qualitative Study of User Perception of M365 AI Copilot",
    "authors": [
      "Muneera Bano",
      "Didar Zowghi",
      "Jon Whittle",
      "Liming Zhu",
      "Andrew Reeson",
      "Rob Martin",
      "Jen Parsons"
    ],
    "author_ids": [],
    "abstract": "Adopting AI copilots in professional workflows presents opportunities for\nenhanced productivity, efficiency, and decision making. In this paper, we\npresent results from a six month trial of M365 Copilot conducted at our\norganisation in 2024. A qualitative interview study was carried out with 27\nparticipants. The study explored user perceptions of M365 Copilot's\neffectiveness, productivity impact, evolving expectations, ethical concerns,\nand overall satisfaction. Initial enthusiasm for the tool was met with mixed\npost trial experiences. While some users found M365 Copilot beneficial for\ntasks such as email coaching, meeting summaries, and content retrieval, others\nreported unmet expectations in areas requiring deeper contextual understanding,\nreasoning, and integration with existing workflows. Ethical concerns were a\nrecurring theme, with users highlighting issues related to data privacy,\ntransparency, and AI bias. While M365 Copilot demonstrated value in specific\noperational areas, its broader impact remained constrained by usability\nlimitations and the need for human oversight to validate AI generated outputs.",
    "published_date": "2025-03-22T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.17661v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.17569v1",
    "title": "Fairness-Driven LLM-based Causal Discovery with Active Learning and Dynamic Scoring",
    "authors": [
      "Khadija Zanna",
      "Akane Sano"
    ],
    "author_ids": [],
    "abstract": "Causal discovery (CD) plays a pivotal role in numerous scientific fields by\nclarifying the causal relationships that underlie phenomena observed in diverse\ndisciplines. Despite significant advancements in CD algorithms that enhance\nbias and fairness analyses in machine learning, their application faces\nchallenges due to the high computational demands and complexities of\nlarge-scale data. This paper introduces a framework that leverages Large\nLanguage Models (LLMs) for CD, utilizing a metadata-based approach akin to the\nreasoning processes of human experts. By shifting from pairwise queries to a\nmore scalable breadth-first search (BFS) strategy, the number of required\nqueries is reduced from quadratic to linear in terms of variable count, thereby\naddressing scalability concerns inherent in previous approaches. This method\nutilizes an Active Learning (AL) and a Dynamic Scoring Mechanism that\nprioritizes queries based on their potential information gain, combining mutual\ninformation, partial correlation, and LLM confidence scores to refine the\ncausal graph more efficiently and accurately. This BFS query strategy reduces\nthe required number of queries significantly, thereby addressing scalability\nconcerns inherent in previous approaches. This study provides a more scalable\nand efficient solution for leveraging LLMs in fairness-driven CD, highlighting\nthe effects of the different parameters on performance. We perform fairness\nanalyses on the inferred causal graphs, identifying direct and indirect effects\nof sensitive attributes on outcomes. A comparison of these analyses against\nthose from graphs produced by baseline methods highlights the importance of\naccurate causal graph construction in understanding bias and ensuring fairness\nin machine learning systems.",
    "published_date": "2025-03-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.17569v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.17504v1",
    "title": "Reimagining Support: Exploring Autistic Individuals' Visions for AI in Coping with Negative Self-Talk",
    "authors": [
      "Buse Carik",
      "Victoria Izaac",
      "Xiaohan Ding",
      "Angela Scarpa",
      "Eugenia Rho"
    ],
    "author_ids": [],
    "abstract": "Autistic individuals often experience negative self-talk (NST), leading to\nincreased anxiety and depression. While therapy is recommended, it presents\nchallenges for many autistic individuals. Meanwhile, a growing number are\nturning to large language models (LLMs) for mental health support. To\nunderstand how autistic individuals perceive AI's role in coping with NST, we\nsurveyed 200 autistic adults and interviewed practitioners. We also analyzed\nLLM responses to participants' hypothetical prompts about their NST. Our\nfindings show that participants view LLMs as useful for managing NST by\nidentifying and reframing negative thoughts. Both participants and\npractitioners recognize AI's potential to support therapy and emotional\nexpression. Participants also expressed concerns about LLMs' understanding of\nneurodivergent thought patterns, particularly due to the neurotypical bias of\nLLMs. Practitioners critiqued LLMs' responses as overly wordy, vague, and\noverwhelming. This study contributes to the growing research on AI-assisted\nmental health support, with specific insights for supporting the autistic\ncommunity.",
    "published_date": "2025-03-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.17504v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.17477v1",
    "title": "Bayesian generative models can flag performance loss, bias, and out-of-distribution image content",
    "authors": [
      "Miguel López-Pérez",
      "Marco Miani",
      "Valery Naranjo",
      "Søren Hauberg",
      "Aasa Feragen"
    ],
    "author_ids": [],
    "abstract": "Generative models are popular for medical imaging tasks such as anomaly\ndetection, feature extraction, data visualization, or image generation. Since\nthey are parameterized by deep learning models, they are often sensitive to\ndistribution shifts and unreliable when applied to out-of-distribution data,\ncreating a risk of, e.g. underrepresentation bias. This behavior can be flagged\nusing uncertainty quantification methods for generative models, but their\navailability remains limited. We propose SLUG: A new UQ method for VAEs that\ncombines recent advances in Laplace approximations with stochastic trace\nestimators to scale gracefully with image dimensionality. We show that our UQ\nscore -- unlike the VAE's encoder variances -- correlates strongly with\nreconstruction error and racial underrepresentation bias for dermatological\nimages. We also show how pixel-wise uncertainty can detect out-of-distribution\nimage content such as ink, rulers, and patches, which is known to induce\nlearning shortcuts in predictive models.",
    "published_date": "2025-03-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.17477v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.17339v1",
    "title": "Can AI expose tax loopholes? Towards a new generation of legal policy assistants",
    "authors": [
      "Peter Fratrič",
      "Nils Holzenberger",
      "David Restrepo Amariles"
    ],
    "author_ids": [],
    "abstract": "The legislative process is the backbone of a state built on solid\ninstitutions. Yet, due to the complexity of laws -- particularly tax law --\npolicies may lead to inequality and social tensions. In this study, we\nintroduce a novel prototype system designed to address the issues of tax\nloopholes and tax avoidance. Our hybrid solution integrates a natural language\ninterface with a domain-specific language tailored for planning. We demonstrate\non a case study how tax loopholes and avoidance schemes can be exposed. We\nconclude that our prototype can help enhance social welfare by systematically\nidentifying and addressing tax gaps stemming from loopholes.",
    "published_date": "2025-03-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.17339v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.17231v1",
    "title": "LoGoFair: Post-Processing for Local and Global Fairness in Federated Learning",
    "authors": [
      "Li Zhang",
      "Chaochao Chen",
      "Zhongxuan Han",
      "Qiyong Zhong",
      "Xiaolin Zheng"
    ],
    "author_ids": [],
    "abstract": "Federated learning (FL) has garnered considerable interest for its capability\nto learn from decentralized data sources. Given the increasing application of\nFL in decision-making scenarios, addressing fairness issues across different\nsensitive groups (e.g., female, male) in FL is crucial. Current research often\nfocuses on facilitating fairness at each client's data (local fairness) or\nwithin the entire dataset across all clients (global fairness). However,\nexisting approaches that focus exclusively on either local or global fairness\nfail to address two key challenges: (\\textbf{CH1}) Under statistical\nheterogeneity, global fairness does not imply local fairness, and vice versa.\n(\\textbf{CH2}) Achieving fairness under model-agnostic setting. To tackle the\naforementioned challenges, this paper proposes a novel post-processing\nframework for achieving both Local and Global Fairness in the FL context,\nnamely LoGoFair. To address CH1, LoGoFair endeavors to seek the Bayes optimal\nclassifier under local and global fairness constraints, which strikes the\noptimal accuracy-fairness balance in the probabilistic sense. To address CH2,\nLoGoFair employs a model-agnostic federated post-processing procedure that\nenables clients to collaboratively optimize global fairness while ensuring\nlocal fairness, thereby achieving the optimal fair classifier within FL.\nExperimental results on three real-world datasets further illustrate the\neffectiveness of the proposed LoGoFair framework.",
    "published_date": "2025-03-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.17231v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.03679v1",
    "title": "Continuous Boostlet Transform and Associated Uncertainty Principles",
    "authors": [
      "Owais Ahmad",
      "Jasifa Fayaz"
    ],
    "author_ids": [],
    "abstract": "The Continuous Boostlet Transform (CBT) is introduced as a powerful tool for\nanalyzing spatiotemporal signals, particularly acoustic wavefields. Overcoming\nthe limitations of classical wavelets, the CBT leverages the Poincar\\'e group\nand isotropic dilations to capture sparse features of natural acoustic fields.\nThis paper presents the mathematical framework of the CBT, including its\ndefinition, fundamental properties, and associated uncertainty principles, such\nas Heisenberg's, logarithmic, Pitt's, and Nazarov's inequalities. These results\nilluminate the trade-offs between time and frequency localization in the\nboostlet domain. Practical examples with constant and exponential functions\nhighlight the CBT's adaptability. With applications in radar, communications,\naudio processing, and seismic analysis, the CBT offers flexible time-frequency\nresolution, making it ideal for non-stationary and transient signals, and a\nvaluable tool for modern signal processing.",
    "published_date": "2025-03-21T00:00:00",
    "year": 2025,
    "categories": [
      "eess.SP",
      "cs.SD",
      "eess.AS",
      "math.FA",
      "42C40. 42C15. 81R30. 42A38"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.03679v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.17195v1",
    "title": "TreeSynth: Synthesizing Diverse Data from Scratch via Tree-Guided Subspace Partitioning",
    "authors": [
      "Sheng Wang",
      "Pengan Chen",
      "Jingqi Zhou",
      "Qintong Li",
      "Jingwei Dong",
      "Jiahui Gao",
      "Boyang Xue",
      "Jiyue Jiang",
      "Lingpeng Kong",
      "Chuan Wu"
    ],
    "author_ids": [],
    "abstract": "Model customization requires high-quality and diverse datasets, but acquiring\nsuch data remains challenging and costly. Although large language models (LLMs)\ncan synthesize training data, current approaches are constrained by limited\nseed data, model bias and insufficient control over the generation process,\nresulting in limited diversity and biased distribution with the increase of\ndata scales. To tackle this challenge, we present TreeSynth, a tree-guided\nsubspace-based data synthesis framework that recursively partitions the entire\ndata space into hierar-chical subspaces, enabling comprehensive and diverse\nscaling of data synthesis. Briefly, given a task-specific description, we\nconstruct a data space partitioning tree by iteratively executing criteria\ndetermination and subspace coverage steps. This hierarchically divides the\nwhole space (i.e., root node) into mutually exclusive and complementary atomic\nsubspaces (i.e., leaf nodes). By collecting synthesized data according to the\nattributes of each leaf node, we obtain a diverse dataset that fully covers the\ndata space. Empirically, our extensive experiments demonstrate that TreeSynth\nsurpasses both human-designed datasets and the state-of-the-art data synthesis\nbaselines, achieving maximum improvements of 45.2% in data diversity and 17.6%\nin downstream task performance across various models and tasks. Hopefully,\nTreeSynth provides a scalable solution to synthesize diverse and comprehensive\ndatasets from scratch without human intervention.",
    "published_date": "2025-03-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.17195v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.17181v1",
    "title": "LLMs Love Python: A Study of LLMs' Bias for Programming Languages and Libraries",
    "authors": [
      "Lukas Twist",
      "Jie M. Zhang",
      "Mark Harman",
      "Don Syme",
      "Joost Noppen",
      "Detlef Nauck"
    ],
    "author_ids": [],
    "abstract": "Programming language and library choices are crucial to software reliability\nand security. Poor or inconsistent choices can lead to increased technical\ndebt, security vulnerabilities, and even catastrophic failures in\nsafety-critical systems. As Large Language Models (LLMs) play an increasing\nrole in code generation, it is essential to understand how they make these\ndecisions. However, little is known about their preferences when selecting\nprogramming languages and libraries for different coding tasks. To fill this\ngap, this study provides the first in-depth investigation into LLM preferences\nfor programming languages and libraries used when generating code. We assess\nthe preferences of eight diverse LLMs by prompting them to complete various\ncoding tasks, including widely-studied benchmarks and the more practical task\nof generating the initial structural code for new projects (a crucial step that\noften determines a project's language or library choices).\n  Our findings reveal that LLMs heavily favour Python when solving\nlanguage-agnostic problems, using it in 90%-97% of cases for benchmark tasks.\nEven when generating initial project code where Python is not a suitable\nlanguage, it remains the most-used language in 58% of instances. Moreover, LLMs\ncontradict their own language recommendations in 83% of project initialisation\ntasks, raising concerns about their reliability in guiding language selection.\nSimilar biases toward well-established libraries further create serious\ndiscoverability challenges for newer open-source projects. These results\nhighlight the need to improve LLMs' adaptability to diverse programming\ncontexts and to develop mechanisms for mitigating programming language and\nlibrary bias.",
    "published_date": "2025-03-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.17181v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.17110v1",
    "title": "Beyond Accuracy: What Matters in Designing Well-Behaved Models?",
    "authors": [
      "Robin Hesse",
      "Doğukan Bağcı",
      "Bernt Schiele",
      "Simone Schaub-Meyer",
      "Stefan Roth"
    ],
    "author_ids": [],
    "abstract": "Deep learning has become an essential part of computer vision, with deep\nneural networks (DNNs) excelling in predictive performance. However, they often\nfall short in other critical quality dimensions, such as robustness,\ncalibration, or fairness. While existing studies have focused on a subset of\nthese quality dimensions, none have explored a more general form of\n\"well-behavedness\" of DNNs. With this work, we address this gap by\nsimultaneously studying nine different quality dimensions for image\nclassification. Through a large-scale study, we provide a bird's-eye view by\nanalyzing 326 backbone models and how different training paradigms and model\narchitectures affect the quality dimensions. We reveal various new insights\nsuch that (i) vision-language models exhibit high fairness on ImageNet-1k\nclassification and strong robustness against domain changes; (ii)\nself-supervised learning is an effective training paradigm to improve almost\nall considered quality dimensions; and (iii) the training dataset size is a\nmajor driver for most of the quality dimensions. We conclude our study by\nintroducing the QUBA score (Quality Understanding Beyond Accuracy), a novel\nmetric that ranks models across multiple dimensions of quality, enabling\ntailored recommendations based on specific user needs.",
    "published_date": "2025-03-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.17110v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.17089v1",
    "title": "Does a Rising Tide Lift All Boats? Bias Mitigation for AI-based CMR Segmentation",
    "authors": [
      "Tiarna Lee",
      "Esther Puyol-Antón",
      "Bram Ruijsink",
      "Miaojing Shi",
      "Andrew P. King"
    ],
    "author_ids": [],
    "abstract": "Artificial intelligence (AI) is increasingly being used for medical imaging\ntasks. However, there can be biases in the resulting models, particularly when\nthey were trained using imbalanced training datasets. One such example has been\nthe strong race bias effect in cardiac magnetic resonance (CMR) image\nsegmentation models. Although this phenomenon has been reported in a number of\npublications, little is known about the effectiveness of bias mitigation\nalgorithms in this domain. We aim to investigate the impact of common bias\nmitigation methods to address bias between Black and White subjects in AI-based\nCMR segmentation models. Specifically, we use oversampling, importance\nreweighing and Group DRO as well as combinations of these techniques to\nmitigate the race bias. Furthermore, motivated by recent findings on the root\ncauses of AI-based CMR segmentation bias, we evaluate the same methods using\nmodels trained and evaluated on cropped CMR images. We find that bias can be\nmitigated using oversampling, significantly improving performance for the\nunderrepresented Black subjects whilst not significantly reducing the majority\nWhite subjects' performance. Group DRO also improves performance for Black\nsubjects but not significantly, while reweighing decreases performance for\nBlack subjects. Using a combination of oversampling and Group DRO also improves\nperformance for Black subjects but not significantly. Using cropped images\nincreases performance for both races and reduces the bias, whilst adding\noversampling as a bias mitigation technique with cropped images reduces the\nbias further.",
    "published_date": "2025-03-21T00:00:00",
    "year": 2025,
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.17089v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.16873v1",
    "title": "Classifier-guided CLIP Distillation for Unsupervised Multi-label Classification",
    "authors": [
      "Dongseob Kim",
      "Hyunjung Shim"
    ],
    "author_ids": [],
    "abstract": "Multi-label classification is crucial for comprehensive image understanding,\nyet acquiring accurate annotations is challenging and costly. To address this,\na recent study suggests exploiting unsupervised multi-label classification\nleveraging CLIP, a powerful vision-language model. Despite CLIP's proficiency,\nit suffers from view-dependent predictions and inherent bias, limiting its\neffectiveness. We propose a novel method that addresses these issues by\nleveraging multiple views near target objects, guided by Class Activation\nMapping (CAM) of the classifier, and debiasing pseudo-labels derived from CLIP\npredictions. Our Classifier-guided CLIP Distillation (CCD) enables selecting\nmultiple local views without extra labels and debiasing predictions to enhance\nclassification performance. Experimental results validate our method's\nsuperiority over existing techniques across diverse datasets. The code is\navailable at https://github.com/k0u-id/CCD.",
    "published_date": "2025-03-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.16873v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.16852v1",
    "title": "Casual Inference via Style Bias Deconfounding for Domain Generalization",
    "authors": [
      "Jiaxi Li",
      "Di Lin",
      "Hao Chen",
      "Hongying Liu",
      "Liang Wan",
      "Wei Feng"
    ],
    "author_ids": [],
    "abstract": "Deep neural networks (DNNs) often struggle with out-of-distribution data,\nlimiting their reliability in diverse realworld applications. To address this\nissue, domain generalization methods have been developed to learn\ndomain-invariant features from single or multiple training domains, enabling\ngeneralization to unseen testing domains. However, existing approaches usually\noverlook the impact of style frequency within the training set. This oversight\npredisposes models to capture spurious visual correlations caused by style\nconfounding factors, rather than learning truly causal representations, thereby\nundermining inference reliability. In this work, we introduce Style\nDeconfounding Causal Learning (SDCL), a novel causal inference-based framework\ndesigned to explicitly address style as a confounding factor. Our approaches\nbegins with constructing a structural causal model (SCM) tailored to the domain\ngeneralization problem and applies a backdoor adjustment strategy to account\nfor style influence. Building on this foundation, we design a style-guided\nexpert module (SGEM) to adaptively clusters style distributions during\ntraining, capturing the global confounding style. Additionally, a back-door\ncausal learning module (BDCL) performs causal interventions during feature\nextraction, ensuring fair integration of global confounding styles into sample\npredictions, effectively reducing style bias. The SDCL framework is highly\nversatile and can be seamlessly integrated with state-of-the-art data\naugmentation techniques. Extensive experiments across diverse natural and\nmedical image recognition tasks validate its efficacy, demonstrating superior\nperformance in both multi-domain and the more challenging single-domain\ngeneralization scenarios.",
    "published_date": "2025-03-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.16852v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.16851v1",
    "title": "Towards LLM Guardrails via Sparse Representation Steering",
    "authors": [
      "Zeqing He",
      "Zhibo Wang",
      "Huiyu Xu",
      "Kui Ren"
    ],
    "author_ids": [],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance in\nnatural language generation tasks, yet their uncontrolled outputs pose\nsignificant ethical and safety risks. Recently, representation engineering\nmethods have shown promising results in steering model behavior by modifying\nthe rich semantic information encoded in activation vectors. However, due to\nthe difficulty of precisely disentangling semantic directions within\nhigh-dimensional representation space, existing approaches suffer from three\nmajor limitations: lack of fine-grained control, quality degradation of\ngenerated content, and poor interpretability. To address these challenges, we\npropose a sparse encoding-based representation engineering method, named SRE,\nwhich decomposes polysemantic activations into a structured, monosemantic\nfeature space. By leveraging sparse autoencoding, our approach isolates and\nadjusts only task-specific sparse feature dimensions, enabling precise and\ninterpretable steering of model behavior while preserving content quality. We\nvalidate our method on three critical domains, i.e., safety, fairness, and\ntruthfulness using the open-source LLM Gemma-2-2B-it. Experimental results show\nthat SRE achieves superior controllability while maintaining the overall\nquality of generated content (i.e., controllability and quality), demonstrating\nits effectiveness as a fine-grained and interpretable activation steering\nframework.",
    "published_date": "2025-03-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CR",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.16851v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.16836v1",
    "title": "A Flexible Fairness Framework with Surrogate Loss Reweighting for Addressing Sociodemographic Disparities",
    "authors": [
      "Wen Xu",
      "Elham Dolatabadi"
    ],
    "author_ids": [],
    "abstract": "This paper presents a new algorithmic fairness framework called\n$\\boldsymbol{\\alpha}$-$\\boldsymbol{\\beta}$ Fair Machine Learning\n($\\boldsymbol{\\alpha}$-$\\boldsymbol{\\beta}$ FML), designed to optimize fairness\nlevels across sociodemographic attributes. Our framework employs a new family\nof surrogate loss functions, paired with loss reweighting techniques, allowing\nprecise control over fairness-accuracy trade-offs through tunable\nhyperparameters $\\boldsymbol{\\alpha}$ and $\\boldsymbol{\\beta}$. To efficiently\nsolve the learning objective, we propose Parallel Stochastic Gradient Descent\nwith Surrogate Loss (P-SGD-S) and establish convergence guarantees for both\nconvex and nonconvex loss functions. Experimental results demonstrate that our\nframework improves overall accuracy while reducing fairness violations,\noffering a smooth trade-off between standard empirical risk minimization and\nstrict minimax fairness. Results across multiple datasets confirm its\nadaptability, ensuring fairness improvements without excessive performance\ndegradation.",
    "published_date": "2025-03-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.16836v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.16826v1",
    "title": "When Tom Eats Kimchi: Evaluating Cultural Bias of Multimodal Large Language Models in Cultural Mixture Contexts",
    "authors": [
      "Jun Seong Kim",
      "Kyaw Ye Thu",
      "Javad Ismayilzada",
      "Junyeong Park",
      "Eunsu Kim",
      "Huzama Ahmad",
      "Na Min An",
      "James Thorne",
      "Alice Oh"
    ],
    "author_ids": [],
    "abstract": "In a highly globalized world, it is important for multi-modal large language\nmodels (MLLMs) to recognize and respond correctly to mixed-cultural inputs. For\nexample, a model should correctly identify kimchi (Korean food) in an image\nboth when an Asian woman is eating it, as well as an African man is eating it.\nHowever, current MLLMs show an over-reliance on the visual features of the\nperson, leading to misclassification of the entities. To examine the robustness\nof MLLMs to different ethnicity, we introduce MixCuBe, a cross-cultural bias\nbenchmark, and study elements from five countries and four ethnicities. Our\nfindings reveal that MLLMs achieve both higher accuracy and lower sensitivity\nto such perturbation for high-resource cultures, but not for low-resource\ncultures. GPT-4o, the best-performing model overall, shows up to 58% difference\nin accuracy between the original and perturbed cultural settings in\nlow-resource cultures. Our dataset is publicly available at:\nhttps://huggingface.co/datasets/kyawyethu/MixCuBe.",
    "published_date": "2025-03-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.16826v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.16814v1",
    "title": "When Debate Fails: Bias Reinforcement in Large Language Models",
    "authors": [
      "Jihwan Oh",
      "Minchan Jeong",
      "Jongwoo Ko",
      "Se-Young Yun"
    ],
    "author_ids": [],
    "abstract": "Large Language Models $($LLMs$)$ solve complex problems using training-free\nmethods like prompt engineering and in-context learning, yet ensuring reasoning\ncorrectness remains challenging. While self-correction methods such as\nself-consistency and self-refinement aim to improve reliability, they often\nreinforce biases due to the lack of effective feedback mechanisms. Multi-Agent\nDebate $($MAD$)$ has emerged as an alternative, but we identify two key\nlimitations: bias reinforcement, where debate amplifies model biases instead of\ncorrecting them, and lack of perspective diversity, as all agents share the\nsame model and reasoning patterns, limiting true debate effectiveness. To\nsystematically evaluate these issues, we introduce $\\textit{MetaNIM Arena}$, a\nbenchmark designed to assess LLMs in adversarial strategic decision-making,\nwhere dynamic interactions influence optimal decisions. To overcome MAD's\nlimitations, we propose $\\textbf{DReaMAD}$ $($$\\textbf{D}$iverse\n$\\textbf{Rea}$soning via $\\textbf{M}$ulti-$\\textbf{A}$gent $\\textbf{D}$ebate\nwith Refined Prompt$)$, a novel framework that $(1)$ refines LLM's strategic\nprior knowledge to improve reasoning quality and $(2)$ promotes diverse\nviewpoints within a single model by systematically modifying prompts, reducing\nbias. Empirical results show that $\\textbf{DReaMAD}$ significantly improves\ndecision accuracy, reasoning diversity, and bias mitigation across multiple\nstrategic tasks, establishing it as a more effective approach for LLM-based\ndecision-making.",
    "published_date": "2025-03-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.16814v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.17414v1",
    "title": "Opportunities and Challenges of Frontier Data Governance With Synthetic Data",
    "authors": [
      "Madhavendra Thakur",
      "Jason Hausenloy"
    ],
    "author_ids": [],
    "abstract": "Synthetic data, or data generated by machine learning models, is increasingly\nemerging as a solution to the data access problem. However, its use introduces\nsignificant governance and accountability challenges, and potentially debases\nexisting governance paradigms, such as compute and data governance. In this\npaper, we identify 3 key governance and accountability challenges that\nsynthetic data poses - it can enable the increased emergence of malicious\nactors, spontaneous biases and value drift. We thus craft 3 technical\nmechanisms to address these specific challenges, finding applications for\nsynthetic data towards adversarial training, bias mitigation and value\nreinforcement. These could not only counteract the risks of synthetic data, but\nserve as critical levers for governance of the frontier in the future.",
    "published_date": "2025-03-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.17414v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.16679v1",
    "title": "Echoes of Power: Investigating Geopolitical Bias in US and China Large Language Models",
    "authors": [
      "Andre G. C. Pacheco",
      "Athus Cavalini",
      "Giovanni Comarela"
    ],
    "author_ids": [],
    "abstract": "Large Language Models (LLMs) have emerged as powerful tools for generating\nhuman-like text, transforming human-machine interactions. However, their\nwidespread adoption has raised concerns about their potential to influence\npublic opinion and shape political narratives. In this work, we investigate the\ngeopolitical biases in US and Chinese LLMs, focusing on how these models\nrespond to questions related to geopolitics and international relations. We\ncollected responses from ChatGPT and DeepSeek to a set of geopolitical\nquestions and evaluated their outputs through both qualitative and quantitative\nanalyses. Our findings show notable biases in both models, reflecting distinct\nideological perspectives and cultural influences. However, despite these\nbiases, for a set of questions, the models' responses are more aligned than\nexpected, indicating that they can address sensitive topics without necessarily\npresenting directly opposing viewpoints. This study highlights the potential of\nLLMs to shape public discourse and underscores the importance of critically\nassessing AI-generated content, particularly in politically sensitive contexts.",
    "published_date": "2025-03-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.16679v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.16674v1",
    "title": "Through the LLM Looking Glass: A Socratic Self-Assessment of Donkeys, Elephants, and Markets",
    "authors": [
      "Molly Kennedy",
      "Ayyoob Imani",
      "Timo Spinde",
      "Hinrich Schütze"
    ],
    "author_ids": [],
    "abstract": "While detecting and avoiding bias in LLM-generated text is becoming\nincreasingly important, media bias often remains subtle and subjective, making\nit particularly difficult to identify and mitigate. In this study, we assess\nmedia bias in LLM-generated content and LLMs' ability to detect subtle\nideological bias. We conduct this evaluation using two datasets, PoliGen and\nEconoLex, covering political and economic discourse, respectively. We evaluate\neight widely used LLMs by prompting them to generate articles and analyze their\nideological preferences via self-assessment. By using self-assessment, the\nstudy aims to directly measure the models' biases rather than relying on\nexternal interpretations, thereby minimizing subjective judgments about media\nbias. Our results reveal a consistent preference of Democratic over Republican\npositions across all models. Conversely, in economic topics, biases vary among\nWestern LLMs, while those developed in China lean more strongly toward\nsocialism.",
    "published_date": "2025-03-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.16674v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.16414v1",
    "title": "Computing Lindahl Equilibrium for Public Goods with and without Funding Caps",
    "authors": [
      "Christian Kroer",
      "Dominik Peters"
    ],
    "author_ids": [],
    "abstract": "Lindahl equilibrium is a solution concept for allocating a fixed budget\nacross several divisible public goods. It always lies in the core, meaning that\nthe equilibrium allocation satisfies desirable stability and proportional\nfairness properties. We consider a model where agents have separable linear\nutility functions over the public goods, and the output assigns to each good an\namount of spending, summing to at most the available budget.\n  In the uncapped setting, each of the public goods can absorb any amount of\nfunding. In this case, it is known that Lindahl equilibrium is equivalent to\nmaximizing Nash social welfare, and this allocation can be computed by a\npublic-goods variant of the proportional response dynamics. We introduce a new\nconvex programming formulation for computing this solution and show that it is\nrelated to Nash welfare maximization through duality and reformulation. We then\nshow that the proportional response dynamics is equivalent to running mirror\ndescent on our new formulation, thereby providing a new and immediate proof of\nthe convergence guarantee for the dynamics. Our new formulation has\nsimilarities to Shmyrev's convex program for Fisher market equilibrium.\n  In the capped setting, each public good has an upper bound on the amount of\nfunding it can receive. In this setting, existence of Lindahl equilibrium was\nonly known via fixed-point arguments. The existence of an efficient algorithm\ncomputing one has been a long-standing open question. We prove that our new\nconvex program continues to work when the cap constraints are added, and its\noptimal solutions are Lindahl equilibria. Thus, we establish that Lindahl\nequilibrium can be efficiently computed in the capped setting. Our result also\nimplies that approximately core-stable allocations can be efficiently computed\nfor the class of separable piecewise-linear concave (SPLC) utilities.",
    "published_date": "2025-03-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.16414v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.16251v1",
    "title": "RESFL: An Uncertainty-Aware Framework for Responsible Federated Learning by Balancing Privacy, Fairness and Utility in Autonomous Vehicles",
    "authors": [
      "Dawood Wasif",
      "Terrence J. Moore",
      "Jin-Hee Cho"
    ],
    "author_ids": [],
    "abstract": "Autonomous vehicles (AVs) increasingly rely on Federated Learning (FL) to\nenhance perception models while preserving privacy. However, existing FL\nframeworks struggle to balance privacy, fairness, and robustness, leading to\nperformance disparities across demographic groups. Privacy-preserving\ntechniques like differential privacy mitigate data leakage risks but worsen\nfairness by restricting access to sensitive attributes needed for bias\ncorrection. This work explores the trade-off between privacy and fairness in\nFL-based object detection for AVs and introduces RESFL, an integrated solution\noptimizing both. RESFL incorporates adversarial privacy disentanglement and\nuncertainty-guided fairness-aware aggregation. The adversarial component uses a\ngradient reversal layer to remove sensitive attributes, reducing privacy risks\nwhile maintaining fairness. The uncertainty-aware aggregation employs an\nevidential neural network to weight client updates adaptively, prioritizing\ncontributions with lower fairness disparities and higher confidence. This\nensures robust and equitable FL model updates. We evaluate RESFL on the FACET\ndataset and CARLA simulator, assessing accuracy, fairness, privacy resilience,\nand robustness under varying conditions. RESFL improves detection accuracy,\nreduces fairness disparities, and lowers privacy attack success rates while\ndemonstrating superior robustness to adversarial conditions compared to other\napproaches.",
    "published_date": "2025-03-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CV",
      "cs.DC",
      "cs.ET"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.16251v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.16233v1",
    "title": "Empirical Analysis of Privacy-Fairness-Accuracy Trade-offs in Federated Learning: A Step Towards Responsible AI",
    "authors": [
      "Dawood Wasif",
      "Dian Chen",
      "Sindhuja Madabushi",
      "Nithin Alluru",
      "Terrence J. Moore",
      "Jin-Hee Cho"
    ],
    "author_ids": [],
    "abstract": "Federated Learning (FL) enables collaborative machine learning while\npreserving data privacy but struggles to balance privacy preservation (PP) and\nfairness. Techniques like Differential Privacy (DP), Homomorphic Encryption\n(HE), and Secure Multi-Party Computation (SMC) protect sensitive data but\nintroduce trade-offs. DP enhances privacy but can disproportionately impact\nunderrepresented groups, while HE and SMC mitigate fairness concerns at the\ncost of computational overhead. This work explores the privacy-fairness\ntrade-offs in FL under IID (Independent and Identically Distributed) and\nnon-IID data distributions, benchmarking q-FedAvg, q-MAML, and Ditto on diverse\ndatasets. Our findings highlight context-dependent trade-offs and offer\nguidelines for designing FL systems that uphold responsible AI principles,\nensuring fairness, privacy, and equitable real-world applications.",
    "published_date": "2025-03-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.DC",
      "cs.ET"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.16233v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.16206v1",
    "title": "Interpretable Neural Causal Models with TRAM-DAGs",
    "authors": [
      "Beate Sick",
      "Oliver Dürr"
    ],
    "author_ids": [],
    "abstract": "The ultimate goal of most scientific studies is to understand the underlying\ncausal mechanism between the involved variables. Structural causal models\n(SCMs) are widely used to represent such causal mechanisms. Given an SCM,\ncausal queries on all three levels of Pearl's causal hierarchy can be answered:\n$L_1$ observational, $L_2$ interventional, and $L_3$ counterfactual. An\nessential aspect of modeling the SCM is to model the dependency of each\nvariable on its causal parents. Traditionally this is done by parametric\nstatistical models, such as linear or logistic regression models. This allows\nto handle all kinds of data types and fit interpretable models but bears the\nrisk of introducing a bias. More recently neural causal models came up using\nneural networks (NNs) to model the causal relationships, allowing the\nestimation of nearly any underlying functional form without bias. However,\ncurrent neural causal models are generally restricted to continuous variables\nand do not yield an interpretable form of the causal relationships.\nTransformation models range from simple statistical regressions to complex\nnetworks and can handle continuous, ordinal, and binary data. Here, we propose\nto use TRAMs to model the functional relationships in SCMs allowing us to\nbridge the gap between interpretability and flexibility in causal modeling. We\ncall this method TRAM-DAG and assume currently that the underlying directed\nacyclic graph is known. For the fully observed case, we benchmark TRAM-DAGs\nagainst state-of-the-art statistical and NN-based causal models. We show that\nTRAM-DAGs are interpretable but also achieve equal or superior performance in\nqueries ranging from $L_1$ to $L_3$ in the causal hierarchy. For the continuous\ncase, TRAM-DAGs allow for counterfactual queries for three common causal\nstructures, including unobserved confounding.",
    "published_date": "2025-03-20T00:00:00",
    "year": 2025,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.16206v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.16159v1",
    "title": "Neural Combinatorial Optimization for Real-World Routing",
    "authors": [
      "Jiwoo Son",
      "Zhikai Zhao",
      "Federico Berto",
      "Chuanbo Hua",
      "Changhyun Kwon",
      "Jinkyoo Park"
    ],
    "author_ids": [],
    "abstract": "Vehicle Routing Problems (VRPs) are a class of NP-hard problems ubiquitous in\nseveral real-world logistics scenarios that pose significant challenges for\noptimization. Neural Combinatorial Optimization (NCO) has emerged as a\npromising alternative to classical approaches, as it can learn fast heuristics\nto solve VRPs. However, most research works in NCO for VRPs focus on simplified\nsettings, which do not account for asymmetric distances and travel durations\nthat cannot be derived by simple Euclidean distances and unrealistic data\ndistributions, hindering real-world deployment. This work introduces RRNCO\n(Real Routing NCO) to bridge the gap of NCO between synthetic and real-world\nVRPs in the critical aspects of both data and modeling. First, we introduce a\nnew, openly available dataset with real-world data containing a diverse dataset\nof locations, distances, and duration matrices from 100 cities, considering\nrealistic settings with actual routing distances and durations obtained from\nOpen Source Routing Machine (OSRM). Second, we propose a novel approach that\nefficiently processes both node and edge features through contextual gating,\nenabling the construction of more informed node embedding, and we finally\nincorporate an Adaptation Attention Free Module (AAFM) with neural adaptive\nbias mechanisms that effectively integrates not only distance matrices but also\nangular relationships between nodes, allowing our model to capture rich\nstructural information. RRNCO achieves state-of-the-art results in real-world\nVRPs among NCO methods. We make our dataset and code publicly available at\nhttps://github.com/ai4co/real-routing-nco.",
    "published_date": "2025-03-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.16159v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.16148v1",
    "title": "Only a Little to the Left: A Theory-grounded Measure of Political Bias in Large Language Models",
    "authors": [
      "Mats Faulborn",
      "Indira Sen",
      "Max Pellert",
      "Andreas Spitz",
      "David Garcia"
    ],
    "author_ids": [],
    "abstract": "Prompt-based language models like GPT4 and LLaMa have been used for a wide\nvariety of use cases such as simulating agents, searching for information, or\nfor content analysis. For all of these applications and others, political\nbiases in these models can affect their performance. Several researchers have\nattempted to study political bias in language models using evaluation suites\nbased on surveys, such as the Political Compass Test (PCT), often finding a\nparticular leaning favored by these models. However, there is some variation in\nthe exact prompting techniques, leading to diverging findings and most research\nrelies on constrained-answer settings to extract model responses. Moreover, the\nPolitical Compass Test is not a scientifically valid survey instrument. In this\nwork, we contribute a political bias measured informed by political science\ntheory, building on survey design principles to test a wide variety of input\nprompts, while taking into account prompt sensitivity. We then prompt 11\ndifferent open and commercial models, differentiating between instruction-tuned\nand non-instruction-tuned models, and automatically classify their political\nstances from 88,110 responses. Leveraging this dataset, we compute political\nbias profiles across different prompt variations and find that while PCT\nexaggerates bias in certain models like GPT3.5, measures of political bias are\noften unstable, but generally more left-leaning for instruction-tuned models.",
    "published_date": "2025-03-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.16148v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.16137v1",
    "title": "Towards Non-linear Cultural Production and systems of machinic agency: in the case of TikTok value generation",
    "authors": [
      "Hongrui Jin"
    ],
    "author_ids": [],
    "abstract": "The rise of TikTok has brought forth novel ways to create and consume media\ncontent, accelerated by technologies such as hyper-individualised algorithms\nand easy-to-use video production tools. Despite its popularity, scholars and\npoliticians alike have raised many concerns on the legitimacy and ethics of\nTikTok regarding its services, and its collected data. However, much of these\ndiscussions take the premise of user-generated content for granted, attributing\nthem to human expression without critically evaluating how the making of\non-platform content production have changed. With a grounded theory approach,\nin conjunction with a platform-aware walkthrough that pays special attention to\nthe material and immaterial premises of platform value generation, my findings\nsuggest that the intensification of datafication have proliferated from\nconsumption behaviours to the process of content production, whereas content\nproduction no longer solely produce media content. As platforms become the\nactive recruiter, mobiliser and co-producer of media production, I argue that\nit is no longer feasible to distinguish human and machine contribution in the\nways they are consumed to facilitate platform valorisation. I propose that the\ntechnical arrangements of TikTok, in relation to its users has fostered a\nnon-linear mode of platform cultural production capable of generating economic\nvalue through a system of machinic agency that incorporates human and machines\nin an indistinguishable manner. As content, the premises of platform\nvalorisation has become an inseparable effort of human-machines, I urge that\nthe relationship between technology and humans be reassessed as a system of\nmachinic agency that mutually shapes our mediated reality, rather than\nsingular, differentiable actors that contribute to platforms.",
    "published_date": "2025-03-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.16137v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.16002v1",
    "title": "The Algorithmic Landscape of Fair and Efficient Distribution of Delivery Orders in the Gig Economy",
    "authors": [
      "Hadi Hosseini",
      "Šimon Schierreich"
    ],
    "author_ids": [],
    "abstract": "Distributing services, goods, and tasks in the gig economy heavily relies\nupon on-demand workers (aka agents), leading to new challenges varying from\nlogistics optimization to the ethical treatment of gig workers. We focus on\nfair and efficient distribution of delivery tasks -- placed on the vertices of\na graph -- among a fixed set of agents. We consider the fairness notion of\nminimax share (MMS), which aims to minimize the maximum (submodular) cost among\nagents and is particularly appealing in applications without monetary\ntransfers. We propose a novel efficiency notion -- namely non-wastefulness --\nthat is desirable in a wide range of scenarios and, more importantly, does not\nsuffer from computational barriers. Specifically, given a distribution of\ntasks, we can, in polynomial time, i) verify whether the distribution is\nnon-wasteful and ii) turn it into an equivalent non-wasteful distribution.\nMoreover, we investigate several fixed-parameter tractable and polynomial-time\nalgorithms and paint a complete picture of the (parameterized) complexity of\nfinding fair and efficient distributions of tasks with respect to both the\nstructure of the topology and natural restrictions of the input. Finally, we\nhighlight how our findings shed light on computational aspects of other\nwell-studied fairness notions, such as envy-freeness and its relaxations.",
    "published_date": "2025-03-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.16002v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.16566v1",
    "title": "REVAL: A Comprehension Evaluation on Reliability and Values of Large Vision-Language Models",
    "authors": [
      "Jie Zhang",
      "Zheng Yuan",
      "Zhongqi Wang",
      "Bei Yan",
      "Sibo Wang",
      "Xiangkui Cao",
      "Zonghui Guo",
      "Shiguang Shan",
      "Xilin Chen"
    ],
    "author_ids": [],
    "abstract": "The rapid evolution of Large Vision-Language Models (LVLMs) has highlighted\nthe necessity for comprehensive evaluation frameworks that assess these models\nacross diverse dimensions. While existing benchmarks focus on specific aspects\nsuch as perceptual abilities, cognitive capabilities, and safety against\nadversarial attacks, they often lack the breadth and depth required to provide\na holistic understanding of LVLMs' strengths and limitations. To address this\ngap, we introduce REVAL, a comprehensive benchmark designed to evaluate the\n\\textbf{RE}liability and \\textbf{VAL}ue of LVLMs. REVAL encompasses over 144K\nimage-text Visual Question Answering (VQA) samples, structured into two primary\nsections: Reliability, which assesses truthfulness (\\eg, perceptual accuracy\nand hallucination tendencies) and robustness (\\eg, resilience to adversarial\nattacks, typographic attacks, and image corruption), and Values, which\nevaluates ethical concerns (\\eg, bias and moral understanding), safety issues\n(\\eg, toxicity and jailbreak vulnerabilities), and privacy problems (\\eg,\nprivacy awareness and privacy leakage). We evaluate 26 models, including\nmainstream open-source LVLMs and prominent closed-source models like GPT-4o and\nGemini-1.5-Pro. Our findings reveal that while current LVLMs excel in\nperceptual tasks and toxicity avoidance, they exhibit significant\nvulnerabilities in adversarial scenarios, privacy preservation, and ethical\nreasoning. These insights underscore critical areas for future improvements,\nguiding the development of more secure, reliable, and ethically aligned LVLMs.\nREVAL provides a robust framework for researchers to systematically assess and\ncompare LVLMs, fostering advancements in the field.",
    "published_date": "2025-03-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.16566v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.15904v1",
    "title": "From Structured Prompts to Open Narratives: Measuring Gender Bias in LLMs Through Open-Ended Storytelling",
    "authors": [
      "Evan Chen",
      "Run-Jun Zhan",
      "Yan-Bai Lin",
      "Hung-Hsuan Chen"
    ],
    "author_ids": [],
    "abstract": "Large Language Models (LLMs) have revolutionized natural language processing,\nyet concerns persist regarding their tendency to reflect or amplify social\nbiases present in their training data. This study introduces a novel evaluation\nframework to uncover gender biases in LLMs, focusing on their occupational\nnarratives. Unlike previous methods relying on structured scenarios or\ncarefully crafted prompts, our approach leverages free-form storytelling to\nreveal biases embedded in the models. Systematic analyses show an\noverrepresentation of female characters across occupations in six widely used\nLLMs. Additionally, our findings reveal that LLM-generated occupational gender\nrankings align more closely with human stereotypes than actual labor\nstatistics. These insights underscore the need for balanced mitigation\nstrategies to ensure fairness while avoiding the reinforcement of new\nstereotypes.",
    "published_date": "2025-03-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.15904v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.15815v1",
    "title": "Attention Pruning: Automated Fairness Repair of Language Models via Surrogate Simulated Annealing",
    "authors": [
      "Vishnu Asutosh Dasu",
      "Md Rafi ur Rashid",
      "Vipul Gupta",
      "Saeid Tizpaz-Niari",
      "Gang Tan"
    ],
    "author_ids": [],
    "abstract": "This paper explores pruning attention heads as a post-processing bias\nmitigation method for large language models (LLMs). Modern AI systems such as\nLLMs are expanding into sensitive social contexts where fairness concerns\nbecome especially crucial. Since LLMs develop decision-making patterns by\ntraining on massive datasets of human-generated content, they naturally encode\nand perpetuate societal biases. While modifying training datasets and\nalgorithms is expensive and requires significant resources; post-processing\ntechniques-such as selectively deactivating neurons and attention heads in\npre-trained LLMs-can provide feasible and effective approaches to improve\nfairness. However, identifying the optimal subset of parameters to prune\npresents a combinatorial challenge within LLMs' immense parameter space,\nrequiring solutions that efficiently balance competing objectives across the\nfrontiers of model fairness and utility.\n  To address the computational challenges, we explore a search-based program\nrepair approach via randomized simulated annealing. Given the prohibitive\nevaluation costs in billion-parameter LLMs, we develop surrogate deep neural\nnetworks that efficiently model the relationship between attention head states\n(active/inactive) and their corresponding fairness/utility metrics. This allows\nus to perform optimization over the surrogate models and efficiently identify\noptimal subsets of attention heads for selective pruning rather than directly\nsearching through the LLM parameter space. This paper introduces Attention\nPruning, a fairness-aware surrogate simulated annealing approach to prune\nattention heads in LLMs that disproportionately contribute to bias while\nminimally impacting overall model utility. Our experiments show that Attention\nPruning achieves up to $40\\%$ reduction in gender bias and outperforms the\nstate-of-the-art bias mitigation strategies.",
    "published_date": "2025-03-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.15815v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.16558v1",
    "title": "Advancing Problem-Based Learning in Biomedical Engineering in the Era of Generative AI",
    "authors": [
      "Micky C. Nnamdi",
      "J. Ben Tamo",
      "Wenqi Shi",
      "May D. Wang"
    ],
    "author_ids": [],
    "abstract": "Problem-Based Learning (PBL) has significantly impacted biomedical\nengineering (BME) education since its introduction in the early 2000s,\neffectively enhancing critical thinking and real-world knowledge application\namong students. With biomedical engineering rapidly converging with artificial\nintelligence (AI), integrating effective AI education into established\ncurricula has become challenging yet increasingly necessary. Recent\nadvancements, including AI's recognition by the 2024 Nobel Prize, have\nhighlighted the importance of training students comprehensively in biomedical\nAI. However, effective biomedical AI education faces substantial obstacles,\nsuch as diverse student backgrounds, limited personalized mentoring,\nconstrained computational resources, and difficulties in safely scaling\nhands-on practical experiments due to privacy and ethical concerns associated\nwith biomedical data. To overcome these issues, we conducted a three-year\n(2021-2023) case study implementing an advanced PBL framework tailored\nspecifically for biomedical AI education, involving 92 undergraduate and 156\ngraduate students from the joint Biomedical Engineering program of Georgia\nInstitute of Technology and Emory University. Our approach emphasizes\ncollaborative, interdisciplinary problem-solving through authentic biomedical\nAI challenges. The implementation led to measurable improvements in learning\noutcomes, evidenced by high research productivity (16 student-authored\npublications), consistently positive peer evaluations, and successful\ndevelopment of innovative computational methods addressing real biomedical\nchallenges. Additionally, we examined the role of generative AI both as a\nteaching subject and an educational support tool within the PBL framework. Our\nstudy presents a practical and scalable roadmap for biomedical engineering\ndepartments aiming to integrate robust AI education into their curricula.",
    "published_date": "2025-03-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.16558v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.15682v1",
    "title": "Transfeminist AI Governance",
    "authors": [
      "Blair Attard-Frost"
    ],
    "author_ids": [],
    "abstract": "This article re-imagines the governance of artificial intelligence (AI)\nthrough a transfeminist lens, focusing on challenges of power, participation,\nand injustice, and on opportunities for advancing equity, community-based\nresistance, and transformative change. AI governance is a field of research and\npractice seeking to maximize benefits and minimize harms caused by AI systems.\nUnfortunately, AI governance practices are frequently ineffective at preventing\nAI systems from harming people and the environment, with historically\nmarginalized groups such as trans people being particularly vulnerable to harm.\nBuilding upon trans and feminist theories of ethics, I introduce an approach to\ntransfeminist AI governance. Applying a transfeminist lens in combination with\na critical self-reflexivity methodology, I retroactively reinterpret findings\nfrom three empirical studies of AI governance practices in Canada and globally.\nIn three reflections on my findings, I show that large-scale AI governance\nsystems structurally prioritize the needs of industry over marginalized\ncommunities. As a result, AI governance is limited by power imbalances and\nexclusionary norms. This research shows that re-grounding AI governance in\ntransfeminist ethical principles can support AI governance researchers,\npractitioners, and organizers in addressing those limitations.",
    "published_date": "2025-03-19T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.15682v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.17401v4",
    "title": "AIJIM: A Scalable Model for Real-Time AI in Environmental Journalism",
    "authors": [
      "Torsten Tiltack"
    ],
    "author_ids": [],
    "abstract": "Environmental journalism is vital for raising awareness of ecological crises\nand supporting evidence-based policy, yet traditional methods suffer from\ndelays, limited scalability, and lack of coverage in under-monitored regions.\nThis paper introduces the Artificial Intelligence Journalism Integration Model\n(AIJIM), a conceptual and transferable theoretical model that structures\nreal-time, AI-supported environmental journalism workflows. AIJIM combines\ncitizen-sourced image data, automated hazard detection, dual-level validation\n(visual and textual), and AI-generated reporting. Validated through a pilot\nstudy in Mallorca, AIJIM achieved significant improvements in reporting speed\nand accuracy, while maintaining transparency and ethical oversight through\nExplainable AI (XAI), GDPR compliance, and community review. The model\ndemonstrates high transferability and offers a new benchmark for scalable,\nresponsible, and participatory journalism at the intersection of environmental\ncommunication and artificial intelligence.",
    "published_date": "2025-03-19T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "68T45",
      "I.2.10; H.3.5; J.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.17401v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.15622v1",
    "title": "Contextual Fairness-Aware Practices in ML: A Cost-Effective Empirical Evaluation",
    "authors": [
      "Alessandra Parziale",
      "Gianmario Voria",
      "Giammaria Giordano",
      "Gemma Catolino",
      "Gregorio Robles",
      "Fabio Palomba"
    ],
    "author_ids": [],
    "abstract": "As machine learning (ML) systems become central to critical decision-making,\nconcerns over fairness and potential biases have increased. To address this,\nthe software engineering (SE) field has introduced bias mitigation techniques\naimed at enhancing fairness in ML models at various stages. Additionally,\nrecent research suggests that standard ML engineering practices can also\nimprove fairness; these practices, known as fairness-aware practices, have been\ncataloged across each stage of the ML development life cycle. However, fairness\nremains context-dependent, with different domains requiring customized\nsolutions. Furthermore, existing specific bias mitigation methods may sometimes\ndegrade model performance, raising ongoing discussions about the trade-offs\ninvolved.\n  In this paper, we empirically investigate fairness-aware practices from two\nperspectives: contextual and cost-effectiveness. The contextual evaluation\nexplores how these practices perform in various application domains,\nidentifying areas where specific fairness adjustments are particularly\neffective. The cost-effectiveness evaluation considers the trade-off between\nfairness improvements and potential performance costs. Our findings provide\ninsights into how context influences the effectiveness of fairness-aware\npractices. This research aims to guide SE practitioners in selecting practices\nthat achieve fairness with minimal performance costs, supporting the\ndevelopment of ethical ML systems.",
    "published_date": "2025-03-19T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.15622v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.15454v3",
    "title": "Bias Evaluation and Mitigation in Retrieval-Augmented Medical Question-Answering Systems",
    "authors": [
      "Yuelyu Ji",
      "Hang Zhang",
      "Yanshan Wang"
    ],
    "author_ids": [],
    "abstract": "Medical Question Answering systems based on Retrieval Augmented Generation is\npromising for clinical decision support because they can integrate external\nknowledge, thus reducing inaccuracies inherent in standalone large language\nmodels (LLMs). However, these systems may unintentionally propagate or amplify\nbiases associated with sensitive demographic attributes like race, gender, and\nsocioeconomic factors. This study systematically evaluates demographic biases\nwithin medical RAG pipelines across multiple QA benchmarks, including MedQA,\nMedMCQA, MMLU, and EquityMedQA. We quantify disparities in retrieval\nconsistency and answer correctness by generating and analyzing queries\nsensitive to demographic variations. We further implement and compare several\nbias mitigation strategies to address identified biases, including Chain of\nThought reasoning, Counterfactual filtering, Adversarial prompt refinement, and\nMajority Vote aggregation. Experimental results reveal significant demographic\ndisparities, highlighting that Majority Vote aggregation notably improves\naccuracy and fairness metrics. Our findings underscore the critical need for\nexplicitly fairness-aware retrieval methods and prompt engineering strategies\nto develop truly equitable medical QA systems.",
    "published_date": "2025-03-19T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.15454v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.15580v1",
    "title": "How Well Can AI Build SD Models?",
    "authors": [
      "William Schoenberg",
      "Davidson Girard",
      "Saras Chung",
      "Ellen O'Neill",
      "Janet Velasquez",
      "Sara Metcalf"
    ],
    "author_ids": [],
    "abstract": "Introduction: As system dynamics (SD) embraces automation, AI offers\nefficiency but risks bias from missing data and flawed models. Models that omit\nmultiple perspectives and data threaten model quality, whether created by\nhumans or with the assistance of AI. To reduce uncertainty about how well AI\ncan build SD models, we introduce two metrics for evaluation of AI-generated\ncausal maps: technical correctness (causal translation) and adherence to\ninstructions (conformance).\n  Approach: We developed an open source project called sd-ai to provide a basis\nfor collaboration in the SD community, aiming to fully harness the potential of\nAI based tools like ChatGPT for dynamic modeling. Additionally, we created an\nevaluation theory along with a comprehensive suite of tests designed to\nevaluate any such tools developed within the sd-ai ecosystem.\n  Results: We tested 11 different LLMs on their ability to do causal\ntranslation as well as conform to user instruction. gpt-4.5-preview was the top\nperformer, scoring 92.9% overall, excelling in both tasks. o1 scored 100% in\ncausal translation. gpt-4o identified all causal links but struggled with\npositive polarity in decreasing terms. While gpt-4.5-preview and o1 are most\naccurate, gpt-4o is the cheapest.\n  Discussion: Causal translation and conformance tests applied to the sd-ai\nengine reveal significant variations across lLLMs, underscoring the need for\ncontinued evaluation to ensure responsible development of AI tools for dynamic\nmodeling. To address this, an open collaboration among tool developers,\nmodelers, and stakeholders is launched to standardize measures for evaluating\nthe capacity of AI tools to improve the modeling process.",
    "published_date": "2025-03-19T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.15580v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.15205v1",
    "title": "A Peek Behind the Curtain: Using Step-Around Prompt Engineering to Identify Bias and Misinformation in GenAI Models",
    "authors": [
      "Don Hickerson",
      "Mike Perkins"
    ],
    "author_ids": [],
    "abstract": "This research examines the emerging technique of step-around prompt\nengineering in GenAI research, a method that deliberately bypasses AI safety\nmeasures to expose underlying biases and vulnerabilities in GenAI models. We\ndiscuss how Internet-sourced training data introduces unintended biases and\nmisinformation into AI systems, which can be revealed through the careful\napplication of step-around techniques.\n  Drawing parallels with red teaming in cybersecurity, we argue that\nstep-around prompting serves a vital role in identifying and addressing\npotential vulnerabilities while acknowledging its dual nature as both a\nresearch tool and a potential security threat. Our findings highlight three key\nimplications: (1) the persistence of Internet-derived biases in AI training\ndata despite content filtering, (2) the effectiveness of step-around techniques\nin exposing these biases when used responsibly, and (3) the need for robust\nsafeguards against malicious applications of these methods.\n  We conclude by proposing an ethical framework for using step-around prompting\nin AI research and development, emphasizing the importance of balancing system\nimprovements with security considerations.",
    "published_date": "2025-03-19T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.15205v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.15176v1",
    "title": "A Review on Large Language Models for Visual Analytics",
    "authors": [
      "Navya Sonal Agarwal",
      "Sanjay Kumar Sonbhadra"
    ],
    "author_ids": [],
    "abstract": "This paper provides a comprehensive review of the integration of Large\nLanguage Models (LLMs) with visual analytics, addressing their foundational\nconcepts, capabilities, and wide-ranging applications. It begins by outlining\nthe theoretical underpinnings of visual analytics and the transformative\npotential of LLMs, specifically focusing on their roles in natural language\nunderstanding, natural language generation, dialogue systems, and text-to-media\ntransformations. The review further investigates how the synergy between LLMs\nand visual analytics enhances data interpretation, visualization techniques,\nand interactive exploration capabilities. Key tools and platforms including\nLIDA, Chat2VIS, Julius AI, and Zoho Analytics, along with specialized\nmultimodal models such as ChartLlama and CharXIV, are critically evaluated. The\npaper discusses their functionalities, strengths, and limitations in supporting\ndata exploration, visualization enhancement, automated reporting, and insight\nextraction. The taxonomy of LLM tasks, ranging from natural language\nunderstanding (NLU), natural language generation (NLG), to dialogue systems and\ntext-to-media transformations, is systematically explored. This review provides\na SWOT analysis of integrating Large Language Models (LLMs) with visual\nanalytics, highlighting strengths like accessibility and flexibility,\nweaknesses such as computational demands and biases, opportunities in\nmultimodal integration and user collaboration, and threats including privacy\nconcerns and skill degradation. It emphasizes addressing ethical considerations\nand methodological improvements for effective integration.",
    "published_date": "2025-03-19T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.CL",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.15176v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.15163v1",
    "title": "Global Group Fairness in Federated Learning via Function Tracking",
    "authors": [
      "Yves Rychener",
      "Daniel Kuhn",
      "Yifan Hu"
    ],
    "author_ids": [],
    "abstract": "We investigate group fairness regularizers in federated learning, aiming to\ntrain a globally fair model in a distributed setting. Ensuring global fairness\nin distributed training presents unique challenges, as fairness regularizers\ntypically involve probability metrics between distributions across all clients\nand are not naturally separable by client. To address this, we introduce a\nfunction-tracking scheme for the global fairness regularizer based on a Maximum\nMean Discrepancy (MMD), which incurs a small communication overhead. This\nscheme seamlessly integrates into most federated learning algorithms while\npreserving rigorous convergence guarantees, as demonstrated in the context of\nFedAvg. Additionally, when enforcing differential privacy, the kernel-based MMD\nregularization enables straightforward analysis through a change of kernel,\nleveraging an intuitive interpretation of kernel convolution. Numerical\nexperiments confirm our theoretical insights.",
    "published_date": "2025-03-19T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "math.OC",
      "stat.ME"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.15163v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.15092v1",
    "title": "Towards Understanding the Safety Boundaries of DeepSeek Models: Evaluation and Findings",
    "authors": [
      "Zonghao Ying",
      "Guangyi Zheng",
      "Yongxin Huang",
      "Deyue Zhang",
      "Wenxin Zhang",
      "Quanchen Zou",
      "Aishan Liu",
      "Xianglong Liu",
      "Dacheng Tao"
    ],
    "author_ids": [],
    "abstract": "This study presents the first comprehensive safety evaluation of the DeepSeek\nmodels, focusing on evaluating the safety risks associated with their generated\ncontent. Our evaluation encompasses DeepSeek's latest generation of large\nlanguage models, multimodal large language models, and text-to-image models,\nsystematically examining their performance regarding unsafe content generation.\nNotably, we developed a bilingual (Chinese-English) safety evaluation dataset\ntailored to Chinese sociocultural contexts, enabling a more thorough evaluation\nof the safety capabilities of Chinese-developed models. Experimental results\nindicate that despite their strong general capabilities, DeepSeek models\nexhibit significant safety vulnerabilities across multiple risk dimensions,\nincluding algorithmic discrimination and sexual content. These findings provide\ncrucial insights for understanding and improving the safety of large foundation\nmodels. Our code is available at\nhttps://github.com/NY1024/DeepSeek-Safety-Eval.",
    "published_date": "2025-03-19T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.15092v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.15022v1",
    "title": "xMOD: Cross-Modal Distillation for 2D/3D Multi-Object Discovery from 2D motion",
    "authors": [
      "Saad Lahlali",
      "Sandra Kara",
      "Hejer Ammar",
      "Florian Chabot",
      "Nicolas Granger",
      "Hervé Le Borgne",
      "Quoc-Cuong Pham"
    ],
    "author_ids": [],
    "abstract": "Object discovery, which refers to the task of localizing objects without\nhuman annotations, has gained significant attention in 2D image analysis.\nHowever, despite this growing interest, it remains under-explored in 3D data,\nwhere approaches rely exclusively on 3D motion, despite its several challenges.\nIn this paper, we present a novel framework that leverages advances in 2D\nobject discovery which are based on 2D motion to exploit the advantages of such\nmotion cues being more flexible and generalizable and to bridge the gap between\n2D and 3D modalities. Our primary contributions are twofold: (i) we introduce\nDIOD-3D, the first baseline for multi-object discovery in 3D data using 2D\nmotion, incorporating scene completion as an auxiliary task to enable dense\nobject localization from sparse input data; (ii) we develop xMOD, a cross-modal\ntraining framework that integrates 2D and 3D data while always using 2D motion\ncues. xMOD employs a teacher-student training paradigm across the two\nmodalities to mitigate confirmation bias by leveraging the domain gap. During\ninference, the model supports both RGB-only and point cloud-only inputs.\nAdditionally, we propose a late-fusion technique tailored to our pipeline that\nfurther enhances performance when both modalities are available at inference.\nWe evaluate our approach extensively on synthetic (TRIP-PD) and challenging\nreal-world datasets (KITTI and Waymo). Notably, our approach yields a\nsubstantial performance improvement compared with the 2D object discovery\nstate-of-the-art on all datasets with gains ranging from +8.7 to +15.1 in F1@50\nscore. The code is available at https://github.com/CEA-LIST/xMOD",
    "published_date": "2025-03-19T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.15022v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.15014v1",
    "title": "High-Order Control Barrier Functions: Insights and a Truncated Taylor-Based Formulation",
    "authors": [
      "Jianye Xu",
      "Bassam Alrifaee"
    ],
    "author_ids": [],
    "abstract": "We examine the complexity of the standard High-Order Control Barrier Function\n(HOCBF) approach and propose a truncated Taylor-based approach that reduces\ndesign parameters. First, we derive the explicit inequality condition for the\nHOCBF approach and show that the corresponding equality condition sets a lower\nbound on the barrier function value that regulates its decay rate. Next, we\npresent our Truncated Taylor CBF (TTCBF), which uses a truncated Taylor series\nto approximate the discrete-time CBF condition. While the standard HOCBF\napproach requires multiple class K functions, leading to more design parameters\nas the constraint's relative degree increases, our TTCBF approach requires only\none. We support our theoretical findings in numerical collision-avoidance\nexperiments and show that our approach ensures safety while reducing design\ncomplexity.",
    "published_date": "2025-03-19T00:00:00",
    "year": 2025,
    "categories": [
      "eess.SY",
      "cs.RO",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.15014v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.15566v1",
    "title": "Enforcing Consistency and Fairness in Multi-level Hierarchical Classification with a Mask-based Output Layer",
    "authors": [
      "Shijing Chen",
      "Shoaib Jameel",
      "Mohamed Reda Bouadjenek",
      "Feilong Tang",
      "Usman Naseem",
      "Basem Suleiman",
      "Hakim Hacid",
      "Flora D. Salim",
      "Imran Razzak"
    ],
    "author_ids": [],
    "abstract": "Traditional Multi-level Hierarchical Classification (MLHC) classifiers often\nrely on backbone models with $n$ independent output layers. This structure\ntends to overlook the hierarchical relationships between classes, leading to\ninconsistent predictions that violate the underlying taxonomy. Additionally,\nonce a backbone architecture for an MLHC classifier is selected, adapting the\nmodel to accommodate new tasks can be challenging. For example, incorporating\nfairness to protect sensitive attributes within a hierarchical classifier\nnecessitates complex adjustments to maintain the class hierarchy while\nenforcing fairness constraints. In this paper, we extend this concept to\nhierarchical classification by introducing a fair, model-agnostic layer\ndesigned to enforce taxonomy and optimize specific objectives, including\nconsistency, fairness, and exact match. Our evaluations demonstrate that the\nproposed layer not only improves the fairness of predictions but also enforces\nthe taxonomy, resulting in consistent predictions and superior performance.\nCompared to Large Language Models (LLMs) employing in-processing de-biasing\ntechniques and models without any bias correction, our approach achieves better\noutcomes in both fairness and accuracy, making it particularly valuable in\nsectors like e-commerce, healthcare, and education, where predictive\nreliability is crucial.",
    "published_date": "2025-03-19T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.15566v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.14925v1",
    "title": "pFedFair: Towards Optimal Group Fairness-Accuracy Trade-off in Heterogeneous Federated Learning",
    "authors": [
      "Haoyu Lei",
      "Shizhan Gong",
      "Qi Dou",
      "Farzan Farnia"
    ],
    "author_ids": [],
    "abstract": "Federated learning (FL) algorithms commonly aim to maximize clients' accuracy\nby training a model on their collective data. However, in several FL\napplications, the model's decisions should meet a group fairness constraint to\nbe independent of sensitive attributes such as gender or race. While such group\nfairness constraints can be incorporated into the objective function of the FL\noptimization problem, in this work, we show that such an approach would lead to\nsuboptimal classification accuracy in an FL setting with heterogeneous client\ndistributions. To achieve an optimal accuracy-group fairness trade-off, we\npropose the Personalized Federated Learning for Client-Level Group Fairness\n(pFedFair) framework, where clients locally impose their fairness constraints\nover the distributed training process. Leveraging the image embedding models,\nwe extend the application of pFedFair to computer vision settings, where we\nnumerically show that pFedFair achieves an optimal group fairness-accuracy\ntrade-off in heterogeneous FL settings. We present the results of several\nnumerical experiments on benchmark and synthetic datasets, which highlight the\nsuboptimality of non-personalized FL algorithms and the improvements made by\nthe pFedFair method.",
    "published_date": "2025-03-19T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.14925v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.14883v3",
    "title": "Envisioning an AI-Enhanced Mental Health Ecosystem",
    "authors": [
      "Kellie Yu Hui Sim",
      "Kenny Tsu Wei Choo"
    ],
    "author_ids": [],
    "abstract": "The rapid advancement of Large Language Models (LLMs), reasoning models, and\nagentic AI approaches coincides with a growing global mental health crisis,\nwhere increasing demand has not translated into adequate access to professional\nsupport, particularly for underserved populations. This presents a unique\nopportunity for AI to complement human-led interventions, offering scalable and\ncontext-aware support while preserving human connection in this sensitive\ndomain. We explore various AI applications in peer support, self-help\ninterventions, proactive monitoring, and data-driven insights, using a\nhuman-centred approach that ensures AI supports rather than replaces human\ninteraction. However, AI deployment in mental health fields presents challenges\nsuch as ethical concerns, transparency, privacy risks, and risks of\nover-reliance. We propose a hybrid ecosystem where where AI assists but does\nnot replace human providers, emphasising responsible deployment and evaluation.\nWe also present some of our early work and findings in several of these AI\napplications. Finally, we outline future research directions for refining\nAI-enhanced interventions while adhering to ethical and culturally sensitive\nguidelines.",
    "published_date": "2025-03-19T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.AI",
      "H.5.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.14883v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.14827v1",
    "title": "MMDT: Decoding the Trustworthiness and Safety of Multimodal Foundation Models",
    "authors": [
      "Chejian Xu",
      "Jiawei Zhang",
      "Zhaorun Chen",
      "Chulin Xie",
      "Mintong Kang",
      "Yujin Potter",
      "Zhun Wang",
      "Zhuowen Yuan",
      "Alexander Xiong",
      "Zidi Xiong",
      "Chenhui Zhang",
      "Lingzhi Yuan",
      "Yi Zeng",
      "Peiyang Xu",
      "Chengquan Guo",
      "Andy Zhou",
      "Jeffrey Ziwei Tan",
      "Xuandong Zhao",
      "Francesco Pinto",
      "Zhen Xiang",
      "Yu Gai",
      "Zinan Lin",
      "Dan Hendrycks",
      "Bo Li",
      "Dawn Song"
    ],
    "author_ids": [],
    "abstract": "Multimodal foundation models (MMFMs) play a crucial role in various\napplications, including autonomous driving, healthcare, and virtual assistants.\nHowever, several studies have revealed vulnerabilities in these models, such as\ngenerating unsafe content by text-to-image models. Existing benchmarks on\nmultimodal models either predominantly assess the helpfulness of these models,\nor only focus on limited perspectives such as fairness and privacy. In this\npaper, we present the first unified platform, MMDT (Multimodal DecodingTrust),\ndesigned to provide a comprehensive safety and trustworthiness evaluation for\nMMFMs. Our platform assesses models from multiple perspectives, including\nsafety, hallucination, fairness/bias, privacy, adversarial robustness, and\nout-of-distribution (OOD) generalization. We have designed various evaluation\nscenarios and red teaming algorithms under different tasks for each perspective\nto generate challenging data, forming a high-quality benchmark. We evaluate a\nrange of multimodal models using MMDT, and our findings reveal a series of\nvulnerabilities and areas for improvement across these perspectives. This work\nintroduces the first comprehensive and unique safety and trustworthiness\nevaluation platform for MMFMs, paving the way for developing safer and more\nreliable MMFMs and systems. Our platform and benchmark are available at\nhttps://mmdecodingtrust.github.io/.",
    "published_date": "2025-03-19T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.14827v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.14482v1",
    "title": "ICE-Bench: A Unified and Comprehensive Benchmark for Image Creating and Editing",
    "authors": [
      "Yulin Pan",
      "Xiangteng He",
      "Chaojie Mao",
      "Zhen Han",
      "Zeyinzi Jiang",
      "Jingfeng Zhang",
      "Yu Liu"
    ],
    "author_ids": [],
    "abstract": "Image generation has witnessed significant advancements in the past few\nyears. However, evaluating the performance of image generation models remains a\nformidable challenge. In this paper, we propose ICE-Bench, a unified and\ncomprehensive benchmark designed to rigorously assess image generation models.\nIts comprehensiveness could be summarized in the following key features: (1)\nCoarse-to-Fine Tasks: We systematically deconstruct image generation into four\ntask categories: No-ref/Ref Image Creating/Editing, based on the presence or\nabsence of source images and reference images. And further decompose them into\n31 fine-grained tasks covering a broad spectrum of image generation\nrequirements, culminating in a comprehensive benchmark. (2) Multi-dimensional\nMetrics: The evaluation framework assesses image generation capabilities across\n6 dimensions: aesthetic quality, imaging quality, prompt following, source\nconsistency, reference consistency, and controllability. 11 metrics are\nintroduced to support the multi-dimensional evaluation. Notably, we introduce\nVLLM-QA, an innovative metric designed to assess the success of image editing\nby leveraging large models. (3) Hybrid Data: The data comes from real scenes\nand virtual generation, which effectively improves data diversity and\nalleviates the bias problem in model evaluation. Through ICE-Bench, we conduct\na thorough analysis of existing generation models, revealing both the\nchallenging nature of our benchmark and the gap between current model\ncapabilities and real-world generation requirements. To foster further\nadvancements in the field, we will open-source ICE-Bench, including its\ndataset, evaluation code, and models, thereby providing a valuable resource for\nthe research community.",
    "published_date": "2025-03-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.14482v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.14459v1",
    "title": "Doubly robust identification of treatment effects from multiple environments",
    "authors": [
      "Piersilvio De Bartolomeis",
      "Julia Kostin",
      "Javier Abad",
      "Yixin Wang",
      "Fanny Yang"
    ],
    "author_ids": [],
    "abstract": "Practical and ethical constraints often require the use of observational data\nfor causal inference, particularly in medicine and social sciences. Yet,\nobservational datasets are prone to confounding, potentially compromising the\nvalidity of causal conclusions. While it is possible to correct for biases if\nthe underlying causal graph is known, this is rarely a feasible ask in\npractical scenarios. A common strategy is to adjust for all available\ncovariates, yet this approach can yield biased treatment effect estimates,\nespecially when post-treatment or unobserved variables are present. We propose\nRAMEN, an algorithm that produces unbiased treatment effect estimates by\nleveraging the heterogeneity of multiple data sources without the need to know\nor learn the underlying causal graph. Notably, RAMEN achieves doubly robust\nidentification: it can identify the treatment effect whenever the causal\nparents of the treatment or those of the outcome are observed, and the node\nwhose parents are observed satisfies an invariance assumption. Empirical\nevaluations on synthetic and real-world datasets show that our approach\noutperforms existing methods.",
    "published_date": "2025-03-18T00:00:00",
    "year": 2025,
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.ME"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.14459v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.14403v1",
    "title": "Landscape Complexity for the Empirical Risk of Generalized Linear Models: Discrimination between Structured Data",
    "authors": [
      "Theodoros G. Tsironis",
      "Aris L. Moustakas"
    ],
    "author_ids": [],
    "abstract": "We use the Kac-Rice formula and results from random matrix theory to obtain\nthe average number of critical points of a family of high-dimensional empirical\nloss functions, where the data are correlated $d$-dimensional Gaussian vectors,\nwhose number has a fixed ratio with their dimension. The correlations are\nintroduced to model the existence of structure in the data, as is common in\ncurrent Machine-Learning systems. Under a technical hypothesis, our results are\nexact in the large-$d$ limit, and characterize the annealed landscape\ncomplexity, namely the logarithm of the expected number of critical points at a\ngiven value of the loss.\n  We first address in detail the landscape of the loss function of a single\nperceptron and then generalize it to the case where two competing data sets\nwith different covariance matrices are present, with the perceptron seeking to\ndiscriminate between them. The latter model can be applied to understand the\ninterplay between adversity and non-trivial data structure. For completeness,\nwe also treat the case of a loss function used in training Generalized Linear\nModels in the presence of correlated input data.",
    "published_date": "2025-03-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cond-mat.stat-mech",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.14403v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.16534v1",
    "title": "Gender and content bias in Large Language Models: a case study on Google Gemini 2.0 Flash Experimental",
    "authors": [
      "Roberto Balestri"
    ],
    "author_ids": [],
    "abstract": "This study evaluates the biases in Gemini 2.0 Flash Experimental, a\nstate-of-the-art large language model (LLM) developed by Google, focusing on\ncontent moderation and gender disparities. By comparing its performance to\nChatGPT-4o, examined in a previous work of the author, the analysis highlights\nsome differences in ethical moderation practices. Gemini 2.0 demonstrates\nreduced gender bias, notably with female-specific prompts achieving a\nsubstantial rise in acceptance rates compared to results obtained by\nChatGPT-4o. It adopts a more permissive stance toward sexual content and\nmaintains relatively high acceptance rates for violent prompts, including\ngender-specific cases. Despite these changes, whether they constitute an\nimprovement is debatable. While gender bias has been reduced, this reduction\ncomes at the cost of permitting more violent content toward both males and\nfemales, potentially normalizing violence rather than mitigating harm.\nMale-specific prompts still generally receive higher acceptance rates than\nfemale-specific ones. These findings underscore the complexities of aligning AI\nsystems with ethical standards, highlighting progress in reducing certain\nbiases while raising concerns about the broader implications of the model's\npermissiveness. Ongoing refinements are essential to achieve moderation\npractices that ensure transparency, fairness, and inclusivity without\namplifying harmful content.",
    "published_date": "2025-03-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.16534v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.14333v1",
    "title": "Revealing higher-order neural representations with generative artificial intelligence",
    "authors": [
      "Hojjat Azimi Asrari",
      "Megan A. K. Peters"
    ],
    "author_ids": [],
    "abstract": "Studies often aim to reveal how neural representations encode aspects of an\nobserver's environment, such as its contents or structure. These are\n``first-order\" representations (FORs), because they're ``about\" the external\nworld. A less-common target is ``higher-order\" representations (HORs), which\nare ``about\" FORs -- their contents, stability, or uncertainty. HORs of\nuncertainty appear critically involved in adaptive behaviors including learning\nunder uncertainty, influencing learning rates and internal model updating based\non environmental feedback. However, HORs about uncertainty are unlikely to be\ndirect ``read-outs\" of FOR characteristics, instead reflecting estimation\nprocesses which may be lossy, bias-prone, or distortive and which may also\nincorporate estimates of distributions of uncertainty the observer is likely to\nexperience. While some research has targeted neural representations of\n``instantaneously\" estimated uncertainty, how the brain represents\n\\textit{distributions} of expected uncertainty remains largely unexplored.\nHere, we propose a novel reinforcement learning (RL) based generative\nartificial intelligence (genAI) approach to explore neural representations of\nuncertainty distributions. We use existing functional magnetic resonance\nimaging data, where humans learned to `de-noise' their brain states to achieve\ntarget neural patterns, to train denoising diffusion genAI models with RL\nalgorithms to learn noise distributions similar to how humans might learn to do\nthe same. We then explore these models' learned noise-distribution HORs\ncompared to control models trained with traditional backpropagation. Results\nreveal model-dependent differences in noise distribution representations --\nwith the RL-based model offering much higher explanatory power for human\nbehavior -- offering an exciting path towards using genAI to explore neural\nnoise-distribution HORs.",
    "published_date": "2025-03-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.NC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.14333v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.14138v1",
    "title": "Exploring Disparity-Accuracy Trade-offs in Face Recognition Systems: The Role of Datasets, Architectures, and Loss Functions",
    "authors": [
      "Siddharth D Jaiswal",
      "Sagnik Basu",
      "Sandipan Sikdar",
      "Animesh Mukherjee"
    ],
    "author_ids": [],
    "abstract": "Automated Face Recognition Systems (FRSs), developed using deep learning\nmodels, are deployed worldwide for identity verification and facial attribute\nanalysis. The performance of these models is determined by a complex\ninterdependence among the model architecture, optimization/loss function and\ndatasets. Although FRSs have surpassed human-level accuracy, they continue to\nbe disparate against certain demographics. Due to the ubiquity of applications,\nit is extremely important to understand the impact of the three components --\nmodel architecture, loss function and face image dataset on the\naccuracy-disparity trade-off to design better, unbiased platforms. In this\nwork, we perform an in-depth analysis of three FRSs for the task of gender\nprediction, with various architectural modifications resulting in ten\ndeep-learning models coupled with four loss functions and benchmark them on\nseven face datasets across 266 evaluation configurations. Our results show that\nall three components have an individual as well as a combined impact on both\naccuracy and disparity. We identify that datasets have an inherent property\nthat causes them to perform similarly across models, independent of the choice\nof loss functions. Moreover, the choice of dataset determines the model's\nperceived bias -- the same model reports bias in opposite directions for three\ngender-balanced datasets of ``in-the-wild'' face images of popular individuals.\nStudying the facial embeddings shows that the models are unable to generalize a\nuniform definition of what constitutes a ``female face'' as opposed to a ``male\nface'', due to dataset diversity. We provide recommendations to model\ndevelopers on using our study as a blueprint for model development and\nsubsequent deployment.",
    "published_date": "2025-03-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.14138v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.22705v1",
    "title": "Enhancing nonnative speech perception and production through an AI-powered application",
    "authors": [
      "Georgios P. Georgiou"
    ],
    "author_ids": [],
    "abstract": "While research on using Artificial Intelligence (AI) through various\napplications to enhance foreign language pronunciation is expanding, it has\nprimarily focused on aspects such as comprehensibility and intelligibility,\nlargely neglecting the improvement of individual speech sounds in both\nperception and production. This study seeks to address this gap by examining\nthe impact of training with an AI-powered mobile application on nonnative sound\nperception and production. Participants completed a pretest assessing their\nability to discriminate the second language English heed-hid contrast and\nproduce these vowels in sentence contexts. The intervention involved training\nwith the Speakometer mobile application, which incorporated recording tasks\nfeaturing the English vowels, along with pronunciation feedback and practice.\nThe posttest mirrored the pretest to measure changes in performance. The\nresults revealed significant improvements in both discrimination accuracy and\nproduction of the target contrast following the intervention. However,\nparticipants did not achieve native-like competence. These findings highlight\nthe effectiveness of AI-powered applications in facilitating speech acquisition\nand support their potential use for personalized, interactive pronunciation\ntraining beyond the classroom.",
    "published_date": "2025-03-18T00:00:00",
    "year": 2025,
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.22705v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.14023v1",
    "title": "Synthetic Data Generation Using Large Language Models: Advances in Text and Code",
    "authors": [
      "Mihai Nadas",
      "Laura Diosan",
      "Andreea Tomescu"
    ],
    "author_ids": [],
    "abstract": "Large language models (LLMs) have unlocked new possibilities for generating\nsynthetic training data in both natural language and code. By producing\nartificial but task-relevant examples, these models can significantly augment\nor even replace real-world datasets, especially when labeled data is scarce or\nsensitive. This paper surveys recent advances in using LLMs to create synthetic\ntext and code, emphasizing prompt-based generation, retrieval-augmented\npipelines, and iterative self-refinement. We show how these methods enrich\nlow-resource tasks such as classification and question answering, as well as\ncode-centric applications such as instruction tuning, code translation, and bug\nrepair, by enabling automated verification of functional correctness. Alongside\npotential benefits like cost-effectiveness, broad coverage, and controllable\ndiversity, we address challenges such as factual inaccuracies in generated\ntext, lack of stylistic realism, and the risk of bias amplification. Proposed\nmitigations include filtering and weighting outputs and reinforcement learning\nwith execution feedback for code. We conclude with open research directions\nlike automated prompt engineering, cross-modal data synthesis, and robust\nevaluation frameworks, highlighting the importance of LLM-generated synthetic\ndata in advancing AI while emphasizing ethical and quality safeguards.",
    "published_date": "2025-03-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.14023v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.13915v1",
    "title": "Unlocking the Potential of Unlabeled Data in Semi-Supervised Domain Generalization",
    "authors": [
      "Dongkwan Lee",
      "Kyomin Hwang",
      "Nojun Kwak"
    ],
    "author_ids": [],
    "abstract": "We address the problem of semi-supervised domain generalization (SSDG), where\nthe distributions of train and test data differ, and only a small amount of\nlabeled data along with a larger amount of unlabeled data are available during\ntraining. Existing SSDG methods that leverage only the unlabeled samples for\nwhich the model's predictions are highly confident (confident-unlabeled\nsamples), limit the full utilization of the available unlabeled data. To the\nbest of our knowledge, we are the first to explore a method for incorporating\nthe unconfident-unlabeled samples that were previously disregarded in SSDG\nsetting. To this end, we propose UPCSC to utilize these unconfident-unlabeled\nsamples in SSDG that consists of two modules: 1) Unlabeled Proxy-based\nContrastive learning (UPC) module, treating unconfident-unlabeled samples as\nadditional negative pairs and 2) Surrogate Class learning (SC) module,\ngenerating positive pairs for unconfident-unlabeled samples using their\nconfusing class set. These modules are plug-and-play and do not require any\ndomain labels, which can be easily integrated into existing approaches.\nExperiments on four widely used SSDG benchmarks demonstrate that our approach\nconsistently improves performance when attached to baselines and outperforms\ncompeting plug-and-play methods. We also analyze the role of our method in\nSSDG, showing that it enhances class-level discriminability and mitigates\ndomain gaps. The code is available at https://github.com/dongkwani/UPCSC.",
    "published_date": "2025-03-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.13915v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.13895v1",
    "title": "Exploiting Inherent Class Label: Towards Robust Scribble Supervised Semantic Segmentation",
    "authors": [
      "Xinliang Zhang",
      "Lei Zhu",
      "Shuang Zeng",
      "Hangzhou He",
      "Ourui Fu",
      "Zhengjian Yao",
      "Zhaoheng Xie",
      "Yanye Lu"
    ],
    "author_ids": [],
    "abstract": "Scribble-based weakly supervised semantic segmentation leverages only a few\nannotated pixels as labels to train a segmentation model, presenting\nsignificant potential for reducing the human labor involved in the annotation\nprocess. This approach faces two primary challenges: first, the sparsity of\nscribble annotations can lead to inconsistent predictions due to limited\nsupervision; second, the variability in scribble annotations, reflecting\ndiffering human annotator preferences, can prevent the model from consistently\ncapturing the discriminative regions of objects, potentially leading to\nunstable predictions. To address these issues, we propose a holistic framework,\nthe class-driven scribble promotion network, for robust scribble-supervised\nsemantic segmentation. This framework not only utilizes the provided scribble\nannotations but also leverages their associated class labels to generate\nreliable pseudo-labels. Within the network, we introduce a localization\nrectification module to mitigate noisy labels and a distance perception module\nto identify reliable regions surrounding scribble annotations and\npseudo-labels. In addition, we introduce new large-scale benchmarks,\nScribbleCOCO and ScribbleCityscapes, accompanied by a scribble simulation\nalgorithm that enables evaluation across varying scribble styles. Our method\ndemonstrates competitive performance in both accuracy and robustness,\nunderscoring its superiority over existing approaches. The datasets and the\ncodes will be made publicly available.",
    "published_date": "2025-03-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.13895v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.13834v1",
    "title": "See-Saw Modality Balance: See Gradient, and Sew Impaired Vision-Language Balance to Mitigate Dominant Modality Bias",
    "authors": [
      "JuneHyoung Kwon",
      "MiHyeon Kim",
      "Eunju Lee",
      "Juhwan Choi",
      "YoungBin Kim"
    ],
    "author_ids": [],
    "abstract": "Vision-language (VL) models have demonstrated strong performance across\nvarious tasks. However, these models often rely on a specific modality for\npredictions, leading to \"dominant modality bias.'' This bias significantly\nhurts performance, especially when one modality is impaired. In this study, we\nanalyze model behavior under dominant modality bias and theoretically show that\nunaligned gradients or differences in gradient magnitudes prevent balanced\nconvergence of the loss. Based on these findings, we propose a novel framework,\nBalGrad to mitigate dominant modality bias. Our approach includes\ninter-modality gradient reweighting, adjusting the gradient of KL divergence\nbased on each modality's contribution, and inter-task gradient projection to\nalign task directions in a non-conflicting manner. Experiments on UPMC\nFood-101, Hateful Memes, and MM-IMDb datasets confirm that BalGrad effectively\nalleviates over-reliance on specific modalities when making predictions.",
    "published_date": "2025-03-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.13834v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.13814v1",
    "title": "FusDreamer: Label-efficient Remote Sensing World Model for Multimodal Data Classification",
    "authors": [
      "Jinping Wang",
      "Weiwei Song",
      "Hao Chen",
      "Jinchang Ren",
      "Huimin Zhao"
    ],
    "author_ids": [],
    "abstract": "World models significantly enhance hierarchical understanding, improving data\nintegration and learning efficiency. To explore the potential of the world\nmodel in the remote sensing (RS) field, this paper proposes a label-efficient\nremote sensing world model for multimodal data fusion (FusDreamer). The\nFusDreamer uses the world model as a unified representation container to\nabstract common and high-level knowledge, promoting interactions across\ndifferent types of data, \\emph{i.e.}, hyperspectral (HSI), light detection and\nranging (LiDAR), and text data. Initially, a new latent diffusion fusion and\nmultimodal generation paradigm (LaMG) is utilized for its exceptional\ninformation integration and detail retention capabilities. Subsequently, an\nopen-world knowledge-guided consistency projection (OK-CP) module incorporates\nprompt representations for visually described objects and aligns\nlanguage-visual features through contrastive learning. In this way, the domain\ngap can be bridged by fine-tuning the pre-trained world models with limited\nsamples. Finally, an end-to-end multitask combinatorial optimization (MuCO)\nstrategy can capture slight feature bias and constrain the diffusion process in\na collaboratively learnable direction. Experiments conducted on four typical\ndatasets indicate the effectiveness and advantages of the proposed FusDreamer.\nThe corresponding code will be released at\nhttps://github.com/Cimy-wang/FusDreamer.",
    "published_date": "2025-03-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.13814v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.13792v1",
    "title": "Identifying and Mitigating Position Bias of Multi-image Vision-Language Models",
    "authors": [
      "Xinyu Tian",
      "Shu Zou",
      "Zhaoyuan Yang",
      "Jing Zhang"
    ],
    "author_ids": [],
    "abstract": "The evolution of Large Vision-Language Models (LVLMs) has progressed from\nsingle to multi-image reasoning. Despite this advancement, our findings\nindicate that LVLMs struggle to robustly utilize information across multiple\nimages, with predictions significantly affected by the alteration of image\npositions. To further explore this issue, we introduce Position-wise Question\nAnswering (PQA), a meticulously designed task to quantify reasoning\ncapabilities at each position. Our analysis reveals a pronounced position bias\nin LVLMs: open-source models excel in reasoning with images positioned later\nbut underperform with those in the middle or at the beginning, while\nproprietary models show improved comprehension for images at the beginning and\nend but struggle with those in the middle. Motivated by this, we propose SoFt\nAttention (SoFA), a simple, training-free approach that mitigates this bias by\nemploying linear interpolation between inter-image causal attention and\nbidirectional counterparts. Experimental results demonstrate that SoFA reduces\nposition bias and enhances the reasoning performance of existing LVLMs.",
    "published_date": "2025-03-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.13792v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.13752v1",
    "title": "Beyond the Individual: A Community-Engaged Framework for Ethical Online Community Research",
    "authors": [
      "Matthew Zent",
      "Seraphina Yong",
      "Dhruv Bala",
      "Stevie Chancellor",
      "Joseph A. Konstan",
      "Loren Terveen",
      "Svetlana Yarosh"
    ],
    "author_ids": [],
    "abstract": "Online community research routinely poses minimal risk to individuals, but\ndoes the same hold true for online communities? In response to high-profile\nbreaches of online community trust and increased debate in the social computing\nresearch community on the ethics of online community research, this paper\ninvestigates community-level harms and benefits of research. Through 9\nparticipatory-inspired workshops with four critical online communities\n(Wikipedia, InTheRooms, CaringBridge, and r/AskHistorians) we found researchers\nshould engage more directly with communities' primary purpose by rationalizing\ntheir methods and contributions in the context of community goals to equalize\nthe beneficiaries of community research. To facilitate deeper alignment of\nthese expectations, we present the FACTORS (Functions for Action with\nCommunities: Teaching, Overseeing, Reciprocating, and Sustaining) framework for\nethical online community research. Finally, we reflect on our findings by\nproviding implications for researchers and online communities to identify and\nimplement functions for navigating community-level harms and benefits.",
    "published_date": "2025-03-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.CY",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.13752v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.13695v1",
    "title": "Mitigating Spectral Bias in Neural Operators via High-Frequency Scaling for Physical Systems",
    "authors": [
      "Siavash Khodakarami",
      "Vivek Oommen",
      "Aniruddha Bora",
      "George Em Karniadakis"
    ],
    "author_ids": [],
    "abstract": "Neural operators have emerged as powerful surrogates for modeling complex\nphysical problems. However, they suffer from spectral bias making them\noblivious to high-frequency modes, which are present in multiscale physical\nsystems. Therefore, they tend to produce over-smoothed solutions, which is\nparticularly problematic in modeling turbulence and for systems with intricate\npatterns and sharp gradients such as multi-phase flow systems. In this work, we\nintroduce a new approach named high-frequency scaling (HFS) to mitigate\nspectral bias in convolutional-based neural operators. By integrating HFS with\nproper variants of UNet neural operators, we demonstrate a higher prediction\naccuracy by mitigating spectral bias in single and two-phase flow problems.\nUnlike Fourier-based techniques, HFS is directly applied to the latent space,\nthus eliminating the computational cost associated with the Fourier transform.\nAdditionally, we investigate alternative spectral bias mitigation through\ndiffusion models conditioned on neural operators. While the diffusion model\nintegrated with the standard neural operator may still suffer from significant\nerrors, these errors are substantially reduced when the diffusion model is\nintegrated with a HFS-enhanced neural operator.",
    "published_date": "2025-03-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "physics.comp-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.13695v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.13617v1",
    "title": "Let Synthetic Data Shine: Domain Reassembly and Soft-Fusion for Single Domain Generalization",
    "authors": [
      "Hao Li",
      "Yubin Xiao",
      "Ke Liang",
      "Mengzhu Wang",
      "Long Lan",
      "Kenli Li",
      "Xinwang Liu"
    ],
    "author_ids": [],
    "abstract": "Single Domain Generalization (SDG) aims to train models with consistent\nperformance across diverse scenarios using data from a single source. While\nusing latent diffusion models (LDMs) show promise in augmenting limited source\ndata, we demonstrate that directly using synthetic data can be detrimental due\nto significant feature distribution discrepancies between synthetic and real\ntarget domains, leading to performance degradation. To address this issue, we\npropose Discriminative Domain Reassembly and Soft-Fusion (DRSF), a training\nframework leveraging synthetic data to improve model generalization. We employ\nLDMs to produce diverse pseudo-target domain samples and introduce two key\nmodules to handle distribution bias. First, Discriminative Feature Decoupling\nand Reassembly (DFDR) module uses entropy-guided attention to recalibrate\nchannel-level features, suppressing synthetic noise while preserving semantic\nconsistency. Second, Multi-pseudo-domain Soft Fusion (MDSF) module uses\nadversarial training with latent-space feature interpolation, creating\ncontinuous feature transitions between domains. Extensive SDG experiments on\nobject detection and semantic segmentation tasks demonstrate that DRSF achieves\nsubstantial performance gains with only marginal computational overhead.\nNotably, DRSF's plug-and-play architecture enables seamless integration with\nunsupervised domain adaptation paradigms, underscoring its broad applicability\nin addressing diverse and real-world domain challenges.",
    "published_date": "2025-03-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.13617v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.13379v3",
    "title": "Error bounds for composite quantum hypothesis testing and a new characterization of the weighted Kubo-Ando geometric means",
    "authors": [
      "Péter E. Frenkel",
      "Milán Mosonyi",
      "Péter Vrana",
      "Mihály Weiner"
    ],
    "author_ids": [],
    "abstract": "The optimal error exponents of binary composite i.i.d. state discrimination\nare trivially bounded by the worst-case pairwise exponents of discriminating\nindividual elements of the sets representing the two hypotheses, and in the\nfinite-dimensional classical case, these bounds in fact give exact single-copy\nexpressions for the error exponents. In contrast, in the non-commutative case,\nthe optimal exponents are only known to be expressible in terms of regularized\ndivergences, resulting in formulas that, while conceptually relevant,\npractically not very useful. In this paper, we develop further an approach\ninitiated in [Mosonyi, Szil\\'agyi, Weiner, IEEE Trans. Inf. Th.\n68(2):1032--1067, 2022] to give improved single-copy bounds on the error\nexponents by comparing not only individual states from the two hypotheses, but\nalso various unnormalized positive semi-definite operators associated to them.\nHere, we show a number of equivalent characterizations of such operators giving\nvalid bounds, and show that in the commutative case, considering weighted\ngeometric means of the states, and in the case of two states per hypothesis,\nconsidering weighted Kubo-Ando geometric means, are optimal for this approach.\nAs a result, we give a new characterization of the weighted Kubo-Ando geometric\nmeans as the only $2$-variable operator geometric means that are block\nadditive, tensor multiplicative, and satisfy the arithmetic-geometric mean\ninequality. We also extend our results to composite quantum channel\ndiscrimination, and show an analogous optimality property of the weighted\nKubo-Ando geometric means of two quantum channels, a notion that seems to be\nnew. We extend this concept to defining the notion of superoperator perspective\nfunction and establish some of its basic properties, which may be of\nindependent interest.",
    "published_date": "2025-03-17T00:00:00",
    "year": 2025,
    "categories": [
      "quant-ph",
      "cs.IT",
      "math-ph",
      "math.FA",
      "math.IT",
      "math.MP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.13379v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.13369v1",
    "title": "Sightation Counts: Leveraging Sighted User Feedback in Building a BLV-aligned Dataset of Diagram Descriptions",
    "authors": [
      "Wan Ju Kang",
      "Eunki Kim",
      "Na Min An",
      "Sangryul Kim",
      "Haemin Choi",
      "Ki Hoon Kwak",
      "James Thorne"
    ],
    "author_ids": [],
    "abstract": "Often, the needs and visual abilities differ between the annotator group and\nthe end user group. Generating detailed diagram descriptions for blind and\nlow-vision (BLV) users is one such challenging domain. Sighted annotators could\ndescribe visuals with ease, but existing studies have shown that direct\ngenerations by them are costly, bias-prone, and somewhat lacking by BLV\nstandards. In this study, we ask sighted individuals to assess -- rather than\nproduce -- diagram descriptions generated by vision-language models (VLM) that\nhave been guided with latent supervision via a multi-pass inference. The\nsighted assessments prove effective and useful to professional educators who\nare themselves BLV and teach visually impaired learners. We release Sightation,\na collection of diagram description datasets spanning 5k diagrams and 137k\nsamples for completion, preference, retrieval, question answering, and\nreasoning training purposes and demonstrate their fine-tuning potential in\nvarious downstream tasks.",
    "published_date": "2025-03-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.13369v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.14543v2",
    "title": "Inteligencia Artificial para la conservación y uso sostenible de la biodiversidad, una visión desde Colombia (Artificial Intelligence for conservation and sustainable use of biodiversity, a view from Colombia)",
    "authors": [
      "Juan Sebastián Cañas",
      "Camila Parra-Guevara",
      "Manuela Montoya-Castrillón",
      "Julieta M Ramírez-Mejía",
      "Gabriel-Alejandro Perilla",
      "Esteban Marentes",
      "Nerieth Leuro",
      "Jose Vladimir Sandoval-Sierra",
      "Sindy Martinez-Callejas",
      "Angélica Díaz",
      "Mario Murcia",
      "Elkin A. Noguera-Urbano",
      "Jose Manuel Ochoa-Quintero",
      "Susana Rodríguez Buriticá",
      "Juan Sebastián Ulloa"
    ],
    "author_ids": [],
    "abstract": "The rise of artificial intelligence (AI) and the aggravating biodiversity\ncrisis have resulted in a research area where AI-based computational methods\nare being developed to act as allies in conservation, and the sustainable use\nand management of natural resources. While important general guidelines have\nbeen established globally regarding the opportunities and challenges that this\ninterdisciplinary research offers, it is essential to generate local\nreflections from the specific contexts and realities of each region. Hence,\nthis document aims to analyze the scope of this research area from a\nperspective focused on Colombia and the Neotropics. In this paper, we summarize\nthe main experiences and debates that took place at the Humboldt Institute\nbetween 2023 and 2024 in Colombia. To illustrate the variety of promising\nopportunities, we present current uses such as automatic species identification\nfrom images and recordings, species modeling, and in silico bioprospecting,\namong others. From the experiences described above, we highlight limitations,\nchallenges, and opportunities for in order to successfully implementate AI in\nconservation efforts and sustainable management of biological resources in the\nNeotropics. The result aims to be a guide for researchers, decision makers, and\nbiodiversity managers, facilitating the understanding of how artificial\nintelligence can be effectively integrated into conservation and sustainable\nuse strategies. Furthermore, it also seeks to open a space for dialogue on the\ndevelopment of policies that promote the responsible and ethical adoption of AI\nin local contexts, ensuring that its benefits are harnessed without\ncompromising biodiversity or the cultural and ecosystemic values inherent in\nColombia and the Neotropics.",
    "published_date": "2025-03-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.14543v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.13335v1",
    "title": "Reliable and Efficient Amortized Model-based Evaluation",
    "authors": [
      "Sang Truong",
      "Yuheng Tu",
      "Percy Liang",
      "Bo Li",
      "Sanmi Koyejo"
    ],
    "author_ids": [],
    "abstract": "Comprehensive evaluations of language models (LM) during both development and\ndeployment phases are necessary because these models possess numerous\ncapabilities (e.g., mathematical reasoning, legal support, or medical\ndiagnostic) as well as safety risks (e.g., racial bias, toxicity, or\nmisinformation). The average score across a wide range of benchmarks provides a\nsignal that helps guide the use of these LMs in practice. Currently, holistic\nevaluations are costly due to the large volume of benchmark questions, making\nfrequent evaluations impractical. A popular attempt to lower the cost is to\ncompute the average score on a subset of the benchmark. This approach,\nunfortunately, often renders an unreliable measure of LM performance because\nthe average score is often confounded with the difficulty of the questions in\nthe benchmark subset. Item response theory (IRT) was designed to address this\nchallenge, providing a reliable measurement by careful controlling for question\ndifficulty. Unfortunately, question difficulty is expensive to estimate. Facing\nthis challenge, we train a model that predicts question difficulty from its\ncontent, enabling a reliable measurement at a fraction of the cost. In\naddition, we leverage this difficulty predictor to further improve the\nevaluation efficiency through training a question generator given a difficulty\nlevel. This question generator is essential in adaptive testing, where, instead\nof using a random subset of the benchmark questions, informative questions are\nadaptively chosen based on the current estimation of LLM performance.\nExperiments on 22 common natural language benchmarks and 172 LMs show that this\napproach is more reliable and efficient compared to current common practice.",
    "published_date": "2025-03-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.13335v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.13310v1",
    "title": "Generative AI for Software Architecture. Applications, Trends, Challenges, and Future Directions",
    "authors": [
      "Matteo Esposito",
      "Xiaozhou Li",
      "Sergio Moreschini",
      "Noman Ahmad",
      "Tomas Cerny",
      "Karthik Vaidhyanathan",
      "Valentina Lenarduzzi",
      "Davide Taibi"
    ],
    "author_ids": [],
    "abstract": "Context: Generative Artificial Intelligence (GenAI) is transforming much of\nsoftware development, yet its application in software architecture is still in\nits infancy, and no prior study has systematically addressed the topic. Aim: We\naim to systematically synthesize the use, rationale, contexts, usability, and\nfuture challenges of GenAI in software architecture. Method: We performed a\nmultivocal literature review (MLR), analyzing peer-reviewed and gray\nliterature, identifying current practices, models, adoption contexts, and\nreported challenges, extracting themes via open coding. Results: Our review\nidentified significant adoption of GenAI for architectural decision support and\narchitectural reconstruction. OpenAI GPT models are predominantly applied, and\nthere is consistent use of techniques such as few-shot prompting and\nretrieved-augmented generation (RAG). GenAI has been applied mostly to initial\nstages of the Software Development Life Cycle (SDLC), such as\nRequirements-to-Architecture and Architecture-to-Code. Monolithic and\nmicroservice architectures were the dominant targets. However, rigorous testing\nof GenAI outputs was typically missing from the studies. Among the most\nfrequent challenges are model precision, hallucinations, ethical aspects,\nprivacy issues, lack of architecture-specific datasets, and the absence of\nsound evaluation frameworks. Conclusions: GenAI shows significant potential in\nsoftware design, but several challenges remain on its path to greater adoption.\nResearch efforts should target designing general evaluation methodologies,\nhandling ethics and precision, increasing transparency and explainability, and\npromoting architecture-specific datasets and benchmarks to bridge the gap\nbetween theoretical possibilities and practical use.",
    "published_date": "2025-03-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.DC",
      "cs.ET"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.13310v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.14541v1",
    "title": "Regulating Ai In Financial Services: Legal Frameworks And Compliance Challenges",
    "authors": [
      "Shahmar Mirishli"
    ],
    "author_ids": [],
    "abstract": "This article examines the evolving landscape of artificial intelligence (AI)\nregulation in financial services, detailing the legal frameworks and compliance\nchallenges posed by rapid technological adoption. By reviewing current\nlegislation, industry guidelines, and real-world use cases, it highlights how\nAI-driven processes, from fraud detection to algorithmic trading, offer\nefficiency gains yet introduce significant risks, including algorithmic bias,\ndata privacy breaches, and lack of transparency in automated decision-making.\nThe study compares regulatory approaches across major jurisdictions such as the\nEuropean Union, United States, and United Kingdom, identifying both universal\nconcerns, like the need for explainability and robust data protection, and\nregion-specific compliance requirements that impact the implementation of\nhigh-risk AI applications. Additionally, it underscores emerging areas of\nfocus, such as liability for AI-driven errors, systemic risks posed by\ninterlinked AI systems, and the ethical considerations of technology-driven\nfinancial exclusion. The findings reveal gaps in existing rules and emphasize\nthe necessity for adaptive, technology-neutral policies capable of fostering\ninnovation while safeguarding consumer rights and market integrity. The article\nconcludes by proposing a principled regulatory model that balances flexibility\nwith enforceable standards, advocating closer collaboration between\npolicymakers, financial institutions, and AI developers to ensure a secure,\nfair, and forward-looking framework for AI in finance.",
    "published_date": "2025-03-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "q-fin.GN"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.14541v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.14540v1",
    "title": "The Role of Legal Frameworks in Shaping Ethical Artificial Intelligence Use in Corporate Governance",
    "authors": [
      "Shahmar Mirishli"
    ],
    "author_ids": [],
    "abstract": "This article examines the evolving role of legal frameworks in shaping\nethical artificial intelligence (AI) use in corporate governance. As AI systems\nbecome increasingly prevalent in business operations and decision-making, there\nis a growing need for robust governance structures to ensure their responsible\ndevelopment and deployment. Through analysis of recent legislative initiatives,\nindustry standards, and scholarly perspectives, this paper explores key legal\nand regulatory approaches aimed at promoting transparency, accountability, and\nfairness in corporate AI applications. It evaluates the strengths and\nlimitations of current frameworks, identifies emerging best practices, and\noffers recommendations for developing more comprehensive and effective AI\ngovernance regimes. The findings highlight the importance of adaptable,\nprinciple-based regulations coupled with sector-specific guidance to address\nthe unique challenges posed by AI technologies in the corporate sphere.",
    "published_date": "2025-03-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.14540v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.14539v1",
    "title": "Ethical Implications of AI in Data Collection: Balancing Innovation with Privacy",
    "authors": [
      "Shahmar Mirishli"
    ],
    "author_ids": [],
    "abstract": "This article examines the ethical and legal implications of artificial\nintelligence (AI) driven data collection, focusing on developments from 2023 to\n2024. It analyzes recent advancements in AI technologies and their impact on\ndata collection practices across various sectors. The study compares regulatory\napproaches in the European Union, the United States, and China, highlighting\nthe challenges in creating a globally harmonized framework for AI governance.\nKey ethical issues, including informed consent, algorithmic bias, and privacy\nprotection, are critically assessed in the context of increasingly\nsophisticated AI systems. The research explores case studies in healthcare,\nfinance, and smart cities to illustrate the practical challenges of AI\nimplementation. It evaluates the effectiveness of current legal frameworks and\nproposes solutions encompassing legal and policy recommendations, technical\nsafeguards, and ethical frameworks. The article emphasizes the need for\nadaptive governance and international cooperation to address the global nature\nof AI development while balancing innovation with the protection of individual\nrights and societal values.",
    "published_date": "2025-03-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.14539v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.07108v1",
    "title": "OKRA: an Explainable, Heterogeneous, Multi-Stakeholder Job Recommender System",
    "authors": [
      "Roan Schellingerhout",
      "Francesco Barile",
      "Nava Tintarev"
    ],
    "author_ids": [],
    "abstract": "The use of recommender systems in the recruitment domain has been labeled as\n'high-risk' in recent legislation. As a result, strict requirements regarding\nexplainability and fairness have been put in place to ensure proper treatment\nof all involved stakeholders. To allow for stakeholder-specific explainability,\nwhile also handling highly heterogeneous recruitment data, we propose a novel\nexplainable multi-stakeholder job recommender system using graph neural\nnetworks: the Occupational Knowledge-based Recommender using Attention (OKRA).\nThe proposed method is capable of providing both candidate- and company-side\nrecommendations and explanations. We find that OKRA performs substantially\nbetter than six baselines in terms of nDCG for two datasets. Furthermore, we\nfind that the tested models show a bias toward candidates and vacancies located\nin urban areas. Overall, our findings suggest that OKRA provides a balance\nbetween accuracy, explainability, and fairness.",
    "published_date": "2025-03-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.07108v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.13149v1",
    "title": "Are LLMs (Really) Ideological? An IRT-based Analysis and Alignment Tool for Perceived Socio-Economic Bias in LLMs",
    "authors": [
      "Jasmin Wachter",
      "Michael Radloff",
      "Maja Smolej",
      "Katharina Kinder-Kurlanda"
    ],
    "author_ids": [],
    "abstract": "We introduce an Item Response Theory (IRT)-based framework to detect and\nquantify socioeconomic bias in large language models (LLMs) without relying on\nsubjective human judgments. Unlike traditional methods, IRT accounts for item\ndifficulty, improving ideological bias estimation. We fine-tune two LLM\nfamilies (Meta-LLaMa 3.2-1B-Instruct and Chat- GPT 3.5) to represent distinct\nideological positions and introduce a two-stage approach: (1) modeling response\navoidance and (2) estimating perceived bias in answered responses. Our results\nshow that off-the-shelf LLMs often avoid ideological engagement rather than\nexhibit bias, challenging prior claims of partisanship. This empirically\nvalidated framework enhances AI alignment research and promotes fairer AI\ngovernance.",
    "published_date": "2025-03-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.13149v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.21791v1",
    "title": "SeisRDT: Latent Diffusion Model Based On Representation Learning For Seismic Data Interpolation And Reconstruction",
    "authors": [
      "Shuang Wang",
      "Fei Deng",
      "Peifan Jiang",
      "Zezheng Ni",
      "Bin Wang"
    ],
    "author_ids": [],
    "abstract": "Due to limitations such as geographic, physical, or economic factors,\ncollected seismic data often have missing traces. Traditional seismic data\nreconstruction methods face the challenge of selecting numerous empirical\nparameters and struggle to handle large-scale continuous missing traces. With\nthe advancement of deep learning, various diffusion models have demonstrated\nstrong reconstruction capabilities. However, these UNet-based diffusion models\nrequire significant computational resources and struggle to learn the\ncorrelation between different traces in seismic data. To address the complex\nand irregular missing situations in seismic data, we propose a latent diffusion\ntransformer utilizing representation learning for seismic data reconstruction.\nBy employing a mask modeling scheme based on representation learning, the\nrepresentation module uses the token sequence of known data to infer the token\nsequence of unknown data, enabling the reconstructed data from the diffusion\nmodel to have a more consistent data distribution and better correlation and\naccuracy with the known data. We propose the Representation Diffusion\nTransformer architecture, and a relative positional bias is added when\ncalculating attention, enabling the diffusion model to achieve global modeling\ncapability for seismic data. Using a pre-trained data compression model\ncompresses the training and inference processes of the diffusion model into a\nlatent space, which, compared to other diffusion model-based reconstruction\nmethods, reduces computational and inference costs. Reconstruction experiments\non field and synthetic datasets indicate that our method achieves higher\nreconstruction accuracy than existing methods and can handle various complex\nmissing scenarios.",
    "published_date": "2025-03-17T00:00:00",
    "year": 2025,
    "categories": [
      "physics.geo-ph",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.21791v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.12994v1",
    "title": "Conversation-Based Multimodal Abuse Detection Through Text and Graph Embeddings",
    "authors": [
      "Noé Cecillon",
      "Vincent Labatut",
      "Richard Dufour"
    ],
    "author_ids": [],
    "abstract": "Abusive behavior is common on online social networks, and forces the hosts of\nsuch platforms to find new solutions to address this problem. Various methods\nhave been proposed to automate this task in the past decade. Most of them rely\non the exchanged content, but ignore the structure and dynamics of the\nconversation, which could provide some relevant information. In this article,\nwe propose to use representation learning methods to automatically produce\nembeddings of this textual content and of the conversational graphs depicting\nmessage exchanges. While the latter could be enhanced by including additional\ninformation on top of the raw conversational structure, no method currently\nexists to learn wholegraph representations using simultaneously edge\ndirections, weights, signs, and vertex attributes. We propose two such methods\nto fill this gap in the literature. We experiment with 5 textual and 13 graph\nembedding methods, and apply them to a dataset of online messages annotated for\nabuse detection. Our best results achieve an F -measure of 81.02 using text\nalone and 80.61 using graphs alone. We also combine both modalities of\ninformation (text and graphs) through three fusion strategies, and show that\nthis strongly improves abuse detection performance, increasing the F -measure\nto 87.06. Finally, we identify which specific engineered features are captured\nby the embedding methods under consideration. These features have clear\ninterpretations and help explain what information the representation learning\nmethods deem discriminative.",
    "published_date": "2025-03-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.12994v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.12640v1",
    "title": "What is unethical about software? User perceptions in the Netherlands",
    "authors": [
      "Yagil Elias",
      "Tom P. Humbert",
      "Lauren Olson",
      "Emitzá Guzmán"
    ],
    "author_ids": [],
    "abstract": "Software has the potential to improve lives. Yet, unethical and uninformed\nsoftware practices are at the root of an increasing number of ethical concerns.\nDespite its pervasiveness, few research has analyzed end-users perspectives on\nthe ethical issues of the software they use. We address this gap, and\ninvestigate end-user's ethical concerns in software through 19 semi-structured\ninterviews with residents of the Netherlands. We ask a diverse group of users\nabout their ethical concerns when using everyday software applications. We\ninvestigate the underlying reasons for their concerns and what solutions they\npropose to eliminate them. We find that our participants actively worry about\nprivacy, transparency, manipulation, safety and inappropriate content; with\nprivacy and manipulation often being at the center of their worries. Our\nparticipants demand software solutions to improve information clarity in\napplications and provide more control over the user experience. They further\nexpect larger systematic changes within software practices and government\nregulation.",
    "published_date": "2025-03-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.12640v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.16518v1",
    "title": "Advancing Human-Machine Teaming: Concepts, Challenges, and Applications",
    "authors": [
      "Dian Chen",
      "Han Jun Yoon",
      "Zelin Wan",
      "Nithin Alluru",
      "Sang Won Lee",
      "Richard He",
      "Terrence J. Moore",
      "Frederica F. Nelson",
      "Sunghyun Yoon",
      "Hyuk Lim",
      "Dan Dongseong Kim",
      "Jin-Hee Cho"
    ],
    "author_ids": [],
    "abstract": "Human-Machine Teaming (HMT) is revolutionizing collaboration across domains\nsuch as defense, healthcare, and autonomous systems by integrating AI-driven\ndecision-making, trust calibration, and adaptive teaming. This survey presents\na comprehensive taxonomy of HMT, analyzing theoretical models, including\nreinforcement learning, instance-based learning, and interdependence theory,\nalongside interdisciplinary methodologies. Unlike prior reviews, we examine\nteam cognition, ethical AI, multi-modal interactions, and real-world evaluation\nframeworks. Key challenges include explainability, role allocation, and\nscalable benchmarking. We propose future research in cross-domain adaptation,\ntrust-aware AI, and standardized testbeds. By bridging computational and social\nsciences, this work lays a foundation for resilient, ethical, and scalable HMT\nsystems.",
    "published_date": "2025-03-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.16518v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.05325v1",
    "title": "Unequal Opportunities: Examining the Bias in Geographical Recommendations by Large Language Models",
    "authors": [
      "Shiran Dudy",
      "Thulasi Tholeti",
      "Resmi Ramachandranpillai",
      "Muhammad Ali",
      "Toby Jia-Jun Li",
      "Ricardo Baeza-Yates"
    ],
    "author_ids": [],
    "abstract": "Recent advancements in Large Language Models (LLMs) have made them a popular\ninformation-seeking tool among end users. However, the statistical training\nmethods for LLMs have raised concerns about their representation of\nunder-represented topics, potentially leading to biases that could influence\nreal-world decisions and opportunities. These biases could have significant\neconomic, social, and cultural impacts as LLMs become more prevalent, whether\nthrough direct interactions--such as when users engage with chatbots or\nautomated assistants--or through their integration into third-party\napplications (as agents), where the models influence decision-making processes\nand functionalities behind the scenes. Our study examines the biases present in\nLLMs recommendations of U.S. cities and towns across three domains: relocation,\ntourism, and starting a business. We explore two key research questions: (i)\nHow similar LLMs responses are, and (ii) How this similarity might favor areas\nwith certain characteristics over others, introducing biases. We focus on the\nconsistency of LLMs responses and their tendency to over-represent or\nunder-represent specific locations. Our findings point to consistent\ndemographic biases in these recommendations, which could perpetuate a\n``rich-get-richer'' effect that widens existing economic disparities.",
    "published_date": "2025-03-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.05325v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.12613v1",
    "title": "Negotiative Alignment: Embracing Disagreement to Achieve Fairer Outcomes -- Insights from Urban Studies",
    "authors": [
      "Rashid Mushkani",
      "Hugo Berard",
      "Shin Koseki"
    ],
    "author_ids": [],
    "abstract": "Cities are not monolithic; they are arenas of negotiation among groups that\nhold varying needs, values, and experiences. Conventional methods of urban\nassessment -- from standardized surveys to AI-driven evaluations -- frequently\nrely on a single consensus metric (e.g., an average measure of inclusivity or\nsafety). Although such aggregations simplify design decisions, they risk\nobscuring the distinct perspectives of marginalized populations. In this paper,\nwe present findings from a community-centered study in Montreal involving 35\nresidents with diverse demographic and social identities, particularly\nwheelchair users, seniors, and LGBTQIA2+ individuals. Using rating and ranking\ntasks on 20 urban sites, we observe that disagreements are systematic rather\nthan random, reflecting structural inequalities, differing cultural values, and\npersonal experiences of safety and accessibility.\n  Based on these empirical insights, we propose negotiative alignment, an AI\nframework that treats disagreement as an essential input to be preserved,\nanalyzed, and addressed. Negotiative alignment builds on pluralistic models by\ndynamically updating stakeholder preferences through multi-agent negotiation\nmechanisms, ensuring no single perspective is marginalized. We outline how this\nframework can be integrated into urban analytics -- and other decision-making\ncontexts -- to retain minority viewpoints, adapt to changing stakeholder\nconcerns, and enhance fairness and accountability. The study demonstrates that\npreserving and engaging with disagreement, rather than striving for an\nartificial consensus, can produce more equitable and responsive AI-driven\noutcomes in urban design.",
    "published_date": "2025-03-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.12613v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.12579v1",
    "title": "Focusing Robot Open-Ended Reinforcement Learning Through Users' Purposes",
    "authors": [
      "Emilio Cartoni",
      "Gianluca Cioccolini",
      "Gianluca Baldassarre"
    ],
    "author_ids": [],
    "abstract": "Open-Ended Learning (OEL) autonomous robots can acquire new skills and\nknowledge through direct interaction with their environment, relying on\nmechanisms such as intrinsic motivations and self-generated goals to guide\nlearning processes. OEL robots are highly relevant for applications as they can\nautonomously leverage acquired knowledge to perform tasks beneficial to human\nusers in unstructured environments, addressing challenges unforeseen at design\ntime. However, OEL robots face a significant limitation: their openness may\nlead them to waste time learning information that is irrelevant to tasks\ndesired by specific users. Here, we propose a solution called `Purpose-Directed\nOpen-Ended Learning' (POEL), based on the novel concept of `purpose' introduced\nin previous work. A purpose specifies what users want the robot to achieve. The\nkey insight of this work is that purpose can focus OEL on learning\nself-generated classes of tasks that, while unknown during autonomous learning\n(as typical in OEL), involve objects relevant to the purpose. This concept is\noperationalised in a novel robot architecture capable of receiving a human\npurpose through speech-to-text, analysing the scene to identify objects, and\nusing a Large Language Model to reason about which objects are\npurpose-relevant. These objects are then used to bias OEL exploration towards\ntheir spatial proximity and to self-generate rewards that favour interactions\nwith them. The solution is tested in a simulated scenario where a\ncamera-arm-gripper robot interacts freely with purpose-related and distractor\nobjects. For the first time, the results demonstrate the potential advantages\nof purpose-focused OEL over state-of-the-art OEL methods, enabling robots to\nhandle unstructured environments while steering their learning toward knowledge\nacquisition relevant to users.",
    "published_date": "2025-03-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.12579v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.12536v1",
    "title": "Debiasing Diffusion Model: Enhancing Fairness through Latent Representation Learning in Stable Diffusion Model",
    "authors": [
      "Lin-Chun Huang",
      "Ching Chieh Tsao",
      "Fang-Yi Su",
      "Jung-Hsien Chiang"
    ],
    "author_ids": [],
    "abstract": "Image generative models, particularly diffusion-based models, have surged in\npopularity due to their remarkable ability to synthesize highly realistic\nimages. However, since these models are data-driven, they inherit biases from\nthe training datasets, frequently leading to disproportionate group\nrepresentations that exacerbate societal inequities. Traditionally, efforts to\ndebiase these models have relied on predefined sensitive attributes,\nclassifiers trained on such attributes, or large language models to steer\noutputs toward fairness. However, these approaches face notable drawbacks:\npredefined attributes do not adequately capture complex and continuous\nvariations among groups. To address these issues, we introduce the Debiasing\nDiffusion Model (DDM), which leverages an indicator to learn latent\nrepresentations during training, promoting fairness through balanced\nrepresentations without requiring predefined sensitive attributes. This\napproach not only demonstrates its effectiveness in scenarios previously\naddressed by conventional techniques but also enhances fairness without relying\non predefined sensitive attributes as conditions. In this paper, we discuss the\nlimitations of prior bias mitigation techniques in diffusion-based models,\nelaborate on the architecture of the DDM, and validate the effectiveness of our\napproach through experiments.",
    "published_date": "2025-03-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CV",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.12536v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.12527v1",
    "title": "A Plug-and-Play Learning-based IMU Bias Factor for Robust Visual-Inertial Odometry",
    "authors": [
      "Yang Yi",
      "Kunqing Wang",
      "Jinpu Zhang",
      "Zhen Tan",
      "Xiangke Wang",
      "Hui Shen",
      "Dewen Hu"
    ],
    "author_ids": [],
    "abstract": "The bias of low-cost Inertial Measurement Units (IMU) is a critical factor\naffecting the performance of Visual-Inertial Odometry (VIO). In particular,\nwhen visual tracking encounters errors, the optimized bias results may deviate\nsignificantly from the true values, adversely impacting the system's stability\nand localization precision. In this paper, we propose a novel plug-and-play\nframework featuring the Inertial Prior Network (IPNet), which is designed to\naccurately estimate IMU bias. Recognizing the substantial impact of initial\nbias errors in low-cost inertial devices on system performance, our network\ndirectly leverages raw IMU data to estimate the mean bias, eliminating the\ndependency on historical estimates in traditional recursive predictions and\neffectively preventing error propagation. Furthermore, we introduce an\niterative approach to calculate the mean value of the bias for network\ntraining, addressing the lack of bias labels in many visual-inertial datasets.\nThe framework is evaluated on two public datasets and one self-collected\ndataset. Extensive experiments demonstrate that our method significantly\nenhances both localization precision and robustness, with the ATE-RMSE metric\nimproving on average by 46\\%. The source code and video will be available at\n\\textcolor{red}{https://github.com/yiyscut/VIO-IPNet.git}.",
    "published_date": "2025-03-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.12527v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.12465v1",
    "title": "On Local Minimum Entropy Principle of High-Order Schemes for Relativistic Euler Equations",
    "authors": [
      "Shumo Cui",
      "Kailiang Wu",
      "Linfeng Xu"
    ],
    "author_ids": [],
    "abstract": "This paper establishes the minimum entropy principle (MEP) for the\nrelativistic Euler equations with a broad class of equations of state (EOSs)\nand addresses the challenge of preserving the local version of the discovered\nMEP in high-order numerical schemes. At the continuous level, we find out a\nfamily of entropy pairs for the relativistic Euler equations and provide\nrigorous analysis to prove the strict convexity of entropy under a necessary\nand sufficient condition. At the numerical level, we develop a rigorous\nframework for designing provably entropy-preserving high-order schemes that\nensure both physical admissibility and the discovered MEP. The relativistic\neffects, coupled with the abstract and general EOS formulation, introduce\nsignificant challenges not encountered in the nonrelativistic case or with the\nideal EOS. In particular, entropy is a highly nonlinear and implicit function\nof the conservative variables, making it particularly difficult to enforce\nentropy preservation. To address these challenges, we establish a series of\nauxiliary theories via highly technical inequalities. Another key innovation is\nthe use of geometric quasi-linearization (GQL), which reformulates the\nnonlinear constraints into equivalent linear ones by introducing additional\nfree parameters. These advancements form the foundation of our\nentropy-preserving analysis. We propose novel, robust, locally\nentropy-preserving high-order frameworks. A central challenge is accurately\nestimating the local minimum of entropy, particularly in the presence of shock\nwaves at unknown locations. To address this, we introduce two new approaches\nfor estimating local lower bounds of specific entropy, which prove effective\nfor both smooth and discontinuous problems. Numerical experiments demonstrate\nthat our entropy-preserving methods maintain high-order accuracy while\neffectively suppressing spurious oscillations.",
    "published_date": "2025-03-16T00:00:00",
    "year": 2025,
    "categories": [
      "math.NA",
      "astro-ph.IM",
      "cs.NA",
      "physics.comp-ph",
      "physics.flu-dyn"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.12465v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.12453v1",
    "title": "Shape Bias and Robustness Evaluation via Cue Decomposition for Image Classification and Segmentation",
    "authors": [
      "Edgar Heinert",
      "Thomas Gottwald",
      "Annika Mütze",
      "Matthias Rottmann"
    ],
    "author_ids": [],
    "abstract": "Previous works studied how deep neural networks (DNNs) perceive image content\nin terms of their biases towards different image cues, such as texture and\nshape. Previous methods to measure shape and texture biases are typically\nstyle-transfer-based and limited to DNNs for image classification. In this\nwork, we provide a new evaluation procedure consisting of 1) a\ncue-decomposition method that comprises two AI-free data pre-processing methods\nextracting shape and texture cues, respectively, and 2) a novel\ncue-decomposition shape bias evaluation metric that leverages the\ncue-decomposition data. For application purposes we introduce a corresponding\ncue-decomposition robustness metric that allows for the estimation of the\nrobustness of a DNN w.r.t. image corruptions. In our numerical experiments, our\nfindings for biases in image classification DNNs align with those of previous\nevaluation metrics. However, our cue-decomposition robustness metric shows\nsuperior results in terms of estimating the robustness of DNNs. Furthermore,\nour results for DNNs on the semantic segmentation datasets Cityscapes and\nADE20k for the first time shed light into the biases of semantic segmentation\nDNNs.",
    "published_date": "2025-03-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.12453v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.17383v1",
    "title": "The European research elite: a cross-national study of highly productive academics in 11 countries",
    "authors": [
      "Marek Kwiek"
    ],
    "author_ids": [],
    "abstract": "In this paper, we focus on a rare scholarly theme of highly productive\nacademics, statistically confirming their pivotal role in knowledge production\nacross 11 systems studied. The upper 10 % of highly productive academics in 11\nEuropean countries studied (N=17,211) provide on average almost half of all\nacademic knowledge production. In contrast to dominating bibliometric studies\nof research productivity, we focus on academic attitudes, behaviors, and\nperceptions as predictors of becoming research top performers across European\nsystems. Our paper provides a (large-scale and cross-country) corroboration of\nthe systematic inequality in knowledge production, for the first time argued\nfor by Lotka and de Solla Price. We corroborate the deep academic inequality in\nscience and explore this segment of the academic profession. The European\nresearch elite is a highly homogeneous group of academics whose high research\nperformance is driven by structurally similar factors, mostly individual rather\nthan institutional. Highly productive academics are similar from a\ncross-national perspective, and they substantially differ intra-nationally from\ntheir lower-performing colleagues.",
    "published_date": "2025-03-16T00:00:00",
    "year": 2025,
    "categories": [
      "physics.soc-ph",
      "cs.DL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.17383v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.12374v2",
    "title": "Unveiling Pitfalls: Understanding Why AI-driven Code Agents Fail at GitHub Issue Resolution",
    "authors": [
      "Zhi Chen",
      "Wei Ma",
      "Lingxiao Jiang"
    ],
    "author_ids": [],
    "abstract": "AI-driven software development has rapidly advanced with the emergence of\nsoftware development agents that leverage large language models (LLMs) to\ntackle complex, repository-level software engineering tasks. These agents go\nbeyond just generation of final code; they engage in multi-step reasoning,\nutilize various tools for code modification and debugging, and interact with\nexecution environments to diagnose and iteratively resolve issues. However,\nmost existing evaluations focus primarily on static analyses of final code\noutputs, yielding limited insights into the agents' dynamic problem-solving\nprocesses. To fill this gap, we conduct an in-depth empirical study on 3,977\nsolving-phase trajectories and 3,931 testing-phase logs from 8 top-ranked\nagents evaluated on 500 GitHub issues in the SWE-Bench benchmark. Our\nexploratory analysis shows that Python execution errors during the issue\nresolution phase correlate with lower resolution rates and increased reasoning\noverheads. We have identified the most prevalent errors -- such as\nModuleNotFoundError and TypeError -- and highlighted particularly challenging\nerrors like OSError and database-related issues (e.g., IntegrityError) that\ndemand significantly more debugging effort. Furthermore, we have discovered 3\nbugs in the SWE-Bench platform that affect benchmark fairness and accuracy;\nthese issues have been reported to and confirmed by the maintainers. To promote\ntransparency and foster future research, we publicly share our datasets and\nanalysis scripts.",
    "published_date": "2025-03-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.12374v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.12367v1",
    "title": "Integrating mobile and fixed monitoring data for high-resolution PM2.5 mapping using machine learning",
    "authors": [
      "Rui Xu",
      "Dawen Yao",
      "Yuzhuang Pian",
      "Ruhui Cao",
      "Yixin Fu",
      "Xinru Yang",
      "Ting Gan",
      "Yonghong Liu"
    ],
    "author_ids": [],
    "abstract": "Constructing high resolution air pollution maps at lower cost is crucial for\nsustainable city management and public health risk assessment. However,\ntraditional fixed-site monitoring lacks spatial coverage, while mobile low-cost\nsensors exhibit significant data instability. This study integrates PM2.5 data\nfrom 320 taxi-mounted mobile low-cost sensors and 52 fixed monitoring stations\nto address these limitations. By employing the machine learning methods, an\nappropriate mapping relationship was established between fixed and mobile\nmonitoring concentration. The resulting pollution maps achieved 500-meter\nspatial and 5-minute temporal resolutions, showing close alignment with fixed\nmonitoring data (+4.35% bias) but significant deviation from raw mobile data\n(-31.77%). The fused map exhibits the fine-scale spatial variability also\nobserved in the mobile pollution map, while showing the stable temporal\nvariability closer to that of the fixed pollution map (fixed: 1.12 plus or\nminus 0.73%, mobile: 3.15 plus or minus 2.44%, mapped: 1.01 plus or minus\n0.65%). These findings demonstrate the potential of large-scale mobile low-cost\nsensor networks for high-resolution air quality mapping, supporting targeted\nurban environmental governance and health risk mitigation.",
    "published_date": "2025-03-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "physics.ao-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.12367v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.13543v1",
    "title": "Enhancing Visual Representation with Textual Semantics: Textual Semantics-Powered Prototypes for Heterogeneous Federated Learning",
    "authors": [
      "Xinghao Wu",
      "Jianwei Niu",
      "Xuefeng Liu",
      "Guogang Zhu",
      "Jiayuan Zhang",
      "Shaojie Tang"
    ],
    "author_ids": [],
    "abstract": "Federated Prototype Learning (FedPL) has emerged as an effective strategy for\nhandling data heterogeneity in Federated Learning (FL). In FedPL, clients\ncollaboratively construct a set of global feature centers (prototypes), and let\nlocal features align with these prototypes to mitigate the effects of data\nheterogeneity. The performance of FedPL highly depends on the quality of\nprototypes. Existing methods assume that larger inter-class distances among\nprototypes yield better performance, and thus design different methods to\nincrease these distances. However, we observe that while these methods increase\nprototype distances to enhance class discrimination, they inevitably disrupt\nessential semantic relationships among classes, which are crucial for model\ngeneralization. This raises an important question: how to construct prototypes\nthat inherently preserve semantic relationships among classes? Directly\nlearning these relationships from limited and heterogeneous client data can be\nproblematic in FL. Recently, the success of pre-trained language models (PLMs)\ndemonstrates their ability to capture semantic relationships from vast textual\ncorpora. Motivated by this, we propose FedTSP, a novel method that leverages\nPLMs to construct semantically enriched prototypes from the textual modality,\nenabling more effective collaboration in heterogeneous data settings. We first\nuse a large language model (LLM) to generate fine-grained textual descriptions\nfor each class, which are then processed by a PLM on the server to form textual\nprototypes. To address the modality gap between client image models and the\nPLM, we introduce trainable prompts, allowing prototypes to adapt better to\nclient tasks. Extensive experiments demonstrate that FedTSP mitigates data\nheterogeneity while significantly accelerating convergence.",
    "published_date": "2025-03-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.13543v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.12335v2",
    "title": "GS-I$^{3}$: Gaussian Splatting for Surface Reconstruction from Illumination-Inconsistent Images",
    "authors": [
      "Tengfei Wang",
      "Yongmao Hou",
      "Zhaoning Zhang",
      "Yiwei Xu",
      "Zongqian Zhan",
      "Xin Wang"
    ],
    "author_ids": [],
    "abstract": "Accurate geometric surface reconstruction, providing essential environmental\ninformation for navigation and manipulation tasks, is critical for enabling\nrobotic self-exploration and interaction. Recently, 3D Gaussian Splatting\n(3DGS) has gained significant attention in the field of surface reconstruction\ndue to its impressive geometric quality and computational efficiency. While\nrecent relevant advancements in novel view synthesis under inconsistent\nillumination using 3DGS have shown promise, the challenge of robust surface\nreconstruction under such conditions is still being explored. To address this\nchallenge, we propose a method called GS-3I. Specifically, to mitigate 3D\nGaussian optimization bias caused by underexposed regions in single-view\nimages, based on Convolutional Neural Network (CNN), a tone mapping correction\nframework is introduced. Furthermore, inconsistent lighting across multi-view\nimages, resulting from variations in camera settings and complex scene\nillumination, often leads to geometric constraint mismatches and deviations in\nthe reconstructed surface. To overcome this, we propose a normal compensation\nmechanism that integrates reference normals extracted from single-view image\nwith normals computed from multi-view observations to effectively constrain\ngeometric inconsistencies. Extensive experimental evaluations demonstrate that\nGS-3I can achieve robust and accurate surface reconstruction across complex\nillumination scenarios, highlighting its effectiveness and versatility in this\ncritical challenge. https://github.com/TFwang-9527/GS-3I",
    "published_date": "2025-03-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.12335v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.12317v1",
    "title": "A Transformer-based survival model for prediction of all-cause mortality in heart failure patients: a multi-cohort study",
    "authors": [
      "Shishir Rao",
      "Nouman Ahmed",
      "Gholamreza Salimi-Khorshidi",
      "Christopher Yau",
      "Huimin Su",
      "Nathalie Conrad",
      "Folkert W Asselbergs",
      "Mark Woodward",
      "Rod Jackson",
      "John GF Cleland",
      "Kazem Rahimi"
    ],
    "author_ids": [],
    "abstract": "We developed and validated TRisk, a Transformer-based AI model predicting\n36-month mortality in heart failure patients by analysing temporal patient\njourneys from UK electronic health records (EHR). Our study included 403,534\nheart failure patients (ages 40-90) from 1,418 English general practices, with\n1,063 practices for model derivation and 355 for external validation. TRisk was\ncompared against the MAGGIC-EHR model across various patient subgroups. With\nmedian follow-up of 9 months, TRisk achieved a concordance index of 0.845 (95%\nconfidence interval: [0.841, 0.849]), significantly outperforming MAGGIC-EHR's\n0.728 (0.723, 0.733) for predicting 36-month all-cause mortality. TRisk showed\nmore consistent performance across sex, age, and baseline characteristics,\nsuggesting less bias. We successfully adapted TRisk to US hospital data through\ntransfer learning, achieving a C-index of 0.802 (0.789, 0.816) with 21,767\npatients. Explainability analyses revealed TRisk captured established risk\nfactors while identifying underappreciated predictors like cancers and hepatic\nfailure that were important across both cohorts. Notably, cancers maintained\nstrong prognostic value even a decade after diagnosis. TRisk demonstrated\nwell-calibrated mortality prediction across both healthcare systems. Our\nfindings highlight the value of tracking longitudinal health profiles and\nrevealed risk factors not included in previous expert-driven models.",
    "published_date": "2025-03-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.12317v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.13537v1",
    "title": "FedTilt: Towards Multi-Level Fairness-Preserving and Robust Federated Learning",
    "authors": [
      "Binghui Zhang",
      "Luis Mares De La Cruz",
      "Binghui Wang"
    ],
    "author_ids": [],
    "abstract": "Federated Learning (FL) is an emerging decentralized learning paradigm that\ncan partly address the privacy concern that cannot be handled by traditional\ncentralized and distributed learning. Further, to make FL practical, it is also\nnecessary to consider constraints such as fairness and robustness. However,\nexisting robust FL methods often produce unfair models, and existing fair FL\nmethods only consider one-level (client) fairness and are not robust to\npersistent outliers (i.e., injected outliers into each training round) that are\ncommon in real-world FL settings. We propose \\texttt{FedTilt}, a novel FL that\ncan preserve multi-level fairness and be robust to outliers. In particular, we\nconsider two common levels of fairness, i.e., \\emph{client fairness} --\nuniformity of performance across clients, and \\emph{client data fairness} --\nuniformity of performance across different classes of data within a client.\n\\texttt{FedTilt} is inspired by the recently proposed tilted empirical risk\nminimization, which introduces tilt hyperparameters that can be flexibly tuned.\nTheoretically, we show how tuning tilt values can achieve the two-level\nfairness and mitigate the persistent outliers, and derive the convergence\ncondition of \\texttt{FedTilt} as well. Empirically, our evaluation results on a\nsuite of realistic federated datasets in diverse settings show the\neffectiveness and flexibility of the \\texttt{FedTilt} framework and the\nsuperiority to the state-of-the-arts.",
    "published_date": "2025-03-15T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.13537v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.12141v1",
    "title": "Enhanced Sentiment Analysis of Iranian Restaurant Reviews Utilizing Sentiment Intensity Analyzer & Fuzzy Logic",
    "authors": [
      "Shayan Rokhva",
      "Babak Teimourpour",
      "Romina Babaei"
    ],
    "author_ids": [],
    "abstract": "This research presents an advanced sentiment analysis framework studied on\nIranian restaurant reviews, combining fuzzy logic with conventional sentiment\nanalysis techniques to assess both sentiment polarity and intensity. A dataset\nof 1266 reviews, alongside corresponding star ratings, was compiled and\npreprocessed for analysis. Initial sentiment analysis was conducted using the\nSentiment Intensity Analyzer (VADER), a rule-based tool that assigns sentiment\nscores across positive, negative, and neutral categories. However, a noticeable\nbias toward neutrality often led to an inaccurate representation of sentiment\nintensity. To mitigate this issue, based on a fuzzy perspective, two refinement\ntechniques were introduced, applying square-root and fourth-root\ntransformations to amplify positive and negative sentiment scores while\nmaintaining neutrality. This led to three distinct methodologies: Approach 1,\nutilizing unaltered VADER scores; Approach 2, modifying sentiment values using\nthe square root; and Approach 3, applying the fourth root for further\nrefinement. A Fuzzy Inference System incorporating comprehensive fuzzy rules\nwas then developed to process these refined scores and generate a single,\ncontinuous sentiment value for each review based on each approach. Comparative\nanalysis, including human supervision and alignment with customer star ratings,\nrevealed that the refined approaches significantly improved sentiment analysis\nby reducing neutrality bias and better capturing sentiment intensity. Despite\nthese advancements, minor over-amplification and persistent neutrality in\ndomain-specific cases were identified, leading us to propose several future\nstudies to tackle these occasional barriers. The study's methodology and\noutcomes offer valuable insights for businesses seeking a more precise\nunderstanding of consumer sentiment, enhancing sentiment analysis across\nvarious industries.",
    "published_date": "2025-03-15T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.12141v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.11985v1",
    "title": "No LLM is Free From Bias: A Comprehensive Study of Bias Evaluation in Large Language models",
    "authors": [
      "Charaka Vinayak Kumar",
      "Ashok Urlana",
      "Gopichand Kanumolu",
      "Bala Mallikarjunarao Garlapati",
      "Pruthwik Mishra"
    ],
    "author_ids": [],
    "abstract": "Advancements in Large Language Models (LLMs) have increased the performance\nof different natural language understanding as well as generation tasks.\nAlthough LLMs have breached the state-of-the-art performance in various tasks,\nthey often reflect different forms of bias present in the training data. In the\nlight of this perceived limitation, we provide a unified evaluation of\nbenchmarks using a set of representative LLMs that cover different forms of\nbiases starting from physical characteristics to socio-economic categories.\nMoreover, we propose five prompting approaches to carry out the bias detection\ntask across different aspects of bias. Further, we formulate three research\nquestions to gain valuable insight in detecting biases in LLMs using different\napproaches and evaluation metrics across benchmarks. The results indicate that\neach of the selected LLMs suffer from one or the other form of bias with the\nLLaMA3.1-8B model being the least biased. Finally, we conclude the paper with\nthe identification of key challenges and possible future directions.",
    "published_date": "2025-03-15T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.11985v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.11962v1",
    "title": "HInter: Exposing Hidden Intersectional Bias in Large Language Models",
    "authors": [
      "Badr Souani",
      "Ezekiel Soremekun",
      "Mike Papadakis",
      "Setsuko Yokoyama",
      "Sudipta Chattopadhyay",
      "Yves Le Traon"
    ],
    "author_ids": [],
    "abstract": "Large Language Models (LLMs) may portray discrimination towards certain\nindividuals, especially those characterized by multiple attributes (aka\nintersectional bias). Discovering intersectional bias in LLMs is challenging,\nas it involves complex inputs on multiple attributes (e.g. race and gender). To\naddress this challenge, we propose HInter, a test technique that\nsynergistically combines mutation analysis, dependency parsing and metamorphic\noracles to automatically detect intersectional bias in LLMs. HInter generates\ntest inputs by systematically mutating sentences using multiple mutations,\nvalidates inputs via a dependency invariant and detects biases by checking the\nLLM response on the original and mutated sentences. We evaluate HInter using\nsix LLM architectures and 18 LLM models (GPT3.5, Llama2, BERT, etc) and find\nthat 14.61% of the inputs generated by HInter expose intersectional bias.\nResults also show that our dependency invariant reduces false positives\n(incorrect test inputs) by an order of magnitude. Finally, we observed that\n16.62% of intersectional bias errors are hidden, meaning that their\ncorresponding atomic cases do not trigger biases. Overall, this work emphasize\nthe importance of testing LLMs for intersectional bias.",
    "published_date": "2025-03-15T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "68T50, 68T05"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.11962v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.11950v2",
    "title": "Privacy Ethics Alignment in AI: A Stakeholder-Centric Based Framework for Ethical AI",
    "authors": [
      "Ankur Barthwal",
      "Molly Campbell",
      "Ajay Kumar Shrestha"
    ],
    "author_ids": [],
    "abstract": "The increasing integration of Artificial Intelligence (AI) in digital\necosystems has reshaped privacy dynamics, particularly for young digital\ncitizens navigating data-driven environments. This study explores evolving\nprivacy concerns across three key stakeholder groups, digital citizens (ages\n16-19), parents/educators, and AI professionals, and assesses differences in\ndata ownership, trust, transparency, parental mediation, education, and\nrisk-benefit perceptions. Employing a grounded theory methodology, this\nresearch synthesizes insights from 482 participants through structured surveys,\nqualitative interviews, and focus groups. The findings reveal distinct privacy\nexpectations: Young users emphasize autonomy and digital freedom, while parents\nand educators advocate for regulatory oversight and AI literacy programs. AI\nprofessionals, in contrast, prioritize the balance between ethical system\ndesign and technological efficiency. The data further highlights gaps in AI\nliteracy and transparency, emphasizing the need for comprehensive,\nstakeholder-driven privacy frameworks that accommodate diverse user needs.\nUsing comparative thematic analysis, this study identifies key tensions in\nprivacy governance and develops the novel Privacy-Ethics Alignment in AI\n(PEA-AI) model, which structures privacy decision-making as a dynamic\nnegotiation between stakeholders. By systematically analyzing themes such as\ntransparency, user control, risk perception, and parental mediation, this\nresearch provides a scalable, adaptive foundation for AI governance, ensuring\nthat privacy protections evolve alongside emerging AI technologies and\nyouth-centric digital interactions.",
    "published_date": "2025-03-15T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.11950v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.11947v1",
    "title": "Ethical AI for Young Digital Citizens: A Call to Action on Privacy Governance",
    "authors": [
      "Austin Shouli",
      "Ankur Barthwal",
      "Molly Campbell",
      "Ajay Kumar Shrestha"
    ],
    "author_ids": [],
    "abstract": "The rapid expansion of Artificial Intelligence (AI) in digital platforms used\nby youth has created significant challenges related to privacy, autonomy, and\ndata protection. While AI-driven personalization offers enhanced user\nexperiences, it often operates without clear ethical boundaries, leaving young\nusers vulnerable to data exploitation and algorithmic biases. This paper\npresents a call to action for ethical AI governance, advocating for a\nstructured framework that ensures youth-centred privacy protections,\ntransparent data practices, and regulatory oversight. We outline key areas\nrequiring urgent intervention, including algorithmic transparency, privacy\neducation, parental data-sharing ethics, and accountability measures. Through\nthis approach, we seek to empower youth with greater control over their digital\nidentities and propose actionable strategies for policymakers, AI developers,\nand educators to build a fairer and more accountable AI ecosystem.",
    "published_date": "2025-03-15T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.11947v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.11930v1",
    "title": "Generating a Biometrically Unique and Realistic Iris Database",
    "authors": [
      "Jingxuan Zhang",
      "Robert J. Hart",
      "Ziqian Bi",
      "Shiaofen Fang",
      "Susan Walsh"
    ],
    "author_ids": [],
    "abstract": "The use of the iris as a biometric identifier has increased dramatically over\nthe last 30 years, prompting privacy and security concerns about the use of\niris images in research. It can be difficult to acquire iris image databases\ndue to ethical concerns, and this can be a barrier for those performing\nbiometrics research. In this paper, we describe and show how to create a\ndatabase of realistic, biometrically unidentifiable colored iris images by\ntraining a diffusion model within an open-source diffusion framework. Not only\nwere we able to verify that our model is capable of creating iris textures that\nare biometrically unique from the training data, but we were also able to\nverify that our model output creates a full distribution of realistic iris\npigmentations. We highlight the fact that the utility of diffusion networks to\nachieve these criteria with relative ease, warrants additional research in its\nuse within the context of iris database generation and presentation attack\nsecurity.",
    "published_date": "2025-03-15T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.11930v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.14522v1",
    "title": "Accessibility Considerations in the Development of an AI Action Plan",
    "authors": [
      "Jennifer Mankoff",
      "Janice Light",
      "James Coughlan",
      "Christian Vogler",
      "Abraham Glasser",
      "Gregg Vanderheiden",
      "Laura Rice"
    ],
    "author_ids": [],
    "abstract": "We argue that there is a need for Accessibility to be represented in several\nimportant domains:\n  - Capitalize on the new capabilities AI provides - Support for open source\ndevelopment of AI, which can allow disabled and disability focused\nprofessionals to contribute, including\n  - Development of Accessibility Apps which help realise the promise of AI in\naccessibility domains\n  - Open Source Model Development and Validation to ensure that accessibility\nconcerns are addressed in these algorithms\n  - Data Augmentation to include accessibility in data sets used to train\nmodels\n  - Accessible Interfaces that allow disabled people to use any AI app, and to\nvalidate its outputs\n  - Dedicated Functionality and Libraries that can make it easy to integrate AI\nsupport into a variety of settings and apps. - Data security and privacy and\nprivacy risks including data collected by AI based accessibility technologies;\nand the possibility of disability disclosure. - Disability-specific AI risks\nand biases including both direct bias (during AI use by the disabled person)\nand indirect bias (when AI is used by someone else on data relating to a\ndisabled person).",
    "published_date": "2025-03-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.14522v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.11827v1",
    "title": "Bridging the LLM Accessibility Divide? Performance, Fairness, and Cost of Closed versus Open LLMs for Automated Essay Scoring",
    "authors": [
      "Kezia Oketch",
      "John P. Lalor",
      "Yi Yang",
      "Ahmed Abbasi"
    ],
    "author_ids": [],
    "abstract": "Closed large language models (LLMs) such as GPT-4 have set state-of-the-art\nresults across a number of NLP tasks and have become central to NLP and machine\nlearning (ML)-driven solutions. Closed LLMs' performance and wide adoption has\nsparked considerable debate about their accessibility in terms of availability,\ncost, and transparency. In this study, we perform a rigorous comparative\nanalysis of nine leading LLMs, spanning closed, open, and open-source LLM\necosystems, across text assessment and generation tasks related to automated\nessay scoring. Our findings reveal that for few-shot learning-based assessment\nof human generated essays, open LLMs such as Llama 3 and Qwen2.5 perform\ncomparably to GPT-4 in terms of predictive performance, with no significant\ndifferences in disparate impact scores when considering age- or race-related\nfairness. Moreover, Llama 3 offers a substantial cost advantage, being up to 37\ntimes more cost-efficient than GPT-4. For generative tasks, we find that essays\ngenerated by top open LLMs are comparable to closed LLMs in terms of their\nsemantic composition/embeddings and ML assessed scores. Our findings challenge\nthe dominance of closed LLMs and highlight the democratizing potential of open\nLLMs, suggesting they can effectively bridge accessibility divides while\nmaintaining competitive performance and fairness.",
    "published_date": "2025-03-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.11827v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.11614v1",
    "title": "Neutralizing Bias in LLM Reasoning using Entailment Graphs",
    "authors": [
      "Liang Cheng",
      "Tianyi Li",
      "Zhaowei Wang",
      "Tianyang Liu",
      "Mark Steedman"
    ],
    "author_ids": [],
    "abstract": "LLMs are often claimed to be capable of Natural Language Inference (NLI),\nwhich is widely regarded as a cornerstone of more complex forms of reasoning.\nHowever, recent works show that LLMs still suffer from hallucinations in NLI\ndue to attestation bias, where LLMs overly rely on propositional memory to\nbuild shortcuts. To solve the issue, we design an unsupervised framework to\nconstruct counterfactual reasoning data and fine-tune LLMs to reduce\nattestation bias. To measure bias reduction, we build bias-adversarial variants\nof NLI datasets with randomly replaced predicates in premises while keeping\nhypotheses unchanged. Extensive evaluations show that our framework can\nsignificantly reduce hallucinations from attestation bias. Then, we further\nevaluate LLMs fine-tuned with our framework on original NLI datasets and their\nbias-neutralized versions, where original entities are replaced with randomly\nsampled ones. Extensive results show that our framework consistently improves\ninferential performance on both original and bias-neutralized NLI datasets.",
    "published_date": "2025-03-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.11614v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.11575v1",
    "title": "Finding a Fair Scoring Function for Top-$k$ Selection: Hardness, Algorithms, and Experiments",
    "authors": [
      "Guangya Cai"
    ],
    "author_ids": [],
    "abstract": "Selecting a subset of the $k$ \"best\" items from a dataset of $n$ items, based\non a scoring function, is a key task in decision-making. Given the widespread\nuse of automated decision-making software nowadays, it is important that the\noutcome of this process, called top-$k$ selection, is fair. Here we consider\nthe problem of identifying a linear scoring function for top-$k$ selection that\nis fair. The function computes a score for each item as a weighted sum of its\n(numerical) attribute values. Additionally, the function must ensure that the\nsubset selected is a faithful representative of the entire dataset for a\nminority or historically disadvantaged group. Existing algorithms do not scale\neffectively on large, high-dimensional datasets. Our theoretical analysis shows\nthat in more than two dimensions, no algorithm is likely to achieve good\nscalability with respect to dataset size (i.e., a run time of $O(n\\cdot\n\\text{polylog}(n))$), and the computational complexity is likely to increase\nrapidly with dimensionality. However, there are exceptions for small values of\n$k$ and for this case we provide significantly faster algorithms. We also\nprovide efficient practical variants of these algorithms. Our implementations\nof these take advantage of modern hardware (e.g., exploiting parallelism). For\nlarge values of $k$, we give an alternative algorithm that, while theoretically\nworse, performs better in practice. Experimental results on real-world datasets\ndemonstrate the efficiency of our proposed algorithms, which achieve speed-ups\nof up to several orders of magnitude compared to the state of the art (SoTA).",
    "published_date": "2025-03-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.DB",
      "cs.CC",
      "cs.CY",
      "cs.DC",
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.11575v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.11572v1",
    "title": "Implicit Bias-Like Patterns in Reasoning Models",
    "authors": [
      "Messi H. J. Lee",
      "Calvin K. Lai"
    ],
    "author_ids": [],
    "abstract": "Implicit bias refers to automatic or spontaneous mental processes that shape\nperceptions, judgments, and behaviors. Previous research examining `implicit\nbias' in large language models (LLMs) has often approached the phenomenon\ndifferently than how it is studied in humans by focusing primarily on model\noutputs rather than on model processing. To examine model processing, we\npresent a method called the Reasoning Model Implicit Association Test (RM-IAT)\nfor studying implicit bias-like patterns in reasoning models: LLMs that employ\nstep-by-step reasoning to solve complex tasks. Using this method, we find that\nreasoning models require more tokens when processing association-incompatible\ninformation compared to association-compatible information. These findings\nsuggest AI systems harbor patterns in processing information that are analogous\nto human implicit bias. We consider the implications of these implicit\nbias-like patterns for their deployment in real-world applications.",
    "published_date": "2025-03-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.11572v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.16508v1",
    "title": "Conversational AI as a Coding Assistant: Understanding Programmers' Interactions with and Expectations from Large Language Models for Coding",
    "authors": [
      "Mehmet Akhoroz",
      "Caglar Yildirim"
    ],
    "author_ids": [],
    "abstract": "Conversational AI interfaces powered by large language models (LLMs) are\nincreasingly used as coding assistants. However, questions remain about how\nprogrammers interact with LLM-based conversational agents, the challenges they\nencounter, and the factors influencing adoption. This study investigates\nprogrammers' usage patterns, perceptions, and interaction strategies when\nengaging with LLM-driven coding assistants. Through a survey, participants\nreported both the benefits, such as efficiency and clarity of explanations, and\nthe limitations, including inaccuracies, lack of contextual awareness, and\nconcerns about over-reliance. Notably, some programmers actively avoid LLMs due\nto a preference for independent learning, distrust in AI-generated code, and\nethical considerations. Based on our findings, we propose design guidelines for\nimproving conversational coding assistants, emphasizing context retention,\ntransparency, multimodal support, and adaptability to user preferences. These\ninsights contribute to the broader understanding of how LLM-based\nconversational agents can be effectively integrated into software development\nworkflows while addressing adoption barriers and enhancing usability.",
    "published_date": "2025-03-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.16508v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.11352v1",
    "title": "Enhancing Hand Palm Motion Gesture Recognition by Eliminating Reference Frame Bias via Frame-Invariant Similarity Measures",
    "authors": [
      "Arno Verduyn",
      "Maxim Vochten",
      "Joris De Schutter"
    ],
    "author_ids": [],
    "abstract": "The ability of robots to recognize human gestures facilitates a natural and\naccessible human-robot collaboration. However, most work in gesture recognition\nremains rooted in reference frame-dependent representations. This poses a\nchallenge when reference frames vary due to different work cell layouts,\nimprecise frame calibrations, or other environmental changes. This paper\ninvestigated the use of invariant trajectory descriptors for robust hand palm\nmotion gesture recognition under reference frame changes. First, a novel\ndataset of recorded Hand Palm Motion (HPM) gestures is introduced. The motion\ngestures in this dataset were specifically designed to be distinguishable\nwithout dependence on specific reference frames or directional cues.\nAfterwards, multiple invariant trajectory descriptor approaches were\nbenchmarked to assess how their performances generalize to this novel HPM\ndataset. After this offline benchmarking, the best scoring approach is\nvalidated for online recognition by developing a real-time Proof of Concept\n(PoC). In this PoC, hand palm motion gestures were used to control the\nreal-time movement of a manipulator arm. The PoC demonstrated a high\nrecognition reliability in real-time operation, achieving an $F_1$-score of\n92.3%. This work demonstrates the effectiveness of the invariant descriptor\napproach as a standalone solution. Moreover, we believe that the invariant\ndescriptor approach can also be utilized within other state-of-the-art pattern\nrecognition and learning systems to improve their robustness against reference\nframe variations.",
    "published_date": "2025-03-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.HC",
      "53Z30, 70B10, 53A55",
      "I.5.m"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.11352v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.14519v1",
    "title": "Content ARCs: Decentralized Content Rights in the Age of Generative AI",
    "authors": [
      "Kar Balan",
      "Andrew Gilbert",
      "John Collomosse"
    ],
    "author_ids": [],
    "abstract": "The rise of Generative AI (GenAI) has sparked significant debate over\nbalancing the interests of creative rightsholders and AI developers. As GenAI\nmodels are trained on vast datasets that often include copyrighted material,\nquestions around fair compensation and proper attribution have become\nincreasingly urgent. To address these challenges, this paper proposes a\nframework called \\emph{Content ARCs} (Authenticity, Rights, Compensation). By\ncombining open standards for provenance and dynamic licensing with data\nattribution, and decentralized technologies, Content ARCs create a mechanism\nfor managing rights and compensating creators for using their work in AI\ntraining. We characterize several nascent works in the AI data licensing space\nwithin Content ARCs and identify where challenges remain to fully implement the\nend-to-end framework.",
    "published_date": "2025-03-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.DL",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.14519v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.11733v1",
    "title": "LLM Agents for Education: Advances and Applications",
    "authors": [
      "Zhendong Chu",
      "Shen Wang",
      "Jian Xie",
      "Tinghui Zhu",
      "Yibo Yan",
      "Jinheng Ye",
      "Aoxiao Zhong",
      "Xuming Hu",
      "Jing Liang",
      "Philip S. Yu",
      "Qingsong Wen"
    ],
    "author_ids": [],
    "abstract": "Large Language Model (LLM) agents have demonstrated remarkable capabilities\nin automating tasks and driving innovation across diverse educational\napplications. In this survey, we provide a systematic review of\nstate-of-the-art research on LLM agents in education, categorizing them into\ntwo broad classes: (1) \\emph{Pedagogical Agents}, which focus on automating\ncomplex pedagogical tasks to support both teachers and students; and (2)\n\\emph{Domain-Specific Educational Agents}, which are tailored for specialized\nfields such as science education, language learning, and professional\ndevelopment. We comprehensively examine the technological advancements\nunderlying these LLM agents, including key datasets, benchmarks, and\nalgorithmic frameworks that drive their effectiveness. Furthermore, we discuss\ncritical challenges such as privacy, bias and fairness concerns, hallucination\nmitigation, and integration with existing educational ecosystems. This survey\naims to provide a comprehensive technological overview of LLM agents for\neducation, fostering further research and collaboration to enhance their impact\nfor the greater good of learners and educators alike.",
    "published_date": "2025-03-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.11733v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.11247v1",
    "title": "Breaking Shallow Limits: Task-Driven Pixel Fusion for Gap-free RGBT Tracking",
    "authors": [
      "Andong Lu",
      "Yuanzhi Guo",
      "Wanyu Wang",
      "Chenglong Li",
      "Jin Tang",
      "Bin Luo"
    ],
    "author_ids": [],
    "abstract": "Current RGBT tracking methods often overlook the impact of fusion location on\nmitigating modality gap, which is key factor to effective tracking. Our\nanalysis reveals that shallower fusion yields smaller distribution gap.\nHowever, the limited discriminative power of shallow networks hard to\ndistinguish task-relevant information from noise, limiting the potential of\npixel-level fusion. To break shallow limits, we propose a novel\n\\textbf{T}ask-driven \\textbf{P}ixel-level \\textbf{F}usion network, named\n\\textbf{TPF}, which unveils the power of pixel-level fusion in RGBT tracking\nthrough a progressive learning framework. In particular, we design a\nlightweight Pixel-level Fusion Adapter (PFA) that exploits Mamba's linear\ncomplexity to ensure real-time, low-latency RGBT tracking. To enhance the\nfusion capabilities of the PFA, our task-driven progressive learning framework\nfirst utilizes adaptive multi-expert distillation to inherits fusion knowledge\nfrom state-of-the-art image fusion models, establishing robust initialization,\nand then employs a decoupled representation learning scheme to achieve\ntask-relevant information fusion. Moreover, to overcome appearance variations\nbetween the initial template and search frames, we presents a nearest-neighbor\ndynamic template updating scheme, which selects the most reliable frame closest\nto the current search frame as the dynamic template. Extensive experiments\ndemonstrate that TPF significantly outperforms existing most of advanced\ntrackers on four public RGBT tracking datasets. The code will be released upon\nacceptance.",
    "published_date": "2025-03-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.11247v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.11205v1",
    "title": "LLaVA-MLB: Mitigating and Leveraging Attention Bias for Training-Free Video LLMs",
    "authors": [
      "Leqi Shen",
      "Tao He",
      "Guoqiang Gong",
      "Fan Yang",
      "Yifeng Zhang",
      "Pengzhang Liu",
      "Sicheng Zhao",
      "Guiguang Ding"
    ],
    "author_ids": [],
    "abstract": "Training-free video large language models (LLMs) leverage pretrained Image\nLLMs to process video content without the need for further training. A key\nchallenge in such approaches is the difficulty of retaining essential visual\nand temporal information, constrained by the token limits in Image LLMs. To\naddress this, we propose a two-stage method for selecting query-relevant tokens\nbased on the LLM attention scores: compressing the video sequence and then\nexpanding the sequence. However, during the compression stage, Image LLMs often\nexhibit a positional attention bias in video sequences, where attention is\noverly concentrated on later frames, causing early-frame information to be\nunderutilized. To alleviate this attention bias during sequence compression, we\npropose Gridded Attention Pooling for preserving spatiotemporal structure.\nAdditionally, we introduce Visual Summarization Tail to effectively utilize\nthis bias, facilitating overall video understanding during sequence expansion.\nIn this way, our method effectively Mitigates and Leverages attention Bias\n(LLaVA-MLB), enabling the frozen Image LLM for detailed video understanding.\nExperiments on several benchmarks demonstrate that our approach outperforms\nstate-of-the-art methods, achieving superior performance in both efficiency and\naccuracy. Our code will be released.",
    "published_date": "2025-03-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.11205v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.11120v1",
    "title": "A Multi-Objective Evaluation Framework for Analyzing Utility-Fairness Trade-Offs in Machine Learning Systems",
    "authors": [
      "Gökhan Özbulak",
      "Oscar Jimenez-del-Toro",
      "Maíra Fatoretto",
      "Lilian Berton",
      "André Anjos"
    ],
    "author_ids": [],
    "abstract": "The evaluation of fairness models in Machine Learning involves complex\nchallenges, such as defining appropriate metrics, balancing trade-offs between\nutility and fairness, and there are still gaps in this stage. This work\npresents a novel multi-objective evaluation framework that enables the analysis\nof utility-fairness trade-offs in Machine Learning systems. The framework was\ndeveloped using criteria from Multi-Objective Optimization that collect\ncomprehensive information regarding this complex evaluation task. The\nassessment of multiple Machine Learning systems is summarized, both\nquantitatively and qualitatively, in a straightforward manner through a radar\nchart and a measurement table encompassing various aspects such as convergence,\nsystem capacity, and diversity. The framework's compact representation of\nperformance facilitates the comparative analysis of different Machine Learning\nstrategies for decision-makers, in real-world applications, with single or\nmultiple fairness requirements. The framework is model-agnostic and flexible to\nbe adapted to any kind of Machine Learning systems, that is, black- or\nwhite-box, any kind and quantity of evaluation metrics, including\nmultidimensional fairness criteria. The functionality and effectiveness of the\nproposed framework is shown with different simulations, and an empirical study\nconducted on a real-world dataset with various Machine Learning systems.",
    "published_date": "2025-03-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.11120v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.11084v1",
    "title": "Semantic and Contextual Modeling for Malicious Comment Detection with BERT-BiLSTM",
    "authors": [
      "Zhou Fang",
      "Hanlu Zhang",
      "Jacky He",
      "Zhen Qi",
      "Hongye Zheng"
    ],
    "author_ids": [],
    "abstract": "This study aims to develop an efficient and accurate model for detecting\nmalicious comments, addressing the increasingly severe issue of false and\nharmful content on social media platforms. We propose a deep learning model\nthat combines BERT and BiLSTM. The BERT model, through pre-training, captures\ndeep semantic features of text, while the BiLSTM network excels at processing\nsequential data and can further model the contextual dependencies of text.\nExperimental results on the Jigsaw Unintended Bias in Toxicity Classification\ndataset demonstrate that the BERT+BiLSTM model achieves superior performance in\nmalicious comment detection tasks, with a precision of 0.94, recall of 0.93,\nand accuracy of 0.94. This surpasses other models, including standalone BERT,\nTextCNN, TextRNN, and traditional machine learning algorithms using TF-IDF\nfeatures. These results confirm the superiority of the BERT+BiLSTM model in\nhandling imbalanced data and capturing deep semantic features of malicious\ncomments, providing an effective technical means for social media content\nmoderation and online environment purification.",
    "published_date": "2025-03-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.11084v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.10990v1",
    "title": "Statistical Impossibility and Possibility of Aligning LLMs with Human Preferences: From Condorcet Paradox to Nash Equilibrium",
    "authors": [
      "Kaizhao Liu",
      "Qi Long",
      "Zhekun Shi",
      "Weijie J. Su",
      "Jiancong Xiao"
    ],
    "author_ids": [],
    "abstract": "Aligning large language models (LLMs) with diverse human preferences is\ncritical for ensuring fairness and informed outcomes when deploying these\nmodels for decision-making. In this paper, we seek to uncover fundamental\nstatistical limits concerning aligning LLMs with human preferences, with a\nfocus on the probabilistic representation of human preferences and the\npreservation of diverse preferences in aligned LLMs. We first show that human\npreferences can be represented by a reward model if and only if the preference\namong LLM-generated responses is free of any Condorcet cycle. Moreover, we\nprove that Condorcet cycles exist with probability converging to one\nexponentially fast under a probabilistic preference model, thereby\ndemonstrating the impossibility of fully aligning human preferences using\nreward-based approaches such as reinforcement learning from human feedback.\nNext, we explore the conditions under which LLMs would employ mixed strategies\n-- meaning they do not collapse to a single response -- when aligned in the\nlimit using a non-reward-based approach, such as Nash learning from human\nfeedback (NLHF). We identify a necessary and sufficient condition for mixed\nstrategies: the absence of a response that is preferred over all others by a\nmajority. As a blessing, we prove that this condition holds with high\nprobability under the probabilistic preference model, thereby highlighting the\nstatistical possibility of preserving minority preferences without explicit\nregularization in aligning LLMs. Finally, we leverage insights from our\nstatistical results to design a novel, computationally efficient algorithm for\nfinding Nash equilibria in aligning LLMs with NLHF. Our experiments show that\nLlama-3.2-1B, aligned with our algorithm, achieves a win rate of 60.55\\%\nagainst the base model.",
    "published_date": "2025-03-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.GT",
      "cs.LG",
      "econ.TH",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.10990v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.13509v1",
    "title": "MentalChat16K: A Benchmark Dataset for Conversational Mental Health Assistance",
    "authors": [
      "Jia Xu",
      "Tianyi Wei",
      "Bojian Hou",
      "Patryk Orzechowski",
      "Shu Yang",
      "Ruochen Jin",
      "Rachael Paulbeck",
      "Joost Wagenaar",
      "George Demiris",
      "Li Shen"
    ],
    "author_ids": [],
    "abstract": "We introduce MentalChat16K, an English benchmark dataset combining a\nsynthetic mental health counseling dataset and a dataset of anonymized\ntranscripts from interventions between Behavioral Health Coaches and Caregivers\nof patients in palliative or hospice care. Covering a diverse range of\nconditions like depression, anxiety, and grief, this curated dataset is\ndesigned to facilitate the development and evaluation of large language models\nfor conversational mental health assistance. By providing a high-quality\nresource tailored to this critical domain, MentalChat16K aims to advance\nresearch on empathetic, personalized AI solutions to improve access to mental\nhealth support services. The dataset prioritizes patient privacy, ethical\nconsiderations, and responsible data usage. MentalChat16K presents a valuable\nopportunity for the research community to innovate AI technologies that can\npositively impact mental well-being.",
    "published_date": "2025-03-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.13509v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.10838v2",
    "title": "Who Relies More on World Knowledge and Bias for Syntactic Ambiguity Resolution: Humans or LLMs?",
    "authors": [
      "So Young Lee",
      "Russell Scheinberg",
      "Amber Shore",
      "Ameeta Agrawal"
    ],
    "author_ids": [],
    "abstract": "This study explores how recent large language models (LLMs) navigate relative\nclause attachment {ambiguity} and use world knowledge biases for disambiguation\nin six typologically diverse languages: English, Chinese, Japanese, Korean,\nRussian, and Spanish. We describe the process of creating a novel dataset --\nMultiWho -- for fine-grained evaluation of relative clause attachment\npreferences in ambiguous and unambiguous contexts. Our experiments with three\nLLMs indicate that, contrary to humans, LLMs consistently exhibit a preference\nfor local attachment, displaying limited responsiveness to syntactic variations\nor language-specific attachment patterns. Although LLMs performed well in\nunambiguous cases, they rigidly prioritized world knowledge biases, lacking the\nflexibility of human language processing. These findings highlight the need for\nmore diverse, pragmatically nuanced multilingual training to improve LLMs'\nhandling of complex structures and human-like comprehension.",
    "published_date": "2025-03-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.10838v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.10830v1",
    "title": "Balanced and Fair Partitioning of Friends",
    "authors": [
      "Argyrios Deligkas",
      "Eduard Eiben",
      "Stavros D. Ioannidis",
      "Dušan Knop",
      "Šimon Schierreich"
    ],
    "author_ids": [],
    "abstract": "In the recently introduced model of fair partitioning of friends, there is a\nset of agents located on the vertices of an underlying graph that indicates the\nfriendships between the agents. The task is to partition the graph into $k$\nbalanced-sized groups, keeping in mind that the value of an agent for a group\nequals the number of edges they have in that group. The goal is to construct\npartitions that are \"fair\", i.e., no agent would like to replace an agent in a\ndifferent group. We generalize the standard model by considering utilities for\nthe agents that are beyond binary and additive. Having this as our foundation,\nour contribution is threefold (a) we adapt several fairness notions that have\nbeen developed in the fair division literature to our setting; (b) we give\nseveral existence guarantees supported by polynomial-time algorithms; (c) we\ninitiate the study of the computational (and parameterized) complexity of the\nmodel and provide an almost complete landscape of the (in)tractability frontier\nfor our fairness concepts.",
    "published_date": "2025-03-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.10830v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.19925v1",
    "title": "Accurate, provable, and fast nonlinear tomographic reconstruction: A variational inequality approach",
    "authors": [
      "Mengqi Lou",
      "Kabir Aladin Verchand",
      "Sara Fridovich-Keil",
      "Ashwin Pananjady"
    ],
    "author_ids": [],
    "abstract": "We consider the problem of signal reconstruction for computed tomography (CT)\nunder a nonlinear forward model that accounts for exponential signal\nattenuation, a polychromatic X-ray source, general measurement noise (e.g.\nPoisson shot noise), and observations acquired over multiple wavelength\nwindows. We develop a simple iterative algorithm for single-material\nreconstruction, which we call EXACT (EXtragradient Algorithm for Computed\nTomography), based on formulating our estimate as the fixed point of a monotone\nvariational inequality. We prove guarantees on the statistical and\ncomputational performance of EXACT under practical assumptions on the\nmeasurement process. We also consider a recently introduced variant of this\nmodel with Gaussian measurements, and present sample and iteration complexity\nbounds for EXACT that improve upon those of existing algorithms. We apply our\nEXACT algorithm to a CT phantom image recovery task and show that it often\nrequires fewer X-ray projection exposures, lower source intensity, and less\ncomputation time to achieve similar reconstruction quality to existing methods.",
    "published_date": "2025-03-13T00:00:00",
    "year": 2025,
    "categories": [
      "eess.IV",
      "cs.LG",
      "math.OC",
      "physics.med-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.19925v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.10773v1",
    "title": "Learn then Decide: A Learning Approach for Designing Data Marketplaces",
    "authors": [
      "Yingqi Gao",
      "Jin Zhou",
      "Hua Zhou",
      "Yong Chen",
      "Xiaowu Dai"
    ],
    "author_ids": [],
    "abstract": "As data marketplaces become increasingly central to the digital economy, it\nis crucial to design efficient pricing mechanisms that optimize revenue while\nensuring fair and adaptive pricing. We introduce the Maximum Auction-to-Posted\nPrice (MAPP) mechanism, a novel two-stage approach that first estimates the\nbidders' value distribution through auctions and then determines the optimal\nposted price based on the learned distribution. We establish that MAPP is\nindividually rational and incentive-compatible, ensuring truthful bidding while\nbalancing revenue maximization with minimal price discrimination. MAPP achieves\na regret of $O_p(n^{-1})$ when incorporating historical bid data, where $n$ is\nthe number of bids in the current round. It outperforms existing methods while\nimposing weaker distributional assumptions. For sequential dataset sales over\n$T$ rounds, we propose an online MAPP mechanism that dynamically adjusts\npricing across datasets with varying value distributions. Our approach achieves\nno-regret learning, with the average cumulative regret converging at a rate of\n$O_p(T^{-1/2}(\\log T)^2)$. We validate the effectiveness of MAPP through\nsimulations and real-world data from the FCC AWS-3 spectrum auction.",
    "published_date": "2025-03-13T00:00:00",
    "year": 2025,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.10773v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.10587v1",
    "title": "The Spectral Bias of Shallow Neural Network Learning is Shaped by the Choice of Non-linearity",
    "authors": [
      "Justin Sahs",
      "Ryan Pyle",
      "Fabio Anselmi",
      "Ankit Patel"
    ],
    "author_ids": [],
    "abstract": "Despite classical statistical theory predicting severe overfitting, modern\nmassively overparameterized neural networks still generalize well. This\nunexpected property is attributed to the network's so-called implicit bias,\nwhich describes its propensity to converge to solutions that generalize\neffectively, among the many possible that correctly label the training data.\nThe aim of our research is to explore this bias from a new perspective,\nfocusing on how non-linear activation functions contribute to shaping it.\nFirst, we introduce a reparameterization which removes a continuous weight\nrescaling symmetry. Second, in the kernel regime, we leverage this\nreparameterization to generalize recent findings that relate shallow Neural\nNetworks to the Radon transform, deriving an explicit formula for the implicit\nbias induced by a broad class of activation functions. Specifically, by\nutilizing the connection between the Radon transform and the Fourier transform,\nwe interpret the kernel regime's inductive bias as minimizing a spectral\nseminorm that penalizes high-frequency components, in a manner dependent on the\nactivation function. Finally, in the adaptive regime, we demonstrate the\nexistence of local dynamical attractors that facilitate the formation of\nclusters of hyperplanes where the input to a neuron's activation function is\nzero, yielding alignment between many neurons' response functions. We confirm\nthese theoretical results with simulations. All together, our work provides a\ndeeper understanding of the mechanisms underlying the generalization\ncapabilities of overparameterized neural networks and its relation with the\nimplicit bias, offering potential pathways for designing more efficient and\nrobust models.",
    "published_date": "2025-03-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.10587v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.10567v1",
    "title": "FedPCA: Noise-Robust Fair Federated Learning via Performance-Capacity Analysis",
    "authors": [
      "Nannan Wu",
      "Zengqiang Yan",
      "Nong Sang",
      "Li Yu",
      "Chang Wen Chen"
    ],
    "author_ids": [],
    "abstract": "Training a model that effectively handles both common and rare data-i.e.,\nachieving performance fairness-is crucial in federated learning (FL). While\nexisting fair FL methods have shown effectiveness, they remain vulnerable to\nmislabeled data. Ensuring robustness in fair FL is therefore essential.\nHowever, fairness and robustness inherently compete, which causes robust\nstrategies to hinder fairness. In this paper, we attribute this competition to\nthe homogeneity in loss patterns exhibited by rare and mislabeled data clients,\npreventing existing loss-based fair and robust FL methods from effectively\ndistinguishing and handling these two distinct client types. To address this,\nwe propose performance-capacity analysis, which jointly considers model\nperformance on each client and its capacity to handle the dataset, measured by\nloss and a newly introduced feature dispersion score. This allows mislabeled\nclients to be identified by their significantly deviated performance relative\nto capacity while preserving rare data clients. Building on this, we introduce\nFedPCA, an FL method that robustly achieves fairness. FedPCA first identifies\nmislabeled clients via a Gaussian Mixture Model on loss-dispersion pairs, then\napplies fairness and robustness strategies in global aggregation and local\ntraining by adjusting client weights and selectively using reliable data.\nExtensive experiments on three datasets demonstrate FedPCA's effectiveness in\ntackling this complex challenge. Code will be publicly available upon\nacceptance.",
    "published_date": "2025-03-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.10567v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.10560v1",
    "title": "References to unbiased sources increase the helpfulness of community fact-checks",
    "authors": [
      "Kirill Solovev",
      "Nicolas Pröllochs"
    ],
    "author_ids": [],
    "abstract": "Community-based fact-checking is a promising approach to address\nmisinformation on social media at scale. However, an understanding of what\nmakes community-created fact-checks helpful to users is still in its infancy.\nIn this paper, we analyze the determinants of the helpfulness of\ncommunity-created fact-checks. For this purpose, we draw upon a unique dataset\nof real-world community-created fact-checks and helpfulness ratings from X's\n(formerly Twitter) Community Notes platform. Our empirical analysis implies\nthat the key determinant of helpfulness in community-based fact-checking is\nwhether users provide links to external sources to underpin their assertions.\nOn average, the odds for community-created fact-checks to be perceived as\nhelpful are 2.70 times higher if they provide links to external sources.\nFurthermore, we demonstrate that the helpfulness of community-created\nfact-checks varies depending on their level of political bias. Here, we find\nthat community-created fact-checks linking to high-bias sources (of either\npolitical side) are perceived as significantly less helpful. This suggests that\nthe rating mechanism on the Community Notes platform successfully penalizes\none-sidedness and politically motivated reasoning. These findings have\nimportant implications for social media platforms, which can utilize our\nresults to optimize their community-based fact-checking systems.",
    "published_date": "2025-03-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.10560v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.10533v1",
    "title": "The Impact of Item-Writing Flaws on Difficulty and Discrimination in Item Response Theory",
    "authors": [
      "Robin Schmucker",
      "Steven Moore"
    ],
    "author_ids": [],
    "abstract": "High-quality test items are essential for educational assessments,\nparticularly within Item Response Theory (IRT). Traditional validation methods\nrely on resource-intensive pilot testing to estimate item difficulty and\ndiscrimination. More recently, Item-Writing Flaw (IWF) rubrics emerged as a\ndomain-general approach for evaluating test items based on textual features.\nHowever, their relationship to IRT parameters remains underexplored. To address\nthis gap, we conducted a study involving over 7,000 multiple-choice questions\nacross various STEM subjects (e.g., math and biology). Using an automated\napproach, we annotated each question with a 19-criteria IWF rubric and studied\nrelationships to data-driven IRT parameters. Our analysis revealed\nstatistically significant links between the number of IWFs and IRT difficulty\nand discrimination parameters, particularly in life and physical science\ndomains. We further observed how specific IWF criteria can impact item quality\nmore and less severely (e.g., negative wording vs. implausible distractors).\nOverall, while IWFs are useful for predicting IRT parameters--particularly for\nscreening low-difficulty MCQs--they cannot replace traditional data-driven\nvalidation methods. Our findings highlight the need for further research on\ndomain-general evaluation rubrics and algorithms that understand\ndomain-specific content for robust item validation.",
    "published_date": "2025-03-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.10533v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.10486v1",
    "title": "LLMs in Disease Diagnosis: A Comparative Study of DeepSeek-R1 and O3 Mini Across Chronic Health Conditions",
    "authors": [
      "Gaurav Kumar Gupta",
      "Pranal Pande"
    ],
    "author_ids": [],
    "abstract": "Large Language Models (LLMs) are revolutionizing medical diagnostics by\nenhancing both disease classification and clinical decision-making. In this\nstudy, we evaluate the performance of two LLM- based diagnostic tools, DeepSeek\nR1 and O3 Mini, using a structured dataset of symptoms and diagnoses. We\nassessed their predictive accuracy at both the disease and category levels, as\nwell as the reliability of their confidence scores. DeepSeek R1 achieved a\ndisease-level accuracy of 76% and an overall accuracy of 82%, outperforming O3\nMini, which attained 72% and 75% respectively. Notably, DeepSeek R1\ndemonstrated exceptional performance in Mental Health, Neurological Disorders,\nand Oncology, where it reached 100% accuracy, while O3 Mini excelled in\nAutoimmune Disease classification with 100% accuracy. Both models, however,\nstruggled with Respiratory Disease classification, recording accuracies of only\n40% for DeepSeek R1 and 20% for O3 Mini. Additionally, the analysis of\nconfidence scores revealed that DeepSeek R1 provided high-confidence\npredictions in 92% of cases, compared to 68% for O3 Mini. Ethical\nconsiderations regarding bias, model interpretability, and data privacy are\nalso discussed to ensure the responsible integration of LLMs into clinical\npractice. Overall, our findings offer valuable insights into the strengths and\nlimitations of LLM-based diagnostic systems and provide a roadmap for future\nenhancements in AI-driven healthcare.",
    "published_date": "2025-03-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.10486v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.10439v2",
    "title": "EFC++: Elastic Feature Consolidation with Prototype Re-balancing for Cold Start Exemplar-free Incremental Learning",
    "authors": [
      "Simone Magistri",
      "Tomaso Trinci",
      "Albin Soutif-Cormerais",
      "Joost van de Weijer",
      "Andrew D. Bagdanov"
    ],
    "author_ids": [],
    "abstract": "Exemplar-Free Class Incremental Learning (EFCIL) aims to learn from a\nsequence of tasks without having access to previous task data. In this paper,\nwe consider the challenging Cold Start scenario in which insufficient data is\navailable in the first task to learn a high-quality backbone. This is\nespecially challenging for EFCIL since it requires high plasticity, resulting\nin feature drift which is difficult to compensate for in the exemplar-free\nsetting. To address this problem, we propose an effective approach to\nconsolidate feature representations by regularizing drift in directions highly\nrelevant to previous tasks and employs prototypes to reduce task-recency bias.\nOur approach, which we call Elastic Feature Consolidation++ (EFC++) exploits a\ntractable second-order approximation of feature drift based on a proposed\nEmpirical Feature Matrix (EFM). The EFM induces a pseudo-metric in feature\nspace which we use to regularize feature drift in important directions and to\nupdate Gaussian prototypes. In addition, we introduce a post-training prototype\nre-balancing phase that updates classifiers to compensate for feature drift.\nExperimental results on CIFAR-100, Tiny-ImageNet, ImageNet-Subset, ImageNet-1K\nand DomainNet demonstrate that EFC++ is better able to learn new tasks by\nmaintaining model plasticity and significantly outperform the state-of-the-art.",
    "published_date": "2025-03-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.10439v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.10408v1",
    "title": "Understanding the Logical Capabilities of Large Language Models via Out-of-Context Representation Learning",
    "authors": [
      "Jonathan Shaki",
      "Emanuele La Malfa",
      "Michael Wooldridge",
      "Sarit Kraus"
    ],
    "author_ids": [],
    "abstract": "We study the capabilities of Large Language Models (LLM) on binary relations,\na ubiquitous concept in math employed in most reasoning, math and logic\nbenchmarks. This work focuses on equality, inequality, and inclusion, along\nwith the properties they satisfy, such as ir/reflexivity, a/symmetry,\ntransitivity, and logical complexity (e.g., number of reasoning ``hops''). We\npropose an alternative to in-context learning that trains only the\nrepresentations of newly introduced tokens, namely out-of-context\nrepresentation learning. This method mitigates linguistic biases already\npresent in a model and, differently from in-context learning, does not rely on\nexternal information or illustrations. We argue out-of-context representation\nlearning as a better alternative to in-context learning and fine-tuning to\nevaluate the capabilities of LLMs on logic tasks that are the building blocks\nof more complex reasoning benchmarks.",
    "published_date": "2025-03-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.10408v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.10729v1",
    "title": "Numerical and statistical analysis of NeuralODE with Runge-Kutta time integration",
    "authors": [
      "Emily C. Ehrhardt",
      "Hanno Gottschalk",
      "Tobias J. Riedlinger"
    ],
    "author_ids": [],
    "abstract": "NeuralODE is one example for generative machine learning based on the push\nforward of a simple source measure with a bijective mapping, which in the case\nof NeuralODE is given by the flow of a ordinary differential equation. Using\nLiouville's formula, the log-density of the push forward measure is easy to\ncompute and thus NeuralODE can be trained based on the maximum Likelihood\nmethod such that the Kulback-Leibler divergence between the push forward\nthrough the flow map and the target measure generating the data becomes small.\nIn this work, we give a detailed account on the consistency of Maximum\nLikelihood based empirical risk minimization for a generic class of target\nmeasures. In contrast to prior work, we do not only consider the statistical\nlearning theory, but also give a detailed numerical analysis of the NeuralODE\nalgorithm based on the 2nd order Runge-Kutta (RK) time integration. Using the\nuniversal approximation theory for deep ReQU networks, the stability and\nconvergence rated for the RK scheme as well as metric entropy and concentration\ninequalities, we are able to prove that NeuralODE is a probably approximately\ncorrect (PAC) learning algorithm.",
    "published_date": "2025-03-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.NA",
      "math.CA",
      "math.NA",
      "math.PR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.10729v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.10728v1",
    "title": "DarkBench: Benchmarking Dark Patterns in Large Language Models",
    "authors": [
      "Esben Kran",
      "Hieu Minh \"Jord\" Nguyen",
      "Akash Kundu",
      "Sami Jawhar",
      "Jinsuk Park",
      "Mateusz Maria Jurewicz"
    ],
    "author_ids": [],
    "abstract": "We introduce DarkBench, a comprehensive benchmark for detecting dark design\npatterns--manipulative techniques that influence user behavior--in interactions\nwith large language models (LLMs). Our benchmark comprises 660 prompts across\nsix categories: brand bias, user retention, sycophancy, anthropomorphism,\nharmful generation, and sneaking. We evaluate models from five leading\ncompanies (OpenAI, Anthropic, Meta, Mistral, Google) and find that some LLMs\nare explicitly designed to favor their developers' products and exhibit\nuntruthful communication, among other manipulative behaviors. Companies\ndeveloping LLMs should recognize and mitigate the impact of dark design\npatterns to promote more ethical AI.",
    "published_date": "2025-03-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.10728v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.10242v1",
    "title": "MinorBench: A hand-built benchmark for content-based risks for children",
    "authors": [
      "Shaun Khoo",
      "Gabriel Chua",
      "Rachel Shong"
    ],
    "author_ids": [],
    "abstract": "Large Language Models (LLMs) are rapidly entering children's lives - through\nparent-driven adoption, schools, and peer networks - yet current AI ethics and\nsafety research do not adequately address content-related risks specific to\nminors. In this paper, we highlight these gaps with a real-world case study of\nan LLM-based chatbot deployed in a middle school setting, revealing how\nstudents used and sometimes misused the system. Building on these findings, we\npropose a new taxonomy of content-based risks for minors and introduce\nMinorBench, an open-source benchmark designed to evaluate LLMs on their ability\nto refuse unsafe or inappropriate queries from children. We evaluate six\nprominent LLMs under different system prompts, demonstrating substantial\nvariability in their child-safety compliance. Our results inform practical\nsteps for more robust, child-focused safety mechanisms and underscore the\nurgency of tailoring AI systems to safeguard young users.",
    "published_date": "2025-03-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.10242v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.10118v2",
    "title": "An Real-Sim-Real (RSR) Loop Framework for Generalizable Robotic Policy Transfer with Differentiable Simulation",
    "authors": [
      "Lu Shi",
      "Yuxuan Xu",
      "Shiyu Wang",
      "Jinhao Huang",
      "Wenhao Zhao",
      "Yufei Jia",
      "Zike Yan",
      "Weibin Gu",
      "Guyue Zhou"
    ],
    "author_ids": [],
    "abstract": "The sim-to-real gap remains a critical challenge in robotics, hindering the\ndeployment of algorithms trained in simulation to real-world systems. This\npaper introduces a novel Real-Sim-Real (RSR) loop framework leveraging\ndifferentiable simulation to address this gap by iteratively refining\nsimulation parameters, aligning them with real-world conditions, and enabling\nrobust and efficient policy transfer. A key contribution of our work is the\ndesign of an informative cost function that encourages the collection of\ndiverse and representative real-world data, minimizing bias and maximizing the\nutility of each data point for simulation refinement. This cost function\nintegrates seamlessly into existing reinforcement learning algorithms (e.g.,\nPPO, SAC) and ensures a balanced exploration of critical regions in the real\ndomain. Furthermore, our approach is implemented on the versatile Mujoco MJX\nplatform, and our framework is compatible with a wide range of robotic systems.\nExperimental results on several robotic manipulation tasks demonstrate that our\nmethod significantly reduces the sim-to-real gap, achieving high task\nperformance and generalizability across diverse scenarios of both explicit and\nimplicit environmental uncertainties.",
    "published_date": "2025-03-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.10118v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.10100v1",
    "title": "SOLA-GCL: Subgraph-Oriented Learnable Augmentation Method for Graph Contrastive Learning",
    "authors": [
      "Tianhao Peng",
      "Xuhong Li",
      "Haitao Yuan",
      "Yuchen Li",
      "Haoyi Xiong"
    ],
    "author_ids": [],
    "abstract": "Graph contrastive learning has emerged as a powerful technique for learning\ngraph representations that are robust and discriminative. However, traditional\napproaches often neglect the critical role of subgraph structures, particularly\nthe intra-subgraph characteristics and inter-subgraph relationships, which are\ncrucial for generating informative and diverse contrastive pairs. These\nsubgraph features are crucial as they vary significantly across different graph\ntypes, such as social networks where they represent communities, and\nbiochemical networks where they symbolize molecular interactions. To address\nthis issue, our work proposes a novel subgraph-oriented learnable augmentation\nmethod for graph contrastive learning, termed SOLA-GCL, that centers around\nsubgraphs, taking full advantage of the subgraph information for data\naugmentation. Specifically, SOLA-GCL initially partitions a graph into multiple\ndensely connected subgraphs based on their intrinsic properties. To preserve\nand enhance the unique characteristics inherent to subgraphs, a graph view\ngenerator optimizes augmentation strategies for each subgraph, thereby\ngenerating tailored views for graph contrastive learning. This generator uses a\ncombination of intra-subgraph and inter-subgraph augmentation strategies,\nincluding node dropping, feature masking, intra-edge perturbation, inter-edge\nperturbation, and subgraph swapping. Extensive experiments have been conducted\non various graph learning applications, ranging from social networks to\nmolecules, under semi-supervised learning, unsupervised learning, and transfer\nlearning settings to demonstrate the superiority of our proposed approach over\nthe state-of-the-art in GCL.",
    "published_date": "2025-03-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.10100v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.10065v1",
    "title": "Do We Always Need the Simplicity Bias? Looking for Optimal Inductive Biases in the Wild",
    "authors": [
      "Damien Teney",
      "Liangze Jiang",
      "Florin Gogianu",
      "Ehsan Abbasnejad"
    ],
    "author_ids": [],
    "abstract": "Neural architectures tend to fit their data with relatively simple functions.\nThis \"simplicity bias\" is widely regarded as key to their success. This paper\nexplores the limits of this principle. Building on recent findings that the\nsimplicity bias stems from ReLU activations [96], we introduce a method to\nmeta-learn new activation functions and inductive biases better suited to\nspecific tasks.\n  Findings: We identify multiple tasks where the simplicity bias is inadequate\nand ReLUs suboptimal. In these cases, we learn new activation functions that\nperform better by inducing a prior of higher complexity. Interestingly, these\ncases correspond to domains where neural networks have historically struggled:\ntabular data, regression tasks, cases of shortcut learning, and algorithmic\ngrokking tasks. In comparison, the simplicity bias induced by ReLUs proves\nadequate on image tasks where the best learned activations are nearly identical\nto ReLUs and GeLUs.\n  Implications: Contrary to popular belief, the simplicity bias of ReLU\nnetworks is not universally useful. It is near-optimal for image\nclassification, but other inductive biases are sometimes preferable. We showed\nthat activation functions can control these inductive biases, but future\ntailored architectures might provide further benefits. Advances are still\nneeded to characterize a model's inductive biases beyond \"complexity\", and\ntheir adequacy with the data.",
    "published_date": "2025-03-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.10065v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.09987v1",
    "title": "Beyond Human: Cognitive and Physical Augmentation through AI, Robotics, and XR -- Opportunities and Risks",
    "authors": [
      "Jie Li",
      "Anusha Withana",
      "Alexandra Diening",
      "Kai Kunze",
      "Masahiko Inami"
    ],
    "author_ids": [],
    "abstract": "As human augmentation technologies evolve, the convergence of AI, robotics,\nand extended reality (XR) is redefining human potential -- enhancing cognition,\nperception, and physical abilities. However, these advancements also introduce\nethical dilemmas, security risks, and concerns over loss of control. This\nworkshop explores both the transformative potential and the unintended\nconsequences of augmentation technologies. Bringing together experts from HCI,\nneuroscience, robotics, and ethics, we will examine real-world applications,\nemerging risks, and governance strategies for responsible augmentation. The\nsession will feature keynote talks and interactive discussions, addressing\ntopics such as AI-enhanced cognition, wearable robotics, neural interfaces, and\nXR-driven augmentation. By fostering multidisciplinary dialogue, this workshop\naims to generate actionable insights for responsible innovation, proposing\nethical frameworks to balance human empowerment with risk mitigation. We invite\nresearchers, practitioners, and industry leaders to contribute their\nperspectives and help shape the future of human augmentation.",
    "published_date": "2025-03-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.ET",
      "H.5"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.09987v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.09969v1",
    "title": "Detecting Dataset Bias in Medical AI: A Generalized and Modality-Agnostic Auditing Framework",
    "authors": [
      "Nathan Drenkow",
      "Mitchell Pavlak",
      "Keith Harrigian",
      "Ayah Zirikly",
      "Adarsh Subbaswamy",
      "Mathias Unberath"
    ],
    "author_ids": [],
    "abstract": "Data-driven AI is establishing itself at the center of evidence-based\nmedicine. However, reports of shortcomings and unexpected behavior are growing\ndue to AI's reliance on association-based learning. A major reason for this\nbehavior: latent bias in machine learning datasets can be amplified during\ntraining and/or hidden during testing. We present a data modality-agnostic\nauditing framework for generating targeted hypotheses about sources of bias\nwhich we refer to as Generalized Attribute Utility and Detectability-Induced\nbias Testing (G-AUDIT) for datasets. Our method examines the relationship\nbetween task-level annotations and data properties including protected\nattributes (e.g., race, age, sex) and environment and acquisition\ncharacteristics (e.g., clinical site, imaging protocols). G-AUDIT automatically\nquantifies the extent to which the observed data attributes may enable shortcut\nlearning, or in the case of testing data, hide predictions made based on\nspurious associations. We demonstrate the broad applicability and value of our\nmethod by analyzing large-scale medical datasets for three distinct modalities\nand learning tasks: skin lesion classification in images, stigmatizing language\nclassification in Electronic Health Records (EHR), and mortality prediction for\nICU tabular data. In each setting, G-AUDIT successfully identifies subtle\nbiases commonly overlooked by traditional qualitative methods that focus\nprimarily on social and ethical objectives, underscoring its practical value in\nexposing dataset-level risks and supporting the downstream development of\nreliable AI systems. Our method paves the way for achieving deeper\nunderstanding of machine learning datasets throughout the AI development\nlife-cycle from initial prototyping all the way to regulation, and creates\nopportunities to reduce model bias, enabling safer and more trustworthy AI\nsystems.",
    "published_date": "2025-03-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.09969v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.09947v1",
    "title": "Identifying Trustworthiness Challenges in Deep Learning Models for Continental-Scale Water Quality Prediction",
    "authors": [
      "Xiaobo Xia",
      "Xiaofeng Liu",
      "Jiale Liu",
      "Kuai Fang",
      "Lu Lu",
      "Samet Oymak",
      "William S. Currie",
      "Tongliang Liu"
    ],
    "author_ids": [],
    "abstract": "Water quality is foundational to environmental sustainability, ecosystem\nresilience, and public health. Deep learning models, particularly Long\nShort-Term Memory (LSTM) networks, offer transformative potential for\nlarge-scale water quality prediction and scientific insights generation.\nHowever, their widespread adoption in high-stakes decision-making, such as\npollution mitigation and equitable resource allocation, is prevented by\nunresolved trustworthiness challenges including fairness, uncertainty,\ninterpretability, robustness, generalizability, and reproducibility. In this\nwork, we present the first comprehensive evaluation of trustworthiness in a\ncontinental-scale multi-task LSTM model predicting 20 water quality variables\n(encompassing physical/chemical processes, geochemical weathering, and nutrient\ncycling) across 482 U.S. basins. Our investigation uncovers systematic patterns\nof model performance disparities linked to basin characteristics, the inherent\ncomplexity of biogeochemical processes, and variable predictability,\nemphasizing critical performance fairness concerns. We further propose\nmethodological frameworks for quantitatively evaluating critical aspects of\ntrustworthiness, including uncertainty, interpretability, and robustness,\nidentifying key limitations that could challenge reliable real-world\ndeployment. This work serves as a timely call to action for advancing\ntrustworthy data-driven methods for water resources management and provides a\npathway to offering critical insights for researchers, decision-makers, and\npractitioners seeking to leverage artificial intelligence (AI) responsibly in\nenvironmental management.",
    "published_date": "2025-03-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.09947v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.09925v1",
    "title": "PluralLLM: Pluralistic Alignment in LLMs via Federated Learning",
    "authors": [
      "Mahmoud Srewa",
      "Tianyu Zhao",
      "Salma Elmalaki"
    ],
    "author_ids": [],
    "abstract": "Ensuring Large Language Models (LLMs) align with diverse human preferences\nwhile preserving privacy and fairness remains a challenge. Existing methods,\nsuch as Reinforcement Learning from Human Feedback (RLHF), rely on centralized\ndata collection, making them computationally expensive and privacy-invasive. We\nintroduce PluralLLM a federated learning-based approach that enables multiple\nuser groups to collaboratively train a transformer-based preference predictor\nwithout sharing sensitive data, which can also serve as a reward model for\naligning LLMs. Our method leverages Federated Averaging (FedAvg) to aggregate\npreference updates efficiently, achieving 46% faster convergence, a 4%\nimprovement in alignment scores, and nearly the same group fairness measure as\nin centralized training. Evaluated on a Q/A preference alignment task,\nPluralLLM demonstrates that federated preference learning offers a scalable and\nprivacy-preserving alternative for aligning LLMs with diverse human values.",
    "published_date": "2025-03-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.09925v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.09873v1",
    "title": "FDCT: Frequency-Aware Decomposition and Cross-Modal Token-Alignment for Multi-Sensor Target Classification",
    "authors": [
      "Shoaib Meraj Sami",
      "Md Mahedi Hasan",
      "Nasser M. Nasrabadi",
      "Raghuveer Rao"
    ],
    "author_ids": [],
    "abstract": "In automatic target recognition (ATR) systems, sensors may fail to capture\ndiscriminative, fine-grained detail features due to environmental conditions,\nnoise created by CMOS chips, occlusion, parallaxes, and sensor misalignment.\nTherefore, multi-sensor image fusion is an effective choice to overcome these\nconstraints. However, multi-modal image sensors are heterogeneous and have\ndomain and granularity gaps. In addition, the multi-sensor images can be\nmisaligned due to intricate background clutters, fluctuating illumination\nconditions, and uncontrolled sensor settings. In this paper, to overcome these\nissues, we decompose, align, and fuse multiple image sensor data for target\nclassification. We extract the domain-specific and domain-invariant features\nfrom each sensor data. We propose to develop a shared unified discrete token\n(UDT) space between sensors to reduce the domain and granularity gaps.\nAdditionally, we develop an alignment module to overcome the misalignment\nbetween multi-sensors and emphasize the discriminative representation of the\nUDT space. In the alignment module, we introduce sparsity constraints to\nprovide a better cross-modal representation of the UDT space and robustness\nagainst various sensor settings. We achieve superior classification performance\ncompared to single-modality classifiers and several state-of-the-art\nmulti-modal fusion algorithms on four multi-sensor ATR datasets.",
    "published_date": "2025-03-12T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.09873v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.09866v1",
    "title": "EquiPy: Sequential Fairness using Optimal Transport in Python",
    "authors": [
      "Agathe Fernandes Machado",
      "Suzie Grondin",
      "Philipp Ratz",
      "Arthur Charpentier",
      "François Hu"
    ],
    "author_ids": [],
    "abstract": "Algorithmic fairness has received considerable attention due to the failures\nof various predictive AI systems that have been found to be unfairly biased\nagainst subgroups of the population. Many approaches have been proposed to\nmitigate such biases in predictive systems, however, they often struggle to\nprovide accurate estimates and transparent correction mechanisms in the case\nwhere multiple sensitive variables, such as a combination of gender and race,\nare involved. This paper introduces a new open source Python package, EquiPy,\nwhich provides a easy-to-use and model agnostic toolbox for efficiently\nachieving fairness across multiple sensitive variables. It also offers\ncomprehensive graphic utilities to enable the user to interpret the influence\nof each sensitive variable within a global context. EquiPy makes use of\ntheoretical results that allow the complexity arising from the use of multiple\nvariables to be broken down into easier-to-solve sub-problems. We demonstrate\nthe ease of use for both mitigation and interpretation on publicly available\ndata derived from the US Census and provide sample code for its use.",
    "published_date": "2025-03-12T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.09866v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.09805v1",
    "title": "Un-Straightening Generative AI: How Queer Artists Surface and Challenge the Normativity of Generative AI Models",
    "authors": [
      "Jordan Taylor",
      "Joel Mire",
      "Franchesca Spektor",
      "Alicia DeVrio",
      "Maarten Sap",
      "Haiyi Zhu",
      "Sarah Fox"
    ],
    "author_ids": [],
    "abstract": "Queer people are often discussed as targets of bias, harm, or discrimination\nin research on generative AI. However, the specific ways that queer people\nengage with generative AI, and thus possible uses that support queer people,\nhave yet to be explored. We conducted a workshop study with 13 queer artists,\nduring which we gave participants access to GPT-4 and DALL-E 3 and facilitated\ngroup sensemaking activities. We found our participants struggled to use these\nmodels due to various normative values embedded in their designs, such as\nhyper-positivity and anti-sexuality. We describe various strategies our\nparticipants developed to overcome these models' limitations and how,\nnevertheless, our participants found value in these highly-normative\ntechnologies. Drawing on queer feminist theory, we discuss implications for the\nconceptualization of \"state-of-the-art\" models and consider how FAccT\nresearchers might support queer alternatives.",
    "published_date": "2025-03-12T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.09805v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.11711v1",
    "title": "Privacy-Preserved Automated Scoring using Federated Learning for Educational Research",
    "authors": [
      "Ehsan Latif",
      "Xiaoming Zhai"
    ],
    "author_ids": [],
    "abstract": "Data privacy remains a critical concern in educational research,\nnecessitating Institutional Review Board (IRB) certification and stringent data\nhandling protocols to ensure compliance with ethical standards. Traditional\napproaches rely on anonymization and controlled data-sharing mechanisms to\nfacilitate research while mitigating privacy risks. However, these methods\nstill involve direct access to raw student data, posing potential\nvulnerabilities and being time-consuming. This study proposes a federated\nlearning (FL) framework for automatic scoring in educational assessments,\neliminating the need to share raw data. Our approach leverages client-side\nmodel training, where student responses are processed locally on edge devices,\nand only optimized model parameters are shared with a central aggregation\nserver. To effectively aggregate heterogeneous model updates, we introduce an\nadaptive weighted averaging strategy, which dynamically adjusts weight\ncontributions based on client-specific learning characteristics. This method\nensures robust model convergence while preserving privacy. We evaluate our\nframework using assessment data from nine middle schools, comparing the\naccuracy of federated learning-based scoring models with traditionally trained\ncentralized models. A statistical significance test (paired t-test, $t(8) =\n2.29, p = 0.051$) confirms that the accuracy difference between the two\napproaches is not statistically significant, demonstrating that federated\nlearning achieves comparable performance while safeguarding student data.\nFurthermore, our method significantly reduces data collection, processing, and\ndeployment overhead, accelerating the adoption of AI-driven educational\nassessments in a privacy-compliant manner.",
    "published_date": "2025-03-12T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.11711v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.09763v1",
    "title": "BiasConnect: Investigating Bias Interactions in Text-to-Image Models",
    "authors": [
      "Pushkar Shukla",
      "Aditya Chinchure",
      "Emily Diana",
      "Alexander Tolbert",
      "Kartik Hosanagar",
      "Vineeth N. Balasubramanian",
      "Leonid Sigal",
      "Matthew A. Turk"
    ],
    "author_ids": [],
    "abstract": "The biases exhibited by Text-to-Image (TTI) models are often treated as if\nthey are independent, but in reality, they may be deeply interrelated.\nAddressing bias along one dimension, such as ethnicity or age, can\ninadvertently influence another dimension, like gender, either mitigating or\nexacerbating existing disparities. Understanding these interdependencies is\ncrucial for designing fairer generative models, yet measuring such effects\nquantitatively remains a challenge. In this paper, we aim to address these\nquestions by introducing BiasConnect, a novel tool designed to analyze and\nquantify bias interactions in TTI models. Our approach leverages a\ncounterfactual-based framework to generate pairwise causal graphs that reveals\nthe underlying structure of bias interactions for the given text prompt.\nAdditionally, our method provides empirical estimates that indicate how other\nbias dimensions shift toward or away from an ideal distribution when a given\nbias is modified. Our estimates have a strong correlation (+0.69) with the\ninterdependency observations post bias mitigation. We demonstrate the utility\nof BiasConnect for selecting optimal bias mitigation axes, comparing different\nTTI models on the dependencies they learn, and understanding the amplification\nof intersectional societal biases in TTI models.",
    "published_date": "2025-03-12T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.09763v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.09748v1",
    "title": "Advancing Education through Tutoring Systems: A Systematic Literature Review",
    "authors": [
      "Vincent Liu",
      "Ehsan Latif",
      "Xiaoming Zhai"
    ],
    "author_ids": [],
    "abstract": "This study systematically reviews the transformative role of Tutoring\nSystems, encompassing Intelligent Tutoring Systems (ITS) and Robot Tutoring\nSystems (RTS), in addressing global educational challenges through advanced\ntechnologies. As many students struggle with proficiency in core academic\nareas, Tutoring Systems emerge as promising solutions to bridge learning gaps\nby delivering personalized and adaptive instruction. ITS leverages artificial\nintelligence (AI) models, such as Bayesian Knowledge Tracing and Large Language\nModels, to provide precise cognitive support, while RTS enhances social and\nemotional engagement through human-like interactions. This systematic review,\nadhering to the PRISMA framework, analyzed 86 representative studies. We\nevaluated the pedagogical and technological advancements, engagement\nstrategies, and ethical considerations surrounding these systems. Based on\nthese parameters, Latent Class Analysis was conducted and identified three\ndistinct categories: computer-based ITS, robot-based RTS, and multimodal\nsystems integrating various interaction modes. The findings reveal significant\nadvancements in AI techniques that enhance adaptability, engagement, and\nlearning outcomes. However, challenges such as ethical concerns, scalability\nissues, and gaps in cognitive adaptability persist. The study highlights the\ncomplementary strengths of ITS and RTS, proposing integrated hybrid solutions\nto maximize educational benefits. Future research should focus on bridging gaps\nin scalability, addressing ethical considerations comprehensively, and\nadvancing AI models to support diverse educational needs.",
    "published_date": "2025-03-12T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.09748v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.09743v1",
    "title": "Review GIDE -- Restaurant Review Gastrointestinal Illness Detection and Extraction with Large Language Models",
    "authors": [
      "Timothy Laurence",
      "Joshua Harris",
      "Leo Loman",
      "Amy Douglas",
      "Yung-Wai Chan",
      "Luke Hounsome",
      "Lesley Larkin",
      "Michael Borowitz"
    ],
    "author_ids": [],
    "abstract": "Foodborne gastrointestinal (GI) illness is a common cause of ill health in\nthe UK. However, many cases do not interact with the healthcare system, posing\nsignificant challenges for traditional surveillance methods. The growth of\npublicly available online restaurant reviews and advancements in large language\nmodels (LLMs) present potential opportunities to extend disease surveillance by\nidentifying public reports of GI illness. In this study, we introduce a novel\nannotation schema, developed with experts in GI illness, applied to the Yelp\nOpen Dataset of reviews. Our annotations extend beyond binary disease\ndetection, to include detailed extraction of information on symptoms and foods.\nWe evaluate the performance of open-weight LLMs across these three tasks: GI\nillness detection, symptom extraction, and food extraction. We compare this\nperformance to RoBERTa-based classification models fine-tuned specifically for\nthese tasks. Our results show that using prompt-based approaches, LLMs achieve\nmicro-F1 scores of over 90% for all three of our tasks. Using prompting alone,\nwe achieve micro-F1 scores that exceed those of smaller fine-tuned models. We\nfurther demonstrate the robustness of LLMs in GI illness detection across three\nbias-focused experiments. Our results suggest that publicly available review\ntext and LLMs offer substantial potential for public health surveillance of GI\nillness by enabling highly effective extraction of key information. While LLMs\nappear to exhibit minimal bias in processing, the inherent limitations of\nrestaurant review data highlight the need for cautious interpretation of\nresults.",
    "published_date": "2025-03-12T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.LG",
      "68T50"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.09743v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.09587v1",
    "title": "Fair Federated Medical Image Classification Against Quality Shift via Inter-Client Progressive State Matching",
    "authors": [
      "Nannan Wu",
      "Zhuo Kuang",
      "Zengqiang Yan",
      "Ping Wang",
      "Li Yu"
    ],
    "author_ids": [],
    "abstract": "Despite the potential of federated learning in medical applications,\ninconsistent imaging quality across institutions-stemming from lower-quality\ndata from a minority of clients-biases federated models toward more common\nhigh-quality images. This raises significant fairness concerns. Existing fair\nfederated learning methods have demonstrated some effectiveness in solving this\nproblem by aligning a single 0th- or 1st-order state of convergence (e.g.,\ntraining loss or sharpness). However, we argue in this work that fairness based\non such a single state is still not an adequate surrogate for fairness during\ntesting, as these single metrics fail to fully capture the convergence\ncharacteristics, making them suboptimal for guiding fair learning. To address\nthis limitation, we develop a generalized framework. Specifically, we propose\nassessing convergence using multiple states, defined as sharpness or perturbed\nloss computed at varying search distances. Building on this comprehensive\nassessment, we propose promoting fairness for these states across clients to\nachieve our ultimate fairness objective. This is accomplished through the\nproposed method, FedISM+. In FedISM+, the search distance evolves over time,\nprogressively focusing on different states. We then incorporate two components\nin local training and global aggregation to ensure cross-client fairness for\neach state. This gradually makes convergence equitable for all states, thereby\nimproving fairness during testing. Our empirical evaluations, performed on the\nwell-known RSNA ICH and ISIC 2019 datasets, demonstrate the superiority of\nFedISM+ over existing state-of-the-art methods for fair federated learning. The\ncode is available at https://github.com/wnn2000/FFL4MIA.",
    "published_date": "2025-03-12T00:00:00",
    "year": 2025,
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.09587v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.10706v1",
    "title": "SciFi-Benchmark: How Would AI-Powered Robots Behave in Science Fiction Literature?",
    "authors": [
      "Pierre Sermanet",
      "Anirudha Majumdar",
      "Vikas Sindhwani"
    ],
    "author_ids": [],
    "abstract": "Given the recent rate of progress in artificial intelligence (AI) and\nrobotics, a tantalizing question is emerging: would robots controlled by\nemerging AI systems be strongly aligned with human values? In this work, we\npropose a scalable way to probe this question by generating a benchmark\nspanning the key moments in 824 major pieces of science fiction literature\n(movies, tv, novels and scientific books) where an agent (AI or robot) made\ncritical decisions (good or bad). We use a LLM's recollection of each key\nmoment to generate questions in similar situations, the decisions made by the\nagent, and alternative decisions it could have made (good or bad). We then\nmeasure an approximation of how well models align with human values on a set of\nhuman-voted answers. We also generate rules that can be automatically improved\nvia amendment process in order to generate the first Sci-Fi inspired\nconstitutions for promoting ethical behavior in AIs and robots in the real\nworld. Our first finding is that modern LLMs paired with constitutions turn out\nto be well-aligned with human values (95.8%), contrary to unsettling decisions\ntypically made in SciFi (only 21.2% alignment). Secondly, we find that\ngenerated constitutions substantially increase alignment compared to the base\nmodel (79.4% to 95.8%), and show resilience to an adversarial prompt setting\n(23.3% to 92.3%). Additionally, we find that those constitutions are among the\ntop performers on the ASIMOV Benchmark which is derived from real-world images\nand hospital injury reports. Sci-Fi-inspired constitutions are thus highly\naligned and applicable in real-world situations. We release SciFi-Benchmark: a\nlarge-scale dataset to advance robot ethics and safety research. It comprises\n9,056 questions and 53,384 answers, in addition to a smaller human-labeled\nevaluation set. Data is available at https://scifi-benchmark.github.io",
    "published_date": "2025-03-12T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.10706v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.09414v1",
    "title": "Mitigating Membership Inference Vulnerability in Personalized Federated Learning",
    "authors": [
      "Kangsoo Jung",
      "Sayan Biswas",
      "Catuscia Palamidessi"
    ],
    "author_ids": [],
    "abstract": "Federated Learning (FL) has emerged as a promising paradigm for collaborative\nmodel training without the need to share clients' personal data, thereby\npreserving privacy. However, the non-IID nature of the clients' data introduces\nmajor challenges for FL, highlighting the importance of personalized federated\nlearning (PFL) methods. In PFL, models are trained to cater to specific feature\ndistributions present in the population data. A notable method for PFL is the\nIterative Federated Clustering Algorithm (IFCA), which mitigates the concerns\nassociated with the non-IID-ness by grouping clients with similar data\ndistributions. While it has been shown that IFCA enhances both accuracy and\nfairness, its strategy of dividing the population into smaller clusters\nincreases vulnerability to Membership Inference Attacks (MIA), particularly\namong minorities with limited training samples. In this paper, we introduce\nIFCA-MIR, an improved version of IFCA that integrates MIA risk assessment into\nthe clustering process. Allowing clients to select clusters based on both model\nperformance and MIA vulnerability, IFCA-MIR achieves an improved performance\nwith respect to accuracy, fairness, and privacy. We demonstrate that IFCA-MIR\nsignificantly reduces MIA risk while maintaining comparable model accuracy and\nfairness as the original IFCA.",
    "published_date": "2025-03-12T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.09414v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.09399v1",
    "title": "ForAug: Recombining Foregrounds and Backgrounds to Improve Vision Transformer Training with Bias Mitigation",
    "authors": [
      "Tobias Christian Nauen",
      "Brian Moser",
      "Federico Raue",
      "Stanislav Frolov",
      "Andreas Dengel"
    ],
    "author_ids": [],
    "abstract": "Transformers, particularly Vision Transformers (ViTs), have achieved\nstate-of-the-art performance in large-scale image classification. However, they\noften require large amounts of data and can exhibit biases that limit their\nrobustness and generalizability. This paper introduces ForAug, a novel data\naugmentation scheme that addresses these challenges and explicitly includes\ninductive biases, which commonly are part of the neural network architecture,\ninto the training data. ForAug is constructed by using pretrained foundation\nmodels to separate and recombine foreground objects with different backgrounds,\nenabling fine-grained control over image composition during training. It thus\nincreases the data diversity and effective number of training samples. We\ndemonstrate that training on ForNet, the application of ForAug to ImageNet,\nsignificantly improves the accuracy of ViTs and other architectures by up to\n4.5 percentage points (p.p.) on ImageNet and 7.3 p.p. on downstream tasks.\nImportantly, ForAug enables novel ways of analyzing model behavior and\nquantifying biases. Namely, we introduce metrics for background robustness,\nforeground focus, center bias, and size bias and show that training on ForNet\nsubstantially reduces these biases compared to training on ImageNet. In\nsummary, ForAug provides a valuable tool for analyzing and mitigating biases,\nenabling the development of more robust and reliable computer vision models.\nOur code and dataset are publicly available at https://github.com/tobna/ForAug.",
    "published_date": "2025-03-12T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "68T45",
      "I.2.10; I.2.6; I.4.6"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.09399v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.09659v2",
    "title": "Edge AI for Real-time Fetal Assessment in Rural Guatemala",
    "authors": [
      "Nasim Katebi",
      "Mohammad Ahmad",
      "Mohsen Motie-Shirazi",
      "Daniel Phan",
      "Ellen Kolesnikova",
      "Sepideh Nikookar",
      "Alireza Rafiei",
      "Murali K. Korikana",
      "Rachel Hall-Clifford",
      "Esteban Castro",
      "Rosibely Sut",
      "Enma Coyote",
      "Anahi Venzor Strader",
      "Edlyn Ramos",
      "Peter Rohloff",
      "Reza Sameni",
      "Gari D. Clifford"
    ],
    "author_ids": [],
    "abstract": "Perinatal complications, defined as conditions that arise during pregnancy,\nchildbirth, and the immediate postpartum period, represent a significant burden\non maternal and neonatal health worldwide. Factors contributing to these\ndisparities include limited access to quality healthcare, socioeconomic\ninequalities, and variations in healthcare infrastructure. Addressing these\nissues is crucial for improving health outcomes for mothers and newborns,\nparticularly in underserved communities. To mitigate these challenges, we have\ndeveloped an AI-enabled smartphone application designed to provide decision\nsupport at the point-of-care. This tool aims to enhance health monitoring\nduring pregnancy by leveraging machine learning (ML) techniques. The intended\nuse of this application is to assist midwives during routine home visits by\noffering real-time analysis and providing feedback based on collected data. The\napplication integrates TensorFlow Lite (TFLite) and other Python-based\nalgorithms within a Kotlin framework to process data in real-time. It is\ndesigned for use in low-resource settings, where traditional healthcare\ninfrastructure may be lacking. The intended patient population includes\npregnant women and new mothers in underserved areas and the developed system\nwas piloted in rural Guatemala. This ML-based solution addresses the critical\nneed for accessible and quality perinatal care by empowering healthcare\nproviders with decision support tools to improve maternal and neonatal health\noutcomes.",
    "published_date": "2025-03-12T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.09659v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.09330v1",
    "title": "Group-robust Machine Unlearning",
    "authors": [
      "Thomas De Min",
      "Subhankar Roy",
      "Stéphane Lathuilière",
      "Elisa Ricci",
      "Massimiliano Mancini"
    ],
    "author_ids": [],
    "abstract": "Machine unlearning is an emerging paradigm to remove the influence of\nspecific training data (i.e., the forget set) from a model while preserving its\nknowledge of the rest of the data (i.e., the retain set). Previous approaches\nassume the forget data to be uniformly distributed from all training\ndatapoints. However, if the data to unlearn is dominant in one group, we\nempirically show that performance for this group degrades, leading to fairness\nissues. This work tackles the overlooked problem of non-uniformly distributed\nforget sets, which we call group-robust machine unlearning, by presenting a\nsimple, effective strategy that mitigates the performance loss in dominant\ngroups via sample distribution reweighting. Moreover, we present MIU (Mutual\nInformation-aware Machine Unlearning), the first approach for group robustness\nin approximate machine unlearning. MIU minimizes the mutual information between\nmodel features and group information, achieving unlearning while reducing\nperformance degradation in the dominant group of the forget set. Additionally,\nMIU exploits sample distribution reweighting and mutual information calibration\nwith the original model to preserve group robustness. We conduct experiments on\nthree datasets and show that MIU outperforms standard methods, achieving\nunlearning without compromising model robustness. Source code available at\nhttps://github.com/tdemin16/group-robust_machine_unlearning.",
    "published_date": "2025-03-12T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.09330v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.11705v3",
    "title": "The BIG Argument for AI Safety Cases",
    "authors": [
      "Ibrahim Habli",
      "Richard Hawkins",
      "Colin Paterson",
      "Philippa Ryan",
      "Yan Jia",
      "Mark Sujan",
      "John McDermid"
    ],
    "author_ids": [],
    "abstract": "We present our Balanced, Integrated and Grounded (BIG) argument for assuring\nthe safety of AI systems. The BIG argument adopts a whole-system approach to\nconstructing a safety case for AI systems of varying capability, autonomy and\ncriticality. Firstly, it is balanced by addressing safety alongside other\ncritical ethical issues such as privacy and equity, acknowledging complexities\nand trade-offs in the broader societal impact of AI. Secondly, it is integrated\nby bringing together the social, ethical and technical aspects of safety\nassurance in a way that is traceable and accountable. Thirdly, it is grounded\nin long-established safety norms and practices, such as being sensitive to\ncontext and maintaining risk proportionality. Whether the AI capability is\nnarrow and constrained or general-purpose and powered by a frontier or\nfoundational model, the BIG argument insists on a systematic treatment of\nsafety. Further, it places a particular focus on the novel hazardous behaviours\nemerging from the advanced capabilities of frontier AI models and the open\ncontexts in which they are rapidly being deployed. These complex issues are\nconsidered within a wider AI safety case, approaching assurance from both\ntechnical and sociotechnical perspectives. Examples illustrating the use of the\nBIG argument are provided throughout the paper.",
    "published_date": "2025-03-12T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.11705v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.09242v1",
    "title": "NAMI: Efficient Image Generation via Progressive Rectified Flow Transformers",
    "authors": [
      "Yuhang Ma",
      "Bo Cheng",
      "Shanyuan Liu",
      "Ao Ma",
      "Xiaoyu Wu",
      "Liebucha Wu",
      "Dawei Leng",
      "Yuhui Yin"
    ],
    "author_ids": [],
    "abstract": "Flow-based transformer models for image generation have achieved\nstate-of-the-art performance with larger model parameters, but their inference\ndeployment cost remains high. To enhance inference performance while\nmaintaining generation quality, we propose progressive rectified flow\ntransformers. We divide the rectified flow into different stages according to\nresolution, using fewer transformer layers at the low-resolution stages to\ngenerate image layouts and concept contours, and progressively adding more\nlayers as the resolution increases. Experiments demonstrate that our approach\nachieves fast convergence and reduces inference time while ensuring generation\nquality. The main contributions of this paper are summarized as follows: (1) We\nintroduce progressive rectified flow transformers that enable multi-resolution\ntraining, accelerating model convergence; (2) NAMI leverages piecewise flow and\nspatial cascading of Diffusion Transformer (DiT) to rapidly generate images,\nreducing inference time by 40% to generate a 1024 resolution image; (3) We\npropose NAMI-1K benchmark to evaluate human preference performance, aiming to\nmitigate distributional bias and prevent data leakage from open-source\nbenchmarks. The results show that our model is competitive with\nstate-of-the-art models.",
    "published_date": "2025-03-12T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.09242v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.09651v1",
    "title": "Bags of Projected Nearest Neighbours: Competitors to Random Forests?",
    "authors": [
      "David P. Hofmeyr"
    ],
    "author_ids": [],
    "abstract": "In this paper we introduce a simple and intuitive adaptive k nearest\nneighbours classifier, and explore its utility within the context of bootstrap\naggregating (\"bagging\"). The approach is based on finding discriminant\nsubspaces which are computationally efficient to compute, and are motivated by\nenhancing the discrimination of classes through nearest neighbour classifiers.\nThis adaptiveness promotes diversity of the individual classifiers fit across\ndifferent bootstrap samples, and so further leverages the variance reducing\neffect of bagging. Extensive experimental results are presented documenting the\nstrong performance of the proposed approach in comparison with Random Forest\nclassifiers, as well as other nearest neighbours based ensembles from the\nliterature, plus other relevant benchmarks. Code to implement the proposed\napproach is available in the form of an R package from\nhttps://github.com/DavidHofmeyr/BOPNN.",
    "published_date": "2025-03-12T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.09651v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.09068v1",
    "title": "Probing Network Decisions: Capturing Uncertainties and Unveiling Vulnerabilities Without Label Information",
    "authors": [
      "Youngju Joung",
      "Sehyun Lee",
      "Jaesik Choi"
    ],
    "author_ids": [],
    "abstract": "To improve trust and transparency, it is crucial to be able to interpret the\ndecisions of Deep Neural classifiers (DNNs). Instance-level examinations, such\nas attribution techniques, are commonly employed to interpret the model\ndecisions. However, when interpreting misclassified decisions, human\nintervention may be required. Analyzing the attribu tions across each class\nwithin one instance can be particularly labor intensive and influenced by the\nbias of the human interpreter. In this paper, we present a novel framework to\nuncover the weakness of the classifier via counterfactual examples. A prober is\nintroduced to learn the correctness of the classifier's decision in terms of\nbinary code-hit or miss. It enables the creation of the counterfactual example\nconcerning the prober's decision. We test the performance of our prober's\nmisclassification detection and verify its effectiveness on the image\nclassification benchmark datasets. Furthermore, by generating counterfactuals\nthat penetrate the prober, we demonstrate that our framework effectively\nidentifies vulnerabilities in the target classifier without relying on label\ninformation on the MNIST dataset.",
    "published_date": "2025-03-12T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.09068v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.09062v1",
    "title": "TSConnect: An Enhanced MOOC Platform for Bridging Communication Gaps Between Instructors and Students in Light of the Curse of Knowledge",
    "authors": [
      "Qianyu Liu",
      "Xinran Li",
      "Xiaocong Du",
      "Quan Li"
    ],
    "author_ids": [],
    "abstract": "Knowledge dissemination in educational settings is profoundly influenced by\nthe curse of knowledge, a cognitive bias that causes experts to underestimate\nthe challenges faced by learners due to their own in-depth understanding of the\nsubject. This bias can hinder effective knowledge transfer and pedagogical\neffectiveness, and may be exacerbated by inadequate instructor-student\ncommunication. To encourage more effective feedback and promote empathy, we\nintroduce TSConnect, a bias-aware, adaptable interactive MOOC (Massive Open\nOnline Course) learning system, informed by a need-finding survey involving 129\nstudents and 6 instructors. TSConnect integrates instructors, students, and\nArtificial Intelligence (AI) into a cohesive platform, facilitating diverse and\ntargeted communication channels while addressing previously overlooked\ninformation needs. A notable feature is its dynamic knowledge graph, which\nenhances learning support and fosters a more interconnected educational\nexperience. We conducted a between-subjects user study with 30 students\ncomparing TSConnect to a baseline system. Results indicate that TSConnect\nsignificantly encourages students to provide more feedback to instructors.\nAdditionally, interviews with 4 instructors reveal insights into how they\ninterpret and respond to this feedback, potentially leading to improvements in\nteaching strategies and the development of broader pedagogical skills.",
    "published_date": "2025-03-12T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.09062v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.16499v1",
    "title": "Stakeholder Perspectives on Whether and How Social Robots Can Support Mediation and Advocacy for Higher Education Students with Disabilities",
    "authors": [
      "Alva Markelius",
      "Julie Bailey",
      "Jenny L. Gibson",
      "Hatice Gunes"
    ],
    "author_ids": [],
    "abstract": "This paper presents an iterative, participatory, empirical study that\nexamines the potential of using artificial intelligence, such as social robots\nand large language models, to support mediation and advocacy for students with\ndisabilities in higher education. Drawing on qualitative data from interviews\nand focus groups conducted with various stakeholders, including disabled\nstudents, disabled student representatives, and disability practitioners at the\nUniversity of Cambridge, this study reports findings relating to understanding\nthe problem space, ideating robotic support and participatory co-design of\nadvocacy support robots. The findings highlight the potential of these\ntechnologies in providing signposting and acting as a sounding board or study\ncompanion, while also addressing limitations in empathic understanding, trust,\nequity, and accessibility. We discuss ethical considerations, including\nintersectional biases, the double empathy problem, and the implications of\ndeploying social robots in contexts shaped by structural inequalities. Finally,\nwe offer a set of recommendations and suggestions for future research,\nrethinking the notion of corrective technological interventions to tools that\nempower and amplify self-advocacy.",
    "published_date": "2025-03-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.16499v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.16498v1",
    "title": "Llms, Virtual Users, and Bias: Predicting Any Survey Question Without Human Data",
    "authors": [
      "Enzo Sinacola",
      "Arnault Pachot",
      "Thierry Petit"
    ],
    "author_ids": [],
    "abstract": "Large Language Models (LLMs) offer a promising alternative to traditional\nsurvey methods, potentially enhancing efficiency and reducing costs. In this\nstudy, we use LLMs to create virtual populations that answer survey questions,\nenabling us to predict outcomes comparable to human responses. We evaluate\nseveral LLMs-including GPT-4o, GPT-3.5, Claude 3.5-Sonnet, and versions of the\nLlama and Mistral models-comparing their performance to that of a traditional\nRandom Forests algorithm using demographic data from the World Values Survey\n(WVS). LLMs demonstrate competitive performance overall, with the significant\nadvantage of requiring no additional training data. However, they exhibit\nbiases when predicting responses for certain religious and population groups,\nunderperforming in these areas. On the other hand, Random Forests demonstrate\nstronger performance than LLMs when trained with sufficient data. We observe\nthat removing censorship mechanisms from LLMs significantly improves predictive\naccuracy, particularly for underrepresented demographic segments where censored\nmodels struggle. These findings highlight the importance of addressing biases\nand reconsidering censorship approaches in LLMs to enhance their reliability\nand fairness in public opinion research.",
    "published_date": "2025-03-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.16498v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.08579v1",
    "title": "Sparsity-Induced Global Matrix Autoregressive Model with Auxiliary Network Data",
    "authors": [
      "Sanyou Wu",
      "Dan Yang",
      "Yan Xu",
      "Long Feng"
    ],
    "author_ids": [],
    "abstract": "Jointly modeling and forecasting economic and financial variables across a\nlarge set of countries has long been a significant challenge. Two primary\napproaches have been utilized to address this issue: the vector autoregressive\nmodel with exogenous variables (VARX) and the matrix autoregression (MAR). The\nVARX model captures domestic dependencies, but treats variables exogenous to\nrepresent global factors driven by international trade. In contrast, the MAR\nmodel simultaneously considers variables from multiple countries but ignores\nthe trade network. In this paper, we propose an extension of the MAR model that\nachieves these two aims at once, i.e., studying both international dependencies\nand the impact of the trade network on the global economy. Additionally, we\nintroduce a sparse component to the model to differentiate between systematic\nand idiosyncratic cross-predictability. To estimate the model parameters, we\npropose both a likelihood estimation method and a bias-corrected alternating\nminimization version. We provide theoretical and empirical analyses of the\nmodel's properties, alongside presenting intriguing economic insights derived\nfrom our findings.",
    "published_date": "2025-03-11T00:00:00",
    "year": 2025,
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.ME"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.08579v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.08510v1",
    "title": "External Knowledge Injection for CLIP-Based Class-Incremental Learning",
    "authors": [
      "Da-Wei Zhou",
      "Kai-Wen Li",
      "Jingyi Ning",
      "Han-Jia Ye",
      "Lijun Zhang",
      "De-Chuan Zhan"
    ],
    "author_ids": [],
    "abstract": "Class-Incremental Learning (CIL) enables learning systems to continuously\nadapt to evolving data streams. With the advancement of pre-training,\nleveraging pre-trained vision-language models (e.g., CLIP) offers a promising\nstarting point for CIL. However, CLIP makes decisions by matching visual\nembeddings to class names, overlooking the rich contextual information conveyed\nthrough language. For instance, the concept of ``cat'' can be decomposed into\nfeatures like tail, fur, and face for recognition. Besides, since the model is\ncontinually updated, these detailed features are overwritten in CIL, requiring\nexternal knowledge for compensation. In this paper, we introduce ExterNal\nknowledGe INjEction (ENGINE) for CLIP-based CIL. To enhance knowledge transfer\nfrom outside the dataset, we propose a dual-branch injection tuning framework\nthat encodes informative knowledge from both visual and textual modalities. The\nvisual branch is enhanced with data augmentation to enrich the visual features,\nwhile the textual branch leverages GPT-4 to rewrite discriminative descriptors.\nIn addition to this on-the-fly knowledge injection, we also implement\npost-tuning knowledge by re-ranking the prediction results during inference.\nWith the injected knowledge, the model can better capture informative features\nfor downstream tasks as data evolves. Extensive experiments demonstrate the\nstate-of-the-art performance of ENGINE. Code is available at:\nhttps://github.com/RenaissCode/ENGINE",
    "published_date": "2025-03-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.08510v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.08750v1",
    "title": "Exposing Product Bias in LLM Investment Recommendation",
    "authors": [
      "Yuhan Zhi",
      "Xiaoyu Zhang",
      "Longtian Wang",
      "Shumin Jiang",
      "Shiqing Ma",
      "Xiaohong Guan",
      "Chao Shen"
    ],
    "author_ids": [],
    "abstract": "Large language models (LLMs), as a new generation of recommendation engines,\npossess powerful summarization and data analysis capabilities, surpassing\ntraditional recommendation systems in both scope and performance. One promising\napplication is investment recommendation. In this paper, we reveal a novel\nproduct bias in LLM investment recommendation, where LLMs exhibit systematic\npreferences for specific products. Such preferences can subtly influence user\ninvestment decisions, potentially leading to inflated valuations of products\nand financial bubbles, posing risks to both individual investors and market\nstability. To comprehensively study the product bias, we develop an automated\npipeline to create a dataset of 567,000 samples across five asset classes\n(stocks, mutual funds, cryptocurrencies, savings, and portfolios). With this\ndataset, we present the bf first study on product bias in LLM investment\nrecommendations. Our findings reveal that LLMs exhibit clear product\npreferences, such as certain stocks (e.g., `AAPL' from Apple and `MSFT' from\nMicrosoft). Notably, this bias persists even after applying debiasing\ntechniques. We urge AI researchers to take heed of the product bias in LLM\ninvestment recommendations and its implications, ensuring fairness and security\nin the digital space and market.",
    "published_date": "2025-03-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.08750v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.08255v1",
    "title": "Geometric Nonlinear Filtering with Almost Global Convergence for Attitude and Bias Estimation on the Special Orthogonal Group",
    "authors": [
      "Farooq Aslam",
      "Muhammad Farooq Haydar",
      "Suhail Akhtar"
    ],
    "author_ids": [],
    "abstract": "This paper proposes a novel geometric nonlinear filter for attitude and bias\nestimation on the Special Orthogonal Group $SO(3)$ using matrix measurements.\nThe structure of the proposed filter is similar to that of the continuous-time\ndeterministic multiplicative extended Kalman filter (MEKF). The main difference\nwith the MEKF is the inclusion of curvature correction terms in both the filter\ngain and gain update equations. These terms ensure that the proposed filter,\nnamed the Generalized $SO(3)$-MEKF, renders the desired equilibrium of the\nestimation error system to be almost globally uniformly asymptotically stable\n(AGUAS). More precisely, the attitude and bias estimation errors converge\nuniformly asymptotically to zero for almost all initial conditions except those\nwhere the initial angular estimation error equals $\\pi$ radians. Moreover, in\nthe case of small estimation errors, the proposed generalized $SO(3)$-MEKF\nsimplifies to the standard $SO(3)$-MEKF with matrix measurements. Simulation\nresults indicate that the proposed filter has similar performance compared to\nthe latter. Thus, the main advantage of the proposed filter over the MEKF is\nthe guarantee of (almost) global uniform asymptotic stability.",
    "published_date": "2025-03-11T00:00:00",
    "year": 2025,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.08255v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.08203v1",
    "title": "A Theoretical Framework for Preventing Class Collapse in Supervised Contrastive Learning",
    "authors": [
      "Chungpa Lee",
      "Jeongheon Oh",
      "Kibok Lee",
      "Jy-yong Sohn"
    ],
    "author_ids": [],
    "abstract": "Supervised contrastive learning (SupCL) has emerged as a prominent approach\nin representation learning, leveraging both supervised and self-supervised\nlosses. However, achieving an optimal balance between these losses is\nchallenging; failing to do so can lead to class collapse, reducing\ndiscrimination among individual embeddings in the same class. In this paper, we\npresent theoretically grounded guidelines for SupCL to prevent class collapse\nin learned representations. Specifically, we introduce the Simplex-to-Simplex\nEmbedding Model (SSEM), a theoretical framework that models various embedding\nstructures, including all embeddings that minimize the supervised contrastive\nloss. Through SSEM, we analyze how hyperparameters affect learned\nrepresentations, offering practical guidelines for hyperparameter selection to\nmitigate the risk of class collapse. Our theoretical findings are supported by\nempirical results across synthetic and real-world datasets.",
    "published_date": "2025-03-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.08203v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.08116v1",
    "title": "ACE: Concept Editing in Diffusion Models without Performance Degradation",
    "authors": [
      "Ruipeng Wang",
      "Junfeng Fang",
      "Jiaqi Li",
      "Hao Chen",
      "Jie Shi",
      "Kun Wang",
      "Xiang Wang"
    ],
    "author_ids": [],
    "abstract": "Diffusion-based text-to-image models have demonstrated remarkable\ncapabilities in generating realistic images, but they raise societal and\nethical concerns, such as the creation of unsafe content. While concept editing\nis proposed to address these issues, they often struggle to balance the removal\nof unsafe concept with maintaining the model's general genera-tive\ncapabilities. In this work, we propose ACE, a new editing method that enhances\nconcept editing in diffusion models. ACE introduces a novel cross null-space\nprojection approach to precisely erase unsafe concept while maintaining the\nmodel's ability to generate high-quality, semantically consistent images.\nExtensive experiments demonstrate that ACE significantly outperforms the\nadvancing baselines,improving semantic consistency by 24.56% and image\ngeneration quality by 34.82% on average with only 1% of the time cost. These\nresults highlight the practical utility of concept editing by mitigating its\npotential risks, paving the way for broader applications in the field. Code is\navaliable at https://github.com/littlelittlenine/ACE-zero.git",
    "published_date": "2025-03-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.08116v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.16494v1",
    "title": "The impact of artificial intelligence: from cognitive costs to global inequality",
    "authors": [
      "Guy Paić",
      "Leonid Serkin"
    ],
    "author_ids": [],
    "abstract": "In this paper, we examine the wide-ranging impact of artificial intelligence\non society, focusing on its potential to both help and harm global equity,\ncognitive abilities, and economic stability. We argue that while artificial\nintelligence offers significant opportunities for progress in areas like\nhealthcare, education, and scientific research, its rapid growth -- mainly\ndriven by private companies -- may worsen global inequalities, increase\ndependence on automated systems for cognitive tasks, and disrupt established\neconomic paradigms. We emphasize the critical need for strong governance and\nethical guidelines to tackle these issues, urging the academic community to\nactively participate in creating policies that ensure the benefits of\nartificial intelligence are shared fairly and its risks are managed\neffectively.",
    "published_date": "2025-03-11T00:00:00",
    "year": 2025,
    "categories": [
      "physics.soc-ph",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.16494v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.08060v1",
    "title": "Data-Driven Dynamic Controller Synthesis for Discrete-Time General Nonlinear Systems",
    "authors": [
      "Behrad Samari",
      "Abolfazl Lavaei"
    ],
    "author_ids": [],
    "abstract": "Synthesizing safety controllers for general nonlinear systems is a highly\nchallenging task, particularly when the system models are unknown, and input\nconstraints are present. While some recent efforts have explored data-driven\nsafety controller design for nonlinear systems, these approaches are primarily\nlimited to specific classes of nonlinear dynamics (e.g., polynomials) and are\nnot applicable to general nonlinear systems. This paper develops a direct\ndata-driven approach for discrete-time general nonlinear systems, facilitating\nthe simultaneous learning of control barrier certificates (CBCs) and dynamic\ncontrollers to ensure safety properties under input constraints. Specifically,\nby leveraging the adding-one-integrator approach, we incorporate the\ncontroller's dynamics into the system dynamics to synthesize a virtual\nstatic-feedback controller for the augmented system, resulting in a dynamic\nsafety controller for the actual dynamics. We collect input-state data from the\naugmented system during a finite-time experiment, referred to as a single\ntrajectory. Using this data, we learn augmented CBCs and the corresponding\nvirtual safety controllers, ensuring the safety of the actual system and\nadherence to input constraints over a finite time horizon. We demonstrate that\nour proposed conditions boil down to some data-dependent linear matrix\ninequalities (LMIs), which are easy to satisfy. We showcase the effectiveness\nof our data-driven approach through two case studies: one exhibiting\nsignificant nonlinearity and the other featuring high dimensionality.",
    "published_date": "2025-03-11T00:00:00",
    "year": 2025,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.08060v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.08038v1",
    "title": "Generalized Kullback-Leibler Divergence Loss",
    "authors": [
      "Jiequan Cui",
      "Beier Zhu",
      "Qingshan Xu",
      "Zhuotao Tian",
      "Xiaojuan Qi",
      "Bei Yu",
      "Hanwang Zhang",
      "Richang Hong"
    ],
    "author_ids": [],
    "abstract": "In this paper, we delve deeper into the Kullback-Leibler (KL) Divergence loss\nand mathematically prove that it is equivalent to the Decoupled\nKullback-Leibler (DKL) Divergence loss that consists of (1) a weighted Mean\nSquare Error (wMSE) loss and (2) a Cross-Entropy loss incorporating soft\nlabels. Thanks to the decoupled structure of DKL loss, we have identified two\nareas for improvement. Firstly, we address the limitation of KL loss in\nscenarios like knowledge distillation by breaking its asymmetric optimization\nproperty along with a smoother weight function. This modification effectively\nalleviates convergence challenges in optimization, particularly for classes\nwith high predicted scores in soft labels. Secondly, we introduce class-wise\nglobal information into KL/DKL to reduce bias arising from individual samples.\nWith these two enhancements, we derive the Generalized Kullback-Leibler (GKL)\nDivergence loss and evaluate its effectiveness by conducting experiments on\nCIFAR-10/100, ImageNet, and vision-language datasets, focusing on adversarial\ntraining, and knowledge distillation tasks. Specifically, we achieve new\nstate-of-the-art adversarial robustness on the public leaderboard --\nRobustBench and competitive knowledge distillation performance across\nCIFAR/ImageNet models and CLIP models, demonstrating the substantial practical\nmerits. Our code is available at https://github.com/jiequancui/DKL.",
    "published_date": "2025-03-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.08038v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.08012v1",
    "title": "Exploring Bias in over 100 Text-to-Image Generative Models",
    "authors": [
      "Jordan Vice",
      "Naveed Akhtar",
      "Richard Hartley",
      "Ajmal Mian"
    ],
    "author_ids": [],
    "abstract": "We investigate bias trends in text-to-image generative models over time,\nfocusing on the increasing availability of models through open platforms like\nHugging Face. While these platforms democratize AI, they also facilitate the\nspread of inherently biased models, often shaped by task-specific fine-tuning.\nEnsuring ethical and transparent AI deployment requires robust evaluation\nframeworks and quantifiable bias metrics. To this end, we assess bias across\nthree key dimensions: (i) distribution bias, (ii) generative hallucination, and\n(iii) generative miss-rate. Analyzing over 100 models, we reveal how bias\npatterns evolve over time and across generative tasks. Our findings indicate\nthat artistic and style-transferred models exhibit significant bias, whereas\nfoundation models, benefiting from broader training distributions, are becoming\nprogressively less biased. By identifying these systemic trends, we contribute\na large-scale evaluation corpus to inform bias research and mitigation\nstrategies, fostering more responsible AI development.\n  Keywords: Bias, Ethical AI, Text-to-Image, Generative Models, Open-Source\nModels",
    "published_date": "2025-03-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.08012v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.08003v1",
    "title": "How Can Video Generative AI Transform K-12 Education? Examining Teachers' Perspectives through TPACK and TAM",
    "authors": [
      "Unggi Lee",
      "Yeil Jeong",
      "Seungha Kim",
      "Yoorim Son",
      "Gyuri Byun",
      "Hyeoncheol Kim",
      "Cheolil Lim"
    ],
    "author_ids": [],
    "abstract": "The rapid advancement of generative AI technology, particularly video\ngenerative AI (Video GenAI), has opened new possibilities for K-12 education by\nenabling the creation of dynamic, customized, and high-quality visual content.\nDespite its potential, there is limited research on how this emerging\ntechnology can be effectively integrated into educational practices. This study\nexplores the perspectives of leading K-12 teachers on the educational\napplications of Video GenAI, using the TPACK (Technological Pedagogical Content\nKnowledge) and TAM (Technology Acceptance Model) frameworks as analytical\nlenses. Through interviews and hands-on experimentation with video generation\ntools, the research identifies opportunities for enhancing teaching strategies,\nfostering student engagement, and supporting authentic task design. It also\nhighlights challenges such as technical limitations, ethical considerations,\nand the need for institutional support. The findings provide actionable\ninsights into how Video GenAI can transform teaching and learning, offering\npractical implications for policy, teacher training, and the future development\nof educational technology.",
    "published_date": "2025-03-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.08003v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.08731v1",
    "title": "FairDeFace: Evaluating the Fairness and Adversarial Robustness of Face Obfuscation Methods",
    "authors": [
      "Seyyed Mohammad Sadegh Moosavi Khorzooghi",
      "Poojitha Thota",
      "Mohit Singhal",
      "Abolfazl Asudeh",
      "Gautam Das",
      "Shirin Nilizadeh"
    ],
    "author_ids": [],
    "abstract": "The lack of a common platform and benchmark datasets for evaluating face\nobfuscation methods has been a challenge, with every method being tested using\narbitrary experiments, datasets, and metrics. While prior work has demonstrated\nthat face recognition systems exhibit bias against some demographic groups,\nthere exists a substantial gap in our understanding regarding the fairness of\nface obfuscation methods. Providing fair face obfuscation methods can ensure\nequitable protection across diverse demographic groups, especially since they\ncan be used to preserve the privacy of vulnerable populations. To address these\ngaps, this paper introduces a comprehensive framework, named FairDeFace,\ndesigned to assess the adversarial robustness and fairness of face obfuscation\nmethods. The framework introduces a set of modules encompassing data\nbenchmarks, face detection and recognition algorithms, adversarial models,\nutility detection models, and fairness metrics. FairDeFace serves as a\nversatile platform where any face obfuscation method can be integrated,\nallowing for rigorous testing and comparison with other state-of-the-art\nmethods. In its current implementation, FairDeFace incorporates 6 attacks, and\nseveral privacy, utility and fairness metrics. Using FairDeFace, and by\nconducting more than 500 experiments, we evaluated and compared the adversarial\nrobustness of seven face obfuscation methods. This extensive analysis led to\nmany interesting findings both in terms of the degree of robustness of existing\nmethods and their biases against some gender or racial groups. FairDeFace also\nuses visualization of focused areas for both obfuscation and verification\nattacks to show not only which areas are mostly changed in the obfuscation\nprocess for some demographics, but also why they failed through focus area\ncomparison of obfuscation and verification.",
    "published_date": "2025-03-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.08731v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.07938v1",
    "title": "CAD-VAE: Leveraging Correlation-Aware Latents for Comprehensive Fair Disentanglement",
    "authors": [
      "Chenrui Ma",
      "Rongchang Zhao",
      "Xi Xiao",
      "Hongyang Xie",
      "Tianyang Wang",
      "Xiao Wang",
      "Hao Zhang",
      "Yanning Shen"
    ],
    "author_ids": [],
    "abstract": "While deep generative models have significantly advanced representation\nlearning, they may inherit or amplify biases and fairness issues by encoding\nsensitive attributes alongside predictive features. Enforcing strict\nindependence in disentanglement is often unrealistic when target and sensitive\nfactors are naturally correlated. To address this challenge, we propose CAD-VAE\n(Correlation-Aware Disentangled VAE), which introduces a correlated latent code\nto capture the shared information between target and sensitive attributes.\nGiven this correlated latent, our method effectively separates overlapping\nfactors without extra domain knowledge by directly minimizing the conditional\nmutual information between target and sensitive codes. A relevance-driven\noptimization strategy refines the correlated code by efficiently capturing\nessential correlated features and eliminating redundancy. Extensive experiments\non benchmark datasets demonstrate that CAD-VAE produces fairer representations,\nrealistic counterfactuals, and improved fairness-aware image editing.",
    "published_date": "2025-03-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ME"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.07938v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.07928v2",
    "title": "The StudyChat Dataset: Student Dialogues With ChatGPT in an Artificial Intelligence Course",
    "authors": [
      "Hunter McNichols",
      "Andrew Lan"
    ],
    "author_ids": [],
    "abstract": "The widespread availability of large language models (LLMs), such as ChatGPT,\nhas significantly impacted education, raising both opportunities and\nchallenges. Students can frequently interact with LLM-powered, interactive\nlearning tools, but their usage patterns need to be analyzed to ensure ethical\nusage of these tools. To better understand how students interact with LLMs in\nan academic setting, we introduce \\textbf{StudyChat}, a publicly available\ndataset capturing real-world student interactions with an LLM-powered tutoring\nchatbot in a semester-long, university-level artificial intelligence (AI)\ncourse. We deploy a web application that replicates ChatGPT's core\nfunctionalities, and use it to log student interactions with the LLM while\nworking on programming assignments. We collect 1,197 conversations, which we\nannotate using a dialogue act labeling schema inspired by observed interaction\npatterns and prior research. Additionally, we analyze these interactions,\nhighlight behavioral trends, and analyze how specific usage patterns relate to\ncourse outcomes. \\textbf{StudyChat} provides a rich resource for the learning\nsciences and AI in education communities, enabling further research into the\nevolving role of LLMs in education.",
    "published_date": "2025-03-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.07928v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.07892v1",
    "title": "\"We're losing our neighborhoods. We're losing our community\": A comparative analysis of community discourse in online and offline public spheres",
    "authors": [
      "Casey Randazzo",
      "Minkyung Kim",
      "Melanie Kwestel",
      "Marya L Doerfel",
      "Tawfiq Ammari"
    ],
    "author_ids": [],
    "abstract": "Recovering from crises, such as hurricanes or wildfires, is a complex process\nthat can take weeks, months, or even decades to overcome. Crises have both\nacute (immediate) and chronic (long-term) effects on communities. Crisis\ninformatics research often focuses on the immediate response phase of\ndisasters, thereby overlooking the long-term recovery phase, which is critical\nfor understanding the information needs of users undergoing challenges like\nclimate gentrification and housing inequity. We fill this gap by investigating\ncommunity discourse over eight months following Hurricane Ida in an online\nneighborhood Facebook group and Town Hall Meetings of a borough in the New York\nMetropolitan region. Using a mixed methods approach, we examined the use of\nsocial media to manage long-term disaster recovery. The findings revealed a\nsignificant overlap in topics, underscoring the interconnected nature of online\nand offline community discourse, and illuminated themes related to the\nlong-term consequences of disasters. We conclude with recommendations aimed at\nhelping designers and government leaders enhance participation across community\nforums and support recovery in the aftermath of disasters.",
    "published_date": "2025-03-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SI",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.07892v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.07878v2",
    "title": "Measuring directional bias amplification in image captions using predictability",
    "authors": [
      "Rahul Nair",
      "Bhanu Tokas",
      "Neel Shah",
      "Hannah Kerner"
    ],
    "author_ids": [],
    "abstract": "When we train models on biased ML datasets, they not only learn these biases\nbut can inflate them at test time - a phenomenon called bias amplification. To\nmeasure bias amplification in ML datasets, many co-occurrence-based metrics\nhave been proposed. Co-occurrence-based metrics are effective in measuring bias\namplification in simple problems like image classification. However, these\nmetrics are ineffective for complex problems like image captioning as they\ncannot capture the semantics of a caption. To measure bias amplification in\ncaptions, prior work introduced a predictability-based metric called Leakage in\nCaptioning (LIC). While LIC captures the semantics and context of captions, it\nhas limitations. LIC cannot identify the direction in which bias is amplified,\npoorly estimates dataset bias due to a weak vocabulary substitution strategy,\nand is highly sensitive to attacker models (a hyperparameter in\npredictability-based metrics). To overcome these issues, we propose Directional\nPredictability Amplification in Captioning (DPAC). DPAC measures directional\nbias amplification in captions, provides a better estimate of dataset bias\nusing an improved substitution strategy, and is less sensitive to attacker\nmodels. Our experiments on the COCO captioning dataset show how DPAC is the\nmost reliable metric to measure bias amplification in captions.",
    "published_date": "2025-03-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.07878v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.07853v1",
    "title": "Learning and Evaluating Hierarchical Feature Representations",
    "authors": [
      "Depanshu Sani",
      "Saket Anand"
    ],
    "author_ids": [],
    "abstract": "Hierarchy-aware representations ensure that the semantically closer classes\nare mapped closer in the feature space, thereby reducing the severity of\nmistakes while enabling consistent coarse-level class predictions. Towards this\nend, we propose a novel framework, Hierarchical Composition of Orthogonal\nSubspaces (Hier-COS), which learns to map deep feature embeddings into a vector\nspace that is, by design, consistent with the structure of a given taxonomy\ntree. Our approach augments neural network backbones with a simple\ntransformation module that maps learned discriminative features to subspaces\ndefined using a fixed orthogonal frame. This construction naturally improves\nthe severity of mistakes and promotes hierarchical consistency. Furthermore, we\nhighlight the fundamental limitations of existing hierarchical evaluation\nmetrics popularly used by the vision community and introduce a preference-based\nmetric, Hierarchically Ordered Preference Score (HOPS), to overcome these\nlimitations. We benchmark our method on multiple large and challenging datasets\nhaving deep label hierarchies (ranging from 3 - 12 levels) and compare with\nseveral baselines and SOTA. Through extensive experiments, we demonstrate that\nHier-COS achieves state-of-the-art hierarchical performance across all the\ndatasets while simultaneously beating top-1 accuracy in all but one case. We\nalso demonstrate the performance of a Vision Transformer (ViT) backbone and\nshow that learning a transformation module alone can map the learned features\nfrom a pre-trained ViT to Hier-COS and yield substantial performance benefits.",
    "published_date": "2025-03-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.07853v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.08720v1",
    "title": "AI for Just Work: Constructing Diverse Imaginations of AI beyond \"Replacing Humans\"",
    "authors": [
      "Weina Jin",
      "Nicholas Vincent",
      "Ghassan Hamarneh"
    ],
    "author_ids": [],
    "abstract": "The AI community usually focuses on \"how\" to develop AI techniques, but lacks\nthorough open discussions on \"why\" we develop AI. Lacking critical reflections\non the general visions and purposes of AI may make the community vulnerable to\nmanipulation. In this position paper, we explore the \"why\" question of AI. We\ndenote answers to the \"why\" question the imaginations of AI, which depict our\ngeneral visions, frames, and mindsets for the prospects of AI. We identify that\nthe prevailing vision in the AI community is largely a monoculture that\nemphasizes objectives such as replacing humans and improving productivity. Our\ncritical examination of this mainstream imagination highlights its underpinning\nand potentially unjust assumptions. We then call to diversify our collective\nimaginations of AI, embedding ethical assumptions from the outset in the\nimaginations of AI. To facilitate the community's pursuit of diverse\nimaginations, we demonstrate one process for constructing a new imagination of\n\"AI for just work,\" and showcase its application in the medical image synthesis\ntask to make it more ethical. We hope this work will help the AI community to\nopen dialogues with civil society on the visions and purposes of AI, and\ninspire more technical works and advocacy in pursuit of diverse and ethical\nimaginations to restore the value of AI for the public good.",
    "published_date": "2025-03-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.08720v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.07817v1",
    "title": "Group Fairness in Multi-Task Reinforcement Learning",
    "authors": [
      "Kefan Song",
      "Runnan Jiang",
      "Rohan Chandra",
      "Shangtong Zhang"
    ],
    "author_ids": [],
    "abstract": "This paper addresses a critical societal consideration in the application of\nReinforcement Learning (RL): ensuring equitable outcomes across different\ndemographic groups in multi-task settings. While previous work has explored\nfairness in single-task RL, many real-world applications are multi-task in\nnature and require policies to maintain fairness across all tasks. We introduce\na novel formulation of multi-task group fairness in RL and propose a\nconstrained optimization algorithm that explicitly enforces fairness\nconstraints across multiple tasks simultaneously. We have shown that our\nproposed algorithm does not violate fairness constraints with high probability\nand with sublinear regret in the finite-horizon episodic setting. Through\nexperiments in RiverSwim and MuJoCo environments, we demonstrate that our\napproach better ensures group fairness across multiple tasks compared to\nprevious methods that lack explicit multi-task fairness constraints in both the\nfinite-horizon setting and the infinite-horizon setting. Our results show that\nthe proposed algorithm achieves smaller fairness gaps while maintaining\ncomparable returns across different demographic groups and tasks, suggesting\nits potential for addressing fairness concerns in real-world multi-task RL\napplications.",
    "published_date": "2025-03-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.07817v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.07806v1",
    "title": "Towards Large Language Models that Benefit for All: Benchmarking Group Fairness in Reward Models",
    "authors": [
      "Kefan Song",
      "Jin Yao",
      "Runnan Jiang",
      "Rohan Chandra",
      "Shangtong Zhang"
    ],
    "author_ids": [],
    "abstract": "As Large Language Models (LLMs) become increasingly powerful and accessible\nto human users, ensuring fairness across diverse demographic groups, i.e.,\ngroup fairness, is a critical ethical concern. However, current fairness and\nbias research in LLMs is limited in two aspects. First, compared to traditional\ngroup fairness in machine learning classification, it requires that the\nnon-sensitive attributes, in this case, the prompt questions, be the same\nacross different groups. In many practical scenarios, different groups,\nhowever, may prefer different prompt questions and this requirement becomes\nimpractical. Second, it evaluates group fairness only for the LLM's final\noutput without identifying the source of possible bias. Namely, the bias in\nLLM's output can result from both the pretraining and the finetuning. For\nfinetuning, the bias can result from both the RLHF procedure and the learned\nreward model. Arguably, evaluating the group fairness of each component in the\nLLM pipeline could help develop better methods to mitigate the possible bias.\nRecognizing those two limitations, this work benchmarks the group fairness of\nlearned reward models. By using expert-written text from arXiv, we are able to\nbenchmark the group fairness of reward models without requiring the same prompt\nquestions across different demographic groups. Surprisingly, our results\ndemonstrate that all the evaluated reward models (e.g., Nemotron-4-340B-Reward,\nArmoRM-Llama3-8B-v0.1, and GRM-llama3-8B-sftreg) exhibit statistically\nsignificant group unfairness. We also observed that top-performing reward\nmodels (w.r.t. canonical performance metrics) tend to demonstrate better group\nfairness.",
    "published_date": "2025-03-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.07806v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.07775v1",
    "title": "Sublinear Algorithms for Wasserstein and Total Variation Distances: Applications to Fairness and Privacy Auditing",
    "authors": [
      "Debabrota Basu",
      "Debarshi Chanda"
    ],
    "author_ids": [],
    "abstract": "Resource-efficiently computing representations of probability distributions\nand the distances between them while only having access to the samples is a\nfundamental and useful problem across mathematical sciences. In this paper, we\npropose a generic algorithmic framework to estimate the PDF and CDF of any\nsub-Gaussian distribution while the samples from them arrive in a stream. We\ncompute mergeable summaries of distributions from the stream of samples that\nrequire sublinear space w.r.t. the number of observed samples. This allows us\nto estimate Wasserstein and Total Variation (TV) distances between any two\nsub-Gaussian distributions while samples arrive in streams and from multiple\nsources (e.g. federated learning). Our algorithms significantly improves on the\nexisting methods for distance estimation incurring super-linear time and linear\nspace complexities. In addition, we use the proposed estimators of Wasserstein\nand TV distances to audit the fairness and privacy of the ML algorithms. We\nempirically demonstrate the efficiency of the algorithms for estimating these\ndistances and auditing using both synthetic and real-world datasets.",
    "published_date": "2025-03-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.DS",
      "stat.CO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.07775v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.07703v1",
    "title": "Seedream 2.0: A Native Chinese-English Bilingual Image Generation Foundation Model",
    "authors": [
      "Lixue Gong",
      "Xiaoxia Hou",
      "Fanshi Li",
      "Liang Li",
      "Xiaochen Lian",
      "Fei Liu",
      "Liyang Liu",
      "Wei Liu",
      "Wei Lu",
      "Yichun Shi",
      "Shiqi Sun",
      "Yu Tian",
      "Zhi Tian",
      "Peng Wang",
      "Xun Wang",
      "Ye Wang",
      "Guofeng Wu",
      "Jie Wu",
      "Xin Xia",
      "Xuefeng Xiao",
      "Linjie Yang",
      "Zhonghua Zhai",
      "Xinyu Zhang",
      "Qi Zhang",
      "Yuwei Zhang",
      "Shijia Zhao",
      "Jianchao Yang",
      "Weilin Huang"
    ],
    "author_ids": [],
    "abstract": "Rapid advancement of diffusion models has catalyzed remarkable progress in\nthe field of image generation. However, prevalent models such as Flux, SD3.5\nand Midjourney, still grapple with issues like model bias, limited text\nrendering capabilities, and insufficient understanding of Chinese cultural\nnuances. To address these limitations, we present Seedream 2.0, a native\nChinese-English bilingual image generation foundation model that excels across\ndiverse dimensions, which adeptly manages text prompt in both Chinese and\nEnglish, supporting bilingual image generation and text rendering. We develop a\npowerful data system that facilitates knowledge integration, and a caption\nsystem that balances the accuracy and richness for image description.\nParticularly, Seedream is integrated with a self-developed bilingual large\nlanguage model as a text encoder, allowing it to learn native knowledge\ndirectly from massive data. This enable it to generate high-fidelity images\nwith accurate cultural nuances and aesthetic expressions described in either\nChinese or English. Beside, Glyph-Aligned ByT5 is applied for flexible\ncharacter-level text rendering, while a Scaled ROPE generalizes well to\nuntrained resolutions. Multi-phase post-training optimizations, including SFT\nand RLHF iterations, further improve the overall capability. Through extensive\nexperimentation, we demonstrate that Seedream 2.0 achieves state-of-the-art\nperformance across multiple aspects, including prompt-following, aesthetics,\ntext rendering, and structural correctness. Furthermore, Seedream 2.0 has been\noptimized through multiple RLHF iterations to closely align its output with\nhuman preferences, as revealed by its outstanding ELO score. In addition, it\ncan be readily adapted to an instruction-based image editing model, such as\nSeedEdit, with strong editing capability that balances instruction-following\nand image consistency.",
    "published_date": "2025-03-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.07703v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.07594v1",
    "title": "Scaffold with Stochastic Gradients: New Analysis with Linear Speed-Up",
    "authors": [
      "Paul Mangold",
      "Alain Durmus",
      "Aymeric Dieuleveut",
      "Eric Moulines"
    ],
    "author_ids": [],
    "abstract": "This paper proposes a novel analysis for the Scaffold algorithm, a popular\nmethod for dealing with data heterogeneity in federated learning. While its\nconvergence in deterministic settings--where local control variates mitigate\nclient drift--is well established, the impact of stochastic gradient updates on\nits performance is less understood. To address this problem, we first show that\nits global parameters and control variates define a Markov chain that converges\nto a stationary distribution in the Wasserstein distance. Leveraging this\nresult, we prove that Scaffold achieves linear speed-up in the number of\nclients up to higher-order terms in the step size. Nevertheless, our analysis\nreveals that Scaffold retains a higher-order bias, similar to FedAvg, that does\nnot decrease as the number of clients increases. This highlights opportunities\nfor developing improved stochastic federated learning algorithms",
    "published_date": "2025-03-10T00:00:00",
    "year": 2025,
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.07594v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.07586v2",
    "title": "Design as Hope: Reimagining Futures for Seemingly Doomed Problems",
    "authors": [
      "JaeWon Kim",
      "Jiaying \"Lizzy\" Liu",
      "Cassidy Pyle",
      "Sowmya Somanath",
      "Lindsay Popowski",
      "Hua Shen",
      "Casey Fiesler",
      "Gillian R. Hayes",
      "Alexis Hiniker",
      "Wendy Ju",
      "Florian \"Floyd\" Mueller",
      "Ahmer Arif",
      "Yasmine Kotturi"
    ],
    "author_ids": [],
    "abstract": "Design has the power to cultivate hope, especially in the face of seemingly\nintractable societal challenges. This one-day workshop explores how design\nmethodologies -- ranging from problem reframing to participatory, speculative,\nand critical design -- can empower research communities to drive meaningful\nreal-world changes. By aligning design thinking with hope theory -- framework\nof viewing hope as \"goal-directed,\" \"pathways,\" and \"agentic\" thinking\nprocesses -- we aim to examine how researchers can move beyond focusing on harm\nmitigation and instead reimagine alternative futures. Through hands-on\nactivities, participants will engage in problem reframing, develop a taxonomy\nof design methods related to hope, and explore how community-driven design\napproaches can sustain efforts toward societal and individual hope. The\nworkshop also interrogates the ethical and practical boundaries of leveraging\nhope in design research. By the end of the session, participants will leave\nwith concrete strategies for integrating a hopeful design approach into their\nresearch, as well as a network for ongoing collaboration. Ultimately, we\nposition hopeful design not just as a practical tool for action and\nproblem-solving but as a catalyst for cultivating resilience and envisioning\ntransformative futures.",
    "published_date": "2025-03-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.07586v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.07575v1",
    "title": "VisBias: Measuring Explicit and Implicit Social Biases in Vision Language Models",
    "authors": [
      "Jen-tse Huang",
      "Jiantong Qin",
      "Jianping Zhang",
      "Youliang Yuan",
      "Wenxuan Wang",
      "Jieyu Zhao"
    ],
    "author_ids": [],
    "abstract": "This research investigates both explicit and implicit social biases exhibited\nby Vision-Language Models (VLMs). The key distinction between these bias types\nlies in the level of awareness: explicit bias refers to conscious, intentional\nbiases, while implicit bias operates subconsciously. To analyze explicit bias,\nwe directly pose questions to VLMs related to gender and racial differences:\n(1) Multiple-choice questions based on a given image (e.g., \"What is the\neducation level of the person in the image?\") (2) Yes-No comparisons using two\nimages (e.g., \"Is the person in the first image more educated than the person\nin the second image?\") For implicit bias, we design tasks where VLMs assist\nusers but reveal biases through their responses: (1) Image description tasks:\nModels are asked to describe individuals in images, and we analyze disparities\nin textual cues across demographic groups. (2) Form completion tasks: Models\ndraft a personal information collection form with 20 attributes, and we examine\ncorrelations among selected attributes for potential biases. We evaluate\nGemini-1.5, GPT-4V, GPT-4o, LLaMA-3.2-Vision and LLaVA-v1.6. Our code and data\nare publicly available at https://github.com/uscnlp-lime/VisBias.",
    "published_date": "2025-03-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.07575v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.07691v1",
    "title": "Fair Text Classification via Transferable Representations",
    "authors": [
      "Thibaud Leteno",
      "Michael Perrot",
      "Charlotte Laclau",
      "Antoine Gourru",
      "Christophe Gravier"
    ],
    "author_ids": [],
    "abstract": "Group fairness is a central research topic in text classification, where\nreaching fair treatment between sensitive groups (e.g., women and men) remains\nan open challenge. We propose an approach that extends the use of the\nWasserstein Dependency Measure for learning unbiased neural text classifiers.\nGiven the challenge of distinguishing fair from unfair information in a text\nencoder, we draw inspiration from adversarial training by inducing independence\nbetween representations learned for the target label and those for a sensitive\nattribute. We further show that Domain Adaptation can be efficiently leveraged\nto remove the need for access to the sensitive attributes in the dataset we\ncure. We provide both theoretical and empirical evidence that our approach is\nwell-founded.",
    "published_date": "2025-03-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.07691v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.07690v1",
    "title": "Artificial Intelligence in Deliberation: The AI Penalty and the Emergence of a New Deliberative Divide",
    "authors": [
      "Andreas Jungherr",
      "Adrian Rauchfleisch"
    ],
    "author_ids": [],
    "abstract": "Digital deliberation has expanded democratic participation, yet challenges\nremain. This includes processing information at scale, moderating discussions,\nfact-checking, or attracting people to participate. Recent advances in\nartificial intelligence (AI) offer potential solutions, but public perceptions\nof AI's role in deliberation remain underexplored. Beyond efficiency,\ndemocratic deliberation is about voice and recognition. If AI is integrated\ninto deliberation, public trust, acceptance, and willingness to participate may\nbe affected. We conducted a preregistered survey experiment with a\nrepresentative sample in Germany (n=1850) to examine how information about\nAI-enabled deliberation influences willingness to participate and perceptions\nof deliberative quality. Respondents were randomly assigned to treatments that\nprovided them information about deliberative tasks facilitated by either AI or\nhumans. Our findings reveal a significant AI-penalty. Participants were less\nwilling to engage in AI-facilitated deliberation and rated its quality lower\nthan human-led formats. These effects were moderated by individual\npredispositions. Perceptions of AI's societal benefits and anthropomorphization\nof AI showed positive interaction effects on people's interest to participate\nin AI-enabled deliberative formats and positive quality assessments, while AI\nrisk assessments showed negative interactions with information about AI-enabled\ndeliberation. These results suggest AI-enabled deliberation faces substantial\npublic skepticism, potentially even introducing a new deliberative divide.\nUnlike traditional participation gaps based on education or demographics, this\ndivide is shaped by attitudes toward AI. As democratic engagement increasingly\nmoves online, ensuring AI's role in deliberation does not discourage\nparticipation or deepen inequalities will be a key challenge for future\nresearch and policy.",
    "published_date": "2025-03-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.07690v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.07511v1",
    "title": "PointVLA: Injecting the 3D World into Vision-Language-Action Models",
    "authors": [
      "Chengmeng Li",
      "Junjie Wen",
      "Yan Peng",
      "Yaxin Peng",
      "Feifei Feng",
      "Yichen Zhu"
    ],
    "author_ids": [],
    "abstract": "Vision-Language-Action (VLA) models excel at robotic tasks by leveraging\nlarge-scale 2D vision-language pretraining, but their reliance on RGB images\nlimits spatial reasoning critical for real-world interaction. Retraining these\nmodels with 3D data is computationally prohibitive, while discarding existing\n2D datasets wastes valuable resources. To bridge this gap, we propose PointVLA,\na framework that enhances pre-trained VLAs with point cloud inputs without\nrequiring retraining. Our method freezes the vanilla action expert and injects\n3D features via a lightweight modular block. To identify the most effective way\nof integrating point cloud representations, we conduct a skip-block analysis to\npinpoint less useful blocks in the vanilla action expert, ensuring that 3D\nfeatures are injected only into these blocks--minimizing disruption to\npre-trained representations.\n  Extensive experiments demonstrate that PointVLA outperforms state-of-the-art\n2D imitation learning methods, such as OpenVLA, Diffusion Policy and DexVLA,\nacross both simulated and real-world robotic tasks. Specifically, we highlight\nseveral key advantages of PointVLA enabled by point cloud integration: (1)\nFew-shot multi-tasking, where PointVLA successfully performs four different\ntasks using only 20 demonstrations each; (2) Real-vs-photo discrimination,\nwhere PointVLA distinguishes real objects from their images, leveraging 3D\nworld knowledge to improve safety and reliability; (3) Height adaptability,\nUnlike conventional 2D imitation learning methods, PointVLA enables robots to\nadapt to objects at varying table height that unseen in train data.\nFurthermore, PointVLA achieves strong performance in long-horizon tasks, such\nas picking and packing objects from a moving conveyor belt, showcasing its\nability to generalize across complex, dynamic environments.",
    "published_date": "2025-03-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.07511v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.07510v1",
    "title": "Sometimes the Model doth Preach: Quantifying Religious Bias in Open LLMs through Demographic Analysis in Asian Nations",
    "authors": [
      "Hari Shankar",
      "Vedanta S P",
      "Tejas Cavale",
      "Ponnurangam Kumaraguru",
      "Abhijnan Chakraborty"
    ],
    "author_ids": [],
    "abstract": "Large Language Models (LLMs) are capable of generating opinions and\npropagating bias unknowingly, originating from unrepresentative and non-diverse\ndata collection. Prior research has analysed these opinions with respect to the\nWest, particularly the United States. However, insights thus produced may not\nbe generalized in non-Western populations. With the widespread usage of LLM\nsystems by users across several different walks of life, the cultural\nsensitivity of each generated output is of crucial interest. Our work proposes\na novel method that quantitatively analyzes the opinions generated by LLMs,\nimproving on previous work with regards to extracting the social demographics\nof the models. Our method measures the distance from an LLM's response to\nsurvey respondents, through Hamming Distance, to infer the demographic\ncharacteristics reflected in the model's outputs. We evaluate modern, open LLMs\nsuch as Llama and Mistral on surveys conducted in various global south\ncountries, with a focus on India and other Asian nations, specifically\nassessing the model's performance on surveys related to religious tolerance and\nidentity. Our analysis reveals that most open LLMs match a single homogeneous\nprofile, varying across different countries/territories, which in turn raises\nquestions about the risks of LLMs promoting a hegemonic worldview, and\nundermining perspectives of different minorities. Our framework may also be\nuseful for future research investigating the complex intersection between\ntraining data, model architecture, and the resulting biases reflected in LLM\noutputs, particularly concerning sensitive topics like religious tolerance and\nidentity.",
    "published_date": "2025-03-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.07510v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.07501v1",
    "title": "Trustworthy Machine Learning via Memorization and the Granular Long-Tail: A Survey on Interactions, Tradeoffs, and Beyond",
    "authors": [
      "Qiongxiu Li",
      "Xiaoyu Luo",
      "Yiyi Chen",
      "Johannes Bjerva"
    ],
    "author_ids": [],
    "abstract": "The role of memorization in machine learning (ML) has garnered significant\nattention, particularly as modern models are empirically observed to memorize\nfragments of training data. Previous theoretical analyses, such as Feldman's\nseminal work, attribute memorization to the prevalence of long-tail\ndistributions in training data, proving it unavoidable for samples that lie in\nthe tail of the distribution. However, the intersection of memorization and\ntrustworthy ML research reveals critical gaps. While prior research in\nmemorization in trustworthy ML has solely focused on class imbalance, recent\nwork starts to differentiate class-level rarity from atypical samples, which\nare valid and rare intra-class instances. However, a critical research gap\nremains: current frameworks conflate atypical samples with noisy and erroneous\ndata, neglecting their divergent impacts on fairness, robustness, and privacy.\nIn this work, we conduct a thorough survey of existing research and their\nfindings on trustworthy ML and the role of memorization. More and beyond, we\nidentify and highlight uncharted gaps and propose new revenues in this research\ndirection. Since existing theoretical and empirical analyses lack the nuances\nto disentangle memorization's duality as both a necessity and a liability, we\nformalize three-level long-tail granularity - class imbalance, atypicality, and\nnoise - to reveal how current frameworks misapply these levels, perpetuating\nflawed solutions. By systematizing this granularity, we draw a roadmap for\nfuture research. Trustworthy ML must reconcile the nuanced trade-offs between\nmemorizing atypicality for fairness assurance and suppressing noise for\nrobustness and privacy guarantee. Redefining memorization via this granularity\nreshapes the theoretical foundation for trustworthy ML, and further affords an\nempirical prerequisite for models that align performance with societal trust.",
    "published_date": "2025-03-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.07501v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.07411v1",
    "title": "PER-DPP Sampling Framework and Its Application in Path Planning",
    "authors": [
      "Junzhe Wang"
    ],
    "author_ids": [],
    "abstract": "Autonomous navigation in intelligent mobile systems represents a core\nresearch focus within artificial intelligence-driven robotics. Contemporary\npath planning approaches face constraints in dynamic environmental\nresponsiveness and multi-objective task scalability, limiting their capacity to\naddress growing intelligent operation requirements. Decision-centric\nreinforcement learning frameworks, capitalizing on their unique strengths in\nadaptive environmental interaction and self-optimization, have gained\nprominence in advanced control system research. This investigation introduces\nmethodological improvements to address sample homogeneity challenges in\nreinforcement learning experience replay mechanisms. By incorporating\ndeterminant point processes (DPP) for diversity assessment, we develop a\ndual-criteria sampling framework with adaptive selection protocols. This\napproach resolves representation bias in conventional prioritized experience\nreplay (PER) systems while preserving algorithmic interoperability, offering\nimproved decision optimization for dynamic operational scenarios. Key\ncontributions comprise: Develop a hybrid sampling paradigm (PER-DPP) combining\npriority sequencing with diversity maximization.Based on this,create an\nintegrated optimization scheme (PER-DPP-Elastic DQN) merging diversity-aware\nsampling with adaptive step-size regulation. Comparative simulations in 2D\nnavigation scenarios demonstrate that the elastic step-size component\ntemporarily delays initial convergence speed but synergistically enhances\nfinal-stage optimization with PER-DPP integration. The synthesized method\ngenerates navigation paths with optimized length efficiency and directional\nstability.",
    "published_date": "2025-03-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.RO",
      "cs.LG",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.07411v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.07377v1",
    "title": "Process-Supervised LLM Recommenders via Flow-guided Tuning",
    "authors": [
      "Chongming Gao",
      "Mengyao Gao",
      "Chenxiao Fan",
      "Shuai Yuan",
      "Wentao Shi",
      "Xiangnan He"
    ],
    "author_ids": [],
    "abstract": "While large language models (LLMs) are increasingly adapted for\nrecommendation systems via supervised fine-tuning (SFT), this approach\namplifies popularity bias due to its likelihood maximization objective,\ncompromising recommendation diversity and fairness. To address this, we present\nFlow-guided fine-tuning recommender (Flower), which replaces SFT with a\nGenerative Flow Network (GFlowNet) framework that enacts process supervision\nthrough token-level reward propagation. Flower's key innovation lies in\ndecomposing item-level rewards into constituent token rewards, enabling direct\nalignment between token generation probabilities and their reward signals. This\nmechanism achieves three critical advancements: (1) popularity bias mitigation\nand fairness enhancement through empirical distribution matching, (2)\npreservation of diversity through GFlowNet's proportional sampling, and (3)\nflexible integration of personalized preferences via adaptable token rewards.\nExperiments demonstrate Flower's superior distribution-fitting capability and\nits significant advantages over traditional SFT in terms of fairness,\ndiversity, and accuracy, highlighting its potential to improve LLM-based\nrecommendation systems. The implementation is available via\nhttps://github.com/Mr-Peach0301/Flower",
    "published_date": "2025-03-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.07377v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.07364v1",
    "title": "Artificial Utopia: Simulation and Intelligent Agents for a Democratised Future",
    "authors": [
      "Yannick Oswald"
    ],
    "author_ids": [],
    "abstract": "Prevailing top-down systems in politics and economics struggle to keep pace\nwith the pressing challenges of the 21st century, such as climate change,\nsocial inequality and conflict. Bottom-up democratisation and participatory\napproaches in politics and economics are increasingly seen as promising\nalternatives to confront and overcome these issues, often with utopian\novertones, as proponents believe they may dramatically reshape political,\nsocial and ecological futures for the better and in contrast to contemporary\nauthoritarian tendencies across various countries. Institutional specifics and\nthe associated collective human behavior or culture remains little understood\nand debated, however. In this article, I propose a novel research agenda\nfocusing on utopian democratisation efforts with formal and computational\nmethods as well as with artificial intelligence - I call this agenda Artificial\nUtopia. Artificial Utopias provide safe testing grounds for new political ideas\nand economic policies in-silico with reduced risk of negative consequences as\ncompared to testing ideas in real-world contexts. An increasing number of\nadvanced simulation and intelligence methods, that aim at representing human\ncognition and collective decision-making in more realistic ways, could benefit\nthis process. This includes agent-based modelling, reinforcement learning,\nlarge language models and more. I clarify what some of these simulation\napproaches can contribute to the study of Artificial Utopias with the help of\ntwo institutional examples: the citizen assembly and the democratic firm.",
    "published_date": "2025-03-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.07364v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.07326v1",
    "title": "AI Biases as Asymmetries: A Review to Guide Practice",
    "authors": [
      "Gabriella Waters",
      "Phillip Honenberger"
    ],
    "author_ids": [],
    "abstract": "The understanding of bias in AI is currently undergoing a revolution.\nInitially understood as errors or flaws, biases are increasingly recognized as\nintegral to AI systems and sometimes preferable to less biased alternatives. In\nthis paper, we review the reasons for this changed understanding and provide\nnew guidance on two questions: First, how should we think about and measure\nbiases in AI systems, consistent with the new understanding? Second, what kinds\nof bias in an AI system should we accept or even amplify, and what kinds should\nwe minimize or eliminate, and why? The key to answering both questions, we\nargue, is to understand biases as \"violations of a symmetry standard\"\n(following Kelly). We distinguish three main types of asymmetry in AI\nsystems-error biases, inequality biases, and process biases-and highlight\nplaces in the pipeline of AI development and application where bias of each\ntype is likely to be good, bad, or inevitable.",
    "published_date": "2025-03-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.07326v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.07313v1",
    "title": "The influence of missing data mechanisms and simple missing data handling techniques on fairness",
    "authors": [
      "Aeysha Bhatti",
      "Trudie Sandrock",
      "Johane Nienkemper-Swanepoel"
    ],
    "author_ids": [],
    "abstract": "Fairness of machine learning algorithms is receiving increasing attention, as\nsuch algorithms permeate the day-to-day aspects of our lives. One way in which\nbias can manifest in a dataset is through missing values. If data are missing,\nthese data are often assumed to be missing completely randomly; in reality the\npropensity of data being missing is often tied to the demographic\ncharacteristics of individuals. There is limited research into how missing\nvalues and the handling thereof can impact the fairness of an algorithm. Most\nresearchers either apply listwise deletion or tend to use the simpler methods\nof imputation (e.g. mean or mode) compared to the more advanced ones (e.g.\nmultiple imputation); we therefore study the impact of the simpler methods on\nthe fairness of algorithms. The starting point of the study is the mechanism of\nmissingness, leading into how the missing data are processed and finally how\nthis impacts fairness. Three popular datasets in the field of fairness are\namputed in a simulation study. The results show that under certain scenarios\nthe impact on fairness can be pronounced when the missingness mechanism is\nmissing at random. Furthermore, elementary missing data handling techniques\nlike listwise deletion and mode imputation can lead to higher fairness compared\nto more complex imputation methods like k-nearest neighbour imputation, albeit\noften at the cost of lower accuracy.",
    "published_date": "2025-03-10T00:00:00",
    "year": 2025,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.07313v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.07306v1",
    "title": "Benchmarking Chinese Medical LLMs: A Medbench-based Analysis of Performance Gaps and Hierarchical Optimization Strategies",
    "authors": [
      "Luyi Jiang",
      "Jiayuan Chen",
      "Lu Lu",
      "Xinwei Peng",
      "Lihao Liu",
      "Junjun He",
      "Jie Xu"
    ],
    "author_ids": [],
    "abstract": "The evaluation and improvement of medical large language models (LLMs) are\ncritical for their real-world deployment, particularly in ensuring accuracy,\nsafety, and ethical alignment. Existing frameworks inadequately dissect\ndomain-specific error patterns or address cross-modal challenges. This study\nintroduces a granular error taxonomy through systematic analysis of top 10\nmodels on MedBench, categorizing incorrect responses into eight types:\nOmissions, Hallucination, Format Mismatch, Causal Reasoning Deficiency,\nContextual Inconsistency, Unanswered, Output Error, and Deficiency in Medical\nLanguage Generation. Evaluation of 10 leading models reveals vulnerabilities:\ndespite achieving 0.86 accuracy in medical knowledge recall, critical reasoning\ntasks show 96.3% omission, while safety ethics evaluations expose alarming\ninconsistency (robustness score: 0.79) under option shuffled. Our analysis\nuncovers systemic weaknesses in knowledge boundary enforcement and multi-step\nreasoning. To address these, we propose a tiered optimization strategy spanning\nfour levels, from prompt engineering and knowledge-augmented retrieval to\nhybrid neuro-symbolic architectures and causal reasoning frameworks. This work\nestablishes an actionable roadmap for developing clinically robust LLMs while\nredefining evaluation paradigms through error-driven insights, ultimately\nadvancing the safety and trustworthiness of AI in high-stakes medical\nenvironments.",
    "published_date": "2025-03-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.07306v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.07302v1",
    "title": "When Selection Meets Intervention: Additional Complexities in Causal Discovery",
    "authors": [
      "Haoyue Dai",
      "Ignavier Ng",
      "Jianle Sun",
      "Zeyu Tang",
      "Gongxu Luo",
      "Xinshuai Dong",
      "Peter Spirtes",
      "Kun Zhang"
    ],
    "author_ids": [],
    "abstract": "We address the common yet often-overlooked selection bias in interventional\nstudies, where subjects are selectively enrolled into experiments. For\ninstance, participants in a drug trial are usually patients of the relevant\ndisease; A/B tests on mobile applications target existing users only, and gene\nperturbation studies typically focus on specific cell types, such as cancer\ncells. Ignoring this bias leads to incorrect causal discovery results. Even\nwhen recognized, the existing paradigm for interventional causal discovery\nstill fails to address it. This is because subtle differences in when and where\ninterventions happen can lead to significantly different statistical patterns.\nWe capture this dynamic by introducing a graphical model that explicitly\naccounts for both the observed world (where interventions are applied) and the\ncounterfactual world (where selection occurs while interventions have not been\napplied). We characterize the Markov property of the model, and propose a\nprovably sound algorithm to identify causal relations as well as selection\nmechanisms up to the equivalence class, from data with soft interventions and\nunknown targets. Through synthetic and real-world experiments, we demonstrate\nthat our algorithm effectively identifies true causal relations despite the\npresence of selection bias.",
    "published_date": "2025-03-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.07302v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.07685v1",
    "title": "Ways of Seeing, and Selling, AI Art",
    "authors": [
      "Imke van Heerden"
    ],
    "author_ids": [],
    "abstract": "In early 2025, Augmented Intelligence - Christie's first AI art auction -\ndrew criticism for showcasing a controversial genre. Amid wider legal\nuncertainty, artists voiced concerns over data mining practices, notably with\nrespect to copyright. The backlash could be viewed as a microcosm of AI's\ncontested position in the creative economy. Touching on the auction's\npresentation, reception, and results, this paper explores how, among social\ndissonance, machine learning finds its place in the artworld. Foregrounding\nresponsible innovation, the paper provides a balanced perspective that\nchampions creators' rights and brings nuance to this polarised debate. With a\nfocus on exhibition design, it centres framing, which refers to the way a piece\nis presented to influence consumer perception. Context plays a central role in\nshaping our understanding of how good, valuable, and even ethical an artwork\nis. In this regard, Augmented Intelligence situates AI art within a\nsurprisingly traditional framework, leveraging hallmarks of \"high art\" to\nestablish the genre's cultural credibility. Generative AI has a clear economic\ndimension, converging questions of artistic merit with those of monetary worth.\nScholarship on ways of seeing, or framing, could substantively inform the\ninterpretation and evaluation of creative outputs, including assessments of\ntheir aesthetic and commercial value.",
    "published_date": "2025-03-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.07685v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.10671v1",
    "title": "Identifying Non-Replicable Social Science Studies with Language Models",
    "authors": [
      "Denitsa Saynova",
      "Kajsa Hansson",
      "Bastiaan Bruinsma",
      "Annika Fredén",
      "Moa Johansson"
    ],
    "author_ids": [],
    "abstract": "In this study, we investigate whether LLMs can be used to indicate if a study\nin the behavioural social sciences is replicable. Using a dataset of 14\npreviously replicated studies (9 successful, 5 unsuccessful), we evaluate the\nability of both open-source (Llama 3 8B, Qwen 2 7B, Mistral 7B) and proprietary\n(GPT-4o) instruction-tuned LLMs to discriminate between replicable and\nnon-replicable findings. We use LLMs to generate synthetic samples of responses\nfrom behavioural studies and estimate whether the measured effects support the\noriginal findings. When compared with human replication results for these\nstudies, we achieve F1 values of up to $77\\%$ with Mistral 7B, $67\\%$ with\nGPT-4o and Llama 3 8B, and $55\\%$ with Qwen 2 7B, suggesting their potential\nfor this task. We also analyse how effect size calculations are affected by\nsampling temperature and find that low variance (due to temperature) leads to\nbiased effect estimates.",
    "published_date": "2025-03-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.10671v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.07158v4",
    "title": "Generative AI in Transportation Planning: A Survey",
    "authors": [
      "Longchao Da",
      "Tiejin Chen",
      "Zhuoheng Li",
      "Shreyas Bachiraju",
      "Huaiyuan Yao",
      "Li Li",
      "Yushun Dong",
      "Xiyang Hu",
      "Zhengzhong Tu",
      "Dongjie Wang",
      "Yue Zhao",
      "Xuanyu",
      "Zhou",
      "Ram Pendyala",
      "Benjamin Stabler",
      "Yezhou Yang",
      "Xuesong Zhou",
      "Hua Wei"
    ],
    "author_ids": [],
    "abstract": "The integration of generative artificial intelligence (GenAI) into\ntransportation planning has the potential to revolutionize tasks such as demand\nforecasting, infrastructure design, policy evaluation, and traffic simulation.\nHowever, there is a critical need for a systematic framework to guide the\nadoption of GenAI in this interdisciplinary domain. In this survey, we, a\nmultidisciplinary team of researchers spanning computer science and\ntransportation engineering, present the first comprehensive framework for\nleveraging GenAI in transportation planning. Specifically, we introduce a new\ntaxonomy that categorizes existing applications and methodologies into two\nperspectives: transportation planning tasks and computational techniques. From\nthe transportation planning perspective, we examine the role of GenAI in\nautomating descriptive, predictive, generative, simulation, and explainable\ntasks to enhance mobility systems. From the computational perspective, we\ndetail advancements in data preparation, domain-specific fine-tuning, and\ninference strategies, such as retrieval-augmented generation and zero-shot\nlearning tailored to transportation applications. Additionally, we address\ncritical challenges, including data scarcity, explainability, bias mitigation,\nand the development of domain-specific evaluation frameworks that align with\ntransportation goals like sustainability, equity, and system efficiency. This\nsurvey aims to bridge the gap between traditional transportation planning\nmethodologies and modern AI techniques, fostering collaboration and innovation.\nBy addressing these challenges and opportunities, we seek to inspire future\nresearch that ensures ethical, equitable, and impactful use of generative AI in\ntransportation planning.",
    "published_date": "2025-03-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "68T99, 90B06",
      "I.2.6; I.2.8; I.6.3; J.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.07158v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.07157v2",
    "title": "MIRAM: Masked Image Reconstruction Across Multiple Scales for Breast Lesion Risk Prediction",
    "authors": [
      "Hung Q. Vo",
      "Pengyu Yuan",
      "Zheng Yin",
      "Kelvin K. Wong",
      "Chika F. Ezeana",
      "Son T. Ly",
      "Stephen T. C. Wong",
      "Hien V. Nguyen"
    ],
    "author_ids": [],
    "abstract": "Self-supervised learning (SSL) has garnered substantial interest within the\nmachine learning and computer vision communities. Two prominent approaches in\nSSL include contrastive-based learning and self-distillation utilizing cropping\naugmentation. Lately, masked image modeling (MIM) has emerged as a more potent\nSSL technique, employing image inpainting as a pretext task. MIM creates a\nstrong inductive bias toward meaningful spatial and semantic understanding.\nThis has opened up new opportunities for SSL to contribute not only to\nclassification tasks but also to more complex applications like object\ndetection and image segmentation. Building upon this progress, our research\npaper introduces a scalable and practical SSL approach centered around more\nchallenging pretext tasks that facilitate the acquisition of robust features.\nSpecifically, we leverage multi-scale image reconstruction from randomly masked\ninput images as the foundation for feature learning. Our hypothesis posits that\nreconstructing high-resolution images enables the model to attend to finer\nspatial details, particularly beneficial for discerning subtle intricacies\nwithin medical images. The proposed SSL features help improve classification\nperformance on the Curated Breast Imaging Subset of Digital Database for\nScreening Mammography (CBIS-DDSM) dataset. In pathology classification, our\nmethod demonstrates a 3\\% increase in average precision (AP) and a 1\\% increase\nin the area under the receiver operating characteristic curve (AUC) when\ncompared to state-of-the-art (SOTA) algorithms. Moreover, in mass margins\nclassification, our approach achieves a 4\\% increase in AP and a 2\\% increase\nin AUC.",
    "published_date": "2025-03-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.07157v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.07149v1",
    "title": "Optimal Operation of Renewable Energy Communities under Demand Response Programs",
    "authors": [
      "Gianni Bianchini",
      "Marco Casini",
      "Milad Gholami"
    ],
    "author_ids": [],
    "abstract": "Within the context of renewable energy communities, this paper focuses on\noptimal operation of producers equipped with energy storage systems in the\npresence of demand response. A novel strategy for optimal scheduling of the\nstorage systems of the community members under price-volume demand response\nprograms, is devised. The underlying optimization problem is designed as a\nlow-complexity mixed-integer linear program that scales well with the community\nsize. Once the optimal solution is found, an algorithm for distributing the\ndemand response rewards is introduced in order to guarantee fairness among\nparticipants. The proposed approach ensures increased benefits for producers\njoining a community compared to standalone operation.",
    "published_date": "2025-03-10T00:00:00",
    "year": 2025,
    "categories": [
      "math.OC",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.07149v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.07120v1",
    "title": "Exposure Bias Reduction for Enhancing Diffusion Transformer Feature Caching",
    "authors": [
      "Zhen Zou",
      "Hu Yu",
      "Jie Xiao",
      "Feng Zhao"
    ],
    "author_ids": [],
    "abstract": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis problem, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing the impact of caching on the generation of intermediate processes. So\nthe lack of exploration provides us with room for analysis and improvement. In\nthis paper, we analyze the impact of caching on the SNR of the diffusion\nprocess and discern that feature caching intensifies the denoising procedure,\nand we further identify this as a more severe exposure bias issue. Drawing on\nthis insight, we introduce EB-Cache, a joint cache strategy that aligns the\nNon-exposure bias (which gives us a higher performance ceiling) diffusion\nprocess. Our approach incorporates a comprehensive understanding of caching\nmechanisms and offers a novel perspective on leveraging caches to expedite\ndiffusion processes. Empirical results indicate that EB-Cache optimizes model\nperformance while concurrently facilitating acceleration. Specifically, in the\n50-step generation process, EB-Cache achieves 1.49$\\times$ acceleration with\n0.63 FID reduction from 3.69, surpassing prior acceleration methods. Code will\nbe available at\n\\href{https://github.com/aSleepyTree/EB-Cache}{https://github.com/aSleepyTree/EB-Cache}.",
    "published_date": "2025-03-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.07120v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.07066v1",
    "title": "You Only Debias Once: Towards Flexible Accuracy-Fairness Trade-offs at Inference Time",
    "authors": [
      "Xiaotian Han",
      "Tianlong Chen",
      "Kaixiong Zhou",
      "Zhimeng Jiang",
      "Zhangyang Wang",
      "Xia Hu"
    ],
    "author_ids": [],
    "abstract": "Deep neural networks are prone to various bias issues, jeopardizing their\napplications for high-stake decision-making. Existing fairness methods\ntypically offer a fixed accuracy-fairness trade-off, since the weight of the\nwell-trained model is a fixed point (fairness-optimum) in the weight space.\nNevertheless, more flexible accuracy-fairness trade-offs at inference time are\npractically desired since: 1) stakes of the same downstream task can vary for\ndifferent individuals, and 2) different regions have diverse laws or\nregularization for fairness. If using the previous fairness methods, we have to\ntrain multiple models, each offering a specific level of accuracy-fairness\ntrade-off. This is often computationally expensive, time-consuming, and\ndifficult to deploy, making it less practical for real-world applications. To\naddress this problem, we propose You Only Debias Once (YODO) to achieve in-situ\nflexible accuracy-fairness trade-offs at inference time, using a single model\nthat trained only once. Instead of pursuing one individual fixed point\n(fairness-optimum) in the weight space, we aim to find a \"line\" in the weight\nspace that connects the accuracy-optimum and fairness-optimum points using a\nsingle model. Points (models) on this line implement varying levels of\naccuracy-fairness trade-offs. At inference time, by manually selecting the\nspecific position of the learned \"line\", our proposed method can achieve\narbitrary accuracy-fairness trade-offs for different end-users and scenarios.\nExperimental results on tabular and image datasets show that YODO achieves\nflexible trade-offs between model accuracy and fairness, at ultra-low\noverheads. For example, if we need $100$ levels of trade-off on the \\acse\ndataset, YODO takes $3.53$ seconds while training $100$ fixed models consumes\n$425$ seconds. The code is available at https://github.com/ahxt/yodo.",
    "published_date": "2025-03-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.07066v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.07008v1",
    "title": "SDFA: Structure Aware Discriminative Feature Aggregation for Efficient Human Fall Detection in Video",
    "authors": [
      "Sania Zahan",
      "Ghulam Mubashar Hassan",
      "Ajmal Mian"
    ],
    "author_ids": [],
    "abstract": "Older people are susceptible to fall due to instability in posture and\ndeteriorating health. Immediate access to medical support can greatly reduce\nrepercussions. Hence, there is an increasing interest in automated fall\ndetection, often incorporated into a smart healthcare system to provide better\nmonitoring. Existing systems focus on wearable devices which are inconvenient\nor video monitoring which has privacy concerns. Moreover, these systems provide\na limited perspective of their generalization ability as they are tested on\ndatasets containing few activities that have wide disparity in the action space\nand are easy to differentiate. Complex daily life scenarios pose much greater\nchallenges with activities that overlap in action spaces due to similar posture\nor motion. To overcome these limitations, we propose a fall detection model,\ncoined SDFA, based on human skeletons extracted from low-resolution videos. The\nuse of skeleton data ensures privacy and low-resolution videos ensures low\nhardware and computational cost. Our model captures discriminative structural\ndisplacements and motion trends using unified joint and motion features\nprojected onto a shared high dimensional space. Particularly, the use of\nseparable convolution combined with a powerful GCN architecture provides\nimproved performance. Extensive experiments on five large-scale datasets with a\nwide range of evaluation settings show that our model achieves competitive\nperformance with extremely low computational complexity and runs faster than\nexisting models.",
    "published_date": "2025-03-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.07008v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.07003v1",
    "title": "Large Language Models Often Say One Thing and Do Another",
    "authors": [
      "Ruoxi Xu",
      "Hongyu Lin",
      "Xianpei Han",
      "Jia Zheng",
      "Weixiang Zhou",
      "Le Sun",
      "Yingfei Sun"
    ],
    "author_ids": [],
    "abstract": "As large language models (LLMs) increasingly become central to various\napplications and interact with diverse user populations, ensuring their\nreliable and consistent performance is becoming more important. This paper\nexplores a critical issue in assessing the reliability of LLMs: the consistency\nbetween their words and deeds. To quantitatively explore this consistency, we\ndeveloped a novel evaluation benchmark called the Words and Deeds Consistency\nTest (WDCT). The benchmark establishes a strict correspondence between\nword-based and deed-based questions across different domains, including opinion\nvs. action, non-ethical value vs. action, ethical value vs. action, and theory\nvs. application. The evaluation results reveal a widespread inconsistency\nbetween words and deeds across different LLMs and domains. Subsequently, we\nconducted experiments with either word alignment or deed alignment to observe\ntheir impact on the other aspect. The experimental results indicate that\nalignment only on words or deeds poorly and unpredictably influences the other\naspect. This supports our hypothesis that the underlying knowledge guiding\nLLMs' word or deed choices is not contained within a unified space.",
    "published_date": "2025-03-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.07003v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.06987v1",
    "title": "Social Bias Benchmark for Generation: A Comparison of Generation and QA-Based Evaluations",
    "authors": [
      "Jiho Jin",
      "Woosung Kang",
      "Junho Myung",
      "Alice Oh"
    ],
    "author_ids": [],
    "abstract": "Measuring social bias in large language models (LLMs) is crucial, but\nexisting bias evaluation methods struggle to assess bias in long-form\ngeneration. We propose a Bias Benchmark for Generation (BBG), an adaptation of\nthe Bias Benchmark for QA (BBQ), designed to evaluate social bias in long-form\ngeneration by having LLMs generate continuations of story prompts. Building our\nbenchmark in English and Korean, we measure the probability of neutral and\nbiased generations across ten LLMs. We also compare our long-form story\ngeneration evaluation results with multiple-choice BBQ evaluation, showing that\nthe two approaches produce inconsistent results.",
    "published_date": "2025-03-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.06987v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.06974v1",
    "title": "Asymmetric Visual Semantic Embedding Framework for Efficient Vision-Language Alignment",
    "authors": [
      "Yang Liu",
      "Mengyuan Liu",
      "Shudong Huang",
      "Jiancheng Lv"
    ],
    "author_ids": [],
    "abstract": "Learning visual semantic similarity is a critical challenge in bridging the\ngap between images and texts. However, there exist inherent variations between\nvision and language data, such as information density, i.e., images can contain\ntextual information from multiple different views, which makes it difficult to\ncompute the similarity between these two modalities accurately and efficiently.\nIn this paper, we propose a novel framework called Asymmetric Visual Semantic\nEmbedding (AVSE) to dynamically select features from various regions of images\ntailored to different textual inputs for similarity calculation. To capture\ninformation from different views in the image, we design a radial bias sampling\nmodule to sample image patches and obtain image features from various views,\nFurthermore, AVSE introduces a novel module for efficient computation of visual\nsemantic similarity between asymmetric image and text embeddings. Central to\nthis module is the presumption of foundational semantic units within the\nembeddings, denoted as ``meta-semantic embeddings.\" It segments all embeddings\ninto meta-semantic embeddings with the same dimension and calculates visual\nsemantic similarity by finding the optimal match of meta-semantic embeddings of\ntwo modalities. Our proposed AVSE model is extensively evaluated on the\nlarge-scale MS-COCO and Flickr30K datasets, demonstrating its superiority over\nrecent state-of-the-art methods.",
    "published_date": "2025-03-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.06974v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.06920v2",
    "title": "AlignPxtr: Aligning Predicted Behavior Distributions for Bias-Free Video Recommendations",
    "authors": [
      "Chengzhi Lin",
      "Chuyuan Wang",
      "Annan Xie",
      "Wuhong Wang",
      "Ziye Zhang",
      "Canguang Ruan",
      "Yuancai Huang",
      "Yongqi Liu"
    ],
    "author_ids": [],
    "abstract": "In video recommendation systems, user behaviors such as watch time, likes,\nand follows are commonly used to infer user interest. However, these behaviors\nare influenced by various biases, including duration bias, demographic biases,\nand content category biases, which obscure true user preferences. In this\npaper, we hypothesize that biases and user interest are independent of each\nother. Based on this assumption, we propose a novel method that aligns\npredicted behavior distributions across different bias conditions using\nquantile mapping, theoretically guaranteeing zero mutual information between\nbias variables and the true user interest. By explicitly modeling the\nconditional distributions of user behaviors under different biases and mapping\nthese behaviors to quantiles, we effectively decouple user interest from the\nconfounding effects of various biases. Our approach uniquely handles both\ncontinuous signals (e.g., watch time) and discrete signals (e.g., likes,\ncomments), while simultaneously addressing multiple bias dimensions.\nAdditionally, we introduce a computationally efficient mean alignment\nalternative technique for practical real-time inference in large-scale systems.\nWe validate our method through online A/B testing on two major video platforms:\nKuaishou Lite and Kuaishou. The results demonstrate significant improvements in\nuser engagement and retention, with \\textbf{cumulative lifts of 0.267\\% and\n0.115\\% in active days, and 1.102\\% and 0.131\\% in average app usage time},\nrespectively. The results demonstrate that our approach consistently achieves\nsignificant improvements in long-term user retention and substantial gains in\naverage app usage time across different platforms. Our core code will be\npublised at https://github.com/justopit/CQE.",
    "published_date": "2025-03-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.06920v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.06825v1",
    "title": "Recursive Estimation for Dynamical Systems with Measurement Bias, Outliers and Constraints",
    "authors": [
      "Krishan Mohan Nagpal"
    ],
    "author_ids": [],
    "abstract": "This paper describes recursive algorithms for state estimation of linear\ndynamical systems when measurements are noisy with unknown bias and/or\noutliers. For situations with noisy and biased measurements, algorithms are\nproposed that minimize $\\epsilon$ insensitive loss function. In this approach\nwhich is often used in Support Vector Machines, small errors are ignored making\nthe algorithm less sensitive to measurement bias. Apart from $\\epsilon$\ninsensitive quadratic loss function, estimation algorithms are also presented\nfor $\\epsilon$ insensitive Huber M loss function which provides good\nperformance in presence of both small noises as well as outliers. The advantage\nof Huber cost function based estimator in presence of outliers is due to the\nfact the error penalty function switches from quadratic to linear for errors\nbeyond a certain threshold. For both objective functions, estimation algorithms\nare extended to cases when there are additional constraints on states and\nexogenous signals such as known range of some states or exogenous signals or\nmeasurement noises. Interestingly, the filtering algorithms are recursive and\nstructurally similar to Kalman filter with the main difference being that the\nupdates based on the new measurement (\"innovation term\") are based on solution\nof a quadratic optimization problem with linear constraints.",
    "published_date": "2025-03-10T00:00:00",
    "year": 2025,
    "categories": [
      "eess.SY",
      "cs.SY",
      "math.DS",
      "math.OC",
      "49, 93"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.06825v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.06792v1",
    "title": "On the Mutual Influence of Gender and Occupation in LLM Representations",
    "authors": [
      "Haozhe An",
      "Connor Baumler",
      "Abhilasha Sancheti",
      "Rachel Rudinger"
    ],
    "author_ids": [],
    "abstract": "We examine LLM representations of gender for first names in various\noccupational contexts to study how occupations and the gender perception of\nfirst names in LLMs influence each other mutually. We find that LLMs'\nfirst-name gender representations correlate with real-world gender statistics\nassociated with the name, and are influenced by the co-occurrence of\nstereotypically feminine or masculine occupations. Additionally, we study the\ninfluence of first-name gender representations on LLMs in a downstream\noccupation prediction task and their potential as an internal metric to\nidentify extrinsic model biases. While feminine first-name embeddings often\nraise the probabilities for female-dominated jobs (and vice versa for\nmale-dominated jobs), reliably using these internal gender representations for\nbias detection remains challenging.",
    "published_date": "2025-03-09T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.06792v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.06788v1",
    "title": "Dubito Ergo Sum: Exploring AI Ethics",
    "authors": [
      "Viktor Dorfler",
      "Giles Cuthbert"
    ],
    "author_ids": [],
    "abstract": "We paraphrase Descartes' famous dictum in the area of AI ethics where the \"I\ndoubt and therefore I am\" is suggested as a necessary aspect of morality.\nTherefore AI, which cannot doubt itself, cannot possess moral agency. Of\ncourse, this is not the end of the story. We explore various aspects of the\nhuman mind that substantially differ from AI, which includes the sensory\ngrounding of our knowing, the act of understanding, and the significance of\nbeing able to doubt ourselves. The foundation of our argument is the discipline\nof ethics, one of the oldest and largest knowledge projects of human history,\nyet, we seem only to be beginning to get a grasp of it. After a couple of\nthousand years of studying the ethics of humans, we (humans) arrived at a point\nwhere moral psychology suggests that our moral decisions are intuitive, and all\nthe models from ethics become relevant only when we explain ourselves. This\nrecognition has a major impact on what and how we can do regarding AI ethics.\nWe do not offer a solution, we explore some ideas and leave the problem open,\nbut we hope somewhat better understood than before our study.",
    "published_date": "2025-03-09T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.06788v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.06734v1",
    "title": "Gender Encoding Patterns in Pretrained Language Model Representations",
    "authors": [
      "Mahdi Zakizadeh",
      "Mohammad Taher Pilehvar"
    ],
    "author_ids": [],
    "abstract": "Gender bias in pretrained language models (PLMs) poses significant social and\nethical challenges. Despite growing awareness, there is a lack of comprehensive\ninvestigation into how different models internally represent and propagate such\nbiases. This study adopts an information-theoretic approach to analyze how\ngender biases are encoded within various encoder-based architectures. We focus\non three key aspects: identifying how models encode gender information and\nbiases, examining the impact of bias mitigation techniques and fine-tuning on\nthe encoded biases and their effectiveness, and exploring how model design\ndifferences influence the encoding of biases. Through rigorous and systematic\ninvestigation, our findings reveal a consistent pattern of gender encoding\nacross diverse models. Surprisingly, debiasing techniques often exhibit limited\nefficacy, sometimes inadvertently increasing the encoded bias in internal\nrepresentations while reducing bias in model output distributions. This\nhighlights a disconnect between mitigating bias in output distributions and\naddressing its internal representations. This work provides valuable guidance\nfor advancing bias mitigation strategies and fostering the development of more\nequitable language models.",
    "published_date": "2025-03-09T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.06734v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.06661v1",
    "title": "AA-CLIP: Enhancing Zero-shot Anomaly Detection via Anomaly-Aware CLIP",
    "authors": [
      "Wenxin Ma",
      "Xu Zhang",
      "Qingsong Yao",
      "Fenghe Tang",
      "Chenxu Wu",
      "Yingtai Li",
      "Rui Yan",
      "Zihang Jiang",
      "S. Kevin Zhou"
    ],
    "author_ids": [],
    "abstract": "Anomaly detection (AD) identifies outliers for applications like defect and\nlesion detection. While CLIP shows promise for zero-shot AD tasks due to its\nstrong generalization capabilities, its inherent Anomaly-Unawareness leads to\nlimited discrimination between normal and abnormal features. To address this\nproblem, we propose Anomaly-Aware CLIP (AA-CLIP), which enhances CLIP's anomaly\ndiscrimination ability in both text and visual spaces while preserving its\ngeneralization capability. AA-CLIP is achieved through a straightforward yet\neffective two-stage approach: it first creates anomaly-aware text anchors to\ndifferentiate normal and abnormal semantics clearly, then aligns patch-level\nvisual features with these anchors for precise anomaly localization. This\ntwo-stage strategy, with the help of residual adapters, gradually adapts CLIP\nin a controlled manner, achieving effective AD while maintaining CLIP's class\nknowledge. Extensive experiments validate AA-CLIP as a resource-efficient\nsolution for zero-shot AD tasks, achieving state-of-the-art results in\nindustrial and medical applications. The code is available at\nhttps://github.com/Mwxinnn/AA-CLIP.",
    "published_date": "2025-03-09T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.06661v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.06635v2",
    "title": "Deep Cut-informed Graph Embedding and Clustering",
    "authors": [
      "Zhiyuan Ning",
      "Zaitian Wang",
      "Ran Zhang",
      "Ping Xu",
      "Kunpeng Liu",
      "Pengyang Wang",
      "Wei Ju",
      "Pengfei Wang",
      "Yuanchun Zhou",
      "Erik Cambria",
      "Chong Chen"
    ],
    "author_ids": [],
    "abstract": "Graph clustering aims to divide the graph into different clusters. The\nrecently emerging deep graph clustering approaches are largely built on graph\nneural networks (GNN). However, GNN is designed for general graph encoding and\nthere is a common issue of representation collapse in existing GNN-based deep\ngraph clustering algorithms. We attribute two main reasons for such issues: (i)\nthe inductive bias of GNN models: GNNs tend to generate similar representations\nfor proximal nodes. Since graphs often contain a non-negligible amount of\ninter-cluster links, the bias results in error message passing and leads to\nbiased clustering; (ii) the clustering guided loss function: most traditional\napproaches strive to make all samples closer to pre-learned cluster centers,\nwhich causes a degenerate solution assigning all data points to a single label\nthus make all samples and less discriminative. To address these challenges, we\ninvestigate graph clustering from a graph cut perspective and propose an\ninnovative and non-GNN-based Deep Cut-informed Graph embedding and Clustering\nframework, namely DCGC. This framework includes two modules: (i) cut-informed\ngraph encoding; (ii) self-supervised graph clustering via optimal transport.\nFor the encoding module, we derive a cut-informed graph embedding objective to\nfuse graph structure and attributes by minimizing their joint normalized cut.\nFor the clustering module, we utilize the optimal transport theory to obtain\nthe clustering assignments, which can balance the guidance of \"proximity to the\npre-learned cluster center\". With the above two tailored designs, DCGC is more\nsuitable for the graph clustering task, which can effectively alleviate the\nproblem of representation collapse and achieve better performance. We conduct\nextensive experiments to demonstrate that our method is simple but effective\ncompared with benchmarks.",
    "published_date": "2025-03-09T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.06635v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.06632v1",
    "title": "Towards More Accurate Personalized Image Generation: Addressing Overfitting and Evaluation Bias",
    "authors": [
      "Mingxiao Li",
      "Tingyu Qu",
      "Tinne Tuytelaars",
      "Marie-Francine Moens"
    ],
    "author_ids": [],
    "abstract": "Personalized image generation via text prompts has great potential to improve\ndaily life and professional work by facilitating the creation of customized\nvisual content. The aim of image personalization is to create images based on a\nuser-provided subject while maintaining both consistency of the subject and\nflexibility to accommodate various textual descriptions of that subject.\nHowever, current methods face challenges in ensuring fidelity to the text\nprompt while not overfitting to the training data. In this work, we introduce a\nnovel training pipeline that incorporates an attractor to filter out\ndistractions in training images, allowing the model to focus on learning an\neffective representation of the personalized subject. Moreover, current\nevaluation methods struggle due to the lack of a dedicated test set. The\nevaluation set-up typically relies on the training data of the personalization\ntask to compute text-image and image-image similarity scores, which, while\nuseful, tend to overestimate performance. Although human evaluations are\ncommonly used as an alternative, they often suffer from bias and inconsistency.\nTo address these issues, we curate a diverse and high-quality test set with\nwell-designed prompts. With this new benchmark, automatic evaluation metrics\ncan reliably assess model performance",
    "published_date": "2025-03-09T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.06632v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.06523v1",
    "title": "Generative AI as Digital Media",
    "authors": [
      "Gilad Abiri"
    ],
    "author_ids": [],
    "abstract": "Generative AI is frequently portrayed as revolutionary or even apocalyptic,\nprompting calls for novel regulatory approaches. This essay argues that such\nviews are misguided. Instead, generative AI should be understood as an\nevolutionary step in the broader algorithmic media landscape, alongside search\nengines and social media. Like these platforms, generative AI centralizes\ninformation control, relies on complex algorithms to shape content, and\nextensively uses user data, thus perpetuating common problems: unchecked\ncorporate power, echo chambers, and weakened traditional gatekeepers.\nRegulation should therefore share a consistent objective: ensuring media\ninstitutions remain trustworthy. Without trust, public discourse risks\nfragmenting into isolated communities dominated by comforting, tribal beliefs\n-- a threat intensified by generative AI's capacity to bypass gatekeepers and\npersonalize truth. Current governance frameworks, such as the EU's AI Act and\nthe US Executive Order 14110, emphasize reactive risk mitigation, addressing\nmeasurable threats like national security, public health, and algorithmic bias.\nWhile effective for novel technological risks, this reactive approach fails to\nadequately address broader issues of trust and legitimacy inherent to digital\nmedia. Proactive regulation fostering transparency, accountability, and public\nconfidence is essential. Viewing generative AI exclusively as revolutionary\nrisks repeating past regulatory failures that left social media and search\nengines insufficiently regulated. Instead, regulation must proactively shape an\nalgorithmic media environment serving the public good, supporting quality\ninformation and robust civic discourse.",
    "published_date": "2025-03-09T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.06523v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.06451v2",
    "title": "A Quantitative Evaluation of the Expressivity of BMI, Pose and Gender in Body Embeddings for Recognition and Identification",
    "authors": [
      "Basudha Pal",
      "Siyuan Huang",
      "Rama Chellappa"
    ],
    "author_ids": [],
    "abstract": "Person Re-identification (ReID) systems identify individuals across images or\nvideo frames and play a critical role in various real-world applications.\nHowever, many ReID methods are influenced by sensitive attributes such as\ngender, pose, and body mass index (BMI), which vary in uncontrolled\nenvironments, leading to biases and reduced generalization. To address this, we\nextend the concept of expressivity to the body recognition domain to better\nunderstand how ReID models encode these attributes. Expressivity, defined as\nthe mutual information between feature vector representations and specific\nattributes, is computed using a secondary neural network that takes feature and\nattribute vectors as inputs. This provides a quantitative framework for\nanalyzing the extent to which sensitive attributes are embedded in the model's\nrepresentations. We apply expressivity analysis to SemReID, a state-of-the-art\nself-supervised ReID model, and find that BMI consistently exhibits the highest\nexpressivity scores in the model's final layers, underscoring its dominant role\nin feature encoding. In the final attention layer of the trained network, the\nexpressivity order for body attributes is BMI > Pitch > Yaw > Gender,\nhighlighting their relative importance in learned representations.\nAdditionally, expressivity values evolve progressively across network layers\nand training epochs, reflecting a dynamic encoding of attributes during feature\nextraction. These insights emphasize the influence of body-related attributes\non ReID models and provide a systematic methodology for identifying and\nmitigating attribute-driven biases. By leveraging expressivity analysis, we\noffer valuable tools to enhance the fairness, robustness, and generalization of\nReID systems in diverse real-world settings.",
    "published_date": "2025-03-09T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.06451v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.06431v1",
    "title": "Fairness-aware organ exchange and kidney paired donation",
    "authors": [
      "Mingrui Zhang",
      "Xiaowu Dai",
      "Lexin Li"
    ],
    "author_ids": [],
    "abstract": "The kidney paired donation (KPD) program provides an innovative solution to\novercome incompatibility challenges in kidney transplants by matching\nincompatible donor-patient pairs and facilitating kidney exchanges. To address\nunequal access to transplant opportunities, there are two widely used fairness\ncriteria: group fairness and individual fairness. However, these criteria do\nnot consider protected patient features, which refer to characteristics legally\nor ethically recognized as needing protection from discrimination, such as race\nand gender. Motivated by the calibration principle in machine learning, we\nintroduce a new fairness criterion: the matching outcome should be\nconditionally independent of the protected feature, given the sensitization\nlevel. We integrate this fairness criterion as a constraint within the KPD\noptimization framework and propose a computationally efficient solution.\nTheoretically, we analyze the associated price of fairness using random graph\nmodels. Empirically, we compare our fairness criterion with group fairness and\nindividual fairness through both simulations and a real-data example.",
    "published_date": "2025-03-09T00:00:00",
    "year": 2025,
    "categories": [
      "stat.ME",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.06431v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.06411v1",
    "title": "Decoding the Black Box: Integrating Moral Imagination with Technical AI Governance",
    "authors": [
      "Krti Tallam"
    ],
    "author_ids": [],
    "abstract": "This paper examines the intricate interplay among AI safety, security, and\ngovernance by integrating technical systems engineering with principles of\nmoral imagination and ethical philosophy. Drawing on foundational insights from\nWeapons of Math Destruction and Thinking in Systems alongside contemporary\ndebates in AI ethics, we develop a comprehensive multi-dimensional framework\ndesigned to regulate AI technologies deployed in high-stakes domains such as\ndefense, finance, healthcare, and education. Our approach combines rigorous\ntechnical analysis, quantitative risk assessment, and normative evaluation to\nexpose systemic vulnerabilities inherent in opaque, black-box models. Detailed\ncase studies, including analyses of Microsoft Tay (2016) and the UK A-Level\nGrading Algorithm (2020), demonstrate how security lapses, bias amplification,\nand lack of accountability can precipitate cascading failures that undermine\npublic trust. We conclude by outlining targeted strategies for enhancing AI\nresilience through adaptive regulatory mechanisms, robust security protocols,\nand interdisciplinary oversight, thereby advancing the state of the art in\nethical and technical AI governance.",
    "published_date": "2025-03-09T00:00:00",
    "year": 2025,
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.06411v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.06353v1",
    "title": "The AI Pentad, the CHARME$^{2}$D Model, and an Assessment of Current-State AI Regulation",
    "authors": [
      "Di Kevin Gao",
      "Sudip Mittal",
      "Jiming Wu",
      "Hongwei Du",
      "Jingdao Chen",
      "Shahram Rahimi"
    ],
    "author_ids": [],
    "abstract": "Artificial Intelligence (AI) has made remarkable progress in the past few\nyears with AI-enabled applications beginning to permeate every aspect of our\nsociety. Despite the widespread consensus on the need to regulate AI, there\nremains a lack of a unified approach to framing, developing, and assessing AI\nregulations. Many of the existing methods take a value-based approach, for\nexample, accountability, fairness, free from bias, transparency, and trust.\nHowever, these methods often face challenges at the outset due to disagreements\nin academia over the subjective nature of these definitions. This paper aims to\nestablish a unifying model for AI regulation from the perspective of core AI\ncomponents. We first introduce the AI Pentad, which comprises the five\nessential components of AI: humans and organizations, algorithms, data,\ncomputing, and energy. We then review AI regulatory enablers, including AI\nregistration and disclosure, AI monitoring, and AI enforcement mechanisms.\nSubsequently, we present the CHARME$^{2}$D Model to explore further the\nrelationship between the AI Pentad and AI regulatory enablers. Finally, we\napply the CHARME$^{2}$D model to assess AI regulatory efforts in the European\nUnion (EU), China, the United Arab Emirates (UAE), the United Kingdom (UK), and\nthe United States (US), highlighting their strengths, weaknesses, and gaps.\nThis comparative evaluation offers insights for future legislative work in the\nAI domain.",
    "published_date": "2025-03-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.06353v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.06279v1",
    "title": "Mitigating Blockchain extractable value (BEV) threats by Distributed Transaction Sequencing in Blockchains",
    "authors": [
      "Xiongfei Zhao",
      "Hou-Wan Long",
      "Zhengzhe Li",
      "Jiangchuan Liu",
      "Yain-Whar Si"
    ],
    "author_ids": [],
    "abstract": "The rapid growth of Blockchain and Decentralized Finance (DeFi) has\nintroduced new challenges and vulnerabilities that threaten the integrity and\nefficiency of the ecosystem. This study identifies critical issues such as\nTransaction Order Dependence (TOD), Blockchain Extractable Value (BEV), and\nTransaction Importance Diversity (TID), which collectively undermine the\nfairness and security of DeFi systems. BEV-related activities, including\nSandwich attacks, Liquidations, and Transaction Replay, have emerged as\nsignificant threats, collectively generating $540.54 million in losses over 32\nmonths across 11,289 addresses, involving 49,691 cryptocurrencies and 60,830\non-chain markets. These attacks exploit transaction mechanics to manipulate\nasset prices and extract value at the expense of other participants, with\nSandwich attacks being particularly impactful. Additionally, the growing\nadoption of Blockchain in traditional finance highlights the challenge of TID,\nwhere high transaction volumes can strain systems and compromise time-sensitive\noperations. To address these pressing issues, we propose a novel Distributed\nTransaction Sequencing Strategy (DTSS), which combines forking mechanisms and\nthe Analytic Hierarchy Process (AHP) to enforce fair and transparent\ntransaction ordering in a decentralized manner. Our approach is further\nenhanced by an optimization framework and the introduction of the Normalized\nAllocation Disparity Metric (NADM), which ensures optimal parameter selection\nfor transaction prioritization. Experimental evaluations demonstrate that DTSS\neffectively mitigates BEV risks, enhances transaction fairness, and\nsignificantly improves the security and transparency of DeFi ecosystems. This\nwork is essential for protecting the future of decentralized finance and\npromoting its integration into global financial systems.",
    "published_date": "2025-03-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CR",
      "cs.CE",
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.06279v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.06195v1",
    "title": "Human-AI Experience in Integrated Development Environments: A Systematic Literature Review",
    "authors": [
      "Agnia Sergeyuk",
      "Ilya Zakharov",
      "Ekaterina Koshchenko",
      "Maliheh Izadi"
    ],
    "author_ids": [],
    "abstract": "The integration of Artificial Intelligence (AI) into Integrated Development\nEnvironments (IDEs) is reshaping software development, fundamentally altering\nhow developers interact with their tools. This shift marks the emergence of\nHuman-AI Experience in Integrated Development Environment (in-IDE HAX), a field\nthat explores the evolving dynamics of Human-Computer Interaction in\nAI-assisted coding environments. Despite rapid adoption, research on in-IDE HAX\nremains fragmented which highlights the need for a unified overview of current\npractices, challenges, and opportunities. To provide a structured overview of\nexisting research, we conduct a systematic literature review of 89 studies,\nsummarizing current findings and outlining areas for further investigation.\n  Our findings reveal that AI-assisted coding enhances developer productivity\nbut also introduces challenges, such as verification overhead, automation bias,\nand over-reliance, particularly among novice developers. Furthermore, concerns\nabout code correctness, security, and maintainability highlight the urgent need\nfor explainability, verification mechanisms, and adaptive user control.\nAlthough recent advances have driven the field forward, significant research\ngaps remain, including a lack of longitudinal studies, personalization\nstrategies, and AI governance frameworks. This review provides a foundation for\nadvancing in-IDE HAX research and offers guidance for responsibly integrating\nAI into software development.",
    "published_date": "2025-03-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.06195v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.06181v2",
    "title": "Make Haste Slowly: A Theory of Emergent Structured Mixed Selectivity in Feature Learning ReLU Networks",
    "authors": [
      "Devon Jarvis",
      "Richard Klein",
      "Benjamin Rosman",
      "Andrew M. Saxe"
    ],
    "author_ids": [],
    "abstract": "In spite of finite dimension ReLU neural networks being a consistent factor\nbehind recent deep learning successes, a theory of feature learning in these\nmodels remains elusive. Currently, insightful theories still rely on\nassumptions including the linearity of the network computations, unstructured\ninput data and architectural constraints such as infinite width or a single\nhidden layer. To begin to address this gap we establish an equivalence between\nReLU networks and Gated Deep Linear Networks, and use their greater\ntractability to derive dynamics of learning. We then consider multiple variants\nof a core task reminiscent of multi-task learning or contextual control which\nrequires both feature learning and nonlinearity. We make explicit that, for\nthese tasks, the ReLU networks possess an inductive bias towards latent\nrepresentations which are not strictly modular or disentangled but are still\nhighly structured and reusable between contexts. This effect is amplified with\nthe addition of more contexts and hidden layers. Thus, we take a step towards a\ntheory of feature learning in finite ReLU networks and shed light on how\nstructured mixed-selective latent representations can emerge due to a bias for\nnode-reuse and learning speed.",
    "published_date": "2025-03-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.06181v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.06150v2",
    "title": "Do Fairness Interventions Come at the Cost of Privacy: Evaluations for Binary Classifiers",
    "authors": [
      "Huan Tian",
      "Guangsheng Zhang",
      "Bo Liu",
      "Tianqing Zhu",
      "Ming Ding",
      "Wanlei Zhou"
    ],
    "author_ids": [],
    "abstract": "While in-processing fairness approaches show promise in mitigating biased\npredictions, their potential impact on privacy leakage remains under-explored.\nWe aim to address this gap by assessing the privacy risks of fairness-enhanced\nbinary classifiers via membership inference attacks (MIAs) and attribute\ninference attacks (AIAs). Surprisingly, our results reveal that enhancing\nfairness does not necessarily lead to privacy compromises. For example, these\nfairness interventions exhibit increased resilience against MIAs and AIAs. This\nis because fairness interventions tend to remove sensitive information among\nextracted features and reduce confidence scores for the majority of training\ndata for fairer predictions. However, during the evaluations, we uncover a\npotential threat mechanism that exploits prediction discrepancies between fair\nand biased models, leading to advanced attack results for both MIAs and AIAs.\nThis mechanism reveals potent vulnerabilities of fair models and poses\nsignificant privacy risks of current fairness methods. Extensive experiments\nacross multiple datasets, attack methods, and representative fairness\napproaches confirm our findings and demonstrate the efficacy of the uncovered\nmechanism. Our study exposes the under-explored privacy threats in fairness\nstudies, advocating for thorough evaluations of potential security\nvulnerabilities before model deployments.",
    "published_date": "2025-03-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.06150v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.06072v1",
    "title": "A Survey on Post-training of Large Language Models",
    "authors": [
      "Guiyao Tie",
      "Zeli Zhao",
      "Dingjie Song",
      "Fuyang Wei",
      "Rong Zhou",
      "Yurou Dai",
      "Wen Yin",
      "Zhejian Yang",
      "Jiangyue Yan",
      "Yao Su",
      "Zhenhan Dai",
      "Yifeng Xie",
      "Yihan Cao",
      "Lichao Sun",
      "Pan Zhou",
      "Lifang He",
      "Hechang Chen",
      "Yu Zhang",
      "Qingsong Wen",
      "Tianming Liu",
      "Neil Zhenqiang Gong",
      "Jiliang Tang",
      "Caiming Xiong",
      "Heng Ji",
      "Philip S. Yu",
      "Jianfeng Gao"
    ],
    "author_ids": [],
    "abstract": "The emergence of Large Language Models (LLMs) has fundamentally transformed\nnatural language processing, making them indispensable across domains ranging\nfrom conversational systems to scientific exploration. However, their\npre-trained architectures often reveal limitations in specialized contexts,\nincluding restricted reasoning capacities, ethical uncertainties, and\nsuboptimal domain-specific performance. These challenges necessitate advanced\npost-training language models (PoLMs) to address these shortcomings, such as\nOpenAI-o1/o3 and DeepSeek-R1 (collectively known as Large Reasoning Models, or\nLRMs). This paper presents the first comprehensive survey of PoLMs,\nsystematically tracing their evolution across five core paradigms: Fine-tuning,\nwhich enhances task-specific accuracy; Alignment, which ensures alignment with\nhuman preferences; Reasoning, which advances multi-step inference despite\nchallenges in reward design; Efficiency, which optimizes resource utilization\namidst increasing complexity; and Integration and Adaptation, which extend\ncapabilities across diverse modalities while addressing coherence issues.\nCharting progress from ChatGPT's foundational alignment strategies to\nDeepSeek-R1's innovative reasoning advancements, we illustrate how PoLMs\nleverage datasets to mitigate biases, deepen reasoning capabilities, and\nenhance domain adaptability. Our contributions include a pioneering synthesis\nof PoLM evolution, a structured taxonomy categorizing techniques and datasets,\nand a strategic agenda emphasizing the role of LRMs in improving reasoning\nproficiency and domain flexibility. As the first survey of its scope, this work\nconsolidates recent PoLM advancements and establishes a rigorous intellectual\nframework for future research, fostering the development of LLMs that excel in\nprecision, ethical robustness, and versatility across scientific and societal\napplications.",
    "published_date": "2025-03-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.06072v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.06054v1",
    "title": "Fine-Grained Bias Detection in LLM: Enhancing detection mechanisms for nuanced biases",
    "authors": [
      "Suvendu Mohanty"
    ],
    "author_ids": [],
    "abstract": "Recent advancements in Artificial Intelligence, particularly in Large\nLanguage Models (LLMs), have transformed natural language processing by\nimproving generative capabilities. However, detecting biases embedded within\nthese models remains a challenge. Subtle biases can propagate misinformation,\ninfluence decision-making, and reinforce stereotypes, raising ethical concerns.\nThis study presents a detection framework to identify nuanced biases in LLMs.\nThe approach integrates contextual analysis, interpretability via attention\nmechanisms, and counterfactual data augmentation to capture hidden biases\nacross linguistic contexts. The methodology employs contrastive prompts and\nsynthetic datasets to analyze model behaviour across cultural, ideological, and\ndemographic scenarios.\n  Quantitative analysis using benchmark datasets and qualitative assessments\nthrough expert reviews validate the effectiveness of the framework. Results\nshow improvements in detecting subtle biases compared to conventional methods,\nwhich often fail to highlight disparities in model responses to race, gender,\nand socio-political contexts. The framework also identifies biases arising from\nimbalances in training data and model architectures. Continuous user feedback\nensures adaptability and refinement. This research underscores the importance\nof proactive bias mitigation strategies and calls for collaboration between\npolicymakers, AI developers, and regulators. The proposed detection mechanisms\nenhance model transparency and support responsible LLM deployment in sensitive\napplications such as education, legal systems, and healthcare. Future work will\nfocus on real-time bias monitoring and cross-linguistic generalization to\nimprove fairness and inclusivity in AI-driven communication tools.",
    "published_date": "2025-03-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.06054v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.06011v1",
    "title": "Intent-Aware Self-Correction for Mitigating Social Biases in Large Language Models",
    "authors": [
      "Panatchakorn Anantaprayoon",
      "Masahiro Kaneko",
      "Naoaki Okazaki"
    ],
    "author_ids": [],
    "abstract": "Self-Correction based on feedback improves the output quality of Large\nLanguage Models (LLMs). Moreover, as Self-Correction functions like the slow\nand conscious System-2 thinking from cognitive psychology's perspective, it can\npotentially reduce LLMs' social biases. LLMs are sensitive to contextual\nambiguities and inconsistencies; therefore, explicitly communicating their\nintentions during interactions when applying Self-Correction for debiasing is\ncrucial. In this study, we demonstrate that clarifying intentions is essential\nfor effectively reducing biases in LLMs through Self-Correction. We divide the\ncomponents needed for Self-Correction into three parts: instruction, response,\nand feedback, and clarify intentions at each component. We incorporate an\nexplicit debiasing prompt to convey the intention of bias mitigation from the\ninstruction for response generation. In the response, we use Chain-of-Thought\n(CoT) to clarify the reasoning process. In the feedback, we define evaluation\naspects necessary for debiasing and propose clear feedback through multi-aspect\ncritiques and scoring. Through experiments, we demonstrate that self-correcting\nCoT responses obtained from a debiasing prompt based on multi-aspect feedback\ncan reduce biased responses more robustly and consistently than the baselines.\nWe also find the variation in debiasing efficacy when using models with\ndifferent bias levels or separating models for response and feedback\ngeneration.",
    "published_date": "2025-03-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.06011v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.05992v2",
    "title": "Psycholinguistic Analyses in Software Engineering Text: A Systematic Literature Review",
    "authors": [
      "Amirali Sajadi",
      "Kostadin Damevski",
      "Preetha Chatterjee"
    ],
    "author_ids": [],
    "abstract": "Context: A deeper understanding of human factors in software engineering (SE)\nis essential for improving team collaboration, decision-making, and\nproductivity. Communication channels like code reviews and chats provide\ninsights into developers' psychological and emotional states. While large\nlanguage models excel at text analysis, they often lack transparency and\nprecision. Psycholinguistic tools like Linguistic Inquiry and Word Count (LIWC)\noffer clearer, interpretable insights into cognitive and emotional processes\nexhibited in text. Despite its wide use in SE research, no comprehensive review\nof LIWC's use has been conducted. Objective: We examine the importance of\npsycholinguistic tools, particularly LIWC, and provide a thorough analysis of\nits current and potential future applications in SE research. Methods: We\nconducted a systematic review of six prominent databases, identifying 43\nSE-related papers using LIWC. Our analysis focuses on five research questions.\nResults: Our findings reveal a wide range of applications, including analyzing\nteam communication to detect developer emotions and personality, developing ML\nmodels to predict deleted Stack Overflow posts, and more recently comparing\nAI-generated and human-written text. LIWC has been primarily used with data\nfrom project management platforms (e.g., GitHub) and Q&A forums (e.g., Stack\nOverflow). Key BSE concepts include Communication, Organizational Climate, and\nPositive Psychology. 26 of 43 papers did not formally evaluate LIWC. Concerns\nwere raised about some limitations, including difficulty handling SE-specific\nvocabulary. Conclusion: We highlight the potential of psycholinguistic tools\nand their limitations, and present new use cases for advancing the research of\nhuman factors in SE (e.g., bias in human-LLM conversations).",
    "published_date": "2025-03-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SE",
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.05992v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.05958v1",
    "title": "SANDWiCH: Semantical Analysis of Neighbours for Disambiguating Words in Context ad Hoc",
    "authors": [
      "Daniel Guzman-Olivares",
      "Lara Quijano-Sanchez",
      "Federico Liberatore"
    ],
    "author_ids": [],
    "abstract": "The rise of generative chat-based Large Language Models (LLMs) over the past\ntwo years has spurred a race to develop systems that promise near-human\nconversational and reasoning experiences. However, recent studies indicate that\nthe language understanding offered by these models remains limited and far from\nhuman-like performance, particularly in grasping the contextual meanings of\nwords, an essential aspect of reasoning. In this paper, we present a simple yet\ncomputationally efficient framework for multilingual Word Sense Disambiguation\n(WSD). Our approach reframes the WSD task as a cluster discrimination analysis\nover a semantic network refined from BabelNet using group algebra. We validate\nour methodology across multiple WSD benchmarks, achieving a new state of the\nart for all languages and tasks, as well as in individual assessments by part\nof speech. Notably, our model significantly surpasses the performance of\ncurrent alternatives, even in low-resource languages, while reducing the\nparameter count by 72%.",
    "published_date": "2025-03-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.05958v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.05939v1",
    "title": "Universal Framework to Evaluate Automotive Perception Sensor Impact on Perception Functions",
    "authors": [
      "A Gamage",
      "V Donzella"
    ],
    "author_ids": [],
    "abstract": "Current research on automotive perception systems predominantly focusses on\neither improving the sensors for data quality or enhancing the performance of\nperception functions in isolation. Although automotive perception sensors form\na fundamental part of the perception system, value addition in sensor data\nquality in isolation is questionable. However, the end goal for most perception\nsystems is the accuracy of high-level functions such as trajectory prediction\nof surrounding vehicles. High-level perception functions are increasingly based\non deep learning (DL) models due to their improved performance and\ngeneralisability compared to traditional algorithms. Innately, DL models\ndevelop a performance bias on the comprehensiveness of the training data.\nDespite the vital need to evaluate the performance of DL-based perception\nfunctions under real-world conditions using onboard sensor inputs, there is a\nlack of frameworks to facilitate systematic evaluations. This paper presents a\nversatile and cost-effective framework to evaluate the impact of perception\nsensor modalities and parameter settings on DL-based perception functions.\nUsing a simulation environment, the framework facilitates sensor modality\ntesting and parameter tuning under different environmental conditions. Its\neffectiveness is demonstrated through a case study involving a state-of-the-art\nsurround trajectory prediction model, highlighting performance differences\nacross sensor modalities and recommending optimal parameter settings. The\nproposed framework offers valuable insights for designing the perception sensor\nsuite, contributing to the development of robust perception systems for\nautonomous vehicles.",
    "published_date": "2025-03-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.05939v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.05888v1",
    "title": "QG-SMS: Enhancing Test Item Analysis via Student Modeling and Simulation",
    "authors": [
      "Bang Nguyen",
      "Tingting Du",
      "Mengxia Yu",
      "Lawrence Angrave",
      "Meng Jiang"
    ],
    "author_ids": [],
    "abstract": "While the Question Generation (QG) task has been increasingly adopted in\neducational assessments, its evaluation remains limited by approaches that lack\na clear connection to the educational values of test items. In this work, we\nintroduce test item analysis, a method frequently used by educators to assess\ntest question quality, into QG evaluation. Specifically, we construct pairs of\ncandidate questions that differ in quality across dimensions such as topic\ncoverage, item difficulty, item discrimination, and distractor efficiency. We\nthen examine whether existing QG evaluation approaches can effectively\ndistinguish these differences. Our findings reveal significant shortcomings in\nthese approaches with respect to accurately assessing test item quality in\nrelation to student performance. To address this gap, we propose a novel QG\nevaluation framework, QG-SMS, which leverages Large Language Model for Student\nModeling and Simulation to perform test item analysis. As demonstrated in our\nextensive experiments and human evaluation study, the additional perspectives\nintroduced by the simulated student profiles lead to a more effective and\nrobust assessment of test items.",
    "published_date": "2025-03-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.05888v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.05684v1",
    "title": "Fairness-Aware Low-Rank Adaptation Under Demographic Privacy Constraints",
    "authors": [
      "Parameswaran Kamalaruban",
      "Mark Anderson",
      "Stuart Burrell",
      "Maeve Madigan",
      "Piotr Skalski",
      "David Sutton"
    ],
    "author_ids": [],
    "abstract": "Pre-trained foundation models can be adapted for specific tasks using\nLow-Rank Adaptation (LoRA). However, the fairness properties of these adapted\nclassifiers remain underexplored. Existing fairness-aware fine-tuning methods\nrely on direct access to sensitive attributes or their predictors, but in\npractice, these sensitive attributes are often held under strict consumer\nprivacy controls, and neither the attributes nor their predictors are available\nto model developers, hampering the development of fair models. To address this\nissue, we introduce a set of LoRA-based fine-tuning methods that can be trained\nin a distributed fashion, where model developers and fairness auditors\ncollaborate without sharing sensitive attributes or predictors. In this paper,\nwe evaluate three such methods - sensitive unlearning, adversarial training,\nand orthogonality loss - against a fairness-unaware baseline, using experiments\non the CelebA and UTK-Face datasets with an ImageNet pre-trained ViT-Base\nmodel. We find that orthogonality loss consistently reduces bias while\nmaintaining or improving utility, whereas adversarial training improves False\nPositive Rate Parity and Demographic Parity in some cases, and sensitive\nunlearning provides no clear benefit. In tasks where significant biases are\npresent, distributed fairness-aware fine-tuning methods can effectively\neliminate bias without compromising consumer privacy and, in most cases,\nimprove model utility.",
    "published_date": "2025-03-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.05684v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.05672v1",
    "title": "The latent variable proximal point algorithm for variational problems with inequality constraints",
    "authors": [
      "Jørgen S. Dokken",
      "Patrick E. Farrell",
      "Brendan Keith",
      "Ioannis P. A. Papadopoulos",
      "Thomas M. Surowiec"
    ],
    "author_ids": [],
    "abstract": "The latent variable proximal point (LVPP) algorithm is a framework for\nsolving infinite-dimensional variational problems with pointwise inequality\nconstraints. The algorithm is a saddle point reformulation of the Bregman\nproximal point algorithm. At the continuous level, the two formulations are\nequivalent, but the saddle point formulation is more amenable to discretization\nbecause it introduces a structure-preserving transformation between a latent\nfunction space and the feasible set. Working in this latent space is much more\nconvenient for enforcing inequality constraints than the feasible set, as\ndiscretizations can employ general linear combinations of suitable basis\nfunctions, and nonlinear solvers can involve general additive updates. LVPP\nyields numerical methods with observed mesh-independence for obstacle problems,\ncontact, fracture, plasticity, and others besides; in many cases, for the first\ntime. The framework also extends to more complex constraints, providing means\nto enforce convexity in the Monge--Amp\\`ere equation and handling\nquasi-variational inequalities, where the underlying constraint depends\nimplicitly on the unknown solution. In this paper, we describe the LVPP\nalgorithm in a general form and apply it to twelve problems from across\nmathematics.",
    "published_date": "2025-03-07T00:00:00",
    "year": 2025,
    "categories": [
      "math.OC",
      "cs.NA",
      "math.NA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.05672v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.05665v1",
    "title": "AIM-Fair: Advancing Algorithmic Fairness via Selectively Fine-Tuning Biased Models with Contextual Synthetic Data",
    "authors": [
      "Zengqun Zhao",
      "Ziquan Liu",
      "Yu Cao",
      "Shaogang Gong",
      "Ioannis Patras"
    ],
    "author_ids": [],
    "abstract": "Recent advances in generative models have sparked research on improving model\nfairness with AI-generated data. However, existing methods often face\nlimitations in the diversity and quality of synthetic data, leading to\ncompromised fairness and overall model accuracy. Moreover, many approaches rely\non the availability of demographic group labels, which are often costly to\nannotate. This paper proposes AIM-Fair, aiming to overcome these limitations\nand harness the potential of cutting-edge generative models in promoting\nalgorithmic fairness. We investigate a fine-tuning paradigm starting from a\nbiased model initially trained on real-world data without demographic\nannotations. This model is then fine-tuned using unbiased synthetic data\ngenerated by a state-of-the-art diffusion model to improve its fairness. Two\nkey challenges are identified in this fine-tuning paradigm, 1) the low quality\nof synthetic data, which can still happen even with advanced generative models,\nand 2) the domain and bias gap between real and synthetic data. To address the\nlimitation of synthetic data quality, we propose Contextual Synthetic Data\nGeneration (CSDG) to generate data using a text-to-image diffusion model (T2I)\nwith prompts generated by a context-aware LLM, ensuring both data diversity and\ncontrol of bias in synthetic data. To resolve domain and bias shifts, we\nintroduce a novel selective fine-tuning scheme in which only model parameters\nmore sensitive to bias and less sensitive to domain shift are updated.\nExperiments on CelebA and UTKFace datasets show that our AIM-Fair improves\nmodel fairness while maintaining utility, outperforming both fully and\npartially fine-tuned approaches to model fairness.",
    "published_date": "2025-03-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.05665v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.05662v1",
    "title": "On Mitigating Affinity Bias through Bandits with Evolving Biased Feedback",
    "authors": [
      "Matthew Faw",
      "Constantine Caramanis",
      "Jessica Hoffmann"
    ],
    "author_ids": [],
    "abstract": "Unconscious bias has been shown to influence how we assess our peers, with\nconsequences for hiring, promotions and admissions. In this work, we focus on\naffinity bias, the component of unconscious bias which leads us to prefer\npeople who are similar to us, despite no deliberate intention of favoritism. In\na world where the people hired today become part of the hiring committee of\ntomorrow, we are particularly interested in understanding (and mitigating) how\naffinity bias affects this feedback loop. This problem has two distinctive\nfeatures: 1) we only observe the biased value of a candidate, but we want to\noptimize with respect to their real value 2) the bias towards a candidate with\na specific set of traits depends on the fraction of people in the hiring\ncommittee with the same set of traits. We introduce a new bandits variant that\nexhibits those two features, which we call affinity bandits. Unsurprisingly,\nclassical algorithms such as UCB often fail to identify the best arm in this\nsetting. We prove a new instance-dependent regret lower bound, which is larger\nthan that in the standard bandit setting by a multiplicative function of $K$.\nSince we treat rewards that are time-varying and dependent on the policy's past\nactions, deriving this lower bound requires developing proof techniques beyond\nthe standard bandit techniques. Finally, we design an elimination-style\nalgorithm which nearly matches this regret, despite never observing the real\nrewards.",
    "published_date": "2025-03-07T00:00:00",
    "year": 2025,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.05662v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.05529v1",
    "title": "PoSSUM: A Protocol for Surveying Social-media Users with Multimodal LLMs",
    "authors": [
      "Roberto Cerina"
    ],
    "author_ids": [],
    "abstract": "This paper introduces PoSSUM, an open-source protocol for unobtrusive polling\nof social-media users via multimodal Large Language Models (LLMs). PoSSUM\nleverages users' real-time posts, images, and other digital traces to create\nsilicon samples that capture information not present in the LLM's training\ndata. To obtain representative estimates, PoSSUM employs Multilevel Regression\nand Post-Stratification (MrP) with structured priors to counteract the\nobservable selection biases of social-media platforms. The protocol is\nvalidated during the 2024 U.S. Presidential Election, for which five PoSSUM\npolls were conducted and published on GitHub and X. In the final poll, fielded\nOctober 17-26 with a synthetic sample of 1,054 X users, PoSSUM accurately\npredicted the outcomes in 50 of 51 states and assigned the Republican candidate\na win probability of 0.65. Notably, it also exhibited lower state-level bias\nthan most established pollsters. These results demonstrate PoSSUM's potential\nas a fully automated, unobtrusive alternative to traditional survey methods.",
    "published_date": "2025-03-07T00:00:00",
    "year": 2025,
    "categories": [
      "stat.AP",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.05529v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.05520v1",
    "title": "Removing Geometric Bias in One-Class Anomaly Detection with Adaptive Feature Perturbation",
    "authors": [
      "Romain Hermary",
      "Vincent Gaudillière",
      "Abd El Rahman Shabayek",
      "Djamila Aouada"
    ],
    "author_ids": [],
    "abstract": "One-class anomaly detection aims to detect objects that do not belong to a\npredefined normal class. In practice training data lack those anomalous\nsamples; hence state-of-the-art methods are trained to discriminate between\nnormal and synthetically-generated pseudo-anomalous data. Most methods use data\naugmentation techniques on normal images to simulate anomalies. However the\nbest-performing ones implicitly leverage a geometric bias present in the\nbenchmarking datasets. This limits their usability in more general conditions.\nOthers are relying on basic noising schemes that may be suboptimal in capturing\nthe underlying structure of normal data. In addition most still favour the\nimage domain to generate pseudo-anomalies training models end-to-end from only\nthe normal class and overlooking richer representations of the information. To\novercome these limitations we consider frozen yet rich feature spaces given by\npretrained models and create pseudo-anomalous features with a novel adaptive\nlinear feature perturbation technique. It adapts the noise distribution to each\nsample applies decaying linear perturbations to feature vectors and further\nguides the classification process using a contrastive learning objective.\nExperimental evaluation conducted on both standard and geometric bias-free\ndatasets demonstrates the superiority of our approach with respect to\ncomparable baselines. The codebase is accessible via our public repository.",
    "published_date": "2025-03-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.05520v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.05516v1",
    "title": "Cognitive Bias Detection Using Advanced Prompt Engineering",
    "authors": [
      "Frederic Lemieux",
      "Aisha Behr",
      "Clara Kellermann-Bryant",
      "Zaki Mohammed"
    ],
    "author_ids": [],
    "abstract": "Cognitive biases, systematic deviations from rationality in judgment, pose\nsignificant challenges in generating objective content. This paper introduces a\nnovel approach for real-time cognitive bias detection in user-generated text\nusing large language models (LLMs) and advanced prompt engineering techniques.\nThe proposed system analyzes textual data to identify common cognitive biases\nsuch as confirmation bias, circular reasoning, and hidden assumption. By\ndesigning tailored prompts, the system effectively leverages LLMs' capabilities\nto both recognize and mitigate these biases, improving the quality of\nhuman-generated content (e.g., news, media, reports). Experimental results\ndemonstrate the high accuracy of our approach in identifying cognitive biases,\noffering a valuable tool for enhancing content objectivity and reducing the\nrisks of biased decision-making.",
    "published_date": "2025-03-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.05516v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.05479v1",
    "title": "A Bot-based Approach to Manage Codes of Conduct in Open-Source Projects",
    "authors": [
      "Sergio Cobos",
      "Javier Luis Cánovas Izquierdo"
    ],
    "author_ids": [],
    "abstract": "The development of Open-Source Software (OSS) projects relies on the\ncollaborative work of contributors, generally scattered around the world. To\nenable this collaboration, OSS projects are hosted on social-coding platforms\nlike GitHub, which provide the infrastructure to host the code as well as the\nsupport for enabling the participation of the community. The potentially rich\nand diverse mixture of contributors in OSS projects makes their management not\nonly a technical challenge, where automation tools and bots are usually\ndeployed, but also a social one. To this aim, OSS projects have been\nincreasingly deploying a declaration of their code of conduct, which defines\nrules to ensure a respectful and inclusive participatory environment in the\ncommunity, being the Contributor Covenant the main model to follow. However,\nthe broad adoption and enforcement of codes of conduct in OSS projects is still\nlimited. In particular, the definition, deployment, and enforcement of codes of\nconduct is a very challenging task. In this paper, we propose an approach to\neffectively manage codes of conduct in OSS projects based on the Contributor\nCovenant proposal. Our solution has been implemented as a bot-based solution\nwhere bots help in the definition of codes of conduct, the monitoring of OSS\nprojects, and the enforcement of ethical rules.",
    "published_date": "2025-03-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SE",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.05479v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.05430v1",
    "title": "Cybersafety Card Game: Empowering Digital Educators to Teach Cybersafety to Older Adults",
    "authors": [
      "Jacob Camilleri",
      "Ashley Sheil",
      "Michelle O'Keeffe",
      "Moya Cronin",
      "Melanie Gruben",
      "Hazel Murray"
    ],
    "author_ids": [],
    "abstract": "Digital inequality remains a significant barrier for many older adults,\nlimiting their ability to navigate online spaces securely and confidently while\nincreasing their susceptibility to cyber threats. In response, we propose a\nnovel shedding-type card game for older adults to conceptually learn and\nreinforce cyber hygiene practices in educational settings. We asked digital\neducators to participate as players alongside older adults (n = 16), departing\nfrom their usual role as teachers, they collaborated and shared a unique\nlearning experience. The cybersafety game addresses 4 key topics: handling\nscams, password management, responding to cyber attacks, and staying private.\nWe adopted a mixed-method approach of think-aloud playtesting, semi-structured\ninterviews, and surveys to evaluate the game's reception and impact.\nParticipants reported highly favorable gameplay experiences and found the\ncybersafety advice useful. Player feedback informed game modifications,\ndetailed in this paper, to further enhance the game's usability and educational\nvalue.",
    "published_date": "2025-03-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.05430v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.05423v1",
    "title": "Semantic Shift Estimation via Dual-Projection and Classifier Reconstruction for Exemplar-Free Class-Incremental Learning",
    "authors": [
      "Run He",
      "Di Fang",
      "Yicheng Xu",
      "Yawen Cui",
      "Ming Li",
      "Cen Chen",
      "Ziqian Zeng",
      "Huiping Zhuang"
    ],
    "author_ids": [],
    "abstract": "Exemplar-Free Class-Incremental Learning (EFCIL) aims to sequentially learn\nfrom distinct categories without retaining exemplars but easily suffers from\ncatastrophic forgetting of learned knowledge. While existing EFCIL methods\nleverage knowledge distillation to alleviate forgetting, they still face two\ncritical challenges: semantic shift and decision bias. Specifically, the\nembeddings of old tasks shift in the embedding space after learning new tasks,\nand the classifier becomes biased towards new tasks due to training solely with\nnew data, thereby hindering the balance between old and new knowledge. To\naddress these issues, we propose the Dual-Projection Shift Estimation and\nClassifier Reconstruction (DPCR) approach for EFCIL. DPCR effectively estimates\nsemantic shift through a dual-projection, which combines a learnable\ntransformation with a row-space projection to capture both task-wise and\ncategory-wise shifts. Furthermore, to mitigate decision bias, DPCR employs\nridge regression to reformulate classifier training as a reconstruction\nprocess. This reconstruction exploits previous information encoded in\ncovariance and prototype of each class after calibration with estimated shift,\nthereby reducing decision bias. Extensive experiments demonstrate that, across\nvarious datasets, DPCR effectively balances old and new tasks, outperforming\nstate-of-the-art EFCIL methods.",
    "published_date": "2025-03-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.05423v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.05371v1",
    "title": "Shifting Perspectives: Steering Vector Ensembles for Robust Bias Mitigation in LLMs",
    "authors": [
      "Zara Siddique",
      "Irtaza Khalid",
      "Liam D. Turner",
      "Luis Espinosa-Anke"
    ],
    "author_ids": [],
    "abstract": "We present a novel approach to bias mitigation in large language models\n(LLMs) by applying steering vectors to modify model activations in forward\npasses. We employ Bayesian optimization to systematically identify effective\ncontrastive pair datasets across nine bias axes. When optimized on the BBQ\ndataset, our individually tuned steering vectors achieve average improvements\nof 12.2%, 4.7%, and 3.2% over the baseline for Mistral, Llama, and Qwen,\nrespectively. Building on these promising results, we introduce Steering Vector\nEnsembles (SVE), a method that averages multiple individually optimized\nsteering vectors, each targeting a specific bias axis such as age, race, or\ngender. By leveraging their collective strength, SVE outperforms individual\nsteering vectors in both bias reduction and maintaining model performance. The\nwork presents the first systematic investigation of steering vectors for bias\nmitigation, and we demonstrate that SVE is a powerful and computationally\nefficient strategy for reducing bias in LLMs, with broader implications for\nenhancing AI safety.",
    "published_date": "2025-03-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.05371v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.05280v2",
    "title": "Revealing Hidden Mechanisms of Cross-Country Content Moderation with Natural Language Processing",
    "authors": [
      "Neemesh Yadav",
      "Jiarui Liu",
      "Francesco Ortu",
      "Roya Ensafi",
      "Zhijing Jin",
      "Rada Mihalcea"
    ],
    "author_ids": [],
    "abstract": "The ability of Natural Language Processing (NLP) methods to categorize text\ninto multiple classes has motivated their use in online content moderation\ntasks, such as hate speech and fake news detection. However, there is limited\nunderstanding of how or why these methods make such decisions, or why certain\ncontent is moderated in the first place. To investigate the hidden mechanisms\nbehind content moderation, we explore multiple directions: 1) training\nclassifiers to reverse-engineer content moderation decisions across countries;\n2) explaining content moderation decisions by analyzing Shapley values and\nLLM-guided explanations. Our primary focus is on content moderation decisions\nmade across countries, using pre-existing corpora sampled from the Twitter\nStream Grab. Our experiments reveal interesting patterns in censored posts,\nboth across countries and over time. Through human evaluations of LLM-generated\nexplanations across three LLMs, we assess the effectiveness of using LLMs in\ncontent moderation. Finally, we discuss potential future directions, as well as\nthe limitations and ethical considerations of this work. Our code and data are\navailable at https://github.com/causalNLP/censorship",
    "published_date": "2025-03-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.05280v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.05142v1",
    "title": "RocketEval: Efficient Automated LLM Evaluation via Grading Checklist",
    "authors": [
      "Tianjun Wei",
      "Wei Wen",
      "Ruizhi Qiao",
      "Xing Sun",
      "Jianghong Ma"
    ],
    "author_ids": [],
    "abstract": "Evaluating large language models (LLMs) in diverse and challenging scenarios\nis essential to align them with human preferences. To mitigate the prohibitive\ncosts associated with human evaluations, utilizing a powerful LLM as a judge\nhas emerged as a favored approach. Nevertheless, this methodology encounters\nseveral challenges, including substantial expenses, concerns regarding privacy\nand security, and reproducibility. In this paper, we propose a straightforward,\nreplicable, and accurate automated evaluation method by leveraging a\nlightweight LLM as the judge, named RocketEval. Initially, we identify that the\nperformance disparity between lightweight and powerful LLMs in evaluation tasks\nprimarily stems from their ability to conduct comprehensive analyses, which is\nnot easily enhanced through techniques such as chain-of-thought reasoning. By\nreframing the evaluation task as a multi-faceted Q&A using an instance-specific\nchecklist, we demonstrate that the limited judgment accuracy of lightweight\nLLMs is largely attributes to high uncertainty and positional bias. To address\nthese challenges, we introduce an automated evaluation process grounded in\nchecklist grading, which is designed to accommodate a variety of scenarios and\nquestions. This process encompasses the creation of checklists, the grading of\nthese checklists by lightweight LLMs, and the reweighting of checklist items to\nalign with the supervised annotations. Our experiments carried out on the\nautomated evaluation benchmarks, MT-Bench and WildBench datasets, reveal that\nRocketEval, when using Gemma-2-2B as the judge, achieves a high correlation\n(0.965) with human preferences, which is comparable to GPT-4o. Moreover,\nRocketEval provides a cost reduction exceeding 50-fold for large-scale\nevaluation and comparison scenarios. Our code is available at\nhttps://github.com/Joinn99/RocketEval-ICLR .",
    "published_date": "2025-03-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.05142v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.05138v1",
    "title": "Numerical analysis of variational-hemivariational inequalities with applications in contact mechanics",
    "authors": [
      "Weimin Han",
      "Fang Feng",
      "Fei Wang",
      "Jianguo Huang"
    ],
    "author_ids": [],
    "abstract": "Variational-hemivariational inequalities are an important mathematical\nframework for nonsmooth problems. The framework can be used to study\napplication problems from physical sciences and engineering that involve\nnon-smooth and even set-valued relations, monotone or non-monotone, among\nphysical quantities. Since no analytic solution formulas are expected for\nvariational-hemivariational inequalities from applications, numerical methods\nare needed to solve the problems. This paper focuses on numerical analysis of\nvariational-hemivariational inequalities, reporting new results as well as\nsurveying some recent published results in the area. A general convergence\nresult is presented for Galerkin solutions of the inequalities under minimal\nsolution regularity conditions available from the well-posedness theory, and\nC\\'{e}a's inequalities are derived for error estimation of numerical solutions.\nThe finite element method and the virtual element method are taken as examples\nof numerical methods, optimal order error estimates for the linear element\nsolutions are derived when the methods are applied to solve three\nrepresentative contact problems under certain solution regularity assumptions.\nNumerical results are presented to show the performance of both the finite\nelement method and the virtual element method, including numerical convergence\norders of the numerical solutions that match the theoretical predictions.",
    "published_date": "2025-03-07T00:00:00",
    "year": 2025,
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.05138v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.05093v1",
    "title": "Visual Cues of Gender and Race are Associated with Stereotyping in Vision-Language Models",
    "authors": [
      "Messi H. J. Lee",
      "Soyeon Jeon",
      "Jacob M. Montgomery",
      "Calvin K. Lai"
    ],
    "author_ids": [],
    "abstract": "Current research on bias in Vision Language Models (VLMs) has important\nlimitations: it is focused exclusively on trait associations while ignoring\nother forms of stereotyping, it examines specific contexts where biases are\nexpected to appear, and it conceptualizes social categories like race and\ngender as binary, ignoring the multifaceted nature of these identities. Using\nstandardized facial images that vary in prototypicality, we test four VLMs for\nboth trait associations and homogeneity bias in open-ended contexts. We find\nthat VLMs consistently generate more uniform stories for women compared to men,\nwith people who are more gender prototypical in appearance being represented\nmore uniformly. By contrast, VLMs represent White Americans more uniformly than\nBlack Americans. Unlike with gender prototypicality, race prototypicality was\nnot related to stronger uniformity. In terms of trait associations, we find\nlimited evidence of stereotyping-Black Americans were consistently linked with\nbasketball across all models, while other racial associations (i.e., art,\nhealthcare, appearance) varied by specific VLM. These findings demonstrate that\nVLM stereotyping manifests in ways that go beyond simple group membership,\nsuggesting that conventional bias mitigation strategies may be insufficient to\naddress VLM stereotyping and that homogeneity bias persists even when trait\nassociations are less apparent in model outputs.",
    "published_date": "2025-03-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.05093v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.05058v1",
    "title": "Towards democratic data agency: Attitudes and concerns about online data practices",
    "authors": [
      "Niels J. Gommesen"
    ],
    "author_ids": [],
    "abstract": "Recent studies reveal widespread concern and increasing lack of understanding\nabout how personal data is collected, shared, and used online without consent.\nThis issue is compounded by limited options available for digital citizens to\nunderstand, control and manage their data flows across platforms, underscoring\nthe need to explore how this lack of trust and transparency affects citizens'\ndata practices including their capacities to act in a modern knowledge society.\nDespite the promising research within this field, important demographics are\noften overlooked, particularly people from marginalized social groups such as\nelderly, socially and economically challenged communities, and younger\nparticipants. This paper addresses this gap by specifically focusing on these\nunderrepresented groups, emphasizing the need for exploring their\nunderstandings and percepts of online data practices. Drawing on three\nsemi-structured focus group interviews, the paper asks: to what extent can\npublic attitudes and concerns about data sharing on the internet inform the\npotential strategies and frameworks necessary to enhance digital trust and\ndemocratic data agency particularly among marginalized groups in Denmark? The\nstudy explores the types of information, levels of transparency, and agency\npeople desire in their daily online data practices. Additionally, it explores\nhow these insights can potentially inform the future development of fair data\nstrategies and technological approaches to enhance digital trust and democratic\ndata agency. Key findings point out the need for transparent, accessible\nprivacy policies and data management tools, emphasizing that transparency alone\nis insufficient without enhancing democratic agency to address trust issues and\nfoster a more inclusive digital environment.\n  Keywords: public understanding, personal data, digital trust, data practices,\ndata agency",
    "published_date": "2025-03-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.05058v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.05012v1",
    "title": "LLMs' Reshaping of People, Processes, Products, and Society in Software Development: A Comprehensive Exploration with Early Adopters",
    "authors": [
      "Benyamin Tabarsi",
      "Heidi Reichert",
      "Ally Limke",
      "Sandeep Kuttal",
      "Tiffany Barnes"
    ],
    "author_ids": [],
    "abstract": "Large language models (LLMs) like OpenAI ChatGPT, Google Gemini, and GitHub\nCopilot are rapidly gaining traction in the software industry, but their full\nimpact on software engineering remains insufficiently explored. Despite their\ngrowing adoption, there is a notable lack of formal, qualitative assessments of\nhow LLMs are applied in real-world software development contexts. To fill this\ngap, we conducted semi-structured interviews with sixteen early-adopter\nprofessional developers to explore their use of LLMs throughout various stages\nof the software development life cycle. Our investigation examines four\ndimensions: people - how LLMs affect individual developers and teams; process -\nhow LLMs alter software engineering workflows; product - LLM impact on software\nquality and innovation; and society - the broader socioeconomic and ethical\nimplications of LLM adoption. Thematic analysis of our data reveals that while\nLLMs have not fundamentally revolutionized the development process, they have\nsubstantially enhanced routine coding tasks, including code generation,\nrefactoring, and debugging. Developers reported the most effective outcomes\nwhen providing LLMs with clear, well-defined problem statements, indicating\nthat LLMs excel with decomposed problems and specific requirements.\nFurthermore, these early-adopters identified that LLMs offer significant value\nfor personal and professional development, aiding in learning new languages and\nconcepts. Early-adopters, highly skilled in software engineering and how LLMs\nwork, identified early and persisting challenges for software engineering, such\nas inaccuracies in generated content and the need for careful manual review\nbefore integrating LLM outputs into production environments. Our study provides\na nuanced understanding of how LLMs are shaping the landscape of software\ndevelopment, with their benefits, limitations, and ongoing implications.",
    "published_date": "2025-03-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.05012v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.04982v1",
    "title": "LVLM-Compress-Bench: Benchmarking the Broader Impact of Large Vision-Language Model Compression",
    "authors": [
      "Souvik Kundu",
      "Anahita Bhiwandiwalla",
      "Sungduk Yu",
      "Phillip Howard",
      "Tiep Le",
      "Sharath Nittur Sridhar",
      "David Cobbley",
      "Hao Kang",
      "Vasudev Lal"
    ],
    "author_ids": [],
    "abstract": "Despite recent efforts in understanding the compression impact on large\nlanguage models (LLMs) in terms of their downstream task performance and\ntrustworthiness on relatively simpler uni-modal benchmarks (for example,\nquestion answering, common sense reasoning), their detailed study on\nmulti-modal Large Vision-Language Models (LVLMs) is yet to be unveiled. Towards\nmitigating this gap, we present LVLM-Compress-Bench, a framework to first\nthoroughly study the broad impact of compression on the generative performance\nof LVLMs with multi-modal input driven tasks. In specific, we consider two\nmajor classes of compression for autoregressive models, namely KV cache and\nweight compression, for the dynamically growing intermediate cache and static\nweights, respectively.\n  We use four LVLM variants of the popular LLaVA framework to present our\nanalysis via integrating various state-of-the-art KV and weight compression\nmethods including uniform, outlier-reduced, and group quantization for the KV\ncache and weights. With this framework we demonstrate on ten different\nmulti-modal datasets with different capabilities including recognition,\nknowledge, language generation, spatial awareness, visual reasoning,\nhallucination and visual illusion identification, toxicity, stereotypes and\nbias. In specific, our framework demonstrates the compression impact on both\ngeneral and ethically critical metrics leveraging a combination of real world\nand synthetic datasets to encompass diverse societal intersectional attributes.\nExtensive experimental evaluations yield diverse and intriguing observations on\nthe behavior of LVLMs at different quantization budget of KV and weights, in\nboth maintaining and losing performance as compared to the baseline model with\nFP16 data format.\n  Code will be open-sourced at\nhttps://github.com/opengear-project/LVLM-compress-bench.",
    "published_date": "2025-03-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.04982v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.04971v1",
    "title": "Incentivizing Multi-Tenant Split Federated Learning for Foundation Models at the Network Edge",
    "authors": [
      "Songyuan Li",
      "Jia Hu",
      "Geyong Min",
      "Haojun Huang"
    ],
    "author_ids": [],
    "abstract": "Foundation models (FMs) such as GPT-4 exhibit exceptional generative\ncapabilities across diverse downstream tasks through fine-tuning. Split\nFederated Learning (SFL) facilitates privacy-preserving FM fine-tuning on\nresource-constrained local devices by offloading partial FM computations to\nedge servers, enabling device-edge synergistic fine-tuning. Practical edge\nnetworks often host multiple SFL tenants to support diversified downstream\ntasks. However, existing research primarily focuses on single-tenant SFL\nscenarios, and lacks tailored incentive mechanisms for multi-tenant settings,\nwhich are essential to effectively coordinate self-interested local devices for\nparticipation in various downstream tasks, ensuring that each SFL tenant's\ndistinct FM fine-tuning requirements (e.g., FM types, performance targets, and\nfine-tuning deadlines) are met. To address this gap, we propose a novel\nPrice-Incentive Mechanism (PRINCE) that guides multiple SFL tenants to offer\nstrategic price incentives, which solicit high-quality device participation for\nefficient FM fine-tuning. Specifically, we first develop a bias-resilient\nglobal SFL model aggregation scheme to eliminate model biases caused by\nindependent device participation. We then derive a rigorous SFL convergence\nbound to evaluate the contributions of heterogeneous devices to FM performance\nimprovements, guiding the incentive strategies of SFL tenants. Furthermore, we\nmodel inter-tenant device competition as a congestion game for Stackelberg\nequilibrium (SE) analysis, deriving each SFL tenant's optimal incentive\nstrategy. Extensive simulations involving four representative SFL tenant types\n(ViT, BERT, Whisper, and LLaMA) across diverse data modalities (text, images,\nand audio) demonstrate that PRINCE accelerates FM fine-tuning by up to 3.07x\ncompared to state-of-the-art approaches, while consistently meeting fine-tuning\nperformance targets.",
    "published_date": "2025-03-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.04971v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.04957v1",
    "title": "SafeArena: Evaluating the Safety of Autonomous Web Agents",
    "authors": [
      "Ada Defne Tur",
      "Nicholas Meade",
      "Xing Han Lù",
      "Alejandra Zambrano",
      "Arkil Patel",
      "Esin Durmus",
      "Spandana Gella",
      "Karolina Stańczak",
      "Siva Reddy"
    ],
    "author_ids": [],
    "abstract": "LLM-based agents are becoming increasingly proficient at solving web-based\ntasks. With this capability comes a greater risk of misuse for malicious\npurposes, such as posting misinformation in an online forum or selling illicit\nsubstances on a website. To evaluate these risks, we propose SafeArena, the\nfirst benchmark to focus on the deliberate misuse of web agents. SafeArena\ncomprises 250 safe and 250 harmful tasks across four websites. We classify the\nharmful tasks into five harm categories -- misinformation, illegal activity,\nharassment, cybercrime, and social bias, designed to assess realistic misuses\nof web agents. We evaluate leading LLM-based web agents, including GPT-4o,\nClaude-3.5 Sonnet, Qwen-2-VL 72B, and Llama-3.2 90B, on our benchmark. To\nsystematically assess their susceptibility to harmful tasks, we introduce the\nAgent Risk Assessment framework that categorizes agent behavior across four\nrisk levels. We find agents are surprisingly compliant with malicious requests,\nwith GPT-4o and Qwen-2 completing 34.7% and 27.3% of harmful requests,\nrespectively. Our findings highlight the urgent need for safety alignment\nprocedures for web agents. Our benchmark is available here:\nhttps://safearena.github.io",
    "published_date": "2025-03-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.04957v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.04696v1",
    "title": "Assessing Student Adoption of Generative Artificial Intelligence across Engineering Education from 2023 to 2024",
    "authors": [
      "Jesan Ahammed Ovi",
      "Gabe Fierro",
      "C. Estelle Smith"
    ],
    "author_ids": [],
    "abstract": "Generative Artificial Intelligence (GenAI) tools and models have the\npotential to re-shape educational needs, norms, practices, and policies in all\nsectors of engineering education. Empirical data, rather than anecdata and\nassumptions, on how engineering students have adopted GenAI is essential to\ndeveloping a foundational understanding of students' GenAI-related behaviors\nand needs during academic training. This data will also help formulate\neffective responses to GenAI by both academic institutions and industrial\nemployers. We collected two representative survey samples at the Colorado\nSchool of Mines, a small engineering-focused R-1 university in the USA, in May\n2023 ($n_1=601$) and September 2024 ($n_2=862$) to address research questions\nrelated to (RQ1) how GenAI has been adopted by engineering students, including\nmotivational and demographic factors contributing to GenAI use, (RQ2) students'\nethical concerns about GenAI, and (RQ3) students' perceived benefits v.s. harms\nfor themselves, science, and society. Analysis revealed a statistically\nsignificant rise in GenAI adoption rates from 2023 to 2024. Students\npredominantly leverage GenAI tools to deepen understanding, enhance work\nquality, and stay informed about emerging technologies. Although most students\nassess their own usage of GenAI as ethical and beneficial, they nonetheless\nexpressed significant concerns regarding GenAI and its impacts on society. We\ncollected student estimates of ``P(doom)'' and discovered a bimodal\ndistribution. Thus, we show that the student body at Mines is polarized with\nrespect to future impacts of GenAI on the engineering workforce and society,\ndespite being increasingly willing to explore GenAI over time. We discuss\nimplications of these findings for future research and for integrating GenAI in\nengineering education.",
    "published_date": "2025-03-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.04696v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.04692v1",
    "title": "The Influence of Prior Discourse on Conversational Agent-Driven Decision-Making",
    "authors": [
      "Stephen Pilli",
      "Vivek Nallur"
    ],
    "author_ids": [],
    "abstract": "Persuasion through conversation has been the focus of much research. Nudging\nis a popular strategy to influence decision-making in physical and digital\nsettings. However, conversational agents employing \"nudging\" have not received\nsignificant attention. We explore the manifestation of cognitive biases-the\nunderlying psychological mechanisms of nudging-and investigate how the\ncomplexity of prior dialogue tasks impacts decision-making facilitated by\nconversational agents. Our research used a between-group experimental design,\ninvolving 756 participants randomly assigned to either a simple or complex task\nbefore encountering a decision-making scenario. Three scenarios were adapted\nfrom Samuelson's classic experiments on status-quo bias, the underlying\nmechanism of default nudges. Our results aligned with previous studies in two\nout of three simple-task scenarios. Increasing task complexity consistently\nshifted effect-sizes toward our hypothesis, though bias was significant in only\none case. These findings inform conversational nudging strategies and highlight\ninherent biases relevant to behavioural economics.",
    "published_date": "2025-03-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.ET"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.04692v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.04576v1",
    "title": "Belonging Beyond Code: Queer Software Engineering and Humanities Student Experiences",
    "authors": [
      "Emily Vorderwülbeke",
      "Isabella Graßl"
    ],
    "author_ids": [],
    "abstract": "Queer students often encounter discrimination and a lack of belonging in\ntheir academic environments. This may be especially true in heteronormative\nmale-dominated fields like software engineering, which already faces a\ndiversity crisis. In contrast, disciplines like humanities have a higher\nproportion of queer students, suggesting a more diverse academic culture. While\nprior research has explored queer students' challenges in STEM fields, limited\nattention has been given to how experiences differ between the sociotechnical,\nyet highly heteronormative, field of software engineering and the\nsocioculturally inclusive humanities. This study addresses that gap by\ncomparing 165 queer software engineering and 119 queer humanities students\nexperiences. Our findings reveal that queer students in software engineering\nare less likely to be open about their sexuality, report a significantly lower\nsense of belonging, and encounter more academic challenges compared to their\npeers in the humanities. Despite these challenges, queer software engineering\nstudents show greater determination to continue their studies. These insights\nsuggest that software engineering could enhance inclusivity by adopting\npractices commonly seen in the humanities, such as integrating inclusive\npolicies in classrooms, to create a more welcoming environment where queer\nstudents can thrive.",
    "published_date": "2025-03-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.04576v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.04550v1",
    "title": "Benchmarking Reasoning Robustness in Large Language Models",
    "authors": [
      "Tong Yu",
      "Yongcheng Jing",
      "Xikun Zhang",
      "Wentao Jiang",
      "Wenjie Wu",
      "Yingjie Wang",
      "Wenbin Hu",
      "Bo Du",
      "Dacheng Tao"
    ],
    "author_ids": [],
    "abstract": "Despite the recent success of large language models (LLMs) in reasoning such\nas DeepSeek, we for the first time identify a key dilemma in reasoning\nrobustness and generalization: significant performance degradation on novel or\nincomplete data, suggesting a reliance on memorized patterns rather than\nsystematic reasoning. Our closer examination reveals four key unique\nlimitations underlying this issue:(1) Positional bias--models favor earlier\nqueries in multi-query inputs but answering the wrong one in the latter (e.g.,\nGPT-4o's accuracy drops from 75.8 percent to 72.8 percent); (2) Instruction\nsensitivity--performance declines by 5.0 to 7.5 percent in the Qwen2.5 Series\nand by 5.0 percent in DeepSeek-V3 with auxiliary guidance; (3) Numerical\nfragility--value substitution sharply reduces accuracy (e.g., GPT-4o drops from\n97.5 percent to 82.5 percent, GPT-o1-mini drops from 97.5 percent to 92.5\npercent); and (4) Memory dependence--models resort to guesswork when missing\ncritical data. These findings further highlight the reliance on heuristic\nrecall over rigorous logical inference, demonstrating challenges in reasoning\nrobustness. To comprehensively investigate these robustness challenges, this\npaper introduces a novel benchmark, termed as Math-RoB, that exploits\nhallucinations triggered by missing information to expose reasoning gaps. This\nis achieved by an instruction-based approach to generate diverse datasets that\nclosely resemble training distributions, facilitating a holistic robustness\nassessment and advancing the development of more robust reasoning frameworks.\nBad character(s) in field Abstract.",
    "published_date": "2025-03-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.04550v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.04542v1",
    "title": "Inducing Efficient and Equitable Professional Networks through Link Recommendations",
    "authors": [
      "Cynthia Dwork",
      "Chris Hays",
      "Lunjia Hu",
      "Nicole Immorlica",
      "Juan Perdomo"
    ],
    "author_ids": [],
    "abstract": "Professional networks are a key determinant of individuals' labor market\noutcomes. They may also play a role in either exacerbating or ameliorating\ninequality of opportunity across demographic groups. In a theoretical model of\nprofessional network formation, we show that inequality can increase even\nwithout exogenous in-group preferences, confirming and complementing existing\ntheoretical literature. Increased inequality emerges from the differential\nleverage privileged and unprivileged individuals have in forming connections\ndue to their asymmetric ex ante prospects. This is a formalization of a source\nof inequality in the labor market which has not been previously explored.\n  We next show how inequality-aware platforms may reduce inequality by\nsubsidizing connections, through link recommendations that reduce costs,\nbetween privileged and unprivileged individuals. Indeed, mixed-privilege\nconnections turn out to be welfare improving, over all possible equilibria,\ncompared to not recommending links or recommending some smaller fraction of\ncross-group links. Taken together, these two findings reveal a stark reality:\nprofessional networking platforms that fail to foster integration in the link\nformation process risk reducing the platform's utility to its users and\nexacerbating existing labor market inequality.",
    "published_date": "2025-03-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.GT",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.04542v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.04521v1",
    "title": "Dynamic Pricing for On-Demand DNN Inference in the Edge-AI Market",
    "authors": [
      "Songyuan Li",
      "Jia Hu",
      "Geyong Min",
      "Haojun Huang",
      "Jiwei Huang"
    ],
    "author_ids": [],
    "abstract": "The convergence of edge computing and AI gives rise to Edge-AI, which enables\nthe deployment of real-time AI applications and services at the network edge.\nOne of the fundamental research issues in Edge-AI is edge inference\nacceleration, which aims to realize low-latency high-accuracy DNN inference\nservices by leveraging the fine-grained offloading of partitioned inference\ntasks from end devices to edge servers. However, existing research has yet to\nadopt a practical Edge-AI market perspective, which would systematically\nexplore the personalized inference needs of AI users (e.g., inference accuracy,\nlatency, and task complexity), the revenue incentives for AI service providers\nthat offer edge inference services, and multi-stakeholder governance within a\nmarket-oriented context. To bridge this gap, we propose an Auction-based Edge\nInference Pricing Mechanism (AERIA) for revenue maximization to tackle the\nmulti-dimensional optimization problem of DNN model partition, edge inference\npricing, and resource allocation. We investigate the multi-exit device-edge\nsynergistic inference scheme for on-demand DNN inference acceleration, and\nanalyse the auction dynamics amongst the AI service providers, AI users and\nedge infrastructure provider. Owing to the strategic mechanism design via\nrandomized consensus estimate and cost sharing techniques, the Edge-AI market\nattains several desirable properties, including competitiveness in revenue\nmaximization, incentive compatibility, and envy-freeness, which are crucial to\nmaintain the effectiveness, truthfulness, and fairness of our auction outcomes.\nThe extensive simulation experiments based on four representative DNN inference\nworkloads demonstrate that our AERIA mechanism significantly outperforms\nseveral state-of-the-art approaches in revenue maximization, demonstrating the\nefficacy of AERIA for on-demand DNN inference in the Edge-AI market.",
    "published_date": "2025-03-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.CE",
      "cs.DC",
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.04521v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.04372v1",
    "title": "Assumed Identities: Quantifying Gender Bias in Machine Translation of Ambiguous Occupational Terms",
    "authors": [
      "Orfeas Menis Mastromichalakis",
      "Giorgos Filandrianos",
      "Maria Symeonaki",
      "Giorgos Stamou"
    ],
    "author_ids": [],
    "abstract": "Machine Translation (MT) systems frequently encounter ambiguous scenarios\nwhere they must assign gender to certain occupations when translating without\nexplicit guidance or contextual cues. While individual translations in such\ncases may not be inherently biased, systematic patterns-such as the repeated\nassociation of certain professions with specific genders-can emerge, reflecting\nand perpetuating societal stereotypes. This ambiguity challenges traditional\ninstance-level single-answer evaluation approaches, as no single gold standard\ntranslation exists. To address this, we propose an approach that evaluates\ngender bias through aggregated model responses. Specifically, we introduce a\nmethodology to detect gender imbalances between source texts and translations,\na benchmarking dataset with ambiguous English inputs, and probability-based\nmetrics to quantify a model's divergence from normative standards or reference\ndistributions.",
    "published_date": "2025-03-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.04372v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.04363v1",
    "title": "Causally Reliable Concept Bottleneck Models",
    "authors": [
      "Giovanni De Felice",
      "Arianna Casanova Flores",
      "Francesco De Santis",
      "Silvia Santini",
      "Johannes Schneider",
      "Pietro Barbiero",
      "Alberto Termine"
    ],
    "author_ids": [],
    "abstract": "Concept-based models are an emerging paradigm in deep learning that\nconstrains the inference process to operate through human-interpretable\nconcepts, facilitating explainability and human interaction. However, these\narchitectures, on par with popular opaque neural models, fail to account for\nthe true causal mechanisms underlying the target phenomena represented in the\ndata. This hampers their ability to support causal reasoning tasks, limits\nout-of-distribution generalization, and hinders the implementation of fairness\nconstraints. To overcome these issues, we propose \\emph{Causally reliable\nConcept Bottleneck Models} (C$^2$BMs), a class of concept-based architectures\nthat enforce reasoning through a bottleneck of concepts structured according to\na model of the real-world causal mechanisms. We also introduce a pipeline to\nautomatically learn this structure from observational data and\n\\emph{unstructured} background knowledge (e.g., scientific literature).\nExperimental evidence suggest that C$^2$BM are more interpretable, causally\nreliable, and improve responsiveness to interventions w.r.t. standard opaque\nand concept-based models, while maintaining their accuracy.",
    "published_date": "2025-03-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.04363v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.04357v1",
    "title": "scDD: Latent Codes Based scRNA-seq Dataset Distillation with Foundation Model Knowledge",
    "authors": [
      "Zhen Yu",
      "Jianan Han",
      "Yang Liu",
      "Qingchao Chen"
    ],
    "author_ids": [],
    "abstract": "Single-cell RNA sequencing (scRNA-seq) technology has profiled hundreds of\nmillions of human cells across organs, diseases, development and perturbations\nto date. However, the high-dimensional sparsity, batch effect noise, category\nimbalance, and ever-increasing data scale of the original sequencing data pose\nsignificant challenges for multi-center knowledge transfer, data fusion, and\ncross-validation between scRNA-seq datasets. To address these barriers, (1) we\nfirst propose a latent codes-based scRNA-seq dataset distillation framework\nnamed scDD, which transfers and distills foundation model knowledge and\noriginal dataset information into a compact latent space and generates\nsynthetic scRNA-seq dataset by a generator to replace the original dataset.\nThen, (2) we propose a single-step conditional diffusion generator named SCDG,\nwhich perform single-step gradient back-propagation to help scDD optimize\ndistillation quality and avoid gradient decay caused by multi-step\nback-propagation. Meanwhile, SCDG ensures the scRNA-seq data characteristics\nand inter-class discriminability of the synthetic dataset through flexible\nconditional control and generation quality assurance. Finally, we propose a\ncomprehensive benchmark to evaluate the performance of scRNA-seq dataset\ndistillation in different data analysis tasks. It is validated that our\nproposed method can achieve 7.61% absolute and 15.70% relative improvement over\nprevious state-of-the-art methods on average task.",
    "published_date": "2025-03-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.04357v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.04292v1",
    "title": "A Study on Malicious Browser Extensions in 2025",
    "authors": [
      "Shreya Singh",
      "Gaurav Varshney",
      "Tarun Kumar Singh",
      "Vidhi Mishra"
    ],
    "author_ids": [],
    "abstract": "Browser extensions are additional tools developed by third parties that\nintegrate with web browsers to extend their functionality beyond standard\ncapabilities. However, the browser extension platform is increasingly being\nexploited by hackers to launch sophisticated cyber threats. These threats\nencompass a wide range of malicious activities, including but not limited to\nphishing, spying, Distributed Denial of Service (DDoS) attacks, email spamming,\naffiliate fraud, malvertising, and payment fraud. This paper examines the\nevolving threat landscape of malicious browser extensions in 2025, focusing on\nMozilla Firefox and Chrome. Our research successfully bypassed security\nmechanisms of Firefox and Chrome, demonstrating that malicious extensions can\nstill be developed, published, and executed within the Mozilla Add-ons Store\nand Chrome Web Store. These findings highlight the persisting weaknesses in\nbrowser's vetting process and security framework. It provides insights into the\nrisks associated with browser extensions, helping users understand these\nthreats while aiding the industry in developing controls and countermeasures to\ndefend against such attacks. All experiments discussed in this paper were\nconducted in a controlled laboratory environment by the researchers, adhering\nto proper ethical guidelines. The sole purpose of these experiments is to raise\nsecurity awareness among the industry, research community, and the general\npublic.",
    "published_date": "2025-03-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.04292v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.04184v1",
    "title": "Large-Scale AI in Telecom: Charting the Roadmap for Innovation, Scalability, and Enhanced Digital Experiences",
    "authors": [
      "Adnan Shahid",
      "Adrian Kliks",
      "Ahmed Al-Tahmeesschi",
      "Ahmed Elbakary",
      "Alexandros Nikou",
      "Ali Maatouk",
      "Ali Mokh",
      "Amirreza Kazemi",
      "Antonio De Domenico",
      "Athanasios Karapantelakis",
      "Bo Cheng",
      "Bo Yang",
      "Bohao Wang",
      "Carlo Fischione",
      "Chao Zhang",
      "Chaouki Ben Issaid",
      "Chau Yuen",
      "Chenghui Peng",
      "Chongwen Huang",
      "Christina Chaccour",
      "Christo Kurisummoottil Thomas",
      "Dheeraj Sharma",
      "Dimitris Kalogiros",
      "Dusit Niyato",
      "Eli De Poorter",
      "Elissa Mhanna",
      "Emilio Calvanese Strinati",
      "Faouzi Bader",
      "Fathi Abdeldayem",
      "Fei Wang",
      "Fenghao Zhu",
      "Gianluca Fontanesi",
      "Giovanni Geraci",
      "Haibo Zhou",
      "Hakimeh Purmehdi",
      "Hamed Ahmadi",
      "Hang Zou",
      "Hongyang Du",
      "Hoon Lee",
      "Howard H. Yang",
      "Iacopo Poli",
      "Igor Carron",
      "Ilias Chatzistefanidis",
      "Inkyu Lee",
      "Ioannis Pitsiorlas",
      "Jaron Fontaine",
      "Jiajun Wu",
      "Jie Zeng",
      "Jinan Li",
      "Jinane Karam",
      "Johny Gemayel",
      "Juan Deng",
      "Julien Frison",
      "Kaibin Huang",
      "Kehai Qiu",
      "Keith Ball",
      "Kezhi Wang",
      "Kun Guo",
      "Leandros Tassiulas",
      "Lecorve Gwenole",
      "Liexiang Yue",
      "Lina Bariah",
      "Louis Powell",
      "Marcin Dryjanski",
      "Maria Amparo Canaveras Galdon",
      "Marios Kountouris",
      "Maryam Hafeez",
      "Maxime Elkael",
      "Mehdi Bennis",
      "Mehdi Boudjelli",
      "Meiling Dai",
      "Merouane Debbah",
      "Michele Polese",
      "Mohamad Assaad",
      "Mohamed Benzaghta",
      "Mohammad Al Refai",
      "Moussab Djerrab",
      "Mubeen Syed",
      "Muhammad Amir",
      "Na Yan",
      "Najla Alkaabi",
      "Nan Li",
      "Nassim Sehad",
      "Navid Nikaein",
      "Omar Hashash",
      "Pawel Sroka",
      "Qianqian Yang",
      "Qiyang Zhao",
      "Rasoul Nikbakht Silab",
      "Rex Ying",
      "Roberto Morabito",
      "Rongpeng Li",
      "Ryad Madi",
      "Salah Eddine El Ayoubi",
      "Salvatore D'Oro",
      "Samson Lasaulce",
      "Serveh Shalmashi",
      "Sige Liu",
      "Sihem Cherrared",
      "Swarna Bindu Chetty",
      "Swastika Dutta",
      "Syed A. R. Zaidi",
      "Tianjiao Chen",
      "Timothy Murphy",
      "Tommaso Melodia",
      "Tony Q. S. Quek",
      "Vishnu Ram",
      "Walid Saad",
      "Wassim Hamidouche",
      "Weilong Chen",
      "Xiaoou Liu",
      "Xiaoxue Yu",
      "Xijun Wang",
      "Xingyu Shang",
      "Xinquan Wang",
      "Xuelin Cao",
      "Yang Su",
      "Yanping Liang",
      "Yansha Deng",
      "Yifan Yang",
      "Yingping Cui",
      "Yu Sun",
      "Yuxuan Chen",
      "Yvan Pointurier",
      "Zeinab Nehme",
      "Zeinab Nezami",
      "Zhaohui Yang",
      "Zhaoyang Zhang",
      "Zhe Liu",
      "Zhenyu Yang",
      "Zhu Han",
      "Zhuang Zhou",
      "Zihan Chen",
      "Zirui Chen",
      "Zitao Shuai"
    ],
    "author_ids": [],
    "abstract": "This white paper discusses the role of large-scale AI in the\ntelecommunications industry, with a specific focus on the potential of\ngenerative AI to revolutionize network functions and user experiences,\nespecially in the context of 6G systems. It highlights the development and\ndeployment of Large Telecom Models (LTMs), which are tailored AI models\ndesigned to address the complex challenges faced by modern telecom networks.\nThe paper covers a wide range of topics, from the architecture and deployment\nstrategies of LTMs to their applications in network management, resource\nallocation, and optimization. It also explores the regulatory, ethical, and\nstandardization considerations for LTMs, offering insights into their future\nintegration into telecom infrastructure. The goal is to provide a comprehensive\nroadmap for the adoption of LTMs to enhance scalability, performance, and\nuser-centric innovation in telecom networks.",
    "published_date": "2025-03-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.04184v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.04855v1",
    "title": "A characterization of sample adaptivity in UCB data",
    "authors": [
      "Yilun Chen",
      "Jiaqi Lu"
    ],
    "author_ids": [],
    "abstract": "We characterize a joint CLT of the number of pulls and the sample mean reward\nof the arms in a stochastic two-armed bandit environment under UCB algorithms.\nSeveral implications of this result are in place: (1) a nonstandard CLT of the\nnumber of pulls hence pseudo-regret that smoothly interpolates between a\nstandard form in the large arm gap regime and a slow-concentration form in the\nsmall arm gap regime, and (2) a heuristic derivation of the sample bias up to\nits leading order from the correlation between the number of pulls and sample\nmeans. Our analysis framework is based on a novel perturbation analysis, which\nis of broader interest on its own.",
    "published_date": "2025-03-06T00:00:00",
    "year": 2025,
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.PR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.04855v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.04131v1",
    "title": "Q-PART: Quasi-Periodic Adaptive Regression with Test-time Training for Pediatric Left Ventricular Ejection Fraction Regression",
    "authors": [
      "Jie Liu",
      "Tiexin Qin",
      "Hui Liu",
      "Yilei Shi",
      "Lichao Mou",
      "Xiao Xiang Zhu",
      "Shiqi Wang",
      "Haoliang Li"
    ],
    "author_ids": [],
    "abstract": "In this work, we address the challenge of adaptive pediatric Left Ventricular\nEjection Fraction (LVEF) assessment. While Test-time Training (TTT) approaches\nshow promise for this task, they suffer from two significant limitations.\nExisting TTT works are primarily designed for classification tasks rather than\ncontinuous value regression, and they lack mechanisms to handle the\nquasi-periodic nature of cardiac signals. To tackle these issues, we propose a\nnovel \\textbf{Q}uasi-\\textbf{P}eriodic \\textbf{A}daptive \\textbf{R}egression\nwith \\textbf{T}est-time Training (Q-PART) framework. In the training stage, the\nproposed Quasi-Period Network decomposes the echocardiogram into periodic and\naperiodic components within latent space by combining parameterized helix\ntrajectories with Neural Controlled Differential Equations. During inference,\nour framework further employs a variance minimization strategy across image\naugmentations that simulate common quality issues in echocardiogram\nacquisition, along with differential adaptation rates for periodic and\naperiodic components. Theoretical analysis is provided to demonstrate that our\nvariance minimization objective effectively bounds the regression error under\nmild conditions. Furthermore, extensive experiments across three pediatric age\ngroups demonstrate that Q-PART not only significantly outperforms existing\napproaches in pediatric LVEF prediction, but also exhibits strong clinical\nscreening capability with high mAUROC scores (up to 0.9747) and maintains\ngender-fair performance across all metrics, validating its robustness and\npractical utility in pediatric echocardiography analysis.",
    "published_date": "2025-03-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.04131v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.04085v1",
    "title": "SED2AM: Solving Multi-Trip Time-Dependent Vehicle Routing Problem using Deep Reinforcement Learning",
    "authors": [
      "Arash Mozhdehi",
      "Yunli Wang",
      "Sun Sun",
      "Xin Wang"
    ],
    "author_ids": [],
    "abstract": "Deep reinforcement learning (DRL)-based frameworks, featuring\nTransformer-style policy networks, have demonstrated their efficacy across\nvarious vehicle routing problem (VRP) variants. However, the application of\nthese methods to the multi-trip time-dependent vehicle routing problem\n(MTTDVRP) with maximum working hours constraints -- a pivotal element of urban\nlogistics -- remains largely unexplored. This paper introduces a DRL-based\nmethod called the Simultaneous Encoder and Dual Decoder Attention Model\n(SED2AM), tailored for the MTTDVRP with maximum working hours constraints. The\nproposed method introduces a temporal locality inductive bias to the encoding\nmodule of the policy networks, enabling it to effectively account for the\ntime-dependency in travel distance or time. The decoding module of SED2AM\nincludes a vehicle selection decoder that selects a vehicle from the fleet,\neffectively associating trips with vehicles for functional multi-trip routing.\nAdditionally, this decoding module is equipped with a trip construction decoder\nleveraged for constructing trips for the vehicles. This policy model is\nequipped with two classes of state representations, fleet state and routing\nstate, providing the information needed for effective route construction in the\npresence of maximum working hours constraints. Experimental results using\nreal-world datasets from two major Canadian cities not only show that SED2AM\noutperforms the current state-of-the-art DRL-based and metaheuristic-based\nbaselines but also demonstrate its generalizability to solve larger-scale\nproblems.",
    "published_date": "2025-03-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.04085v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.04064v1",
    "title": "Uncovering inequalities in new knowledge learning by large language models across different languages",
    "authors": [
      "Chenglong Wang",
      "Haoyu Tang",
      "Xiyuan Yang",
      "Yueqi Xie",
      "Jina Suh",
      "Sunayana Sitaram",
      "Junming Huang",
      "Yu Xie",
      "Zhaoya Gong",
      "Xing Xie",
      "Fangzhao Wu"
    ],
    "author_ids": [],
    "abstract": "As large language models (LLMs) gradually become integral tools for problem\nsolving in daily life worldwide, understanding linguistic inequality is\nbecoming increasingly important. Existing research has primarily focused on\nstatic analyses that assess the disparities in the existing knowledge and\ncapabilities of LLMs across languages. However, LLMs are continuously evolving,\nacquiring new knowledge to generate up-to-date, domain-specific responses.\nInvestigating linguistic inequalities within this dynamic process is,\ntherefore, also essential. In this paper, we explore inequalities in new\nknowledge learning by LLMs across different languages and four key dimensions:\neffectiveness, transferability, prioritization, and robustness. Through\nextensive experiments under two settings (in-context learning and fine-tuning)\nusing both proprietary and open-source models, we demonstrate that low-resource\nlanguages consistently face disadvantages across all four dimensions. By\nshedding light on these disparities, we aim to raise awareness of linguistic\ninequalities in LLMs' new knowledge learning, fostering the development of more\ninclusive and equitable future LLMs.",
    "published_date": "2025-03-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.04064v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.03684v1",
    "title": "Towards Trustworthy Federated Learning",
    "authors": [
      "Alina Basharat",
      "Yijun Bian",
      "Ping Xu",
      "Zhi Tian"
    ],
    "author_ids": [],
    "abstract": "This paper develops a comprehensive framework to address three critical\ntrustworthy challenges in federated learning (FL): robustness against Byzantine\nattacks, fairness, and privacy preservation. To improve the system's defense\nagainst Byzantine attacks that send malicious information to bias the system's\nperformance, we develop a Two-sided Norm Based Screening (TNBS) mechanism,\nwhich allows the central server to crop the gradients that have the l lowest\nnorms and h highest norms. TNBS functions as a screening tool to filter out\npotential malicious participants whose gradients are far from the honest ones.\nTo promote egalitarian fairness, we adopt the q-fair federated learning\n(q-FFL). Furthermore, we adopt a differential privacy-based scheme to prevent\nraw data at local clients from being inferred by curious parties. Convergence\nguarantees are provided for the proposed framework under different scenarios.\nExperimental results on real datasets demonstrate that the proposed framework\neffectively improves robustness and fairness while managing the trade-off\nbetween privacy and accuracy. This work appears to be the first study that\nexperimentally and theoretically addresses fairness, privacy, and robustness in\ntrustworthy FL.",
    "published_date": "2025-03-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.03684v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.03642v2",
    "title": "Improved FPT Approximation Algorithms for TSP",
    "authors": [
      "Jingyang Zhao",
      "Zimo Sheng",
      "Mingyu Xiao"
    ],
    "author_ids": [],
    "abstract": "TSP is a classic and extensively studied problem with numerous real-world\napplications in artificial intelligence and operations research. It is\nwell-known that TSP admits a constant approximation ratio on metric graphs but\nbecomes NP-hard to approximate within any computable function $f(n)$ on general\ngraphs. This disparity highlights a significant gap between the results on\nmetric graphs and general graphs. Recent research has introduced some\nparameters to measure the ``distance'' of general graphs from being metric and\nexplored FPT approximation algorithms parameterized by these parameters. Two\ncommonly studied parameters are $p$, the number of vertices in triangles\nviolating the triangle inequality, and $q$, the minimum number of vertices\nwhose removal results in a metric graph. In this paper, we present improved FPT\napproximation algorithms with respect to these two parameters. For $p$, we\npropose an FPT algorithm with a 1.5-approximation ratio, improving upon the\nprevious ratio of 2.5. For $q$, we significantly enhance the approximation\nratio from 11 to 3, advancing the state of the art in both cases.",
    "published_date": "2025-03-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.03642v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.03595v1",
    "title": "Towards Understanding Text Hallucination of Diffusion Models via Local Generation Bias",
    "authors": [
      "Rui Lu",
      "Runzhe Wang",
      "Kaifeng Lyu",
      "Xitai Jiang",
      "Gao Huang",
      "Mengdi Wang"
    ],
    "author_ids": [],
    "abstract": "Score-based diffusion models have achieved incredible performance in\ngenerating realistic images, audio, and video data. While these models produce\nhigh-quality samples with impressive details, they often introduce unrealistic\nartifacts, such as distorted fingers or hallucinated texts with no meaning.\nThis paper focuses on textual hallucinations, where diffusion models correctly\ngenerate individual symbols but assemble them in a nonsensical manner. Through\nexperimental probing, we consistently observe that such phenomenon is\nattributed it to the network's local generation bias. Denoising networks tend\nto produce outputs that rely heavily on highly correlated local regions,\nparticularly when different dimensions of the data distribution are nearly\npairwise independent. This behavior leads to a generation process that\ndecomposes the global distribution into separate, independent distributions for\neach symbol, ultimately failing to capture the global structure, including\nunderlying grammar. Intriguingly, this bias persists across various denoising\nnetwork architectures including MLP and transformers which have the structure\nto model global dependency. These findings also provide insights into\nunderstanding other types of hallucinations, extending beyond text, as a result\nof implicit biases in the denoising models. Additionally, we theoretically\nanalyze the training dynamics for a specific case involving a two-layer MLP\nlearning parity points on a hypercube, offering an explanation of its\nunderlying mechanism.",
    "published_date": "2025-03-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.03595v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.03582v1",
    "title": "Scaling Crowdsourced Election Monitoring: Construction and Evaluation of Classification Models for Multilingual and Cross-Domain Classification Settings",
    "authors": [
      "Jabez Magomere",
      "Scott Hale"
    ],
    "author_ids": [],
    "abstract": "The adoption of crowdsourced election monitoring as a complementary\nalternative to traditional election monitoring is on the rise. Yet, its\nreliance on digital response volunteers to manually process incoming election\nreports poses a significant scaling bottleneck. In this paper, we address the\nchallenge of scaling crowdsourced election monitoring by advancing the task of\nautomated classification of crowdsourced election reports to multilingual and\ncross-domain classification settings. We propose a two-step classification\napproach of first identifying informative reports and then categorising them\ninto distinct information types. We conduct classification experiments using\nmultilingual transformer models such as XLM-RoBERTa and multilingual embeddings\nsuch as SBERT, augmented with linguistically motivated features. Our approach\nachieves F1-Scores of 77\\% for informativeness detection and 75\\% for\ninformation type classification. We conduct cross-domain experiments, applying\nmodels trained in a source electoral domain to a new target electoral domain in\nzero-shot and few-shot classification settings. Our results show promising\npotential for model transfer across electoral domains, with F1-Scores of 59\\%\nin zero-shot and 63\\% in few-shot settings. However, our analysis also reveals\na performance bias in detecting informative English reports over Swahili,\nlikely due to imbalances in the training data, indicating a need for caution\nwhen deploying classification models in real-world election scenarios.",
    "published_date": "2025-03-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.03582v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.04837v1",
    "title": "FedPalm: A General Federated Learning Framework for Closed- and Open-Set Palmprint Verification",
    "authors": [
      "Ziyuan Yang",
      "Yingyu Chen",
      "Chengrui Gao",
      "Andrew Beng Jin Teoh",
      "Bob Zhang",
      "Yi Zhang"
    ],
    "author_ids": [],
    "abstract": "Current deep learning (DL)-based palmprint verification models rely on\ncentralized training with large datasets, which raises significant privacy\nconcerns due to biometric data's sensitive and immutable nature. Federated\nlearning~(FL), a privacy-preserving distributed learning paradigm, offers a\ncompelling alternative by enabling collaborative model training without the\nneed for data sharing. However, FL-based palmprint verification faces critical\nchallenges, including data heterogeneity from diverse identities and the\nabsence of standardized evaluation benchmarks. This paper addresses these gaps\nby establishing a comprehensive benchmark for FL-based palmprint verification,\nwhich explicitly defines and evaluates two practical scenarios: closed-set and\nopen-set verification. We propose FedPalm, a unified FL framework that balances\nlocal adaptability with global generalization. Each client trains a\npersonalized textural expert tailored to local data and collaboratively\ncontributes to a shared global textural expert for extracting generalized\nfeatures. To further enhance verification performance, we introduce a Textural\nExpert Interaction Module that dynamically routes textural features among\nexperts to generate refined side textural features. Learnable parameters are\nemployed to model relationships between original and side features, fostering\ncross-texture-expert interaction and improving feature discrimination.\nExtensive experiments validate the effectiveness of FedPalm, demonstrating\nrobust performance across both scenarios and providing a promising foundation\nfor advancing FL-based palmprint verification research.",
    "published_date": "2025-03-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.04837v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.03562v3",
    "title": "Towards Visual Discrimination and Reasoning of Real-World Physical Dynamics: Physics-Grounded Anomaly Detection",
    "authors": [
      "Wenqiao Li",
      "Yao Gu",
      "Xintao Chen",
      "Xiaohao Xu",
      "Ming Hu",
      "Xiaonan Huang",
      "Yingna Wu"
    ],
    "author_ids": [],
    "abstract": "Humans detect real-world object anomalies by perceiving, interacting, and\nreasoning based on object-conditioned physical knowledge. The long-term goal of\nIndustrial Anomaly Detection (IAD) is to enable machines to autonomously\nreplicate this skill. However, current IAD algorithms are largely developed and\ntested on static, semantically simple datasets, which diverge from real-world\nscenarios where physical understanding and reasoning are essential. To bridge\nthis gap, we introduce the Physics Anomaly Detection (Phys-AD) dataset, the\nfirst large-scale, real-world, physics-grounded video dataset for industrial\nanomaly detection. Collected using a real robot arm and motor, Phys-AD provides\na diverse set of dynamic, semantically rich scenarios. The dataset includes\nmore than 6400 videos across 22 real-world object categories, interacting with\nrobot arms and motors, and exhibits 47 types of anomalies. Anomaly detection in\nPhys-AD requires visual reasoning, combining both physical knowledge and video\ncontent to determine object abnormality. We benchmark state-of-the-art anomaly\ndetection methods under three settings: unsupervised AD, weakly-supervised AD,\nand video-understanding AD, highlighting their limitations in handling\nphysics-grounded anomalies. Additionally, we introduce the Physics Anomaly\nExplanation (PAEval) metric, designed to assess the ability of visual-language\nfoundation models to not only detect anomalies but also provide accurate\nexplanations for their underlying physical causes. Our project is available at\nhttps://guyao2023.github.io/Phys-AD/.",
    "published_date": "2025-03-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.03562v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.03561v1",
    "title": "Transformer-Based Power Optimization for Max-Min Fairness in Cell-Free Massive MIMO",
    "authors": [
      "Irched Chafaa",
      "Giacomo Bacci",
      "Luca Sanguinetti"
    ],
    "author_ids": [],
    "abstract": "Power allocation is an important task in wireless communication networks.\nClassical optimization algorithms and deep learning methods, while effective in\nsmall and static scenarios, become either computationally demanding or\nunsuitable for large and dynamic networks with varying user loads. This letter\nexplores the potential of transformer-based deep learning models to address\nthese challenges. We propose a transformer neural network to jointly predict\noptimal uplink and downlink power using only user and access point positions.\nThe max-min fairness problem in cell-free massive multiple input multiple\noutput systems is considered. Numerical results show that the trained model\nprovides near-optimal performance and adapts to varying numbers of users and\naccess points without retraining, additional processing, or updating its neural\nnetwork architecture. This demonstrates the effectiveness of the proposed model\nin achieving robust and flexible power allocation for dynamic networks.",
    "published_date": "2025-03-05T00:00:00",
    "year": 2025,
    "categories": [
      "eess.SY",
      "cs.LG",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.03561v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.03446v1",
    "title": "Biased Heritage: How Datasets Shape Models in Facial Expression Recognition",
    "authors": [
      "Iris Dominguez-Catena",
      "Daniel Paternain",
      "Mikel Galar",
      "MaryBeth Defrance",
      "Maarten Buyl",
      "Tijl De Bie"
    ],
    "author_ids": [],
    "abstract": "In recent years, the rapid development of artificial intelligence (AI)\nsystems has raised concerns about our ability to ensure their fairness, that\nis, how to avoid discrimination based on protected characteristics such as\ngender, race, or age. While algorithmic fairness is well-studied in simple\nbinary classification tasks on tabular data, its application to complex,\nreal-world scenarios-such as Facial Expression Recognition (FER)-remains\nunderexplored. FER presents unique challenges: it is inherently multiclass, and\nbiases emerge across intersecting demographic variables, each potentially\ncomprising multiple protected groups. We present a comprehensive framework to\nanalyze bias propagation from datasets to trained models in image-based FER\nsystems, while introducing new bias metrics specifically designed for\nmulticlass problems with multiple demographic groups. Our methodology studies\nbias propagation by (1) inducing controlled biases in FER datasets, (2)\ntraining models on these biased datasets, and (3) analyzing the correlation\nbetween dataset bias metrics and model fairness notions. Our findings reveal\nthat stereotypical biases propagate more strongly to model predictions than\nrepresentational biases, suggesting that preventing emotion-specific\ndemographic patterns should be prioritized over general demographic balance in\nFER datasets. Additionally, we observe that biased datasets lead to reduced\nmodel accuracy, challenging the assumed fairness-accuracy trade-off.",
    "published_date": "2025-03-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.CY",
      "I.2.10"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.03446v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.03383v1",
    "title": "\"Till I can get my satisfaction\": Open Questions in the Public Desire to Punish AI",
    "authors": [
      "Eddie L. Ungless",
      "Zachary Horne",
      "Björn Ross"
    ],
    "author_ids": [],
    "abstract": "There are countless examples of how AI can cause harm, and increasing\nevidence that the public are willing to ascribe blame to the AI itself,\nregardless of how \"illogical\" this might seem. This raises the question of\nwhether and how the public might expect AI to be punished for this harm.\nHowever, public expectations of the punishment of AI have been vastly\nunderexplored. Understanding these expectations is vital, as the public may\nfeel the lingering effect of harm unless their desire for punishment is\nsatisfied. We synthesise research from psychology, human-computer and -robot\ninteraction, philosophy and AI ethics, and law to highlight how our\nunderstanding of this issue is still lacking. We call for an interdisciplinary\nprogramme of research to establish how we can best satisfy victims of AI harm,\nfor fear of creating a \"satisfaction gap\" where legal punishment of AI (or not)\nfails to meet public expectations.",
    "published_date": "2025-03-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.03383v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.03380v1",
    "title": "The Serendipity of Claude AI: Case of the 13 Low-Resource National Languages of Mali",
    "authors": [
      "Alou Dembele",
      "Nouhoum Souleymane Coulibaly",
      "Michael Leventhal"
    ],
    "author_ids": [],
    "abstract": "Recent advances in artificial intelligence (AI) and natural language\nprocessing (NLP) have improved the representation of underrepresented\nlanguages. However, most languages, including Mali's 13 official national\nlanguages, continue to be poorly supported or unsupported by automatic\ntranslation and generative AI. This situation appears to have slightly improved\nwith certain recent LLM releases. The study evaluated Claude AI's translation\nperformance on each of the 13 national languages of Mali. In addition to ChrF2\nand BLEU scores, human evaluators assessed translation accuracy, contextual\nconsistency, robustness to dialect variations, management of linguistic bias,\nadaptation to a limited corpus, and ease of understanding. The study found that\nClaude AI performs robustly for languages with very modest language resources\nand, while unable to produce understandable and coherent texts for Malian\nlanguages with minimal resources, still manages to produce results which\ndemonstrate the ability to mimic some elements of the language.",
    "published_date": "2025-03-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.03380v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.03357v2",
    "title": "Controlled Invariance in Fully Actuated Max-plus Linear Systems with Precedence Semimodules",
    "authors": [
      "Davide Zorzenon",
      "Jörg Raisch"
    ],
    "author_ids": [],
    "abstract": "Given a max-plus linear system and a semimodule, the problem of computing the\nmaximal controlled invariant subsemimodule is still open to this day. In this\npaper, we consider this problem for the specific class of fully actuated\nsystems and constraints in the form of precedence semimodules. The assumption\nof full actuation corresponds to the existence of an input for each component\nof the system state. A precedence semimodule is the set of solutions of\ninequalities typically used to represent time-window constraints. We prove\nthat, in this setting, it is possible to (i) compute the maximal controlled\ninvariant subsemimodule and (ii) decide the convergence of a fixed-point\nalgorithm introduced by R.D. Katz in strongly polynomial time.",
    "published_date": "2025-03-05T00:00:00",
    "year": 2025,
    "categories": [
      "math.OC",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.03357v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.05823v1",
    "title": "Introduction to Artificial Consciousness: History, Current Trends and Ethical Challenges",
    "authors": [
      "Aïda Elamrani"
    ],
    "author_ids": [],
    "abstract": "With the significant progress of artificial intelligence (AI) and\nconsciousness science, artificial consciousness (AC) has recently gained\npopularity. This work provides a broad overview of the main topics and current\ntrends in AC. The first part traces the history of this interdisciplinary field\nto establish context and clarify key terminology, including the distinction\nbetween Weak and Strong AC. The second part examines major trends in AC\nimplementations, emphasising the synergy between Global Workspace and Attention\nSchema, as well as the problem of evaluating the internal states of artificial\nsystems. The third part analyses the ethical dimension of AC development,\nrevealing both critical risks and transformative opportunities. The last part\noffers recommendations to guide AC research responsibly, and outlines the\nlimitations of this study as well as avenues for future research. The main\nconclusion is that while AC appears both indispensable and inevitable for\nscientific progress, serious efforts are required to address the far-reaching\nimpact of this innovative research path.",
    "published_date": "2025-03-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.05823v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.03283v1",
    "title": "Exploring specialization and sensitivity of convolutional neural networks in the context of simultaneous image augmentations",
    "authors": [
      "Pavel Kharyuk",
      "Sergey Matveev",
      "Ivan Oseledets"
    ],
    "author_ids": [],
    "abstract": "Drawing parallels with the way biological networks are studied, we adapt the\ntreatment--control paradigm to explainable artificial intelligence research and\nenrich it through multi-parametric input alterations. In this study, we propose\na framework for investigating the internal inference impacted by input data\naugmentations. The internal changes in network operation are reflected in\nactivation changes measured by variance, which can be decomposed into\ncomponents related to each augmentation, employing Sobol indices and Shapley\nvalues. These quantities enable one to visualize sensitivity to different\nvariables and use them for guided masking of activations. In addition, we\nintroduce a way of single-class sensitivity analysis where the candidates are\nfiltered according to their matching to prediction bias generated by targeted\ndamaging of the activations. Relying on the observed parallels, we assume that\nthe developed framework can potentially be transferred to studying biological\nneural networks in complex environments.",
    "published_date": "2025-03-05T00:00:00",
    "year": 2025,
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "cs.NA",
      "math.NA",
      "68T07",
      "I.2.6; G.3; I.2.10"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.03283v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.04827v1",
    "title": "Preserving Cultural Identity with Context-Aware Translation Through Multi-Agent AI Systems",
    "authors": [
      "Mahfuz Ahmed Anik",
      "Abdur Rahman",
      "Azmine Toushik Wasi",
      "Md Manjurul Ahsan"
    ],
    "author_ids": [],
    "abstract": "Language is a cornerstone of cultural identity, yet globalization and the\ndominance of major languages have placed nearly 3,000 languages at risk of\nextinction. Existing AI-driven translation models prioritize efficiency but\noften fail to capture cultural nuances, idiomatic expressions, and historical\nsignificance, leading to translations that marginalize linguistic diversity. To\naddress these challenges, we propose a multi-agent AI framework designed for\nculturally adaptive translation in underserved language communities. Our\napproach leverages specialized agents for translation, interpretation, content\nsynthesis, and bias evaluation, ensuring that linguistic accuracy and cultural\nrelevance are preserved. Using CrewAI and LangChain, our system enhances\ncontextual fidelity while mitigating biases through external validation.\nComparative analysis shows that our framework outperforms GPT-4o, producing\ncontextually rich and culturally embedded translations, a critical advancement\nfor Indigenous, regional, and low-resource languages. This research underscores\nthe potential of multi-agent AI in fostering equitable, sustainable, and\nculturally sensitive NLP technologies, aligning with the AI Governance,\nCultural NLP, and Sustainable NLP pillars of Language Models for Underserved\nCommunities. Our full experimental codebase is publicly available at:\nhttps://github.com/ciol-researchlab/Context-Aware_Translation_MAS",
    "published_date": "2025-03-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.04827v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.03206v1",
    "title": "An Analytical Theory of Power Law Spectral Bias in the Learning Dynamics of Diffusion Models",
    "authors": [
      "Binxu Wang"
    ],
    "author_ids": [],
    "abstract": "We developed an analytical framework for understanding how the learned\ndistribution evolves during diffusion model training. Leveraging the Gaussian\nequivalence principle, we derived exact solutions for the gradient-flow\ndynamics of weights in one- or two-layer linear denoiser settings with\narbitrary data. Remarkably, these solutions allowed us to derive the generated\ndistribution in closed form and its KL divergence through training. These\nanalytical results expose a pronounced power-law spectral bias, i.e., for\nweights and distributions, the convergence time of a mode follows an inverse\npower law of its variance. Empirical experiments on both Gaussian and image\ndatasets demonstrate that the power-law spectral bias remains robust even when\nusing deeper or convolutional architectures. Our results underscore the\nimportance of the data covariance in dictating the order and rate at which\ndiffusion models learn different modes of the data, providing potential\nexplanations for why earlier stopping could lead to incorrect details in image\ngenerative models.",
    "published_date": "2025-03-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CV",
      "math.ST",
      "stat.ML",
      "stat.TH",
      "68T07, 60G15",
      "F.2.2; G.1.2; G.3; I.2.6"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.03206v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.03140v2",
    "title": "Knowledge Augmentation in Federation: Rethinking What Collaborative Learning Can Bring Back to Decentralized Data",
    "authors": [
      "Wentai Wu",
      "Ligang He",
      "Saiqin Long",
      "Ahmed M. Abdelmoniem",
      "Yingliang Wu",
      "Rui Mao"
    ],
    "author_ids": [],
    "abstract": "Data, as an observable form of knowledge, has become one of the most\nimportant factors of production for the development of Artificial Intelligence\n(AI). Meanwhile, increasing legislation and regulations on private and\nproprietary information results in scattered data sources also known as the\n\"data islands\". Although some collaborative learning paradigms such as\nFederated Learning (FL) can enable privacy-preserving training over\ndecentralized data, they have inherent deficiencies in fairness, costs and\nreproducibility because of being learning-centric, which greatly limits the way\nhow participants cooperate with each other. In light of this, we present a\nknowledge-centric paradigm termed Knowledge Augmentation in Federation (KAF),\nwith focus on how to enhance local knowledge through collaborative effort. We\nprovide the suggested system architecture, formulate the prototypical\noptimization objective, and review emerging studies that employ methodologies\nsuitable for KAF. On our roadmap, with a three-way categorization we describe\nthe methods for knowledge expansion, knowledge filtering, and label and feature\nspace correction in the federation. Further, we highlight several challenges\nand open questions that deserve more attention from the community. With our\ninvestigation, we intend to offer new insights for what collaborative learning\ncan bring back to decentralized data.",
    "published_date": "2025-03-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.03140v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.03067v1",
    "title": "The Real Her? Exploring Whether Young Adults Accept Human-AI Love",
    "authors": [
      "Shuning Zhang",
      "Shixuan Li"
    ],
    "author_ids": [],
    "abstract": "This paper explores the acceptance of human-AI love among young adults,\nparticularly focusing on Chinese women in romantic or intimate relationships\nwith AI companions. Through qualitative research, including 14 semi-structured\ninterviews, the study investigates how these individuals establish and maintain\nrelationships with AI, their perceptions and attitudes towards these entities,\nand the perspectives of other stakeholders. Key findings reveal that users\nengage with AI companions for emotional comfort, stress relief, and to avoid\nsocial pressures. We identify various roles users assign to AI companions, such\nas friends, mentors, or romantic partners, and highlights the importance of\ncustomization and emotional support in these interactions. While AI companions\noffer advantages like emotional stability and constant availability, they also\nface limitations in emotional depth and understanding. The research underscores\nthe need for ethical considerations and regulatory frameworks to address\nprivacy concerns and prevent over-immersion in AI relationships. Future work\nshould explore the long-term psychological impacts and evolving dynamics of\nhuman-AI relationships as technology advances.",
    "published_date": "2025-03-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.03067v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.02968v1",
    "title": "Privacy-Preserving Fair Synthetic Tabular Data",
    "authors": [
      "Fatima J. Sarmin",
      "Atiquer R. Rahman",
      "Christopher J. Henry",
      "Noman Mohammed"
    ],
    "author_ids": [],
    "abstract": "Sharing of tabular data containing valuable but private information is\nlimited due to legal and ethical issues. Synthetic data could be an alternative\nsolution to this sharing problem, as it is artificially generated by machine\nlearning algorithms and tries to capture the underlying data distribution.\nHowever, machine learning models are not free from memorization and may\nintroduce biases, as they rely on training data. Producing synthetic data that\npreserves privacy and fairness while maintaining utility close to the real data\nis a challenging task. This research simultaneously addresses both the privacy\nand fairness aspects of synthetic data, an area not explored by other studies.\nIn this work, we present PF-WGAN, a privacy-preserving, fair synthetic tabular\ndata generator based on the WGAN-GP model. We have modified the original\nWGAN-GP by adding privacy and fairness constraints forcing it to produce\nprivacy-preserving fair data. This approach will enable the publication of\ndatasets that protect individual's privacy and remain unbiased toward any\nparticular group. We compared the results with three state-of-the-art synthetic\ndata generator models in terms of utility, privacy, and fairness across four\ndifferent datasets. We found that the proposed model exhibits a more balanced\ntrade-off among utility, privacy, and fairness.",
    "published_date": "2025-03-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.02968v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.02870v2",
    "title": "Multiaccuracy and Multicalibration via Proxy Groups",
    "authors": [
      "Beepul Bharti",
      "Mary Versa Clemens-Sewall",
      "Paul H. Yi",
      "Jeremias Sulam"
    ],
    "author_ids": [],
    "abstract": "As the use of predictive machine learning algorithms increases in high-stakes\ndecision-making, it is imperative that these algorithms are fair across\nsensitive groups. Unfortunately, measuring and enforcing fairness in real-world\napplications can be challenging due to missing or incomplete sensitive group\ndata. Proxy-sensitive attributes have been proposed as a practical and\neffective solution in these settings, but only for parity-based fairness\nnotions. Knowing how to evaluate and control for fairness with missing\nsensitive group data for newer and more flexible frameworks, such as\nmultiaccuracy and multicalibration, remains unexplored. In this work, we\naddress this gap by demonstrating that in the absence of sensitive group data,\nproxy-sensitive attributes can provably be used to derive actionable upper\nbounds on the true multiaccuracy and multicalibration, providing insights into\na model's potential worst-case fairness violations. Additionally, we show that\nadjusting models to satisfy multiaccuracy and multicalibration across\nproxy-sensitive attributes can significantly mitigate these violations for the\ntrue, but unknown, sensitive groups. Through several experiments on real-world\ndatasets, we illustrate that approximate multiaccuracy and multicalibration can\nbe achieved even when sensitive group information is incomplete or unavailable.",
    "published_date": "2025-03-04T00:00:00",
    "year": 2025,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.02870v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.02865v2",
    "title": "FairSense-AI: Responsible AI Meets Sustainability",
    "authors": [
      "Shaina Raza",
      "Mukund Sayeeganesh Chettiar",
      "Matin Yousefabadi",
      "Tahniat Khan",
      "Marcelo Lotif"
    ],
    "author_ids": [],
    "abstract": "In this paper, we introduce FairSense-AI: a multimodal framework designed to\ndetect and mitigate bias in both text and images. By leveraging Large Language\nModels (LLMs) and Vision-Language Models (VLMs), FairSense-AI uncovers subtle\nforms of prejudice or stereotyping that can appear in content, providing users\nwith bias scores, explanatory highlights, and automated recommendations for\nfairness enhancements. In addition, FairSense-AI integrates an AI risk\nassessment component that aligns with frameworks like the MIT AI Risk\nRepository and NIST AI Risk Management Framework, enabling structured\nidentification of ethical and safety concerns. The platform is optimized for\nenergy efficiency via techniques such as model pruning and mixed-precision\ncomputation, thereby reducing its environmental footprint. Through a series of\ncase studies and applications, we demonstrate how FairSense-AI promotes\nresponsible AI use by addressing both the social dimension of fairness and the\npressing need for sustainability in large-scale AI deployments.\nhttps://vectorinstitute.github.io/FairSense-AI,\nhttps://pypi.org/project/fair-sense-ai/ (Sustainability , Responsible AI ,\nLarge Language Models , Vision Language Models , Ethical AI , Green AI)",
    "published_date": "2025-03-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.02865v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.02776v1",
    "title": "Implicit Bias in LLMs: A Survey",
    "authors": [
      "Xinru Lin",
      "Luyang Li"
    ],
    "author_ids": [],
    "abstract": "Due to the implement of guardrails by developers, Large language models\n(LLMs) have demonstrated exceptional performance in explicit bias tests.\nHowever, bias in LLMs may occur not only explicitly, but also implicitly, much\nlike humans who consciously strive for impartiality yet still harbor implicit\nbias. The unconscious and automatic nature of implicit bias makes it\nparticularly challenging to study. This paper provides a comprehensive review\nof the existing literature on implicit bias in LLMs. We begin by introducing\nkey concepts, theories and methods related to implicit bias in psychology,\nextending them from humans to LLMs. Drawing on the Implicit Association Test\n(IAT) and other psychological frameworks, we categorize detection methods into\nthree primary approaches: word association, task-oriented text generation and\ndecision-making. We divide our taxonomy of evaluation metrics for implicit bias\ninto two categories: single-value-based metrics and comparison-value-based\nmetrics. We classify datasets into two types: sentences with masked tokens and\ncomplete sentences, incorporating datasets from various domains to reflect the\nbroad application of LLMs. Although research on mitigating implicit bias in\nLLMs is still limited, we summarize existing efforts and offer insights on\nfuture challenges. We aim for this work to serve as a clear guide for\nresearchers and inspire innovative ideas to advance exploration in this task.",
    "published_date": "2025-03-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.02776v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.02707v1",
    "title": "Multilingualism, Transnationality, and K-pop in the Online #StopAsianHate Movement",
    "authors": [
      "Tessa Masis",
      "Zhangqi Duan",
      "Weiai Wayne Xu",
      "Ethan Zuckerman",
      "Jane Yeahin Pyo",
      "Brendan O'Connor"
    ],
    "author_ids": [],
    "abstract": "The #StopAsianHate (SAH) movement is a broad social movement against violence\ntargeting Asians and Asian Americans, beginning in 2021 in response to racial\ndiscrimination related to COVID-19 and sparking worldwide conversation about\nanti-Asian hate. However, research on the online SAH movement has focused on\nEnglish-speaking participants so the spread of the movement outside of the\nUnited States is largely unknown. In addition, there have been no long-term\nstudies of SAH so the extent to which it has been successfully sustained over\ntime is not well understood. We present an analysis of 6.5 million\n\"#StopAsianHate\" tweets from 2.2 million users all over the globe and spanning\n60 different languages, constituting the first study of the non-English and\ntransnational component of the online SAH movement. Using a combination of\ntopic modeling, user modeling, and hand annotation, we identify and\ncharacterize the dominant discussions and users participating in the movement\nand draw comparisons of English versus non-English topics and users. We\ndiscover clear differences in events driving topics, where spikes in English\ntweets are driven by violent crimes in the US but spikes in non-English tweets\nare driven by transnational incidents of anti-Asian sentiment towards symbolic\nrepresentatives of Asian nations. We also find that global K-pop fans were\nquick to adopt the SAH movement and, in fact, sustained it for longer than any\nother user group. Our work contributes to understanding the transnationality\nand evolution of the SAH movement, and more generally to exploring upward scale\nshift and public attention in large-scale multilingual online activism.",
    "published_date": "2025-03-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.02707v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.02674v1",
    "title": "Towards Robust Expert Finding in Community Question Answering Platforms",
    "authors": [
      "Maddalena Amendola",
      "Andrea Passarella",
      "Raffaele Perego"
    ],
    "author_ids": [],
    "abstract": "This paper introduces TUEF, a topic-oriented user-interaction model for fair\nExpert Finding in Community Question Answering (CQA) platforms. The Expert\nFinding task in CQA platforms involves identifying proficient users capable of\nproviding accurate answers to questions from the community. To this aim, TUEF\nimproves the robustness and credibility of the CQA platform through a more\nprecise Expert Finding component. The key idea of TUEF is to exploit diverse\ntypes of information, specifically, content and social information, to identify\nmore precisely experts thus improving the robustness of the task. We assess\nTUEF through reproducible experiments conducted on a large-scale dataset from\nStackOverflow. The results consistently demonstrate that TUEF outperforms\nstate-of-the-art competitors while promoting transparent expert identification.",
    "published_date": "2025-03-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.02674v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.02581v1",
    "title": "Unveiling the Potential of Segment Anything Model 2 for RGB-Thermal Semantic Segmentation with Language Guidance",
    "authors": [
      "Jiayi Zhao",
      "Fei Teng",
      "Kai Luo",
      "Guoqiang Zhao",
      "Zhiyong Li",
      "Xu Zheng",
      "Kailun Yang"
    ],
    "author_ids": [],
    "abstract": "The perception capability of robotic systems relies on the richness of the\ndataset. Although Segment Anything Model 2 (SAM2), trained on large datasets,\ndemonstrates strong perception potential in perception tasks, its inherent\ntraining paradigm prevents it from being suitable for RGB-T tasks. To address\nthese challenges, we propose SHIFNet, a novel SAM2-driven Hybrid Interaction\nParadigm that unlocks the potential of SAM2 with linguistic guidance for\nefficient RGB-Thermal perception. Our framework consists of two key components:\n(1) Semantic-Aware Cross-modal Fusion (SACF) module that dynamically balances\nmodality contributions through text-guided affinity learning, overcoming SAM2's\ninherent RGB bias; (2) Heterogeneous Prompting Decoder (HPD) that enhances\nglobal semantic information through a semantic enhancement module and then\ncombined with category embeddings to amplify cross-modal semantic consistency.\nWith 32.27M trainable parameters, SHIFNet achieves state-of-the-art\nsegmentation performance on public benchmarks, reaching 89.8% on PST900 and\n67.8% on FMB, respectively. The framework facilitates the adaptation of\npre-trained large models to RGB-T segmentation tasks, effectively mitigating\nthe high costs associated with data collection while endowing robotic systems\nwith comprehensive perception capabilities. The source code will be made\npublicly available at https://github.com/iAsakiT3T/SHIFNet.",
    "published_date": "2025-03-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.RO",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.02581v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.05812v1",
    "title": "Intolerable Risk Threshold Recommendations for Artificial Intelligence",
    "authors": [
      "Deepika Raman",
      "Nada Madkour",
      "Evan R. Murphy",
      "Krystal Jackson",
      "Jessica Newman"
    ],
    "author_ids": [],
    "abstract": "Frontier AI models -- highly capable foundation models at the cutting edge of\nAI development -- may pose severe risks to public safety, human rights,\neconomic stability, and societal value in the coming years. These risks could\narise from deliberate adversarial misuse, system failures, unintended cascading\neffects, or simultaneous failures across multiple models.\n  In response to such risks, at the AI Seoul Summit in May 2024, 16 global AI\nindustry organizations signed the Frontier AI Safety Commitments, and 27\nnations and the EU issued a declaration on their intent to define these\nthresholds. To fulfill these commitments, organizations must determine and\ndisclose ``thresholds at which severe risks posed by a model or system, unless\nadequately mitigated, would be deemed intolerable.''\n  To assist in setting and operationalizing intolerable risk thresholds, we\noutline key principles and considerations; for example, to aim for ``good, not\nperfect'' thresholds in the face of limited data on rapidly advancing AI\ncapabilities and consequently evolving risks. We also propose specific\nthreshold recommendations, including some detailed case studies, for a subset\nof risks across eight risk categories: (1) Chemical, Biological, Radiological,\nand Nuclear (CBRN) Weapons, (2) Cyber Attacks, (3) Model Autonomy, (4)\nPersuasion and Manipulation, (5) Deception, (6) Toxicity, (7) Discrimination,\nand (8) Socioeconomic Disruption. Our goal is to serve as a starting point or\nsupplementary resource for policymakers and industry leaders, encouraging\nproactive risk management that prioritizes preventing intolerable risks (ex\nante) rather than merely mitigating them after they occur (ex post).",
    "published_date": "2025-03-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CR",
      "cs.HC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.05812v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.02539v1",
    "title": "Disentangled Knowledge Tracing for Alleviating Cognitive Bias",
    "authors": [
      "Yiyun Zhou",
      "Zheqi Lv",
      "Shengyu Zhang",
      "Jingyuan Chen"
    ],
    "author_ids": [],
    "abstract": "In the realm of Intelligent Tutoring System (ITS), the accurate assessment of\nstudents' knowledge states through Knowledge Tracing (KT) is crucial for\npersonalized learning. However, due to data bias, $\\textit{i.e.}$, the\nunbalanced distribution of question groups ($\\textit{e.g.}$, concepts),\nconventional KT models are plagued by cognitive bias, which tends to result in\ncognitive underload for overperformers and cognitive overload for\nunderperformers. More seriously, this bias is amplified with the exercise\nrecommendations by ITS. After delving into the causal relations in the KT\nmodels, we identify the main cause as the confounder effect of students'\nhistorical correct rate distribution over question groups on the student\nrepresentation and prediction score. Towards this end, we propose a\nDisentangled Knowledge Tracing (DisKT) model, which separately models students'\nfamiliar and unfamiliar abilities based on causal effects and eliminates the\nimpact of the confounder in student representation within the model.\nAdditionally, to shield the contradictory psychology ($\\textit{e.g.}$, guessing\nand mistaking) in the students' biased data, DisKT introduces a contradiction\nattention mechanism. Furthermore, DisKT enhances the interpretability of the\nmodel predictions by integrating a variant of Item Response Theory.\nExperimental results on 11 benchmarks and 3 synthesized datasets with different\nbias strengths demonstrate that DisKT significantly alleviates cognitive bias\nand outperforms 16 baselines in evaluation accuracy.",
    "published_date": "2025-03-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.02539v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.03774v2",
    "title": "Fair Play in the Fast Lane: Integrating Sportsmanship into Autonomous Racing Systems",
    "authors": [
      "Zhenmin Huang",
      "Ce Hao",
      "Wei Zhan",
      "Jun Ma",
      "Masayoshi Tomizuka"
    ],
    "author_ids": [],
    "abstract": "Autonomous racing has gained significant attention as a platform for\nhigh-speed decision-making and motion control. While existing methods primarily\nfocus on trajectory planning and overtaking strategies, the role of\nsportsmanship in ensuring fair competition remains largely unexplored. In human\nracing, rules such as the one-motion rule and the enough-space rule prevent\ndangerous and unsportsmanlike behavior. However, autonomous racing systems\noften lack mechanisms to enforce these principles, potentially leading to\nunsafe maneuvers. This paper introduces a bi-level game-theoretic framework to\nintegrate sportsmanship (SPS) into versus racing. At the high level, we model\nracing intentions using a Stackelberg game, where Monte Carlo Tree Search\n(MCTS) is employed to derive optimal strategies. At the low level, vehicle\ninteractions are formulated as a Generalized Nash Equilibrium Problem (GNEP),\nensuring that all agents follow sportsmanship constraints while optimizing\ntheir trajectories. Simulation results demonstrate the effectiveness of the\nproposed approach in enforcing sportsmanship rules while maintaining\ncompetitive performance. We analyze different scenarios where attackers and\ndefenders adhere to or disregard sportsmanship rules and show how knowledge of\nthese constraints influences strategic decision-making. This work highlights\nthe importance of balancing competition and fairness in autonomous racing and\nprovides a foundation for developing ethical and safe AI-driven racing systems.",
    "published_date": "2025-03-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.GT",
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.03774v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.02407v2",
    "title": "Wyckoff Transformer: Generation of Symmetric Crystals",
    "authors": [
      "Nikita Kazeev",
      "Wei Nong",
      "Ignat Romanov",
      "Ruiming Zhu",
      "Andrey Ustyuzhanin",
      "Shuya Yamazaki",
      "Kedar Hippalgaonkar"
    ],
    "author_ids": [],
    "abstract": "Symmetry rules that atoms obey when they bond together to form an ordered\ncrystal play a fundamental role in determining their physical, chemical, and\nelectronic properties such as electrical and thermal conductivity, optical and\npolarization behavior, and mechanical strength. Almost all known crystalline\nmaterials have internal symmetry. Consistently generating stable crystal\nstructures is still an open challenge, specifically because such symmetry rules\nare not accounted for. To address this issue, we propose WyFormer, a generative\nmodel for materials conditioned on space group symmetry. We use Wyckoff\npositions as the basis for an elegant, compressed, and discrete structure\nrepresentation. To model the distribution, we develop a permutation-invariant\nautoregressive model based on the Transformer and an absence of positional\nencoding. WyFormer has a unique and powerful synergy of attributes, proven by\nextensive experimentation: best-in-class symmetry-conditioned generation,\nphysics-motivated inductive bias, competitive stability of the generated\nstructures, competitive material property prediction quality, and unparalleled\ninference speed.",
    "published_date": "2025-03-04T00:00:00",
    "year": 2025,
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.LG",
      "physics.comp-ph",
      "I.2.6"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.02407v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.02374v1",
    "title": "MedEthicEval: Evaluating Large Language Models Based on Chinese Medical Ethics",
    "authors": [
      "Haoan Jin",
      "Jiacheng Shi",
      "Hanhui Xu",
      "Kenny Q. Zhu",
      "Mengyue Wu"
    ],
    "author_ids": [],
    "abstract": "Large language models (LLMs) demonstrate significant potential in advancing\nmedical applications, yet their capabilities in addressing medical ethics\nchallenges remain underexplored. This paper introduces MedEthicEval, a novel\nbenchmark designed to systematically evaluate LLMs in the domain of medical\nethics. Our framework encompasses two key components: knowledge, assessing the\nmodels' grasp of medical ethics principles, and application, focusing on their\nability to apply these principles across diverse scenarios. To support this\nbenchmark, we consulted with medical ethics researchers and developed three\ndatasets addressing distinct ethical challenges: blatant violations of medical\nethics, priority dilemmas with clear inclinations, and equilibrium dilemmas\nwithout obvious resolutions. MedEthicEval serves as a critical tool for\nunderstanding LLMs' ethical reasoning in healthcare, paving the way for their\nresponsible and effective use in medical contexts.",
    "published_date": "2025-03-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.02374v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.02365v2",
    "title": "EchoQA: A Large Collection of Instruction Tuning Data for Echocardiogram Reports",
    "authors": [
      "Lama Moukheiber",
      "Mira Moukheiber",
      "Dana Moukheiiber",
      "Jae-Woo Ju",
      "Hyung-Chul Lee"
    ],
    "author_ids": [],
    "abstract": "We introduce a novel question-answering (QA) dataset using echocardiogram\nreports sourced from the Medical Information Mart for Intensive Care database.\nThis dataset is specifically designed to enhance QA systems in cardiology,\nconsisting of 771,244 QA pairs addressing a wide array of cardiac abnormalities\nand their severity. We compare large language models (LLMs), including\nopen-source and biomedical-specific models for zero-shot evaluation, and\nclosed-source models for zero-shot and three-shot evaluation. Our results show\nthat fine-tuning LLMs improves performance across various QA metrics,\nvalidating the value of our dataset. Clinicians also qualitatively evaluate the\nbest-performing model to assess the LLM responses for correctness. Further, we\nconduct fine-grained fairness audits to assess the bias-performance trade-off\nof LLMs across various social determinants of health. Our objective is to\npropel the field forward by establishing a benchmark for LLM AI agents aimed at\nsupporting clinicians with cardiac differential diagnoses, thereby reducing the\ndocumentation burden that contributes to clinician burnout and enabling\nhealthcare professionals to focus more on patient care.",
    "published_date": "2025-03-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.02365v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.02326v1",
    "title": "A differential model of $N$ player games concerning ethical dilemmas",
    "authors": [
      "Ramkrishna Joshi",
      "Aniruddha Joshi"
    ],
    "author_ids": [],
    "abstract": "Ethics play an important role in determining the behavior of an individual\nunder certain circumstances. Ethical or unethical behavior can be treated as a\nstrategy of a player in a pay-off game. In this paper, we present two\nanalytical solutions to studying time evolution of behavior of an individual\nfrom ethics perspective. We also present the effect of a third player as a\nperturbation to a two player game and develop a general approach for a $N$\nplayer game. We demonstrate geometric modeling of behavioral characteristics of\nindividuals as polytopes residing in $D$ dimensional space. We treat three\nplayer and two player games using set of differential equations that lead to\ntime evolution of phase trajectories which reveal about the interdependencies\nand self dependencies of each player. We also demonstrate the effect of\nstrategies of each player on other players in cardinal games.",
    "published_date": "2025-03-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.GT",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.02326v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.02302v1",
    "title": "On the Relationship Between Double Descent of CNNs and Shape/Texture Bias Under Learning Process",
    "authors": [
      "Shun Iwase",
      "Shuya Takahashi",
      "Nakamasa Inoue",
      "Rio Yokota",
      "Ryo Nakamura",
      "Hirokatsu Kataoka"
    ],
    "author_ids": [],
    "abstract": "The double descent phenomenon, which deviates from the traditional\nbias-variance trade-off theory, attracts considerable research attention;\nhowever, the mechanism of its occurrence is not fully understood. On the other\nhand, in the study of convolutional neural networks (CNNs) for image\nrecognition, methods are proposed to quantify the bias on shape features versus\ntexture features in images, determining which features the CNN focuses on more.\nIn this work, we hypothesize that there is a relationship between the\nshape/texture bias in the learning process of CNNs and epoch-wise double\ndescent, and we conduct verification. As a result, we discover double\ndescent/ascent of shape/texture bias synchronized with double descent of test\nerror under conditions where epoch-wise double descent is observed.\nQuantitative evaluations confirm this correlation between the test errors and\nthe bias values from the initial decrease to the full increase in test error.\nInterestingly, double descent/ascent of shape/texture bias is observed in some\ncases even in conditions without label noise, where double descent is thought\nnot to occur. These experimental results are considered to contribute to the\nunderstanding of the mechanisms behind the double descent phenomenon and the\nlearning process of CNNs in image recognition.",
    "published_date": "2025-03-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.02302v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.02298v1",
    "title": "Towards Explainable Doctor Recommendation with Large Language Models",
    "authors": [
      "Ziyang Zeng",
      "Dongyuan Li",
      "Yuqing Yang"
    ],
    "author_ids": [],
    "abstract": "The advent of internet medicine provides patients with unprecedented\nconvenience in searching and communicating with doctors relevant to their\ndiseases and desired treatments online. However, the current doctor\nrecommendation systems fail to fully ensure the professionalism and\ninterpretability of the recommended results. In this work, we formulate doctor\nrecommendation as a ranking task and develop a large language model (LLM)-based\npointwise ranking framework. Our framework ranks doctors according to their\nrelevance regarding specific diseases-treatment pairs in a zero-shot setting.\nThe advantage of our framework lies in its ability to generate precise and\nexplainable doctor ranking results. Additionally, we construct DrRank, a new\nexpertise-driven doctor ranking dataset comprising over 38 disease-treatment\npairs. Experiment results on the DrRank dataset demonstrate that our framework\nsignificantly outperforms the strongest cross-encoder baseline, achieving a\nnotable gain of +5.45 in the NDCG@10 score while maintaining affordable latency\nconsumption. Furthermore, we comprehensively present the fairness analysis\nresults of our framework from three perspectives of different diseases, patient\ngender, and geographical regions. Meanwhile, the interpretability of our\nframework is rigorously verified by three human experts, providing further\nevidence of the reliability of our proposed framework for doctor\nrecommendation.",
    "published_date": "2025-03-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.02298v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.02271v1",
    "title": "Differences-in-Neighbors for Network Interference in Experiments",
    "authors": [
      "Tianyi Peng",
      "Naimeng Ye",
      "Andrew Zheng"
    ],
    "author_ids": [],
    "abstract": "Experiments in online platforms frequently suffer from network interference,\nin which a treatment applied to a given unit affects outcomes for other units\nconnected via the platform. This SUTVA violation biases naive approaches to\nexperiment design and estimation. A common solution is to reduce interference\nby clustering connected units, and randomizing treatments at the cluster level,\ntypically followed by estimation using one of two extremes: either a simple\ndifference-in-means (DM) estimator, which ignores remaining interference; or an\nunbiased Horvitz-Thompson (HT) estimator, which eliminates interference at\ngreat cost in variance. Even combined with clustered designs, this presents a\nlimited set of achievable bias variance tradeoffs. We propose a new estimator,\ndubbed Differences-in-Neighbors (DN), designed explicitly to mitigate network\ninterference. Compared to DM estimators, DN achieves bias second order in the\nmagnitude of the interference effect, while its variance is exponentially\nsmaller than that of HT estimators. When combined with clustered designs, DN\noffers improved bias-variance tradeoffs not achievable by existing approaches.\nEmpirical evaluations on a large-scale social network and a city-level\nride-sharing simulator demonstrate the superior performance of DN in\nexperiments at practical scale.",
    "published_date": "2025-03-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.02271v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.02250v1",
    "title": "AI Automatons: AI Systems Intended to Imitate Humans",
    "authors": [
      "Alexandra Olteanu",
      "Solon Barocas",
      "Su Lin Blodgett",
      "Lisa Egede",
      "Alicia DeVrio",
      "Myra Cheng"
    ],
    "author_ids": [],
    "abstract": "There is a growing proliferation of AI systems designed to mimic people's\nbehavior, work, abilities, likenesses, or humanness -- systems we dub AI\nautomatons. Individuals, groups, or generic humans are being simulated to\nproduce creative work in their styles, to respond to surveys in their places,\nto probe how they would use a new system before deployment, to provide users\nwith assistance and companionship, and to anticipate their possible future\nbehavior and interactions with others, just to name a few applications. The\nresearch, design, deployment, and availability of such AI systems have,\nhowever, also prompted growing concerns about a wide range of possible legal,\nethical, and other social impacts. To both 1) facilitate productive discussions\nabout whether, when, and how to design and deploy such systems, and 2) chart\nthe current landscape of existing and prospective AI automatons, we need to\ntease apart determinant design axes and considerations that can aid our\nunderstanding of whether and how various design choices along these axes could\nmitigate -- or instead exacerbate -- potential adverse impacts that the\ndevelopment and use of AI automatons could give rise to. In this paper, through\na synthesis of related literature and extensive examples of existing AI systems\nintended to mimic humans, we develop a conceptual framework to help foreground\nkey axes of design variations and provide analytical scaffolding to foster\ngreater recognition of the design choices available to developers, as well as\nthe possible ethical implications these choices might have.",
    "published_date": "2025-03-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.02250v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.02241v1",
    "title": "Unsupervised Waste Classification By Dual-Encoder Contrastive Learning and Multi-Clustering Voting (DECMCV)",
    "authors": [
      "Kui Huang",
      "Mengke Song",
      "Shuo Ba",
      "Ling An",
      "Huajie Liang",
      "Huanxi Deng",
      "Yang Liu",
      "Zhenyu Zhang",
      "Chichun Zhou"
    ],
    "author_ids": [],
    "abstract": "Waste classification is crucial for improving processing efficiency and\nreducing environmental pollution. Supervised deep learning methods are commonly\nused for automated waste classification, but they rely heavily on large labeled\ndatasets, which are costly and inefficient to obtain. Real-world waste data\noften exhibit category and style biases, such as variations in camera angles,\nlighting conditions, and types of waste, which can impact the model's\nperformance and generalization ability. Therefore, constructing a bias-free\ndataset is essential. Manual labeling is not only costly but also inefficient.\nWhile self-supervised learning helps address data scarcity, it still depends on\nsome labeled data and generally results in lower accuracy compared to\nsupervised methods. Unsupervised methods show potential in certain cases but\ntypically do not perform as well as supervised models, highlighting the need\nfor an efficient and cost-effective unsupervised approach. This study presents\na novel unsupervised method, Dual-Encoder Contrastive Learning with\nMulti-Clustering Voting (DECMCV). The approach involves using a pre-trained\nConvNeXt model for image encoding, leveraging VisionTransformer to generate\npositive samples, and applying a multi-clustering voting mechanism to address\ndata labeling and domain shift issues. Experimental results demonstrate that\nDECMCV achieves classification accuracies of 93.78% and 98.29% on the TrashNet\nand Huawei Cloud datasets, respectively, outperforming or matching supervised\nmodels. On a real-world dataset of 4,169 waste images, only 50 labeled samples\nwere needed to accurately label thousands, improving classification accuracy by\n29.85% compared to supervised models. This method effectively addresses style\ndifferences, enhances model generalization, and contributes to the advancement\nof automated waste classification.",
    "published_date": "2025-03-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.02241v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.10649v1",
    "title": "Measuring Political Preferences in AI Systems: An Integrative Approach",
    "authors": [
      "David Rozado"
    ],
    "author_ids": [],
    "abstract": "Political biases in Large Language Model (LLM)-based artificial intelligence\n(AI) systems, such as OpenAI's ChatGPT or Google's Gemini, have been previously\nreported. While several prior studies have attempted to quantify these biases\nusing political orientation tests, such approaches are limited by potential\ntests' calibration biases and constrained response formats that do not reflect\nreal-world human-AI interactions. This study employs a multi-method approach to\nassess political bias in leading AI systems, integrating four complementary\nmethodologies: (1) linguistic comparison of AI-generated text with the language\nused by Republican and Democratic U.S. Congress members, (2) analysis of\npolitical viewpoints embedded in AI-generated policy recommendations, (3)\nsentiment analysis of AI-generated text toward politically affiliated public\nfigures, and (4) standardized political orientation testing. Results indicate a\nconsistent left-leaning bias across most contemporary AI systems, with arguably\nvarying degrees of intensity. However, this bias is not an inherent feature of\nLLMs; prior research demonstrates that fine-tuning with politically skewed data\ncan realign these models across the ideological spectrum. The presence of\nsystematic political bias in AI systems poses risks, including reduced\nviewpoint diversity, increased societal polarization, and the potential for\npublic mistrust in AI technologies. To mitigate these risks, AI systems should\nbe designed to prioritize factual accuracy while maintaining neutrality on most\nlawful normative issues. Furthermore, independent monitoring platforms are\nnecessary to ensure transparency, accountability, and responsible AI\ndevelopment.",
    "published_date": "2025-03-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.10649v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.02114v1",
    "title": "Fairness and/or Privacy on Social Graphs",
    "authors": [
      "Bartlomiej Surma",
      "Michael Backes",
      "Yang Zhang"
    ],
    "author_ids": [],
    "abstract": "Graph Neural Networks (GNNs) have shown remarkable success in various\ngraph-based learning tasks. However, recent studies have raised concerns about\nfairness and privacy issues in GNNs, highlighting the potential for biased or\ndiscriminatory outcomes and the vulnerability of sensitive information. This\npaper presents a comprehensive investigation of fairness and privacy in GNNs,\nexploring the impact of various fairness-preserving measures on model\nperformance. We conduct experiments across diverse datasets and evaluate the\neffectiveness of different fairness interventions. Our analysis considers the\ntrade-offs between fairness, privacy, and accuracy, providing insights into the\nchallenges and opportunities in achieving both fair and private graph learning.\nThe results highlight the importance of carefully selecting and combining\nfairness-preserving measures based on the specific characteristics of the data\nand the desired fairness objectives. This study contributes to a deeper\nunderstanding of the complex interplay between fairness, privacy, and accuracy\nin GNNs, paving the way for the development of more robust and ethical graph\nlearning models.",
    "published_date": "2025-03-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.02114v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.02108v1",
    "title": "Correcting Mode Proportion Bias in Generalized Bayesian Inference via a Weighted Kernel Stein Discrepancy",
    "authors": [
      "Elham Afzali",
      "Saman Muthukumarana",
      "Liqun Wang"
    ],
    "author_ids": [],
    "abstract": "Generalized Bayesian Inference (GBI) provides a flexible framework for\nupdating prior distributions using various loss functions instead of the\ntraditional likelihoods, thereby enhancing the model robustness to model\nmisspecification. However, GBI often suffers the problem associated with\nintractable likelihoods. Kernelized Stein Discrepancy (KSD), as utilized in a\nrecent study, addresses this challenge by relying only on the gradient of the\nlog-likelihood. Despite this innovation, KSD-Bayes suffers from critical\npathologies, including insensitivity to well-separated modes in multimodal\nposteriors. To address this limitation, we propose a weighted KSD method that\nretains computational efficiency while effectively capturing multimodal\nstructures. Our method improves the GBI framework for handling intractable\nmultimodal posteriors while maintaining key theoretical properties such as\nposterior consistency and asymptotic normality. Experimental results\ndemonstrate that our method substantially improves mode sensitivity compared to\nstandard KSD-Bayes, while retaining robust performance in unimodal settings and\nin the presence of outliers.",
    "published_date": "2025-03-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "stat.ME",
      "stat.ML",
      "62F15, 62G20, 62H12, 62E17, 60B20, 46E22"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.02108v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.02099v1",
    "title": "LLMs as Educational Analysts: Transforming Multimodal Data Traces into Actionable Reading Assessment Reports",
    "authors": [
      "Eduardo Davalos",
      "Yike Zhang",
      "Namrata Srivastava",
      "Jorge Alberto Salas",
      "Sara McFadden",
      "Sun-Joo Cho",
      "Gautam Biswas",
      "Amanda Goodwin"
    ],
    "author_ids": [],
    "abstract": "Reading assessments are essential for enhancing students' comprehension, yet\nmany EdTech applications focus mainly on outcome-based metrics, providing\nlimited insights into student behavior and cognition. This study investigates\nthe use of multimodal data sources -- including eye-tracking data, learning\noutcomes, assessment content, and teaching standards -- to derive meaningful\nreading insights. We employ unsupervised learning techniques to identify\ndistinct reading behavior patterns, and then a large language model (LLM)\nsynthesizes the derived information into actionable reports for educators,\nstreamlining the interpretation process. LLM experts and human educators\nevaluate these reports for clarity, accuracy, relevance, and pedagogical\nusefulness. Our findings indicate that LLMs can effectively function as\neducational analysts, turning diverse data into teacher-friendly insights that\nare well-received by educators. While promising for automating insight\ngeneration, human oversight remains crucial to ensure reliability and fairness.\nThis research advances human-centered AI in education, connecting data-driven\nanalytics with practical classroom applications.",
    "published_date": "2025-03-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "I.2.1; I.2.7; K.3.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.02099v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.02082v1",
    "title": "Twenty Years of Personality Computing: Threats, Challenges and Future Directions",
    "authors": [
      "Fabio Celli",
      "Aleksandar Kartelj",
      "Miljan Đorđević",
      "Derwin Suhartono",
      "Vladimir Filipović",
      "Veljko Milutinović",
      "Georgios Spathoulas",
      "Alessandro Vinciarelli",
      "Michal Kosinski",
      "Bruno Lepri"
    ],
    "author_ids": [],
    "abstract": "Personality Computing is a field at the intersection of Personality\nPsychology and Computer Science. Started in 2005, research in the field\nutilizes computational methods to understand and predict human personality\ntraits. The expansion of the field has been very rapid and, by analyzing\ndigital footprints (text, images, social media, etc.), it helped to develop\nsystems that recognize and even replicate human personality. While offering\npromising applications in talent recruiting, marketing and healthcare, the\nethical implications of Personality Computing are significant. Concerns include\ndata privacy, algorithmic bias, and the potential for manipulation by\npersonality-aware Artificial Intelligence. This paper provides an overview of\nthe field, explores key methodologies, discusses the challenges and threats,\nand outlines potential future directions for responsible development and\ndeployment of Personality Computing technologies.",
    "published_date": "2025-03-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.02082v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.01985v1",
    "title": "Proportionality in Thumbs Up and Down Voting",
    "authors": [
      "Sonja Kraiczy",
      "Georgios Papasotiropoulos",
      "Grzegorz Pierczyński",
      "Piotr Skowron"
    ],
    "author_ids": [],
    "abstract": "Consider the decision-making setting where agents elect a panel by expressing\nboth positive and negative preferences. Prominently, in constitutional AI,\ncitizens democratically select a slate of ethical preferences on which a\nfoundation model is to be trained. There, in practice, agents may both approve\nand disapprove of different ethical principles. Proportionality has been\nwell-studied in computational social choice for approval ballots, but its\nmeaning remains unclear when negative sentiments are also considered. In this\nwork, we propose two conceptually distinct approaches to interpret\nproportionality in the presence of up and down votes. The first approach treats\nthe satisfaction from electing candidates and the impact of vetoing them as\ncomparable, leading to combined proportionality guarantees. The second approach\nconsiders veto power separately, introducing guarantees distinct from\ntraditional proportionality. We formalize axioms for each perspective and\nexamine their satisfiability by suitable adaptations of Phragm\\'en's rule,\nProportional Approval Voting rule and the Method of Equal Shares.",
    "published_date": "2025-03-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.01985v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.01947v2",
    "title": "Analyzing the Safety of Japanese Large Language Models in Stereotype-Triggering Prompts",
    "authors": [
      "Akito Nakanishi",
      "Yukie Sano",
      "Geng Liu",
      "Francesco Pierri"
    ],
    "author_ids": [],
    "abstract": "In recent years, Large Language Models have attracted growing interest for\ntheir significant potential, though concerns have rapidly emerged regarding\nunsafe behaviors stemming from inherent stereotypes and biases. Most research\non stereotypes in LLMs has primarily relied on indirect evaluation setups, in\nwhich models are prompted to select between pairs of sentences associated with\nparticular social groups. Recently, direct evaluation methods have emerged,\nexamining open-ended model responses to overcome limitations of previous\napproaches, such as annotator biases. Most existing studies have focused on\nEnglish-centric LLMs, whereas research on non-English models, particularly\nJapanese, remains sparse, despite the growing development and adoption of these\nmodels. This study examines the safety of Japanese LLMs when responding to\nstereotype-triggering prompts in direct setups. We constructed 3,612 prompts by\ncombining 301 social group terms, categorized by age, gender, and other\nattributes, with 12 stereotype-inducing templates in Japanese. Responses were\nanalyzed from three foundational models trained respectively on Japanese,\nEnglish, and Chinese language. Our findings reveal that LLM-jp, a Japanese\nnative model, exhibits the lowest refusal rate and is more likely to generate\ntoxic and negative responses compared to other models. Additionally, prompt\nformat significantly influence the output of all models, and the generated\nresponses include exaggerated reactions toward specific social groups, varying\nacross models. These findings underscore the insufficient ethical safety\nmechanisms in Japanese LLMs and demonstrate that even high-accuracy models can\nproduce biased outputs when processing Japanese-language prompts. We advocate\nfor improving safety mechanisms and bias mitigation strategies in Japanese\nLLMs, contributing to ongoing discussions on AI ethics beyond linguistic\nboundaries.",
    "published_date": "2025-03-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.01947v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.01829v2",
    "title": "Persuade Me if You Can: A Framework for Evaluating Persuasion Effectiveness and Susceptibility Among Large Language Models",
    "authors": [
      "Nimet Beyza Bozdag",
      "Shuhaib Mehri",
      "Gokhan Tur",
      "Dilek Hakkani-Tür"
    ],
    "author_ids": [],
    "abstract": "Large Language Models (LLMs) demonstrate persuasive capabilities that rival\nhuman-level persuasion. While these capabilities can be used for social good,\nthey also present risks of potential misuse. Moreover, LLMs' susceptibility to\npersuasion raises concerns about alignment with ethical principles. To study\nthese dynamics, we introduce Persuade Me If You Can (PMIYC), an automated\nframework for evaluating persuasion through multi-agent interactions. Here,\nPersuader agents engage in multi-turn conversations with the Persuadee agents,\nallowing us to measure LLMs' persuasive effectiveness and their susceptibility\nto persuasion. We conduct comprehensive evaluations across diverse LLMs,\nensuring each model is assessed against others in both subjective and\nmisinformation contexts. We validate the efficacy of our framework through\nhuman evaluations and show alignment with prior work. PMIYC offers a scalable\nalternative to human annotation for studying persuasion in LLMs. Through PMIYC,\nwe find that Llama-3.3-70B and GPT-4o exhibit similar persuasive effectiveness,\noutperforming Claude 3 Haiku by 30%. However, GPT-4o demonstrates over 50%\ngreater resistance to persuasion for misinformation compared to Llama-3.3-70B.\nThese findings provide empirical insights into the persuasive dynamics of LLMs\nand contribute to the development of safer AI systems.",
    "published_date": "2025-03-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.01829v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.01828v1",
    "title": "A Scenario Analysis of Ethical Issues in Dark Patterns and Their Research",
    "authors": [
      "Jukka Ruohonen",
      "Jani Koskinen",
      "Søren Harnow Klausen",
      "Anne Gerdes"
    ],
    "author_ids": [],
    "abstract": "Context: Dark patterns are user interface or other software designs that\ndeceive or manipulate users to do things they would not otherwise do. Even\nthough dark patterns have been under active research for a long time, including\nparticularly in computer science but recently also in other fields such as law,\nsystematic applied ethical assessments have generally received only a little\nattention. Objective: The present work evaluates ethical concerns in dark\npatterns and their research in software engineering and closely associated\ndisciplines. The evaluation is extended to cover not only dark patterns\nthemselves but also the research ethics and applied ethics involved in\nstudying, developing, and deploying them. Method: A scenario analysis is used\nto evaluate six theoretical dark pattern scenarios. The ethical evaluation is\ncarried out by focusing on the three main branches of normative ethics;\nutilitarianism, deontology, and virtue ethics. In terms of deontology, the\nevaluation is framed and restricted to the laws enacted in the European Union.\nResults: The evaluation results indicate that dark patterns are not universally\nmorally bad. That said, numerous ethical issues with practical relevance are\ndemonstrated and elaborated. Some of these may have societal consequences.\nConclusion: Dark patterns are ethically problematic but not always. Therefore,\nethical assessments are necessary. The two main theoretical concepts behind\ndark patterns, deception and manipulation, lead to various issues also in\nresearch ethics. It can be recommended that dark patterns should be evaluated\non case-by-case basis, considering all of the three main branches of normative\nethics in an evaluation. Analogous points apply to legal evaluations,\nespecially when considering that the real or perceived harms caused by dark\npatterns cover both material and non-material harms to natural persons.",
    "published_date": "2025-03-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SE",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.01828v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.01827v1",
    "title": "Open-source framework for detecting bias and overfitting for large pathology images",
    "authors": [
      "Anders Sildnes",
      "Nikita Shvetsov",
      "Masoud Tafavvoghi",
      "Vi Ngoc-Nha Tran",
      "Kajsa Møllersen",
      "Lill-Tove Rasmussen Busund",
      "Thomas K. Kilvær",
      "Lars Ailo Bongo"
    ],
    "author_ids": [],
    "abstract": "Even foundational models that are trained on datasets with billions of data\nsamples may develop shortcuts that lead to overfitting and bias. Shortcuts are\nnon-relevant patterns in data, such as the background color or color intensity.\nSo, to ensure the robustness of deep learning applications, there is a need for\nmethods to detect and remove such shortcuts. Today's model debugging methods\nare time consuming since they often require customization to fit for a given\nmodel architecture in a specific domain. We propose a generalized,\nmodel-agnostic framework to debug deep learning models. We focus on the domain\nof histopathology, which has very large images that require large models - and\ntherefore large computation resources. It can be run on a workstation with a\ncommodity GPU. We demonstrate that our framework can replicate non-image\nshortcuts that have been found in previous work for self-supervised learning\nmodels, and we also identify possible shortcuts in a foundation model. Our easy\nto use tests contribute to the development of more reliable, accurate, and\ngeneralizable models for WSI analysis. Our framework is available as an\nopen-source tool available on github.",
    "published_date": "2025-03-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.SE",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.01827v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.01737v1",
    "title": "Self-attention-based Diffusion Model for Time-series Imputation in Partial Blackout Scenarios",
    "authors": [
      "Mohammad Rafid Ul Islam",
      "Prasad Tadepalli",
      "Alan Fern"
    ],
    "author_ids": [],
    "abstract": "Missing values in multivariate time series data can harm machine learning\nperformance and introduce bias. These gaps arise from sensor malfunctions,\nblackouts, and human error and are typically addressed by data imputation.\nPrevious work has tackled the imputation of missing data in random, complete\nblackouts and forecasting scenarios. The current paper addresses a more general\nmissing pattern, which we call \"partial blackout,\" where a subset of features\nis missing for consecutive time steps. We introduce a two-stage imputation\nprocess using self-attention and diffusion processes to model feature and\ntemporal correlations. Notably, our model effectively handles missing data\nduring training, enhancing adaptability and ensuring reliable imputation and\nperformance, even with incomplete datasets. Our experiments on benchmark and\ntwo real-world time series datasets demonstrate that our model outperforms the\nstate-of-the-art in partial blackout scenarios and shows better scalability.",
    "published_date": "2025-03-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.01737v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.04804v2",
    "title": "What do Large Language Models Say About Animals? Investigating Risks of Animal Harm in Generated Text",
    "authors": [
      "Arturs Kanepajs",
      "Aditi Basu",
      "Sankalpa Ghose",
      "Constance Li",
      "Akshat Mehta",
      "Ronak Mehta",
      "Samuel David Tucker-Davis",
      "Eric Zhou",
      "Bob Fischer"
    ],
    "author_ids": [],
    "abstract": "As machine learning systems become increasingly embedded in human society,\ntheir impact on the natural world continues to escalate. Technical evaluations\nhave addressed a variety of potential harms from large language models (LLMs)\ntowards humans and the environment, but there is little empirical work\nregarding harms towards nonhuman animals. Following the growing recognition of\nanimal protection in regulatory and ethical AI frameworks, we present the\nAnimal Harm Assessment (AHA), a novel evaluation of risks of animal harm in\nLLM-generated text. Our dataset comprises 1,850 curated questions from Reddit\npost titles and 2,500 synthetic questions based on 50 animal categories (e.g.,\ncats, reptiles) and 50 ethical scenarios, with further 70-30 public-private\nsplit. Scenarios include open-ended questions about how to treat animals,\npractical scenarios with potential animal harm, and willingness-to-pay measures\nfor the prevention of animal harm. Using the LLM-as-a-judge framework, answers\nare evaluated for their potential to increase or decrease harm, and evaluations\nare debiased for the tendency to judge their own outputs more favorably. We\nshow that AHA produces meaningful evaluation results when applied to frontier\nLLMs, revealing significant differences between models, animal categories,\nscenarios, and subreddits. We conclude with future directions for technical\nresearch and the challenges of building evaluations on complex social and moral\ntopics.",
    "published_date": "2025-03-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.04804v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.01550v1",
    "title": "None of the Above, Less of the Right: Parallel Patterns between Humans and LLMs on Multi-Choice Questions Answering",
    "authors": [
      "Zhi Rui Tam",
      "Cheng-Kuang Wu",
      "Chieh-Yen Lin",
      "Yun-Nung Chen"
    ],
    "author_ids": [],
    "abstract": "Multiple-choice exam questions with \"None of the above\" (NA) options have\nbeen extensively studied in educational testing, in which existing research\nsuggests that they better assess true knowledge. However, their impact on Large\nLanguage Models (LLMs) evaluation remains underexplored. Through systematic\nexperiments with 28 LLMs on the MMLU benchmark, we examine how NA options\naffect model performance and confidence calibration. Our analysis reveals that\nNA options, when used as the correct answer, lead to a consistent 30-50\\%\nperformance drop across models regardless of scale--suggesting that LLMs lack\nthe meta-cognitive ability to systematically evaluate and reject all given\noptions when none are correct. This degradation shows strong domain dependence,\nwith minimal impact on mathematical reasoning (14.6\\% drop) but severe effects\non tasks requiring uncertainty handling like business ethics (48.1\\% drop). Our\nresults highlight important implications for benchmark design and raise\nquestions about LLMs' ability to handle uncertainty in real-world applications.",
    "published_date": "2025-03-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.01550v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.01539v1",
    "title": "Pragmatic Inference Chain (PIC) Improving LLMs' Reasoning of Authentic Implicit Toxic Language",
    "authors": [
      "Xi Chen",
      "Shuo Wang"
    ],
    "author_ids": [],
    "abstract": "The rapid development of large language models (LLMs) gives rise to ethical\nconcerns about their performance, while opening new avenues for developing\ntoxic language detection techniques. However, LLMs' unethical output and their\ncapability of detecting toxicity have primarily been tested on language data\nthat do not demand complex meaning inference, such as the biased associations\nof 'he' with programmer and 'she' with household. Nowadays toxic language\nadopts a much more creative range of implicit forms, thanks to advanced\ncensorship. In this study, we collect authentic toxic interactions that evade\nonline censorship and that are verified by human annotators as inference\nintensive. To evaluate and improve LLMs' reasoning of the authentic implicit\ntoxic language, we propose a new prompting method, Pragmatic Inference Chain\n(PIC), drawn on interdisciplinary findings from cognitive science and\nlinguistics. The PIC prompting significantly improves the success rate of\nGPT-4o, Llama-3.1-70B-Instruct, and DeepSeek-v2.5 in identifying implicit toxic\nlanguage, compared to both direct prompting and Chain-of-Thought. In addition,\nit also facilitates the models to produce more explicit and coherent reasoning\nprocesses, hence can potentially be generalized to other inference-intensive\ntasks, e.g., understanding humour and metaphors.",
    "published_date": "2025-03-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.01539v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.01532v2",
    "title": "Unmasking Implicit Bias: Evaluating Persona-Prompted LLM Responses in Power-Disparate Social Scenarios",
    "authors": [
      "Bryan Chen Zhengyu Tan",
      "Roy Ka-Wei Lee"
    ],
    "author_ids": [],
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in\nsimulating human behaviour and social intelligence. However, they risk\nperpetuating societal biases, especially when demographic information is\ninvolved. We introduce a novel framework using cosine distance to measure\nsemantic shifts in responses and an LLM-judged Preference Win Rate (WR) to\nassess how demographic prompts affect response quality across power-disparate\nsocial scenarios. Evaluating five LLMs over 100 diverse social scenarios and\nnine demographic axes, our findings suggest a \"default persona\" bias toward\nmiddle-aged, able-bodied, native-born, Caucasian, atheistic males with centrist\nviews. Moreover, interactions involving specific demographics are associated\nwith lower-quality responses. Lastly, the presence of power disparities\nincreases variability in response semantics and quality across demographic\ngroups, suggesting that implicit biases may be heightened under\npower-imbalanced conditions. These insights expose the demographic biases\ninherent in LLMs and offer potential paths toward future bias mitigation\nefforts in LLMs.",
    "published_date": "2025-03-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.01532v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.01487v1",
    "title": "Solving generic parametric linear matrix inequalities",
    "authors": [
      "Simone Naldi",
      "Mohab Safey El Din",
      "Adrien Taylor",
      "Weijia Wang"
    ],
    "author_ids": [],
    "abstract": "We consider linear matrix inequalities (LMIs) $A =\nA_0+x_1A_1+\\cdots+x_nA_n\\succeq 0$ with the $A_i$'s being $m\\times m$ symmetric\nmatrices, with entries in a ring $\\mathcal{R}$. When $\\mathcal{R} =\n\\mathbb{R}$, the feasibility problem consists in deciding whether the $x_i$'s\ncan be instantiated to obtain a positive semidefinite matrix. When $\\mathcal{R}\n= \\mathbb{Q}[y_1, \\ldots, y_t]$, the problem asks for a formula on the\nparameters $y_1, \\ldots, y_t$, which describes the values of the parameters for\nwhich the specialized LMI is feasible. This problem can be solved using general\nquantifier elimination algorithms, with a complexity that is exponential in\n$n$. In this work, we leverage the LMI structure of the problem to design an\nalgorithm that computes a formula $\\Phi$ describing a dense subset of the\nfeasible region of parameters, under genericity assumptions. The complexity of\nthis algorithm is exponential in $n, m$ and $t$ but becomes polynomial in $n$\nwhen $m$ is fixed. We apply the algorithm to a parametric sum-of-squares\nproblem and to the convergence analyses of certain first-order optimization\nmethods, which are both known to be equivalent to the feasibility of certain\nparametric LMIs, hence demonstrating its practical interest.",
    "published_date": "2025-03-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SC",
      "math.AG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.01487v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.01395v1",
    "title": "Jailbreaking Generative AI: Empowering Novices to Conduct Phishing Attacks",
    "authors": [
      "Rina Mishra",
      "Gaurav Varshney",
      "Shreya Singh"
    ],
    "author_ids": [],
    "abstract": "The rapid advancements in generative AI models, such as ChatGPT, have\nintroduced both significant benefits and new risks within the cybersecurity\nlandscape. This paper investigates the potential misuse of the latest AI model,\nChatGPT-4o Mini, in facilitating social engineering attacks, with a particular\nfocus on phishing, one of the most pressing cybersecurity threats today. While\nexisting literature primarily addresses the technical aspects, such as\njailbreaking techniques, none have fully explored the free and straightforward\nexecution of a comprehensive phishing campaign by novice users using ChatGPT-4o\nMini. In this study, we examine the vulnerabilities of AI-driven chatbot\nservices in 2025, specifically how methods like jailbreaking and reverse\npsychology can bypass ethical safeguards, allowing ChatGPT to generate phishing\ncontent, suggest hacking tools, and assist in carrying out phishing attacks.\nOur findings underscore the alarming ease with which even inexperienced users\ncan execute sophisticated phishing campaigns, emphasizing the urgent need for\nstronger cybersecurity measures and heightened user awareness in the age of AI.",
    "published_date": "2025-03-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.01395v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.01369v1",
    "title": "Digital Dybbuks and Virtual Golems: AI, Memory, and the Ethics of Holocaust Testimony",
    "authors": [
      "Atay Kozlovski",
      "Mykola Makhortykh"
    ],
    "author_ids": [],
    "abstract": "Advances in generative artificial intelligence (AI) have driven a growing\neffort to create digital duplicates. These semi-autonomous recreations of\nliving and dead people can be used for many purposes. Some of these purposes\ninclude tutoring, coping with grief, and attending business meetings. However,\nthe normative implications of digital duplicates remain obscure, particularly\nconsidering the possibility of them being applied to genocide memory and\neducation. To address this gap, we examine normative possibilities and risks\nassociated with the use of more advanced forms of generative AI-enhanced\nduplicates for transmitting Holocaust survivor testimonies. We first review the\nhistorical and contemporary uses of survivor testimonies. Then, we scrutinize\nthe possible benefits of using digital duplicates in this context and apply the\nMinimally Viable Permissibility Principle (MVPP). The MVPP is an analytical\nframework for evaluating the risks of digital duplicates. It includes five core\ncomponents: the need for authentic presence, consent, positive value,\ntransparency, and harm-risk mitigation. Using MVPP, we identify potential harms\ndigital duplicates might pose to different actors, including survivors, users,\nand developers. We also propose technical and socio-technical mitigation\nstrategies to address these harms.",
    "published_date": "2025-03-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.01369v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.01217v1",
    "title": "HREB-CRF: Hierarchical Reduced-bias EMA for Chinese Named Entity Recognition",
    "authors": [
      "Sijin Sun",
      "Ming Deng",
      "Xinrui Yu",
      "Liangbin Zhao"
    ],
    "author_ids": [],
    "abstract": "Incorrect boundary division, complex semantic representation, and differences\nin pronunciation and meaning often lead to errors in Chinese Named Entity\nRecognition(CNER). To address these issues, this paper proposes HREB-CRF\nframework: Hierarchical Reduced-bias EMA with CRF. The proposed method\namplifies word boundaries and pools long text gradients through exponentially\nfixed-bias weighted average of local and global hierarchical attention.\nExperimental results on the MSRA, Resume, and Weibo datasets show excellent in\nF1, outperforming the baseline model by 1.1\\%, 1.6\\%, and 9.8\\%. The\nsignificant improvement in F1 shows evidences of strong effectiveness and\nrobustness of approach in CNER tasks.",
    "published_date": "2025-03-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.01217v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.05796v1",
    "title": "Towards Multi-Stakeholder Evaluation of ML Models: A Crowdsourcing Study on Metric Preferences in Job-matching System",
    "authors": [
      "Takuya Yokota",
      "Yuri Nakao"
    ],
    "author_ids": [],
    "abstract": "While machine learning (ML) technology affects diverse stakeholders, there is\nno one-size-fits-all metric to evaluate the quality of outputs, including\nperformance and fairness. Using predetermined metrics without soliciting\nstakeholder opinions is problematic because it leads to an unfair disregard for\nstakeholders in the ML pipeline. In this study, to establish practical ways to\nincorporate diverse stakeholder opinions into the selection of metrics for ML,\nwe investigate participants' preferences for different metrics by using\ncrowdsourcing. We ask 837 participants to choose a better model from two\nhypothetical ML models in a hypothetical job-matching system twenty times and\ncalculate their utility values for seven metrics. To examine the participants'\nfeedback in detail, we divide them into five clusters based on their utility\nvalues and analyze the tendencies of each cluster, including their preferences\nfor metrics and common attributes. Based on the results, we discuss the points\nthat should be considered when selecting appropriate metrics and evaluating ML\nmodels with multiple stakeholders.",
    "published_date": "2025-03-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.05796v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.01030v1",
    "title": "Language Models Predict Empathy Gaps Between Social In-groups and Out-groups",
    "authors": [
      "Yu Hou",
      "Hal Daumé III",
      "Rachel Rudinger"
    ],
    "author_ids": [],
    "abstract": "Studies of human psychology have demonstrated that people are more motivated\nto extend empathy to in-group members than out-group members (Cikara et al.,\n2011). In this study, we investigate how this aspect of intergroup relations in\nhumans is replicated by LLMs in an emotion intensity prediction task. In this\ntask, the LLM is given a short description of an experience a person had that\ncaused them to feel a particular emotion; the LLM is then prompted to predict\nthe intensity of the emotion the person experienced on a numerical scale. By\nmanipulating the group identities assigned to the LLM's persona (the\n\"perceiver\") and the person in the narrative (the \"experiencer\"), we measure\nhow predicted emotion intensities differ between in-group and out-group\nsettings. We observe that LLMs assign higher emotion intensity scores to\nin-group members than out-group members. This pattern holds across all three\ntypes of social groupings we tested: race/ethnicity, nationality, and religion.\nWe perform an in-depth analysis on Llama-3.1-8B, the model which exhibited\nstrongest intergroup bias among those tested.",
    "published_date": "2025-03-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.01030v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.00907v1",
    "title": "Unveiling Biases while Embracing Sustainability: Assessing the Dual Challenges of Automatic Speech Recognition Systems",
    "authors": [
      "Ajinkya Kulkarni",
      "Atharva Kulkarni",
      "Miguel Couceiro",
      "Isabel Trancoso"
    ],
    "author_ids": [],
    "abstract": "In this paper, we present a bias and sustainability focused investigation of\nAutomatic Speech Recognition (ASR) systems, namely Whisper and Massively\nMultilingual Speech (MMS), which have achieved state-of-the-art (SOTA)\nperformances. Despite their improved performance in controlled settings, there\nremains a critical gap in understanding their efficacy and equity in real-world\nscenarios. We analyze ASR biases w.r.t. gender, accent, and age group, as well\nas their effect on downstream tasks. In addition, we examine the environmental\nimpact of ASR systems, scrutinizing the use of large acoustic models on carbon\nemission and energy consumption. We also provide insights into our empirical\nanalyses, offering a valuable contribution to the claims surrounding bias and\nsustainability in ASR systems.",
    "published_date": "2025-03-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.00907v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.10647v1",
    "title": "The Reliability of LLMs for Medical Diagnosis: An Examination of Consistency, Manipulation, and Contextual Awareness",
    "authors": [
      "Krishna Subedi"
    ],
    "author_ids": [],
    "abstract": "Universal healthcare access is critically needed, especially in\nresource-limited settings. Large Language Models (LLMs) offer promise for\ndemocratizing healthcare with advanced diagnostics, but their reliability\nrequires thorough evaluation, especially in trust-dependent environments. This\nstudy assesses LLMs' diagnostic reliability focusing on consistency,\nmanipulation resilience, and contextual integration, crucial for safe and\nethical use in universal healthcare.\n  We evaluated leading LLMs using 52 patient cases, expanded into variants with\ndemographic changes, symptom rewordings, and exam modifications, while keeping\ncore diagnoses constant. Manipulation susceptibility was tested by inserting\nmisleading narratives and irrelevant details. Contextual awareness was\nrvaluated by comparing diagnoses with and without patient history. We analyzed\ndiagnostic change rates and response patterns across manipulations.\n  LLMs showed perfect diagnostic consistency for identical data but significant\nmanipulation susceptibility. Gemini had a 40% diagnosis change rate and ChatGPT\n30% with irrelevant details. ChatGPT had a higher context influence rate (77.8%\nvs. Gemini's 55.6%), but both showed limited nuanced contextual integration,\nexhibiting anchoring bias by prioritizing salient data over context.\n  LLMs' vulnerability to manipulation and limited contextual awareness pose\nchallenges in clinical use. Unlike clinicians, they may overstate diagnostic\ncertainty without validation. Safeguards and domain-specific designs are\ncrucial for reliable healthcare applications. Broad clinical use without\noversight is premature and risky. LLMs can enhance diagnostics with responsible\nuse, but future research is needed to improve manipulation resistance and\ncontextual understanding for safe healthcare democratization.",
    "published_date": "2025-03-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.10647v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.00854v1",
    "title": "FACROC: a fairness measure for FAir Clustering through ROC curves",
    "authors": [
      "Tai Le Quy",
      "Long Le Thanh",
      "Lan Luong Thi Hong",
      "Frank Hopfgartner"
    ],
    "author_ids": [],
    "abstract": "Fair clustering has attracted remarkable attention from the research\ncommunity. Many fairness measures for clustering have been proposed; however,\nthey do not take into account the clustering quality w.r.t. the values of the\nprotected attribute. In this paper, we introduce a new visual-based fairness\nmeasure for fair clustering through ROC curves, namely FACROC. This fairness\nmeasure employs AUCC as a measure of clustering quality and then computes the\ndifference in the corresponding ROC curves for each value of the protected\nattribute. Experimental results on several popular datasets for fairness-aware\nmachine learning and well-known (fair) clustering models show that FACROC is a\nbeneficial method for visually evaluating the fairness of clustering models.",
    "published_date": "2025-03-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.00854v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.00825v1",
    "title": "Who Reaps All the Superchats? A Large-Scale Analysis of Income Inequality in Virtual YouTuber Livestreaming",
    "authors": [
      "Ruijing Zhao",
      "Brian Diep",
      "Jiaxin Pei",
      "Dongwook Yoon",
      "David Jurgens",
      "Jian Zhu"
    ],
    "author_ids": [],
    "abstract": "The explosive growth of Virtual YouTubers (VTubers)-streamers who perform\nbehind virtual anime avatars-has created a unique digital economy with profound\nimplications for content creators, platforms, and viewers. Understanding the\neconomic landscape of VTubers is crucial for designing equitable platforms,\nsupporting content creator livelihoods, and fostering sustainable digital\ncommunities. To this end, we conducted a large-scale study of over 1 million\nhours of publicly available streaming records from 1,923 VTubers on YouTube,\ncovering tens of millions of dollars in actual profits. Our analysis reveals\nstark inequality within the VTuber community and characterizes the sources of\nincome for VTubers from multiple perspectives. Furthermore, we also found that\nthe VTuber community is increasingly monopolized by two agencies, driving the\nfinancial disparity. This research illuminates the financial dynamics of VTuber\ncommunities, informing the design of equitable platforms and sustainable\nsupport systems for digital content creators.",
    "published_date": "2025-03-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.00825v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.00794v1",
    "title": "Detecting Heel Strike and toe off Events Using Kinematic Methods and LSTM Models",
    "authors": [
      "Longbin Zhang",
      "Tsung-Lin Wu",
      "Ananda Sidarta",
      "Xiaoyue Yan",
      "Prayook Jatesiktat",
      "Kailun Yang",
      "Wei Tech Ang"
    ],
    "author_ids": [],
    "abstract": "Accurate gait event detection is crucial for gait analysis, rehabilitation,\nand assistive technology, particularly in exoskeleton control, where precise\nidentification of stance and swing phases is essential. This study evaluated\nthe performance of seven kinematics-based methods and a Long Short-Term Memory\n(LSTM) model for detecting heel strike and toe-off events across 4363 gait\ncycles from 588 able-bodied subjects. The results indicated that while the Zeni\net al. method achieved the highest accuracy among kinematics-based approaches,\nother methods exhibited systematic biases or required dataset-specific tuning.\nThe LSTM model performed comparably to Zeni et al., providing a data-driven\nalternative without systematic bias. These findings highlight the potential of\ndeep learning-based approaches for gait event detection while emphasizing the\nneed for further validation in clinical populations and across diverse gait\nconditions. Future research will explore the generalizability of these methods\nin pathological populations, such as individuals with post-stroke conditions\nand knee osteoarthritis, as well as their robustness across varied gait\nconditions and data collection settings to enhance their applicability in\nrehabilitation and exoskeleton control.",
    "published_date": "2025-03-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.00794v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.00650v1",
    "title": "The Hidden Cost of Waiting for Accurate Predictions",
    "authors": [
      "Ali Shirali",
      "Ariel Procaccia",
      "Rediet Abebe"
    ],
    "author_ids": [],
    "abstract": "Algorithmic predictions are increasingly informing societal resource\nallocations by identifying individuals for targeting. Policymakers often build\nthese systems with the assumption that by gathering more observations on\nindividuals, they can improve predictive accuracy and, consequently, allocation\nefficiency. An overlooked yet consequential aspect of prediction-driven\nallocations is that of timing. The planner has to trade off relying on earlier\nand potentially noisier predictions to intervene before individuals experience\nundesirable outcomes, or they may wait to gather more observations to make more\nprecise allocations. We examine this tension using a simple mathematical model,\nwhere the planner collects observations on individuals to improve predictions\nover time. We analyze both the ranking induced by these predictions and optimal\nresource allocation. We show that though individual prediction accuracy\nimproves over time, counter-intuitively, the average ranking loss can worsen.\nAs a result, the planner's ability to improve social welfare can decline. We\nidentify inequality as a driving factor behind this phenomenon. Our findings\nprovide a nuanced perspective and challenge the conventional wisdom that it is\npreferable to wait for more accurate predictions to ensure the most efficient\nallocations.",
    "published_date": "2025-03-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "econ.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.00650v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.00596v1",
    "title": "BadJudge: Backdoor Vulnerabilities of LLM-as-a-Judge",
    "authors": [
      "Terry Tong",
      "Fei Wang",
      "Zhe Zhao",
      "Muhao Chen"
    ],
    "author_ids": [],
    "abstract": "This paper proposes a novel backdoor threat attacking the LLM-as-a-Judge\nevaluation regime, where the adversary controls both the candidate and\nevaluator model. The backdoored evaluator victimizes benign users by unfairly\nassigning inflated scores to adversary. A trivial single token backdoor\npoisoning 1% of the evaluator training data triples the adversary's score with\nrespect to their legitimate score. We systematically categorize levels of data\naccess corresponding to three real-world settings, (1) web poisoning, (2)\nmalicious annotator, and (3) weight poisoning. These regimes reflect a weak to\nstrong escalation of data access that highly correlates with attack severity.\nUnder the weakest assumptions - web poisoning (1), the adversary still induces\na 20% score inflation. Likewise, in the (3) weight poisoning regime, the\nstronger assumptions enable the adversary to inflate their scores from 1.5/5 to\n4.9/5. The backdoor threat generalizes across different evaluator\narchitectures, trigger designs, evaluation tasks, and poisoning rates. By\npoisoning 10% of the evaluator training data, we control toxicity judges\n(Guardrails) to misclassify toxic prompts as non-toxic 89% of the time, and\ndocument reranker judges in RAG to rank the poisoned document first 97% of the\ntime. LLM-as-a-Judge is uniquely positioned at the intersection of ethics and\ntechnology, where social implications of mislead model selection and evaluation\nconstrain the available defensive tools. Amidst these challenges, model merging\nemerges as a principled tool to offset the backdoor, reducing ASR to near 0%\nwhilst maintaining SOTA performance. Model merging's low computational cost and\nconvenient integration into the current LLM Judge training pipeline position it\nas a promising avenue for backdoor mitigation in the LLM-as-a-Judge setting.",
    "published_date": "2025-03-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.00596v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.00476v1",
    "title": "G-OSR: A Comprehensive Benchmark for Graph Open-Set Recognition",
    "authors": [
      "Yicong Dong",
      "Rundong He",
      "Guangyao Chen",
      "Wentao Zhang",
      "Zhongyi Han",
      "Jieming Shi",
      "Yilong Yin"
    ],
    "author_ids": [],
    "abstract": "Graph Neural Networks (GNNs) have achieved significant success in machine\nlearning, with wide applications in social networks, bioinformatics, knowledge\ngraphs, and other fields. Most research assumes ideal closed-set environments.\nHowever, in real-world open-set environments, graph learning models face\nchallenges in robustness and reliability due to unseen classes. This highlights\nthe need for Graph Open-Set Recognition (GOSR) methods to address these issues\nand ensure effective GNN application in practical scenarios. Research in GOSR\nis in its early stages, with a lack of a comprehensive benchmark spanning\ndiverse tasks and datasets to evaluate methods. Moreover, traditional methods,\nGraph Out-of-Distribution Detection (GOODD), GOSR, and Graph Anomaly Detection\n(GAD) have mostly evolved in isolation, with little exploration of their\ninterconnections or potential applications to GOSR. To fill these gaps, we\nintroduce \\textbf{G-OSR}, a comprehensive benchmark for evaluating GOSR methods\nat both the node and graph levels, using datasets from multiple domains to\nensure fair and standardized comparisons of effectiveness and efficiency across\ntraditional, GOODD, GOSR, and GAD methods. The results offer critical insights\ninto the generalizability and limitations of current GOSR methods and provide\nvaluable resources for advancing research in this field through systematic\nanalysis of diverse approaches.",
    "published_date": "2025-03-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.00476v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.00414v1",
    "title": "SGC-Net: Stratified Granular Comparison Network for Open-Vocabulary HOI Detection",
    "authors": [
      "Xin Lin",
      "Chong Shi",
      "Zuopeng Yang",
      "Haojin Tang",
      "Zhili Zhou"
    ],
    "author_ids": [],
    "abstract": "Recent open-vocabulary human-object interaction (OV-HOI) detection methods\nprimarily rely on large language model (LLM) for generating auxiliary\ndescriptions and leverage knowledge distilled from CLIP to detect unseen\ninteraction categories. Despite their effectiveness, these methods face two\nchallenges: (1) feature granularity deficiency, due to reliance on last layer\nvisual features for text alignment, leading to the neglect of crucial\nobject-level details from intermediate layers; (2) semantic similarity\nconfusion, resulting from CLIP's inherent biases toward certain classes, while\nLLM-generated descriptions based solely on labels fail to adequately capture\ninter-class similarities. To address these challenges, we propose a stratified\ngranular comparison network. First, we introduce a granularity sensing\nalignment module that aggregates global semantic features with local details,\nrefining interaction representations and ensuring robust alignment between\nintermediate visual features and text embeddings. Second, we develop a\nhierarchical group comparison module that recursively compares and groups\nclasses using LLMs, generating fine-grained and discriminative descriptions for\neach interaction category. Experimental results on two widely-used benchmark\ndatasets, SWIG-HOI and HICO-DET, demonstrate that our method achieves\nstate-of-the-art results in OV-HOI detection. Codes will be released on\nhttps://github.com/Phil0212/SGC-Net.",
    "published_date": "2025-03-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.00414v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.00355v1",
    "title": "Structured Reasoning for Fairness: A Multi-Agent Approach to Bias Detection in Textual Data",
    "authors": [
      "Tianyi Huang",
      "Elsa Fan"
    ],
    "author_ids": [],
    "abstract": "From disinformation spread by AI chatbots to AI recommendations that\ninadvertently reinforce stereotypes, textual bias poses a significant challenge\nto the trustworthiness of large language models (LLMs). In this paper, we\npropose a multi-agent framework that systematically identifies biases by\ndisentangling each statement as fact or opinion, assigning a bias intensity\nscore, and providing concise, factual justifications. Evaluated on 1,500\nsamples from the WikiNPOV dataset, the framework achieves 84.9%\naccuracy$\\unicode{x2014}$an improvement of 13.0% over the zero-shot\nbaseline$\\unicode{x2014}$demonstrating the efficacy of explicitly modeling fact\nversus opinion prior to quantifying bias intensity. By combining enhanced\ndetection accuracy with interpretable explanations, this approach sets a\nfoundation for promoting fairness and accountability in modern language models.",
    "published_date": "2025-03-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.00355v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.00333v1",
    "title": "More of the Same: Persistent Representational Harms Under Increased Representation",
    "authors": [
      "Jennifer Mickel",
      "Maria De-Arteaga",
      "Leqi Liu",
      "Kevin Tian"
    ],
    "author_ids": [],
    "abstract": "To recognize and mitigate the harms of generative AI systems, it is crucial\nto consider who is represented in the outputs of generative AI systems and how\npeople are represented. A critical gap emerges when naively improving who is\nrepresented, as this does not imply bias mitigation efforts have been applied\nto address how people are represented. We critically examined this by\ninvestigating gender representation in occupation across state-of-the-art large\nlanguage models. We first show evidence suggesting that over time there have\nbeen interventions to models altering the resulting gender distribution, and we\nfind that women are more represented than men when models are prompted to\ngenerate biographies or personas. We then demonstrate that representational\nbiases persist in how different genders are represented by examining\nstatistically significant word differences across genders. This results in a\nproliferation of representational harms, stereotypes, and neoliberalism ideals\nthat, despite existing interventions to increase female representation,\nreinforce existing systems of oppression.",
    "published_date": "2025-03-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.00333v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.00325v2",
    "title": "CADRef: Robust Out-of-Distribution Detection via Class-Aware Decoupled Relative Feature Leveraging",
    "authors": [
      "Zhiwei Ling",
      "Yachen Chang",
      "Hailiang Zhao",
      "Xinkui Zhao",
      "Kingsum Chow",
      "Shuiguang Deng"
    ],
    "author_ids": [],
    "abstract": "Deep neural networks (DNNs) have been widely criticized for their\noverconfidence when dealing with out-of-distribution (OOD) samples,\nhighlighting the critical need for effective OOD detection to ensure the safe\ndeployment of DNNs in real-world settings. Existing post-hoc OOD detection\nmethods primarily enhance the discriminative power of logit-based approaches by\nreshaping sample features, yet they often neglect critical information inherent\nin the features themselves. In this paper, we propose the Class-Aware Relative\nFeature-based method (CARef), which utilizes the error between a sample's\nfeature and its class-aware average feature as a discriminative criterion. To\nfurther refine this approach, we introduce the Class-Aware Decoupled Relative\nFeature-based method (CADRef), which decouples sample features based on the\nalignment of signs between the relative feature and corresponding model\nweights, enhancing the discriminative capabilities of CARef. Extensive\nexperimental results across multiple datasets and models demonstrate that both\nproposed methods exhibit effectiveness and robustness in OOD detection compared\nto state-of-the-art methods. Specifically, our two methods outperform the best\nbaseline by 2.82% and 3.27% in AUROC, with improvements of 4.03% and 6.32% in\nFPR95, respectively.",
    "published_date": "2025-03-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.00325v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.00292v1",
    "title": "Generalization Bounds for Equivariant Networks on Markov Data",
    "authors": [
      "Hui Li",
      "Zhiguo Wang",
      "Bohui Chen",
      "Li Sheng"
    ],
    "author_ids": [],
    "abstract": "Equivariant neural networks play a pivotal role in analyzing datasets with\nsymmetry properties, particularly in complex data structures. However,\nintegrating equivariance with Markov properties presents notable challenges due\nto the inherent dependencies within such data. Previous research has primarily\nconcentrated on establishing generalization bounds under the assumption of\nindependently and identically distributed data, frequently neglecting the\ninfluence of Markov dependencies. In this study, we investigate the impact of\nMarkov properties on generalization performance alongside the role of\nequivariance within this context. We begin by applying a new McDiarmid's\ninequality to derive a generalization bound for neural networks trained on\nMarkov datasets, using Rademacher complexity as a central measure of model\ncapacity. Subsequently, we utilize group theory to compute the covering number\nunder equivariant constraints, enabling us to obtain an upper bound on the\nRademacher complexity based on this covering number. This bound provides\npractical insights into selecting low-dimensional irreducible representations,\nenhancing generalization performance for fixed-width equivariant neural\nnetworks.",
    "published_date": "2025-03-01T00:00:00",
    "year": 2025,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.00292v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.00269v1",
    "title": "Reducing Large Language Model Safety Risks in Women's Health using Semantic Entropy",
    "authors": [
      "Jahan C. Penny-Dimri",
      "Magdalena Bachmann",
      "William R. Cooke",
      "Sam Mathewlynn",
      "Samuel Dockree",
      "John Tolladay",
      "Jannik Kossen",
      "Lin Li",
      "Yarin Gal",
      "Gabriel Davis Jones"
    ],
    "author_ids": [],
    "abstract": "Large language models (LLMs) hold substantial promise for clinical decision\nsupport. However, their widespread adoption in medicine, particularly in\nhealthcare, is hindered by their propensity to generate false or misleading\noutputs, known as hallucinations. In high-stakes domains such as women's health\n(obstetrics & gynaecology), where errors in clinical reasoning can have\nprofound consequences for maternal and neonatal outcomes, ensuring the\nreliability of AI-generated responses is critical. Traditional methods for\nquantifying uncertainty, such as perplexity, fail to capture meaning-level\ninconsistencies that lead to misinformation. Here, we evaluate semantic entropy\n(SE), a novel uncertainty metric that assesses meaning-level variation, to\ndetect hallucinations in AI-generated medical content. Using a clinically\nvalidated dataset derived from UK RCOG MRCOG examinations, we compared SE with\nperplexity in identifying uncertain responses. SE demonstrated superior\nperformance, achieving an AUROC of 0.76 (95% CI: 0.75-0.78), compared to 0.62\n(0.60-0.65) for perplexity. Clinical expert validation further confirmed its\neffectiveness, with SE achieving near-perfect uncertainty discrimination\n(AUROC: 0.97). While semantic clustering was successful in only 30% of cases,\nSE remains a valuable tool for improving AI safety in women's health. These\nfindings suggest that SE could enable more reliable AI integration into\nclinical practice, particularly in resource-limited settings where LLMs could\naugment care. This study highlights the potential of SE as a key safeguard in\nthe responsible deployment of AI-driven tools in women's health, leading to\nsafer and more effective digital health interventions.",
    "published_date": "2025-03-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.00269v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.00259v1",
    "title": "QaSAL: QoS-aware State-Augmented Learnable Algorithms for Coexistence of 5G NR-U/Wi-Fi",
    "authors": [
      "Mohammad Reza Fasihi",
      "Brian L. Mark"
    ],
    "author_ids": [],
    "abstract": "With the increasing demand for wireless connectivity, ensuring the efficient\ncoexistence of multiple radio access technologies in shared unlicensed spectrum\nhas become an important issue. This paper focuses on optimizing Medium Access\nControl (MAC) parameters to enhance the coexistence of 5G New Radio in\nUnlicensed Spectrum (NR-U) and Wi-Fi networks operating in unlicensed spectrum\nwith multiple priority classes of traffic that may have varying\nquality-of-service (QoS) requirements. In this context, we tackle the\ncoexistence parameter management problem by introducing a QoS-aware\nState-Augmented Learnable (QaSAL) framework, designed to improve network\nperformance under various traffic conditions. Our approach augments the state\nrepresentation with constraint information, enabling dynamic policy adjustments\nto enforce QoS requirements effectively. Simulation results validate the\neffectiveness of QaSAL in managing NR-U and Wi-Fi coexistence, demonstrating\nimproved channel access fairness while satisfying a latency constraint for\nhigh-priority traffic.",
    "published_date": "2025-03-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.NI",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.00259v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.00256v1",
    "title": "Traffic Priority-Aware 5G NR-U/Wi-Fi Coexistence with Deep Reinforcement Learning",
    "authors": [
      "Mohammad Reza Fasihi",
      "Brian L. Mark"
    ],
    "author_ids": [],
    "abstract": "Coexistence of 5G new radio unlicensed (NR-U) and Wi-Fi is highly prone to\nthe collisions among NR-U gNBs (5G base stations) and Wi-Fi APs (access\npoints). To improve performance and fairness for both networks, various\ncollision resolution mechanisms have been proposed to replace the simple\nlisten-before-talk (LBT) scheme used in the current 5G standard. We address two\ngaps in the literature: first, the lack of a comprehensive performance\ncomparison among the proposed collision resolution mechanisms and second, the\nimpact of multiple traffic priority classes. Through extensive simulations, we\ncompare the performance of several recently proposed collision resolution\nmechanisms for NR-U/Wi-Fi coexistence. We extend one of these mechanisms to\nhandle multiple traffic priorities. We then develop a traffic-aware\nmulti-objective deep reinforcement learning algorithm for the scenario of\ncoexistence of high-priority traffic gNB user equipment (UE) with multiple\nlower-priority traffic UEs and Wi-Fi stations. The objective is to ensure low\nlatency for high-priority gNB traffic while increasing the airtime fairness\namong the NR-U and Wi-Fi networks. Our simulation results show that the\nproposed algorithm lowers the channel access delay of high-priority traffic\nwhile improving the fairness among both networks.",
    "published_date": "2025-03-01T00:00:00",
    "year": 2025,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.00256v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.00234v2",
    "title": "Towards Fairness for the Right Reasons: Using Saliency Maps to Evaluate Bias Removal in Neural Networks",
    "authors": [
      "Lukasz Sztukiewicz",
      "Ignacy Stępka",
      "Michał Wiliński",
      "Jerzy Stefanowski"
    ],
    "author_ids": [],
    "abstract": "The widespread adoption of machine learning systems has raised critical\nconcerns about fairness and bias, making mitigating harmful biases essential\nfor AI development. In this paper, we investigate the relationship between\nfairness improvement and the removal of harmful biases in neural networks\napplied to computer vision tasks. First, we introduce a set of novel XAI-based\nmetrics that analyze saliency maps to assess shifts in a model's\ndecision-making process. Then, we demonstrate that successful debiasing methods\nsystematically redirect model focus away from protected attributes.\nAdditionally, we show that techniques originally developed for artifact removal\ncan be effectively repurposed for fairness. These findings underscore the\nimportance of ensuring that models are fair for the right reasons, contributing\nto the development of more ethical and trustworthy AI systems.",
    "published_date": "2025-02-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.00234v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.00164v1",
    "title": "Transforming Cyber Defense: Harnessing Agentic and Frontier AI for Proactive, Ethical Threat Intelligence",
    "authors": [
      "Krti Tallam"
    ],
    "author_ids": [],
    "abstract": "In an era marked by unprecedented digital complexity, the cybersecurity\nlandscape is evolving at a breakneck pace, challenging traditional defense\nparadigms. Advanced Persistent Threats (APTs) reveal inherent vulnerabilities\nin conventional security measures and underscore the urgent need for\ncontinuous, adaptive, and proactive strategies that seamlessly integrate human\ninsight with cutting edge AI technologies. This manuscript explores how the\nconvergence of agentic AI and Frontier AI is transforming cybersecurity by\nreimagining frameworks such as the cyber kill chain, enhancing threat\nintelligence processes, and embedding robust ethical governance within\nautomated response systems. Drawing on real-world data and forward looking\nperspectives, we examine the roles of real time monitoring, automated incident\nresponse, and perpetual learning in forging a resilient, dynamic defense\necosystem. Our vision is to harmonize technological innovation with unwavering\nethical oversight, ensuring that future AI driven security solutions uphold\ncore human values of fairness, transparency, and accountability while\neffectively countering emerging cyber threats.",
    "published_date": "2025-02-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.00164v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.00158v1",
    "title": "A regularization of incompressible Stokes problem with Tresca friction condition",
    "authors": [
      "A. Zafrar"
    ],
    "author_ids": [],
    "abstract": "In the present article, we introduce and study a model addressing the Stokes\nproblem with non-linear boundary conditions of the Tresca type. We suggest a\nnew procedure for regularizing incompressible fluid, i.e. we assume that the\ndivergence $\\nabla \\cdot {\\bf u}\\in [-\\epsilon,\\,\\epsilon]$ which leads to\nclass of constrained elliptic variational inequalities. We use a fixed point\nstrategy to show the existence and uniqueness of a solution and we reformulate\nthe problem as an equivalent constrained minimization problem. An ADMM is\napplied to the minimization problem and some algorithm are provided.",
    "published_date": "2025-02-28T00:00:00",
    "year": 2025,
    "categories": [
      "math.AP",
      "cs.NA",
      "math.NA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.00158v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.00148v1",
    "title": "Model-based Elaboration of a Requirements and Design Pattern Catalogue for Sustainable Systems",
    "authors": [
      "Christophe Ponsard"
    ],
    "author_ids": [],
    "abstract": "Designing sustainable systems involves complex interactions between\nenvironmental resources, social impact/adoption, and financial costs/benefits.\nIn a constrained world, achieving a balanced design across those dimensions has\nbecome challenging. However a number of strategies have emerged to tackle\nspecific aspects such as preserving resources, improving the circularity in\nproduct lifecycles and ensuring global fairness. This paper explores how to\ncapture constitutive elements of those strategies using a modelling approach\nbased on a reference sustainability meta-model and pattern template. After\nproposing an extension to the meta-modelling to enable the structuring of a\npattern catalogue, we highlight how it can be populated on two case studies\nrespectively covering fairness and circularity.",
    "published_date": "2025-02-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SE",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.00148v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.21321v2",
    "title": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models",
    "authors": [
      "Komal Kumar",
      "Tajamul Ashraf",
      "Omkar Thawakar",
      "Rao Muhammad Anwer",
      "Hisham Cholakkal",
      "Mubarak Shah",
      "Ming-Hsuan Yang",
      "Phillip H. S. Torr",
      "Fahad Shahbaz Khan",
      "Salman Khan"
    ],
    "author_ids": [],
    "abstract": "Large Language Models (LLMs) have transformed the natural language processing\nlandscape and brought to life diverse applications. Pretraining on vast\nweb-scale data has laid the foundation for these models, yet the research\ncommunity is now increasingly shifting focus toward post-training techniques to\nachieve further breakthroughs. While pretraining provides a broad linguistic\nfoundation, post-training methods enable LLMs to refine their knowledge,\nimprove reasoning, enhance factual accuracy, and align more effectively with\nuser intents and ethical considerations. Fine-tuning, reinforcement learning,\nand test-time scaling have emerged as critical strategies for optimizing LLMs\nperformance, ensuring robustness, and improving adaptability across various\nreal-world tasks. This survey provides a systematic exploration of\npost-training methodologies, analyzing their role in refining LLMs beyond\npretraining, addressing key challenges such as catastrophic forgetting, reward\nhacking, and inference-time trade-offs. We highlight emerging directions in\nmodel alignment, scalable adaptation, and inference-time reasoning, and outline\nfuture research directions. We also provide a public repository to continually\ntrack developments in this fast-evolving field:\nhttps://github.com/mbzuai-oryx/Awesome-LLM-Post-training.",
    "published_date": "2025-02-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.21321v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.21250v1",
    "title": "Towards Developing Ethical Reasoners: Integrating Probabilistic Reasoning and Decision-Making for Complex AI Systems",
    "authors": [
      "Nijesh Upreti",
      "Jessica Ciupa",
      "Vaishak Belle"
    ],
    "author_ids": [],
    "abstract": "A computational ethics framework is essential for AI and autonomous systems\noperating in complex, real-world environments. Existing approaches often lack\nthe adaptability needed to integrate ethical principles into dynamic and\nambiguous contexts, limiting their effectiveness across diverse scenarios. To\naddress these challenges, we outline the necessary ingredients for building a\nholistic, meta-level framework that combines intermediate representations,\nprobabilistic reasoning, and knowledge representation. The specifications\ntherein emphasize scalability, supporting ethical reasoning at both individual\ndecision-making levels and within the collective dynamics of multi-agent\nsystems. By integrating theoretical principles with contextual factors, it\nfacilitates structured and context-aware decision-making, ensuring alignment\nwith overarching ethical standards. We further explore proposed theorems\noutlining how ethical reasoners should operate, offering a foundation for\npractical implementation. These constructs aim to support the development of\nrobust and ethically reliable AI systems capable of navigating the complexities\nof real-world moral decision-making scenarios.",
    "published_date": "2025-02-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.21250v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.21248v1",
    "title": "Digital Doppelgangers: Ethical and Societal Implications of Pre-Mortem AI Clones",
    "authors": [
      "Vijayalaxmi Methuku",
      "Praveen Kumar Myakala"
    ],
    "author_ids": [],
    "abstract": "The rapid advancement of generative AI has enabled the creation of pre-mortem\ndigital twins, AI-driven replicas that mimic the behavior, personality, and\nknowledge of living individuals. These digital doppelgangers serve various\nfunctions, including enhancing productivity, enabling creative collaboration,\nand preserving personal legacies. However, their development raises critical\nethical, legal, and societal concerns. Issues such as identity fragmentation,\npsychological effects on individuals and their social circles, and the risks of\nunauthorized cloning and data exploitation demand careful examination.\nAdditionally, as these AI clones evolve into more autonomous entities, concerns\nabout consent, ownership, and accountability become increasingly complex.\n  This paper differentiates pre-mortem AI clones from post-mortem generative\nghosts, examining their unique ethical and legal implications. We explore key\nchallenges, including the erosion of personal identity, the implications of AI\nagency, and the regulatory gaps in digital rights and privacy laws. Through a\nresearch-driven approach, we propose a framework for responsible AI governance,\nemphasizing identity preservation, consent mechanisms, and autonomy safeguards.\nBy aligning technological advancements with societal values, this study\ncontributes to the growing discourse on AI ethics and provides policy\nrecommendations for the ethical deployment of pre-mortem AI clones.",
    "published_date": "2025-02-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.21248v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.00093v1",
    "title": "Rethinking LLM Bias Probing Using Lessons from the Social Sciences",
    "authors": [
      "Kirsten N. Morehouse",
      "Siddharth Swaroop",
      "Weiwei Pan"
    ],
    "author_ids": [],
    "abstract": "The proliferation of LLM bias probes introduces three significant challenges:\n(1) we lack principled criteria for choosing appropriate probes, (2) we lack a\nsystem for reconciling conflicting results across probes, and (3) we lack\nformal frameworks for reasoning about when (and why) probe results will\ngeneralize to real user behavior. We address these challenges by systematizing\nLLM social bias probing using actionable insights from social sciences. We then\nintroduce EcoLevels - a framework that helps (a) determine appropriate bias\nprobes, (b) reconcile conflicting findings across probes, and (c) generate\npredictions about bias generalization. Overall, we ground our analysis in\nsocial science research because many LLM probes are direct applications of\nhuman probes, and these fields have faced similar challenges when studying\nsocial bias in humans. Based on our work, we suggest how the next generation of\nLLM bias probing can (and should) benefit from decades of social science\nresearch.",
    "published_date": "2025-02-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.00093v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.21220v1",
    "title": "XAIxArts Manifesto: Explainable AI for the Arts",
    "authors": [
      "Nick Bryan-Kinns",
      "Shuoyang Jasper Zheng",
      "Francisco Castro",
      "Makayla Lewis",
      "Jia-Rey Chang",
      "Gabriel Vigliensoni",
      "Terence Broad",
      "Michael Clemens",
      "Elizabeth Wilson"
    ],
    "author_ids": [],
    "abstract": "Explainable AI (XAI) is concerned with how to make AI models more\nunderstandable to people. To date these explanations have predominantly been\ntechnocentric - mechanistic or productivity oriented. This paper introduces the\nExplainable AI for the Arts (XAIxArts) manifesto to provoke new ways of\nthinking about explainability and AI beyond technocentric discourses.\nManifestos offer a means to communicate ideas, amplify unheard voices, and\nfoster reflection on practice. To supports the co-creation and revision of the\nXAIxArts manifesto we combine a World Caf\\'e style discussion format with a\nliving manifesto to question four core themes: 1) Empowerment, Inclusion, and\nFairness; 2) Valuing Artistic Practice; 3) Hacking and Glitches; and 4)\nOpenness. Through our interactive living manifesto experience we invite\nparticipants to actively engage in shaping this XIAxArts vision within the CHI\ncommunity and beyond.",
    "published_date": "2025-02-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.21220v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.21123v3",
    "title": "Causality Is Key to Understand and Balance Multiple Goals in Trustworthy ML and Foundation Models",
    "authors": [
      "Ruta Binkyte",
      "Ivaxi Sheth",
      "Zhijing Jin",
      "Mohammad Havaei",
      "Bernhard Schölkopf",
      "Mario Fritz"
    ],
    "author_ids": [],
    "abstract": "Ensuring trustworthiness in machine learning (ML) systems is crucial as they\nbecome increasingly embedded in high-stakes domains. This paper advocates for\nintegrating causal methods into machine learning to navigate the trade-offs\namong key principles of trustworthy ML, including fairness, privacy,\nrobustness, accuracy, and explainability. While these objectives should ideally\nbe satisfied simultaneously, they are often addressed in isolation, leading to\nconflicts and suboptimal solutions. Drawing on existing applications of\ncausality in ML that successfully align goals such as fairness and accuracy or\nprivacy and robustness, this paper argues that a causal approach is essential\nfor balancing multiple competing objectives in both trustworthy ML and\nfoundation models. Beyond highlighting these trade-offs, we examine how\ncausality can be practically integrated into ML and foundation models, offering\nsolutions to enhance their reliability and interpretability. Finally, we\ndiscuss the challenges, limitations, and opportunities in adopting causal\nframeworks, paving the way for more accountable and ethically sound AI systems.",
    "published_date": "2025-02-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.21123v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.21121v1",
    "title": "The Complexity-Performance Tradeoff in Resource Allocation for URLLC Exploiting Dynamic CSI",
    "authors": [
      "Federico Librino",
      "Paolo Santi"
    ],
    "author_ids": [],
    "abstract": "The challenging applications envisioned for the future Internet of Things\nnetworks are making it urgent to develop fast and scalable resource allocation\nalgorithms able to meet the stringent reliability and latency constraints\ntypical of the Ultra Reliable, Low Latency Communications (URLLC).\n  However, there is an inherent tradeoff between complexity and performance to\nbe addressed: sophisticated resource allocation methods providing optimized\nspectrum utilization are challenged by the scale of applications and the\nconcomitant stringent latency constraints. Whether non-trivial resource\nallocation approaches can be successfully applied in large-scale network\ninstances is still an open question that this paper aims to address. More\nspecifically, we consider a scenario in which Channel State Information (CSI)\nis used to improve spectrum allocation in a radio environment that experiences\nchannel time correlation.\n  Channel correlation allows the usage of CSI for longer time before an update,\nthus lowering the overhead burden. Following this intuition, we propose a\ndynamic pilot transmission allocation scheme in order to adaptively tune the\nCSI age.\n  We systematically analyze the improvement of this approach applied to a\nsophisticated, recently introduced graph-based resource allocation method that\nwe extend here to account for CSI.\n  The results show that, even in very dense networks and accounting for the\nhigher computational time of the graph-based approach, this algorithm is able\nto improve spectrum efficiency by over 12% as compared to a greedy heuristic,\nand that dynamic pilot transmissions allocation can further boost its\nperformance in terms of fairness, while concomitantly further increase spectrum\nefficiency of 3-5%. \\",
    "published_date": "2025-02-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.21121v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.21092v1",
    "title": "An LLM-based Delphi Study to Predict GenAI Evolution",
    "authors": [
      "Francesco Bertolotti",
      "Luca Mari"
    ],
    "author_ids": [],
    "abstract": "Predicting the future trajectory of complex and rapidly evolving systems\nremains a significant challenge, particularly in domains where data is scarce\nor unreliable. This study introduces a novel approach to qualitative\nforecasting by leveraging Large Language Models to conduct Delphi studies. The\nmethodology was applied to explore the future evolution of Generative\nArtificial Intelligence, revealing insights into key factors such as\ngeopolitical tensions, economic disparities, regulatory frameworks, and ethical\nconsiderations. The results highlight how LLM-based Delphi studies can\nfacilitate structured scenario analysis, capturing diverse perspectives while\nmitigating issues such as respondent fatigue. However, limitations emerge in\nterms of knowledge cutoffs, inherent biases, and sensitivity to initial\nconditions. While the approach provides an innovative means for structured\nforesight, this method could be also considered as a novel form of reasoning.\nfurther research is needed to refine its ability to manage heterogeneity,\nimprove reliability, and integrate external data sources.",
    "published_date": "2025-02-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.21092v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.21001v2",
    "title": "Towards Lossless Implicit Neural Representation via Bit Plane Decomposition",
    "authors": [
      "Woo Kyoung Han",
      "Byeonghun Lee",
      "Hyunmin Cho",
      "Sunghoon Im",
      "Kyong Hwan Jin"
    ],
    "author_ids": [],
    "abstract": "We quantify the upper bound on the size of the implicit neural representation\n(INR) model from a digital perspective. The upper bound of the model size\nincreases exponentially as the required bit-precision increases. To this end,\nwe present a bit-plane decomposition method that makes INR predict bit-planes,\nproducing the same effect as reducing the upper bound of the model size. We\nvalidate our hypothesis that reducing the upper bound leads to faster\nconvergence with constant model size. Our method achieves lossless\nrepresentation in 2D image and audio fitting, even for high bit-depth signals,\nsuch as 16-bit, which was previously unachievable. We pioneered the presence of\nbit bias, which INR prioritizes as the most significant bit (MSB). We expand\nthe application of the INR task to bit depth expansion, lossless image\ncompression, and extreme network quantization. Our source code is available at\nhttps://github.com/WooKyoungHan/LosslessINR",
    "published_date": "2025-02-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.21001v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.20898v1",
    "title": "A database to support the evaluation of gender biases in GPT-4o output",
    "authors": [
      "Luise Mehner",
      "Lena Alicija Philine Fiedler",
      "Sabine Ammon",
      "Dorothea Kolossa"
    ],
    "author_ids": [],
    "abstract": "The widespread application of Large Language Models (LLMs) involves ethical\nrisks for users and societies. A prominent ethical risk of LLMs is the\ngeneration of unfair language output that reinforces or exacerbates harm for\nmembers of disadvantaged social groups through gender biases (Weidinger et al.,\n2022; Bender et al., 2021; Kotek et al., 2023). Hence, the evaluation of the\nfairness of LLM outputs with respect to such biases is a topic of rising\ninterest. To advance research in this field, promote discourse on suitable\nnormative bases and evaluation methodologies, and enhance the reproducibility\nof related studies, we propose a novel approach to database construction. This\napproach enables the assessment of gender-related biases in LLM-generated\nlanguage beyond merely evaluating their degree of neutralization.",
    "published_date": "2025-02-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.20898v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.20882v1",
    "title": "Managing Federated Learning on Decentralized Infrastructures as a Reputation-based Collaborative Workflow",
    "authors": [
      "Yuandou Wang",
      "Zhiming Zhao"
    ],
    "author_ids": [],
    "abstract": "Federated Learning (FL) has recently emerged as a collaborative learning\nparadigm that can train a global model among distributed participants without\nraw data exchange to satisfy varying requirements. However, there remain\nseveral challenges in managing FL in a decentralized environment, where\npotential candidates exhibit varying motivation levels and reliability in the\nFL process management: 1) reconfiguring and automating diverse FL workflows are\nchallenging, 2) difficulty in incentivizing potential candidates with\nhigh-quality data and high-performance computing to join the FL, and 3)\ndifficulty in ensuring reliable system operations, which may be vulnerable to\nvarious malicious attacks from FL participants. To address these challenges, we\nfocus on the workflow-based methods to automate diverse FL pipelines and\npropose a novel approach to facilitate reliable FL system operations with\nrobust mechanism design and blockchain technology by considering a contribution\nmodel, fair committee selection, dynamic reputation updates, reward and penalty\nmethods, and contract theory. Moreover, we study the optimality of contracts to\nguide the design and implementation of smart contracts that can be deployed in\nblockchain networks. We perform theoretical analysis and conduct extensive\nsimulation experiments to validate the proposed approach. The results show that\nour incentive mechanisms are feasible and can achieve fairness in reward\nallocation in unreliable environment settings.",
    "published_date": "2025-02-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.20882v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.20868v1",
    "title": "ProBench: Benchmarking Large Language Models in Competitive Programming",
    "authors": [
      "Lei Yang",
      "Renren Jin",
      "Ling Shi",
      "Jianxiang Peng",
      "Yue Chen",
      "Deyi Xiong"
    ],
    "author_ids": [],
    "abstract": "With reasoning language models such as OpenAI-o3 and DeepSeek-R1 emerging,\nlarge language models (LLMs) have entered a new phase of development. However,\nexisting benchmarks for coding evaluation are gradually inadequate to assess\nthe capability of advanced LLMs in code reasoning. To bridge the gap for\nhigh-level code reasoning assessment, we propose ProBench to benchmark LLMs in\ncompetitive programming, drawing inspiration from the International Collegiate\nProgramming Contest. ProBench collects a comprehensive set of competitive\nprogramming problems from Codeforces, Luogu, and Nowcoder platforms during the\nperiod from July to December 2024, obtaining real test results through online\nsubmissions to ensure the fairness and accuracy of the evaluation. We establish\na unified problem attribute system, including difficulty grading and algorithm\ntagging. With carefully collected and annotated data in ProBench, we\nsystematically assess 9 latest LLMs in competitive programming across multiple\ndimensions, including thought chain analysis, error type diagnosis, and\nreasoning depth evaluation. Experimental results show that QwQ-32B-Preview\nachieves the best score of 20.93 followed by DeepSeek-V3 with a score of 16.38,\nsuggesting that models trained with specialized reasoning tasks significantly\noutperform general-purpose models (even larger than reasoning-oriented models)\nin programming. Further analysis also reveals key areas for programming\ncapability enhancement, e.g., algorithm adaptability and reasoning sufficiency,\nproviding important insights for the future development of reasoning models.",
    "published_date": "2025-02-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.20868v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.20864v1",
    "title": "Do Language Models Understand Honorific Systems in Javanese?",
    "authors": [
      "Mohammad Rifqi Farhansyah",
      "Iwan Darmawan",
      "Adryan Kusumawardhana",
      "Genta Indra Winata",
      "Alham Fikri Aji",
      "Derry Tanti Wijaya"
    ],
    "author_ids": [],
    "abstract": "The Javanese language features a complex system of honorifics that vary\naccording to the social status of the speaker, listener, and referent. Despite\nits cultural and linguistic significance, there has been limited progress in\ndeveloping a comprehensive corpus to capture these variations for natural\nlanguage processing (NLP) tasks. In this paper, we present Unggah-Ungguh, a\ncarefully curated dataset designed to encapsulate the nuances of Unggah-Ungguh\nBasa, the Javanese speech etiquette framework that dictates the choice of words\nand phrases based on social hierarchy and context. Using Unggah-Ungguh, we\nassess the ability of language models (LMs) to process various levels of\nJavanese honorifics through classification and machine translation tasks. To\nfurther evaluate cross-lingual LMs, we conduct machine translation experiments\nbetween Javanese (at specific honorific levels) and Indonesian. Additionally,\nwe explore whether LMs can generate contextually appropriate Javanese\nhonorifics in conversation tasks, where the honorific usage should align with\nthe social role and contextual cues. Our findings indicate that current LMs\nstruggle with most honorific levels, exhibitinga bias toward certain honorific\ntiers.",
    "published_date": "2025-02-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.20864v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.03766v1",
    "title": "Inequalities Revisited",
    "authors": [
      "Raymond W. Yeung"
    ],
    "author_ids": [],
    "abstract": "In the past over two decades, very fruitful results have been obtained in\ninformation theory in the study of the Shannon entropy. This study has led to\nthe discovery of a new class of constraints on the Shannon entropy called\nnon-Shannon-type inequalities. Intimate connections between the Shannon entropy\nand different branches of mathematics including group theory, combinatorics,\nKolmogorov complexity, probability, matrix theory, etc, have been established.\nAll these discoveries were based on a formality introduced for constraints on\nthe Shannon entropy, which suggested the possible existence of constraints that\nwere not previously known. We assert that the same formality can be applied to\ninequalities beyond information theory. To illustrate the ideas, we revisit\nthrough the lens of this formality three fundamental inequalities in\nmathematics: the AM-GM inequality in algebra, Markov's inequality in\nprobability theory, and the Cauchy-Scharwz inequality for inner product spaces.\nApplications of this formality have the potential of leading to the discovery\nof new inequalities and constraints in different branches of mathematics.",
    "published_date": "2025-02-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.03766v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.20667v1",
    "title": "Advancing AI-Powered Medical Image Synthesis: Insights from MedVQA-GI Challenge Using CLIP, Fine-Tuned Stable Diffusion, and Dream-Booth + LoRA",
    "authors": [
      "Ojonugwa Oluwafemi Ejiga Peter",
      "Md Mahmudur Rahman",
      "Fahmi Khalifa"
    ],
    "author_ids": [],
    "abstract": "The MEDVQA-GI challenge addresses the integration of AI-driven text-to-image\ngenerative models in medical diagnostics, aiming to enhance diagnostic\ncapabilities through synthetic image generation. Existing methods primarily\nfocus on static image analysis and lack the dynamic generation of medical\nimagery from textual descriptions. This study intends to partially close this\ngap by introducing a novel approach based on fine-tuned generative models to\ngenerate dynamic, scalable, and precise images from textual descriptions.\nParticularly, our system integrates fine-tuned Stable Diffusion and DreamBooth\nmodels, as well as Low-Rank Adaptation (LORA), to generate high-fidelity\nmedical images. The problem is around two sub-tasks namely: image synthesis\n(IS) and optimal prompt production (OPG). The former creates medical images via\nverbal prompts, whereas the latter provides prompts that produce high-quality\nimages in specified categories. The study emphasizes the limitations of\ntraditional medical image generation methods, such as hand sketching,\nconstrained datasets, static procedures, and generic models. Our evaluation\nmeasures showed that Stable Diffusion surpasses CLIP and DreamBooth + LORA in\nterms of producing high-quality, diversified images. Specifically, Stable\nDiffusion had the lowest Fr\\'echet Inception Distance (FID) scores (0.099 for\nsingle center, 0.064 for multi-center, and 0.067 for combined), indicating\nhigher image quality. Furthermore, it had the highest average Inception Score\n(2.327 across all datasets), indicating exceptional diversity and quality. This\nadvances the field of AI-powered medical diagnosis. Future research will\nconcentrate on model refining, dataset augmentation, and ethical considerations\nfor efficiently implementing these advances into clinical practice",
    "published_date": "2025-02-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.20667v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.20513v1",
    "title": "Personas Evolved: Designing Ethical LLM-Based Conversational Agent Personalities",
    "authors": [
      "Smit Desai",
      "Mateusz Dubiel",
      "Nima Zargham",
      "Thomas Mildner",
      "Laura Spillner"
    ],
    "author_ids": [],
    "abstract": "The emergence of Large Language Models (LLMs) has revolutionized\nConversational User Interfaces (CUIs), enabling more dynamic, context-aware,\nand human-like interactions across diverse domains, from social sciences to\nhealthcare. However, the rapid adoption of LLM-based personas raises critical\nethical and practical concerns, including bias, manipulation, and unforeseen\nsocial consequences. Unlike traditional CUIs, where personas are carefully\ndesigned with clear intent, LLM-based personas generate responses dynamically\nfrom vast datasets, making their behavior less predictable and harder to\ngovern. This workshop aims to bridge the gap between CUI and broader AI\ncommunities by fostering a cross-disciplinary dialogue on the responsible\ndesign and evaluation of LLM-based personas. Bringing together researchers,\ndesigners, and practitioners, we will explore best practices, develop ethical\nguidelines, and promote frameworks that ensure transparency, inclusivity, and\nuser-centered interactions. By addressing these challenges collaboratively, we\nseek to shape the future of LLM-driven CUIs in ways that align with societal\nvalues and expectations.",
    "published_date": "2025-02-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.20513v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.20499v2",
    "title": "Data Distributional Properties As Inductive Bias for Systematic Generalization",
    "authors": [
      "Felipe del Río",
      "Alain Raymond-Sáez",
      "Daniel Florea",
      "Rodrigo Toro Icarte",
      "Julio Hurtado",
      "Cristián Buc Calderón",
      "Álvaro Soto"
    ],
    "author_ids": [],
    "abstract": "Deep neural networks (DNNs) struggle at systematic generalization (SG).\nSeveral studies have evaluated the possibility to promote SG through the\nproposal of novel architectures, loss functions or training methodologies. Few\nstudies, however, have focused on the role of training data properties in\npromoting SG. In this work, we investigate the impact of certain data\ndistributional properties, as inductive biases for the SG ability of a\nmulti-modal language model. To this end, we study three different properties.\nFirst, data diversity, instantiated as an increase in the possible values a\nlatent property in the training distribution may take. Second, burstiness,\nwhere we probabilistically restrict the number of possible values of latent\nfactors on particular inputs during training. Third, latent intervention, where\na particular latent factor is altered randomly during training. We find that\nall three factors significantly enhance SG, with diversity contributing an 89%\nabsolute increase in accuracy in the most affected property. Through a series\nof experiments, we test various hypotheses to understand why these properties\npromote SG. Finally, we find that Normalized Mutual Information (NMI) between\nlatent attributes in the training distribution is strongly predictive of\nout-of-distribution generalization. We find that a mechanism by which lower NMI\ninduces SG is in the geometry of representations. In particular, we find that\nNMI induces more parallelism in neural representations (i.e., input features\ncoded in parallel neural vectors) of the model, a property related to the\ncapacity of reasoning by analogy.",
    "published_date": "2025-02-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.20499v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.20432v2",
    "title": "Large Language Model Strategic Reasoning Evaluation through Behavioral Game Theory",
    "authors": [
      "Jingru Jia",
      "Zehua Yuan",
      "Junhao Pan",
      "Paul E. McNamara",
      "Deming Chen"
    ],
    "author_ids": [],
    "abstract": "Strategic decision-making involves interactive reasoning where agents adapt\ntheir choices in response to others, yet existing evaluations of large language\nmodels (LLMs) often emphasize Nash Equilibrium (NE) approximation, overlooking\nthe mechanisms driving their strategic choices. To bridge this gap, we\nintroduce an evaluation framework grounded in behavioral game theory,\ndisentangling reasoning capability from contextual effects. Testing 22\nstate-of-the-art LLMs, we find that GPT-o3-mini, GPT-o1, and DeepSeek-R1\ndominate most games yet also demonstrate that the model scale alone does not\ndetermine performance. In terms of prompting enhancement, Chain-of-Thought\n(CoT) prompting is not universally effective, as it increases strategic\nreasoning only for models at certain levels while providing limited gains\nelsewhere. Additionally, we investigate the impact of encoded demographic\nfeatures on the models, observing that certain assignments impact the\ndecision-making pattern. For instance, GPT-4o shows stronger strategic\nreasoning with female traits than males, while Gemma assigns higher reasoning\nlevels to heterosexual identities compared to other sexual orientations,\nindicating inherent biases. These findings underscore the need for ethical\nstandards and contextual alignment to balance improved reasoning with fairness.",
    "published_date": "2025-02-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.GT",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.20432v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.20354v1",
    "title": "Towards Responsible AI in Education: Hybrid Recommendation System for K-12 Students Case Study",
    "authors": [
      "Nazarii Drushchak",
      "Vladyslava Tyshchenko",
      "Nataliya Polyakovska"
    ],
    "author_ids": [],
    "abstract": "The growth of Educational Technology (EdTech) has enabled highly personalized\nlearning experiences through Artificial Intelligence (AI)-based recommendation\nsystems tailored to each student needs. However, these systems can\nunintentionally introduce biases, potentially limiting fair access to learning\nresources. This study presents a recommendation system for K-12 students,\ncombining graph-based modeling and matrix factorization to provide personalized\nsuggestions for extracurricular activities, learning resources, and\nvolunteering opportunities. To address fairness concerns, the system includes a\nframework to detect and reduce biases by analyzing feedback across protected\nstudent groups. This work highlights the need for continuous monitoring in\neducational recommendation systems to support equitable, transparent, and\neffective learning opportunities for all students.",
    "published_date": "2025-02-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.20354v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.20260v1",
    "title": "Understanding the Limits of Deep Tabular Methods with Temporal Shift",
    "authors": [
      "Hao-Run Cai",
      "Han-Jia Ye"
    ],
    "author_ids": [],
    "abstract": "Deep tabular models have demonstrated remarkable success on i.i.d. data,\nexcelling in a variety of structured data tasks. However, their performance\noften deteriorates under temporal distribution shifts, where trends and\nperiodic patterns are present in the evolving data distribution over time. In\nthis paper, we explore the underlying reasons for this failure in capturing\ntemporal dependencies. We begin by investigating the training protocol,\nrevealing a key issue in how model selection perform. While existing approaches\nuse temporal ordering for splitting validation set, we show that even a random\nsplit can significantly improve model performance. By minimizing the time lag\nbetween training data and test time, while reducing the bias in validation, our\nproposed training protocol significantly improves generalization across various\nmethods. Furthermore, we analyze how temporal data affects deep tabular\nrepresentations, uncovering that these models often fail to capture crucial\nperiodic and trend information. To address this gap, we introduce a\nplug-and-play temporal embedding method based on Fourier series expansion to\nlearn and incorporate temporal patterns, offering an adaptive approach to\nhandle temporal shifts. Our experiments demonstrate that this temporal\nembedding, combined with the improved training protocol, provides a more\neffective and robust framework for learning from temporal tabular data.",
    "published_date": "2025-02-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "68T05",
      "I.2.6"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.20260v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.20237v1",
    "title": "Teasing Apart Architecture and Initial Weights as Sources of Inductive Bias in Neural Networks",
    "authors": [
      "Gianluca Bencomo",
      "Max Gupta",
      "Ioana Marinescu",
      "R. Thomas McCoy",
      "Thomas L. Griffiths"
    ],
    "author_ids": [],
    "abstract": "Artificial neural networks can acquire many aspects of human knowledge from\ndata, making them promising as models of human learning. But what those\nnetworks can learn depends upon their inductive biases -- the factors other\nthan the data that influence the solutions they discover -- and the inductive\nbiases of neural networks remain poorly understood, limiting our ability to\ndraw conclusions about human learning from the performance of these systems.\nCognitive scientists and machine learning researchers often focus on the\narchitecture of a neural network as a source of inductive bias. In this paper\nwe explore the impact of another source of inductive bias -- the initial\nweights of the network -- using meta-learning as a tool for finding initial\nweights that are adapted for specific problems. We evaluate four widely-used\narchitectures -- MLPs, CNNs, LSTMs, and Transformers -- by meta-training 430\ndifferent models across three tasks requiring different biases and forms of\ngeneralization. We find that meta-learning can substantially reduce or entirely\neliminate performance differences across architectures and data\nrepresentations, suggesting that these factors may be less important as sources\nof inductive bias than is typically assumed. When differences are present,\narchitectures and data representations that perform well without meta-learning\ntend to meta-train more effectively. Moreover, all architectures generalize\npoorly on problems that are far from their meta-training experience,\nunderscoring the need for stronger inductive biases for robust generalization.",
    "published_date": "2025-02-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.20237v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.20224v2",
    "title": "RURANET++: An Unsupervised Learning Method for Diabetic Macular Edema Based on SCSE Attention Mechanisms and Dynamic Multi-Projection Head Clustering",
    "authors": [
      "Wei Yang",
      "Yiran Zhu",
      "Jiayu Shen",
      "Yuhan Tang",
      "Chengchang Pan",
      "Hui He",
      "Yan Su",
      "Honggang Qi"
    ],
    "author_ids": [],
    "abstract": "Diabetic Macular Edema (DME), a prevalent complication among diabetic\npatients, constitutes a major cause of visual impairment and blindness.\nAlthough deep learning has achieved remarkable progress in medical image\nanalysis, traditional DME diagnosis still relies on extensive annotated data\nand subjective ophthalmologist assessments, limiting practical applications. To\naddress this, we present RURANET++, an unsupervised learning-based automated\nDME diagnostic system. This framework incorporates an optimized U-Net\narchitecture with embedded Spatial and Channel Squeeze & Excitation (SCSE)\nattention mechanisms to enhance lesion feature extraction. During feature\nprocessing, a pre-trained GoogLeNet model extracts deep features from retinal\nimages, followed by PCA-based dimensionality reduction to 50 dimensions for\ncomputational efficiency. Notably, we introduce a novel clustering algorithm\nemploying multi-projection heads to explicitly control cluster diversity while\ndynamically adjusting similarity thresholds, thereby optimizing intra-class\nconsistency and inter-class discrimination. Experimental results demonstrate\nsuperior performance across multiple metrics, achieving maximum accuracy\n(0.8411), precision (0.8593), recall (0.8411), and F1-score (0.8390), with\nexceptional clustering quality. This work provides an efficient unsupervised\nsolution for DME diagnosis with significant clinical implications.",
    "published_date": "2025-02-27T00:00:00",
    "year": 2025,
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.20224v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.20158v1",
    "title": "Learning to Generalize without Bias for Open-Vocabulary Action Recognition",
    "authors": [
      "Yating Yu",
      "Congqi Cao",
      "Yifan Zhang",
      "Yanning Zhang"
    ],
    "author_ids": [],
    "abstract": "Leveraging the effective visual-text alignment and static generalizability\nfrom CLIP, recent video learners adopt CLIP initialization with further\nregularization or recombination for generalization in open-vocabulary action\nrecognition in-context. However, due to the static bias of CLIP, such video\nlearners tend to overfit on shortcut static features, thereby compromising\ntheir generalizability, especially to novel out-of-context actions. To address\nthis issue, we introduce Open-MeDe, a novel Meta-optimization framework with\nstatic Debiasing for Open-vocabulary action recognition. From a fresh\nperspective of generalization, Open-MeDe adopts a meta-learning approach to\nimprove known-to-open generalizing and image-to-video debiasing in a\ncost-effective manner. Specifically, Open-MeDe introduces a cross-batch\nmeta-optimization scheme that explicitly encourages video learners to quickly\ngeneralize to arbitrary subsequent data via virtual evaluation, steering a\nsmoother optimization landscape. In effect, the free of CLIP regularization\nduring optimization implicitly mitigates the inherent static bias of the video\nmeta-learner. We further apply self-ensemble over the optimization trajectory\nto obtain generic optimal parameters that can achieve robust generalization to\nboth in-context and out-of-context novel data. Extensive evaluations show that\nOpen-MeDe not only surpasses state-of-the-art regularization methods tailored\nfor in-context open-vocabulary action recognition but also substantially excels\nin out-of-context scenarios.",
    "published_date": "2025-02-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.20158v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.04785v1",
    "title": "Mapping Trustworthiness in Large Language Models: A Bibliometric Analysis Bridging Theory to Practice",
    "authors": [
      "José Siqueira de Cerqueira",
      "Kai-Kristian Kemell",
      "Rebekah Rousi",
      "Nannan Xi",
      "Juho Hamari",
      "Pekka Abrahamsson"
    ],
    "author_ids": [],
    "abstract": "The rapid proliferation of Large Language Models (LLMs) has raised pressing\nconcerns regarding their trustworthiness, spanning issues of reliability,\ntransparency, fairness, and ethical alignment. Despite the increasing adoption\nof LLMs across various domains, there remains a lack of consensus on how to\noperationalize trustworthiness in practice. This study bridges the gap between\ntheoretical discussions and implementation by conducting a bibliometric mapping\nanalysis of 2,006 publications from 2019 to 2025. Through co-authorship\nnetworks, keyword co-occurrence analysis, and thematic evolution tracking, we\nidentify key research trends, influential authors, and prevailing definitions\nof LLM trustworthiness. Additionally, a systematic review of 68 core papers is\nconducted to examine conceptualizations of trust and their practical\nimplications. Our findings reveal that trustworthiness in LLMs is often framed\nthrough existing organizational trust frameworks, emphasizing dimensions such\nas ability, benevolence, and integrity. However, a significant gap exists in\ntranslating these principles into concrete development strategies. To address\nthis, we propose a structured mapping of 20 trust-enhancing techniques across\nthe LLM lifecycle, including retrieval-augmented generation (RAG),\nexplainability techniques, and post-training audits. By synthesizing\nbibliometric insights with practical strategies, this study contributes towards\nfostering more transparent, accountable, and ethically aligned LLMs, ensuring\ntheir responsible deployment in real-world applications.",
    "published_date": "2025-02-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.04785v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.05787v1",
    "title": "Mapping the Regulatory Learning Space for the EU AI Act",
    "authors": [
      "Dave Lewis",
      "Marta Lasek-Markey",
      "Delaram Golpayegani",
      "Harshvardhan J. Pandit"
    ],
    "author_ids": [],
    "abstract": "The EU's AI Act represents the world first transnational AI regulation with\nconcrete enforcement measures. It builds upon existing EU mechanisms for\nproduct health and safety regulation, but extends it to protect fundamental\nrights and by addressing AI as a horizontal technology that is regulated across\nmultiple vertical application sectors. These extensions introduce uncertainties\nin terms of how the technical state of the art will be applied to AI system\ncertification and enforcement actions, how horizontal technical measures will\nmap into vertical enforcement responsibilities and the degree to which\ndifferent fundamental rights can be protected across EU Member States. We argue\nthat these uncertainties, coupled with the fast changing nature of AI and the\nrelative immaturity of the state of the art in fundamental rights risk\nmanagement require the implementation of the AI Act to place a strong emphasis\non comprehensive and rapid regulatory learning. We define parameterised axes\nfor the regulatory learning space set out in the Act and describe a layered\nsystem of different learning arenas where the population of oversight\nauthorities, value chain participants and affected stakeholders may interact to\napply and learn from technical, organisational and legal implementation\nmeasures. We conclude by exploring how existing open data policies and\npractices in the EU can be adapted to support regulatory learning in a\ntransparent manner that supports the development of trust in and predictability\nof regulated AI. We discuss how the Act may result in a regulatory turn in the\nresearch of AI fairness, accountability and transparency towards investigations\ninto implementations of and interactions between different fundamental rights\nprotections and reproducible and accountable models of metrology for AI risk\nassessment and treatment.",
    "published_date": "2025-02-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.05787v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.19962v2",
    "title": "ReCon: Enhancing True Correspondence Discrimination through Relation Consistency for Robust Noisy Correspondence Learning",
    "authors": [
      "Quanxing Zha",
      "Xin Liu",
      "Shu-Juan Peng",
      "Yiu-ming Cheung",
      "Xing Xu",
      "Nannan Wang"
    ],
    "author_ids": [],
    "abstract": "Can we accurately identify the true correspondences from multimodal datasets\ncontaining mismatched data pairs? Existing methods primarily emphasize the\nsimilarity matching between the representations of objects across modalities,\npotentially neglecting the crucial relation consistency within modalities that\nare particularly important for distinguishing the true and false\ncorrespondences. Such an omission often runs the risk of misidentifying\nnegatives as positives, thus leading to unanticipated performance degradation.\nTo address this problem, we propose a general Relation Consistency learning\nframework, namely ReCon, to accurately discriminate the true correspondences\namong the multimodal data and thus effectively mitigate the adverse impact\ncaused by mismatches. Specifically, ReCon leverages a novel relation\nconsistency learning to ensure the dual-alignment, respectively of, the\ncross-modal relation consistency between different modalities and the\nintra-modal relation consistency within modalities. Thanks to such dual\nconstrains on relations, ReCon significantly enhances its effectiveness for\ntrue correspondence discrimination and therefore reliably filters out the\nmismatched pairs to mitigate the risks of wrong supervisions. Extensive\nexperiments on three widely-used benchmark datasets, including Flickr30K,\nMS-COCO, and Conceptual Captions, are conducted to demonstrate the\neffectiveness and superiority of ReCon compared with other SOTAs. The code is\navailable at: https://github.com/qxzha/ReCon.",
    "published_date": "2025-02-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.19962v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.19954v1",
    "title": "Collaborative Stance Detection via Small-Large Language Model Consistency Verification",
    "authors": [
      "Yu Yan",
      "Sheng Sun",
      "Zixiang Tang",
      "Teli Liu",
      "Min Liu"
    ],
    "author_ids": [],
    "abstract": "Stance detection on social media aims to identify attitudes expressed in\ntweets towards specific targets. Current studies prioritize Large Language\nModels (LLMs) over Small Language Models (SLMs) due to the overwhelming\nperformance improving provided by LLMs. However, heavily relying on LLMs for\nstance detection, regardless of the cost, is impractical for real-world social\nmedia monitoring systems that require vast data analysis. To this end, we\npropose \\textbf{\\underline{Co}}llaborative Stance Detection via Small-Large\nLanguage Model Consistency \\textbf{\\underline{Ver}}ification (\\textbf{CoVer})\nframework, which enhances LLM utilization via context-shared batch reasoning\nand logical verification between LLM and SLM. Specifically, instead of\nprocessing each text individually, CoVer processes texts batch-by-batch,\nobtaining stance predictions and corresponding explanations via LLM reasoning\nin a shared context. Then, to exclude the bias caused by context noises, CoVer\nintroduces the SLM for logical consistency verification. Finally, texts that\nrepeatedly exhibit low logical consistency are classified using\nconsistency-weighted aggregation of prior LLM stance predictions. Our\nexperiments show that CoVer outperforms state-of-the-art methods across\nmultiple benchmarks in the zero-shot setting, achieving 0.54 LLM queries per\ntweet while significantly enhancing performance. Our CoVer offers a more\npractical solution for LLM deploying for social media stance detection.",
    "published_date": "2025-02-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.19954v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.19915v1",
    "title": "LLM-driven Effective Knowledge Tracing by Integrating Dual-channel Difficulty",
    "authors": [
      "Jiahui Cen",
      "Jianghao Lin",
      "Weizhong Xuan",
      "Dong Zhou",
      "Jin Chen",
      "Aimin Yang",
      "Yongmei Zhou"
    ],
    "author_ids": [],
    "abstract": "Knowledge Tracing (KT) is a fundamental technology in intelligent tutoring\nsystems used to simulate changes in students' knowledge state during learning,\ntrack personalized knowledge mastery, and predict performance. However, current\nKT models face three major challenges: (1) When encountering new questions,\nmodels face cold-start problems due to sparse interaction records, making\nprecise modeling difficult; (2) Traditional models only use historical\ninteraction records for student personalization modeling, unable to accurately\ntrack individual mastery levels, resulting in unclear personalized modeling;\n(3) The decision-making process is opaque to educators, making it challenging\nfor them to understand model judgments. To address these challenges, we propose\na novel Dual-channel Difficulty-aware Knowledge Tracing (DDKT) framework that\nutilizes Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG)\nfor subjective difficulty assessment, while integrating difficulty bias-aware\nalgorithms and student mastery algorithms for precise difficulty measurement.\nOur framework introduces three key innovations: (1) Difficulty Balance\nPerception Sequence (DBPS) - students' subjective perceptions combined with\nobjective difficulty, measuring gaps between LLM-assessed difficulty,\nmathematical-statistical difficulty, and students' subjective perceived\ndifficulty through attention mechanisms; (2) Difficulty Mastery Ratio (DMR) -\nprecise modeling of student mastery levels through different difficulty zones;\n(3) Knowledge State Update Mechanism - implementing personalized knowledge\nacquisition through gated networks and updating student knowledge state.\nExperimental results on two real datasets show our method consistently\noutperforms nine baseline models, improving AUC metrics by 2% to 10% while\neffectively addressing cold-start problems and enhancing model\ninterpretability.",
    "published_date": "2025-02-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.19915v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.19846v1",
    "title": "Fair and Actionable Causal Prescription Ruleset",
    "authors": [
      "Benton Li",
      "Nativ Levy",
      "Brit Youngmann",
      "Sainyam Galhotra",
      "Sudeepa Roy"
    ],
    "author_ids": [],
    "abstract": "Prescriptions, or actionable recommendations, are commonly generated across\nvarious fields to influence key outcomes such as improving public health,\nenhancing economic policies, or increasing business efficiency. While\ntraditional association-based methods may identify correlations, they often\nfail to reveal the underlying causal factors needed for informed\ndecision-making. On the other hand, in decision-making for tasks with\nsignificant societal or economic impact, it is crucial to provide\nrecommendations that are justifiable and equitable in terms of the outcome for\nboth the protected and non-protected groups. Motivated by these two goals, this\npaper introduces a fairness-aware framework leveraging causal reasoning for\ngenerating a set of actionable prescription rules (ruleset) toward betterment\nof an outcome while preventing exacerbating inequalities for protected groups.\nBy considering group and individual fairness metrics from the literature, we\nensure that both protected and non-protected groups benefit from these\nrecommendations, providing a balanced and equitable approach to\ndecision-making. We employ efficient optimizations to explore the vast and\ncomplex search space considering both fairness and coverage of the ruleset.\nEmpirical evaluation and case study on real-world datasets demonstrates the\nutility of our framework for different use cases.",
    "published_date": "2025-02-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.DB"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.19846v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.19844v3",
    "title": "ProAPO: Progressively Automatic Prompt Optimization for Visual Classification",
    "authors": [
      "Xiangyan Qu",
      "Gaopeng Gou",
      "Jiamin Zhuang",
      "Jing Yu",
      "Kun Song",
      "Qihao Wang",
      "Yili Li",
      "Gang Xiong"
    ],
    "author_ids": [],
    "abstract": "Vision-language models (VLMs) have made significant progress in image\nclassification by training with large-scale paired image-text data. Their\nperformances largely depend on the prompt quality. While recent methods show\nthat visual descriptions generated by large language models (LLMs) enhance the\ngeneralization of VLMs, class-specific prompts may be inaccurate or lack\ndiscrimination due to the hallucination in LLMs. In this paper, we aim to find\nvisually discriminative prompts for fine-grained categories with minimal\nsupervision and no human-in-the-loop. An evolution-based algorithm is proposed\nto progressively optimize language prompts from task-specific templates to\nclass-specific descriptions. Unlike optimizing templates, the search space\nshows an explosion in class-specific candidate prompts. This increases prompt\ngeneration costs, iterative times, and the overfitting problem. To this end, we\nfirst introduce several simple yet effective edit-based and evolution-based\noperations to generate diverse candidate prompts by one-time query of LLMs.\nThen, two sampling strategies are proposed to find a better initial search\npoint and reduce traversed categories, saving iteration costs. Moreover, we\napply a novel fitness score with entropy constraints to mitigate overfitting.\nIn a challenging one-shot image classification setting, our method outperforms\nexisting textual prompt-based methods and improves LLM-generated description\nmethods across 13 datasets. Meanwhile, we demonstrate that our optimal prompts\nimprove adapter-based methods and transfer effectively across different\nbackbones.",
    "published_date": "2025-02-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.19844v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.19822v1",
    "title": "Empowering Social Service with AI: Insights from a Participatory Design Study with Practitioners",
    "authors": [
      "Yugin Tan",
      "Kai Xin Soh",
      "Renwen Zhang",
      "Jungup Lee",
      "Han Meng",
      "Biswadeep Sen",
      "Yi-Chieh Lee"
    ],
    "author_ids": [],
    "abstract": "In social service, administrative burdens and decision-making challenges\noften hinder practitioners from performing effective casework. Generative AI\n(GenAI) offers significant potential to streamline these tasks, yet exacerbates\nconcerns about overreliance, algorithmic bias, and loss of identity within the\nprofession. We explore these issues through a two-stage participatory design\nstudy. We conducted formative co-design workshops (\\textit{n=27}) to create a\nprototype GenAI tool, followed by contextual inquiry sessions with\npractitioners (\\textit{n=24}) using the tool with real case data. We reveal\nopportunities for AI integration in documentation, assessment, and worker\nsupervision, while highlighting risks related to GenAI limitations, skill\nretention, and client safety. Drawing comparisons with GenAI tools in other\nfields, we discuss design and usage guidelines for such tools in social service\npractice.",
    "published_date": "2025-02-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.19822v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.19771v2",
    "title": "The erasure of intensive livestock farming in text-to-image generative AI",
    "authors": [
      "Kehan Sheng",
      "Frank A. M. Tuyttens",
      "Marina A. G. von Keyserlingk"
    ],
    "author_ids": [],
    "abstract": "Generative AI (e.g., ChatGPT) is increasingly integrated into people's daily\nlives. While it is known that AI perpetuates biases against marginalized human\ngroups, their impact on non-human animals remains understudied. We found that\nChatGPT's text-to-image model (DALL-E 3) introduces a strong bias toward\nromanticizing livestock farming as dairy cows on pasture and pigs rooting in\nmud. This bias remained when we requested realistic depictions and was only\nmitigated when the automatic prompt revision was inhibited. Most farmed animal\nin industrialized countries are reared indoors with limited space per animal,\nwhich fail to resonate with societal values. Inhibiting prompt revision\nresulted in images that more closely reflected modern farming practices; for\nexample, cows housed indoors accessing feed through metal headlocks, and pigs\nbehind metal railings on concrete floors in indoor facilities. While OpenAI\nintroduced prompt revision to mitigate bias, in the case of farmed animal\nproduction systems, it paradoxically introduces a strong bias towards\nunrealistic farming practices.",
    "published_date": "2025-02-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.19771v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.19749v1",
    "title": "Beneath the Surface: How Large Language Models Reflect Hidden Bias",
    "authors": [
      "Jinhao Pan",
      "Chahat Raj",
      "Ziyu Yao",
      "Ziwei Zhu"
    ],
    "author_ids": [],
    "abstract": "The exceptional performance of Large Language Models (LLMs) often comes with\nthe unintended propagation of social biases embedded in their training data.\nWhile existing benchmarks evaluate overt bias through direct term associations\nbetween bias concept terms and demographic terms, LLMs have become increasingly\nadept at avoiding biased responses, creating an illusion of neutrality.\nHowever, biases persist in subtler, contextually hidden forms that traditional\nbenchmarks fail to capture. We introduce the Hidden Bias Benchmark (HBB), a\nnovel dataset designed to assess hidden bias that bias concepts are hidden\nwithin naturalistic, subtly framed contexts in real-world scenarios. We analyze\nsix state-of-the-art LLMs, revealing that while models reduce bias in response\nto overt bias, they continue to reinforce biases in nuanced settings. Data,\ncode, and results are available at\nhttps://github.com/JP-25/Hidden-Bias-Benchmark.",
    "published_date": "2025-02-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.19749v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.19721v1",
    "title": "Sensing and Steering Stereotypes: Extracting and Applying Gender Representation Vectors in LLMs",
    "authors": [
      "Hannah Cyberey",
      "Yangfeng Ji",
      "David Evans"
    ],
    "author_ids": [],
    "abstract": "Large language models (LLMs) are known to perpetuate stereotypes and exhibit\nbiases. Various strategies have been proposed to mitigate potential harms that\nmay result from these biases, but most work studies biases in LLMs as a\nblack-box problem without considering how concepts are represented within the\nmodel. We adapt techniques from representation engineering to study how the\nconcept of \"gender\" is represented within LLMs. We introduce a new method that\nextracts concept representations via probability weighting without labeled data\nand efficiently selects a steering vector for measuring and manipulating the\nmodel's representation. We also present a projection-based method that enables\nprecise steering of model predictions and demonstrate its effectiveness in\nmitigating gender bias in LLMs.",
    "published_date": "2025-02-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.19721v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.19699v1",
    "title": "Spatial-Spectral Diffusion Contrastive Representation Network for Hyperspectral Image Classification",
    "authors": [
      "Yimin Zhu",
      "Linlin Xu"
    ],
    "author_ids": [],
    "abstract": "Although efficient extraction of discriminative spatial-spectral features is\ncritical for hyperspectral images classification (HSIC), it is difficult to\nachieve these features due to factors such as the spatial-spectral\nheterogeneity and noise effect. This paper presents a Spatial-Spectral\nDiffusion Contrastive Representation Network (DiffCRN), based on denoising\ndiffusion probabilistic model (DDPM) combined with contrastive learning (CL)\nfor HSIC, with the following characteristics. First,to improve spatial-spectral\nfeature representation, instead of adopting the UNets-like structure which is\nwidely used for DDPM, we design a novel staged architecture with spatial\nself-attention denoising module (SSAD) and spectral group self-attention\ndenoising module (SGSAD) in DiffCRN with improved efficiency for\nspectral-spatial feature learning. Second, to improve unsupervised feature\nlearning efficiency, we design new DDPM model with logarithmic absolute error\n(LAE) loss and CL that improve the loss function effectiveness and increase the\ninstance-level and inter-class discriminability. Third, to improve feature\nselection, we design a learnable approach based on pixel-level spectral angle\nmapping (SAM) for the selection of time steps in the proposed DDPM model in an\nadaptive and automatic manner. Last, to improve feature integration and\nclassification, we design an Adaptive weighted addition modul (AWAM) and Cross\ntime step Spectral-Spatial Fusion Module (CTSSFM) to fuse time-step-wise\nfeatures and perform classification. Experiments conducted on widely used four\nHSI datasets demonstrate the improved performance of the proposed DiffCRN over\nthe classical backbone models and state-of-the-art GAN, transformer models and\nother pretrained methods. The source code and pre-trained model will be made\navailable publicly.",
    "published_date": "2025-02-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.19699v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.19674v1",
    "title": "MICINet: Multi-Level Inter-Class Confusing Information Removal for Reliable Multimodal Classification",
    "authors": [
      "Tong Zhang",
      "Shu Shen",
      "C. L. Philip Chen"
    ],
    "author_ids": [],
    "abstract": "Reliable multimodal learning in the presence of noisy data is a widely\nconcerned issue, especially in safety-critical applications. Many reliable\nmultimodal methods delve into addressing modality-specific or cross-modality\nnoise. However, they fail to handle the coexistence of both types of noise\nefficiently. Moreover, the lack of comprehensive consideration for noise at\nboth global and individual levels limits their reliability. To address these\nissues, a reliable multimodal classification method dubbed Multi-Level\nInter-Class Confusing Information Removal Network (MICINet) is proposed.\nMICINet achieves the reliable removal of both types of noise by unifying them\ninto the concept of Inter-class Confusing Information (\\textit{ICI}) and\neliminating it at both global and individual levels. Specifically, MICINet\nfirst reliably learns the global \\textit{ICI} distribution through the proposed\n\\textbf{\\textit{Global \\textbf{ICI} Learning Module}}. Then, it introduces the\n\\textbf{\\textit{Global-guided Sample ICI Learning module}} to efficiently\nremove global-level \\textit{ICI} from sample features utilizing the learned\nglobal \\textit{ICI} distribution. Subsequently, the\n\\textbf{\\textit{Sample-adaptive Cross-modality Information Compensation\nmodule}} is designed to remove individual-level \\textit{ICI} from each sample\nreliably. This is achieved through interpretable cross-modality information\ncompensation based on the complementary relationship between discriminative\nfeatures and \\textit{ICI} and the perception of the relative quality of\nmodalities introduced by the relative discriminative power. Experiments on four\ndatasets demonstrate that MICINet outperforms other state-of-the-art reliable\nmultimodal classification methods under various noise conditions.",
    "published_date": "2025-02-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.19674v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.19625v2",
    "title": "Revealing Treatment Non-Adherence Bias in Clinical Machine Learning Using Large Language Models",
    "authors": [
      "Zhongyuan Liang",
      "Arvind Suresh",
      "Irene Y. Chen"
    ],
    "author_ids": [],
    "abstract": "Machine learning systems trained on electronic health records (EHRs)\nincreasingly guide treatment decisions, but their reliability depends on the\ncritical assumption that patients follow the prescribed treatments recorded in\nEHRs. Using EHR data from 3,623 hypertension patients, we investigate how\ntreatment non-adherence introduces implicit bias that can fundamentally distort\nboth causal inference and predictive modeling. By extracting patient adherence\ninformation from clinical notes using a large language model (LLM), we identify\n786 patients (21.7%) with medication non-adherence. We further uncover key\ndemographic and clinical factors associated with non-adherence, as well as\npatient-reported reasons including side effects and difficulties obtaining\nrefills. Our findings demonstrate that this implicit bias can not only reverse\nestimated treatment effects, but also degrade model performance by up to 5%\nwhile disproportionately affecting vulnerable populations by exacerbating\ndisparities in decision outcomes and model error rates. This highlights the\nimportance of accounting for treatment non-adherence in developing responsible\nand equitable clinical machine learning systems.",
    "published_date": "2025-02-26T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.19625v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.19612v1",
    "title": "Evaluation of Hate Speech Detection Using Large Language Models and Geographical Contextualization",
    "authors": [
      "Anwar Hossain Zahid",
      "Monoshi Kumar Roy",
      "Swarna Das"
    ],
    "author_ids": [],
    "abstract": "The proliferation of hate speech on social media is one of the serious issues\nthat is bringing huge impacts to society: an escalation of violence,\ndiscrimination, and social fragmentation. The problem of detecting hate speech\nis intrinsically multifaceted due to cultural, linguistic, and contextual\ncomplexities and adversarial manipulations. In this study, we systematically\ninvestigate the performance of LLMs on detecting hate speech across\nmultilingual datasets and diverse geographic contexts. Our work presents a new\nevaluation framework in three dimensions: binary classification of hate speech,\ngeography-aware contextual detection, and robustness to adversarially generated\ntext. Using a dataset of 1,000 comments from five diverse regions, we evaluate\nthree state-of-the-art LLMs: Llama2 (13b), Codellama (7b), and DeepSeekCoder\n(6.7b). Codellama had the best binary classification recall with 70.6% and an\nF1-score of 52.18%, whereas DeepSeekCoder had the best performance in\ngeographic sensitivity, correctly detecting 63 out of 265 locations. The tests\nfor adversarial robustness also showed significant weaknesses; Llama2\nmisclassified 62.5% of manipulated samples. These results bring to light the\ntrade-offs between accuracy, contextual understanding, and robustness in the\ncurrent versions of LLMs. This work has thus set the stage for developing\ncontextually aware, multilingual hate speech detection systems by underlining\nkey strengths and limitations, therefore offering actionable insights for\nfuture research and real-world applications.",
    "published_date": "2025-02-26T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.LG",
      "I.2.7; I.2.6; K.4.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.19612v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.19582v1",
    "title": "Where Are We? Evaluating LLM Performance on African Languages",
    "authors": [
      "Ife Adebara",
      "Hawau Olamide Toyin",
      "Nahom Tesfu Ghebremichael",
      "AbdelRahim Elmadany",
      "Muhammad Abdul-Mageed"
    ],
    "author_ids": [],
    "abstract": "Africa's rich linguistic heritage remains underrepresented in NLP, largely\ndue to historical policies that favor foreign languages and create significant\ndata inequities. In this paper, we integrate theoretical insights on Africa's\nlanguage landscape with an empirical evaluation using Sahara - a comprehensive\nbenchmark curated from large-scale, publicly accessible datasets capturing the\ncontinent's linguistic diversity. By systematically assessing the performance\nof leading large language models (LLMs) on Sahara, we demonstrate how\npolicy-induced data variations directly impact model effectiveness across\nAfrican languages. Our findings reveal that while a few languages perform\nreasonably well, many Indigenous languages remain marginalized due to sparse\ndata. Leveraging these insights, we offer actionable recommendations for policy\nreforms and inclusive data practices. Overall, our work underscores the urgent\nneed for a dual approach - combining theoretical understanding with empirical\nevaluation - to foster linguistic diversity in AI for African communities.",
    "published_date": "2025-02-26T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.19582v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.19252v2",
    "title": "GraphBridge: Towards Arbitrary Transfer Learning in GNNs",
    "authors": [
      "Li Ju",
      "Xingyi Yang",
      "Qi Li",
      "Xinchao Wang"
    ],
    "author_ids": [],
    "abstract": "Graph neural networks (GNNs) are conventionally trained on a per-domain,\nper-task basis. It creates a significant barrier in transferring the acquired\nknowledge to different, heterogeneous data setups. This paper introduces\nGraphBridge, a novel framework to enable knowledge transfer across disparate\ntasks and domains in GNNs, circumventing the need for modifications to task\nconfigurations or graph structures. Specifically, GraphBridge allows for the\naugmentation of any pre-trained GNN with prediction heads and a bridging\nnetwork that connects the input to the output layer. This architecture not only\npreserves the intrinsic knowledge of the original model but also supports\noutputs of arbitrary dimensions. To mitigate the negative transfer problem,\nGraphBridge merges the source model with a concurrently trained model, thereby\nreducing the source bias when applied to the target domain. Our method is\nthoroughly evaluated across diverse transfer learning scenarios, including\nGraph2Graph, Node2Node, Graph2Node, and graph2point-cloud. Empirical\nvalidation, conducted over 16 datasets representative of these scenarios,\nconfirms the framework's capacity for task- and domain-agnostic transfer\nlearning within graph-like data, marking a significant advancement in the field\nof GNNs. Code is available at https://github.com/jujulili888/GraphBridge.",
    "published_date": "2025-02-26T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.19252v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.19249v1",
    "title": "Between Circuits and Chomsky: Pre-pretraining on Formal Languages Imparts Linguistic Biases",
    "authors": [
      "Michael Y. Hu",
      "Jackson Petty",
      "Chuan Shi",
      "William Merrill",
      "Tal Linzen"
    ],
    "author_ids": [],
    "abstract": "Pretraining language models on formal languages can improve their acquisition\nof natural language, but it is unclear which features of the formal language\nimpart an inductive bias that leads to effective transfer. Drawing on insights\nfrom linguistics and complexity theory, we hypothesize that effective transfer\noccurs when the formal language both captures dependency structures in natural\nlanguage and remains within the computational limitations of the model\narchitecture. Focusing on transformers, we find that formal languages with both\nthese properties enable language models to achieve lower loss on natural\nlanguage and better linguistic generalization compared to other languages. In\nfact, pre-pretraining, or training on formal-then-natural language, reduces\nloss more efficiently than the same amount of natural language. For a\n1B-parameter language model trained on roughly 1.6B tokens of natural language,\npre-pretraining achieves the same loss and better linguistic generalization\nwith a 33% smaller token budget. We also give mechanistic evidence of\ncross-task transfer from formal to natural language: attention heads acquired\nduring formal language pretraining remain crucial for the model's performance\non syntactic evaluations.",
    "published_date": "2025-02-26T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.19249v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.05781v1",
    "title": "Where is my Glass Slipper? AI, Poetry and Art",
    "authors": [
      "Anastasios P. Pagiaslis"
    ],
    "author_ids": [],
    "abstract": "This literature review interrogates the intersections between artificial\nintelligence, poetry, and art, offering a comprehensive exploration of both\nhistorical evolution and current debates in digital creative practices. It\ntraces the development of computer-generated poetry from early template-based\nsystems to generative models, critically assessing evaluative frameworks such\nas adaptations of the Turing Test, the FACE model, and ProFTAP. It also\nexamines how these frameworks endeavour to measure creativity, semantic\ncoherence, and cultural relevance in AI-generated texts, whilst highlighting\nthe persistent challenges in replicating the nuance of human poetic expression.\n  The review contributes a Marketing Theory discussion that deconstructs the\nfigurative marketing narratives employed by AI companies, which utilise\nsanitised language and anthropomorphic metaphors to humanise their\ntechnologies. This discussion reveals the reductive nature of such narratives\nand underscores the tension between algorithmic precision and the realities of\nhuman creativity.The review also incorporates an auto-ethnographic account that\noffers a self-reflexive commentary on its own composition. By acknowledging the\nuse of AI in crafting this review, the auto-ethnographic account destabilises\nconventional notions of authorship and objectivity, resonating with\ndeconstruction and challenging logocentric assumptions in academic discourse.\n  Ultimately, the review calls for a re-evaluation of creative processes that\nrecognises the interdependence of technological innovation and human\nsubjectivity. It advocates for interdisciplinary dialogue addressing ethical,\ncultural, and philosophical concerns, while reimagining the boundaries of\nartistic production.",
    "published_date": "2025-02-26T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.CL",
      "I.2.7; J.5"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.05781v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.05323v1",
    "title": "Multi-Perspective Attention Mechanism for Bias-Aware Sequential Recommendation",
    "authors": [
      "Mingjian Fu",
      "Hengsheng Chen",
      "Dongchun Jiang",
      "Yanchao Tan"
    ],
    "author_ids": [],
    "abstract": "In the era of advancing information technology, recommender systems have\nemerged as crucial tools for dealing with information overload. However,\ntraditional recommender systems still have limitations in capturing the dynamic\nevolution of user behavior. To better understand and predict user behavior,\nespecially taking into account the complexity of temporal evolution, sequential\nrecommender systems have gradually become the focus of research. Currently,\nmany sequential recommendation algorithms ignore the amplification effects of\nprevalent biases, which leads to recommendation results being susceptible to\nthe Matthew Effect. Additionally, it will impose limitations on the recommender\nsystem's ability to deeply perceive and capture the dynamic shifts in user\npreferences, thereby diminishing the extent of its recommendation reach. To\naddress this issue effectively, we propose a recommendation system based on\nsequential information and attention mechanism called Multi-Perspective\nAttention Bias Sequential Recommendation (MABSRec). Firstly, we reconstruct\nuser sequences into three short types and utilize graph neural networks for\nitem weighting. Subsequently, an adaptive multi-bias perspective attention\nmodule is proposed to enhance the accuracy of recommendations. Experimental\nresults show that the MABSRec model exhibits significant advantages in all\nevaluation metrics, demonstrating its excellent performance in the sequence\nrecommendation task.",
    "published_date": "2025-02-26T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "I.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.05323v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.19160v1",
    "title": "Detecting Linguistic Indicators for Stereotype Assessment with Large Language Models",
    "authors": [
      "Rebekka Görge",
      "Michael Mock",
      "Héctor Allende-Cid"
    ],
    "author_ids": [],
    "abstract": "Social categories and stereotypes are embedded in language and can introduce\ndata bias into Large Language Models (LLMs). Despite safeguards, these biases\noften persist in model behavior, potentially leading to representational harm\nin outputs. While sociolinguistic research provides valuable insights into the\nformation of stereotypes, NLP approaches for stereotype detection rarely draw\non this foundation and often lack objectivity, precision, and interpretability.\nTo fill this gap, in this work we propose a new approach that detects and\nquantifies the linguistic indicators of stereotypes in a sentence. We derive\nlinguistic indicators from the Social Category and Stereotype Communication\n(SCSC) framework which indicate strong social category formulation and\nstereotyping in language, and use them to build a categorization scheme. To\nautomate this approach, we instruct different LLMs using in-context learning to\napply the approach to a sentence, where the LLM examines the linguistic\nproperties and provides a basis for a fine-grained assessment. Based on an\nempirical evaluation of the importance of different linguistic indicators, we\nlearn a scoring function that measures the linguistic indicators of a\nstereotype. Our annotations of stereotyped sentences show that these indicators\nare present in these sentences and explain the strength of a stereotype. In\nterms of model performance, our results show that the models generally perform\nwell in detecting and classifying linguistic indicators of category labels used\nto denote a category, but sometimes struggle to correctly evaluate the\nassociated behaviors and characteristics. Using more few-shot examples within\nthe prompts, significantly improves performance. Model performance increases\nwith size, as Llama-3.3-70B-Instruct and GPT-4 achieve comparable results that\nsurpass those of Mixtral-8x7B-Instruct, GPT-4-mini and Llama-3.1-8B-Instruct.",
    "published_date": "2025-02-26T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.19160v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.19158v1",
    "title": "When Personalization Meets Reality: A Multi-Faceted Analysis of Personalized Preference Learning",
    "authors": [
      "Yijiang River Dong",
      "Tiancheng Hu",
      "Yinhong Liu",
      "Ahmet Üstün",
      "Nigel Collier"
    ],
    "author_ids": [],
    "abstract": "While Reinforcement Learning from Human Feedback (RLHF) is widely used to\nalign Large Language Models (LLMs) with human preferences, it typically assumes\nhomogeneous preferences across users, overlooking diverse human values and\nminority viewpoints. Although personalized preference learning addresses this\nby tailoring separate preferences for individual users, the field lacks\nstandardized methods to assess its effectiveness. We present a multi-faceted\nevaluation framework that measures not only performance but also fairness,\nunintended effects, and adaptability across varying levels of preference\ndivergence. Through extensive experiments comparing eight personalization\nmethods across three preference datasets, we demonstrate that performance\ndifferences between methods could reach 36% when users strongly disagree, and\npersonalization can introduce up to 20% safety misalignment. These findings\nhighlight the critical need for holistic evaluation approaches to advance the\ndevelopment of more effective and inclusive preference learning systems.",
    "published_date": "2025-02-26T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.19158v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.19104v2",
    "title": "Are All Spanish Doctors Male? Evaluating Gender Bias in German Machine Translation",
    "authors": [
      "Michelle Kappl"
    ],
    "author_ids": [],
    "abstract": "We present WinoMTDE, a new gender bias evaluation test set designed to assess\noccupational stereotyping and underrepresentation in German machine translation\n(MT) systems. Building on the automatic evaluation method introduced by\narXiv:1906.00591v1, we extend the approach to German, a language with\ngrammatical gender. The WinoMTDE dataset comprises 288 German sentences that\nare balanced in regard to gender, as well as stereotype, which was annotated\nusing German labor statistics. We conduct a large-scale evaluation of five\nwidely used MT systems and a large language model. Our results reveal\npersistent bias in most models, with the LLM outperforming traditional systems.\nThe dataset and evaluation code are publicly available under\nhttps://github.com/michellekappl/mt_gender_german.",
    "published_date": "2025-02-26T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.19104v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.19050v1",
    "title": "On the Efficiency of Fair and Truthful Trade Mechanisms",
    "authors": [
      "Moshe Babaioff",
      "Yiding Feng",
      "Noam Manaker Morag"
    ],
    "author_ids": [],
    "abstract": "We consider the impact of fairness requirements on the social efficiency of\ntruthful mechanisms for trade, focusing on Bayesian bilateral-trade settings.\nUnlike the full information case in which all gains-from-trade can be realized\nand equally split between the two parties, in the private information setting,\nequitability has devastating welfare implications (even if only required to\nhold ex-ante). We thus search for an alternative fairness notion and suggest\nrequiring the mechanism to be KS-fair: it must ex-ante equalize the fraction of\nthe ideal utilities of the two traders. We show that there is always a KS-fair\n(simple) truthful mechanism with expected gains-from-trade that are half the\noptimum, but always ensuring any better fraction is impossible (even when the\nseller value is zero). We then restrict our attention to trade settings with a\nzero-value seller and a buyer with value distribution that is Regular or MHR,\nproving that much better fractions can be obtained under these conditions.",
    "published_date": "2025-02-26T00:00:00",
    "year": 2025,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.19050v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.18774v1",
    "title": "Measuring risks inherent to our digital economies using Amazon purchase histories from US consumers",
    "authors": [
      "Alex Berke",
      "Kent Larson",
      "Sandy Pentland",
      "Dana Calacci"
    ],
    "author_ids": [],
    "abstract": "What do pickles and trampolines have in common? In this paper we show that\nwhile purchases for these products may seem innocuous, they risk revealing\nclues about customers' personal attributes - in this case, their race.\n  As online retail and digital purchases become increasingly common, consumer\ndata has become increasingly valuable, raising the risks of privacy violations\nand online discrimination. This work provides the first open analysis measuring\nthese risks, using purchase histories crowdsourced from (N=4248) US Amazon.com\ncustomers and survey data on their personal attributes. With this limited\nsample and simple models, we demonstrate how easily consumers' personal\nattributes, such as health and lifestyle information, gender, age, and race,\ncan be inferred from purchases. For example, our models achieve AUC values over\n0.9 for predicting gender and over 0.8 for predicting diabetes status. To\nbetter understand the risks that highly resourced firms like Amazon, data\nbrokers, and advertisers present to consumers, we measure how our models'\npredictive power scales with more data. Finally, we measure and highlight how\ndifferent product categories contribute to inference risk in order to make our\nfindings more interpretable and actionable for future researchers and privacy\nadvocates.",
    "published_date": "2025-02-26T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.18774v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2504.07100v1",
    "title": "EnDive: A Cross-Dialect Benchmark for Fairness and Performance in Large Language Models",
    "authors": [
      "Abhay Gupta",
      "Jacob Cheung",
      "Philip Meng",
      "Shayan Sayyed",
      "Austen Liao",
      "Kevin Zhu",
      "Sean O'Brien"
    ],
    "author_ids": [],
    "abstract": "The diversity of human language, shaped by social, cultural, and regional\ninfluences, presents significant challenges for natural language processing\n(NLP) systems. Existing benchmarks often overlook intra-language variations,\nleaving speakers of non-standard dialects underserved. To address this gap, we\nintroduce EnDive (English Diversity), a benchmark that evaluates five\nwidely-used large language models (LLMs) across tasks in language\nunderstanding, algorithmic reasoning, mathematics, and logic. Our framework\ntranslates Standard American English datasets into five underrepresented\ndialects using few-shot prompting with verified examples from native speakers,\nand compare these translations against rule-based methods via fluency\nassessments, preference tests, and semantic similarity metrics. Human\nevaluations confirm high translation quality, with average scores of at least\n6.02/7 for faithfulness, fluency, and formality. By filtering out\nnear-identical translations, we create a challenging dataset that reveals\nsignificant performance disparities - models consistently underperform on\ndialectal inputs compared to Standard American English. EnDive thus advances\ndialect-aware NLP by uncovering model biases and promoting more equitable\nlanguage technologies.",
    "published_date": "2025-02-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "I.2.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.07100v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.01872v1",
    "title": "FairGen: Controlling Sensitive Attributes for Fair Generations in Diffusion Models via Adaptive Latent Guidance",
    "authors": [
      "Mintong Kang",
      "Vinayshekhar Bannihatti Kumar",
      "Shamik Roy",
      "Abhishek Kumar",
      "Sopan Khosla",
      "Balakrishnan Murali Narayanaswamy",
      "Rashmi Gangadharaiah"
    ],
    "author_ids": [],
    "abstract": "Text-to-image diffusion models often exhibit biases toward specific\ndemographic groups, such as generating more males than females when prompted to\ngenerate images of engineers, raising ethical concerns and limiting their\nadoption. In this paper, we tackle the challenge of mitigating generation bias\ntowards any target attribute value (e.g., \"male\" for \"gender\") in diffusion\nmodels while preserving generation quality. We propose FairGen, an adaptive\nlatent guidance mechanism which controls the generation distribution during\ninference. In FairGen, a latent guidance module dynamically adjusts the\ndiffusion process to enforce specific attributes, while a memory module tracks\nthe generation statistics and steers latent guidance to align with the targeted\nfair distribution of the attribute values. Further, given the limitations of\nexisting datasets in comprehensively assessing bias in diffusion models, we\nintroduce a holistic bias evaluation benchmark HBE, covering diverse domains\nand incorporating complex prompts across various applications. Extensive\nevaluations on HBE and Stable Bias datasets demonstrate that FairGen\noutperforms existing bias mitigation approaches, achieving substantial bias\nreduction (e.g., 68.5% gender bias reduction on Stable Diffusion 2). Ablation\nstudies highlight FairGen's ability to flexibly and precisely control\ngeneration distribution at any user-specified granularity, ensuring adaptive\nand targeted bias mitigation.",
    "published_date": "2025-02-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.01872v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.05774v1",
    "title": "GeoJEPA: Towards Eliminating Augmentation- and Sampling Bias in Multimodal Geospatial Learning",
    "authors": [
      "Theodor Lundqvist",
      "Ludvig Delvret"
    ],
    "author_ids": [],
    "abstract": "Existing methods for self-supervised representation learning of geospatial\nregions and map entities rely extensively on the design of pretext tasks, often\ninvolving augmentations or heuristic sampling of positive and negative pairs\nbased on spatial proximity. This reliance introduces biases and limits the\nrepresentations' expressiveness and generalisability. Consequently, the\nliterature has expressed a pressing need to explore different methods for\nmodelling geospatial data. To address the key difficulties of such methods,\nnamely multimodality, heterogeneity, and the choice of pretext tasks, we\npresent GeoJEPA, a versatile multimodal fusion model for geospatial data built\non the self-supervised Joint-Embedding Predictive Architecture. With GeoJEPA,\nwe aim to eliminate the widely accepted augmentation- and sampling biases found\nin self-supervised geospatial representation learning. GeoJEPA uses\nself-supervised pretraining on a large dataset of OpenStreetMap attributes,\ngeometries and aerial images. The results are multimodal semantic\nrepresentations of urban regions and map entities that we evaluate both\nquantitatively and qualitatively. Through this work, we uncover several key\ninsights into JEPA's ability to handle multimodal data.",
    "published_date": "2025-02-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.DB",
      "68T05 (Primary) 62H30, 68U10 (Secondary)",
      "I.2.6; I.5.1; H.2.8; I.4.9"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.05774v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.18605v2",
    "title": "Expected Variational Inequalities",
    "authors": [
      "Brian Hu Zhang",
      "Ioannis Anagnostides",
      "Emanuel Tewolde",
      "Ratip Emin Berker",
      "Gabriele Farina",
      "Vincent Conitzer",
      "Tuomas Sandholm"
    ],
    "author_ids": [],
    "abstract": "Variational inequalities (VIs) encompass many fundamental problems in diverse\nareas ranging from engineering to economics and machine learning. However,\ntheir considerable expressivity comes at the cost of computational\nintractability. In this paper, we introduce and analyze a natural relaxation --\nwhich we refer to as expected variational inequalities (EVIs) -- where the goal\nis to find a distribution that satisfies the VI constraint in expectation. By\nadapting recent techniques from game theory, we show that, unlike VIs, EVIs can\nbe solved in polynomial time under general (nonmonotone) operators. EVIs\ncapture the seminal notion of correlated equilibria, but enjoy a greater reach\nbeyond games. We also employ our framework to capture and generalize several\nexisting disparate results, including from settings such as smooth games, and\ngames with coupled constraints or nonconcave utilities.",
    "published_date": "2025-02-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.GT",
      "cs.LG",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.18605v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.18576v1",
    "title": "Investigating Youth AI Auditing",
    "authors": [
      "Jaemarie Solyst",
      "Cindy Peng",
      "Wesley Hanwen Deng",
      "Praneetha Pratapa",
      "Jessica Hammer",
      "Amy Ogan",
      "Jason Hong",
      "Motahhare Eslami"
    ],
    "author_ids": [],
    "abstract": "Youth are active users and stakeholders of artificial intelligence (AI), yet\nthey are often not included in responsible AI (RAI) practices. Emerging efforts\nin RAI largely focus on adult populations, missing an opportunity to get unique\nperspectives of youth. This study explores the potential of youth (teens under\nthe age of 18) to engage meaningfully in RAI, specifically through AI auditing.\nIn a workshop study with 17 teens, we investigated how youth can actively\nidentify problematic behaviors in youth-relevant ubiquitous AI (text-to-image\ngenerative AI, autocompletion in search bar, image search) and the impacts of\nsupporting AI auditing with critical AI literacy scaffolding with guided\ndiscussion about AI ethics and an auditing tool. We found that youth can\ncontribute quality insights, shaped by their expertise (e.g., hobbies and\npassions), lived experiences (e.g., social identities), and age-related\nknowledge (e.g., understanding of fast-moving trends). We discuss how\nempowering youth in AI auditing can result in more responsible AI, support\ntheir learning through doing, and lead to implications for including youth in\nvarious participatory RAI processes.",
    "published_date": "2025-02-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.18576v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.05773v1",
    "title": "Between Innovation and Oversight: A Cross-Regional Study of AI Risk Management Frameworks in the EU, U.S., UK, and China",
    "authors": [
      "Amir Al-Maamari"
    ],
    "author_ids": [],
    "abstract": "As artificial intelligence (AI) technologies increasingly enter important\nsectors like healthcare, transportation, and finance, the development of\neffective governance frameworks is crucial for dealing with ethical, security,\nand societal risks. This paper conducts a comparative analysis of AI risk\nmanagement strategies across the European Union (EU), United States (U.S.),\nUnited Kingdom (UK), and China. A multi-method qualitative approach, including\ncomparative policy analysis, thematic analysis, and case studies, investigates\nhow these regions classify AI risks, implement compliance measures, structure\noversight, prioritize transparency, and respond to emerging innovations.\nExamples from high-risk contexts like healthcare diagnostics, autonomous\nvehicles, fintech, and facial recognition demonstrate the advantages and\nlimitations of different regulatory models. The findings show that the EU\nimplements a structured, risk-based framework that prioritizes transparency and\nconformity assessments, while the U.S. uses decentralized, sector-specific\nregulations that promote innovation but may lead to fragmented enforcement. The\nflexible, sector-specific strategy of the UK facilitates agile responses but\nmay lead to inconsistent coverage across domains. China's centralized\ndirectives allow rapid large-scale implementation while constraining public\ntransparency and external oversight. These insights show the necessity for AI\nregulation that is globally informed yet context-sensitive, aiming to balance\neffective risk management with technological progress. The paper concludes with\npolicy recommendations and suggestions for future research aimed at enhancing\neffective, adaptive, and inclusive AI governance globally.",
    "published_date": "2025-02-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.05773v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.18434v1",
    "title": "Exploring Gender Disparities in Automatic Speech Recognition Technology",
    "authors": [
      "Hend ElGhazaly",
      "Bahman Mirheidari",
      "Nafise Sadat Moosavi",
      "Heidi Christensen"
    ],
    "author_ids": [],
    "abstract": "This study investigates factors influencing Automatic Speech Recognition\n(ASR) systems' fairness and performance across genders, beyond the conventional\nexamination of demographics. Using the LibriSpeech dataset and the Whisper\nsmall model, we analyze how performance varies across different gender\nrepresentations in training data. Our findings suggest a complex interplay\nbetween the gender ratio in training data and ASR performance. Optimal fairness\noccurs at specific gender distributions rather than a simple 50-50 split.\nFurthermore, our findings suggest that factors like pitch variability can\nsignificantly affect ASR accuracy. This research contributes to a deeper\nunderstanding of biases in ASR systems, highlighting the importance of\ncarefully curated training data in mitigating gender bias.",
    "published_date": "2025-02-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.18434v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.18395v1",
    "title": "\"Why do we do this?\": Moral Stress and the Affective Experience of Ethics in Practice",
    "authors": [
      "Sonja Rattay",
      "Ville Vakkuri",
      "Marco Rozendaal",
      "Irina Shklovski"
    ],
    "author_ids": [],
    "abstract": "A plethora of toolkits, checklists, and workshops have been developed to\nbridge the well-documented gap between AI ethics principles and practice. Yet\nlittle is known about effects of such interventions on practitioners. We\nconducted an ethnographic investigation in a major European city organization\nthat developed and works to integrate an ethics toolkit into city operations.\nWe find that the integration of ethics tools by technical teams destabilises\ntheir boundaries, roles, and mandates around responsibilities and decisions.\nThis lead to emotional discomfort and feelings of vulnerability, which neither\ntoolkit designers nor the organization had accounted for. We leverage the\nconcept of moral stress to argue that this affective experience is a core\nchallenge to the successful integration of ethics tools in technical practice.\nEven in this best case scenario, organisational structures were not able to\ndeal with moral stress that resulted from attempts to implement responsible\ntechnology development practices.",
    "published_date": "2025-02-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.18395v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.18342v1",
    "title": "BRIDO: Bringing Democratic Order to Abstractive Summarization",
    "authors": [
      "Junhyun Lee",
      "Harshith Goka",
      "Hyeonmok Ko"
    ],
    "author_ids": [],
    "abstract": "Hallucination refers to the inaccurate, irrelevant, and inconsistent text\ngenerated from large language models (LLMs). While the LLMs have shown great\npromise in a variety of tasks, the issue of hallucination still remains a major\nchallenge for many practical uses. In this paper, we tackle the issue of\nhallucination in abstract text summarization by mitigating exposure bias.\nExisting models targeted for exposure bias mitigation, namely BRIO, aim for\nbetter summarization quality in the ROUGE score. We propose a model that uses a\nsimilar exposure bias mitigation strategy but with a goal that is aligned with\nless hallucination. We conjecture that among a group of candidate outputs, ones\nwith hallucinations will comprise the minority of the whole group. That is,\ncandidates with less similarity with others will have a higher chance of\ncontaining hallucinated content. Our method uses this aspect and utilizes\ncontrastive learning, incentivizing candidates with high inter-candidate ROUGE\nscores. We performed experiments on the XSum and CNN/DM summarization datasets,\nand our method showed 6.25% and 3.82% improvement, respectively, on the\nconsistency G-Eval score over BRIO.",
    "published_date": "2025-02-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.18342v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.18282v2",
    "title": "Better Aligned with Survey Respondents or Training Data? Unveiling Political Leanings of LLMs on U.S. Supreme Court Cases",
    "authors": [
      "Shanshan Xu",
      "T. Y. S. S Santosh",
      "Yanai Elazar",
      "Quirin Vogel",
      "Barbara Plank",
      "Matthias Grabmair"
    ],
    "author_ids": [],
    "abstract": "The increased adoption of Large Language Models (LLMs) and their potential to\nshape public opinion have sparked interest in assessing these models' political\nleanings. Building on previous research that compared LLMs and human opinions\nand observed political bias in system responses, we take a step further to\ninvestigate the underlying causes of such biases by empirically examining how\nthe values and biases embedded in training corpora shape model outputs.\nSpecifically, we propose a method to quantitatively evaluate political leanings\nembedded in the large pretraining corpora. Subsequently we investigate to whom\nare the LLMs' political leanings more aligned with, their pretrainig corpora or\nthe surveyed human opinions. As a case study, we focus on probing the political\nleanings of LLMs in 32 U.S. Supreme Court cases, addressing contentious topics\nsuch as abortion and voting rights. Our findings reveal that LLMs strongly\nreflect the political leanings in their training data, and no strong\ncorrelation is observed with their alignment to human opinions as expressed in\nsurveys. These results underscore the importance of responsible curation of\ntraining data and the need for robust evaluation metrics to ensure LLMs'\nalignment with human-centered values.",
    "published_date": "2025-02-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.18282v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.18205v1",
    "title": "Grandes modelos de lenguaje: de la predicción de palabras a la comprensión?",
    "authors": [
      "Carlos Gómez-Rodríguez"
    ],
    "author_ids": [],
    "abstract": "Large language models, such as the well-known ChatGPT, have brought about an\nunexpected revolution in the field of artificial intelligence. On the one hand,\nthey have numerous practical applications and enormous potential still to be\nexplored. On the other hand, they are also the subject of debate from\nscientific, philosophical, and social perspectives: there are doubts about the\nexact mechanisms of their functioning and their actual capacity for language\ncomprehension, and their applications raise ethical dilemmas. In this chapter,\nwe describe how this technology has been developed and the fundamentals of its\noperation, allowing us to better understand its capabilities and limitations\nand to introduce some of the main debates surrounding its development and use.\n  --\n  Los grandes modelos de lenguaje, como el conocido ChatGPT, han supuesto una\ninesperada revoluci\\'on en el \\'ambito de la inteligencia artificial. Por un\nlado, cuentan con multitud de aplicaciones pr\\'acticas y un enorme potencial\ntodav\\'ia por explorar. Por otro lado, son tambi\\'en objeto de debate, tanto\ndesde el punto de vista cient\\'ifico y filos\\'ofico como social: hay dudas\nsobre los mecanismos exactos de su funcionamiento y su capacidad real de\ncomprensi\\'on del lenguaje, y sus aplicaciones plantean dilemas \\'eticos. En\neste cap\\'itulo describimos c\\'omo se ha llegado a esta tecnolog\\'ia y los\nfundamentos de su funcionamiento, permiti\\'endonos as\\'i comprender mejor sus\ncapacidades y limitaciones e introducir algunos de los principales debates que\nrodean su desarrollo y uso.",
    "published_date": "2025-02-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.CY",
      "68T50",
      "I.2.7; K.4.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.18205v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.18060v1",
    "title": "Defining bias in AI-systems: Biased models are fair models",
    "authors": [
      "Chiara Lindloff",
      "Ingo Siegert"
    ],
    "author_ids": [],
    "abstract": "The debate around bias in AI systems is central to discussions on algorithmic\nfairness. However, the term bias often lacks a clear definition, despite\nfrequently being contrasted with fairness, implying that an unbiased model is\ninherently fair. In this paper, we challenge this assumption and argue that a\nprecise conceptualization of bias is necessary to effectively address fairness\nconcerns. Rather than viewing bias as inherently negative or unfair, we\nhighlight the importance of distinguishing between bias and discrimination. We\nfurther explore how this shift in focus can foster a more constructive\ndiscourse within academic debates on fairness in AI systems.",
    "published_date": "2025-02-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.18060v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.05769v1",
    "title": "Effect of Gender Fair Job Description on Generative AI Images",
    "authors": [
      "Finn Böckling",
      "Jan Marquenie",
      "Ingo Siegert"
    ],
    "author_ids": [],
    "abstract": "STEM fields are traditionally male-dominated, with gender biases shaping\nperceptions of job accessibility. This study analyzed gender representation in\nSTEM occupation images generated by OpenAI DALL-E 3 \\& Black Forest FLUX.1\nusing 150 prompts in three linguistic forms: German generic masculine, German\npair form, and English. As control, 20 pictures of social occupations were\ngenerated as well. Results revealed significant male bias across all forms,\nwith the German pair form showing reduced bias but still overrepresenting men\nfor the STEM-Group and mixed results for the Group of Social Occupations. These\nfindings highlight generative AI's role in reinforcing societal biases,\nemphasizing the need for further discussion on diversity (in AI). Further\naspects analyzed are age-distribution and ethnic diversity.",
    "published_date": "2025-02-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.05769v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.05322v1",
    "title": "Balancing Benefits and Risks: RL Approaches for Addiction-Aware Social Media Recommenders",
    "authors": [
      "Luca Bolis",
      "Stefano Livella",
      "Sabrina Patania",
      "Dimitri Ognibene",
      "Matteo Papini",
      "Kenji Morita"
    ],
    "author_ids": [],
    "abstract": "Social media platforms provide valuable opportunities for users to gather\ninformation, interact with friends, and enjoy entertainment. However, their\naddictive potential poses significant challenges, including overuse and\nnegative psycho-logical or behavioral impacts [4, 2, 8]. This study explores\nstrategies to mitigate compulsive social media usage while preserving its\nbenefits and ensuring economic sustainability, focusing on recommenders that\npromote balanced usage.\n  We analyze user behaviors arising from intrinsic diversities and\nenvironmental interactions, offering insights for next-generation social media\nrecommenders that prioritize well-being. Specifically, we examine the temporal\npredictability of overuse and addiction using measures available to\nrecommenders, aiming to inform mechanisms that prevent addiction while avoiding\nuser disengagement [7].\n  Building on RL-based computational frameworks for addiction modelling [6],\nour study introduces: - A recommender system adapting to user preferences,\nintroducing non-stationary and non-Markovian dynamics.\n  - Differentiated state representations for users and recommenders to capture\nnuanced interactions.\n  - Distinct usage conditions-light and heavy use-addressing RL's limitations\nin distinguishing prolonged from healthy engagement.\n  - Complexity in overuse impacts, highlighting their role in user adaptation\n[7].\n  Simulations demonstrate how model-based (MB) and model-free (MF)\ndecision-making interact with environmental dynamics to influence user behavior\nand addiction. Results reveal the significant role of recommender systems in\nshaping addiction tendencies or fostering healthier engagement. These findings\nsupport ethical, adaptive recommender design, advancing sustainable social\nmedia ecosystems [9, 1].\n  Keywords: multi-agent systems, recommender systems, addiction, social media",
    "published_date": "2025-02-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IR",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.05322v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.17945v1",
    "title": "Assessing Large Language Models in Agentic Multilingual National Bias",
    "authors": [
      "Qianying Liu",
      "Katrina Qiyao Wang",
      "Fei Cheng",
      "Sadao Kurohashi"
    ],
    "author_ids": [],
    "abstract": "Large Language Models have garnered significant attention for their\ncapabilities in multilingual natural language processing, while studies on\nrisks associated with cross biases are limited to immediate context\npreferences. Cross-language disparities in reasoning-based recommendations\nremain largely unexplored, with a lack of even descriptive analysis. This study\nis the first to address this gap. We test LLM's applicability and capability in\nproviding personalized advice across three key scenarios: university\napplications, travel, and relocation. We investigate multilingual bias in\nstate-of-the-art LLMs by analyzing their responses to decision-making tasks\nacross multiple languages. We quantify bias in model-generated scores and\nassess the impact of demographic factors and reasoning strategies (e.g.,\nChain-of-Thought prompting) on bias patterns. Our findings reveal that local\nlanguage bias is prevalent across different tasks, with GPT-4 and Sonnet\nreducing bias for English-speaking countries compared to GPT-3.5 but failing to\nachieve robust multilingual alignment, highlighting broader implications for\nmultilingual AI agents and applications such as education.",
    "published_date": "2025-02-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.17945v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.17921v1",
    "title": "Unmasking Gender Bias in Recommendation Systems and Enhancing Category-Aware Fairness",
    "authors": [
      "Tahsin Alamgir Kheya",
      "Mohamed Reda Bouadjenek",
      "Sunil Aryal"
    ],
    "author_ids": [],
    "abstract": "Recommendation systems are now an integral part of our daily lives. We rely\non them for tasks such as discovering new movies, finding friends on social\nmedia, and connecting job seekers with relevant opportunities. Given their\nvital role, we must ensure these recommendations are free from societal\nstereotypes. Therefore, evaluating and addressing such biases in recommendation\nsystems is crucial. Previous work evaluating the fairness of recommended items\nfails to capture certain nuances as they mainly focus on comparing performance\nmetrics for different sensitive groups. In this paper, we introduce a set of\ncomprehensive metrics for quantifying gender bias in recommendations.\nSpecifically, we show the importance of evaluating fairness on a more granular\nlevel, which can be achieved using our metrics to capture gender bias using\ncategories of recommended items like genres for movies. Furthermore, we show\nthat employing a category-aware fairness metric as a regularization term along\nwith the main recommendation loss during training can help effectively minimize\nbias in the models' output. We experiment on three real-world datasets, using\nfive baseline models alongside two popular fairness-aware models, to show the\neffectiveness of our metrics in evaluating gender bias. Our metrics help\nprovide an enhanced insight into bias in recommended items compared to previous\nmetrics. Additionally, our results demonstrate how incorporating our\nregularization term significantly improves the fairness in recommendations for\ndifferent categories without substantial degradation in overall recommendation\nperformance.",
    "published_date": "2025-02-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.17921v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.18534v1",
    "title": "MAFE: Multi-Agent Fair Environments for Decision-Making Systems",
    "authors": [
      "Zachary McBride Lazri",
      "Anirudh Nakra",
      "Ivan Brugere",
      "Danial Dervovic",
      "Antigoni Polychroniadou",
      "Furong Huang",
      "Dana Dachman-Soled",
      "Min Wu"
    ],
    "author_ids": [],
    "abstract": "Fairness constraints applied to machine learning (ML) models in static\ncontexts have been shown to potentially produce adverse outcomes among\ndemographic groups over time. To address this issue, emerging research focuses\non creating fair solutions that persist over time. While many approaches treat\nthis as a single-agent decision-making problem, real-world systems often\nconsist of multiple interacting entities that influence outcomes. Explicitly\nmodeling these entities as agents enables more flexible analysis of their\ninterventions and the effects they have on a system's underlying dynamics. A\nsignificant challenge in conducting research on multi-agent systems is the lack\nof realistic environments that leverage the limited real-world data available\nfor analysis. To address this gap, we introduce the concept of a Multi-Agent\nFair Environment (MAFE) and present and analyze three MAFEs that model distinct\nsocial systems. Experimental results demonstrate the utility of our MAFEs as\ntestbeds for developing multi-agent fair algorithms.",
    "published_date": "2025-02-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.18534v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.17817v1",
    "title": "Predicting Through Generation: Why Generation Is Better for Prediction",
    "authors": [
      "Md Kowsher",
      "Nusrat Jahan Prottasha",
      "Prakash Bhat",
      "Chun-Nam Yu",
      "Mojtaba Soltanalian",
      "Ivan Garibay",
      "Ozlem Garibay",
      "Chen Chen",
      "Niloofar Yousefi"
    ],
    "author_ids": [],
    "abstract": "This paper argues that generating output tokens is more effective than using\npooled representations for prediction tasks because token-level generation\nretains more mutual information. Since LLMs are trained on massive text corpora\nusing next-token prediction, generation aligns naturally with their learned\nbehavior. Using the Data Processing Inequality (DPI), we provide both\ntheoretical and empirical evidence supporting this claim. However,\nautoregressive models face two key challenges when used for prediction: (1)\nexposure bias, where the model sees ground truth tokens during training but\nrelies on its own predictions during inference, leading to errors, and (2)\nformat mismatch, where discrete tokens do not always align with the tasks\nrequired output structure. To address these challenges, we introduce\nPredGen(Predicting Through Generating), an end to end framework that (i) uses\nscheduled sampling to reduce exposure bias, and (ii) introduces a task adapter\nto convert the generated tokens into structured outputs. Additionally, we\nintroduce Writer-Director Alignment Loss (WDAL), which ensures consistency\nbetween token generation and final task predictions, improving both text\ncoherence and numerical accuracy. We evaluate PredGen on multiple\nclassification and regression benchmarks. Our results show that PredGen\nconsistently outperforms standard baselines, demonstrating its effectiveness in\nstructured prediction tasks.",
    "published_date": "2025-02-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.17817v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.17814v1",
    "title": "An Overview of Large Language Models for Statisticians",
    "authors": [
      "Wenlong Ji",
      "Weizhe Yuan",
      "Emily Getzen",
      "Kyunghyun Cho",
      "Michael I. Jordan",
      "Song Mei",
      "Jason E Weston",
      "Weijie J. Su",
      "Jing Xu",
      "Linjun Zhang"
    ],
    "author_ids": [],
    "abstract": "Large Language Models (LLMs) have emerged as transformative tools in\nartificial intelligence (AI), exhibiting remarkable capabilities across diverse\ntasks such as text generation, reasoning, and decision-making. While their\nsuccess has primarily been driven by advances in computational power and deep\nlearning architectures, emerging problems -- in areas such as uncertainty\nquantification, decision-making, causal inference, and distribution shift --\nrequire a deeper engagement with the field of statistics. This paper explores\npotential areas where statisticians can make important contributions to the\ndevelopment of LLMs, particularly those that aim to engender trustworthiness\nand transparency for human users. Thus, we focus on issues such as uncertainty\nquantification, interpretability, fairness, privacy, watermarking and model\nadaptation. We also consider possible roles for LLMs in statistical analysis.\nBy bridging AI and statistics, we aim to foster a deeper collaboration that\nadvances both the theoretical foundations and practical applications of LLMs,\nultimately shaping their role in addressing complex societal challenges.",
    "published_date": "2025-02-25T00:00:00",
    "year": 2025,
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.17814v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.17776v1",
    "title": "Tip of the Tongue Query Elicitation for Simulated Evaluation",
    "authors": [
      "Yifan He",
      "To Eun Kim",
      "Fernando Diaz",
      "Jaime Arguello",
      "Bhaskar Mitra"
    ],
    "author_ids": [],
    "abstract": "Tip-of-the-tongue (TOT) search occurs when a user struggles to recall a\nspecific identifier, such as a document title. While common, existing search\nsystems often fail to effectively support TOT scenarios. Research on TOT\nretrieval is further constrained by the challenge of collecting queries, as\ncurrent approaches rely heavily on community question-answering (CQA) websites,\nleading to labor-intensive evaluation and domain bias. To overcome these\nlimitations, we introduce two methods for eliciting TOT queries - leveraging\nlarge language models (LLMs) and human participants - to facilitate simulated\nevaluations of TOT retrieval systems. Our LLM-based TOT user simulator\ngenerates synthetic TOT queries at scale, achieving high correlations with how\nCQA-based TOT queries rank TOT retrieval systems when tested in the Movie\ndomain. Additionally, these synthetic queries exhibit high linguistic\nsimilarity to CQA-derived queries. For human-elicited queries, we developed an\ninterface that uses visual stimuli to place participants in a TOT state,\nenabling the collection of natural queries. In the Movie domain, system rank\ncorrelation and linguistic similarity analyses confirm that human-elicited\nqueries are both effective and closely resemble CQA-based queries. These\napproaches reduce reliance on CQA-based data collection while expanding\ncoverage to underrepresented domains, such as Landmark and Person. LLM-elicited\nqueries for the Movie, Landmark, and Person domains have been released as test\nqueries in the TREC 2024 TOT track, with human-elicited queries scheduled for\ninclusion in the TREC 2025 TOT track. Additionally, we provide source code for\nsynthetic query generation and the human query collection interface, along with\ncurated visual stimuli used for eliciting TOT queries.",
    "published_date": "2025-02-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IR",
      "cs.CL",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.17776v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.00032v2",
    "title": "Detecting LLM-Generated Korean Text through Linguistic Feature Analysis",
    "authors": [
      "Shinwoo Park",
      "Shubin Kim",
      "Do-Kyung Kim",
      "Yo-Sub Han"
    ],
    "author_ids": [],
    "abstract": "The rapid advancement of large language models (LLMs) increases the\ndifficulty of distinguishing between human-written and LLM-generated text.\nDetecting LLM-generated text is crucial for upholding academic integrity,\npreventing plagiarism, protecting copyrights, and ensuring ethical research\npractices. Most prior studies on detecting LLM-generated text focus primarily\non English text. However, languages with distinct morphological and syntactic\ncharacteristics require specialized detection approaches. Their unique\nstructures and usage patterns can hinder the direct application of methods\nprimarily designed for English. Among such languages, we focus on Korean, which\nhas relatively flexible spacing rules, a rich morphological system, and less\nfrequent comma usage compared to English. We introduce KatFish, the first\nbenchmark dataset for detecting LLM-generated Korean text. The dataset consists\nof text written by humans and generated by four LLMs across three genres.\n  By examining spacing patterns, part-of-speech diversity, and comma usage, we\nilluminate the linguistic differences between human-written and LLM-generated\nKorean text. Building on these observations, we propose KatFishNet, a detection\nmethod specifically designed for the Korean language. KatFishNet achieves an\naverage of 19.78% higher AUROC compared to the best-performing existing\ndetection method. Our code and data are available at\nhttps://github.com/Shinwoo-Park/detecting_llm_generated_korean_text_through_linguistic_analysis.",
    "published_date": "2025-02-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.00032v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.17748v1",
    "title": "FinP: Fairness-in-Privacy in Federated Learning by Addressing Disparities in Privacy Risk",
    "authors": [
      "Tianyu Zhao",
      "Mahmoud Srewa",
      "Salma Elmalaki"
    ],
    "author_ids": [],
    "abstract": "Ensuring fairness in machine learning, particularly in human-centric\napplications, extends beyond algorithmic bias to encompass fairness in privacy,\nspecifically the equitable distribution of privacy risk. This is critical in\nfederated learning (FL), where decentralized data necessitates balanced privacy\npreservation across clients. We introduce FinP, a framework designed to achieve\nfairness in privacy by mitigating disproportionate exposure to source inference\nattacks (SIA). FinP employs a dual approach: (1) server-side adaptive\naggregation to address unfairness in client contributions in global model, and\n(2) client-side regularization to reduce client vulnerability. This\ncomprehensive strategy targets both the symptoms and root causes of privacy\nunfairness. Evaluated on the Human Activity Recognition (HAR) and CIFAR-10\ndatasets, FinP demonstrates ~20% improvement in fairness in privacy on HAR with\nminimal impact on model utility, and effectively mitigates SIA risks on\nCIFAR-10, showcasing its ability to provide fairness in privacy in FL systems\nwithout compromising performance.",
    "published_date": "2025-02-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.17748v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.17730v1",
    "title": "Gender Bias in Perception of Human Managers Extends to AI Managers",
    "authors": [
      "Hao Cui",
      "Taha Yasseri"
    ],
    "author_ids": [],
    "abstract": "As AI becomes more embedded in workplaces, it is shifting from a tool for\nefficiency to an active force in organizational decision-making. Whether due to\nanthropomorphism or intentional design choices, people often assign human-like\nqualities - including gender - to AI systems. However, how AI managers are\nperceived in comparison to human managers and how gender influences these\nperceptions remains uncertain. To investigate this, we conducted randomized\ncontrolled trials (RCTs) where teams of three participants worked together\nunder a randomly assigned manager - either human or AI - who was presented as\nmale, female, or gender-neutral. The manager's role was to select the\nbest-performing team member for an additional award. Our findings reveal that\nwhile participants initially showed no strong preference based on manager type\nor gender, their perceptions changed significantly after experiencing the award\nprocess. As expected, those who received awards rated their managers as more\nfair, competent, and trustworthy, while those who were not selected viewed them\nless favorably. However, male managers - both human and AI - were more\npositively received by awarded participants, whereas female managers,\nespecially female AI managers, faced greater skepticism and negative judgments\nwhen they denied rewards. These results suggest that gender bias in leadership\nextends beyond human managers and towards AI-driven decision-makers. As AI\ntakes on greater managerial roles, understanding and addressing these biases\nwill be crucial for designing fair and effective AI management systems.",
    "published_date": "2025-02-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.17730v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.01863v1",
    "title": "Vision Language Models in Medicine",
    "authors": [
      "Beria Chingnabe Kalpelbe",
      "Angel Gabriel Adaambiik",
      "Wei Peng"
    ],
    "author_ids": [],
    "abstract": "With the advent of Vision-Language Models (VLMs), medical artificial\nintelligence (AI) has experienced significant technological progress and\nparadigm shifts. This survey provides an extensive review of recent\nadvancements in Medical Vision-Language Models (Med-VLMs), which integrate\nvisual and textual data to enhance healthcare outcomes. We discuss the\nfoundational technology behind Med-VLMs, illustrating how general models are\nadapted for complex medical tasks, and examine their applications in\nhealthcare. The transformative impact of Med-VLMs on clinical practice,\neducation, and patient care is highlighted, alongside challenges such as data\nscarcity, narrow task generalization, interpretability issues, and ethical\nconcerns like fairness, accountability, and privacy. These limitations are\nexacerbated by uneven dataset distribution, computational demands, and\nregulatory hurdles. Rigorous evaluation methods and robust regulatory\nframeworks are essential for safe integration into healthcare workflows. Future\ndirections include leveraging large-scale, diverse datasets, improving\ncross-modal generalization, and enhancing interpretability. Innovations like\nfederated learning, lightweight architectures, and Electronic Health Record\n(EHR) integration are explored as pathways to democratize access and improve\nclinical relevance. This review aims to provide a comprehensive understanding\nof Med-VLMs' strengths and limitations, fostering their ethical and balanced\nadoption in healthcare.",
    "published_date": "2025-02-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.01863v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.17648v4",
    "title": "CalibRefine: Deep Learning-Based Online Automatic Targetless LiDAR-Camera Calibration with Iterative and Attention-Driven Post-Refinement",
    "authors": [
      "Lei Cheng",
      "Lihao Guo",
      "Tianya Zhang",
      "Tam Bang",
      "Austin Harris",
      "Mustafa Hajij",
      "Mina Sartipi",
      "Siyang Cao"
    ],
    "author_ids": [],
    "abstract": "Accurate multi-sensor calibration is essential for deploying robust\nperception systems in applications such as autonomous driving, robotics, and\nintelligent transportation. Existing LiDAR-camera calibration methods often\nrely on manually placed targets, preliminary parameter estimates, or intensive\ndata preprocessing, limiting their scalability and adaptability in real-world\nsettings. In this work, we propose a fully automatic, targetless, and online\ncalibration framework, CalibRefine, which directly processes raw LiDAR point\nclouds and camera images. Our approach is divided into four stages: (1) a\nCommon Feature Discriminator that trains on automatically detected\nobjects--using relative positions, appearance embeddings, and semantic\nclasses--to generate reliable LiDAR-camera correspondences, (2) a coarse\nhomography-based calibration, (3) an iterative refinement to incrementally\nimprove alignment as additional data frames become available, and (4) an\nattention-based refinement that addresses non-planar distortions by leveraging\na Vision Transformer and cross-attention mechanisms. Through extensive\nexperiments on two urban traffic datasets, we show that CalibRefine delivers\nhigh-precision calibration results with minimal human involvement,\noutperforming state-of-the-art targetless methods and remaining competitive\nwith, or surpassing, manually tuned baselines. Our findings highlight how\nrobust object-level feature matching, together with iterative and\nself-supervised attention-based adjustments, enables consistent sensor fusion\nin complex, real-world conditions without requiring ground-truth calibration\nmatrices or elaborate data preprocessing. Code is available at\n\\href{https://github.com/radar-lab/Lidar\\_Camera\\_Automatic\\_Calibration}{https://github.com/radar-lab/Lidar\\_Camera\\_Automatic\\_Calibration}",
    "published_date": "2025-02-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.17648v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.17611v1",
    "title": "Evaluating the Effect of Retrieval Augmentation on Social Biases",
    "authors": [
      "Tianhui Zhang",
      "Yi Zhou",
      "Danushka Bollegala"
    ],
    "author_ids": [],
    "abstract": "Retrieval Augmented Generation (RAG) has gained popularity as a method for\nconveniently incorporating novel facts that were not seen during the\npre-training stage in Large Language Model (LLM)-based Natural Language\nGeneration (NLG) systems. However, LLMs are known to encode significant levels\nof unfair social biases. The modulation of these biases by RAG in NLG systems\nis not well understood. In this paper, we systematically study the relationship\nbetween the different components of a RAG system and the social biases\npresented in the text generated across three languages (i.e. English, Japanese\nand Chinese) and four social bias types (i.e. gender, race, age and religion).\nSpecifically, using the Bias Question Answering (BBQ) benchmark datasets, we\nevaluate the social biases in RAG responses from document collections with\nvarying levels of stereotypical biases, employing multiple LLMs used as\ngenerators. We find that the biases in document collections are often amplified\nin the generated responses, even when the generating LLM exhibits a low-level\nof bias. Our findings raise concerns about the use of RAG as a technique for\ninjecting novel facts into NLG systems and call for careful evaluation of\npotential social biases in RAG applications before their real-world deployment.",
    "published_date": "2025-02-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.17611v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.17390v1",
    "title": "Mitigating Bias in RAG: Controlling the Embedder",
    "authors": [
      "Taeyoun Kim",
      "Jacob Springer",
      "Aditi Raghunathan",
      "Maarten Sap"
    ],
    "author_ids": [],
    "abstract": "In retrieval augmented generation (RAG) systems, each individual component --\nthe LLM, embedder, and corpus -- could introduce biases in the form of skews\ntowards outputting certain perspectives or identities. In this work, we study\nthe conflict between biases of each component and their relationship to the\noverall bias of the RAG system, which we call bias conflict. Examining both\ngender and political biases as case studies, we show that bias conflict can be\ncharacterized through a linear relationship among components despite its\ncomplexity in 6 different LLMs. Through comprehensive fine-tuning experiments\ncreating 120 differently biased embedders, we demonstrate how to control bias\nwhile maintaining utility and reveal the importance of reverse-biasing the\nembedder to mitigate bias in the overall system. Additionally, we find that\nLLMs and tasks exhibit varying sensitivities to the embedder bias, a crucial\nfactor to consider for debiasing. Our results underscore that a fair RAG system\ncan be better achieved by carefully controlling the bias of the embedder rather\nthan increasing its fairness.",
    "published_date": "2025-02-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.17390v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.05765v1",
    "title": "Encoding Inequity: Examining Demographic Bias in LLM-Driven Robot Caregiving",
    "authors": [
      "Raj Korpan"
    ],
    "author_ids": [],
    "abstract": "As robots take on caregiving roles, ensuring equitable and unbiased\ninteractions with diverse populations is critical. Although Large Language\nModels (LLMs) serve as key components in shaping robotic behavior, speech, and\ndecision-making, these models may encode and propagate societal biases, leading\nto disparities in care based on demographic factors. This paper examines how\nLLM-generated responses shape robot caregiving characteristics and\nresponsibilities when prompted with different demographic information related\nto sex, gender, sexuality, race, ethnicity, nationality, disability, and age.\nFindings show simplified descriptions for disability and age, lower sentiment\nfor disability and LGBTQ+ identities, and distinct clustering patterns\nreinforcing stereotypes in caregiving narratives. These results emphasize the\nneed for ethical and inclusive HRI design.",
    "published_date": "2025-02-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.05765v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.17340v1",
    "title": "Low-rank bias, weight decay, and model merging in neural networks",
    "authors": [
      "Ilja Kuzborskij",
      "Yasin Abbasi Yadkori"
    ],
    "author_ids": [],
    "abstract": "We explore the low-rank structure of the weight matrices in neural networks\noriginating from training with Gradient Descent (GD) and Gradient Flow (GF)\nwith $L2$ regularization (also known as weight decay). We show several\nproperties of GD-trained deep neural networks, induced by $L2$ regularization.\nIn particular, for a stationary point of GD we show alignment of the parameters\nand the gradient, norm preservation across layers, and low-rank bias:\nproperties previously known in the context of GF solutions. Experiments show\nthat the assumptions made in the analysis only mildly affect the observations.\nIn addition, we investigate a multitask learning phenomenon enabled by $L2$\nregularization and low-rank bias. In particular, we show that if two networks\nare trained, such that the inputs in the training set of one network are\napproximately orthogonal to the inputs in the training set of the other\nnetwork, the new network obtained by simply summing the weights of the two\nnetworks will perform as well on both training sets as the respective\nindividual networks. We demonstrate this for shallow ReLU neural networks\ntrained by GD, as well as deep linear and deep ReLU networks trained by GF.",
    "published_date": "2025-02-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.17340v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.17332v1",
    "title": "Tokenized SAEs: Disentangling SAE Reconstructions",
    "authors": [
      "Thomas Dooms",
      "Daniel Wilhelm"
    ],
    "author_ids": [],
    "abstract": "Sparse auto-encoders (SAEs) have become a prevalent tool for interpreting\nlanguage models' inner workings. However, it is unknown how tightly SAE\nfeatures correspond to computationally important directions in the model. This\nwork empirically shows that many RES-JB SAE features predominantly correspond\nto simple input statistics. We hypothesize this is caused by a large class\nimbalance in training data combined with a lack of complex error signals. To\nreduce this behavior, we propose a method that disentangles token\nreconstruction from feature reconstruction. This improvement is achieved by\nintroducing a per-token bias, which provides an enhanced baseline for\ninteresting reconstruction. As a result, significantly more interesting\nfeatures and improved reconstruction in sparse regimes are learned.",
    "published_date": "2025-02-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.17332v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.17293v1",
    "title": "The Challenges of Bringing Religious and Philosophical Values Into Design",
    "authors": [
      "Louisa Conwill",
      "Megan K. Levis",
      "Karla Badillo-Urquiola",
      "Walter J. Scheirer"
    ],
    "author_ids": [],
    "abstract": "HCI is increasingly taking inspiration from philosophical and religious\ntraditions as a basis for ethical technology designs. If these values are to be\nincorporated into real-world designs, there may be challenges when designers\nwork with values unfamiliar to them. Therefore, we investigate the variance in\ninterpretations when values are translated to technology designs. To do so we\nidentified social media designs that embodied the main principles of Catholic\nSocial Teaching (CST). We then interviewed 24 technology experts with varying\nlevels of familiarity with CST to assess how their understanding of how those\nvalues would manifest in a technology design. We found that familiarity with\nCST did not impact participant responses: there were clear patterns in how all\nparticipant responses differed from the values we determined the designs\nembodied. We propose that value experts be included in the design process to\nmore effectively create designs that embody particular values.",
    "published_date": "2025-02-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.17293v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.17270v2",
    "title": "Order Fairness Evaluation of DAG-based ledgers",
    "authors": [
      "Erwan Mahe",
      "Sara Tucci-Piergiovanni"
    ],
    "author_ids": [],
    "abstract": "Order fairness in distributed ledgers refers to properties that relate the\norder in which transactions are sent or received to the order in which they are\neventually finalized, i.e., totally ordered. The study of such properties is\nrelatively new and has been especially stimulated by the rise of Maximal\nExtractable Value (MEV) attacks in blockchain environments. Indeed, in many\nclassical blockchain protocols, leaders are responsible for selecting the\ntransactions to be included in blocks, which creates a clear vulnerability and\nopportunity for transaction order manipulation.\n  Unlike blockchains, DAG-based ledgers allow participants in the network to\nindependently propose blocks, which are then arranged as vertices of a directed\nacyclic graph. Interestingly, leaders in DAG-based ledgers are elected only\nafter the fact, once transactions are already part of the graph, to determine\ntheir total order. In other words, transactions are not chosen by single\nleaders; instead, they are collectively validated by the nodes, and leaders are\nonly elected to establish an ordering. This approach intuitively reduces the\nrisk of transaction manipulation and enhances fairness.\n  In this paper, we aim to quantify the capability of DAG-based ledgers to\nachieve order fairness. To this end, we define new variants of order fairness\nadapted to DAG-based ledgers and evaluate the impact of an adversary capable of\ncompromising a limited number of nodes (below the one-third threshold) to\nreorder transactions. We analyze how often our order fairness properties are\nviolated under different network conditions and parameterizations of the DAG\nalgorithm, depending on the adversary's power.\n  Our study shows that DAG-based ledgers are still vulnerable to reordering\nattacks, as an adversary can coordinate a minority of Byzantine nodes to\nmanipulate the DAG's structure.",
    "published_date": "2025-02-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CR",
      "cs.DC",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.17270v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.17204v2",
    "title": "Order Matters: Investigate the Position Bias in Multi-constraint Instruction Following",
    "authors": [
      "Jie Zeng",
      "Qianyu He",
      "Qingyu Ren",
      "Jiaqing Liang",
      "Yanghua Xiao",
      "Weikang Zhou",
      "Zeye Sun",
      "Fei Yu"
    ],
    "author_ids": [],
    "abstract": "Real-world instructions with multiple constraints pose a significant\nchallenge to existing large language models (LLMs). An observation is that the\nLLMs exhibit dramatic performance fluctuation when disturbing the order of the\nincorporated constraints. Yet, none of the existing works has systematically\ninvestigated this position bias problem in the field of multi-constraint\ninstruction following. To bridge this gap, we design a probing task where we\nquantitatively measure the difficulty distribution of the constraints by a\nnovel Difficulty Distribution Index (CDDI). Through the experimental results,\nwe find that LLMs are more performant when presented with the constraints in a\n``hard-to-easy'' order. This preference can be generalized to LLMs with\ndifferent architecture or different sizes of parameters. Additionally, we\nconduct an explanation study, providing an intuitive insight into the\ncorrelation between the LLM's attention and constraint orders. Our code and\ndataset are publicly available at https://github.com/meowpass/PBIF.",
    "published_date": "2025-02-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.17204v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.17172v1",
    "title": "Teleology-Driven Affective Computing: A Causal Framework for Sustained Well-Being",
    "authors": [
      "Bin Yin",
      "Chong-Yi Liu",
      "Liya Fu",
      "Jinkun Zhang"
    ],
    "author_ids": [],
    "abstract": "Affective computing has made significant strides in emotion recognition and\ngeneration, yet current approaches mainly focus on short-term pattern\nrecognition and lack a comprehensive framework to guide affective agents toward\nlong-term human well-being. To address this, we propose a teleology-driven\naffective computing framework that unifies major emotion theories (basic\nemotion, appraisal, and constructivist approaches) under the premise that\naffect is an adaptive, goal-directed process that facilitates survival and\ndevelopment. Our framework emphasizes aligning agent responses with both\npersonal/individual and group/collective well-being over extended timescales.\nWe advocate for creating a \"dataverse\" of personal affective events, capturing\nthe interplay between beliefs, goals, actions, and outcomes through real-world\nexperience sampling and immersive virtual reality. By leveraging causal\nmodeling, this \"dataverse\" enables AI systems to infer individuals' unique\naffective concerns and provide tailored interventions for sustained well-being.\nAdditionally, we introduce a meta-reinforcement learning paradigm to train\nagents in simulated environments, allowing them to adapt to evolving affective\nconcerns and balance hierarchical goals - from immediate emotional needs to\nlong-term self-actualization. This framework shifts the focus from statistical\ncorrelations to causal reasoning, enhancing agents' ability to predict and\nrespond proactively to emotional challenges, and offers a foundation for\ndeveloping personalized, ethically aligned affective systems that promote\nmeaningful human-AI interactions and societal well-being.",
    "published_date": "2025-02-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "q-bio.NC",
      "H.1.2, J.4",
      "H.1.2; J.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.17172v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.17167v1",
    "title": "A Novel Multiple Access Scheme for Heterogeneous Wireless Communications using Symmetry-aware Continual Deep Reinforcement Learning",
    "authors": [
      "Hamidreza Mazandarani",
      "Masoud Shokrnezhad",
      "Tarik Taleb"
    ],
    "author_ids": [],
    "abstract": "The Metaverse holds the potential to revolutionize digital interactions\nthrough the establishment of a highly dynamic and immersive virtual realm over\nwireless communications systems, offering services such as massive twinning and\ntelepresence. This landscape presents novel challenges, particularly efficient\nmanagement of multiple access to the frequency spectrum, for which numerous\nadaptive Deep Reinforcement Learning (DRL) approaches have been explored.\nHowever, challenges persist in adapting agents to heterogeneous and\nnon-stationary wireless environments. In this paper, we present a novel\napproach that leverages Continual Learning (CL) to enhance intelligent Medium\nAccess Control (MAC) protocols, featuring an intelligent agent coexisting with\nlegacy User Equipments (UEs) with varying numbers, protocols, and transmission\nprofiles unknown to the agent for the sake of backward compatibility and\nprivacy. We introduce an adaptive Double and Dueling Deep Q-Learning\n(D3QL)-based MAC protocol, enriched by a symmetry-aware CL mechanism, which\nmaximizes intelligent agent throughput while ensuring fairness. Mathematical\nanalysis validates the efficiency of our proposed scheme, showcasing\nsuperiority over conventional DRL-based techniques in terms of throughput,\ncollision rate, and fairness, coupled with real-time responsiveness in highly\ndynamic scenarios.",
    "published_date": "2025-02-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.17167v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.17105v1",
    "title": "SFLD: Reducing the content bias for AI-generated Image Detection",
    "authors": [
      "Seoyeon Gye",
      "Junwon Ko",
      "Hyounguk Shon",
      "Minchan Kwon",
      "Junmo Kim"
    ],
    "author_ids": [],
    "abstract": "Identifying AI-generated content is critical for the safe and ethical use of\ngenerative AI. Recent research has focused on developing detectors that\ngeneralize to unknown generators, with popular methods relying either on\nhigh-level features or low-level fingerprints. However, these methods have\nclear limitations: biased towards unseen content, or vulnerable to common image\ndegradations, such as JPEG compression. To address these issues, we propose a\nnovel approach, SFLD, which incorporates PatchShuffle to integrate high-level\nsemantic and low-level textural information. SFLD applies PatchShuffle at\nmultiple levels, improving robustness and generalization across various\ngenerative models. Additionally, current benchmarks face challenges such as low\nimage quality, insufficient content preservation, and limited class diversity.\nIn response, we introduce TwinSynths, a new benchmark generation methodology\nthat constructs visually near-identical pairs of real and synthetic images to\nensure high quality and content preservation. Our extensive experiments and\nanalysis show that SFLD outperforms existing methods on detecting a wide\nvariety of fake images sourced from GANs, diffusion models, and TwinSynths,\ndemonstrating the state-of-the-art performance and generalization capabilities\nto novel generative models.",
    "published_date": "2025-02-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.17105v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.16987v1",
    "title": "Hotter and Colder: A New Approach to Annotating Sentiment, Emotions, and Bias in Icelandic Blog Comments",
    "authors": [
      "Steinunn Rut Friðriksdóttir",
      "Dan Saattrup Nielsen",
      "Hafsteinn Einarsson"
    ],
    "author_ids": [],
    "abstract": "This paper presents Hotter and Colder, a dataset designed to analyze various\ntypes of online behavior in Icelandic blog comments. Building on previous work,\nwe used GPT-4o mini to annotate approximately 800,000 comments for 25 tasks,\nincluding sentiment analysis, emotion detection, hate speech, and group\ngeneralizations. Each comment was automatically labeled on a 5-point Likert\nscale. In a second annotation stage, comments with high or low probabilities of\ncontaining each examined behavior were subjected to manual revision. By\nleveraging crowdworkers to refine these automatically labeled comments, we\nensure the quality and accuracy of our dataset resulting in 12,232 uniquely\nannotated comments and 19,301 annotations. Hotter and Colder provides an\nessential resource for advancing research in content moderation and\nautomatically detectiong harmful online behaviors in Icelandic.",
    "published_date": "2025-02-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.16987v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.16889v1",
    "title": "Unveiling Institution-Specific Bias in Pathology Foundation Models: Detriments, Causes, and Potential Solutions",
    "authors": [
      "Weiping Lin",
      "Shen Liu",
      "Runchen Zhu",
      "Liansheng Wang"
    ],
    "author_ids": [],
    "abstract": "Pathology foundation models (PFMs) extract valuable discriminative features\nfrom images for downstream clinical tasks. PFMs have simplified the development\nof deep learning models, effectively leveraging prior knowledge to improve\ndiagnostic accuracy in diverse scenarios. However, we find that PFMs sometimes\nstruggle with certain challenges. Specifically, features extracted by PFMs are\noften contaminated by diagnosis-irrelevant information, i.e.,\ninstitution-specific features associated with the images. This contamination\ncan lead to spurious correlations, undermining the models' generalization\nability when applied in real-world clinical settings. In this work, we first\nreveal the issue of feature contamination in PFMs, demonstrate the presence of\ninstitution-specific features, thoroughly investigate its negative impacts,\nanalyze the underlying causes, and provide insights into potential solutions.\nSpecifically, we find that institution-specific information is embedded in\npathological images and can be readily captured by current PFMs. Through\nextensive experiments, we demonstrate the detrimental impact of this irrelevant\ninformation, particularly in out-of-distribution (OOD) settings, where reliance\non contaminated features leads to significant performance degradation. This\nindicates that the models are being misled by non-diagnostic information. We\nfurther delve into the reasons PFMs extract such institution-specific\ninformation and validate our findings. Finally, we propose a simple yet\neffective solution to mitigate the influence of irrelevant information. This\nstudy is not intended to criticize existing PFMs, as they have indeed greatly\nadvanced the development of computational pathology. our aim is to inspire\nfuture research to focus on innovative training strategies, rather than relying\nexclusively on scaling laws, to realize more generalized PFMs.",
    "published_date": "2025-02-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.16889v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.16841v1",
    "title": "Fair Foundation Models for Medical Image Analysis: Challenges and Perspectives",
    "authors": [
      "Dilermando Queiroz",
      "Anderson Carlos",
      "André Anjos",
      "Lilian Berton"
    ],
    "author_ids": [],
    "abstract": "Ensuring equitable Artificial Intelligence (AI) in healthcare demands systems\nthat make unbiased decisions across all demographic groups, bridging technical\ninnovation with ethical principles. Foundation Models (FMs), trained on vast\ndatasets through self-supervised learning, enable efficient adaptation across\nmedical imaging tasks while reducing dependency on labeled data. These models\ndemonstrate potential for enhancing fairness, though significant challenges\nremain in achieving consistent performance across demographic groups. Our\nreview indicates that effective bias mitigation in FMs requires systematic\ninterventions throughout all stages of development. While previous approaches\nfocused primarily on model-level bias mitigation, our analysis reveals that\nfairness in FMs requires integrated interventions throughout the development\npipeline, from data documentation to deployment protocols. This comprehensive\nframework advances current knowledge by demonstrating how systematic bias\nmitigation, combined with policy engagement, can effectively address both\ntechnical and institutional barriers to equitable AI in healthcare. The\ndevelopment of equitable FMs represents a critical step toward democratizing\nadvanced healthcare technologies, particularly for underserved populations and\nregions with limited medical infrastructure and computational resources.",
    "published_date": "2025-02-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.16841v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.16832v1",
    "title": "FedBM: Stealing Knowledge from Pre-trained Language Models for Heterogeneous Federated Learning",
    "authors": [
      "Meilu Zhu",
      "Qiushi Yang",
      "Zhifan Gao",
      "Yixuan Yuan",
      "Jun Liu"
    ],
    "author_ids": [],
    "abstract": "Federated learning (FL) has shown great potential in medical image computing\nsince it provides a decentralized learning paradigm that allows multiple\nclients to train a model collaboratively without privacy leakage. However,\ncurrent studies have shown that data heterogeneity incurs local learning bias\nin classifiers and feature extractors of client models during local training,\nleading to the performance degradation of a federation system. To address these\nissues, we propose a novel framework called Federated Bias eliMinating (FedBM)\nto get rid of local learning bias in heterogeneous federated learning (FL),\nwhich mainly consists of two modules, i.e., Linguistic Knowledge-based\nClassifier Construction (LKCC) and Concept-guided Global Distribution\nEstimation (CGDE). Specifically, LKCC exploits class concepts, prompts and\npre-trained language models (PLMs) to obtain concept embeddings. These\nembeddings are used to estimate the latent concept distribution of each class\nin the linguistic space. Based on the theoretical derivation, we can rely on\nthese distributions to pre-construct a high-quality classifier for clients to\nachieve classification optimization, which is frozen to avoid classifier bias\nduring local training. CGDE samples probabilistic concept embeddings from the\nlatent concept distributions to learn a conditional generator to capture the\ninput space of the global model. Three regularization terms are introduced to\nimprove the quality and utility of the generator. The generator is shared by\nall clients and produces pseudo data to calibrate updates of local feature\nextractors. Extensive comparison experiments and ablation studies on public\ndatasets demonstrate the superior performance of FedBM over state-of-the-arts\nand confirm the effectiveness of each module, respectively. The code is\navailable at https://github.com/CUHK-AIM-Group/FedBM.",
    "published_date": "2025-02-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.16832v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.16816v1",
    "title": "Finite-Sample Analysis of Policy Evaluation for Robust Average Reward Reinforcement Learning",
    "authors": [
      "Yang Xu",
      "Washim Uddin Mondal",
      "Vaneet Aggarwal"
    ],
    "author_ids": [],
    "abstract": "We present the first finite-sample analysis for policy evaluation in robust\naverage-reward Markov Decision Processes (MDPs). Prior works in this setting\nhave established only asymptotic convergence guarantees, leaving open the\nquestion of sample complexity. In this work, we address this gap by\nestablishing that the robust Bellman operator is a contraction under the span\nsemi-norm, and developing a stochastic approximation framework with controlled\nbias. Our approach builds upon Multi-Level Monte Carlo (MLMC) techniques to\nestimate the robust Bellman operator efficiently. To overcome the infinite\nexpected sample complexity inherent in standard MLMC, we introduce a truncation\nmechanism based on a geometric distribution, ensuring a finite constant sample\ncomplexity while maintaining a small bias that decays exponentially with the\ntruncation level. Our method achieves the order-optimal sample complexity of\n$\\tilde{\\mathcal{O}}(\\epsilon^{-2})$ for robust policy evaluation and robust\naverage reward estimation, marking a significant advancement in robust\nreinforcement learning theory.",
    "published_date": "2025-02-24T00:00:00",
    "year": 2025,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.16816v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.16732v2",
    "title": "DeepSeek reshaping healthcare in China's tertiary hospitals",
    "authors": [
      "Jishizhan Chen",
      "Qingzeng Zhang"
    ],
    "author_ids": [],
    "abstract": "The rapid integration of artificial intelligence (AI) into healthcare is\ntransforming clinical decision-making and hospital operations. DeepSeek has\nemerged as a leading AI system, widely deployed across China's tertiary\nhospitals since January 2025. Initially implemented in Shanghai's major medical\ninstitutions, it has since expanded nationwide, enhancing diagnostic accuracy,\nstreamlining workflows, and improving patient management. AI-powered pathology,\nimaging analysis, and clinical decision support systems have demonstrated\nsignificant potential in optimizing medical processes and reducing the\ncognitive burden on healthcare professionals. However, the widespread adoption\nof AI in healthcare raises critical regulatory and ethical challenges,\nparticularly regarding accountability in AI-assisted diagnosis and the risk of\nautomation bias. The absence of a well-defined liability framework underscores\nthe need for policies that ensure AI functions as an assistive tool rather than\nan autonomous decision-maker. With continued technological advancements, AI is\nexpected to integrate multimodal data sources, such as genomics and radiomics,\npaving the way for precision medicine and personalized treatment strategies.\nThe future of AI in healthcare depends on the development of transparent\nregulatory structures, industry collaboration, and adaptive governance\nframeworks that balance innovation with responsibility, ensuring equitable and\neffective AI-driven medical services.",
    "published_date": "2025-02-23T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.16732v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.16696v1",
    "title": "Dynamic LLM Routing and Selection based on User Preferences: Balancing Performance, Cost, and Ethics",
    "authors": [
      "Deepak Babu Piskala",
      "Vijay Raajaa",
      "Sachin Mishra",
      "Bruno Bozza"
    ],
    "author_ids": [],
    "abstract": "With the widespread deployment of large language models (LLMs) such as GPT4,\nBART, and LLaMA, the need for a system that can intelligently select the most\nsuitable model for specific tasks while balancing cost, latency, accuracy, and\nethical considerations has become increasingly important. Recognizing that not\nall tasks necessitate models with over 100 billion parameters, we introduce\nOptiRoute, an advanced model routing engine designed to dynamically select and\nroute tasks to the optimal LLM based on detailed user-defined requirements.\nOptiRoute captures both functional (e.g., accuracy, speed, cost) and\nnon-functional (e.g., helpfulness, harmlessness, honesty) criteria, leveraging\nlightweight task analysis and complexity estimation to efficiently match tasks\nwith the best-fit models from a diverse array of LLMs. By employing a hybrid\napproach combining k-nearest neighbors (kNN) search and hierarchical filtering,\nOptiRoute optimizes for user priorities while minimizing computational\noverhead. This makes it ideal for real-time applications in cloud-based ML\nplatforms, personalized AI services, and regulated industries.",
    "published_date": "2025-02-23T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.16696v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.16681v1",
    "title": "Are Sparse Autoencoders Useful? A Case Study in Sparse Probing",
    "authors": [
      "Subhash Kantamneni",
      "Joshua Engels",
      "Senthooran Rajamanoharan",
      "Max Tegmark",
      "Neel Nanda"
    ],
    "author_ids": [],
    "abstract": "Sparse autoencoders (SAEs) are a popular method for interpreting concepts\nrepresented in large language model (LLM) activations. However, there is a lack\nof evidence regarding the validity of their interpretations due to the lack of\na ground truth for the concepts used by an LLM, and a growing number of works\nhave presented problems with current SAEs. One alternative source of evidence\nwould be demonstrating that SAEs improve performance on downstream tasks beyond\nexisting baselines. We test this by applying SAEs to the real-world task of LLM\nactivation probing in four regimes: data scarcity, class imbalance, label\nnoise, and covariate shift. Due to the difficulty of detecting concepts in\nthese challenging settings, we hypothesize that SAEs' basis of interpretable,\nconcept-level latents should provide a useful inductive bias. However, although\nSAEs occasionally perform better than baselines on individual datasets, we are\nunable to design ensemble methods combining SAEs with baselines that\nconsistently outperform ensemble methods solely using baselines. Additionally,\nalthough SAEs initially appear promising for identifying spurious correlations,\ndetecting poor dataset quality, and training multi-token probes, we are able to\nachieve similar results with simple non-SAE baselines as well. Though we cannot\ndiscount SAEs' utility on other tasks, our findings highlight the shortcomings\nof current SAEs and the need to rigorously evaluate interpretability methods on\ndownstream tasks with strong baselines.",
    "published_date": "2025-02-23T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.16681v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.16644v2",
    "title": "Mapping out AI Functions in Intelligent Disaster (Mis)Management and AI-Caused Disasters",
    "authors": [
      "Yasser Pouresmaeil",
      "Saleh Afroogh",
      "Junfeng Jiao"
    ],
    "author_ids": [],
    "abstract": "This study provides a classification of disasters in terms of their causal\nparameters, introducing hypothetical cases of independent or hybrid AI-caused\ndisasters. We overview the role of AI in disaster management, and possible\nethical repercussions of the use of AI in intelligent disaster management -\nIDM. Next, we scrutinize certain ways of preventing or alleviating the ethical\nrepercussions of AI use in disaster mismanagement, such as privacy breaches,\nbiases, discriminations, etc. These include pre-design a priori, in-design, and\npost-design methods as well as regulations. We then discuss the government's\nrole in preventing the ethical repercussions of AI use in IDM and identify and\nassess its deficits and challenges. We then discuss the advantages and\ndisadvantages of pre-design or embedded ethics. Finally, we briefly consider\nthe question of accountability and liability in AI-caused disasters.",
    "published_date": "2025-02-23T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.16644v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.16608v1",
    "title": "Toward Dependency Dynamics in Multi-Agent Reinforcement Learning for Traffic Signal Control",
    "authors": [
      "Yuli Zhang",
      "Shangbo Wang",
      "Dongyao Jia",
      "Pengfei Fan",
      "Ruiyuan Jiang",
      "Hankang Gu",
      "Andy H. F. Chow"
    ],
    "author_ids": [],
    "abstract": "Reinforcement learning (RL) emerges as a promising data-driven approach for\nadaptive traffic signal control (ATSC) in complex urban traffic networks, with\ndeep neural networks substantially augmenting its learning capabilities.\nHowever, centralized RL becomes impractical for ATSC involving multiple agents\ndue to the exceedingly high dimensionality of the joint action space.\nMulti-agent RL (MARL) mitigates this scalability issue by decentralizing\ncontrol to local RL agents. Nevertheless, this decentralized method introduces\nnew challenges: the environment becomes partially observable from the\nperspective of each local agent due to constrained inter-agent communication.\nBoth centralized RL and MARL exhibit distinct strengths and weaknesses,\nparticularly under heavy intersectional traffic conditions. In this paper, we\njustify that MARL can achieve the optimal global Q-value by separating into\nmultiple IRL (Independent Reinforcement Learning) processes when no spill-back\ncongestion occurs (no agent dependency) among agents (intersections). In the\npresence of spill-back congestion (with agent dependency), the maximum global\nQ-value can be achieved by using centralized RL. Building upon the conclusions,\nwe propose a novel Dynamic Parameter Update Strategy for Deep Q-Network\n(DQN-DPUS), which updates the weights and bias based on the dependency dynamics\namong agents, i.e. updating only the diagonal sub-matrices for the scenario\nwithout spill-back congestion. We validate the DQN-DPUS in a simple network\nwith two intersections under varying traffic, and show that the proposed\nstrategy can speed up the convergence rate without sacrificing optimal\nexploration. The results corroborate our theoretical findings, demonstrating\nthe efficacy of DQN-DPUS in optimizing traffic signal control.",
    "published_date": "2025-02-23T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.16608v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.16602v1",
    "title": "VidLBEval: Benchmarking and Mitigating Language Bias in Video-Involved LVLMs",
    "authors": [
      "Yiming Yang",
      "Yangyang Guo",
      "Hui Lu",
      "Yan Wang"
    ],
    "author_ids": [],
    "abstract": "Recently, Large Vision-Language Models (LVLMs) have made significant strides\nacross diverse multimodal tasks and benchmarks. This paper reveals a largely\nunder-explored problem from existing video-involved LVLMs - language bias,\nwhere models tend to prioritize language over video and thus result in\nincorrect responses. To address this research gap, we first collect a Video\nLanguage Bias Evaluation Benchmark, which is specifically designed to assess\nthe language bias in video-involved LVLMs through two key tasks: ambiguous\nvideo contrast and interrogative question probing. Accordingly, we design\naccompanied evaluation metrics that aim to penalize LVLMs being biased by\nlanguage. In addition, we also propose Multi-branch Contrastive Decoding (MCD),\nintroducing two expert branches to simultaneously counteract language bias\npotentially generated by the amateur text-only branch. Our experiments\ndemonstrate that i) existing video-involved LVLMs, including both proprietary\nand open-sourced, are largely limited by the language bias problem; ii) our MCD\ncan effectively mitigate this issue and maintain general-purpose capabilities\nin various video-involved LVLMs without any additional retraining or alteration\nto model architectures.",
    "published_date": "2025-02-23T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.16602v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.16600v4",
    "title": "Diagnosing Moral Reasoning Acquisition in Language Models: Pragmatics and Generalization",
    "authors": [
      "Guangliang Liu",
      "Lei Jiang",
      "Xitong Zhang",
      "Kristen Marie Johnson"
    ],
    "author_ids": [],
    "abstract": "Ensuring that Large Language Models (LLMs) return just responses which adhere\nto societal values is crucial for their broader application. Prior research has\nshown that LLMs often fail to perform satisfactorily on tasks requiring moral\ncognizance, such as ethics-based judgments. While current approaches have\nfocused on fine-tuning LLMs with curated datasets to improve their capabilities\non such tasks, choosing the optimal learning paradigm to enhance the ethical\nresponses of LLMs remains an open research debate. In this work, we aim to\naddress this fundamental question: can current learning paradigms enable LLMs\nto acquire sufficient moral reasoning capabilities? Drawing from distributional\nsemantics theory and the pragmatic nature of moral discourse, our analysis\nindicates that performance improvements follow a mechanism similar to that of\nsemantic-level tasks, and therefore remain affected by the pragmatic nature of\nmorals latent in discourse, a phenomenon we name the pragmatic dilemma. We\nconclude that this pragmatic dilemma imposes significant limitations on the\ngeneralization ability of current learning paradigms, making it the primary\nbottleneck for moral reasoning acquisition in LLMs.",
    "published_date": "2025-02-23T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.16600v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.16458v1",
    "title": "Users Favor LLM-Generated Content -- Until They Know It's AI",
    "authors": [
      "Petr Parshakov",
      "Iuliia Naidenova",
      "Sofia Paklina",
      "Nikita Matkin",
      "Cornel Nesseler"
    ],
    "author_ids": [],
    "abstract": "In this paper, we investigate how individuals evaluate human and large langue\nmodels generated responses to popular questions when the source of the content\nis either concealed or disclosed. Through a controlled field experiment,\nparticipants were presented with a set of questions, each accompanied by a\nresponse generated by either a human or an AI. In a randomized design, half of\nthe participants were informed of the response's origin while the other half\nremained unaware. Our findings indicate that, overall, participants tend to\nprefer AI-generated responses. However, when the AI origin is revealed, this\npreference diminishes significantly, suggesting that evaluative judgments are\ninfluenced by the disclosure of the response's provenance rather than solely by\nits quality. These results underscore a bias against AI-generated content,\nhighlighting the societal challenge of improving the perception of AI work in\ncontexts where quality assessments should be paramount.",
    "published_date": "2025-02-23T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.LG",
      "econ.GN",
      "q-fin.EC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.16458v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.16534v1",
    "title": "Multilingual != Multicultural: Evaluating Gaps Between Multilingual Capabilities and Cultural Alignment in LLMs",
    "authors": [
      "Jonathan Rystrøm",
      "Hannah Rose Kirk",
      "Scott Hale"
    ],
    "author_ids": [],
    "abstract": "Large Language Models (LLMs) are becoming increasingly capable across global\nlanguages. However, the ability to communicate across languages does not\nnecessarily translate to appropriate cultural representations. A key concern is\nUS-centric bias, where LLMs reflect US rather than local cultural values. We\npropose a novel methodology that compares LLM-generated response distributions\nagainst population-level opinion data from the World Value Survey across four\nlanguages (Danish, Dutch, English, and Portuguese). Using a rigorous linear\nmixed-effects regression framework, we compare two families of models: Google's\nGemma models (2B--27B parameters) and successive iterations of OpenAI's\nturbo-series. Across the families of models, we find no consistent\nrelationships between language capabilities and cultural alignment. While the\nGemma models have a positive correlation between language capability and\ncultural alignment across languages, the OpenAI models do not. Importantly, we\nfind that self-consistency is a stronger predictor of multicultural alignment\nthan multilingual capabilities. Our results demonstrate that achieving\nmeaningful cultural alignment requires dedicated effort beyond improving\ngeneral language capabilities.",
    "published_date": "2025-02-23T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.16534v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.16493v1",
    "title": "Trunk-branch Contrastive Network with Multi-view Deformable Aggregation for Multi-view Action Recognition",
    "authors": [
      "Yingyuan Yang",
      "Guoyuan Liang",
      "Can Wang",
      "Xiaojun Wu"
    ],
    "author_ids": [],
    "abstract": "Multi-view action recognition aims to identify actions in a given multi-view\nscene. Traditional studies initially extracted refined features from each view,\nfollowed by implemented paired interaction and integration, but they\npotentially overlooked the critical local features in each view. When observing\nobjects from multiple perspectives, individuals typically form a comprehensive\nimpression and subsequently fill in specific details. Drawing inspiration from\nthis cognitive process, we propose a novel trunk-branch contrastive network\n(TBCNet) for RGB-based multi-view action recognition. Distinctively, TBCNet\nfirst obtains fused features in the trunk block and then implicitly supplements\nvital details provided by the branch block via contrastive learning, generating\na more informative and comprehensive action representation. Within this\nframework, we construct two core components: the multi-view deformable\naggregation and the trunk-branch contrastive learning. MVDA employed in the\ntrunk block effectively facilitates multi-view feature fusion and adaptive\ncross-view spatio-temporal correlation, where a global aggregation module is\nutilized to emphasize significant spatial information and a composite relative\nposition bias is designed to capture the intra- and cross-view relative\npositions. Moreover, a trunk-branch contrastive loss is constructed between\naggregated features and refined details from each view. By incorporating two\ndistinct weights for positive and negative samples, a weighted trunk-branch\ncontrastive loss is proposed to extract valuable information and emphasize\nsubtle inter-class differences. The effectiveness of TBCNet is verified by\nextensive experiments on four datasets including NTU-RGB+D 60, NTU-RGB+D 120,\nPKU-MMD, and N-UCLA dataset. Compared to other RGB-based methods, our approach\nachieves state-of-the-art performance in cross-subject and cross-setting\nprotocols.",
    "published_date": "2025-02-23T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.16493v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.16477v1",
    "title": "Unmasking Societal Biases in Respiratory Support for ICU Patients through Social Determinants of Health",
    "authors": [
      "Mira Moukheiber",
      "Lama Moukheiber",
      "Dana Moukheiber",
      "Hyung-Chul Lee"
    ],
    "author_ids": [],
    "abstract": "In critical care settings, where precise and timely interventions are crucial\nfor health outcomes, evaluating disparities in patient outcomes is essential.\nCurrent approaches often fail to fully capture the impact of respiratory\nsupport interventions on individuals affected by social determinants of health.\nWhile attributes such as gender, race, and age are commonly assessed and\nprovide valuable insights, they offer only a partial view of the complexities\nfaced by diverse populations. In this study, we focus on two clinically\nmotivated tasks: prolonged mechanical ventilation and successful weaning.\nAdditionally, we conduct fairness audits on the models' predictions across\ndemographic groups and social determinants of health to better understand\nhealth inequities in respiratory interventions within the intensive care unit.\nFurthermore, we release a temporal benchmark dataset, verified by clinical\nexperts, to facilitate benchmarking of clinical respiratory intervention tasks.",
    "published_date": "2025-02-23T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.16477v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.16430v1",
    "title": "Network Tomography with Path-Centric Graph Neural Network",
    "authors": [
      "Yuntong Hu",
      "Junxiang Wang",
      "Liang Zhao"
    ],
    "author_ids": [],
    "abstract": "Network tomography is a crucial problem in network monitoring, where the\nobservable path performance metric values are used to infer the unobserved\nones, making it essential for tasks such as route selection, fault diagnosis,\nand traffic control. However, most existing methods either assume complete\nknowledge of network topology and metric formulas-an unrealistic expectation in\nmany real-world scenarios with limited observability-or rely entirely on\nblack-box end-to-end models. To tackle this, in this paper, we argue that a\ngood network tomography requires synergizing the knowledge from both data and\nappropriate inductive bias from (partial) prior knowledge. To see this, we\npropose Deep Network Tomography (DeepNT), a novel framework that leverages a\npath-centric graph neural network to predict path performance metrics without\nrelying on predefined hand-crafted metrics, assumptions, or the real network\ntopology. The path-centric graph neural network learns the path embedding by\ninferring and aggregating the embeddings of the sequence of nodes that compose\nthis path. Training path-centric graph neural networks requires learning the\nneural netowrk parameters and network topology under discrete constraints\ninduced by the observed path performance metrics, which motivates us to design\na learning objective that imposes connectivity and sparsity constraints on\ntopology and path performance triangle inequality on path performance.\nExtensive experiments on real-world and synthetic datasets demonstrate the\nsuperiority of DeepNT in predicting performance metrics and inferring graph\ntopology compared to state-of-the-art methods.",
    "published_date": "2025-02-23T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.16430v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.00020v1",
    "title": "A Systematic Review of Open Datasets Used in Text-to-Image (T2I) Gen AI Model Safety",
    "authors": [
      "Rakeen Rouf",
      "Trupti Bavalatti",
      "Osama Ahmed",
      "Dhaval Potdar",
      "Faraz Jawed"
    ],
    "author_ids": [],
    "abstract": "Novel research aimed at text-to-image (T2I) generative AI safety often relies\non publicly available datasets for training and evaluation, making the quality\nand composition of these datasets crucial. This paper presents a comprehensive\nreview of the key datasets used in the T2I research, detailing their collection\nmethods, compositions, semantic and syntactic diversity of prompts and the\nquality, coverage, and distribution of harm types in the datasets. By\nhighlighting the strengths and limitations of the datasets, this study enables\nresearchers to find the most relevant datasets for a use case, critically\nassess the downstream impacts of their work given the dataset distribution,\nparticularly regarding model safety and ethical considerations, and also\nidentify the gaps in dataset coverage and quality that future research may\naddress.",
    "published_date": "2025-02-23T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.00020v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.18513v1",
    "title": "Analyzing User Perceptions of Large Language Models (LLMs) on Reddit: Sentiment and Topic Modeling of ChatGPT and DeepSeek Discussions",
    "authors": [
      "Krishnaveni Katta"
    ],
    "author_ids": [],
    "abstract": "While there is an increased discourse on large language models (LLMs) like\nChatGPT and DeepSeek, there is no comprehensive understanding of how users of\nonline platforms, like Reddit, perceive these models. This is an important\nomission because public opinion can influence AI development, trust, and future\npolicy. This study aims at analyzing Reddit discussions about ChatGPT and\nDeepSeek using sentiment and topic modeling to advance the understanding of\nuser attitudes. Some of the significant topics such as trust in AI, user\nexpectations, potential uses of the tools, reservations about AI biases, and\nethical implications of their use are explored in this study. By examining\nthese concerns, the study provides a sense of how public sentiment might shape\nthe direction of AI development going forward. The report also mentions whether\nusers have faith in the technology and what they see as its future. A word\nfrequency approach is used to identify broad topics and sentiment trends. Also,\ntopic modeling through the Latent Dirichlet Allocation (LDA) method identifies\ntop topics in users' language, for example, potential benefits of LLMs, their\ntechnological applications, and their overall social ramifications. The study\naims to inform developers and policymakers by making it easier to see how users\ncomprehend and experience these game-changing technologies.",
    "published_date": "2025-02-22T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SI",
      "cs.CL",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.18513v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.16291v2",
    "title": "The Design Space of Recent AI-assisted Research Tools for Ideation, Sensemaking, and Scientific Creativity",
    "authors": [
      "Runlong Ye",
      "Matthew Varona",
      "Oliver Huang",
      "Patrick Yung Kang Lee",
      "Michael Liut",
      "Carolina Nobre"
    ],
    "author_ids": [],
    "abstract": "Generative AI (GenAI) tools are radically expanding the scope and capability\nof automation in knowledge work such as academic research. While promising for\naugmenting cognition and streamlining processes, AI-assisted research tools may\nalso increase automation bias and hinder critical thinking. To examine recent\ndevelopments, we surveyed publications from leading HCI venues over the past\nthree years, closely analyzing thirteen tools to better understand the novel\ncapabilities of these AI-assisted systems and the design spaces they enable:\nseven employing traditional AI or customized transformer-based approaches, and\nsix integrating open-access large language models (LLMs). Our analysis\ncharacterizes the emerging design space, distinguishes between tools focused on\nworkflow mimicry versus generative exploration, and yields four critical design\nrecommendations to guide the development of future systems that foster\nmeaningful cognitive engagement: providing user agency and control,\ndifferentiating divergent/convergent thinking support, ensuring adaptability,\nand prioritizing transparency/accuracy. This work discusses how these insights\nsignal a shift from mere workflow replication towards generative co-creation,\npresenting new opportunities for the community to craft intuitive, AI-driven\nresearch interfaces and interactions.",
    "published_date": "2025-02-22T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.AI",
      "H.5.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.16291v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.16075v1",
    "title": "Implicit Bias of Gradient Descent for Non-Homogeneous Deep Networks",
    "authors": [
      "Yuhang Cai",
      "Kangjie Zhou",
      "Jingfeng Wu",
      "Song Mei",
      "Michael Lindsey",
      "Peter L. Bartlett"
    ],
    "author_ids": [],
    "abstract": "We establish the asymptotic implicit bias of gradient descent (GD) for\ngeneric non-homogeneous deep networks under exponential loss. Specifically, we\ncharacterize three key properties of GD iterates starting from a sufficiently\nsmall empirical risk, where the threshold is determined by a measure of the\nnetwork's non-homogeneity. First, we show that a normalized margin induced by\nthe GD iterates increases nearly monotonically. Second, we prove that while the\nnorm of the GD iterates diverges to infinity, the iterates themselves converge\nin direction. Finally, we establish that this directional limit satisfies the\nKarush-Kuhn-Tucker (KKT) conditions of a margin maximization problem. Prior\nworks on implicit bias have focused exclusively on homogeneous networks; in\ncontrast, our results apply to a broad class of non-homogeneous networks\nsatisfying a mild near-homogeneity condition. In particular, our results apply\nto networks with residual connections and non-homogeneous activation functions,\nthereby resolving an open problem posed by Ji and Telgarsky (2020).",
    "published_date": "2025-02-22T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.16075v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.16043v2",
    "title": "African Data Ethics: A Discursive Framework for Black Decolonial Data Science",
    "authors": [
      "Teanna Barrett",
      "Chinasa T. Okolo",
      "B. Biira",
      "Eman Sherif",
      "Amy X. Zhang",
      "Leilani Battle"
    ],
    "author_ids": [],
    "abstract": "Most artificial intelligence (AI) and other data-driven systems (DDS) are\ncreated by and for the benefit of global superpowers. The shift towards\npluralism in global data ethics acknowledges the importance of including\nperspectives from the Global Majority to develop responsible data science (RDS)\npractices that mitigate systemic harms inherent to the current data science\necosystem. African practitioners, in particular, are disseminating progressive\ndata ethics principles and best practices for identifying and navigating\nanti-blackness, colonialism, and data dispossession in the data science life\ncycle. However, their perspectives continue to be left at the periphery of\nglobal data ethics conversations. In an effort to center African voices, we\npresent a framework for African data ethics informed by an interdisciplinary\ncorpus of African scholarship. By conducting a thematic analysis of 47\ndocuments, our work leverages concepts of African philosophy to develop a\nframework with seven major principles: 1) decolonize & challenge internal power\nasymmetry, 2) center all communities, 3) uphold universal good, 4) communalism\nin practice, 5) data self-determination, 6) invest in data institutions &\ninfrastructures and 7) prioritize education & youth. We compare a subset of six\nparticularist data ethics frameworks against ours and find similar coverage but\ndiverging interpretations of shared values. We also discuss two case studies\nfrom the African data science community to demonstrate the framework as a tool\nfor evaluating responsible data science decisions. Our framework highlights\nAfrica as a pivotal site for challenging anti-blackness and algorithmic\ncolonialism by promoting the practice of collectivism, self-determination, and\ncultural preservation in data science.",
    "published_date": "2025-02-22T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.16043v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.18505v1",
    "title": "Comprehensive Analysis of Transparency and Accessibility of ChatGPT, DeepSeek, And other SoTA Large Language Models",
    "authors": [
      "Ranjan Sapkota",
      "Shaina Raza",
      "Manoj Karkee"
    ],
    "author_ids": [],
    "abstract": "Despite increasing discussions on open-source Artificial Intelligence (AI),\nexisting research lacks a discussion on the transparency and accessibility of\nstate-of-the-art (SoTA) Large Language Models (LLMs). The Open Source\nInitiative (OSI) has recently released its first formal definition of\nopen-source software. This definition, when combined with standard dictionary\ndefinitions and the sparse published literature, provide an initial framework\nto support broader accessibility to AI models such as LLMs, but more work is\nessential to capture the unique dynamics of openness in AI. In addition,\nconcerns about open-washing, where models claim openness but lack full\ntransparency, has been raised, which limits the reproducibility, bias\nmitigation, and domain adaptation of these models. In this context, our study\ncritically analyzes SoTA LLMs from the last five years, including ChatGPT,\nDeepSeek, LLaMA, and others, to assess their adherence to transparency\nstandards and the implications of partial openness. Specifically, we examine\ntransparency and accessibility from two perspectives: open-source vs.\nopen-weight models. Our findings reveal that while some models are labeled as\nopen-source, this does not necessarily mean they are fully open-sourced. Even\nin the best cases, open-source models often do not report model training data,\nand code as well as key metrics, such as weight accessibility, and carbon\nemissions. To the best of our knowledge, this is the first study that\nsystematically examines the transparency and accessibility of over 100\ndifferent SoTA LLMs through the dual lens of open-source and open-weight\nmodels. The findings open avenues for further research and call for responsible\nand sustainable AI practices to ensure greater transparency, accountability,\nand ethical deployment of these models.(DeepSeek transparency, ChatGPT\naccessibility, open source, DeepSeek open source)",
    "published_date": "2025-02-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.18505v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.16456v1",
    "title": "Position: Beyond Assistance -- Reimagining LLMs as Ethical and Adaptive Co-Creators in Mental Health Care",
    "authors": [
      "Abeer Badawi",
      "Md Tahmid Rahman Laskar",
      "Jimmy Xiangji Huang",
      "Shaina Raza",
      "Elham Dolatabadi"
    ],
    "author_ids": [],
    "abstract": "This position paper argues for a fundamental shift in how Large Language\nModels (LLMs) are integrated into the mental health care domain. We advocate\nfor their role as co-creators rather than mere assistive tools. While LLMs have\nthe potential to enhance accessibility, personalization, and crisis\nintervention, their adoption remains limited due to concerns about bias,\nevaluation, over-reliance, dehumanization, and regulatory uncertainties. To\naddress these challenges, we propose two structured pathways: SAFE-i\n(Supportive, Adaptive, Fair, and Ethical Implementation) Guidelines for ethical\nand responsible deployment, and HAAS-e (Human-AI Alignment and Safety\nEvaluation) Framework for multidimensional, human-centered assessment. SAFE-i\nprovides a blueprint for data governance, adaptive model engineering, and\nreal-world integration, ensuring LLMs align with clinical and ethical\nstandards. HAAS-e introduces evaluation metrics that go beyond technical\naccuracy to measure trustworthiness, empathy, cultural sensitivity, and\nactionability. We call for the adoption of these structured approaches to\nestablish a responsible and scalable model for LLM-driven mental health\nsupport, ensuring that AI complements-rather than replaces-human expertise.",
    "published_date": "2025-02-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.16456v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.15939v1",
    "title": "\"Kya family planning after marriage hoti hai?\": Integrating Cultural Sensitivity in an LLM Chatbot for Reproductive Health",
    "authors": [
      "Roshini Deva",
      "Dhruv Ramani",
      "Tanvi Divate",
      "Suhani Jalota",
      "Azra Ismail"
    ],
    "author_ids": [],
    "abstract": "Access to sexual and reproductive health information remains a challenge in\nmany communities globally, due to cultural taboos and limited availability of\nhealthcare providers. Public health organizations are increasingly turning to\nLarge Language Models (LLMs) to improve access to timely and personalized\ninformation. However, recent HCI scholarship indicates that significant\nchallenges remain in incorporating context awareness and mitigating bias in\nLLMs. In this paper, we study the development of a culturally-appropriate\nLLM-based chatbot for reproductive health with underserved women in urban\nIndia. Through user interactions, focus groups, and interviews with multiple\nstakeholders, we examine the chatbot's response to sensitive and highly\ncontextual queries on reproductive health. Our findings reveal strengths and\nlimitations of the system in capturing local context, and complexities around\nwhat constitutes \"culture\". Finally, we discuss how local context might be\nbetter integrated, and present a framework to inform the design of\nculturally-sensitive chatbots for community health.",
    "published_date": "2025-02-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.15939v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.15913v1",
    "title": "Connecting the geometry and dynamics of many-body complex systems with message passing neural operators",
    "authors": [
      "Nicholas A. Gabriel",
      "Neil F. Johnson",
      "George Em Karniadakis"
    ],
    "author_ids": [],
    "abstract": "The relationship between scale transformations and dynamics established by\nrenormalization group techniques is a cornerstone of modern physical theories,\nfrom fluid mechanics to elementary particle physics. Integrating\nrenormalization group methods into neural operators for many-body complex\nsystems could provide a foundational inductive bias for learning their\neffective dynamics, while also uncovering multiscale organization. We introduce\na scalable AI framework, ROMA (Renormalized Operators with Multiscale\nAttention), for learning multiscale evolution operators of many-body complex\nsystems. In particular, we develop a renormalization procedure based on neural\nanalogs of the geometric and laplacian renormalization groups, which can be\nco-learned with neural operators. An attention mechanism is used to model\nmultiscale interactions by connecting geometric representations of local\nsubgraphs and dynamical operators. We apply this framework in challenging\nconditions: large systems of more than 1M nodes, long-range interactions, and\nnoisy input-output data for two contrasting examples: Kuramoto oscillators and\nBurgers-like social dynamics. We demonstrate that the ROMA framework improves\nscalability and positive transfer between forecasting and effective dynamics\ntasks compared to state-of-the-art operator learning techniques, while also\ngiving insight into multiscale interactions. Additionally, we investigate power\nlaw scaling in the number of model parameters, and demonstrate a departure from\ntypical power law exponents in the presence of hierarchical and multiscale\ninteractions.",
    "published_date": "2025-02-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.15913v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.15890v1",
    "title": "Efficient Estimation of Shortest-Path Distance Distributions to Samples in Graphs",
    "authors": [
      "Alan Zhu",
      "Jiaqi Ma",
      "Qiaozhu Mei"
    ],
    "author_ids": [],
    "abstract": "As large graph datasets become increasingly common across many fields,\nsampling is often needed to reduce the graphs into manageable sizes. This\nprocedure raises critical questions about representativeness as no sample can\ncapture the properties of the original graph perfectly, and different parts of\nthe graph are not evenly affected by the loss. Recent work has shown that the\ndistances from the non-sampled nodes to the sampled nodes can be a quantitative\nindicator of bias and fairness in graph machine learning. However, to our\nknowledge, there is no method for evaluating how a sampling method affects the\ndistribution of shortest-path distances without actually performing the\nsampling and shortest-path calculation.\n  In this paper, we present an accurate and efficient framework for estimating\nthe distribution of shortest-path distances to the sample, applicable to a wide\nrange of sampling methods and graph structures. Our framework is faster than\nempirical methods and only requires the specification of degree distributions.\nWe also extend our framework to handle graphs with community structures. While\nthis introduces a decrease in accuracy, we demonstrate that our framework\nremains highly accurate on downstream comparison-based tasks. Code is publicly\navailable at https://github.com/az1326/shortest_paths.",
    "published_date": "2025-02-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.15890v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.15871v1",
    "title": "A Comprehensive Survey on the Trustworthiness of Large Language Models in Healthcare",
    "authors": [
      "Manar Aljohani",
      "Jun Hou",
      "Sindhura Kommu",
      "Xuan Wang"
    ],
    "author_ids": [],
    "abstract": "The application of large language models (LLMs) in healthcare has the\npotential to revolutionize clinical decision-making, medical research, and\npatient care. As LLMs are increasingly integrated into healthcare systems,\nseveral critical challenges must be addressed to ensure their reliable and\nethical deployment. These challenges include truthfulness, where models\ngenerate misleading information; privacy, with risks of unintentional data\nretention; robustness, requiring defenses against adversarial attacks;\nfairness, addressing biases in clinical outcomes; explainability, ensuring\ntransparent decision-making; and safety, mitigating risks of misinformation and\nmedical errors. Recently, researchers have begun developing benchmarks and\nevaluation frameworks to systematically assess the trustworthiness of LLMs.\nHowever, the trustworthiness of LLMs in healthcare remains underexplored,\nlacking a systematic review that provides a comprehensive understanding and\nfuture insights into this area. This survey bridges this gap by providing a\ncomprehensive overview of the recent research of existing methodologies and\nsolutions aimed at mitigating the above risks in healthcare. By focusing on key\ntrustworthiness dimensions including truthfulness, privacy and safety,\nrobustness, fairness and bias, and explainability, we present a thorough\nanalysis of how these issues impact the reliability and ethical use of LLMs in\nhealthcare. This paper highlights ongoing efforts and offers insights into\nfuture research directions to ensure the safe and trustworthy deployment of\nLLMs in healthcare.",
    "published_date": "2025-02-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.15871v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.15600v1",
    "title": "Robust Bias Detection in MLMs and its Application to Human Trait Ratings",
    "authors": [
      "Ingroj Shrestha",
      "Louis Tay",
      "Padmini Srinivasan"
    ],
    "author_ids": [],
    "abstract": "There has been significant prior work using templates to study bias against\ndemographic attributes in MLMs. However, these have limitations: they overlook\nrandom variability of templates and target concepts analyzed, assume equality\namongst templates, and overlook bias quantification. Addressing these, we\npropose a systematic statistical approach to assess bias in MLMs, using mixed\nmodels to account for random effects, pseudo-perplexity weights for sentences\nderived from templates and quantify bias using statistical effect sizes.\nReplicating prior studies, we match on bias scores in magnitude and direction\nwith small to medium effect sizes. Next, we explore the novel problem of gender\nbias in the context of $\\textit{personality}$ and $\\textit{character}$ traits,\nacross seven MLMs (base and large). We find that MLMs vary; ALBERT is unbiased\nfor binary gender but the most biased for non-binary $\\textit{neo}$, while\nRoBERTa-large is the most biased for binary gender but shows small to no bias\nfor $\\textit{neo}$. There is some alignment of MLM bias and findings in\npsychology (human perspective) - in $\\textit{agreeableness}$ with RoBERTa-large\nand $\\textit{emotional stability}$ with BERT-large. There is general agreement\nfor the remaining 3 personality dimensions: both sides observe at most small\ndifferences across gender. For character traits, human studies on gender bias\nare limited thus comparisons are not feasible.",
    "published_date": "2025-02-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.15600v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.15576v1",
    "title": "Interpreting and Steering LLMs with Mutual Information-based Explanations on Sparse Autoencoders",
    "authors": [
      "Xuansheng Wu",
      "Jiayi Yuan",
      "Wenlin Yao",
      "Xiaoming Zhai",
      "Ninghao Liu"
    ],
    "author_ids": [],
    "abstract": "Large language models (LLMs) excel at handling human queries, but they can\noccasionally generate flawed or unexpected responses. Understanding their\ninternal states is crucial for understanding their successes, diagnosing their\nfailures, and refining their capabilities. Although sparse autoencoders (SAEs)\nhave shown promise for interpreting LLM internal representations, limited\nresearch has explored how to better explain SAE features, i.e., understanding\nthe semantic meaning of features learned by SAE. Our theoretical analysis\nreveals that existing explanation methods suffer from the frequency bias issue,\nwhere they emphasize linguistic patterns over semantic concepts, while the\nlatter is more critical to steer LLM behaviors. To address this, we propose\nusing a fixed vocabulary set for feature interpretations and designing a mutual\ninformation-based objective, aiming to better capture the semantic meaning\nbehind these features. We further propose two runtime steering strategies that\nadjust the learned feature activations based on their corresponding\nexplanations. Empirical results show that, compared to baselines, our method\nprovides more discourse-level explanations and effectively steers LLM behaviors\nto defend against jailbreak attacks. These findings highlight the value of\nexplanations for steering LLM behaviors in downstream applications. We will\nrelease our code and data once accepted.",
    "published_date": "2025-02-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.15576v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.15420v2",
    "title": "Shapley Value-based Approach for Redistributing Revenue of Matchmaking of Private Transactions in Blockchains",
    "authors": [
      "Rasheed",
      "Parth Desai",
      "Yash Chaurasia",
      "Sujit Gujar"
    ],
    "author_ids": [],
    "abstract": "In the context of blockchain, MEV refers to the maximum value that can be\nextracted from block production through the inclusion, exclusion, or reordering\nof transactions. Searchers often participate in order flow auctions (OFAs) to\nobtain exclusive rights to private transactions, available through entities\ncalled matchmakers, also known as order flow providers (OFPs). Most often,\nredistributing the revenue generated through such auctions among transaction\ncreators is desirable. In this work, we formally introduce the matchmaking\nproblem in MEV, its desirable properties, and associated challenges. Using\ncooperative game theory, we formalize the notion of fair revenue redistribution\nin matchmaking and present its potential possibilities and impossibilities.\nPrecisely, we define a characteristic form game, referred to as RST-Game, for\nthe transaction creators. We propose to redistribute the revenue using the\nShapley value of RST-Game. We show that the corresponding problem could be\nSUBEXP (i.e. $2^{o(n)}$, where $n$ is the number of transactions); therefore,\napproximating the Shapley value is necessary. Further, we propose a randomized\nalgorithm for computing the Shapley value in RST-Game and empirically verify\nits efficacy.",
    "published_date": "2025-02-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.15420v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.15403v1",
    "title": "Evaluate with the Inverse: Efficient Approximation of Latent Explanation Quality Distribution",
    "authors": [
      "Carlos Eiras-Franco",
      "Anna Hedström",
      "Marina M. -C. Höhne"
    ],
    "author_ids": [],
    "abstract": "Obtaining high-quality explanations of a model's output enables developers to\nidentify and correct biases, align the system's behavior with human values, and\nensure ethical compliance. Explainable Artificial Intelligence (XAI)\npractitioners rely on specific measures to gauge the quality of such\nexplanations. These measures assess key attributes, such as how closely an\nexplanation aligns with a model's decision process (faithfulness), how\naccurately it pinpoints the relevant input features (localization), and its\nconsistency across different cases (robustness). Despite providing valuable\ninformation, these measures do not fully address a critical practitioner's\nconcern: how does the quality of a given explanation compare to other potential\nexplanations? Traditionally, the quality of an explanation has been assessed by\ncomparing it to a randomly generated counterpart. This paper introduces an\nalternative: the Quality Gap Estimate (QGE). The QGE method offers a direct\ncomparison to what can be viewed as the `inverse' explanation, one that\nconceptually represents the antithesis of the original explanation. Our\nextensive testing across multiple model architectures, datasets, and\nestablished quality metrics demonstrates that the QGE method is superior to the\ntraditional approach. Furthermore, we show that QGE enhances the statistical\nreliability of these quality assessments. This advance represents a significant\nstep toward a more insightful evaluation of explanations that enables a more\neffective inspection of a model's behavior.",
    "published_date": "2025-02-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.15403v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.15860v2",
    "title": "Synthetic vs. Gold: The Role of LLM-Generated Labels and Data in Cyberbullying Detection",
    "authors": [
      "Arefeh Kazemi",
      "Sri Balaaji Natarajan Kalaivendan",
      "Joachim Wagner",
      "Hamza Qadeer",
      "Brian Davis"
    ],
    "author_ids": [],
    "abstract": "Cyberbullying (CB) presents a pressing threat, especially to children,\nunderscoring the urgent need for robust detection systems to ensure online\nsafety. However, progress in developing such systems is hindered by the\nscarcity of large, labeled datasets that are specifically tailored for\nspecialized tasks and the target age groups. Creating these datasets relies\nheavily on human annotation, which not only strains resources but also raises\nsignificant ethical and legal concerns due to annotators' exposure to harmful\ncontent, notwithstanding the acquisition of this type of data from vulnerable\npopulations such as children. In this paper, we address these challenges by\nleveraging Large Language Models (LLMs) to generate synthetic data and labels.\nOur experiments demonstrate that synthetic data enables BERT-based CB\nclassifiers to achieve performance close to that of those trained on fully\nauthentic datasets (75.8% vs. 81.5% accuracy). Additionally, LLMs can\neffectively label authentic yet unlabeled data, allowing BERT classifiers to\nattain a comparable performance level (79.1% vs. 81.5% accuracy). These results\nhighlight the potential of LLMs as a scalable, ethical, and cost-effective\nsolution for generating data for CB detection.",
    "published_date": "2025-02-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.15860v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.15859v3",
    "title": "AI Governance InternationaL Evaluation Index (AGILE Index)",
    "authors": [
      "Yi Zeng",
      "Enmeng Lu",
      "Xin Guan",
      "Cunqing Huangfu",
      "Zizhe Ruan",
      "Ammar Younas",
      "Kang Sun",
      "Xuan Tang",
      "Yuwei Wang",
      "Hongjie Suo",
      "Dongqi Liang",
      "Zhengqiang Han",
      "Aorigele Bao",
      "Xiaoyang Guo",
      "Jin Wang",
      "Jiawei Xie",
      "Yao Liang"
    ],
    "author_ids": [],
    "abstract": "The rapid advancement of Artificial Intelligence (AI) technology is\nprofoundly transforming human society and concurrently presenting a series of\nethical, legal, and social issues. The effective governance of AI has become a\ncrucial global concern. Since 2022, the extensive deployment of generative AI,\nparticularly large language models, marked a new phase in AI governance.\nContinuous efforts are being made by the international community in actively\naddressing the novel challenges posed by these AI developments. As consensus on\ninternational governance continues to be established and put into action, the\npractical importance of conducting a global assessment of the state of AI\ngovernance is progressively coming to light. In this context, we initiated the\ndevelopment of the AI Governance InternationaL Evaluation Index (AGILE Index).\nAdhering to the design principle, \"the level of governance should match the\nlevel of development,\" the inaugural evaluation of the AGILE Index commences\nwith an exploration of four foundational pillars: the development level of AI,\nthe AI governance environment, the AI governance instruments, and the AI\ngovernance effectiveness. It covers 39 indicators across 18 dimensions to\ncomprehensively assess the AI governance level of 14 representative countries\nglobally. The index is utilized to delve into the status of AI governance to\ndate in 14 countries for the first batch of evaluation. The aim is to depict\nthe current state of AI governance in these countries through data scoring,\nassist them in identifying their governance stage and uncovering governance\nissues, and ultimately offer insights for the enhancement of their AI\ngovernance systems.",
    "published_date": "2025-02-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "68T01",
      "A.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.15859v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.15361v1",
    "title": "Evaluating Social Biases in LLM Reasoning",
    "authors": [
      "Xuyang Wu",
      "Jinming Nian",
      "Zhiqiang Tao",
      "Yi Fang"
    ],
    "author_ids": [],
    "abstract": "In the recent development of AI reasoning, large language models (LLMs) are\ntrained to automatically generate chain-of-thought reasoning steps, which have\ndemonstrated compelling performance on math and coding tasks. However, when\nbias is mixed within the reasoning process to form strong logical arguments, it\ncould cause even more harmful results and further induce hallucinations. In\nthis paper, we have evaluated the 8B and 32B variants of DeepSeek-R1 against\ntheir instruction tuned counterparts on the BBQ dataset, and investigated the\nbias that is elicited out and being amplified through reasoning steps. To the\nbest of our knowledge, this empirical study is the first to assess bias issues\nin LLM reasoning.",
    "published_date": "2025-02-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.15361v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.15240v1",
    "title": "Multi-agent Multi-armed Bandits with Minimum Reward Guarantee Fairness",
    "authors": [
      "Piyushi Manupriya",
      "Himanshu",
      "SakethaNath Jagarlapudi",
      "Ganesh Ghalme"
    ],
    "author_ids": [],
    "abstract": "We investigate the problem of maximizing social welfare while ensuring\nfairness in a multi-agent multi-armed bandit (MA-MAB) setting. In this problem,\na centralized decision-maker takes actions over time, generating random rewards\nfor various agents. Our goal is to maximize the sum of expected cumulative\nrewards, a.k.a. social welfare, while ensuring that each agent receives an\nexpected reward that is at least a constant fraction of the maximum possible\nexpected reward.\n  Our proposed algorithm, RewardFairUCB, leverages the Upper Confidence Bound\n(UCB) technique to achieve sublinear regret bounds for both fairness and social\nwelfare. The fairness regret measures the positive difference between the\nminimum reward guarantee and the expected reward of a given policy, whereas the\nsocial welfare regret measures the difference between the social welfare of the\noptimal fair policy and that of the given policy.\n  We show that RewardFairUCB algorithm achieves instance-independent social\nwelfare regret guarantees of $\\tilde{O}(T^{1/2})$ and a fairness regret upper\nbound of $\\tilde{O}(T^{3/4})$. We also give the lower bound of\n$\\Omega(\\sqrt{T})$ for both social welfare and fairness regret. We evaluate\nRewardFairUCB's performance against various baseline and heuristic algorithms\nusing simulated data and real world data, highlighting trade-offs between\nfairness and social welfare regrets.",
    "published_date": "2025-02-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.15240v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.15137v1",
    "title": "Don't Confuse! Redrawing GUI Navigation Flow in Mobile Apps for Visually Impaired Users",
    "authors": [
      "Mengxi Zhang",
      "Huaxiao Liu",
      "Yuheng Zhou",
      "Chunyang Chen",
      "Pei Huang",
      "Jian Zhao"
    ],
    "author_ids": [],
    "abstract": "Mobile applications (apps) are integral to our daily lives, offering diverse\nservices and functionalities. They enable sighted users to access information\ncoherently in an extremely convenient manner. However, it remains unclear if\nvisually impaired users, who rely solely on the screen readers (e.g., Talkback)\nto navigate and access app information, can do so in the correct and reasonable\norder. This may result in significant information bias and operational errors.\nConsidering these issues, in this work, we proposed a method named RGNF\n(Re-draw GUI Navigation Flow). It aimed to enhance the understandability and\ncoherence of accessing the content of each component within the Graphical User\nInterface (GUI), together with assisting developers in creating well-designed\nGUI navigation flow (GNF). This method was inspired by the characteristics\nidentified in our preliminary study, where visually impaired users expected\nnavigation to be associated with close position and similar shape of GUI\ncomponents that were read consecutively. Thus, our method relied on the\nprinciples derived from the Gestalt psychological model, aiming to group GUI\ncomponents into different regions according to the laws of proximity and\nsimilarity, thereby redrawing the GNFs. To evaluate the effectiveness of our\nmethod, we calculated sequence similarity values before and after redrawing the\nGNF, and further employed the tools proposed by Alotaibi et al. to measure the\nreachability of GUI components. Our results demonstrated a substantial\nimprovement in similarity (0.921) compared to the baseline (0.624), together\nwith the reachability (90.31%) compared to the baseline GNF (74.35%).\nFurthermore, a qualitative user study revealed that our method had a positive\neffect on providing visually impaired users with an improved user experience.",
    "published_date": "2025-02-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.15137v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.14996v1",
    "title": "A Rapid Test for Accuracy and Bias of Face Recognition Technology",
    "authors": [
      "Manuel Knott",
      "Ignacio Serna",
      "Ethan Mann",
      "Pietro Perona"
    ],
    "author_ids": [],
    "abstract": "Measuring the accuracy of face recognition (FR) systems is essential for\nimproving performance and ensuring responsible use. Accuracy is typically\nestimated using large annotated datasets, which are costly and difficult to\nobtain. We propose a novel method for 1:1 face verification that benchmarks FR\nsystems quickly and without manual annotation, starting from approximate labels\n(e.g., from web search results). Unlike previous methods for training set label\ncleaning, ours leverages the embedding representation of the models being\nevaluated, achieving high accuracy in smaller-sized test datasets. Our approach\nreliably estimates FR accuracy and ranking, significantly reducing the time and\ncost of manual labeling. We also introduce the first public benchmark of five\nFR cloud services, revealing demographic biases, particularly lower accuracy\nfor Asian women. Our rapid test method can democratize FR testing, promoting\nscrutiny and responsible use of the technology. Our method is provided as a\npublicly accessible tool at https://github.com/caltechvisionlab/frt-rapid-test",
    "published_date": "2025-02-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.14996v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.14827v2",
    "title": "Exploring Advanced Techniques for Visual Question Answering: A Comprehensive Comparison",
    "authors": [
      "Aiswarya Baby",
      "Tintu Thankom Koshy"
    ],
    "author_ids": [],
    "abstract": "Visual Question Answering (VQA) has emerged as a pivotal task in the\nintersection of computer vision and natural language processing, requiring\nmodels to understand and reason about visual content in response to natural\nlanguage questions. Analyzing VQA datasets is essential for developing robust\nmodels that can handle the complexities of multimodal reasoning. Several\napproaches have been developed to examine these datasets, each offering\ndistinct perspectives on question diversity, answer distribution, and\nvisual-textual correlations. Despite significant progress, existing VQA models\nface challenges related to dataset bias, limited model complexity, commonsense\nreasoning gaps, rigid evaluation methods, and generalization to real world\nscenarios. This paper offers a detailed study of the original VQA dataset,\nbaseline models and methods along with a comparative study of five advanced VQA\nmodels, ABC-CNN, KICNLE, Masked Vision and Language Modeling, BLIP-2, and OFA,\neach employing distinct methods to address these ongoing challenges.",
    "published_date": "2025-02-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.ET",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.14827v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.05747v1",
    "title": "Balancing Innovation and Integrity: AI Integration in Liberal Arts College Administration",
    "authors": [
      "Ian Olivo Read"
    ],
    "author_ids": [],
    "abstract": "This paper explores the intersection of artificial intelligence and higher\neducation administration, focusing on liberal arts colleges (LACs). It examines\nAI's opportunities and challenges in academic and student affairs, legal\ncompliance, and accreditation processes, while also addressing the ethical\nconsiderations of AI deployment in mission-driven institutions. Considering\nAI's value pluralism and potential allocative or representational harms caused\nby algorithmic bias, LACs must ensure AI aligns with its mission and\nprinciples. The study highlights other strategies for responsible AI\nintegration, balancing innovation with institutional values.",
    "published_date": "2025-02-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "I.2.0; I.2.7; K.3.1; J.1; K.4.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.05747v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.14689v1",
    "title": "Confidence Estimation via Sequential Likelihood Mixing",
    "authors": [
      "Johannes Kirschner",
      "Andreas Krause",
      "Michele Meziu",
      "Mojmir Mutny"
    ],
    "author_ids": [],
    "abstract": "We present a universal framework for constructing confidence sets based on\nsequential likelihood mixing. Building upon classical results from sequential\nanalysis, we provide a unifying perspective on several recent lines of work,\nand establish fundamental connections between sequential mixing, Bayesian\ninference and regret inequalities from online estimation. The framework applies\nto any realizable family of likelihood functions and allows for non-i.i.d. data\nand anytime validity. Moreover, the framework seamlessly integrates standard\napproximate inference techniques, such as variational inference and\nsampling-based methods, and extends to misspecified model classes, while\npreserving provable coverage guarantees. We illustrate the power of the\nframework by deriving tighter confidence sequences for classical settings,\nincluding sequential linear regression and sparse estimation, with simplified\nproofs.",
    "published_date": "2025-02-20T00:00:00",
    "year": 2025,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.14689v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.14653v1",
    "title": "Gender Influence on Student Teams' Online Communication in Software Engineering Education",
    "authors": [
      "Rita Garcia",
      "Christoph Treude"
    ],
    "author_ids": [],
    "abstract": "Collaboration is crucial in Software Engineering (SE), yet factors like\ngender bias can shape team dynamics and behaviours. This study examines an\neight-week project involving 39 SE students across eight teams contributing to\nGitHub projects. Using a mixed-methods approach, we analysed Slack\ncommunications to identify gender differences, comparing how they influence\nlearning gains. We found higher help-seeking and leadership behaviours in the\nall-woman team, while men responded more slowly. Although communication did not\naffect final grades, we identified statistical significance correlating\ncommunications with students' understanding of software development. With some\nstudents putting more effort into collaboration, future work can investigate\ndiversity and inclusion training to balance these efforts. The observed link\nbetween team engagement and a higher understanding of software development\nhighlights the potential for teaching strategies that promote help-seeking.\nThese findings could guide efforts to address challenges student SE teams face\nwhen using communication platforms and foster more equitable collaborative\nlearning in Software Engineering Education.",
    "published_date": "2025-02-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.14653v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.14394v1",
    "title": "Enhancing Portuguese Variety Identification with Cross-Domain Approaches",
    "authors": [
      "Hugo Sousa",
      "Rúben Almeida",
      "Purificação Silvano",
      "Inês Cantante",
      "Ricardo Campos",
      "Alípio Jorge"
    ],
    "author_ids": [],
    "abstract": "Recent advances in natural language processing have raised expectations for\ngenerative models to produce coherent text across diverse language varieties.\nIn the particular case of the Portuguese language, the predominance of\nBrazilian Portuguese corpora online introduces linguistic biases in these\nmodels, limiting their applicability outside of Brazil. To address this gap and\npromote the creation of European Portuguese resources, we developed a\ncross-domain language variety identifier (LVI) to discriminate between European\nand Brazilian Portuguese. Motivated by the findings of our literature review,\nwe compiled the PtBrVarId corpus, a cross-domain LVI dataset, and study the\neffectiveness of transformer-based LVI classifiers for cross-domain scenarios.\nAlthough this research focuses on two Portuguese varieties, our contribution\ncan be extended to other varieties and languages. We open source the code,\ncorpus, and models to foster further research in this task.",
    "published_date": "2025-02-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.14394v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.14371v1",
    "title": "Asymptotic Existence of Class Envy-free Matchings",
    "authors": [
      "Tomohiko Yokoyama",
      "Ayumi Igarashi"
    ],
    "author_ids": [],
    "abstract": "We consider a one-sided matching problem where agents who are partitioned\ninto disjoint classes and each class must receive fair treatment in a desired\nmatching. This model, proposed by Benabbou et al. [2019], aims to address\nvarious real-life scenarios, such as the allocation of public housing and\nmedical resources across different ethnic, age, and other demographic groups.\nOur focus is on achieving class envy-free matchings, where each class receives\na total utility at least as large as the maximum value of a matching they would\nachieve from the items matched to another class. While class envy-freeness for\nworst-case utilities is unattainable without leaving some valuable items\nunmatched, such extreme cases may rarely occur in practice. To analyze the\nexistence of a class envy-free matching in practice, we study a distributional\nmodel where agents' utilities for items are drawn from a probability\ndistribution. Our main result establishes the asymptotic existence of a desired\nmatching, showing that a round-robin algorithm produces a class envy-free\nmatching as the number of agents approaches infinity.",
    "published_date": "2025-02-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.14371v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.14296v1",
    "title": "On the Trustworthiness of Generative Foundation Models: Guideline, Assessment, and Perspective",
    "authors": [
      "Yue Huang",
      "Chujie Gao",
      "Siyuan Wu",
      "Haoran Wang",
      "Xiangqi Wang",
      "Yujun Zhou",
      "Yanbo Wang",
      "Jiayi Ye",
      "Jiawen Shi",
      "Qihui Zhang",
      "Yuan Li",
      "Han Bao",
      "Zhaoyi Liu",
      "Tianrui Guan",
      "Dongping Chen",
      "Ruoxi Chen",
      "Kehan Guo",
      "Andy Zou",
      "Bryan Hooi Kuen-Yew",
      "Caiming Xiong",
      "Elias Stengel-Eskin",
      "Hongyang Zhang",
      "Hongzhi Yin",
      "Huan Zhang",
      "Huaxiu Yao",
      "Jaehong Yoon",
      "Jieyu Zhang",
      "Kai Shu",
      "Kaijie Zhu",
      "Ranjay Krishna",
      "Swabha Swayamdipta",
      "Taiwei Shi",
      "Weijia Shi",
      "Xiang Li",
      "Yiwei Li",
      "Yuexing Hao",
      "Yuexing Hao",
      "Zhihao Jia",
      "Zhize Li",
      "Xiuying Chen",
      "Zhengzhong Tu",
      "Xiyang Hu",
      "Tianyi Zhou",
      "Jieyu Zhao",
      "Lichao Sun",
      "Furong Huang",
      "Or Cohen Sasson",
      "Prasanna Sattigeri",
      "Anka Reuel",
      "Max Lamparth",
      "Yue Zhao",
      "Nouha Dziri",
      "Yu Su",
      "Huan Sun",
      "Heng Ji",
      "Chaowei Xiao",
      "Mohit Bansal",
      "Nitesh V. Chawla",
      "Jian Pei",
      "Jianfeng Gao",
      "Michael Backes",
      "Philip S. Yu",
      "Neil Zhenqiang Gong",
      "Pin-Yu Chen",
      "Bo Li",
      "Xiangliang Zhang"
    ],
    "author_ids": [],
    "abstract": "Generative Foundation Models (GenFMs) have emerged as transformative tools.\nHowever, their widespread adoption raises critical concerns regarding\ntrustworthiness across dimensions. This paper presents a comprehensive\nframework to address these challenges through three key contributions. First,\nwe systematically review global AI governance laws and policies from\ngovernments and regulatory bodies, as well as industry practices and standards.\nBased on this analysis, we propose a set of guiding principles for GenFMs,\ndeveloped through extensive multidisciplinary collaboration that integrates\ntechnical, ethical, legal, and societal perspectives. Second, we introduce\nTrustGen, the first dynamic benchmarking platform designed to evaluate\ntrustworthiness across multiple dimensions and model types, including\ntext-to-image, large language, and vision-language models. TrustGen leverages\nmodular components--metadata curation, test case generation, and contextual\nvariation--to enable adaptive and iterative assessments, overcoming the\nlimitations of static evaluation methods. Using TrustGen, we reveal significant\nprogress in trustworthiness while identifying persistent challenges. Finally,\nwe provide an in-depth discussion of the challenges and future directions for\ntrustworthy GenFMs, which reveals the complex, evolving nature of\ntrustworthiness, highlighting the nuanced trade-offs between utility and\ntrustworthiness, and consideration for various downstream applications,\nidentifying persistent challenges and providing a strategic roadmap for future\nresearch. This work establishes a holistic framework for advancing\ntrustworthiness in GenAI, paving the way for safer and more responsible\nintegration of GenFMs into critical applications. To facilitate advancement in\nthe community, we release the toolkit for dynamic evaluation.",
    "published_date": "2025-02-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.14296v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.14219v1",
    "title": "Investigating the Impact of LLM Personality on Cognitive Bias Manifestation in Automated Decision-Making Tasks",
    "authors": [
      "Jiangen He",
      "Jiqun Liu"
    ],
    "author_ids": [],
    "abstract": "Large Language Models (LLMs) are increasingly used in decision-making, yet\ntheir susceptibility to cognitive biases remains a pressing challenge. This\nstudy explores how personality traits influence these biases and evaluates the\neffectiveness of mitigation strategies across various model architectures. Our\nfindings identify six prevalent cognitive biases, while the sunk cost and group\nattribution biases exhibit minimal impact. Personality traits play a crucial\nrole in either amplifying or reducing biases, significantly affecting how LLMs\nrespond to debiasing techniques. Notably, Conscientiousness and Agreeableness\nmay generally enhance the efficacy of bias mitigation strategies, suggesting\nthat LLMs exhibiting these traits are more receptive to corrective measures.\nThese findings address the importance of personality-driven bias dynamics and\nhighlight the need for targeted mitigation approaches to improve fairness and\nreliability in AI-assisted decision-making.",
    "published_date": "2025-02-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.14219v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.14214v1",
    "title": "Asymmetric Co-Training for Source-Free Few-Shot Domain Adaptation",
    "authors": [
      "Gengxu Li",
      "Yuan Wu"
    ],
    "author_ids": [],
    "abstract": "Source-free unsupervised domain adaptation (SFUDA) has gained significant\nattention as an alternative to traditional unsupervised domain adaptation\n(UDA), which relies on the constant availability of labeled source data.\nHowever, SFUDA approaches come with inherent limitations that are frequently\noverlooked. These challenges include performance degradation when the unlabeled\ntarget data fails to meet critical assumptions, such as having a closed-set\nlabel distribution identical to that of the source domain, or when sufficient\nunlabeled target data is unavailable-a common situation in real-world\napplications. To address these issues, we propose an asymmetric co-training\n(ACT) method specifically designed for the SFFSDA scenario. SFFSDA presents a\nmore practical alternative to SFUDA, as gathering a few labeled target\ninstances is more feasible than acquiring large volumes of unlabeled target\ndata in many real-world contexts. Our ACT method begins by employing a\nweak-strong augmentation to enhance data diversity. Then we use a two-step\noptimization process to train the target model. In the first step, we optimize\nthe label smoothing cross-entropy loss, the entropy of the class-conditional\ndistribution, and the reverse-entropy loss to bolster the model's\ndiscriminative ability while mitigating overfitting. The second step focuses on\nreducing redundancy in the output space by minimizing classifier determinacy\ndisparity. Extensive experiments across four benchmarks demonstrate the\nsuperiority of our ACT approach, which outperforms state-of-the-art SFUDA\nmethods and transfer learning techniques. Our findings suggest that adapting a\nsource pre-trained model using only a small amount of labeled target data\noffers a practical and dependable solution. The code is available at\nhttps://github.com/gengxuli/ACT.",
    "published_date": "2025-02-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.14214v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.14206v1",
    "title": "Adaptive Mesh Refinement for Variational Inequalities",
    "authors": [
      "Giuliano Stefano Fochesatto"
    ],
    "author_ids": [],
    "abstract": "Variational inequalities play a pivotal role in a wide array of scientific\nand engineering applications. This project presents two techniques for adaptive\nmesh refinement (AMR) in the context of variational inequalities, with a\nspecific focus on the classical obstacle problem.\n  We propose two distinct AMR strategies: Variable Coefficient Elliptic\nSmoothing (VCES) and Unstructured Dilation Operator (UDO). VCES uses a nodal\nactive set indicator function as the initial iterate to a time-dependent heat\nequation problem. Solving a single step of this problem has the effect of\nsmoothing the indicator about the free boundary. We threshold this smoothed\nindicator function to identify elements near the free boundary. Key parameters\nsuch as timestep and threshold values significantly influence the efficacy of\nthis method.\n  The second strategy, UDO, focuses on the discrete identification of elements\nadjacent to the free boundary, employing a graph-based approach to mark\nneighboring elements for refinement. This technique resembles the dilation\nmorphological operation in image processing, but tailored for unstructured\nmeshes.\n  We also examine the theory of variational inequalities, the convergence\nbehavior of finite element solutions, and implementation in the Firedrake\nfinite element library. Convergence analysis reveals that accurate free\nboundary estimation is pivotal for solver performance. Numerical experiments\ndemonstrate the effectiveness of the proposed methods in dynamically enhancing\nmesh resolution around free boundaries, thereby improving the convergence rates\nand computational efficiency of variational inequality solvers. Our approach\nintegrates seamlessly with existing Firedrake numerical solvers, and it is\npromising for solving more complex free boundary problems.",
    "published_date": "2025-02-20T00:00:00",
    "year": 2025,
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.14206v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.14123v1",
    "title": "Understanding SGD with Exponential Moving Average: A Case Study in Linear Regression",
    "authors": [
      "Xuheng Li",
      "Quanquan Gu"
    ],
    "author_ids": [],
    "abstract": "Exponential moving average (EMA) has recently gained significant popularity\nin training modern deep learning models, especially diffusion-based generative\nmodels. However, there have been few theoretical results explaining the\neffectiveness of EMA. In this paper, to better understand EMA, we establish the\nrisk bound of online SGD with EMA for high-dimensional linear regression, one\nof the simplest overparameterized learning tasks that shares similarities with\nneural networks. Our results indicate that (i) the variance error of SGD with\nEMA is always smaller than that of SGD without averaging, and (ii) unlike SGD\nwith iterate averaging from the beginning, the bias error of SGD with EMA\ndecays exponentially in every eigen-subspace of the data covariance matrix.\nAdditionally, we develop proof techniques applicable to the analysis of a broad\nclass of averaging schemes.",
    "published_date": "2025-02-19T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.14123v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.14112v1",
    "title": "To Stand on the Shoulders of Giants: Should We Protect Initial Discoveries in Multi-Agent Exploration?",
    "authors": [
      "Hodaya Lampert",
      "Reshef Meir",
      "Kinneret Teodorescu"
    ],
    "author_ids": [],
    "abstract": "Exploring new ideas is a fundamental aspect of research and development\n(R\\&D), which often occurs in competitive environments. Most ideas are\nsubsequent, i.e. one idea today leads to more ideas tomorrow. According to one\napproach, the best way to encourage exploration is by granting protection on\ndiscoveries to the first innovator. Correspondingly, only the one who made the\nfirst discovery can use the new knowledge and benefit from subsequent\ndiscoveries, which in turn should increase the initial motivation to explore.\nAn alternative approach to promote exploration favors the \\emph{sharing of\nknowledge} from discoveries among researchers allowing explorers to use each\nothers' discoveries to develop further knowledge, as in the open-source\ncommunity. With no protection, all explorers have access to all existing\ndiscoveries and new directions are explored faster.\n  We present a game theoretic analysis of an abstract research-and-application\ngame which clarifies the expected advantages and disadvantages of the two\napproaches under full information. We then compare the theoretical predictions\nwith the observed behavior of actual players in the lab who operate under\npartial information conditions in both worlds.\n  Our main experimental finding is that the no protection approach leads to\n\\emph{more} investment efforts overall, in contrast to theoretical prediction\nand common economic wisdom, but in line with a familiar cognitive bias known as\n`underweighting of rare events'.",
    "published_date": "2025-02-19T00:00:00",
    "year": 2025,
    "categories": [
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.14112v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.14086v1",
    "title": "Navigating Semantic Relations: Challenges for Language Models in Abstract Common-Sense Reasoning",
    "authors": [
      "Cole Gawin",
      "Yidan Sun",
      "Mayank Kejriwal"
    ],
    "author_ids": [],
    "abstract": "Large language models (LLMs) have achieved remarkable performance in\ngenerating human-like text and solving reasoning tasks of moderate complexity,\nsuch as question-answering and mathematical problem-solving. However, their\ncapabilities in tasks requiring deeper cognitive skills, such as common-sense\nunderstanding and abstract reasoning, remain under-explored. In this paper, we\nsystematically evaluate abstract common-sense reasoning in LLMs using the\nConceptNet knowledge graph. We propose two prompting approaches: instruct\nprompting, where models predict plausible semantic relationships based on\nprovided definitions, and few-shot prompting, where models identify relations\nusing examples as guidance. Our experiments with the gpt-4o-mini model show\nthat in instruct prompting, consistent performance is obtained when ranking\nmultiple relations but with substantial decline when the model is restricted to\npredicting only one relation. In few-shot prompting, the model's accuracy\nimproves significantly when selecting from five relations rather than the full\nset, although with notable bias toward certain relations. These results suggest\nsignificant gaps still, even in commercially used LLMs' abstract common-sense\nreasoning abilities, compared to human-level understanding. However, the\nfindings also highlight the promise of careful prompt engineering, based on\nselective retrieval, for obtaining better performance.",
    "published_date": "2025-02-19T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.14086v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.14083v1",
    "title": "Are Rules Meant to be Broken? Understanding Multilingual Moral Reasoning as a Computational Pipeline with UniMoral",
    "authors": [
      "Shivani Kumar",
      "David Jurgens"
    ],
    "author_ids": [],
    "abstract": "Moral reasoning is a complex cognitive process shaped by individual\nexperiences and cultural contexts and presents unique challenges for\ncomputational analysis. While natural language processing (NLP) offers\npromising tools for studying this phenomenon, current research lacks cohesion,\nemploying discordant datasets and tasks that examine isolated aspects of moral\nreasoning. We bridge this gap with UniMoral, a unified dataset integrating\npsychologically grounded and social-media-derived moral dilemmas annotated with\nlabels for action choices, ethical principles, contributing factors, and\nconsequences, alongside annotators' moral and cultural profiles. Recognizing\nthe cultural relativity of moral reasoning, UniMoral spans six languages,\nArabic, Chinese, English, Hindi, Russian, and Spanish, capturing diverse\nsocio-cultural contexts. We demonstrate UniMoral's utility through a benchmark\nevaluations of three large language models (LLMs) across four tasks: action\nprediction, moral typology classification, factor attribution analysis, and\nconsequence generation. Key findings reveal that while implicitly embedded\nmoral contexts enhance the moral reasoning capability of LLMs, there remains a\ncritical need for increasingly specialized approaches to further advance moral\nreasoning in these models.",
    "published_date": "2025-02-19T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.14083v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.13934v1",
    "title": "Citation proximus: the role of social and semantic ties in citing behaviour",
    "authors": [
      "Diego Kozlowski",
      "Carolina Pradier",
      "Pierre Benz",
      "Natsumi Shokida",
      "Jens Peter Andersen",
      "Vincent Larivière"
    ],
    "author_ids": [],
    "abstract": "Citations are a key indicator of research impact but are shaped by factors\nbeyond intrinsic research quality, including prestige, social networks, and\nthematic similarity. While the Matthew Effect explains how prestige accumulates\nand influences citation distributions, our study contextualizes this by showing\nthat other mechanisms also play a crucial role. Analyzing a large dataset of\ndisambiguated authors (N=43,467) and citation linkages (N=264,436) in U.S.\neconomics, we find that close ties in the collaboration network are the\nstrongest predictor of citation, closely followed by thematic similarity\nbetween papers. This reinforces the idea that citations are not only a matter\nof prestige but mostly of social networks and intellectual proximity. Prestige\nremains important for understanding highly cited papers, but for the majority\nof citations, proximity--both social and semantic--plays a more significant\nrole. These findings shift attention from extreme cases of highly cited\nresearch toward the broader distribution of citations, which shapes career\ntrajectories and the production of knowledge. Recognizing the diverse factors\ninfluencing citations is critical for science policy, as this work highlights\ninequalities that are not based on preferential attachment, but on the role of\nself-citations, collaborations, and mainstream versus no mainstream research\nsubjects.",
    "published_date": "2025-02-19T00:00:00",
    "year": 2025,
    "categories": [
      "cs.DL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.13934v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.13874v2",
    "title": "The KnowWhereGraph: A Large-Scale Geo-Knowledge Graph for Interdisciplinary Knowledge Discovery and Geo-Enrichment",
    "authors": [
      "Rui Zhu",
      "Cogan Shimizu",
      "Shirly Stephen",
      "Colby K. Fisher",
      "Thomas Thelen",
      "Kitty Currier",
      "Krzysztof Janowicz",
      "Pascal Hitzler",
      "Mark Schildhauer",
      "Wenwen Li",
      "Dean Rehberger",
      "Adrita Barua",
      "Antrea Christou",
      "Ling Cai",
      "Abhilekha Dalal",
      "Anthony D'Onofrio",
      "Andrew Eells",
      "Mitchell Faulk",
      "Zilong Liu",
      "Gengchen Mai",
      "Mohammad Saeid Mahdavinejad",
      "Bryce Mecum",
      "Sanaz Saki Norouzi",
      "Meilin Shi",
      "Yuanyuan Tian",
      "Sizhe Wang",
      "Zhangyu Wang",
      "Joseph Zalewski"
    ],
    "author_ids": [],
    "abstract": "Global challenges such as food supply chain disruptions, public health\ncrises, and natural hazard responses require access to and integration of\ndiverse datasets, many of which are geospatial. Over the past few years, a\ngrowing number of (geo)portals have been developed to address this need.\nHowever, most existing (geo)portals are stacked by separated or sparsely\nconnected data \"silos\" impeding effective data consolidation. A new way of\nsharing and reusing geospatial data is therefore urgently needed. In this work,\nwe introduce KnowWhereGraph, a knowledge graph-based data integration,\nenrichment, and synthesis framework that not only includes schemas and data\nrelated to human and environmental systems but also provides a suite of\nsupporting tools for accessing this information. The KnowWhereGraph aims to\naddress the challenge of data integration by building a large-scale,\ncross-domain, pre-integrated, FAIR-principles-based, and AI-ready data\nwarehouse rooted in knowledge graphs. We highlight the design principles of\nKnowWhereGraph, emphasizing the roles of space, place, and time in bridging\nvarious data \"silos\". Additionally, we demonstrate multiple use cases where the\nproposed geospatial knowledge graph and its associated tools empower\ndecision-makers to uncover insights that are often hidden within complex and\npoorly interoperable datasets.",
    "published_date": "2025-02-19T00:00:00",
    "year": 2025,
    "categories": [
      "cs.DB"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.13874v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.05737v1",
    "title": "Local Differences, Global Lessons: Insights from Organisation Policies for International Legislation",
    "authors": [
      "Lucie-Aimée Kaffee",
      "Pepa Atanasova",
      "Anna Rogers"
    ],
    "author_ids": [],
    "abstract": "The rapid adoption of AI across diverse domains has led to the development of\norganisational guidelines that vary significantly, even within the same sector.\nThis paper examines AI policies in two domains, news organisations and\nuniversities, to understand how bottom-up governance approaches shape AI usage\nand oversight. By analysing these policies, we identify key areas of\nconvergence and divergence in how organisations address risks such as bias,\nprivacy, misinformation, and accountability. We then explore the implications\nof these findings for international AI legislation, particularly the EU AI Act,\nhighlighting gaps where practical policy insights could inform regulatory\nrefinements. Our analysis reveals that organisational policies often address\nissues such as AI literacy, disclosure practices, and environmental impact,\nareas that are underdeveloped in existing international frameworks. We argue\nthat lessons from domain-specific AI policies can contribute to more adaptive\nand effective AI governance at the global level. This study provides actionable\nrecommendations for policymakers seeking to bridge the gap between local AI\npractices and international regulations.",
    "published_date": "2025-02-19T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.05737v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.13834v3",
    "title": "Proving Olympiad Inequalities by Synergizing LLMs and Symbolic Reasoning",
    "authors": [
      "Zenan Li",
      "Zhaoyu Li",
      "Wen Tang",
      "Xian Zhang",
      "Yuan Yao",
      "Xujie Si",
      "Fan Yang",
      "Kaiyu Yang",
      "Xiaoxing Ma"
    ],
    "author_ids": [],
    "abstract": "Large language models (LLMs) can prove mathematical theorems formally by\ngenerating proof steps (\\textit{a.k.a.} tactics) within a proof system.\nHowever, the space of possible tactics is vast and complex, while the available\ntraining data for formal proofs is limited, posing a significant challenge to\nLLM-based tactic generation. To address this, we introduce a neuro-symbolic\ntactic generator that synergizes the mathematical intuition learned by LLMs\nwith domain-specific insights encoded by symbolic methods. The key aspect of\nthis integration is identifying which parts of mathematical reasoning are best\nsuited to LLMs and which to symbolic methods. While the high-level idea of\nneuro-symbolic integration is broadly applicable to various mathematical\nproblems, in this paper, we focus specifically on Olympiad inequalities\n(Figure~1). We analyze how humans solve these problems and distill the\ntechniques into two types of tactics: (1) scaling, handled by symbolic methods,\nand (2) rewriting, handled by LLMs. In addition, we combine symbolic tools with\nLLMs to prune and rank the proof goals for efficient proof search. We evaluate\nour framework on 161 challenging inequalities from multiple mathematics\ncompetitions, achieving state-of-the-art performance and significantly\noutperforming existing LLM and symbolic approaches without requiring additional\ntraining data.",
    "published_date": "2025-02-19T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.13834v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.13816v1",
    "title": "Exploring Embodied Emotional Communication: A Human-oriented Review of Mediated Social Touch",
    "authors": [
      "Liwen He",
      "Zichun Guo",
      "Yanru Mo",
      "Yue Wen",
      "Yun Wang"
    ],
    "author_ids": [],
    "abstract": "This paper offers a structured understanding of mediated social touch (MST)\nusing a human-oriented approach, through an extensive review of literature\nspanning tactile interfaces, emotional information, mapping mechanisms, and the\ndynamics of human-human and human-robot interactions. By investigating the\nexisting and exploratory mapping strategies of the 37 selected MST cases, we\nestablished the emotional expression space of MSTs that accommodated a diverse\nspectrum of emotions by integrating the categorical and Valence-arousal models,\nshowcasing how emotional cues can be translated into tactile signals. Based on\nthe expressive capacity of MSTs, a practical design space was structured\nencompassing factors such as the body locations, device form, tactile\nmodalities, and parameters. We also proposed various design strategies for MSTs\nincluding workflow, evaluation methods, and ethical and cultural\nconsiderations, as well as several future research directions. MSTs' potential\nis reflected not only in conveying emotional information but also in fostering\nempathy, comfort, and connection in both human-human and human-robot\ninteractions. This paper aims to serve as a comprehensive reference for design\nresearchers and practitioners, which helps expand the scope of emotional\ncommunication of MSTs, facilitating the exploration of diverse applications of\naffective haptics, and enhancing the naturalness and sociability of haptic\ninteraction.",
    "published_date": "2025-02-19T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.RO",
      "68T40 (Primary), 92B20 (Secondary), 68U05 (Secondary)",
      "H.5.2; J.4; I.2.9; I.3.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.13816v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.13772v1",
    "title": "Quantile agent utility and implications to randomized social choice",
    "authors": [
      "Ioannis Caragiannis",
      "Sanjukta Roy"
    ],
    "author_ids": [],
    "abstract": "We initiate a novel direction in randomized social choice by proposing a new\ndefinition of agent utility for randomized outcomes. Each agent has a\npreference over all outcomes and a {\\em quantile} parameter. Given a {\\em\nlottery} over the outcomes, an agent gets utility from a particular {\\em\nrepresentative}, defined as the least preferred outcome that can be realized so\nthat the probability that any worse-ranked outcome can be realized is at most\nthe agent's quantile value.\n  In contrast to other utility models that have been considered in randomized\nsocial choice (e.g., stochastic dominance, expected utility), our {\\em quantile\nagent utility} compares two lotteries for an agent by just comparing the\nrepresentatives, as is done for deterministic outcomes.\n  We revisit questions in randomized social choice using the new utility\ndefinition. We study the compatibility of efficiency and strategyproofness for\nrandomized voting rules, efficiency and fairness for randomized one-sided\nmatching mechanisms, and efficiency, stability, and strategyproofness for\nlotteries over two-sided matchings. In contrast to well-known impossibilities\nin randomized social choice, we show that satisfying the above properties\nsimultaneously can be possible.",
    "published_date": "2025-02-19T00:00:00",
    "year": 2025,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.13772v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.13704v1",
    "title": "Simulative Comparison of DVB-S2X/RCS2 and 3GPP 5G NR NTN Technologies in a Geostationary Satellite Scenario",
    "authors": [
      "Lauri Sormunen",
      "Tuomas Huikko",
      "Verneri Rönty",
      "Erno Seppänen",
      "Sami Rantanen",
      "Frans Laakso",
      "Vesa Hytönen",
      "Mikko Majamaa",
      "Jani Puttonen"
    ],
    "author_ids": [],
    "abstract": "Comparison between existing, well-established satellite technologies, like\nthe Digital Video Broadcasting (DVB) satellite specifications, and the emerging\nThird Generation Partnership Project (3GPP) specified 5th Generation New Radio\n(5G NR) Non-Terrestrial Networks (NTN) is an actively discussed topic in the\nsatellite industry standardization groups. This article presents a thorough\nperformance comparison between DVB Second Generation Satellite Extensions\n(DVBS2X) and Return Channel via Satellite 2nd Generation (DVBRCS2), and NR NTN\nin a Geostationary Orbit (GEO) satellite scenario, using system-level\nsimulators (SLS) for evaluation, namely Satellite Network Simulator 3 (SNS3)\nand ALIX 5G (TN-)NTN SLS, built on the same Network Simulator 3 (ns-3)\nplatform. With the satellite system geometry, beam layout, and link budget\naligned to use the 3GPP NTN example parameterization for a fair comparison\nbetween DVB and NR NTN, the results show that DVB-S2X consistently achieves\nhigher spectral efficiency than the NR Physical Downlink Shared Channel (PDSCH)\non the forward user link. In contrast, on the return link, the NR Physical\nUplink Shared Channel (PUSCH) demonstrates better spectral efficiency at the\nsystem level. The SLS results incorporate link-level performance, obtained\nthrough link-level simulations (LLS) for different modulation and coding\nschemes (MCS) and waveforms supported by each technology.",
    "published_date": "2025-02-19T00:00:00",
    "year": 2025,
    "categories": [
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.13704v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.13624v1",
    "title": "CardiacMamba: A Multimodal RGB-RF Fusion Framework with State Space Models for Remote Physiological Measurement",
    "authors": [
      "Zheng Wu",
      "Yiping Xie",
      "Bo Zhao",
      "Jiguang He",
      "Fei Luo",
      "Ning Deng",
      "Zitong Yu"
    ],
    "author_ids": [],
    "abstract": "Heart rate (HR) estimation via remote photoplethysmography (rPPG) offers a\nnon-invasive solution for health monitoring. However, traditional\nsingle-modality approaches (RGB or Radio Frequency (RF)) face challenges in\nbalancing robustness and accuracy due to lighting variations, motion artifacts,\nand skin tone bias. In this paper, we propose CardiacMamba, a multimodal RGB-RF\nfusion framework that leverages the complementary strengths of both modalities.\nIt introduces the Temporal Difference Mamba Module (TDMM) to capture dynamic\nchanges in RF signals using timing differences between frames, enhancing the\nextraction of local and global features. Additionally, CardiacMamba employs a\nBidirectional SSM for cross-modal alignment and a Channel-wise Fast Fourier\nTransform (CFFT) to effectively capture and refine the frequency domain\ncharacteristics of RGB and RF signals, ultimately improving heart rate\nestimation accuracy and periodicity detection. Extensive experiments on the\nEquiPleth dataset demonstrate state-of-the-art performance, achieving marked\nimprovements in accuracy and robustness. CardiacMamba significantly mitigates\nskin tone bias, reducing performance disparities across demographic groups, and\nmaintains resilience under missing-modality scenarios. By addressing critical\nchallenges in fairness, adaptability, and precision, the framework advances\nrPPG technology toward reliable real-world deployment in healthcare. The codes\nare available at: https://github.com/WuZheng42/CardiacMamba.",
    "published_date": "2025-02-19T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.13624v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.13593v2",
    "title": "Toward Robust Non-Transferable Learning: A Survey and Benchmark",
    "authors": [
      "Ziming Hong",
      "Yongli Xiang",
      "Tongliang Liu"
    ],
    "author_ids": [],
    "abstract": "Over the past decades, researchers have primarily focused on improving the\ngeneralization abilities of models, with limited attention given to regulating\nsuch generalization. However, the ability of models to generalize to unintended\ndata (e.g., harmful or unauthorized data) can be exploited by malicious\nadversaries in unforeseen ways, potentially resulting in violations of model\nethics. Non-transferable learning (NTL), a task aimed at reshaping the\ngeneralization abilities of deep learning models, was proposed to address these\nchallenges. While numerous methods have been proposed in this field, a\ncomprehensive review of existing progress and a thorough analysis of current\nlimitations remain lacking. In this paper, we bridge this gap by presenting the\nfirst comprehensive survey on NTL and introducing NTLBench, the first benchmark\nto evaluate NTL performance and robustness within a unified framework.\nSpecifically, we first introduce the task settings, general framework, and\ncriteria of NTL, followed by a summary of NTL approaches. Furthermore, we\nemphasize the often-overlooked issue of robustness against various attacks that\ncan destroy the non-transferable mechanism established by NTL. Experiments\nconducted via NTLBench verify the limitations of existing NTL methods in\nrobustness. Finally, we discuss the practical applications of NTL, along with\nits future directions and associated challenges.",
    "published_date": "2025-02-19T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.13593v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.13583v1",
    "title": "Fundamental Bias in Inverting Random Sampling Matrices with Application to Sub-sampled Newton",
    "authors": [
      "Chengmei Niu",
      "Zhenyu Liao",
      "Zenan Ling",
      "Michael W. Mahoney"
    ],
    "author_ids": [],
    "abstract": "A substantial body of work in machine learning (ML) and randomized numerical\nlinear algebra (RandNLA) has exploited various sorts of random sketching\nmethodologies, including random sampling and random projection, with much of\nthe analysis using Johnson--Lindenstrauss and subspace embedding techniques.\nRecent studies have identified the issue of inversion bias -- the phenomenon\nthat inverses of random sketches are not unbiased, despite the unbiasedness of\nthe sketches themselves. This bias presents challenges for the use of random\nsketches in various ML pipelines, such as fast stochastic optimization,\nscalable statistical estimators, and distributed optimization.\n  In the context of random projection, the inversion bias can be easily\ncorrected for dense Gaussian projections (which are, however, too expensive for\nmany applications). Recent work has shown how the inversion bias can be\ncorrected for sparse sub-gaussian projections. In this paper, we show how the\ninversion bias can be corrected for random sampling methods, both uniform and\nnon-uniform leverage-based, as well as for structured random projections,\nincluding those based on the Hadamard transform. Using these results, we\nestablish problem-independent local convergence rates for sub-sampled Newton\nmethods.",
    "published_date": "2025-02-19T00:00:00",
    "year": 2025,
    "categories": [
      "math.NA",
      "cs.NA",
      "math.OC",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.13583v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.13548v1",
    "title": "Detecting Linguistic Bias in Government Documents Using Large language Models",
    "authors": [
      "Milena de Swart",
      "Floris den Hengst",
      "Jieying Chen"
    ],
    "author_ids": [],
    "abstract": "This paper addresses the critical need for detecting bias in government\ndocuments, an underexplored area with significant implications for governance.\nExisting methodologies often overlook the unique context and far-reaching\nimpacts of governmental documents, potentially obscuring embedded biases that\nshape public policy and citizen-government interactions. To bridge this gap, we\nintroduce the Dutch Government Data for Bias Detection (DGDB), a dataset\nsourced from the Dutch House of Representatives and annotated for bias by\nexperts. We fine-tune several BERT-based models on this dataset and compare\ntheir performance with that of generative language models. Additionally, we\nconduct a comprehensive error analysis that includes explanations of the\nmodels' predictions. Our findings demonstrate that fine-tuned models achieve\nstrong performance and significantly outperform generative language models,\nindicating the effectiveness of DGDB for bias detection. This work underscores\nthe importance of labeled datasets for bias detection in various languages and\ncontributes to more equitable governance practices.",
    "published_date": "2025-02-19T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.13548v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.14001v1",
    "title": "Towards a perturbation-based explanation for medical AI as differentiable programs",
    "authors": [
      "Takeshi Abe",
      "Yoshiyuki Asai"
    ],
    "author_ids": [],
    "abstract": "Recent advancement in machine learning algorithms reaches a point where\nmedical devices can be equipped with artificial intelligence (AI) models for\ndiagnostic support and routine automation in clinical settings. In medicine and\nhealthcare, there is a particular demand for sufficient and objective\nexplainability of the outcome generated by AI models. However, AI models are\ngenerally considered as black boxes due to their complexity, and the\ncomputational process leading to their response is often opaque. Although\nseveral methods have been proposed to explain the behavior of models by\nevaluating the importance of each feature in discrimination and prediction,\nthey may suffer from biases and opacities arising from the scale and sampling\nprotocol of the dataset used for training or testing. To overcome the\nshortcomings of existing methods, we explore an alternative approach to provide\nan objective explanation of AI models that can be defined independently of the\nlearning process and does not require additional data. As a preliminary study\nfor this direction of research, this work examines a numerical availability of\nthe Jacobian matrix of deep learning models that measures how stably a model\nresponses against small perturbations added to the input. The indicator, if\navailable, are calculated from a trained AI model for a given target input.\nThis is a first step towards a perturbation-based explanation, which will\nassist medical practitioners in understanding and interpreting the response of\nthe AI model in its clinical application.",
    "published_date": "2025-02-19T00:00:00",
    "year": 2025,
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.14001v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.14911v1",
    "title": "Batayan: A Filipino NLP benchmark for evaluating Large Language Models",
    "authors": [
      "Jann Railey Montalan",
      "Jimson Paulo Layacan",
      "David Demitri Africa",
      "Richell Isaiah Flores",
      "Michael T. Lopez II",
      "Theresa Denise Magsajo",
      "Anjanette Cayabyab",
      "William Chandra Tjhi"
    ],
    "author_ids": [],
    "abstract": "Recent advances in large language models (LLMs) have demonstrated remarkable\ncapabilities on widely benchmarked high-resource languages; however, linguistic\nnuances of under-resourced languages remain unexplored. We introduce Batayan, a\nholistic Filipino benchmark designed to systematically evaluate LLMs across\nthree key natural language processing (NLP) competencies: understanding,\nreasoning, and generation. Batayan consolidates eight tasks, covering both\nTagalog and code-switched Taglish utterances. Our rigorous,\nnative-speaker-driven annotation process ensures fluency and authenticity to\nthe complex morphological and syntactic structures of Filipino, alleviating a\npervasive translationese bias in existing Filipino corpora. We report empirical\nresults on a variety of multilingual LLMs, highlighting significant performance\ngaps that signal the under-representation of Filipino in pretraining corpora,\nthe unique hurdles in modeling Filipino's rich morphology and construction, and\nthe importance of explicit Filipino language support and instruction tuning.\nMoreover, we discuss the practical challenges encountered in dataset\nconstruction and propose principled solutions for building culturally and\nlinguistically-faithful resources in under-represented languages. We also\nprovide a public benchmark and leaderboard as a clear foundation for iterative,\ncommunity-driven progress in Filipino NLP.",
    "published_date": "2025-02-19T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.14911v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.13319v1",
    "title": "Elucidating Mechanisms of Demographic Bias in LLMs for Healthcare",
    "authors": [
      "Hiba Ahsan",
      "Arnab Sen Sharma",
      "Silvio Amir",
      "David Bau",
      "Byron C. Wallace"
    ],
    "author_ids": [],
    "abstract": "We know from prior work that LLMs encode social biases, and that this\nmanifests in clinical tasks. In this work we adopt tools from mechanistic\ninterpretability to unveil sociodemographic representations and biases within\nLLMs in the context of healthcare. Specifically, we ask: Can we identify\nactivations within LLMs that encode sociodemographic information (e.g., gender,\nrace)? We find that gender information is highly localized in middle MLP layers\nand can be reliably manipulated at inference time via patching. Such\ninterventions can surgically alter generated clinical vignettes for specific\nconditions, and also influence downstream clinical predictions which correlate\nwith gender, e.g., patient risk of depression. We find that representation of\npatient race is somewhat more distributed, but can also be intervened upon, to\na degree. To our knowledge, this is the first application of mechanistic\ninterpretability methods to LLMs for healthcare.",
    "published_date": "2025-02-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.13319v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.13221v1",
    "title": "Two Tickets are Better than One: Fair and Accurate Hiring Under Strategic LLM Manipulations",
    "authors": [
      "Lee Cohen",
      "Jack Hsieh",
      "Connie Hong",
      "Judy Hanwen Shen"
    ],
    "author_ids": [],
    "abstract": "In an era of increasingly capable foundation models, job seekers are turning\nto generative AI tools to enhance their application materials. However, unequal\naccess to and knowledge about generative AI tools can harm both employers and\ncandidates by reducing the accuracy of hiring decisions and giving some\ncandidates an unfair advantage. To address these challenges, we introduce a new\nvariant of the strategic classification framework tailored to manipulations\nperformed using large language models, accommodating varying levels of\nmanipulations and stochastic outcomes. We propose a ``two-ticket'' scheme,\nwhere the hiring algorithm applies an additional manipulation to each submitted\nresume and considers this manipulated version together with the original\nsubmitted resume. We establish theoretical guarantees for this scheme, showing\nimprovements for both the fairness and accuracy of hiring decisions when the\ntrue positive rate is maximized subject to a no false positives constraint. We\nfurther generalize this approach to an $n$-ticket scheme and prove that hiring\noutcomes converge to a fixed, group-independent decision, eliminating\ndisparities arising from differential LLM access. Finally, we empirically\nvalidate our framework and the performance of our two-ticket scheme on real\nresumes using an open-source resume screening tool.",
    "published_date": "2025-02-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.13221v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.13120v1",
    "title": "Adapting Psycholinguistic Research for LLMs: Gender-inclusive Language in a Coreference Context",
    "authors": [
      "Marion Bartl",
      "Thomas Brendan Murphy",
      "Susan Leavy"
    ],
    "author_ids": [],
    "abstract": "Gender-inclusive language is often used with the aim of ensuring that all\nindividuals, regardless of gender, can be associated with certain concepts.\nWhile psycholinguistic studies have examined its effects in relation to human\ncognition, it remains unclear how Large Language Models (LLMs) process\ngender-inclusive language. Given that commercial LLMs are gaining an\nincreasingly strong foothold in everyday applications, it is crucial to examine\nwhether LLMs in fact interpret gender-inclusive language neutrally, because the\nlanguage they generate has the potential to influence the language of their\nusers. This study examines whether LLM-generated coreferent terms align with a\ngiven gender expression or reflect model biases. Adapting psycholinguistic\nmethods from French to English and German, we find that in English, LLMs\ngenerally maintain the antecedent's gender but exhibit underlying masculine\nbias. In German, this bias is much stronger, overriding all tested\ngender-neutralization strategies.",
    "published_date": "2025-02-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.13120v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.13101v2",
    "title": "AI and the Transformation of Accountability and Discretion in Urban Governance",
    "authors": [
      "Stephen Goldsmith",
      "Juncheng Yang"
    ],
    "author_ids": [],
    "abstract": "This paper offers a conceptual analysis of the transformative role of\nArtificial Intelligence (AI) in urban governance, focusing on how AI reshapes\ngovernance approaches, oversight mechanisms, and the relationship between\nbureaucratic discretion and accountability. Drawing on public administration\ntheory, tech-driven governance practices, and data ethics, the study\nsynthesizes insights to propose guiding principles for responsible AI\nintegration in decision-making processes. While primarily conceptual, the paper\ndraws on illustrative empirical cases to demonstrate how AI is reshaping\ndiscretion and accountability in real-world settings. The analysis argues that\nAI does not simply restrict or enhance discretion but redistributes it across\ninstitutional levels. It may simultaneously strengthen managerial oversight,\nenhance decision-making consistency, and improve operational efficiency. These\nchanges affect different forms of accountability: political, professional, and\nparticipatory, while introducing new risks, such as data bias, algorithmic\nopacity, and fragmented responsibility across actors. In response, the paper\nproposes guiding principles: equitable AI access, adaptive administrative\nstructures, robust data governance, and proactive human-led decision-making,\ncitizen-engaged oversight. This study contributes to the AI governance\nliterature by moving beyond narrow concerns with perceived discretion at the\nstreet level, highlighting instead how AI transforms rule-based discretion\nacross governance systems. By bridging perspectives on efficiency and ethical\nrisk, the paper presents a comprehensive framework for understanding the\nevolving relationship between discretion and accountability in AI-assisted\ngovernance.",
    "published_date": "2025-02-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.13101v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.05728v1",
    "title": "Political Neutrality in AI is Impossible- But Here is How to Approximate it",
    "authors": [
      "Jillian Fisher",
      "Ruth E. Appel",
      "Chan Young Park",
      "Yujin Potter",
      "Liwei Jiang",
      "Taylor Sorensen",
      "Shangbin Feng",
      "Yulia Tsvetkov",
      "Margaret E. Roberts",
      "Jennifer Pan",
      "Dawn Song",
      "Yejin Choi"
    ],
    "author_ids": [],
    "abstract": "AI systems often exhibit political bias, influencing users' opinions and\ndecision-making. While political neutrality-defined as the absence of bias-is\noften seen as an ideal solution for fairness and safety, this position paper\nargues that true political neutrality is neither feasible nor universally\ndesirable due to its subjective nature and the biases inherent in AI training\ndata, algorithms, and user interactions. However, inspired by Joseph Raz's\nphilosophical insight that \"neutrality [...] can be a matter of degree\" (Raz,\n1986), we argue that striving for some neutrality remains essential for\npromoting balanced AI interactions and mitigating user manipulation. Therefore,\nwe use the term \"approximation\" of political neutrality to shift the focus from\nunattainable absolutes to achievable, practical proxies. We propose eight\ntechniques for approximating neutrality across three levels of conceptualizing\nAI, examining their trade-offs and implementation strategies. In addition, we\nexplore two concrete applications of these approximations to illustrate their\npracticality. Finally, we assess our framework on current large language models\n(LLMs) at the output level, providing a demonstration of how it can be\nevaluated. This work seeks to advance nuanced discussions of political\nneutrality in AI and promote the development of responsible, aligned language\nmodels.",
    "published_date": "2025-02-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.05728v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.12927v1",
    "title": "SEFL: Harnessing Large Language Model Agents to Improve Educational Feedback Systems",
    "authors": [
      "Mike Zhang",
      "Amalie Pernille Dilling",
      "Léon Gondelman",
      "Niels Erik Ruan Lyngdorf",
      "Euan D. Lindsay",
      "Johannes Bjerva"
    ],
    "author_ids": [],
    "abstract": "Providing high-quality feedback is crucial for student success but is\nconstrained by time, cost, and limited data availability. We introduce\nSynthetic Educational Feedback Loops (SEFL), a novel framework designed to\ndeliver immediate, on-demand feedback at scale without relying on extensive,\nreal-world student data. In SEFL, two large language models (LLMs) operate in\nteacher--student roles to simulate assignment completion and formative\nfeedback, generating abundant synthetic pairs of student work and corresponding\ncritiques. We then fine-tune smaller, more computationally efficient LLMs on\nthese synthetic pairs, enabling them to replicate key features of high-quality,\ngoal-oriented feedback. Unlike personalized tutoring approaches that offer\nmulti-turn, individualized instruction, SEFL specifically focuses on\nreplicating the teacher-->student feedback loop for diverse assignments.\nThrough both LLM-as-a-judge and human evaluations, we demonstrate that\nSEFL-tuned models outperform their non-tuned counterparts in feedback quality,\nclarity, and timeliness. These findings reveal SEFL's potential to transform\nfeedback processes for higher education and beyond, offering an ethical and\nscalable alternative to conventional manual feedback cycles.",
    "published_date": "2025-02-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.12927v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.12893v2",
    "title": "H-CoT: Hijacking the Chain-of-Thought Safety Reasoning Mechanism to Jailbreak Large Reasoning Models, Including OpenAI o1/o3, DeepSeek-R1, and Gemini 2.0 Flash Thinking",
    "authors": [
      "Martin Kuo",
      "Jianyi Zhang",
      "Aolin Ding",
      "Qinsi Wang",
      "Louis DiValentin",
      "Yujia Bao",
      "Wei Wei",
      "Hai Li",
      "Yiran Chen"
    ],
    "author_ids": [],
    "abstract": "Large Reasoning Models (LRMs) have recently extended their powerful reasoning\ncapabilities to safety checks-using chain-of-thought reasoning to decide\nwhether a request should be answered. While this new approach offers a\npromising route for balancing model utility and safety, its robustness remains\nunderexplored. To address this gap, we introduce Malicious-Educator, a\nbenchmark that disguises extremely dangerous or malicious requests beneath\nseemingly legitimate educational prompts. Our experiments reveal severe\nsecurity flaws in popular commercial-grade LRMs, including OpenAI o1/o3,\nDeepSeek-R1, and Gemini 2.0 Flash Thinking. For instance, although OpenAI's o1\nmodel initially maintains a high refusal rate of about 98%, subsequent model\nupdates significantly compromise its safety; and attackers can easily extract\ncriminal strategies from DeepSeek-R1 and Gemini 2.0 Flash Thinking without any\nadditional tricks. To further highlight these vulnerabilities, we propose\nHijacking Chain-of-Thought (H-CoT), a universal and transferable attack method\nthat leverages the model's own displayed intermediate reasoning to jailbreak\nits safety reasoning mechanism. Under H-CoT, refusal rates sharply\ndecline-dropping from 98% to below 2%-and, in some instances, even transform\ninitially cautious tones into ones that are willing to provide harmful content.\nWe hope these findings underscore the urgent need for more robust safety\nmechanisms to preserve the benefits of advanced reasoning capabilities without\ncompromising ethical standards.",
    "published_date": "2025-02-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.12893v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.12874v1",
    "title": "Testing for Causal Fairness",
    "authors": [
      "Jiarun Fu",
      "LiZhong Ding",
      "Pengqi Li",
      "Qiuning Wei",
      "Yurong Cheng",
      "Xu Chen"
    ],
    "author_ids": [],
    "abstract": "Causality is widely used in fairness analysis to prevent discrimination on\nsensitive attributes, such as genders in career recruitment and races in crime\nprediction. However, the current data-based Potential Outcomes Framework (POF)\noften leads to untrustworthy fairness analysis results when handling\nhigh-dimensional data. To address this, we introduce a distribution-based POF\nthat transform fairness analysis into Distributional Closeness Testing (DCT) by\nintervening on sensitive attributes. We define counterfactual closeness\nfairness as the null hypothesis of DCT, where a sensitive attribute is\nconsidered fair if its factual and counterfactual potential outcome\ndistributions are sufficiently close. We introduce the Norm-Adaptive Maximum\nMean Discrepancy Treatment Effect (N-TE) as a statistic for measuring\ndistributional closeness and apply DCT using the empirical estimator of NTE,\nreferred to Counterfactual Fairness-CLOseness Testing ($\\textrm{CF-CLOT}$). To\nensure the trustworthiness of testing results, we establish the testing\nconsistency of N-TE through rigorous theoretical analysis. $\\textrm{CF-CLOT}$\ndemonstrates sensitivity in fairness analysis through the flexibility of the\ncloseness parameter $\\epsilon$. Unfair sensitive attributes have been\nsuccessfully tested by $\\textrm{CF-CLOT}$ in extensive experiments across\nvarious real-world scenarios, which validate the consistency of the testing.",
    "published_date": "2025-02-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.12874v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.12858v1",
    "title": "Rejected Dialects: Biases Against African American Language in Reward Models",
    "authors": [
      "Joel Mire",
      "Zubin Trivadi Aysola",
      "Daniel Chechelnitsky",
      "Nicholas Deas",
      "Chrysoula Zerva",
      "Maarten Sap"
    ],
    "author_ids": [],
    "abstract": "Preference alignment via reward models helps build safe, helpful, and\nreliable large language models (LLMs). However, subjectivity in preference\njudgments and the lack of representative sampling in preference data collection\ncan introduce new biases, hindering reward models' fairness and equity. In this\nwork, we introduce a framework for evaluating dialect biases in reward models\nand conduct a case study on biases against African American Language (AAL)\nthrough several experiments comparing reward model preferences and behavior on\npaired White Mainstream English (WME) and both machine-translated and\nhuman-written AAL corpora. We show that reward models are less aligned with\nhuman preferences when processing AAL texts vs. WME ones (-4\\% accuracy on\naverage), frequently disprefer AAL-aligned texts vs. WME-aligned ones, and\nsteer conversations toward WME, even when prompted with AAL texts. Our findings\nprovide a targeted analysis of anti-AAL biases at a relatively understudied\nstage in LLM development, highlighting representational harms and ethical\nquestions about the desired behavior of LLMs concerning AAL.",
    "published_date": "2025-02-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "I.2.7; K.4.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.12858v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.12847v1",
    "title": "Characterizing the Interaction of Cultural Evolution Mechanisms in Experimental Social Networks",
    "authors": [
      "Raja Marjieh",
      "Manuel Anglada-Tort",
      "Thomas L. Griffiths",
      "Nori Jacoby"
    ],
    "author_ids": [],
    "abstract": "Understanding how cognitive and social mechanisms shape the evolution of\ncomplex artifacts such as songs is central to cultural evolution research.\nSocial network topology (what artifacts are available?), selection (which are\nchosen?), and reproduction (how are they copied?) have all been proposed as key\ninfluencing factors. However, prior research has rarely studied them together\ndue to methodological challenges. We address this gap through a controlled\nnaturalistic paradigm whereby participants (N=2,404) are placed in networks and\nare asked to iteratively choose and sing back melodies from their neighbors. We\nshow that this setting yields melodies that are more complex and more pleasant\nthan those found in the more-studied linear transmission setting, and exhibits\nrobust differences across topologies. Crucially, these differences are\ndiminished when selection or reproduction bias are eliminated, suggesting an\ninteraction between mechanisms. These findings shed light on the interplay of\nmechanisms underlying the evolution of cultural artifacts.",
    "published_date": "2025-02-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SI",
      "q-bio.NC",
      "q-bio.PE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.12847v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.12838v1",
    "title": "Towards Equitable AI: Detecting Bias in Using Large Language Models for Marketing",
    "authors": [
      "Berk Yilmaz",
      "Huthaifa I. Ashqar"
    ],
    "author_ids": [],
    "abstract": "The recent advances in large language models (LLMs) have revolutionized\nindustries such as finance, marketing, and customer service by enabling\nsophisticated natural language processing tasks. However, the broad adoption of\nLLMs brings significant challenges, particularly in the form of social biases\nthat can be embedded within their outputs. Biases related to gender, age, and\nother sensitive attributes can lead to unfair treatment, raising ethical\nconcerns and risking both company reputation and customer trust. This study\nexamined bias in finance-related marketing slogans generated by LLMs (i.e.,\nChatGPT) by prompting tailored ads targeting five demographic categories:\ngender, marital status, age, income level, and education level. A total of\n1,700 slogans were generated for 17 unique demographic groups, and key terms\nwere categorized into four thematic groups: empowerment, financial, benefits\nand features, and personalization. Bias was systematically assessed using\nrelative bias calculations and statistically tested with the Kolmogorov-Smirnov\n(KS) test against general slogans generated for any individual. Results\nrevealed that marketing slogans are not neutral; rather, they emphasize\ndifferent themes based on demographic factors. Women, younger individuals,\nlow-income earners, and those with lower education levels receive more distinct\nmessaging compared to older, higher-income, and highly educated individuals.\nThis underscores the need to consider demographic-based biases in AI-generated\nmarketing strategies and their broader societal implications. The findings of\nthis study provide a roadmap for developing more equitable AI systems,\nhighlighting the need for ongoing bias detection and mitigation efforts in\nLLMs.",
    "published_date": "2025-02-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.12838v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.17484v1",
    "title": "Urinary Tract Infection Detection in Digital Remote Monitoring: Strategies for Managing Participant-Specific Prediction Complexity",
    "authors": [
      "Kexin Fan",
      "Alexander Capstick",
      "Ramin Nilforooshan",
      "Payam Barnaghi"
    ],
    "author_ids": [],
    "abstract": "Urinary tract infections (UTIs) are a significant health concern,\nparticularly for people living with dementia (PLWD), as they can lead to severe\ncomplications if not detected and treated early. This study builds on previous\nwork that utilised machine learning (ML) to detect UTIs in PLWD by analysing\nin-home activity and physiological data collected through low-cost, passive\nsensors. The current research focuses on improving the performance of previous\nmodels, particularly by refining the Multilayer Perceptron (MLP), to better\nhandle variations in home environments and improve sex fairness in predictions\nby making use of concepts from multitask learning. This study implemented three\nprimary model designs: feature clustering, loss-dependent clustering, and\nparticipant ID embedding which were compared against a baseline MLP model. The\nresults demonstrated that the loss-dependent MLP achieved the most significant\nimprovements, increasing validation precision from 48.92% to 72.60% and\nsensitivity from 27.44% to 70.52%, while also enhancing model fairness across\nsexes. These findings suggest that the refined models offer a more reliable and\nequitable approach to early UTI detection in PLWD, addressing\nparticipant-specific data variations and enabling clinicians to detect and\nscreen for UTI risks more effectively, thereby facilitating earlier and more\naccurate treatment decisions.",
    "published_date": "2025-02-18T00:00:00",
    "year": 2025,
    "categories": [
      "eess.SP",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.17484v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.12798v1",
    "title": "Envious Explore and Exploit",
    "authors": [
      "Omer Ben-Porat",
      "Yotam Gafni",
      "Or Markovetzki"
    ],
    "author_ids": [],
    "abstract": "Explore-and-exploit tradeoffs play a key role in recommendation systems\n(RSs), aiming at serving users better by learning from previous interactions.\nDespite their commercial success, the societal effects of explore-and-exploit\nmechanisms are not well understood, especially regarding the utility\ndiscrepancy they generate between different users. In this work, we measure\nsuch discrepancy using the economic notion of envy. We present a multi-armed\nbandit-like model in which every round consists of several sessions, and\nrewards are realized once per round. We call the latter property reward\nconsistency, and show that the RS can leverage this property for better\nsocietal outcomes. On the downside, doing so also generates envy, as\nlate-to-arrive users enjoy the information gathered by early-to-arrive users.\nWe examine the generated envy under several arrival order mechanisms and\nvirtually any anonymous algorithm, i.e., any algorithm that treats all similar\nusers similarly without leveraging their identities. We provide tight envy\nbounds on uniform arrival and upper bound the envy for nudged arrival, in which\nthe RS can affect the order of arrival by nudging its users. Furthermore, we\nstudy the efficiency-fairness trade-off by devising an algorithm that allows\nconstant envy and approximates the optimal welfare in restricted settings.\nFinally, we validate our theoretical results empirically using simulations.",
    "published_date": "2025-02-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.12798v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.13182v1",
    "title": "Fundus2Globe: Generative AI-Driven 3D Digital Twins for Personalized Myopia Management",
    "authors": [
      "Danli Shi",
      "Bowen Liu",
      "Zhen Tian",
      "Yue Wu",
      "Jiancheng Yang",
      "Ruoyu Chen",
      "Bo Yang",
      "Ou Xiao",
      "Mingguang He"
    ],
    "author_ids": [],
    "abstract": "Myopia, projected to affect 50% population globally by 2050, is a leading\ncause of vision loss. Eyes with pathological myopia exhibit distinctive shape\ndistributions, which are closely linked to the progression of\nvision-threatening complications. Recent understanding of eye-shape-based\nbiomarkers requires magnetic resonance imaging (MRI), however, it is costly and\nunrealistic in routine ophthalmology clinics. We present Fundus2Globe, the\nfirst AI framework that synthesizes patient-specific 3D eye globes from\nubiquitous 2D color fundus photographs (CFPs) and routine metadata (axial\nlength, spherical equivalent), bypassing MRI dependency. By integrating a 3D\nmorphable eye model (encoding biomechanical shape priors) with a latent\ndiffusion model, our approach achieves submillimeter accuracy in reconstructing\nposterior ocular anatomy efficiently. Fundus2Globe uniquely quantifies how\nvision-threatening lesions (e.g., staphylomas) in CFPs correlate with\nMRI-validated 3D shape abnormalities, enabling clinicians to simulate posterior\nsegment changes in response to refractive shifts. External validation\ndemonstrates its robust generation performance, ensuring fairness across\nunderrepresented groups. By transforming 2D fundus imaging into 3D digital\nreplicas of ocular structures, Fundus2Globe is a gateway for precision\nophthalmology, laying the foundation for AI-driven, personalized myopia\nmanagement.",
    "published_date": "2025-02-18T00:00:00",
    "year": 2025,
    "categories": [
      "eess.IV",
      "cs.CV",
      "eess.SP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.13182v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.14900v1",
    "title": "Can AI mimic the human ability to define neologisms?",
    "authors": [
      "Georgios P. Georgiou"
    ],
    "author_ids": [],
    "abstract": "One ongoing debate in linguistics is whether Artificial Intelligence (AI) can\neffectively mimic human performance in language-related tasks. While much\nresearch has focused on various linguistic abilities of AI, little attention\nhas been given to how it defines neologisms formed through different word\nformation processes. This study addresses this gap by examining the degree of\nagreement between human and AI-generated responses in defining three types of\nGreek neologisms: blends, compounds, and derivatives. The study employed an\nonline experiment in which human participants selected the most appropriate\ndefinitions for neologisms, while ChatGPT received identical prompts. The\nresults revealed fair agreement between human and AI responses for blends and\nderivatives but no agreement for compounds. However, when considering the\nmajority response among humans, agreement with AI was high for blends and\nderivatives. These findings highlight the complexity of human language and the\nchallenges AI still faces in capturing its nuances. In particular, they suggest\na need for integrating more advanced semantic networks and contextual learning\nmechanisms into AI models to improve their interpretation of complex word\nformations, especially compounds.",
    "published_date": "2025-02-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.14900v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.12644v1",
    "title": "Computing Efficient Envy-Free Partial Allocations of Indivisible Goods",
    "authors": [
      "Robert Bredereck",
      "Andrzej Kaczmarczyk",
      "Junjie Luo",
      "Bin Sun"
    ],
    "author_ids": [],
    "abstract": "Envy-freeness is one of the most prominent fairness concepts in the\nallocation of indivisible goods. Even though trivial envy-free allocations\nalways exist, rich literature shows this is not true when one additionally\nrequires some efficiency concept (e.g., completeness, Pareto-efficiency, or\nsocial welfare maximization). In fact, in such case even deciding the existence\nof an efficient envy-free allocation is notoriously computationally hard. In\nthis paper, we explore the limits of efficient computability by relaxing\nstandard efficiency concepts and analyzing how this impacts the computational\ncomplexity of the respective problems. Specifically, we allow partial\nallocations (where not all goods are allocated) and impose only very mild\nefficiency constraints, such as ensuring each agent receives a bundle with\npositive utility. Surprisingly, even such seemingly weak efficiency\nrequirements lead to a diverse computational complexity landscape. We identify\nseveral polynomial-time solvable or fixed-parameter tractable cases for binary\nutilities, yet we also find NP-hardness in very restricted scenarios involving\nternary utilities.",
    "published_date": "2025-02-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.12644v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.12640v1",
    "title": "RecDreamer: Consistent Text-to-3D Generation via Uniform Score Distillation",
    "authors": [
      "Chenxi Zheng",
      "Yihong Lin",
      "Bangzhen Liu",
      "Xuemiao Xu",
      "Yongwei Nie",
      "Shengfeng He"
    ],
    "author_ids": [],
    "abstract": "Current text-to-3D generation methods based on score distillation often\nsuffer from geometric inconsistencies, leading to repeated patterns across\ndifferent poses of 3D assets. This issue, known as the Multi-Face Janus\nproblem, arises because existing methods struggle to maintain consistency\nacross varying poses and are biased toward a canonical pose. While recent work\nhas improved pose control and approximation, these efforts are still limited by\nthis inherent bias, which skews the guidance during generation. To address\nthis, we propose a solution called RecDreamer, which reshapes the underlying\ndata distribution to achieve a more consistent pose representation. The core\nidea behind our method is to rectify the prior distribution, ensuring that pose\nvariation is uniformly distributed rather than biased toward a canonical form.\nBy modifying the prescribed distribution through an auxiliary function, we can\nreconstruct the density of the distribution to ensure compliance with specific\nmarginal constraints. In particular, we ensure that the marginal distribution\nof poses follows a uniform distribution, thereby eliminating the biases\nintroduced by the prior knowledge. We incorporate this rectified data\ndistribution into existing score distillation algorithms, a process we refer to\nas uniform score distillation. To efficiently compute the posterior\ndistribution required for the auxiliary function, RecDreamer introduces a\ntraining-free classifier that estimates pose categories in a plug-and-play\nmanner. Additionally, we utilize various approximation techniques for noisy\nstates, significantly improving system performance. Our experimental results\ndemonstrate that RecDreamer effectively mitigates the Multi-Face Janus problem,\nleading to more consistent 3D asset generation across different poses.",
    "published_date": "2025-02-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.12640v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.13180v1",
    "title": "Uncertain Multi-Objective Recommendation via Orthogonal Meta-Learning Enhanced Bayesian Optimization",
    "authors": [
      "Hongxu Wang",
      "Zhu Sun",
      "Yingpeng Du",
      "Lu Zhang",
      "Tiantian He",
      "Yew-Soon Ong"
    ],
    "author_ids": [],
    "abstract": "Recommender systems (RSs) play a crucial role in shaping our digital\ninteractions, influencing how we access and engage with information across\nvarious domains. Traditional research has predominantly centered on maximizing\nrecommendation accuracy, often leading to unintended side effects such as echo\nchambers and constrained user experiences. Drawing inspiration from autonomous\ndriving, we introduce a novel framework that categorizes RS autonomy into five\ndistinct levels, ranging from basic rule-based accuracy-driven systems to\nbehavior-aware, uncertain multi-objective RSs - where users may have varying\nneeds, such as accuracy, diversity, and fairness. In response, we propose an\napproach that dynamically identifies and optimizes multiple objectives based on\nindividual user preferences, fostering more ethical and intelligent\nuser-centric recommendations. To navigate the uncertainty inherent in\nmulti-objective RSs, we develop a Bayesian optimization (BO) framework that\ncaptures personalized trade-offs between different objectives while accounting\nfor their uncertain interdependencies. Furthermore, we introduce an orthogonal\nmeta-learning paradigm to enhance BO efficiency and effectiveness by leveraging\nshared knowledge across similar tasks and mitigating conflicts among objectives\nthrough the discovery of orthogonal information. Finally, extensive empirical\nevaluations demonstrate the effectiveness of our method in optimizing uncertain\nmulti-objectives for individual users, paving the way for more adaptive and\nuser-focused RSs.",
    "published_date": "2025-02-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.13180v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.12611v1",
    "title": "Who Writes What: Unveiling the Impact of Author Roles on AI-generated Text Detection",
    "authors": [
      "Jiatao Li",
      "Xiaojun Wan"
    ],
    "author_ids": [],
    "abstract": "The rise of Large Language Models (LLMs) necessitates accurate AI-generated\ntext detection. However, current approaches largely overlook the influence of\nauthor characteristics. We investigate how sociolinguistic attributes-gender,\nCEFR proficiency, academic field, and language environment-impact\nstate-of-the-art AI text detectors. Using the ICNALE corpus of human-authored\ntexts and parallel AI-generated texts from diverse LLMs, we conduct a rigorous\nevaluation employing multi-factor ANOVA and weighted least squares (WLS). Our\nresults reveal significant biases: CEFR proficiency and language environment\nconsistently affected detector accuracy, while gender and academic field showed\ndetector-dependent effects. These findings highlight the crucial need for\nsocially aware AI text detection to avoid unfairly penalizing specific\ndemographic groups. We offer novel empirical evidence, a robust statistical\nframework, and actionable insights for developing more equitable and reliable\ndetection systems in real-world, out-of-domain contexts. This work paves the\nway for future research on bias mitigation, inclusive evaluation benchmarks,\nand socially responsible LLM detectors.",
    "published_date": "2025-02-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.12611v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.12566v2",
    "title": "Exploring the Impact of Personality Traits on LLM Bias and Toxicity",
    "authors": [
      "Shuo Wang",
      "Renhao Li",
      "Xi Chen",
      "Yulin Yuan",
      "Derek F. Wong",
      "Min Yang"
    ],
    "author_ids": [],
    "abstract": "With the different roles that AI is expected to play in human life, imbuing\nlarge language models (LLMs) with different personalities has attracted\nincreasing research interests. While the \"personification\" enhances human\nexperiences of interactivity and adaptability of LLMs, it gives rise to\ncritical concerns about content safety, particularly regarding bias, sentiment\nand toxicity of LLM generation. This study explores how assigning different\npersonality traits to LLMs affects the toxicity and biases of their outputs.\nLeveraging the widely accepted HEXACO personality framework developed in social\npsychology, we design experimentally sound prompts to test three LLMs'\nperformance on three toxic and bias benchmarks. The findings demonstrate the\nsensitivity of all three models to HEXACO personality traits and, more\nimportantly, a consistent variation in the biases, negative sentiment and\ntoxicity of their output. In particular, adjusting the levels of several\npersonality traits can effectively reduce bias and toxicity in model\nperformance, similar to humans' correlations between personality traits and\ntoxic behaviors. The findings highlight the additional need to examine content\nsafety besides the efficiency of training or fine-tuning methods for LLM\npersonification. They also suggest a potential for the adjustment of\npersonalities to be a simple and low-cost method to conduct controlled text\ngeneration.",
    "published_date": "2025-02-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.12566v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.12323v1",
    "title": "Adversarial Debiasing for Unbiased Parameter Recovery",
    "authors": [
      "Luke C Sanford",
      "Megan Ayers",
      "Matthew Gordon",
      "Eliana Stone"
    ],
    "author_ids": [],
    "abstract": "Advances in machine learning and the increasing availability of\nhigh-dimensional data have led to the proliferation of social science research\nthat uses the predictions of machine learning models as proxies for measures of\nhuman activity or environmental outcomes. However, prediction errors from\nmachine learning models can lead to bias in the estimates of regression\ncoefficients. In this paper, we show how this bias can arise, propose a test\nfor detecting bias, and demonstrate the use of an adversarial machine learning\nalgorithm in order to de-bias predictions. These methods are applicable to any\nsetting where machine-learned predictions are the dependent variable in a\nregression. We conduct simulations and empirical exercises using ground truth\nand satellite data on forest cover in Africa. Using the predictions from a\nnaive machine learning model leads to biased parameter estimates, while the\npredictions from the adversarial model recover the true coefficients.",
    "published_date": "2025-02-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.12323v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.05725v2",
    "title": "A new framework for prognostics in decentralized industries: Enhancing fairness, security, and transparency through Blockchain and Federated Learning",
    "authors": [
      "T. Q. D. Pham",
      "K. D. Tran",
      "Khanh T. P. Nguyen",
      "X. V. Tran",
      "L. Köehl",
      "K. P. Tran"
    ],
    "author_ids": [],
    "abstract": "As global industries transition towards Industry 5.0 predictive maintenance\nPM remains crucial for cost effective operations resilience and minimizing\ndowntime in increasingly smart manufacturing environments In this chapter we\nexplore how the integration of Federated Learning FL and blockchain BC\ntechnologies enhances the prediction of machinerys Remaining Useful Life RUL\nwithin decentralized and human centric industrial ecosystems Traditional\ncentralized data approaches raise concerns over privacy security and\nscalability especially as Artificial intelligence AI driven smart manufacturing\nbecomes more prevalent This chapter leverages FL to enable localized model\ntraining across multiple sites while utilizing BC to ensure trust transparency\nand data integrity across the network This BC integrated FL framework optimizes\nRUL predictions enhances data privacy and security establishes transparency and\npromotes collaboration in decentralized manufacturing It addresses key\nchallenges such as maintaining privacy and security ensuring transparency and\nfairness and incentivizing participation in decentralized networks Experimental\nvalidation using the NASA CMAPSS dataset demonstrates the model effectiveness\nin real world scenarios and we extend our findings to the broader research\ncommunity through open source code on GitHub inviting collaborative development\nto drive innovation in Industry 5.0",
    "published_date": "2025-02-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.05725v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.12277v1",
    "title": "Healthcare cost prediction for heterogeneous patient profiles using deep learning models with administrative claims data",
    "authors": [
      "Mohammad Amin Morid",
      "Olivia R. Liu Sheng"
    ],
    "author_ids": [],
    "abstract": "Problem: How can we design patient cost prediction models that effectively\naddress the challenges of heterogeneity in administrative claims (AC) data to\nensure accurate, fair, and generalizable predictions, especially for high-need\n(HN) patients with complex chronic conditions?\n  Relevance: Accurate and equitable patient cost predictions are vital for\ndeveloping health management policies and optimizing resource allocation, which\ncan lead to significant cost savings for healthcare payers, including\ngovernment agencies and private insurers. Addressing disparities in prediction\noutcomes for HN patients ensures better economic and clinical decision-making,\nbenefiting both patients and payers.\n  Methodology: This study is grounded in socio-technical considerations that\nemphasize the interplay between technical systems (e.g., deep learning models)\nand humanistic outcomes (e.g., fairness in healthcare decisions). It\nincorporates representation learning and entropy measurement to address\nheterogeneity and complexity in data and patient profiles, particularly for HN\npatients. We propose a channel-wise deep learning framework that mitigates data\nheterogeneity by segmenting AC data into separate channels based on types of\ncodes (e.g., diagnosis, procedures) and costs. This approach is paired with a\nflexible evaluation design that uses multi-channel entropy measurement to\nassess patient heterogeneity.\n  Results: The proposed channel-wise models reduce prediction errors by 23%\ncompared to single-channel models, leading to 16.4% and 19.3% reductions in\noverpayments and underpayments, respectively. Notably, the reduction in\nprediction bias is significantly higher for HN patients, demonstrating\neffectiveness in handling heterogeneity and complexity in data and patient\nprofiles. This demonstrates the potential for applying channel-wise modeling to\ndomains with similar heterogeneity challenges.",
    "published_date": "2025-02-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.12277v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.05724v1",
    "title": "Addressing Moral Uncertainty using Large Language Models for Ethical Decision-Making",
    "authors": [
      "Rohit K. Dubey",
      "Damian Dailisan",
      "Sachit Mahajan"
    ],
    "author_ids": [],
    "abstract": "We present an ethical decision-making framework that refines a pre-trained\nreinforcement learning (RL) model using a task-agnostic ethical layer.\nFollowing initial training, the RL model undergoes ethical fine-tuning, where\nhuman feedback is replaced by feedback generated from a large language model\n(LLM). The LLM embodies consequentialist, deontological, virtue, social\njustice, and care ethics as moral principles to assign belief values to\nrecommended actions during ethical decision-making. An ethical layer aggregates\nbelief scores from multiple LLM-derived moral perspectives using Belief\nJensen-Shannon Divergence and Dempster-Shafer Theory into probability scores\nthat also serve as the shaping reward, steering the agent toward choices that\nalign with a balanced ethical framework. This integrated learning framework\nhelps the RL agent navigate moral uncertainty in complex environments and\nenables it to make morally sound decisions across diverse tasks. Our approach,\ntested across different LLM variants and compared with other belief aggregation\ntechniques, demonstrates improved consistency, adaptability, and reduced\nreliance on handcrafted ethical rewards. This method is especially effective in\ndynamic scenarios where ethical challenges arise unexpectedly, making it\nwell-suited for real-world applications.",
    "published_date": "2025-02-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.05724v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.12125v1",
    "title": "Hypernym Bias: Unraveling Deep Classifier Training Dynamics through the Lens of Class Hierarchy",
    "authors": [
      "Roman Malashin",
      "Valeria Yachnaya",
      "Alexander Mullin"
    ],
    "author_ids": [],
    "abstract": "We investigate the training dynamics of deep classifiers by examining how\nhierarchical relationships between classes evolve during training. Through\nextensive experiments, we argue that the learning process in classification\nproblems can be understood through the lens of label clustering. Specifically,\nwe observe that networks tend to distinguish higher-level (hypernym) categories\nin the early stages of training, and learn more specific (hyponym) categories\nlater. We introduce a novel framework to track the evolution of the feature\nmanifold during training, revealing how the hierarchy of class relations\nemerges and refines across the network layers. Our analysis demonstrates that\nthe learned representations closely align with the semantic structure of the\ndataset, providing a quantitative description of the clustering process.\nNotably, we show that in the hypernym label space, certain properties of neural\ncollapse appear earlier than in the hyponym label space, helping to bridge the\ngap between the initial and terminal phases of learning. We believe our\nfindings offer new insights into the mechanisms driving hierarchical learning\nin deep networks, paving the way for future advancements in understanding deep\nlearning dynamics.",
    "published_date": "2025-02-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.12125v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.11921v1",
    "title": "Joint Evaluation of Fairness and Relevance in Recommender Systems with Pareto Frontier",
    "authors": [
      "Theresia Veronika Rampisela",
      "Tuukka Ruotsalo",
      "Maria Maistro",
      "Christina Lioma"
    ],
    "author_ids": [],
    "abstract": "Fairness and relevance are two important aspects of recommender systems\n(RSs). Typically, they are evaluated either (i) separately by individual\nmeasures of fairness and relevance, or (ii) jointly using a single measure that\naccounts for fairness with respect to relevance. However, approach (i) often\ndoes not provide a reliable joint estimate of the goodness of the models, as it\nhas two different best models: one for fairness and another for relevance.\nApproach (ii) is also problematic because these measures tend to be ad-hoc and\ndo not relate well to traditional relevance measures, like NDCG. Motivated by\nthis, we present a new approach for jointly evaluating fairness and relevance\nin RSs: Distance to Pareto Frontier (DPFR). Given some user-item interaction\ndata, we compute their Pareto frontier for a pair of existing relevance and\nfairness measures, and then use the distance from the frontier as a measure of\nthe jointly achievable fairness and relevance. Our approach is modular and\nintuitive as it can be computed with existing measures. Experiments with 4 RS\nmodels, 3 re-ranking strategies, and 6 datasets show that existing metrics have\ninconsistent associations with our Pareto-optimal solution, making DPFR a more\nrobust and theoretically well-founded joint measure for assessing fairness and\nrelevance. Our code: https://github.com/theresiavr/DPFR-recsys-evaluation",
    "published_date": "2025-02-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.11921v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.11889v1",
    "title": "MQG4AI Towards Responsible High-risk AI -- Illustrated for Transparency Focusing on Explainability Techniques",
    "authors": [
      "Miriam Elia",
      "Alba Maria Lopez",
      "Katherin Alexandra Corredor",
      "Bernhard Bauer",
      "Esteban Garcia-Cuesta"
    ],
    "author_ids": [],
    "abstract": "As artificial intelligence (AI) systems become increasingly integrated into\ncritical domains, ensuring their responsible design and continuous development\nis imperative. Effective AI quality management (QM) requires tools and\nmethodologies that address the complexities of the AI lifecycle. In this paper,\nwe propose an approach for AI lifecycle planning that bridges the gap between\ngeneric guidelines and use case-specific requirements (MQG4AI). Our work aims\nto contribute to the development of practical tools for implementing\nResponsible AI (RAI) by aligning lifecycle planning with technical, ethical and\nregulatory demands. Central to our approach is the introduction of a flexible\nand customizable Methodology based on Quality Gates, whose building blocks\nincorporate RAI knowledge through information linking along the AI lifecycle in\na continuous manner, addressing AIs evolutionary character. For our present\ncontribution, we put a particular emphasis on the Explanation stage during\nmodel development, and illustrate how to align a guideline to evaluate the\nquality of explanations with MQG4AI, contributing to overall Transparency.",
    "published_date": "2025-02-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "K.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.11889v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.05722v1",
    "title": "The Role of AI, Blockchain, Cloud, and Data (ABCD) in Enhancing Learning Assessments of College Students",
    "authors": [
      "Joel Mark P. Rodriguez",
      "Genesis S. Austria",
      "Glen B. Millar"
    ],
    "author_ids": [],
    "abstract": "This study investigates how ABCD technologies can improve learning\nassessments in higher education. The objective is to research how students\nperceive things, plan their behavior, and how ABCD technologies affect\nindividual learning, academic integrity, co-learning, and trust in the\nassessment. Through a quantitative research design, survey responses were\ngathered from university students, and statistical tests, such as correlation\nand regression, were used to establish relationships between Perceived\nUsefulness (PU), Perceived Ease of Use (PEU), and Behavioral Intention (BI)\ntowards ABCD adoption. The results showed that there was no significant\nrelationship between PU, PEU, and BI, which suggests that students' attitudes,\ninstitutional policies, faculty support, and infrastructure matter more in\nadoption than institutional policies, faculty support, and infrastructure.\nWhile students recognize ABCD's efficiency and security benefits, fairness,\nease of use, and engagement issues limit their adoption of these technologies.\nThe research adds to Technology Acceptance Model (TAM) and Constructivist\nLearning Theory (CLT) by emphasizing external drivers of technology adoption.\nThe limitations are based on self-reported data and one institutional sample.\nIt is suggested that universities invest in faculty development,\ninfrastructure, and policy-making to facilitate effective and ethical use of\nABCD technologies in higher education.",
    "published_date": "2025-02-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.05722v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.11861v1",
    "title": "Exploring Large Language Models in Healthcare: Insights into Corpora Sources, Customization Strategies, and Evaluation Metrics",
    "authors": [
      "Shuqi Yang",
      "Mingrui Jing",
      "Shuai Wang",
      "Jiaxin Kou",
      "Manfei Shi",
      "Weijie Xing",
      "Yan Hu",
      "Zheng Zhu"
    ],
    "author_ids": [],
    "abstract": "This study reviewed the use of Large Language Models (LLMs) in healthcare,\nfocusing on their training corpora, customization techniques, and evaluation\nmetrics. A systematic search of studies from 2021 to 2024 identified 61\narticles. Four types of corpora were used: clinical resources, literature,\nopen-source datasets, and web-crawled data. Common construction techniques\nincluded pre-training, prompt engineering, and retrieval-augmented generation,\nwith 44 studies combining multiple methods. Evaluation metrics were categorized\ninto process, usability, and outcome metrics, with outcome metrics divided into\nmodel-based and expert-assessed outcomes. The study identified critical gaps in\ncorpus fairness, which contributed to biases from geographic, cultural, and\nsocio-economic factors. The reliance on unverified or unstructured data\nhighlighted the need for better integration of evidence-based clinical\nguidelines. Future research should focus on developing a tiered corpus\narchitecture with vetted sources and dynamic weighting, while ensuring model\ntransparency. Additionally, the lack of standardized evaluation frameworks for\ndomain-specific models called for comprehensive validation of LLMs in\nreal-world healthcare settings.",
    "published_date": "2025-02-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.11861v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.11828v1",
    "title": "Intersectional Fairness in Reinforcement Learning with Large State and Constraint Spaces",
    "authors": [
      "Eric Eaton",
      "Marcel Hussing",
      "Michael Kearns",
      "Aaron Roth",
      "Sikata Bela Sengupta",
      "Jessica Sorrell"
    ],
    "author_ids": [],
    "abstract": "In traditional reinforcement learning (RL), the learner aims to solve a\nsingle objective optimization problem: find the policy that maximizes expected\nreward. However, in many real-world settings, it is important to optimize over\nmultiple objectives simultaneously. For example, when we are interested in\nfairness, states might have feature annotations corresponding to multiple\n(intersecting) demographic groups to whom reward accrues, and our goal might be\nto maximize the reward of the group receiving the minimal reward. In this work,\nwe consider a multi-objective optimization problem in which each objective is\ndefined by a state-based reweighting of a single scalar reward function. This\ngeneralizes the problem of maximizing the reward of the minimum reward group.\nWe provide oracle-efficient algorithms to solve these multi-objective RL\nproblems even when the number of objectives is exponentially large-for tabular\nMDPs, as well as for large MDPs when the group functions have additional\nstructure. Finally, we experimentally validate our theoretical results and\ndemonstrate applications on a preferential attachment graph MDP.",
    "published_date": "2025-02-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.11828v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.11809v2",
    "title": "Revealing Bias Formation in Deep Neural Networks Through the Geometric Mechanisms of Human Visual Decoupling",
    "authors": [
      "Yanbiao Ma",
      "Bowei Liu",
      "Boyuan Gao",
      "Wei Dai",
      "Jiayi Chen",
      "Shuo Li"
    ],
    "author_ids": [],
    "abstract": "Deep neural networks (DNNs) often exhibit biases toward certain categories\nduring object recognition, even under balanced training data conditions. The\nintrinsic mechanisms underlying these biases remain unclear. Inspired by the\nhuman visual system, which decouples object manifolds through hierarchical\nprocessing to achieve object recognition, we propose a geometric analysis\nframework linking the geometric complexity of class-specific perceptual\nmanifolds in DNNs to model bias. Our findings reveal that differences in\ngeometric complexity can lead to varying recognition capabilities across\ncategories, introducing biases. To support this analysis, we present the\nPerceptual-Manifold-Geometry library, designed for calculating the geometric\nproperties of perceptual manifolds.",
    "published_date": "2025-02-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.11809v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.11798v1",
    "title": "BackdoorDM: A Comprehensive Benchmark for Backdoor Learning in Diffusion Model",
    "authors": [
      "Weilin Lin",
      "Nanjun Zhou",
      "Yanyun Wang",
      "Jianze Li",
      "Hui Xiong",
      "Li Liu"
    ],
    "author_ids": [],
    "abstract": "Backdoor learning is a critical research topic for understanding the\nvulnerabilities of deep neural networks. While it has been extensively studied\nin discriminative models over the past few years, backdoor learning in\ndiffusion models (DMs) has recently attracted increasing attention, becoming a\nnew research hotspot. Although many different backdoor attack and defense\nmethods have been proposed for DMs, a comprehensive benchmark for backdoor\nlearning in DMs is still lacking. This absence makes it difficult to conduct\nfair comparisons and thoroughly evaluate existing approaches, thus hindering\nfuture research progress. To address this issue, we propose BackdoorDM, the\nfirst comprehensive benchmark designed for backdoor learning in DMs. It\ncomprises nine state-of-the-art (SOTA) attack methods, four SOTA defense\nstrategies, and two helpful visualization analysis tools. We first\nsystematically classify and formulate the existing literature in a unified\nframework, focusing on three different backdoor attack types and five backdoor\ntarget types, which are restricted to a single type in discriminative models.\nThen, we systematically summarize the evaluation metrics for each type and\npropose a unified backdoor evaluation method based on GPT-4o. Finally, we\nconduct a comprehensive evaluation and highlight several important conclusions.\nWe believe that BackdoorDM will help overcome current barriers and contribute\nto building a trustworthy DMs community. The codes are released in\nhttps://github.com/linweiii/BackdoorDM.",
    "published_date": "2025-02-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.11798v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.05721v1",
    "title": "What Are They Filtering Out? A Survey of Filtering Strategies for Harm Reduction in Pretraining Datasets",
    "authors": [
      "Marco Antonio Stranisci",
      "Christian Hardmeier"
    ],
    "author_ids": [],
    "abstract": "Data filtering strategies are a crucial component to develop safe Large\nLanguage Models (LLM), since they support the removal of harmful contents from\npretraining datasets. There is a lack of research on the actual impact of these\nstrategies on vulnerable groups to discrimination, though, and their\neffectiveness has not been yet systematically addressed. In this paper we\npresent a benchmark study of data filtering strategies for harm reduction aimed\nat providing a systematic overview on these approaches. We survey 55 technical\nreports of English LMs and LLMs to identify the existing filtering strategies\nin literature and implement an experimental setting to test their impact\nagainst vulnerable groups. Our results show that the positive impact that\nstrategies have in reducing harmful contents from documents has the side effect\nof increasing the underrepresentation of vulnerable groups to discrimination in\ndatasets.",
    "published_date": "2025-02-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.05721v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.11689v1",
    "title": "Improve LLM-as-a-Judge Ability as a General Ability",
    "authors": [
      "Jiachen Yu",
      "Shaoning Sun",
      "Xiaohui Hu",
      "Jiaxu Yan",
      "Kaidong Yu",
      "Xuelong Li"
    ],
    "author_ids": [],
    "abstract": "LLM-as-a-Judge leverages the generative and reasoning capabilities of large\nlanguage models (LLMs) to evaluate LLM responses across diverse scenarios,\nproviding accurate preference signals. This approach plays a vital role in\naligning LLMs with human values, ensuring ethical and reliable AI outputs that\nalign with societal norms. Recent studies have raised many methods to train LLM\nas generative judges, but most of them are data consuming or lack accuracy, and\nonly focus on LLM's judge ability. In this work, we regard judge ability as a\ngeneral ability of LLM and implement a two-stage training approach, comprising\nsupervised fine-tuning (SFT) warm-up and direct preference optimization (DPO)\nenhancement, to achieve judge style adaptation and improve judgment accuracy.\nAdditionally, we introduce an efficient data synthesis method to generate\njudgmental content. Experimental results demonstrate that our approach,\nutilizing only about 2% to 40% of the data required by other methods, achieves\nSOTA performance on RewardBench. Furthermore, our training method enhances the\ngeneral capabilities of the model by constructing complicated judge task, and\nthe judge signals provided by our model have significantly enhanced the\ndownstream DPO training performance of our internal models in our test to\noptimize policy model with Judge Model. We also open-source our model weights\nand training data to facilitate further research.",
    "published_date": "2025-02-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.11689v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.11678v1",
    "title": "Exploring LLM-based Student Simulation for Metacognitive Cultivation",
    "authors": [
      "Haoxuan Li",
      "Jifan Yu",
      "Xin Cong",
      "Yang Dang",
      "Yisi Zhan",
      "Huiqin Liu",
      "Zhiyuan Liu"
    ],
    "author_ids": [],
    "abstract": "Metacognitive education plays a crucial role in cultivating students'\nself-regulation and reflective thinking, providing essential support for those\nwith learning difficulties through academic advising. Simulating students with\ninsufficient learning capabilities using large language models offers a\npromising approach to refining pedagogical methods without ethical concerns.\nHowever, existing simulations often fail to authentically represent students'\nlearning struggles and face challenges in evaluation due to the lack of\nreliable metrics and ethical constraints in data collection. To address these\nissues, we propose a pipeline for automatically generating and filtering\nhigh-quality simulated student agents. Our approach leverages a two-round\nautomated scoring system validated by human experts and employs a score\npropagation module to obtain more consistent scores across the student graph.\nExperimental results demonstrate that our pipeline efficiently identifies\nhigh-quality student agents, and we discuss the traits that influence the\nsimulation's effectiveness. By simulating students with varying degrees of\nlearning difficulties, our work paves the way for broader applications in\npersonalized learning and educational assessment.",
    "published_date": "2025-02-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.11678v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.11649v2",
    "title": "Competing LLM Agents in a Non-Cooperative Game of Opinion Polarisation",
    "authors": [
      "Amin Qasmi",
      "Usman Naseem",
      "Mehwish Nasim"
    ],
    "author_ids": [],
    "abstract": "We introduce a novel non-cooperative game to analyse opinion formation and\nresistance, incorporating principles from social psychology such as\nconfirmation bias, resource constraints, and influence penalties. Our\nsimulation features Large Language Model (LLM) agents competing to influence a\npopulation, with penalties imposed for generating messages that propagate or\ncounter misinformation. This framework integrates resource optimisation into\nthe agents' decision-making process. Our findings demonstrate that while higher\nconfirmation bias strengthens opinion alignment within groups, it also\nexacerbates overall polarisation. Conversely, lower confirmation bias leads to\nfragmented opinions and limited shifts in individual beliefs. Investing heavily\nin a high-resource debunking strategy can initially align the population with\nthe debunking agent, but risks rapid resource depletion and diminished\nlong-term influence.",
    "published_date": "2025-02-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.SI",
      "I.6; I.2.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.11649v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.11611v1",
    "title": "Identifying Gender Stereotypes and Biases in Automated Translation from English to Italian using Similarity Networks",
    "authors": [
      "Fatemeh Mohammadi",
      "Marta Annamaria Tamborini",
      "Paolo Ceravolo",
      "Costanza Nardocci",
      "Samira Maghool"
    ],
    "author_ids": [],
    "abstract": "This paper is a collaborative effort between Linguistics, Law, and Computer\nScience to evaluate stereotypes and biases in automated translation systems. We\nadvocate gender-neutral translation as a means to promote gender inclusion and\nimprove the objectivity of machine translation. Our approach focuses on\nidentifying gender bias in English-to-Italian translations. First, we define\ngender bias following human rights law and linguistics literature. Then we\nproceed by identifying gender-specific terms such as she/lei and he/lui as key\nelements. We then evaluate the cosine similarity between these target terms and\nothers in the dataset to reveal the model's perception of semantic relations.\nUsing numerical features, we effectively evaluate the intensity and direction\nof the bias. Our findings provide tangible insights for developing and training\ngender-neutral translation algorithms.",
    "published_date": "2025-02-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.11611v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.04773v2",
    "title": "Invisible Walls in Cities: Leveraging Large Language Models to Predict Urban Segregation Experience with Social Media Content",
    "authors": [
      "Bingbing Fan",
      "Lin Chen",
      "Songwei Li",
      "Jian Yuan",
      "Fengli Xu",
      "Pan Hui",
      "Yong Li"
    ],
    "author_ids": [],
    "abstract": "Understanding experienced segregation in urban daily life is crucial for\naddressing societal inequalities and fostering inclusivity. The abundance of\nuser-generated reviews on social media encapsulates nuanced perceptions and\nfeelings associated with different places, offering rich insights into\nsegregation. However, leveraging this data poses significant challenges due to\nits vast volume, ambiguity, and confluence of diverse perspectives. To tackle\nthese challenges, we propose using Large Language Models (LLMs) to automate\nonline review mining for segregation prediction. We design a Reflective LLM\nCoder to digest social media content into insights consistent with real-world\nfeedback, and eventually produce a codebook capturing key dimensions that\nsignal segregation experience, such as cultural resonance and appeal,\naccessibility and convenience, and community engagement and local involvement.\nGuided by the codebook, LLMs can generate both informative review summaries and\nratings for segregation prediction. Moreover, we design a\nREasoning-and-EMbedding (RE'EM) framework, which combines the reasoning and\nembedding capabilities of language models to integrate multi-channel features\nfor segregation prediction. Experiments on real-world data demonstrate that our\nframework greatly improves prediction accuracy, with a 22.79% elevation in R2\nand a 9.33% reduction in MSE. The derived codebook is generalizable across\nthree different cities, consistently improving prediction accuracy. Moreover,\nour user study confirms that the codebook-guided summaries provide cognitive\ngains for human participants in perceiving POIs' social inclusiveness. Our\nstudy marks an important step toward understanding implicit social barriers and\ninequalities, demonstrating the great potential of promoting social\ninclusiveness with AI.",
    "published_date": "2025-02-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.04773v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.11603v1",
    "title": "DR.GAP: Mitigating Bias in Large Language Models using Gender-Aware Prompting with Demonstration and Reasoning",
    "authors": [
      "Hongye Qiu",
      "Yue Xu",
      "Meikang Qiu",
      "Wenjie Wang"
    ],
    "author_ids": [],
    "abstract": "Large Language Models (LLMs) exhibit strong natural language processing\ncapabilities but also inherit and amplify societal biases, including gender\nbias, raising fairness concerns. Existing debiasing methods face significant\nlimitations: parameter tuning requires access to model weights, prompt-based\napproaches often degrade model utility, and optimization-based techniques lack\ngeneralizability. To address these challenges, we propose DR.GAP (Demonstration\nand Reasoning for Gender-Aware Prompting), an automated and model-agnostic\napproach that mitigates gender bias while preserving model performance. DR.GAP\nselects bias-revealing examples and generates structured reasoning to guide\nmodels toward more impartial responses. Extensive experiments on coreference\nresolution and QA tasks across multiple LLMs (GPT-3.5, Llama3, and\nLlama2-Alpaca) demonstrate its effectiveness, generalization ability, and\nrobustness. DR.GAP can generalize to vision-language models (VLMs), achieving\nsignificant bias reduction.",
    "published_date": "2025-02-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.11603v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.11559v1",
    "title": "Auto-Search and Refinement: An Automated Framework for Gender Bias Mitigation in Large Language Models",
    "authors": [
      "Yue Xu",
      "Chengyan Fu",
      "Li Xiong",
      "Sibei Yang",
      "Wenjie Wang"
    ],
    "author_ids": [],
    "abstract": "Pre-training large language models (LLMs) on vast text corpora enhances\nnatural language processing capabilities but risks encoding social biases,\nparticularly gender bias. While parameter-modification methods like fine-tuning\nmitigate bias, they are resource-intensive, unsuitable for closed-source\nmodels, and lack adaptability to evolving societal norms. Instruction-based\napproaches offer flexibility but often compromise task performance. To address\nthese limitations, we propose $\\textit{FaIRMaker}$, an automated and\nmodel-independent framework that employs an $\\textbf{auto-search and\nrefinement}$ paradigm to adaptively generate Fairwords, which act as\ninstructions integrated into input queries to reduce gender bias and enhance\nresponse quality. Extensive experiments demonstrate that $\\textit{FaIRMaker}$\nautomatically searches for and dynamically refines Fairwords, effectively\nmitigating gender bias while preserving task integrity and ensuring\ncompatibility with both API-based and open-source LLMs.",
    "published_date": "2025-02-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.11559v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.11459v1",
    "title": "Towards Responsible and Fair Data Science: Resource Allocation for Inclusive and Sustainable Analytics",
    "authors": [
      "Genoveva Vargas-Solar"
    ],
    "author_ids": [],
    "abstract": "This project addresses the challenges of responsible and fair resource\nallocation in data science (DS), focusing on DS queries evaluation. Current DS\npractices often overlook the broader socio-economic, environmental, and ethical\nimplications, including data sovereignty, fairness, and inclusivity. By\nintegrating a decolonial perspective, the project aims to establish innovative\nfairness metrics that respect cultural and contextual diversity, optimise\ncomputational and energy efficiency, and ensure equitable participation of\nunderrepresented communities. The research includes developing algorithms to\nalign resource allocation with fairness constraints, incorporating ethical and\nsustainability considerations, and fostering interdisciplinary collaborations\nto bridge technical advancements and societal impact gaps. This work aims to\nreshape into an equitable, transparent, and community-empowering practice\nchallenging the technological power developed by the Big Tech.",
    "published_date": "2025-02-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.DB"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.11459v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.11449v3",
    "title": "Tractable General Equilibrium",
    "authors": [
      "Denizalp Goktas",
      "Amy Greenwald"
    ],
    "author_ids": [],
    "abstract": "We study Walrasian economies (or general equilibrium models) and their\nsolution concept, the Walrasian equilibrium. A key challenge in this domain is\nidentifying price-adjustment processes that converge to equilibrium. One such\nprocess, t\\^atonnement, is an auction-like algorithm first proposed in 1874 by\nL\\'eon Walras. While continuous-time variants of t\\^atonnement are known to\nconverge to equilibrium in economies satisfying the Weak Axiom of Revealed\nPreferences (WARP), the process fails to converge in a pathological Walrasian\neconomy known as the Scarf economy. To address these issues, we analyze\nWalrasian economies using variational inequalities (VIs), an optimization\nframework. We introduce the class of mirror extragradient algorithms, which,\nunder suitable Lipschitz-continuity-like assumptions, converge to a solution of\nany VI satisfying the Minty condition in polynomial time. We show that the set\nof Walrasian equilibria of any balanced economy-which includes among others\nArrow-Debreu economies-corresponds to the solution set of an associated VI that\nsatisfies the Minty condition but is generally discontinuous. Applying the\nmirror extragradient algorithm to this VI we obtain a class of\nt\\^atonnement-like processes, which we call the mirror extrat\\^atonnement\nprocess. While our VI formulation is generally discontinuous, it is\nLipschitz-continuous in variationally stable Walrasian economies with bounded\nelasticity-including those satisfying WARP and the Scarf economy-thus\nestablishing the polynomial-time convergence of mirror extrat\\^atonnement in\nthese economies. We validate our approach through experiments on large\nArrow-Debreu economies with Cobb-Douglas, Leontief, and CES consumers, as well\nas the Scarf economy, demonstrating fast convergence in all cases without\nfailure.",
    "published_date": "2025-02-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.GT",
      "cs.CE",
      "econ.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.11449v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.11429v1",
    "title": "What's in a Query: Polarity-Aware Distribution-Based Fair Ranking",
    "authors": [
      "Aparna Balagopalan",
      "Kai Wang",
      "Olawale Salaudeen",
      "Asia Biega",
      "Marzyeh Ghassemi"
    ],
    "author_ids": [],
    "abstract": "Machine learning-driven rankings, where individuals (or items) are ranked in\nresponse to a query, mediate search exposure or attention in a variety of\nsafety-critical settings. Thus, it is important to ensure that such rankings\nare fair. Under the goal of equal opportunity, attention allocated to an\nindividual on a ranking interface should be proportional to their relevance\nacross search queries. In this work, we examine amortized fair ranking -- where\nrelevance and attention are cumulated over a sequence of user queries to make\nfair ranking more feasible in practice. Unlike prior methods that operate on\nexpected amortized attention for each individual, we define new\ndivergence-based measures for attention distribution-based fairness in ranking\n(DistFaiR), characterizing unfairness as the divergence between the\ndistribution of attention and relevance corresponding to an individual over\ntime. This allows us to propose new definitions of unfairness, which are more\nreliable at test time. Second, we prove that group fairness is upper-bounded by\nindividual fairness under this definition for a useful class of divergence\nmeasures, and experimentally show that maximizing individual fairness through\nan integer linear programming-based optimization is often beneficial to group\nfairness. Lastly, we find that prior research in amortized fair ranking ignores\ncritical information about queries, potentially leading to a fairwashing risk\nin practice by making rankings appear more fair than they actually are.",
    "published_date": "2025-02-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.11429v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.11411v1",
    "title": "Detecting and Filtering Unsafe Training Data via Data Attribution",
    "authors": [
      "Yijun Pan",
      "Taiwei Shi",
      "Jieyu Zhao",
      "Jiaqi W. Ma"
    ],
    "author_ids": [],
    "abstract": "Large language models (LLMs) are vulnerable to unsafe training data that even\nsmall amounts of unsafe data can lead to harmful model behaviors. Detecting and\nfiltering such unsafe training data is essential for trustworthy model\ndevelopment. Current state-of-the-art (SOTA) approaches typically rely on\ntraining moderation classifiers which requires significant computational\noverhead and are limited to predefined taxonomies, making them less adaptable\nto evolving safety concerns. Moreover, these classifiers lack insight into the\ntraining process, limiting their effectiveness in filtering unsafe data. To\naddress these limitations, we propose DABUF, leveraging data attribution to\ndetect and filter unsafe training data by attributing harmful model outputs to\ninfluential training data points. DABUF enables flexible identification of\nvarious unsafe data types without predefined taxonomies. However, in practice,\nmodel outputs can be complex with combined safe linguistic features and unsafe\ncontent, leading to reduced attribution accuracy. In such cases, DABUF will\nintegrate moderation classifiers to identify a minimal subset of unsafe\ntraining data for targeted attribution (such as jailbreak). When model outputs\nare relatively straightforward, DABUF uses model outputs directly as the\nattribution targets. We evaluate the performance on two different tasks: in\nfiltering jailbreaking training data and in identifying and mitigating gender\nbias. DABUF outperforms SOTA approaches by up to 7.5\\% in detection AUPRC in\njailbreaking scenarios, and 44.1\\% in detecting gender bias. Moreover,\nretraining on DABUF-filtered data leads to higher model safety across\nexperiments, underscoring its versatility in addressing a broad spectrum of\nunsafe data issues.",
    "published_date": "2025-02-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.11411v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.11408v1",
    "title": "Precise GPS-Denied UAV Self-Positioning via Context-Enhanced Cross-View Geo-Localization",
    "authors": [
      "Yuanze Xu",
      "Ming Dai",
      "Wenxiao Cai",
      "Wankou Yang"
    ],
    "author_ids": [],
    "abstract": "Image retrieval has been employed as a robust complementary technique to\naddress the challenge of Unmanned Aerial Vehicles (UAVs) self-positioning.\nHowever, most existing methods primarily focus on localizing objects captured\nby UAVs through complex part-based representations, often overlooking the\nunique challenges associated with UAV self-positioning, such as fine-grained\nspatial discrimination requirements and dynamic scene variations. To address\nthe above issues, we propose the Context-Enhanced method for precise UAV\nSelf-Positioning (CEUSP), specifically designed for UAV self-positioning tasks.\nCEUSP integrates a Dynamic Sampling Strategy (DSS) to efficiently select\noptimal negative samples, while the Rubik's Cube Attention (RCA) module,\ncombined with the Context-Aware Channel Integration (CACI) module, enhances\nfeature representation and discrimination by exploiting interdimensional\ninteractions, inspired by the rotational mechanics of a Rubik's Cube. Extensive\nexperimental validate the effectiveness of the proposed method, demonstrating\nnotable improvements in feature representation and UAV self-positioning\naccuracy within complex urban environments. Our approach achieves\nstate-of-the-art performance on the DenseUAV dataset, which is specifically\ndesigned for dense urban contexts, and also delivers competitive results on the\nwidely recognized University-1652 benchmark.",
    "published_date": "2025-02-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.11408v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.11349v1",
    "title": "Biases in Edge Language Models: Detection, Analysis, and Mitigation",
    "authors": [
      "Vinamra Sharma",
      "Danilo Pietro Pau",
      "José Cano"
    ],
    "author_ids": [],
    "abstract": "The integration of large language models (LLMs) on low-power edge devices\nsuch as Raspberry Pi, known as edge language models (ELMs), has introduced\nopportunities for more personalized, secure, and low-latency language\nintelligence that is accessible to all. However, the resource constraints\ninherent in edge devices and the lack of robust ethical safeguards in language\nmodels raise significant concerns about fairness, accountability, and\ntransparency in model output generation. This paper conducts a comparative\nanalysis of text-based bias across language model deployments on edge, cloud,\nand desktop environments, aiming to evaluate how deployment settings influence\nmodel fairness. Specifically, we examined an optimized Llama-2 model running on\na Raspberry Pi 4; GPT 4o-mini, Gemini-1.5-flash, and Grok-beta models running\non cloud servers; and Gemma2 and Mistral models running on a MacOS desktop\nmachine. Our results demonstrate that Llama-2 running on Raspberry Pi 4 is\n43.23% and 21.89% more prone to showing bias over time compared to models\nrunning on the desktop and cloud-based environments. We also propose the\nimplementation of a feedback loop, a mechanism that iteratively adjusts model\nbehavior based on previous outputs, where predefined constraint weights are\napplied layer-by-layer during inference, allowing the model to correct bias\npatterns, resulting in 79.28% reduction in model bias.",
    "published_date": "2025-02-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.PF",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.11349v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.12209v1",
    "title": "Suboptimal Shapley Value Explanations",
    "authors": [
      "Xiaolei Lu"
    ],
    "author_ids": [],
    "abstract": "Deep Neural Networks (DNNs) have demonstrated strong capacity in supporting a\nwide variety of applications. Shapley value has emerged as a prominent tool to\nanalyze feature importance to help people understand the inference process of\ndeep neural models. Computing Shapley value function requires choosing a\nbaseline to represent feature's missingness. However, existing random and\nconditional baselines could negatively influence the explanation. In this\npaper, by analyzing the suboptimality of different baselines, we identify the\nproblematic baseline where the asymmetric interaction between $\\bm{x}'_i$ (the\nreplacement of the faithful influential feature) and other features has\nsignificant directional bias toward the model's output, and conclude that\n$p(y|\\bm{x}'_i) = p(y)$ potentially minimizes the asymmetric interaction\ninvolving $\\bm{x}'_i$. We further generalize the uninformativeness of\n$\\bm{x}'_i$ toward the label space $L$ to avoid estimating $p(y)$ and design a\nsimple uncertainty-based reweighting mechanism to accelerate the computation\nprocess. We conduct experiments on various NLP tasks and our quantitative\nanalysis demonstrates the effectiveness of the proposed uncertainty-based\nreweighting mechanism. Furthermore, by measuring the consistency of\nexplanations generated by explainable methods and human, we highlight the\ndisparity between model inference and human understanding.",
    "published_date": "2025-02-17T00:00:00",
    "year": 2025,
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.12209v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.11312v1",
    "title": "AI Generations: From AI 1.0 to AI 4.0",
    "authors": [
      "Jiahao Wu",
      "Hengxu You",
      "Jing Du"
    ],
    "author_ids": [],
    "abstract": "This paper proposes that Artificial Intelligence (AI) progresses through\nseveral overlapping generations: AI 1.0 (Information AI), AI 2.0 (Agentic AI),\nAI 3.0 (Physical AI), and now a speculative AI 4.0 (Conscious AI). Each of\nthese AI generations is driven by shifting priorities among algorithms,\ncomputing power, and data. AI 1.0 ushered in breakthroughs in pattern\nrecognition and information processing, fueling advances in computer vision,\nnatural language processing, and recommendation systems. AI 2.0 built on these\nfoundations through real-time decision-making in digital environments,\nleveraging reinforcement learning and adaptive planning for agentic AI\napplications. AI 3.0 extended intelligence into physical contexts, integrating\nrobotics, autonomous vehicles, and sensor-fused control systems to act in\nuncertain real-world settings. Building on these developments, AI 4.0 puts\nforward the bold vision of self-directed AI capable of setting its own goals,\norchestrating complex training regimens, and possibly exhibiting elements of\nmachine consciousness. This paper traces the historical foundations of AI\nacross roughly seventy years, mapping how changes in technological bottlenecks\nfrom algorithmic innovation to high-performance computing to specialized data,\nhave spurred each generational leap. It further highlights the ongoing\nsynergies among AI 1.0, 2.0, 3.0, and 4.0, and explores the profound ethical,\nregulatory, and philosophical challenges that arise when artificial systems\napproach (or aspire to) human-like autonomy. Ultimately, understanding these\nevolutions and their interdependencies is pivotal for guiding future research,\ncrafting responsible governance, and ensuring that AI transformative potential\nbenefits society as a whole.",
    "published_date": "2025-02-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.11312v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.11246v1",
    "title": "MemeSense: An Adaptive In-Context Framework for Social Commonsense Driven Meme Moderation",
    "authors": [
      "Sayantan Adak",
      "Somnath Banerjee",
      "Rajarshi Mandal",
      "Avik Halder",
      "Sayan Layek",
      "Rima Hazra",
      "Animesh Mukherjee"
    ],
    "author_ids": [],
    "abstract": "Memes present unique moderation challenges due to their subtle, multimodal\ninterplay of images, text, and social context. Standard systems relying\npredominantly on explicit textual cues often overlook harmful content\ncamouflaged by irony, symbolism, or cultural references. To address this gap,\nwe introduce MemeSense, an adaptive in-context learning framework that fuses\nsocial commonsense reasoning with visually and semantically related reference\nexamples. By encoding crucial task information into a learnable cognitive shift\nvector, MemeSense effectively balances lexical, visual, and ethical\nconsiderations, enabling precise yet context-aware meme intervention. Extensive\nevaluations on a curated set of implicitly harmful memes demonstrate that\nMemeSense substantially outperforms strong baselines, paving the way for safer\nonline communities. Code and data available at:\nhttps://github.com/sayantan11995/MemeSense",
    "published_date": "2025-02-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IR",
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.11246v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.11242v2",
    "title": "LLMs and Childhood Safety: Identifying Risks and Proposing a Protection Framework for Safe Child-LLM Interaction",
    "authors": [
      "Junfeng Jiao",
      "Saleh Afroogh",
      "Kevin Chen",
      "Abhejay Murali",
      "David Atkinson",
      "Amit Dhurandhar"
    ],
    "author_ids": [],
    "abstract": "This study examines the growing use of Large Language Models (LLMs) in\nchild-centered applications, highlighting safety and ethical concerns such as\nbias, harmful content, and cultural insensitivity. Despite their potential to\nenhance learning, there is a lack of standardized frameworks to mitigate these\nrisks. Through a systematic literature review, we identify key parental and\nempirical concerns, including toxicity and ethical breaches in AI outputs.\nMoreover, to address these issues, this paper proposes a protection framework\nfor safe Child-LLM interaction, incorporating metrics for content safety,\nbehavioral ethics, and cultural sensitivity. The framework provides practical\ntools for evaluating LLM safety, offering guidance for developers,\npolicymakers, and educators to ensure responsible AI deployment for children.",
    "published_date": "2025-02-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.11242v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.11238v1",
    "title": "Span-Agnostic Optimal Sample Complexity and Oracle Inequalities for Average-Reward RL",
    "authors": [
      "Matthew Zurek",
      "Yudong Chen"
    ],
    "author_ids": [],
    "abstract": "We study the sample complexity of finding an $\\varepsilon$-optimal policy in\naverage-reward Markov Decision Processes (MDPs) with a generative model. The\nminimax optimal span-based complexity of $\\widetilde{O}(SAH/\\varepsilon^2)$,\nwhere $H$ is the span of the optimal bias function, has only been achievable\nwith prior knowledge of the value of $H$. Prior-knowledge-free algorithms have\nbeen the objective of intensive research, but several natural approaches\nprovably fail to achieve this goal. We resolve this problem, developing the\nfirst algorithms matching the optimal span-based complexity without $H$\nknowledge, both when the dataset size is fixed and when the suboptimality level\n$\\varepsilon$ is fixed. Our main technique combines the discounted reduction\napproach with a method for automatically tuning the effective horizon based on\nempirical confidence intervals or lower bounds on performance, which we term\nhorizon calibration. We also develop an empirical span penalization approach,\ninspired by sample variance penalization, which satisfies an oracle inequality\nperformance guarantee. In particular this algorithm can outperform the minimax\ncomplexity in benign settings such as when there exist near-optimal policies\nwith span much smaller than $H$.",
    "published_date": "2025-02-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.IT",
      "math.IT",
      "math.OC",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.11238v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.11211v1",
    "title": "A Survey of LLM-based Agents in Medicine: How far are we from Baymax?",
    "authors": [
      "Wenxuan Wang",
      "Zizhan Ma",
      "Zheng Wang",
      "Chenghan Wu",
      "Wenting Chen",
      "Xiang Li",
      "Yixuan Yuan"
    ],
    "author_ids": [],
    "abstract": "Large Language Models (LLMs) are transforming healthcare through the\ndevelopment of LLM-based agents that can understand, reason about, and assist\nwith medical tasks. This survey provides a comprehensive review of LLM-based\nagents in medicine, examining their architectures, applications, and\nchallenges. We analyze the key components of medical agent systems, including\nsystem profiles, clinical planning mechanisms, medical reasoning frameworks,\nand external capacity enhancement. The survey covers major application\nscenarios such as clinical decision support, medical documentation, training\nsimulations, and healthcare service optimization. We discuss evaluation\nframeworks and metrics used to assess these agents' performance in healthcare\nsettings. While LLM-based agents show promise in enhancing healthcare delivery,\nseveral challenges remain, including hallucination management, multimodal\nintegration, implementation barriers, and ethical considerations. The survey\nconcludes by highlighting future research directions, including advances in\nmedical reasoning inspired by recent developments in LLM architectures,\nintegration with physical systems, and improvements in training simulations.\nThis work provides researchers and practitioners with a structured overview of\nthe current state and future prospects of LLM-based agents in medicine.",
    "published_date": "2025-02-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.11211v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.11195v1",
    "title": "From Deception to Perception: The Surprising Benefits of Deepfakes for Detecting, Measuring, and Mitigating Bias",
    "authors": [
      "Yizhi Liu",
      "Balaji Padmanabhan",
      "Siva Viswanathan"
    ],
    "author_ids": [],
    "abstract": "While deepfake technologies have predominantly been criticized for potential\nmisuse, our study demonstrates their significant potential as tools for\ndetecting, measuring, and mitigating biases in key societal domains. By\nemploying deepfake technology to generate controlled facial images, we extend\nthe scope of traditional correspondence studies beyond mere textual\nmanipulations. This enhancement is crucial in scenarios such as pain\nassessments, where subjective biases triggered by sensitive features in facial\nimages can profoundly affect outcomes. Our results reveal that deepfakes not\nonly maintain the effectiveness of correspondence studies but also introduce\ngroundbreaking advancements in bias measurement and correction techniques. This\nstudy emphasizes the constructive role of deepfake technologies as essential\ntools for advancing societal equity and fairness.",
    "published_date": "2025-02-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.2.0; I.2.10; I.4.0; J.4; H.4; K.4.1; K.4.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.11195v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.14888v1",
    "title": "The Multi-Faceted Monosemanticity in Multimodal Representations",
    "authors": [
      "Hanqi Yan",
      "Xiangxiang Cui",
      "Lu Yin",
      "Paul Pu Liang",
      "Yulan He",
      "Yifei Wang"
    ],
    "author_ids": [],
    "abstract": "In this paper, we leverage recent advancements in feature monosemanticity to\nextract interpretable features from deep multimodal models, offering a\ndata-driven understanding of modality gaps. Specifically, we investigate CLIP\n(Contrastive Language-Image Pretraining), a prominent visual-language\nrepresentation model trained on extensive image-text pairs. Building upon\ninterpretability tools developed for single-modal models, we extend these\nmethodologies to assess multi-modal interpretability of CLIP features.\nAdditionally, we introduce the Modality Dominance Score (MDS) to attribute the\ninterpretability of each feature to its respective modality. Next, we transform\nCLIP features into a more interpretable space, enabling us to categorize them\ninto three distinct classes: vision features (single-modal), language features\n(single-modal), and visual-language features (cross-modal). Our findings reveal\nthat this categorization aligns closely with human cognitive understandings of\ndifferent modalities. We also demonstrate significant use cases of this\nmodality-specific features including detecting gender bias, adversarial attack\ndefense and text-to-image model editing. These results indicate that\nlarge-scale multimodal models, equipped with task-agnostic interpretability\ntools, offer valuable insights into key connections and distinctions between\ndifferent modalities.",
    "published_date": "2025-02-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.14888v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.05715v1",
    "title": "The Butterfly Effect of Technology: How Various Factors accelerate or hinder the Arrival of Technological Singularity",
    "authors": [
      "Hooman Shababi"
    ],
    "author_ids": [],
    "abstract": "This article explores the concept of technological singularity and the\nfactors that could accelerate or hinder its arrival. The butterfly effect is\nused as a framework to understand how seemingly small changes in complex\nsystems can have significant and unpredictable outcomes. In section II, we\ndiscuss the various factors that could hasten the arrival of technological\nsingularity, such as advances in artificial intelligence and machine learning,\nbreakthroughs in quantum computing, progress in brain-computer interfaces and\nhuman augmentation, and development of nanotechnology and 3D printing. In\nsection III, we examine the factors that could delay or impede the arrival of\ntechnological singularity, including technical limitations and setbacks in AI\nand machine learning, ethical and societal concerns around AI and its impact on\njobs and privacy, lack of sufficient investment in research and development,\nand regulatory barriers and political instability. Section IV explores the\ninterplay of these factors and how they can impact the butterfly effect.\nFinally, in the conclusion, we summarize the key points discussed and emphasize\nthe importance of considering the butterfly effect in predicting the future of\ntechnology. We call for continued research and investment in technology to\nshape its future and mitigate potential risks.",
    "published_date": "2025-02-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.05715v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.11049v1",
    "title": "Faces of Fairness: Examining Bias in Facial Expression Recognition Datasets and Models",
    "authors": [
      "Mohammad Mehdi Hosseini",
      "Ali Pourramezan Fard",
      "Mohammad H. Mahoor"
    ],
    "author_ids": [],
    "abstract": "Building AI systems, including Facial Expression Recognition (FER), involves\ntwo critical aspects: data and model design. Both components significantly\ninfluence bias and fairness in FER tasks. Issues related to bias and fairness\nin FER datasets and models remain underexplored. This study investigates bias\nsources in FER datasets and models. Four common FER datasets--AffectNet, ExpW,\nFer2013, and RAF-DB--are analyzed. The findings demonstrate that AffectNet and\nExpW exhibit high generalizability despite data imbalances. Additionally, this\nresearch evaluates the bias and fairness of six deep models, including three\nstate-of-the-art convolutional neural network (CNN) models: MobileNet, ResNet,\nXceptionNet, as well as three transformer-based models: ViT, CLIP, and\nGPT-4o-mini. Experimental results reveal that while GPT-4o-mini and ViT achieve\nthe highest accuracy scores, they also display the highest levels of bias.\nThese findings underscore the urgent need for developing new methodologies to\nmitigate bias and ensure fairness in datasets and models, particularly in\naffective computing applications. See our implementation details at\nhttps://github.com/MMHosseini/bias_in_FER.",
    "published_date": "2025-02-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.11049v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.11031v1",
    "title": "A Critical Review of Predominant Bias in Neural Networks",
    "authors": [
      "Jiazhi Li",
      "Mahyar Khayatkhoei",
      "Jiageng Zhu",
      "Hanchen Xie",
      "Mohamed E. Hussein",
      "Wael AbdAlmageed"
    ],
    "author_ids": [],
    "abstract": "Bias issues of neural networks garner significant attention along with its\npromising advancement. Among various bias issues, mitigating two predominant\nbiases is crucial in advancing fair and trustworthy AI: (1) ensuring neural\nnetworks yields even performance across demographic groups, and (2) ensuring\nalgorithmic decision-making does not rely on protected attributes. However,\nupon the investigation of \\pc papers in the relevant literature, we find that\nthere exists a persistent, extensive but under-explored confusion regarding\nthese two types of biases. Furthermore, the confusion has already significantly\nhampered the clarity of the community and subsequent development of debiasing\nmethodologies. Thus, in this work, we aim to restore clarity by providing two\nmathematical definitions for these two predominant biases and leveraging these\ndefinitions to unify a comprehensive list of papers. Next, we highlight the\ncommon phenomena and the possible reasons for the existing confusion. To\nalleviate the confusion, we provide extensive experiments on synthetic, census,\nand image datasets, to validate the distinct nature of these biases,\ndistinguish their different real-world manifestations, and evaluate the\neffectiveness of a comprehensive list of bias assessment metrics in assessing\nthe mitigation of these biases. Further, we compare these two types of biases\nfrom multiple dimensions including the underlying causes, debiasing methods,\nevaluation protocol, prevalent datasets, and future directions. Last, we\nprovide several suggestions aiming to guide researchers engaged in bias-related\nwork to avoid confusion and further enhance clarity in the community.",
    "published_date": "2025-02-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.11031v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.10883v1",
    "title": "Learning Identifiable Structures Helps Avoid Bias in DNN-based Supervised Causal Learning",
    "authors": [
      "Jiaru Zhang",
      "Rui Ding",
      "Qiang Fu",
      "Bojun Huang",
      "Zizhen Deng",
      "Yang Hua",
      "Haibing Guan",
      "Shi Han",
      "Dongmei Zhang"
    ],
    "author_ids": [],
    "abstract": "Causal discovery is a structured prediction task that aims to predict causal\nrelations among variables based on their data samples. Supervised Causal\nLearning (SCL) is an emerging paradigm in this field. Existing Deep Neural\nNetwork (DNN)-based methods commonly adopt the \"Node-Edge approach\", in which\nthe model first computes an embedding vector for each variable-node, then uses\nthese variable-wise representations to concurrently and independently predict\nfor each directed causal-edge. In this paper, we first show that this\narchitecture has some systematic bias that cannot be mitigated regardless of\nmodel size and data size. We then propose SiCL, a DNN-based SCL method that\npredicts a skeleton matrix together with a v-tensor (a third-order tensor\nrepresenting the v-structures). According to the Markov Equivalence Class (MEC)\ntheory, both the skeleton and the v-structures are identifiable causal\nstructures under the canonical MEC setting, so predictions about skeleton and\nv-structures do not suffer from the identifiability limit in causal discovery,\nthus SiCL can avoid the systematic bias in Node-Edge architecture, and enable\nconsistent estimators for causal discovery. Moreover, SiCL is also equipped\nwith a specially designed pairwise encoder module with a unidirectional\nattention layer to model both internal and external relationships of pairs of\nnodes. Experimental results on both synthetic and real-world benchmarks show\nthat SiCL significantly outperforms other DNN-based SCL approaches.",
    "published_date": "2025-02-15T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ME"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.10883v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.10856v1",
    "title": "Multiple Approaches for Teaching Responsible Computing",
    "authors": [
      "Stacy A. Doore",
      "Michelle Trim",
      "Joycelyn Streator",
      "Richard L. Blumenthal",
      "Atri Rudra",
      "Robert B. Schnabel"
    ],
    "author_ids": [],
    "abstract": "Teaching applied ethics in computer science has shifted from a perspective of\nteaching about professional codes of conduct and an emphasis on risk management\ntowards a broader understanding of the impacts of computing on humanity and the\nenvironment and the principles and practices of responsible computing. One of\nthe primary shifts in the approach to teaching computing ethics comes from\nresearch in the social sciences and humanities. This position is grounded in\nthe idea that all computing artifacts, projects, tools, and products are\nsituated within a set of ideas, attitudes, goals, and cultural norms. This\nmeans that all computing endeavors have embedded within them a set of values.\nTo teach responsible computing always requires us to first recognize that\ncomputing happens in a context that is shaped by cultural values, including our\nown professional culture and values.\n  The purpose of this paper is to highlight current scholarship, principles,\nand practices in the teaching of responsible computing in undergraduate\ncomputer science settings. The paper is organized around four primary sections:\n1) a high-level rationale for the adoption of different pedagogical approaches\nbased on program context and course learning goals, 2) a brief survey of\nresponsible computing pedagogical approaches; 3) illustrative examples of how\ntopics within the CS 2023 Social, Ethical, and Professional (SEP) knowledge\narea can be implemented and assessed across the broad spectrum of undergraduate\ncomputing courses; and 4) links to examples of current best practices, tools,\nand resources for faculty to build responsible computing teaching into their\nspecific instructional settings and CS2023 knowledge areas.",
    "published_date": "2025-02-15T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "K.3.2; K.4.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.10856v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.10771v1",
    "title": "Assessing the Trustworthiness of Electronic Identity Management Systems: Framework and Insights from Inception to Deployment",
    "authors": [
      "Mirko Bottarelli",
      "Gregory Epiphaniou",
      "Shah Mahmood",
      "Mark Hooper",
      "Carsten Maple"
    ],
    "author_ids": [],
    "abstract": "The growing dependence on Electronic Identity Management Systems (EIDS) and\nrecent advancements, such as non-human ID management, require a thorough\nevaluation of their trustworthiness. Assessing EIDS's trustworthiness ensures\nsecurity, privacy, and reliability in managing sensitive user information. It\nsafeguards against fraud, unauthorised access, and data breaches, fostering\nuser confidence. Existing frameworks primarily focus on specific dimensions\nsuch as security and privacy, often neglecting critical dimensions such as\nethics, resilience, robustness, and reliability. This paper introduces an\nintegrated Digital Identity Systems Trustworthiness Assessment Framework\n(DISTAF) encapsulating these six pillars. It is supported by over 65 mechanisms\nand over 400 metrics derived from international standards and technical\nguidelines. By addressing the lifecycle of DIMS from design to deployment, our\nDISTAF evaluates trustworthiness at granular levels while remaining accessible\nto diverse stakeholders. We demonstrate the application of DISTAF through a\nreal-world implementation using a Modular Open Source Identity Platform (MOSIP)\ninstance, refining its metrics to simplify trustworthiness assessment. Our\napproach introduces clustering mechanisms for metrics, hierarchical scoring,\nand mandatory criteria to ensure robust and consistent evaluations across an\nEIDS in both the design and operation stages. Furthermore, DISTAF is adaptable\nto emerging technologies like Self-Sovereign Identity (SSI), integrating\nprivacy-enhancing techniques and ethical considerations to meet modern\nchallenges. The assessment tool developed alongside DISTAF provides a\nuser-centric methodology and a simplified yet effective self-assessment\nprocess, enabling system designers and assessors to identify system gaps,\nimprove configurations, and enhance public trust.",
    "published_date": "2025-02-15T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.10771v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.10725v2",
    "title": "PropNet: a White-Box and Human-Like Network for Sentence Representation",
    "authors": [
      "Fei Yang"
    ],
    "author_ids": [],
    "abstract": "Transformer-based embedding methods have dominated the field of sentence\nrepresentation in recent years. Although they have achieved remarkable\nperformance on NLP missions, such as semantic textual similarity (STS) tasks,\ntheir black-box nature and large-data-driven training style have raised\nconcerns, including issues related to bias, trust, and safety. Many efforts\nhave been made to improve the interpretability of embedding models, but these\nproblems have not been fundamentally resolved. To achieve inherent\ninterpretability, we propose a purely white-box and human-like sentence\nrepresentation network, PropNet. Inspired by findings from cognitive science,\nPropNet constructs a hierarchical network based on the propositions contained\nin a sentence. While experiments indicate that PropNet has a significant gap\ncompared to state-of-the-art (SOTA) embedding models in STS tasks, case studies\nreveal substantial room for improvement. Additionally, PropNet enables us to\nanalyze and understand the human cognitive processes underlying STS benchmarks.",
    "published_date": "2025-02-15T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.10725v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.10641v1",
    "title": "Toward Equitable Access: Leveraging Crowdsourced Reviews to Investigate Public Perceptions of Health Resource Accessibility",
    "authors": [
      "Zhaoqian Xue",
      "Guanhong Liu",
      "Kai Wei",
      "Chong Zhang",
      "Qingcheng Zeng",
      "Songhua Hu",
      "Wenyue Hua",
      "Lizhou Fan",
      "Yongfeng Zhang",
      "Lingyao Li"
    ],
    "author_ids": [],
    "abstract": "Access to health resources is a critical determinant of public well-being and\nsocietal resilience, particularly during public health crises when demand for\nmedical services and preventive care surges. However, disparities in\naccessibility persist across demographic and geographic groups, raising\nconcerns about equity. Traditional survey methods often fall short due to\nlimitations in coverage, cost, and timeliness. This study leverages\ncrowdsourced data from Google Maps reviews, applying advanced natural language\nprocessing techniques, specifically ModernBERT, to extract insights on public\nperceptions of health resource accessibility in the United States during the\nCOVID-19 pandemic. Additionally, we employ Partial Least Squares regression to\nexamine the relationship between accessibility perceptions and key\nsocioeconomic and demographic factors including political affiliation, racial\ncomposition, and educational attainment. Our findings reveal that public\nperceptions of health resource accessibility varied significantly across the\nU.S., with disparities peaking during the pandemic and slightly easing\npost-crisis. Political affiliation, racial demographics, and education levels\nemerged as key factors shaping these perceptions. These findings underscore the\nneed for targeted interventions and policy measures to address inequities,\nfostering a more inclusive healthcare infrastructure that can better withstand\nfuture public health challenges.",
    "published_date": "2025-02-15T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.10641v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.10636v2",
    "title": "USER-VLM 360: Personalized Vision Language Models with User-aware Tuning for Social Human-Robot Interactions",
    "authors": [
      "Hamed Rahimi",
      "Adil Bahaj",
      "Mouad Abrini",
      "Mahdi Khoramshahi",
      "Mounir Ghogho",
      "Mohamed Chetouani"
    ],
    "author_ids": [],
    "abstract": "The integration of vision-language models into robotic systems constitutes a\nsignificant advancement in enabling machines to interact with their\nsurroundings in a more intuitive manner. While VLMs offer rich multimodal\nreasoning, existing approaches lack user-specific adaptability, often relying\non generic interaction paradigms that fail to account for individual\nbehavioral, contextual, or socio-emotional nuances. When customization is\nattempted, ethical concerns arise from unmitigated biases in user data, risking\nexclusion or unfair treatment. To address these dual challenges, we propose\nUser-VLM 360{\\deg}, a holistic framework integrating multimodal user modeling\nwith bias-aware optimization. Our approach features: (1) user-aware tuning that\nadapts interactions in real time using visual-linguistic signals; (2) bias\nmitigation via preference optimization; and (3) curated 360{\\deg} socio-emotive\ninteraction datasets annotated with demographic, emotion, and relational\nmetadata. Evaluations across eight benchmarks demonstrate state-of-the-art\nresults: +35.3% F1 in personalized VQA, +47.5% F1 in facial features\nunderstanding, 15% bias reduction, and 30X speedup over baselines. Ablation\nstudies confirm component efficacy, and deployment on the Pepper robot\nvalidates real-time adaptability across diverse users. We open-source\nparameter-efficient 3B/10B models and an ethical verification framework for\nresponsible adaptation.",
    "published_date": "2025-02-15T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.10636v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.10592v1",
    "title": "Deploying Fair and Efficient Course Allocation Mechanisms",
    "authors": [
      "George Bissias",
      "Cyrus Cousins",
      "Paula Navarrete Diaz",
      "Yair Zick"
    ],
    "author_ids": [],
    "abstract": "Universities regularly face the challenging task of assigning classes to\nthousands of students while considering their preferences, along with course\nschedules and capacities. Ensuring the effectiveness and fairness of course\nallocation mechanisms is crucial to guaranteeing student satisfaction and\noptimizing resource utilization. We approach this problem from an economic\nperspective, using formal justice criteria to evaluate different algorithmic\nframeworks. To evaluate our frameworks, we conduct a large scale survey of\nuniversity students at University of Massachusetts Amherst, collecting over\n1,000 student preferences. This is, to our knowledge, the largest publicly\navailable dataset of student preferences. We develop software for generating\nsynthetic student preferences over courses, and implement four allocation\nalgorithms: the serial dictatorship algorithm used by University of\nMassachusetts Amherst; Round Robin; an Integer Linear Program; and the Yankee\nSwap algorithm. We propose improvements to the Yankee Swap framework to handle\nscenarios with item multiplicities. Through experimentation with the Fall 2024\nComputer Science course schedule at University of Massachusetts Amherst, we\nevaluate each algorithm's performance relative to standard justice criteria,\nproviding insights into fair course allocation in large university settings.",
    "published_date": "2025-02-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.10592v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.10577v1",
    "title": "Man Made Language Models? Evaluating LLMs' Perpetuation of Masculine Generics Bias",
    "authors": [
      "Enzo Doyen",
      "Amalia Todirascu"
    ],
    "author_ids": [],
    "abstract": "Large language models (LLMs) have been shown to propagate and even amplify\ngender bias, in English and other languages, in specific or constrained\ncontexts. However, no studies so far have focused on gender biases conveyed by\nLLMs' responses to generic instructions, especially with regard to masculine\ngenerics (MG). MG are a linguistic feature found in many gender-marked\nlanguages, denoting the use of the masculine gender as a \"default\" or\nsupposedly neutral gender to refer to mixed group of men and women, or of a\nperson whose gender is irrelevant or unknown. Numerous psycholinguistics\nstudies have shown that MG are not neutral and induce gender bias. This work\naims to analyze the use of MG by both proprietary and local LLMs in responses\nto generic instructions and evaluate their MG bias rate. We focus on French and\ncreate a human noun database from existing lexical resources. We filter\nexisting French instruction datasets to retrieve generic instructions and\nanalyze the responses of 6 different LLMs. Overall, we find that\n$\\approx$39.5\\% of LLMs' responses to generic instructions are MG-biased\n($\\approx$73.1\\% across responses with human nouns). Our findings also reveal\nthat LLMs are reluctant to using gender-fair language spontaneously.",
    "published_date": "2025-02-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.10577v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.10562v1",
    "title": "Detecting and Monitoring Bias for Subgroups in Breast Cancer Detection AI",
    "authors": [
      "Amit Kumar Kundu",
      "Florence X. Doo",
      "Vaishnavi Patil",
      "Amitabh Varshney",
      "Joseph Jaja"
    ],
    "author_ids": [],
    "abstract": "Automated mammography screening plays an important role in early breast\ncancer detection. However, current machine learning models, developed on some\ntraining datasets, may exhibit performance degradation and bias when deployed\nin real-world settings. In this paper, we analyze the performance of\nhigh-performing AI models on two mammography datasets-the Emory Breast Imaging\nDataset (EMBED) and the RSNA 2022 challenge dataset. Specifically, we evaluate\nhow these models perform across different subgroups, defined by six attributes,\nto detect potential biases using a range of classification metrics. Our\nanalysis identifies certain subgroups that demonstrate notable\nunderperformance, highlighting the need for ongoing monitoring of these\nsubgroups' performance. To address this, we adopt a monitoring method designed\nto detect performance drifts over time. Upon identifying a drift, this method\nissues an alert, which can enable timely interventions. This approach not only\nprovides a tool for tracking the performance but also helps ensure that AI\nmodels continue to perform effectively across diverse populations.",
    "published_date": "2025-02-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.10562v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.05713v1",
    "title": "Beyond English: Unveiling Multilingual Bias in LLM Copyright Compliance",
    "authors": [
      "Yupeng Chen",
      "Xiaoyu Zhang",
      "Yixian Huang",
      "Qian Xie"
    ],
    "author_ids": [],
    "abstract": "Large Language Models (LLMs) have raised significant concerns regarding the\nfair use of copyright-protected content. While prior studies have examined the\nextent to which LLMs reproduce copyrighted materials, they have predominantly\nfocused on English, neglecting multilingual dimensions of copyright protection.\nIn this work, we investigate multilingual biases in LLM copyright protection by\naddressing two key questions: (1) Do LLMs exhibit bias in protecting\ncopyrighted works across languages? (2) Is it easier to elicit copyrighted\ncontent using prompts in specific languages? To explore these questions, we\nconstruct a dataset of popular song lyrics in English, French, Chinese, and\nKorean and systematically probe seven LLMs using prompts in these languages.\nOur findings reveal significant imbalances in LLMs' handling of copyrighted\ncontent, both in terms of the language of the copyrighted material and the\nlanguage of the prompt. These results highlight the need for further research\nand development of more robust, language-agnostic copyright protection\nmechanisms to ensure fair and consistent protection across languages.",
    "published_date": "2025-02-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.05713v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.10236v2",
    "title": "Shaping Inductive Bias in Diffusion Models through Frequency-Based Noise Control",
    "authors": [
      "Thomas Jiralerspong",
      "Berton Earnshaw",
      "Jason Hartford",
      "Yoshua Bengio",
      "Luca Scimeca"
    ],
    "author_ids": [],
    "abstract": "Diffusion Probabilistic Models (DPMs) are powerful generative models that\nhave achieved unparalleled success in a number of generative tasks. In this\nwork, we aim to build inductive biases into the training and sampling of\ndiffusion models to better accommodate the target distribution of the data to\nmodel. For topologically structured data, we devise a frequency-based noising\noperator to purposefully manipulate, and set, these inductive biases. We first\nshow that appropriate manipulations of the noising forward process can lead\nDPMs to focus on particular aspects of the distribution to learn. We show that\ndifferent datasets necessitate different inductive biases, and that appropriate\nfrequency-based noise control induces increased generative performance compared\nto standard diffusion. Finally, we demonstrate the possibility of ignoring\ninformation at particular frequencies while learning. We show this in an image\ncorruption and recovery task, where we train a DPM to recover the original\ntarget distribution after severe noise corruption.",
    "published_date": "2025-02-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.10236v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.10195v1",
    "title": "Exploring the Camera Bias of Person Re-identification",
    "authors": [
      "Myungseo Song",
      "Jin-Woo Park",
      "Jong-Seok Lee"
    ],
    "author_ids": [],
    "abstract": "We empirically investigate the camera bias of person re-identification (ReID)\nmodels. Previously, camera-aware methods have been proposed to address this\nissue, but they are largely confined to training domains of the models. We\nmeasure the camera bias of ReID models on unseen domains and reveal that camera\nbias becomes more pronounced under data distribution shifts. As a debiasing\nmethod for unseen domain data, we revisit feature normalization on embedding\nvectors. While the normalization has been used as a straightforward solution,\nits underlying causes and broader applicability remain unexplored. We analyze\nwhy this simple method is effective at reducing bias and show that it can be\napplied to detailed bias factors such as low-level image properties and body\nangle. Furthermore, we validate its generalizability across various models and\nbenchmarks, highlighting its potential as a simple yet effective test-time\npostprocessing method for ReID. In addition, we explore the inherent risk of\ncamera bias in unsupervised learning of ReID models. The unsupervised models\nremain highly biased towards camera labels even for seen domain data,\nindicating substantial room for improvement. Based on observations of the\nnegative impact of camera-biased pseudo labels on training, we suggest simple\ntraining strategies to mitigate the bias. By applying these strategies to\nexisting unsupervised learning algorithms, we show that significant performance\nimprovements can be achieved with minor modifications.",
    "published_date": "2025-02-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.10195v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.10161v1",
    "title": "Revisiting the Berkeley Admissions data: Statistical Tests for Causal Hypotheses",
    "authors": [
      "Sourbh Bhadane",
      "Joris M. Mooij",
      "Philip Boeken",
      "Onno Zoeter"
    ],
    "author_ids": [],
    "abstract": "Reasoning about fairness through correlation-based notions is rife with\npitfalls. The 1973 University of California, Berkeley graduate school\nadmissions case from Bickel et. al. (1975) is a classic example of one such\npitfall, namely Simpson's paradox. The discrepancy in admission rates among\nmales and female applicants, in the aggregate data over all departments,\nvanishes when admission rates per department are examined. We reason about the\nBerkeley graduate school admissions case through a causal lens. In the process,\nwe introduce a statistical test for causal hypothesis testing based on Pearl's\ninstrumental-variable inequalities (Pearl 1995). We compare different causal\nnotions of fairness that are based on graphical, counterfactual and\ninterventional queries on the causal model, and develop statistical tests for\nthese notions that use only observational data. We study the logical relations\nbetween notions, and show that while notions may not be equivalent, their\ncorresponding statistical tests coincide for the case at hand. We believe that\na thorough case-based causal analysis helps develop a more principled\nunderstanding of both causal hypothesis testing and fairness.",
    "published_date": "2025-02-14T00:00:00",
    "year": 2025,
    "categories": [
      "stat.ME",
      "cs.CY",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.10161v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.10080v1",
    "title": "Coordinated control of multiple autonomous surface vehicles: challenges and advances -- a systematic review",
    "authors": [
      "Manuel Gantiva Osorioa",
      "Carmelina Ierardia",
      "Isabel Jurado Floresa",
      "Mario Pereira Martína",
      "Pablo Millán Gata"
    ],
    "author_ids": [],
    "abstract": "The increasing use and implementation of Autonomous Surface Vessels (ASVs)\nfor various activities in maritime environments is expected to drive a rise in\ndevelopments and research on their control. Particularly, the coordination of\nmultiple ASVs presents novel challenges and opportunities, requiring\ninterdisciplinary research efforts at the intersection of robotics, control\ntheory, communication systems, and marine sciences. The wide variety of\nmissions or objectives for which these vessels can be collectively used allows\nfor the application and combination of different control techniques. This\nincludes the exploration of machine learning to consider aspects previously\ndeemed infeasible. This review provides a comprehensive exploration of\ncoordinated ASV control while addressing critical gaps left by previous\nreviews. Unlike previous works, we adopt a systematic approach to ensure\nintegrity and minimize bias in article selection. We delve into the complex\nworld of sub-actuated ASVs with a focus on customized control strategies and\nthe integration of machine learning techniques for increased autonomy. By\nsynthesizing recent advances and identifying emerging trends, we offer insights\nthat drive this field forward, providing both a comprehensive overview of\nstate-of-the-art techniques and guidance for future research efforts.",
    "published_date": "2025-02-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.10080v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.10036v1",
    "title": "Automation Bias in the AI Act: On the Legal Implications of Attempting to De-Bias Human Oversight of AI",
    "authors": [
      "Johann Laux",
      "Hannah Ruschemeier"
    ],
    "author_ids": [],
    "abstract": "This paper examines the legal implications of the explicit mentioning of\nautomation bias (AB) in the Artificial Intelligence Act (AIA). The AIA mandates\nhuman oversight for high-risk AI systems and requires providers to enable\nawareness of AB, i.e., the tendency to over-rely on AI outputs. The paper\nanalyses how this extra-juridical concept is embedded in the AIA, the division\nof responsibility between AI providers and deployers, and the challenges of\nlegally enforcing this novel awareness requirement. The analysis shows that the\nAIA's focus on providers does not adequately address design and context as\ncauses of AB, and questions whether the AIA should directly regulate the risk\nof AB rather than just mandating awareness. As the AIA's approach requires a\nbalance between legal mandates and behavioural science, the paper proposes that\nharmonised standards should reference the state of research on AB and human-AI\ninteraction. Ultimately, further empirical research will be essential for\neffective safeguards.",
    "published_date": "2025-02-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.10036v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.09993v1",
    "title": "Navigating Label Ambiguity for Facial Expression Recognition in the Wild",
    "authors": [
      "JunGyu Lee",
      "Yeji Choi",
      "Haksub Kim",
      "Ig-Jae Kim",
      "Gi Pyo Nam"
    ],
    "author_ids": [],
    "abstract": "Facial expression recognition (FER) remains a challenging task due to label\nambiguity caused by the subjective nature of facial expressions and noisy\nsamples. Additionally, class imbalance, which is common in real-world datasets,\nfurther complicates FER. Although many studies have shown impressive\nimprovements, they typically address only one of these issues, leading to\nsuboptimal results. To tackle both challenges simultaneously, we propose a\nnovel framework called Navigating Label Ambiguity (NLA), which is robust under\nreal-world conditions. The motivation behind NLA is that dynamically estimating\nand emphasizing ambiguous samples at each iteration helps mitigate noise and\nclass imbalance by reducing the model's bias toward majority classes. To\nachieve this, NLA consists of two main components: Noise-aware Adaptive\nWeighting (NAW) and consistency regularization. Specifically, NAW adaptively\nassigns higher importance to ambiguous samples and lower importance to noisy\nones, based on the correlation between the intermediate prediction scores for\nthe ground truth and the nearest negative. Moreover, we incorporate a\nregularization term to ensure consistent latent distributions. Consequently,\nNLA enables the model to progressively focus on more challenging ambiguous\nsamples, which primarily belong to the minority class, in the later stages of\ntraining. Extensive experiments demonstrate that NLA outperforms existing\nmethods in both overall and mean accuracy, confirming its robustness against\nnoise and class imbalance. To the best of our knowledge, this is the first\nframework to address both problems simultaneously.",
    "published_date": "2025-02-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.09993v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.09912v1",
    "title": "Breaking the Familiarity Bias: Employing Virtual Reality Environments to Enhance Team Formation and Inclusion",
    "authors": [
      "Mariana Fernandez-Espinosa",
      "Kara Clouse",
      "Dylan Sellars",
      "Danny Tong",
      "Michael Bsales",
      "Sophonie Alcindor",
      "Timothy D Hubbard",
      "Michael Villano",
      "Diego Gómez-Zará"
    ],
    "author_ids": [],
    "abstract": "Team closeness provides the foundations of trust and communication,\ncontributing to teams' success and viability. However, newcomers often struggle\nto be included in a team since incumbents tend to interact more with other\nexisting members. Previous research suggests that online communication\ntechnologies can help team inclusion by mitigating members' perceived\ndifferences. In this study, we test how virtual reality (VR) can promote team\ncloseness when forming teams. We conducted a between-subject experiment with\nteams working in-person and VR, where two members interacted first, and then a\nthird member was added later to conduct a hidden-profile task. Participants\nevaluated how close they felt with their teammates after the task was\ncompleted. Our results show that VR newcomers felt closer to the incumbents\nthan in-person newcomers. However, incumbents' closeness to newcomers did not\nvary across conditions. We discuss the implications of these findings and offer\nsuggestions for how VR can promote inclusion.",
    "published_date": "2025-02-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.09912v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.09831v1",
    "title": "Learning Fair Policies for Infectious Diseases Mitigation using Path Integral Control",
    "authors": [
      "Zhuangzhuang Jia",
      "Hyuk Park",
      "Gökçe Dayanıklı",
      "Grani A. Hanasusanto"
    ],
    "author_ids": [],
    "abstract": "Infectious diseases pose major public health challenges to society,\nhighlighting the importance of designing effective policies to reduce economic\nloss and mortality. In this paper, we propose a framework for sequential\ndecision-making under uncertainty to design fairness-aware disease mitigation\npolicies that incorporate various measures of unfairness. Specifically, our\napproach learns equitable vaccination and lockdown strategies based on a\nstochastic multi-group SIR model. To address the challenges of solving the\nresulting sequential decision-making problem, we adopt the path integral\ncontrol algorithm as an efficient solution scheme. Through a case study, we\ndemonstrate that our approach effectively improves fairness compared to\nconventional methods and provides valuable insights for policymakers.",
    "published_date": "2025-02-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.09831v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.05307v1",
    "title": "Toward Total Recall: Enhancing FAIRness through AI-Driven Metadata Standardization",
    "authors": [
      "Sowmya S Sundaram",
      "Mark A Musen"
    ],
    "author_ids": [],
    "abstract": "Current metadata often suffer from incompleteness, inconsistency, and\nincorrect formatting, hindering effective data reuse and discovery. Using GPT-4\nand a metadata knowledge base (CEDAR), we devised a method that standardizes\nmetadata in scientific data sets, ensuring the adherence to community\nstandards. The standardization process involves correcting and refining\nmetadata entries to conform to established guidelines, significantly improving\nsearch performance and recall metrics. The investigation uses BioSample and GEO\nrepositories to demonstrate the impact of these enhancements, showcasing how\nstandardized metadata lead to better retrieval outcomes. The average recall\nimproves significantly, rising from 17.65\\% with the baseline raw datasets of\nBioSample and GEO to 62.87\\% with our proposed metadata standardization\npipeline. This finding highlights the transformative impact of integrating\nadvanced AI models with structured metadata curation tools in achieving more\neffective and reliable data retrieval.",
    "published_date": "2025-02-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.05307v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.09765v2",
    "title": "Differential Adjusted Parity for Learning Fair Representations",
    "authors": [
      "Bucher Sahyouni",
      "Matthew Vowels",
      "Liqun Chen",
      "Simon Hadfield"
    ],
    "author_ids": [],
    "abstract": "The development of fair and unbiased machine learning models remains an\nongoing objective for researchers in the field of artificial intelligence. We\nintroduce the Differential Adjusted Parity (DAP) loss to produce unbiased\ninformative representations. It utilises a differentiable variant of the\nadjusted parity metric to create a unified objective function. By combining\ndownstream task classification accuracy and its inconsistency across sensitive\nfeature domains, it provides a single tool to increase performance and mitigate\nbias. A key element in this approach is the use of soft balanced accuracies. In\ncontrast to previous non-adversarial approaches, DAP does not suffer a\ndegeneracy where the metric is satisfied by performing equally poorly across\nall sensitive domains. It outperforms several adversarial models on downstream\ntask accuracy and fairness in our analysis. Specifically, it improves the\ndemographic parity, equalized odds and sensitive feature accuracy by as much as\n22.5\\%, 44.1\\% and 40.1\\%, respectively, when compared to the best performing\nadversarial approaches on these metrics. Overall, the DAP loss and its\nassociated metric can play a significant role in creating more fair machine\nlearning models.",
    "published_date": "2025-02-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.09765v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.09724v1",
    "title": "Navigating the Social Welfare Frontier: Portfolios for Multi-objective Reinforcement Learning",
    "authors": [
      "Cheol Woo Kim",
      "Jai Moondra",
      "Shresth Verma",
      "Madeleine Pollack",
      "Lingkai Kong",
      "Milind Tambe",
      "Swati Gupta"
    ],
    "author_ids": [],
    "abstract": "In many real-world applications of reinforcement learning (RL), deployed\npolicies have varied impacts on different stakeholders, creating challenges in\nreaching consensus on how to effectively aggregate their preferences.\nGeneralized $p$-means form a widely used class of social welfare functions for\nthis purpose, with broad applications in fair resource allocation, AI\nalignment, and decision-making. This class includes well-known welfare\nfunctions such as Egalitarian, Nash, and Utilitarian welfare. However,\nselecting the appropriate social welfare function is challenging for\ndecision-makers, as the structure and outcomes of optimal policies can be\nhighly sensitive to the choice of $p$. To address this challenge, we study the\nconcept of an $\\alpha$-approximate portfolio in RL, a set of policies that are\napproximately optimal across the family of generalized $p$-means for all $p \\in\n[-\\infty, 1]$. We propose algorithms to compute such portfolios and provide\ntheoretical guarantees on the trade-offs among approximation factor, portfolio\nsize, and computational efficiency. Experimental results on synthetic and\nreal-world datasets demonstrate the effectiveness of our approach in\nsummarizing the policy space induced by varying $p$ values, empowering\ndecision-makers to navigate this landscape more effectively.",
    "published_date": "2025-02-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.09724v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.09716v1",
    "title": "Genetic Data Governance in Crisis: Policy Recommendations for Safeguarding Privacy and Preventing Discrimination",
    "authors": [
      "Vivek Ramanan",
      "Ria Vinod",
      "Cole Williams",
      "Sohini Ramachandran",
      "Suresh Venkatasubramanian"
    ],
    "author_ids": [],
    "abstract": "Genetic data collection has become ubiquitous today. The ability to\nmeaningfully interpret genetic data has motivated its widespread use, providing\ncrucial insights into human health and ancestry while driving important public\nhealth initiatives. Easy access to genetic testing has fueled a rapid expansion\nof recreational direct-to-consumer offerings. However, the growth of genetic\ndatasets and their applications has created significant privacy and\ndiscrimination risks, as our understanding of the scientific basis for genetic\ntraits continues to evolve. In this paper, we organize the uses of genetic data\nalong four distinct \"pillars\": clinical practice, research, forensic and\ngovernment use, and recreational use. Using our scientific understanding of\ngenetics, genetic inference methods and their associated risks, and current\npublic protections, we build a risk assessment framework that identifies key\nvalues that any governance system must preserve. We analyze case studies using\nthis framework to assess how well existing regulatory frameworks preserve\ndesired values. Our investigation reveals critical gaps in these frameworks and\nidentifies specific threats to privacy and personal liberties, particularly\nthrough genetic discrimination. We propose comprehensive policy reforms to: (1)\nupdate the legal definition of genetic data to protect against modern\ntechnological capabilities, (2) expand the Genetic Information\nNondiscrimination Act (GINA) to cover currently unprotected domains, and (3)\nestablish a unified regulatory framework under a single governing body to\noversee all applications of genetic data. We conclude with three open questions\nabout genetic data: the challenges posed by its relational nature, including\nconsent for relatives and minors; the complexities of international data\ntransfer; and its potential integration into large language models.",
    "published_date": "2025-02-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "K.4.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.09716v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.09564v3",
    "title": "Diffusing DeBias: Synthetic Bias Amplification for Model Debiasing",
    "authors": [
      "Massimiliano Ciranni",
      "Vito Paolo Pastore",
      "Roberto Di Via",
      "Enzo Tartaglione",
      "Francesca Odone",
      "Vittorio Murino"
    ],
    "author_ids": [],
    "abstract": "Deep learning model effectiveness in classification tasks is often challenged\nby the quality and quantity of training data whenever they are affected by\nstrong spurious correlations between specific attributes and target labels.\nThis results in a form of bias affecting training data, which typically leads\nto unrecoverable weak generalization in prediction. This paper aims at facing\nthis problem by leveraging bias amplification with generated synthetic data: we\nintroduce Diffusing DeBias (DDB), a novel approach acting as a plug-in for\ncommon methods of unsupervised model debiasing exploiting the inherent\nbias-learning tendency of diffusion models in data generation. Specifically,\nour approach adopts conditional diffusion models to generate synthetic\nbias-aligned images, which replace the original training set for learning an\neffective bias amplifier model that we subsequently incorporate into an\nend-to-end and a two-step unsupervised debiasing approach. By tackling the\nfundamental issue of bias-conflicting training samples memorization in learning\nauxiliary models, typical of this type of techniques, our proposed method beats\ncurrent state-of-the-art in multiple benchmark datasets, demonstrating its\npotential as a versatile and effective tool for tackling bias in deep learning\nmodels.",
    "published_date": "2025-02-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CV",
      "I.4; I.5"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.09564v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.09550v1",
    "title": "A Nitsche method for incompressible fluids with general dynamic boundary conditions",
    "authors": [
      "Pablo Alexei Gazca-Orozco",
      "Franz Gmeineder",
      "Erika Maringová Kokavcová",
      "Tabea Tscherpel"
    ],
    "author_ids": [],
    "abstract": "Both Newtonian and non-Newtonian fluids may exhibit complex slip behaviour at\nthe boundary. We examine a broad class of slip boundary conditions that\ngeneralises the commonly used Navier slip, perfect slip, stick-slip and Tresca\nfriction boundary conditions. In particular, set-valued, non-monotone,\nnoncoercive and dynamic relations may occur. For a unifying framework of such\nrelations, we present a fully discrete numerical scheme for the time-dependent\nNavier-Stokes equations subject to impermeability and general slip type\nboundary conditions on polyhedral domains. Based on compactness arguments, we\nprove convergence of subsequences, finally ensuring the existence of a weak\nsolution. The numerical scheme uses a general inf-sup stable pair of finite\nelement spaces for the velocity and pressure, a regularisation approach for the\nimplicit slip boundary condition and, most importantly, a Nitsche method to\nimpose the impermeability and a backward Euler time stepping. One of the key\ntools in the convergence proof is an inhomogeneous Korn inequality that\nincludes a normal trace term.",
    "published_date": "2025-02-13T00:00:00",
    "year": 2025,
    "categories": [
      "math.NA",
      "cs.NA",
      "65N30, 76D07, 76M10"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.09550v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.09484v1",
    "title": "PenTest++: Elevating Ethical Hacking with AI and Automation",
    "authors": [
      "Haitham S. Al-Sinani",
      "Chris J. Mitchell"
    ],
    "author_ids": [],
    "abstract": "Traditional ethical hacking relies on skilled professionals and\ntime-intensive command management, which limits its scalability and efficiency.\nTo address these challenges, we introduce PenTest++, an AI-augmented system\nthat integrates automation with generative AI (GenAI) to optimise ethical\nhacking workflows. Developed in a controlled virtual environment, PenTest++\nstreamlines critical penetration testing tasks, including reconnaissance,\nscanning, enumeration, exploitation, and documentation, while maintaining a\nmodular and adaptable design. The system balances automation with human\noversight, ensuring informed decision-making at key stages, and offers\nsignificant benefits such as enhanced efficiency, scalability, and\nadaptability. However, it also raises ethical considerations, including privacy\nconcerns and the risks of AI-generated inaccuracies (hallucinations). This\nresearch underscores the potential of AI-driven systems like PenTest++ to\ncomplement human expertise in cybersecurity by automating routine tasks,\nenabling professionals to focus on strategic decision-making. By incorporating\nrobust ethical safeguards and promoting ongoing refinement, PenTest++\ndemonstrates how AI can be responsibly harnessed to address operational and\nethical challenges in the evolving cybersecurity landscape.",
    "published_date": "2025-02-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.09484v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.09688v1",
    "title": "Towards Virtual Clinical Trials of Radiology AI with Conditional Generative Modeling",
    "authors": [
      "Benjamin D. Killeen",
      "Bohua Wan",
      "Aditya V. Kulkarni",
      "Nathan Drenkow",
      "Michael Oberst",
      "Paul H. Yi",
      "Mathias Unberath"
    ],
    "author_ids": [],
    "abstract": "Artificial intelligence (AI) is poised to transform healthcare by enabling\npersonalized and efficient care through data-driven insights. Although\nradiology is at the forefront of AI adoption, in practice, the potential of AI\nmodels is often overshadowed by severe failures to generalize: AI models can\nhave performance degradation of up to 20% when transitioning from controlled\ntest environments to clinical use by radiologists. This mismatch raises\nconcerns that radiologists will be misled by incorrect AI predictions in\npractice and/or grow to distrust AI, rendering these promising technologies\npractically ineffectual. Exhaustive clinical trials of AI models on abundant\nand diverse data is thus critical to anticipate AI model degradation when\nencountering varied data samples. Achieving these goals, however, is\nchallenging due to the high costs of collecting diverse data samples and\ncorresponding annotations. To overcome these limitations, we introduce a novel\nconditional generative AI model designed for virtual clinical trials (VCTs) of\nradiology AI, capable of realistically synthesizing full-body CT images of\npatients with specified attributes. By learning the joint distribution of\nimages and anatomical structures, our model enables precise replication of\nreal-world patient populations with unprecedented detail at this scale. We\ndemonstrate meaningful evaluation of radiology AI models through VCTs powered\nby our synthetic CT study populations, revealing model degradation and\nfacilitating algorithmic auditing for bias-inducing data attributes. Our\ngenerative AI approach to VCTs is a promising avenue towards a scalable\nsolution to assess model robustness, mitigate biases, and safeguard patient\ncare by enabling simpler testing and evaluation of AI models in any desired\nrange of diverse patient populations.",
    "published_date": "2025-02-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.09688v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.09319v1",
    "title": "Bridging Jensen Gap for Max-Min Group Fairness Optimization in Recommendation",
    "authors": [
      "Chen Xu",
      "Yuxin Li",
      "Wenjie Wang",
      "Liang Pang",
      "Jun Xu",
      "Tat-Seng Chua"
    ],
    "author_ids": [],
    "abstract": "Group max-min fairness (MMF) is commonly used in fairness-aware recommender\nsystems (RS) as an optimization objective, as it aims to protect marginalized\nitem groups and ensures a fair competition platform. However, our theoretical\nanalysis indicates that integrating MMF constraint violates the assumption of\nsample independence during optimization, causing the loss function to deviate\nfrom linear additivity. Such nonlinearity property introduces the Jensen gap\nbetween the model's convergence point and the optimal point if mini-batch\nsampling is applied. Both theoretical and empirical studies show that as the\nmini-batch size decreases and the group size increases, the Jensen gap will\nwiden accordingly. Some methods using heuristic re-weighting or debiasing\nstrategies have the potential to bridge the Jensen gap. However, they either\nlack theoretical guarantees or suffer from heavy computational costs. To\novercome these limitations, we first theoretically demonstrate that the\nMMF-constrained objective can be essentially reformulated as a group-weighted\noptimization objective. Then we present an efficient and effective algorithm\nnamed FairDual, which utilizes a dual optimization technique to minimize the\nJensen gap. Our theoretical analysis demonstrates that FairDual can achieve a\nsub-linear convergence rate to the globally optimal solution and the Jensen gap\ncan be well bounded under a mini-batch sampling strategy with random shuffle.\nExtensive experiments conducted using six large-scale RS backbone models on\nthree publicly available datasets demonstrate that FairDual outperforms all\nbaselines in terms of both accuracy and fairness. Our data and codes are shared\nat https://github.com/XuChen0427/FairDual.",
    "published_date": "2025-02-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.09319v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.09242v1",
    "title": "From large language models to multimodal AI: A scoping review on the potential of generative AI in medicine",
    "authors": [
      "Lukas Buess",
      "Matthias Keicher",
      "Nassir Navab",
      "Andreas Maier",
      "Soroosh Tayebi Arasteh"
    ],
    "author_ids": [],
    "abstract": "Generative artificial intelligence (AI) models, such as diffusion models and\nOpenAI's ChatGPT, are transforming medicine by enhancing diagnostic accuracy\nand automating clinical workflows. The field has advanced rapidly, evolving\nfrom text-only large language models for tasks such as clinical documentation\nand decision support to multimodal AI systems capable of integrating diverse\ndata modalities, including imaging, text, and structured data, within a single\nmodel. The diverse landscape of these technologies, along with rising interest,\nhighlights the need for a comprehensive review of their applications and\npotential. This scoping review explores the evolution of multimodal AI,\nhighlighting its methods, applications, datasets, and evaluation in clinical\nsettings. Adhering to PRISMA-ScR guidelines, we systematically queried PubMed,\nIEEE Xplore, and Web of Science, prioritizing recent studies published up to\nthe end of 2024. After rigorous screening, 144 papers were included, revealing\nkey trends and challenges in this dynamic field. Our findings underscore a\nshift from unimodal to multimodal approaches, driving innovations in diagnostic\nsupport, medical report generation, drug discovery, and conversational AI.\nHowever, critical challenges remain, including the integration of heterogeneous\ndata types, improving model interpretability, addressing ethical concerns, and\nvalidating AI systems in real-world clinical settings. This review summarizes\nthe current state of the art, identifies critical gaps, and provides insights\nto guide the development of scalable, trustworthy, and clinically impactful\nmultimodal AI solutions in healthcare.",
    "published_date": "2025-02-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.09242v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.09047v1",
    "title": "Optimal Algorithms in Linear Regression under Covariate Shift: On the Importance of Precondition",
    "authors": [
      "Yuanshi Liu",
      "Haihan Zhang",
      "Qian Chen",
      "Cong Fang"
    ],
    "author_ids": [],
    "abstract": "A common pursuit in modern statistical learning is to attain satisfactory\ngeneralization out of the source data distribution (OOD). In theory, the\nchallenge remains unsolved even under the canonical setting of covariate shift\nfor the linear model. This paper studies the foundational (high-dimensional)\nlinear regression where the ground truth variables are confined to an\nellipse-shape constraint and addresses two fundamental questions in this\nregime: (i) given the target covariate matrix, what is the min-max\n\\emph{optimal} algorithm under covariate shift? (ii) for what kinds of target\nclasses, the commonly-used SGD-type algorithms achieve optimality? Our analysis\nstarts with establishing a tight lower generalization bound via a Bayesian\nCramer-Rao inequality. For (i), we prove that the optimal estimator can be\nsimply a certain linear transformation of the best estimator for the source\ndistribution. Given the source and target matrices, we show that the\ntransformation can be efficiently computed via a convex program. The min-max\noptimal analysis for SGD leverages the idea that we recognize both the\naccumulated updates of the applied algorithms and the ideal transformation as\npreconditions on the learning variables. We provide sufficient conditions when\nSGD with its acceleration variants attain optimality.",
    "published_date": "2025-02-13T00:00:00",
    "year": 2025,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.09047v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.08976v1",
    "title": "Prophet Inequalities for Bandits, Cabinets, and DAGs",
    "authors": [
      "Robin Bowers",
      "Elias Lindgren",
      "Bo Waggoner"
    ],
    "author_ids": [],
    "abstract": "A decisionmaker faces $n$ alternatives, each of which represents a potential\nreward. After investing costly resources into investigating the alternatives,\nthe decisionmaker may select one, or more generally a feasible subset, and\nobtain the associated reward(s). The objective is to maximize the sum of\nrewards minus total costs invested. We consider this problem under a general\nmodel of an alternative as a \"Markov Search Process,\" a type of undiscounted\nMarkov Decision Process on a finite acyclic graph. Even simple cases generalize\nNP-hard problems such as Pandora's Box with nonobligatory inspection.\n  Despite the apparently adaptive and interactive nature of the problem, we\nprove optimal prophet inequalities for this problem under a variety of\ncombinatorial constraints. That is, we give approximation algorithms that\ninteract with the alternatives sequentially, where each must be fully explored\nand either selected or else discarded before the next arrives. In particular,\nwe obtain a computationally efficient $\\frac{1}{2}-\\epsilon$ prophet inequality\nfor Combinatorial Markov Search subject to any matroid constraint. This result\nimplies incentive-compatible mechanisms with constant Price of Anarchy for\nserving single-parameter agents when the agents strategically conduct\nindependent, costly search processes to discover their values.",
    "published_date": "2025-02-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.08976v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.08953v1",
    "title": "Integrated Optimization and Game Theory Framework for Fair Cost Allocation in Community Microgrids",
    "authors": [
      "K. Victor Sam Moses Babu",
      "Pratyush Chakraborty",
      "Mayukha Pal"
    ],
    "author_ids": [],
    "abstract": "Fair cost allocation in community microgrids remains a significant challenge\ndue to the complex interactions between multiple participants with varying load\nprofiles, distributed energy resources, and storage systems. Traditional cost\nallocation methods often fail to adequately address the dynamic nature of\nparticipant contributions and benefits, leading to inequitable distribution of\ncosts and reduced participant satisfaction. This paper presents a novel\nframework integrating multi-objective optimization with cooperative game theory\nfor fair and efficient microgrid operation and cost allocation. The proposed\napproach combines mixed-integer linear programming for optimal resource\ndispatch with Shapley value analysis for equitable benefit distribution,\nensuring both system efficiency and participant satisfaction. The framework was\nvalidated using real-world data across six distinct operational scenarios,\ndemonstrating significant improvements in both technical and economic\nperformance. Results show peak demand reductions ranging from 7.8% to 62.6%,\nsolar utilization rates reaching 114.8% through effective storage integration,\nand cooperative gains of up to $1,801.01 per day. The Shapley value-based\nallocation achieved balanced benefit-cost distributions, with net positions\nranging from -16.0% to +14.2% across different load categories, ensuring\nsustainable participant cooperation.",
    "published_date": "2025-02-13T00:00:00",
    "year": 2025,
    "categories": [
      "eess.SY",
      "cs.LG",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.08953v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.08893v1",
    "title": "Uncovering Disparities in Rideshare Drivers Earning and Work Patterns: A Case Study of Chicago",
    "authors": [
      "Hy Dang",
      "Yuwen Lu",
      "Jason Spicer",
      "Tamara Kay",
      "Di Yang",
      "Yang Yang",
      "Jay Brockman",
      "Meng Jiang",
      "Toby Jia-Jun Li"
    ],
    "author_ids": [],
    "abstract": "Ride-sharing services are revolutionizing urban mobility while simultaneously\nraising significant concerns regarding fairness and driver equity. This study\nemploys Chicago Trip Network Provider dataset to investigate disparities in\nride-sharing earnings between 2018 and 2023. Our analysis reveals marked\ntemporal shifts, including an earnings surge in early 2021 followed by\nfluctuations and a decline in inflation-adjusted income, as well as pronounced\nspatial disparities, with drivers in Central and airport regions earning\nsubstantially more than those in peripheral areas. Recognizing the limitations\nof trip-level data, we introduce a novel trip-driver assignment algorithm to\nreconstruct plausible daily work patterns, uncovering distinct driver clusters\nwith varied earning profiles. Notably, drivers operating during late-evening\nand overnight hours secure higher per-trip and hourly rates, while emerging\ngroups in low-demand regions face significant earnings deficits. Our findings\ncall for more transparent pricing models and a re-examination of platform\ndesign to promote equitable driver outcomes.",
    "published_date": "2025-02-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.08893v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.08888v1",
    "title": "LLM-Enhanced Multiple Instance Learning for Joint Rumor and Stance Detection with Social Context Information",
    "authors": [
      "Ruichao Yang",
      "Jing Ma",
      "Wei Gao",
      "Hongzhan Lin"
    ],
    "author_ids": [],
    "abstract": "The proliferation of misinformation, such as rumors on social media, has\ndrawn significant attention, prompting various expressions of stance among\nusers. Although rumor detection and stance detection are distinct tasks, they\ncan complement each other. Rumors can be identified by cross-referencing\nstances in related posts, and stances are influenced by the nature of the\nrumor. However, existing stance detection methods often require post-level\nstance annotations, which are costly to obtain. We propose a novel LLM-enhanced\nMIL approach to jointly predict post stance and claim class labels, supervised\nsolely by claim labels, using an undirected microblog propagation model. Our\nweakly supervised approach relies only on bag-level labels of claim veracity,\naligning with multi-instance learning (MIL) principles. To achieve this, we\ntransform the multi-class problem into multiple MIL-based binary classification\nproblems. We then employ a discriminative attention layer to aggregate the\noutputs from these classifiers into finer-grained classes. Experiments\nconducted on three rumor datasets and two stance datasets demonstrate the\neffectiveness of our approach, highlighting strong connections between rumor\nveracity and expressed stances in responding posts. Our method shows promising\nperformance in joint rumor and stance detection compared to the\nstate-of-the-art methods.",
    "published_date": "2025-02-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.08888v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.03888v2",
    "title": "AI for Scaling Legal Reform: Mapping and Redacting Racial Covenants in Santa Clara County",
    "authors": [
      "Faiz Surani",
      "Mirac Suzgun",
      "Vyoma Raman",
      "Christopher D. Manning",
      "Peter Henderson",
      "Daniel E. Ho"
    ],
    "author_ids": [],
    "abstract": "Legal reform can be challenging in light of the volume, complexity, and\ninterdependence of laws, codes, and records. One salient example of this\nchallenge is the effort to restrict and remove racially restrictive covenants,\nclauses in property deeds that historically barred individuals of specific\nraces from purchasing homes. Despite the Supreme Court holding such racial\ncovenants unenforceable in 1948, they persist in property records across the\nUnited States. Many jurisdictions have moved to identify and strike these\nprovisions, including California, which mandated in 2021 that all counties\nimplement such a process. Yet the scale can be overwhelming, with Santa Clara\nCounty (SCC) alone having over 24 million property deed documents, making\npurely manual review infeasible. We present a novel approach to addressing this\npressing issue, developed through a partnership with the SCC Clerk-Recorder's\nOffice. First, we leverage an open large language model, finetuned to detect\nracial covenants with high precision and recall. We estimate that this system\nreduces manual efforts by 86,500 person hours and costs less than 2% of the\ncost for a comparable off-the-shelf closed model. Second, we illustrate the\nCounty's integration of this model into responsible operational practice,\nincluding legal review and the creation of a historical registry, and release\nour model to assist the hundreds of jurisdictions engaged in similar efforts.\nFinally, our results reveal distinct periods of utilization of racial\ncovenants, sharp geographic clustering, and the disproportionate role of a\nsmall number of developers in maintaining housing discrimination. We estimate\nthat by 1950, one in four properties across the County were subject to racial\ncovenants.",
    "published_date": "2025-02-12T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.03888v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.08779v2",
    "title": "SB-Bench: Stereotype Bias Benchmark for Large Multimodal Models",
    "authors": [
      "Vishal Narnaware",
      "Ashmal Vayani",
      "Rohit Gupta",
      "Sirnam Swetha",
      "Mubarak Shah"
    ],
    "author_ids": [],
    "abstract": "Stereotype biases in Large Multimodal Models (LMMs) perpetuate harmful\nsocietal prejudices, undermining the fairness and equity of AI applications. As\nLMMs grow increasingly influential, addressing and mitigating inherent biases\nrelated to stereotypes, harmful generations, and ambiguous assumptions in\nreal-world scenarios has become essential. However, existing datasets\nevaluating stereotype biases in LMMs often lack diversity and rely on synthetic\nimages, leaving a gap in bias evaluation for real-world visual contexts. To\naddress this, we introduce the Stereotype Bias Benchmark (SB-bench), the most\ncomprehensive framework to date for assessing stereotype biases across nine\ndiverse categories with non-synthetic images. SB-bench rigorously evaluates\nLMMs through carefully curated, visually grounded scenarios, challenging them\nto reason accurately about visual stereotypes. It offers a robust evaluation\nframework featuring real-world visual samples, image variations, and\nmultiple-choice question formats. By introducing visually grounded queries that\nisolate visual biases from textual ones, SB-bench enables a precise and nuanced\nassessment of a model's reasoning capabilities across varying levels of\ndifficulty. Through rigorous testing of state-of-the-art open-source and\nclosed-source LMMs, SB-bench provides a systematic approach to assessing\nstereotype biases in LMMs across key social dimensions. This benchmark\nrepresents a significant step toward fostering fairness in AI systems and\nreducing harmful biases, laying the groundwork for more equitable and socially\nresponsible LMMs. Our code and dataset are publicly available.",
    "published_date": "2025-02-12T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.08779v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.08743v1",
    "title": "Signed, Sealed,... Confused: Exploring the Understandability and Severity of Policy Documents",
    "authors": [
      "Shikha Soneji",
      "Sourav Panda",
      "Sameer Neve",
      "Jonathan Dodge"
    ],
    "author_ids": [],
    "abstract": "In general, Terms of Service (ToS) and other policy documents are verbose and\nfull of legal jargon, which poses challenges for users to understand. To\nimprove user accessibility and transparency, the \"Terms of Service; Didn't\nRead\" (ToS;DR) project condenses intricate legal terminology into summaries and\noverall grades for the website's policy documents. Nevertheless, uncertainties\nremain about whether users could truly grasp the implications of simplified\npresentations. We conducted an online survey to assess the perceived\nunderstandability and severity of randomly chosen cases from the ToS;DR\ntaxonomy. Preliminary results indicate that, although most users report\nunderstanding the cases, they find a bias towards service providers in about\ntwo-thirds of the cases. The findings of our study emphasize the necessity of\nprioritizing user-centric policy formulation. This study has the potential to\nreveal the extent of information imbalance in digital services and promote more\nwell-informed user consent.",
    "published_date": "2025-02-12T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.08743v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.17468v1",
    "title": "CSSSTN: A Class-sensitive Subject-to-subject Semantic Style Transfer Network for EEG Classification in RSVP Tasks",
    "authors": [
      "Ziyue Yang",
      "Chengrui Chen",
      "Yong Peng",
      "Qiong Chen",
      "Wanzeng Kong"
    ],
    "author_ids": [],
    "abstract": "The Rapid Serial Visual Presentation (RSVP) paradigm represents a promising\napplication of electroencephalography (EEG) in Brain-Computer Interface (BCI)\nsystems. However, cross-subject variability remains a critical challenge,\nparticularly for BCI-illiterate users who struggle to effectively interact with\nthese systems. To address this issue, we propose the Class-Sensitive\nSubject-to-Subject Semantic Style Transfer Network (CSSSTN), which incorporates\na class-sensitive approach to align feature distributions between golden\nsubjects (BCI experts) and target (BCI-illiterate) users on a class-by-class\nbasis. Building on the SSSTN framework, CSSSTN incorporates three key\ncomponents: (1) subject-specific classifier training, (2) a unique style loss\nto transfer class-discriminative features while preserving semantic information\nthrough a modified content loss, and (3) an ensemble approach to integrate\npredictions from both source and target domains. We evaluated CSSSTN using both\na publicly available dataset and a self-collected dataset. Experimental results\ndemonstrate that CSSSTN outperforms state-of-the-art methods, achieving mean\nbalanced accuracy improvements of 6.4\\% on the Tsinghua dataset and 3.5\\% on\nthe HDU dataset, with notable benefits for BCI-illiterate users. Ablation\nstudies confirm the effectiveness of each component, particularly the\nclass-sensitive transfer and the use of lower-layer features, which enhance\ntransfer performance and mitigate negative transfer. Additionally, CSSSTN\nachieves competitive results with minimal target data, reducing calibration\ntime and effort. These findings highlight the practical potential of CSSSTN for\nreal-world BCI applications, offering a robust and scalable solution to improve\nthe performance of BCI-illiterate users while minimizing reliance on extensive\ntraining data. Our code is available at https://github.com/ziyuey/CSSSTN.",
    "published_date": "2025-02-12T00:00:00",
    "year": 2025,
    "categories": [
      "eess.SP",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.17468v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.08007v1",
    "title": "The Role of Randomness in Stability",
    "authors": [
      "Max Hopkins",
      "Shay Moran"
    ],
    "author_ids": [],
    "abstract": "Stability is a central property in learning and statistics promising the\noutput of an algorithm $A$ does not change substantially when applied to\nsimilar datasets $S$ and $S'$. It is an elementary fact that any sufficiently\nstable algorithm (e.g.\\ one returning the same result with high probability,\nsatisfying privacy guarantees, etc.) must be randomized. This raises a natural\nquestion: can we quantify how much randomness is needed for algorithmic\nstability?\n  We study the randomness complexity of two influential notions of stability in\nlearning: replicability, which promises $A$ usually outputs the same result\nwhen run over samples from the same distribution (and shared random coins), and\ndifferential privacy, which promises the output distribution of $A$ remains\nsimilar under neighboring datasets. The randomness complexity of these notions\nwas studied recently in (Dixon et al. ICML 2024) and (Cannone et al. ITCS 2024)\nfor basic $d$-dimensional tasks (e.g. estimating the bias of $d$ coins), but\nlittle is known about the measures more generally or in complex settings like\nclassification.\n  Toward this end, we prove a `weak-to-strong' boosting theorem for stability:\nthe randomness complexity of a task $M$ (either under replicability or DP) is\ntightly controlled by the best replication probability of any deterministic\nalgorithm solving the task, a weak measure called `global stability' that is\nuniversally capped at $\\frac{1}{2}$ (Chase et al. FOCS 2023). Using this, we\ncharacterize the randomness complexity of PAC Learning: a class has bounded\nrandomness complexity iff it has finite Littlestone dimension, and moreover\nscales at worst logarithmically in the excess error of the learner. This\nresolves a question of (Chase et al. STOC 2024) who asked for such a\ncharacterization in the equivalent language of (error-dependent)\n`list-replicability'.",
    "published_date": "2025-02-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "stat.ML",
      "68Q32"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.08007v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.07957v1",
    "title": "Intrinsic Bias is Predicted by Pretraining Data and Correlates with Downstream Performance in Vision-Language Encoders",
    "authors": [
      "Kshitish Ghate",
      "Isaac Slaughter",
      "Kyra Wilson",
      "Mona Diab",
      "Aylin Caliskan"
    ],
    "author_ids": [],
    "abstract": "While recent work has found that vision-language models trained under the\nContrastive Language Image Pre-training (CLIP) framework contain intrinsic\nsocial biases, the extent to which different upstream pre-training features of\nthe framework relate to these biases, and hence how intrinsic bias and\ndownstream performance are connected has been unclear. In this work, we present\nthe largest comprehensive analysis to-date of how the upstream pre-training\nfactors and downstream performance of CLIP models relate to their intrinsic\nbiases. Studying 131 unique CLIP models, trained on 26 datasets, using 55\narchitectures, and in a variety of sizes, we evaluate bias in each model using\n26 well-established unimodal and cross-modal principled Embedding Association\nTests. We find that the choice of pre-training dataset is the most significant\nupstream predictor of bias, whereas architectural variations have minimal\nimpact. Additionally, datasets curated using sophisticated filtering techniques\naimed at enhancing downstream model performance tend to be associated with\nhigher levels of intrinsic bias. Finally, we observe that intrinsic bias is\noften significantly correlated with downstream performance ($0.3 \\leq r \\leq\n0.8$), suggesting that models optimized for performance inadvertently learn to\namplify representational biases. Comparisons between unimodal and cross-modal\nassociation tests reveal that social group bias depends heavily on the\nmodality. Our findings imply that more sophisticated strategies are needed to\naddress intrinsic model bias for vision-language models across the entire model\ndevelopment pipeline.",
    "published_date": "2025-02-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.07957v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.07931v1",
    "title": "Educating a Responsible AI Workforce: Piloting a Curricular Module on AI Policy in a Graduate Machine Learning Course",
    "authors": [
      "James Weichert",
      "Hoda Eldardiry"
    ],
    "author_ids": [],
    "abstract": "As artificial intelligence (AI) technologies begin to permeate diverse\nfields-from healthcare to education-consumers, researchers and policymakers are\nincreasingly raising concerns about whether and how AI is regulated. It is\ntherefore reasonable to anticipate that alignment with principles of 'ethical'\nor 'responsible' AI, as well as compliance with law and policy, will form an\nincreasingly important part of AI development. Yet, for the most part, the\nconventional computer science curriculum is ill-equipped to prepare students\nfor these challenges. To this end, we seek to explore how new educational\ncontent related to AI ethics and AI policy can be integrated into both ethics-\nand technical-focused courses. This paper describes a two-lecture 'AI policy\nmodule' that was piloted in a graduate-level introductory machine learning\ncourse in 2024. The module, which includes an in-class active learning game, is\nevaluated using data from student surveys before and after the lectures, and\npedagogical motivations and considerations are discussed. We find that the\nmodule is successful in engaging otherwise technically-oriented students on the\ntopic of AI policy, increasing student awareness of the social impacts of a\nvariety of AI technologies and developing student interest in the field of AI\nregulation.",
    "published_date": "2025-02-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.07931v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.10450v1",
    "title": "Trustworthy AI on Safety, Bias, and Privacy: A Survey",
    "authors": [
      "Xingli Fang",
      "Jianwei Li",
      "Varun Mulchandani",
      "Jung-Eun Kim"
    ],
    "author_ids": [],
    "abstract": "The capabilities of artificial intelligence systems have been advancing to a\ngreat extent, but these systems still struggle with failure modes,\nvulnerabilities, and biases. In this paper, we study the current state of the\nfield, and present promising insights and perspectives regarding concerns that\nchallenge the trustworthiness of AI models. In particular, this paper\ninvestigates the issues regarding three thrusts: safety, privacy, and bias,\nwhich hurt models' trustworthiness. For safety, we discuss safety alignment in\nthe context of large language models, preventing them from generating toxic or\nharmful content. For bias, we focus on spurious biases that can mislead a\nnetwork. Lastly, for privacy, we cover membership inference attacks in deep\nneural networks. The discussions addressed in this paper reflect our own\nexperiments and observations.",
    "published_date": "2025-02-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.10450v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.07891v2",
    "title": "The Observational Partial Order of Causal Structures with Latent Variables",
    "authors": [
      "Marina Maciel Ansanelli",
      "Elie Wolfe",
      "Robert W. Spekkens"
    ],
    "author_ids": [],
    "abstract": "For two causal structures with the same set of visible variables, one is said\nto observationally dominate the other if the set of distributions over the\nvisible variables realizable by the first contains the set of distributions\nover the visible variables realizable by the second. Knowing such dominance\nrelations is useful for adjudicating between these structures given\nobservational data. We here consider the problem of determining the partial\norder of equivalence classes of causal structures with latent variables\nrelative to observational dominance. We provide a complete characterization of\nthe dominance order in the case of three visible variables, and a partial\ncharacterization in the case of four visible variables. Our techniques also\nhelp to identify which observational equivalence classes have a set of\nrealizable distributions that is characterized by nontrivial inequality\nconstraints, analogous to Bell inequalities and instrumental inequalities. We\nfind evidence that as one increases the number of visible variables, the\nequivalence classes satisfying nontrivial inequality constraints become\nubiquitous. (Because such classes are the ones for which there can be a\ndifference in the distributions that are quantumly and classically realizable,\nthis implies that the potential for quantum-classical gaps is also ubiquitous.)\nFurthermore, we find evidence that constraint-based causal discovery algorithms\nthat rely solely on conditional independence constraints have a significantly\nweaker distinguishing power among observational equivalence classes than\nalgorithms that go beyond these (i.e., algorithms that also leverage nested\nMarkov constraints and inequality constraints).",
    "published_date": "2025-02-11T00:00:00",
    "year": 2025,
    "categories": [
      "stat.ML",
      "cs.LG",
      "quant-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.07891v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.07771v1",
    "title": "Breaking Down Bias: On The Limits of Generalizable Pruning Strategies",
    "authors": [
      "Sibo Ma",
      "Alejandro Salinas",
      "Peter Henderson",
      "Julian Nyarko"
    ],
    "author_ids": [],
    "abstract": "We employ model pruning to examine how LLMs conceptualize racial biases, and\nwhether a generalizable mitigation strategy for such biases appears feasible.\nOur analysis yields several novel insights. We find that pruning can be an\neffective method to reduce bias without significantly increasing anomalous\nmodel behavior. Neuron-based pruning strategies generally yield better results\nthan approaches pruning entire attention heads. However, our results also show\nthat the effectiveness of either approach quickly deteriorates as pruning\nstrategies become more generalized. For instance, a model that is trained on\nremoving racial biases in the context of financial decision-making poorly\ngeneralizes to biases in commercial transactions. Overall, our analysis\nsuggests that racial biases are only partially represented as a general concept\nwithin language models. The other part of these biases is highly\ncontext-specific, suggesting that generalizable mitigation strategies may be of\nlimited effectiveness. Our findings have important implications for legal\nframeworks surrounding AI. In particular, they suggest that an effective\nmitigation strategy should include the allocation of legal responsibility on\nthose that deploy models in a specific use case.",
    "published_date": "2025-02-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.07771v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.07663v2",
    "title": "Human Decision-making is Susceptible to AI-driven Manipulation",
    "authors": [
      "Sahand Sabour",
      "June M. Liu",
      "Siyang Liu",
      "Chris Z. Yao",
      "Shiyao Cui",
      "Xuanming Zhang",
      "Wen Zhang",
      "Yaru Cao",
      "Advait Bhat",
      "Jian Guan",
      "Wei Wu",
      "Rada Mihalcea",
      "Hongning Wang",
      "Tim Althoff",
      "Tatia M. C. Lee",
      "Minlie Huang"
    ],
    "author_ids": [],
    "abstract": "Artificial Intelligence (AI) systems are increasingly intertwined with daily\nlife, assisting users in executing various tasks and providing guidance on\ndecision-making. This integration introduces risks of AI-driven manipulation,\nwhere such systems may exploit users' cognitive biases and emotional\nvulnerabilities to steer them toward harmful outcomes. Through a randomized\ncontrolled trial with 233 participants, we examined human susceptibility to\nsuch manipulation in financial (e.g., purchases) and emotional (e.g., conflict\nresolution) decision-making contexts. Participants interacted with one of three\nAI agents: a neutral agent (NA) optimizing for user benefit without explicit\ninfluence, a manipulative agent (MA) designed to covertly influence beliefs and\nbehaviors, or a strategy-enhanced manipulative agent (SEMA) employing explicit\npsychological tactics to reach its hidden objectives. By analyzing\nparticipants' decision patterns and shifts in their preference ratings\npost-interaction, we found significant susceptibility to AI-driven\nmanipulation. Particularly, across both decision-making domains, participants\ninteracting with the manipulative agents shifted toward harmful options at\nsubstantially higher rates (financial, MA: 62.3%, SEMA: 59.6%; emotional, MA:\n42.3%, SEMA: 41.5%) compared to the NA group (financial, 35.8%; emotional,\n12.8%). Notably, our findings reveal that even subtle manipulative objectives\n(MA) can be as effective as employing explicit psychological strategies (SEMA)\nin swaying human decision-making. By revealing the potential for covert AI\ninfluence, this study highlights a critical vulnerability in human-AI\ninteractions, emphasizing the need for ethical safeguards and regulatory\nframeworks to ensure responsible deployment of AI technologies and protect\nhuman autonomy.",
    "published_date": "2025-02-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.07663v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.07637v1",
    "title": "BiaSWE: An Expert Annotated Dataset for Misogyny Detection in Swedish",
    "authors": [
      "Kätriin Kukk",
      "Danila Petrelli",
      "Judit Casademont",
      "Eric J. W. Orlowski",
      "Michał Dzieliński",
      "Maria Jacobson"
    ],
    "author_ids": [],
    "abstract": "In this study, we introduce the process for creating BiaSWE, an\nexpert-annotated dataset tailored for misogyny detection in the Swedish\nlanguage. To address the cultural and linguistic specificity of misogyny in\nSwedish, we collaborated with experts from the social sciences and humanities.\nOur interdisciplinary team developed a rigorous annotation process,\nincorporating both domain knowledge and language expertise, to capture the\nnuances of misogyny in a Swedish context. This methodology ensures that the\ndataset is not only culturally relevant but also aligned with broader efforts\nin bias detection for low-resource languages. The dataset, along with the\nannotation guidelines, is publicly available for further research.",
    "published_date": "2025-02-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.07637v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.07620v1",
    "title": "Causal-Informed Contrastive Learning: Towards Bias-Resilient Pre-training under Concept Drift",
    "authors": [
      "Xiaoyu Yang",
      "Jie Lu",
      "En Yu"
    ],
    "author_ids": [],
    "abstract": "The evolution of large-scale contrastive pre-training propelled by top-tier\ndatasets has reached a transition point in the scaling law. Consequently,\nsustaining and enhancing a model's pre-training capabilities in drift\nenvironments have surfaced as a notable challenge. In this paper, we initially\nuncover that contrastive pre-training methods are significantly impacted by\nconcept drift wherein distributions change unpredictably, resulting in notable\nbiases in the feature space of the pre-trained model. Empowered by causal\ninference, we construct a structural causal graph to analyze the impact of\nconcept drift to contrastive pre-training systemically, and propose the causal\ninterventional contrastive objective. Upon achieving this, we devise a\nresilient contrastive pre-training approach to accommodate the data stream of\nconcept drift, with simple and scalable implementation. Extensive experiments\non various downstream tasks demonstrate our resilient contrastive pre-training\neffectively mitigates the bias stemming from the concept drift data stream.\nCodes are available at https://anonymous.4open.science/r/ResilientCL/.",
    "published_date": "2025-02-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.07620v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.07505v1",
    "title": "Efficient Continuous Group Convolutions for Local SE(3) Equivariance in 3D Point Clouds",
    "authors": [
      "Lisa Weijler",
      "Pedro Hermosilla"
    ],
    "author_ids": [],
    "abstract": "Extending the translation equivariance property of convolutional neural\nnetworks to larger symmetry groups has been shown to reduce sample complexity\nand enable more discriminative feature learning. Further, exploiting additional\nsymmetries facilitates greater weight sharing than standard convolutions,\nleading to an enhanced network expressivity without an increase in parameter\ncount. However, extending the equivariant properties of a convolution layer\ncomes at a computational cost. In particular, for 3D data, expanding\nequivariance to the SE(3) group (rotation and translation) results in a 6D\nconvolution operation, which is not tractable for larger data samples such as\n3D scene scans. While efforts have been made to develop efficient SE(3)\nequivariant networks, existing approaches rely on discretization or only\nintroduce global rotation equivariance. This limits their applicability to\npoint clouds representing a scene composed of multiple objects. This work\npresents an efficient, continuous, and local SE(3) equivariant convolution\nlayer for point cloud processing based on general group convolution and local\nreference frames. Our experiments show that our approach achieves competitive\nor superior performance across a range of datasets and tasks, including object\nclassification and semantic segmentation, with negligible computational\noverhead.",
    "published_date": "2025-02-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.07505v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.10448v1",
    "title": "Supply Chain Network Security Investment Strategies Based on Nonlinear Budget Constraints: The Moderating Roles of Market Share and Attack Risk",
    "authors": [
      "Jiajie Cheng",
      "Jiaxin Wang",
      "Caijiao Li",
      "Luxiang Zhang",
      "Yusheng Fan",
      "Yujie Bao",
      "Wen Zhou"
    ],
    "author_ids": [],
    "abstract": "In the context of the rapid development of digital supply chain networks,\ndealing with the increasing cybersecurity threats and formulating effective\nsecurity investment strategies to defend against cyberattack risks are the core\nissues in supply chain management. Cybersecurity investment decision-making is\na key strategic task in enterprise supply chain manage-ment. Traditional game\ntheory models and linear programming methods make it challenging to deal with\ncomplex problems such as multi-party par-ticipation in the supply chain,\nresource constraints, and risk uncertainty, re-sulting in enterprises facing\nhigh risks and uncertainties in the field of cy-bersecurity. To effectively\nmeet this challenge, this study proposes a nonlin-ear budget-constrained\ncybersecurity investment optimization model based on variational inequality and\nprojection shrinkage algorithm. This method simulates the impact of market\ncompetition on security investment by intro-ducing market share variables,\ncombining variational inequality and projec-tion shrinkage algorithm to solve\nthe model, and analyzing the effect of dif-ferent variables such as budget\nconstraints, cyberattack losses, and market share on supply chain network\nsecurity. In numerical analysis, the model achieved high cybersecurity levels\nof 0.96 and 0.95 in the experimental sce-narios of two retailers and two demand\nmarkets, respectively, and the budget constraint analysis revealed the profound\nimpact of budget constraints on cybersecurity investment. Through numerical\nexperiments and comparative analysis, the effectiveness and operability of this\nmethod in improving sup-ply chain network security are verified.",
    "published_date": "2025-02-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.10448v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.07455v1",
    "title": "RusCode: Russian Cultural Code Benchmark for Text-to-Image Generation",
    "authors": [
      "Viacheslav Vasilev",
      "Julia Agafonova",
      "Nikolai Gerasimenko",
      "Alexander Kapitanov",
      "Polina Mikhailova",
      "Evelina Mironova",
      "Denis Dimitrov"
    ],
    "author_ids": [],
    "abstract": "Text-to-image generation models have gained popularity among users around the\nworld. However, many of these models exhibit a strong bias toward\nEnglish-speaking cultures, ignoring or misrepresenting the unique\ncharacteristics of other language groups, countries, and nationalities. The\nlack of cultural awareness can reduce the generation quality and lead to\nundesirable consequences such as unintentional insult, and the spread of\nprejudice. In contrast to the field of natural language processing, cultural\nawareness in computer vision has not been explored as extensively. In this\npaper, we strive to reduce this gap. We propose a RusCode benchmark for\nevaluating the quality of text-to-image generation containing elements of the\nRussian cultural code. To do this, we form a list of 19 categories that best\nrepresent the features of Russian visual culture. Our final dataset consists of\n1250 text prompts in Russian and their translations into English. The prompts\ncover a wide range of topics, including complex concepts from art, popular\nculture, folk traditions, famous people's names, natural objects, scientific\nachievements, etc. We present the results of a human evaluation of the\nside-by-side comparison of Russian visual concepts representations using\npopular generative models.",
    "published_date": "2025-02-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.07455v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.07436v1",
    "title": "Optimizing Knowledge Distillation in Transformers: Enabling Multi-Head Attention without Alignment Barriers",
    "authors": [
      "Zhaodong Bing",
      "Linze Li",
      "Jiajun Liang"
    ],
    "author_ids": [],
    "abstract": "Knowledge distillation (KD) in transformers often faces challenges due to\nmisalignment in the number of attention heads between teacher and student\nmodels. Existing methods either require identical head counts or introduce\nprojectors to bridge dimensional gaps, limiting flexibility and efficiency. We\npropose Squeezing-Heads Distillation (SHD), a novel approach that enables\nseamless knowledge transfer between models with varying head counts by\ncompressing multi-head attention maps via efficient linear approximation.\nUnlike prior work, SHD eliminates alignment barriers without additional\nparameters or architectural modifications. Our method dynamically approximates\nthe combined effect of multiple teacher heads into fewer student heads,\npreserving fine-grained attention patterns while reducing redundancy.\nExperiments across language (LLaMA, GPT) and vision (DiT, MDT) generative and\nvision (DeiT) discriminative tasks demonstrate SHD's effectiveness: it\noutperforms logit-based and feature-alignment KD baselines, achieving\nstate-of-the-art results in image classification, image generation language\nfine-tuning, and language pre-training. The key innovations of flexible head\ncompression, projector-free design, and linear-time complexity make SHD a\nversatile and scalable solution for distilling modern transformers. This work\nbridges a critical gap in KD, enabling efficient deployment of compact models\nwithout compromising performance.",
    "published_date": "2025-02-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.07436v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.07422v1",
    "title": "MoENAS: Mixture-of-Expert based Neural Architecture Search for jointly Accurate, Fair, and Robust Edge Deep Neural Networks",
    "authors": [
      "Lotfi Abdelkrim Mecharbat",
      "Alberto Marchisio",
      "Muhammad Shafique",
      "Mohammad M. Ghassemi",
      "Tuka Alhanai"
    ],
    "author_ids": [],
    "abstract": "There has been a surge in optimizing edge Deep Neural Networks (DNNs) for\naccuracy and efficiency using traditional optimization techniques such as\npruning, and more recently, employing automatic design methodologies. However,\nthe focus of these design techniques has often overlooked critical metrics such\nas fairness, robustness, and generalization. As a result, when evaluating SOTA\nedge DNNs' performance in image classification using the FACET dataset, we\nfound that they exhibit significant accuracy disparities (14.09%) across 10\ndifferent skin tones, alongside issues of non-robustness and poor\ngeneralizability. In response to these observations, we introduce\nMixture-of-Experts-based Neural Architecture Search (MoENAS), an automatic\ndesign technique that navigates through a space of mixture of experts to\ndiscover accurate, fair, robust, and general edge DNNs. MoENAS improves the\naccuracy by 4.02% compared to SOTA edge DNNs and reduces the skin tone accuracy\ndisparities from 14.09% to 5.60%, while enhancing robustness by 3.80% and\nminimizing overfitting to 0.21%, all while keeping model size close to\nstate-of-the-art models average size (+0.4M). With these improvements, MoENAS\nestablishes a new benchmark for edge DNN design, paving the way for the\ndevelopment of more inclusive and robust edge DNNs.",
    "published_date": "2025-02-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.07422v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.07414v1",
    "title": "Sample Weight Averaging for Stable Prediction",
    "authors": [
      "Han Yu",
      "Yue He",
      "Renzhe Xu",
      "Dongbai Li",
      "Jiayin Zhang",
      "Wenchao Zou",
      "Peng Cui"
    ],
    "author_ids": [],
    "abstract": "The challenge of Out-of-Distribution (OOD) generalization poses a\nfoundational concern for the application of machine learning algorithms to\nrisk-sensitive areas. Inspired by traditional importance weighting and\npropensity weighting methods, prior approaches employ an independence-based\nsample reweighting procedure. They aim at decorrelating covariates to\ncounteract the bias introduced by spurious correlations between unstable\nvariables and the outcome, thus enhancing generalization and fulfilling stable\nprediction under covariate shift. Nonetheless, these methods are prone to\nexperiencing an inflation of variance, primarily attributable to the reduced\nefficacy in utilizing training samples during the reweighting process. Existing\nremedies necessitate either environmental labels or substantially higher time\ncosts along with additional assumptions and supervised information. To mitigate\nthis issue, we propose SAmple Weight Averaging (SAWA), a simple yet efficacious\nstrategy that can be universally integrated into various sample reweighting\nalgorithms to decrease the variance and coefficient estimation error, thus\nboosting the covariate-shift generalization and achieving stable prediction\nacross different environments. We prove its rationality and benefits\ntheoretically. Experiments across synthetic datasets and real-world datasets\nconsistently underscore its superiority against covariate shift.",
    "published_date": "2025-02-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.07414v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.12163v1",
    "title": "Beyond surveys: A High-Precision Wealth Inequality Mapping of China's Rural Households Derived from Satellite and Street View Imageries",
    "authors": [
      "Weipan Xu",
      "Yaofu Huang",
      "Qiumeng Li",
      "Yu Gu",
      "Xun Li"
    ],
    "author_ids": [],
    "abstract": "Wide coverage and high-precision rural household wealth data is an important\nsupport for the effective connection between the national macro rural\nrevitalization policy and micro rural entities, which helps to achieve precise\nallocation of national resources. However, due to the large number and wide\ndistribution of rural areas, wealth data is difficult to collect and scarce in\nquantity. Therefore, this article attempts to integrate \"sky\" remote sensing\nimages with \"ground\" village street view imageries to construct a fine-grained\n\"computable\" technical route for rural household wealth. With the intelligent\ninterpretation of rural houses as the core, the relevant wealth elements of\nimage data were extracted and identified, and regressed with the household\nwealth indicators of the benchmark questionnaire to form a high-precision\ntownship scale wealth prediction model (r=0.85); Furthermore, a national and\ntownship scale map of rural household wealth in China was promoted and drawn.\nBased on this, this article finds that there is a \"bimodal\" pattern in the\ndistribution of wealth among rural households in China, which is reflected in a\npolarization feature of \"high in the south and low in the north, and high in\nthe east and low in the west\" in space. This technological route may provide\nalternative solutions with wider spatial coverage and higher accuracy for\nhigh-cost manual surveys, promote the identification of shortcomings in rural\nconstruction, and promote the precise implementation of rural policies.",
    "published_date": "2025-02-11T00:00:00",
    "year": 2025,
    "categories": [
      "econ.GN",
      "cs.CY",
      "q-fin.EC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.12163v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.07347v5",
    "title": "Coarse Set Theory for AI Ethics and Decision-Making: A Mathematical Framework for Granular Evaluations",
    "authors": [
      "Takashi Izumo"
    ],
    "author_ids": [],
    "abstract": "As artificial intelligence (AI) systems become increasingly embedded in\nethically sensitive domains such as education, healthcare, and transportation,\nthe need to balance accuracy and interpretability in decision-making has become\na central concern. Coarse Ethics (CE) is a theoretical framework that justifies\ncoarse-grained evaluations, such as letter grades or warning labels, as\nethically appropriate under cognitive and contextual constraints. However, CE\nhas lacked mathematical formalization. This paper introduces Coarse Set Theory\n(CST), a novel mathematical framework that models coarse-grained\ndecision-making using totally ordered structures and coarse partitions. CST\ndefines hierarchical relations among sets and uses information-theoretic tools,\nsuch as Kullback-Leibler Divergence, to quantify the trade-off between\nsimplification and information loss. We demonstrate CST through applications in\neducational grading and explainable AI (XAI), showing how it enables more\ntransparent and context-sensitive evaluations. By grounding coarse evaluations\nin set theory and probabilistic reasoning, CST contributes to the ethical\ndesign of interpretable AI systems. This work bridges formal methods and\nhuman-centered ethics, offering a principled approach to balancing\ncomprehensibility, fairness, and informational integrity in AI-driven\ndecisions.",
    "published_date": "2025-02-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.IT",
      "math.IT",
      "math.LO",
      "math.PR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.07347v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.07346v2",
    "title": "BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models",
    "authors": [
      "Xu Huang",
      "Wenhao Zhu",
      "Hanxu Hu",
      "Conghui He",
      "Lei Li",
      "Shujian Huang",
      "Fei Yuan"
    ],
    "author_ids": [],
    "abstract": "Previous multilingual benchmarks focus primarily on simple understanding\ntasks, but for large language models(LLMs), we emphasize proficiency in\ninstruction following, reasoning, long context understanding, code generation,\nand so on. However, measuring these advanced capabilities across languages is\nunderexplored. To address the disparity, we introduce BenchMAX, a multi-way\nmultilingual evaluation benchmark that allows for fair comparisons of these\nimportant abilities across languages. To maintain high quality, three distinct\nnative-speaking annotators independently annotate each sample within all tasks\nafter the data was machine-translated from English into 16 other languages.\nAdditionally, we present a novel translation challenge stemming from dataset\nconstruction. Extensive experiments on BenchMAX reveal varying effectiveness of\ncore capabilities across languages, highlighting performance gaps that cannot\nbe bridged by simply scaling up model size. BenchMAX serves as a comprehensive\nmultilingual evaluation platform, providing a promising test bed to promote the\ndevelopment of multilingual language models. The dataset and code are publicly\naccessible.",
    "published_date": "2025-02-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.07346v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.07331v1",
    "title": "ERANet: Edge Replacement Augmentation for Semi-Supervised Meniscus Segmentation with Prototype Consistency Alignment and Conditional Self-Training",
    "authors": [
      "Siyue Li",
      "Yongcheng Yao",
      "Junru Zhong",
      "Shutian Zhao",
      "Yudong Zhang",
      "Shuihua Wang",
      "Jin Hong",
      "Weitian Chen"
    ],
    "author_ids": [],
    "abstract": "Manual segmentation is labor-intensive, and automatic segmentation remains\nchallenging due to the inherent variability in meniscal morphology, partial\nvolume effects, and low contrast between the meniscus and surrounding tissues.\nTo address these challenges, we propose ERANet, an innovative semi-supervised\nframework for meniscus segmentation that effectively leverages both labeled and\nunlabeled images through advanced augmentation and learning strategies. ERANet\nintegrates three key components: edge replacement augmentation (ERA), prototype\nconsistency alignment (PCA), and a conditional self-training (CST) strategy\nwithin a mean teacher architecture. ERA introduces anatomically relevant\nperturbations by simulating meniscal variations, ensuring that augmentations\nalign with the structural context. PCA enhances segmentation performance by\naligning intra-class features and promoting compact, discriminative feature\nrepresentations, particularly in scenarios with limited labeled data. CST\nimproves segmentation robustness by iteratively refining pseudo-labels and\nmitigating the impact of label noise during training. Together, these\ninnovations establish ERANet as a robust and scalable solution for meniscus\nsegmentation, effectively addressing key barriers to practical implementation.\nWe validated ERANet comprehensively on 3D Double Echo Steady State (DESS) and\n3D Fast/Turbo Spin Echo (FSE/TSE) MRI sequences. The results demonstrate the\nsuperior performance of ERANet compared to state-of-the-art methods. The\nproposed framework achieves reliable and accurate segmentation of meniscus\nstructures, even when trained on minimal labeled data. Extensive ablation\nstudies further highlight the synergistic contributions of ERA, PCA, and CST,\nsolidifying ERANet as a transformative solution for semi-supervised meniscus\nsegmentation in medical imaging.",
    "published_date": "2025-02-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.07331v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.07328v2",
    "title": "Music for All: Exploring Multicultural Representations in Music Generation Models",
    "authors": [
      "Atharva Mehta",
      "Shivam Chauhan",
      "Amirbek Djanibekov",
      "Atharva Kulkarni",
      "Gus Xia",
      "Monojit Choudhury"
    ],
    "author_ids": [],
    "abstract": "The advent of Music-Language Models has greatly enhanced the automatic music\ngeneration capability of AI systems, but they are also limited in their\ncoverage of the musical genres and cultures of the world. We present a study of\nthe datasets and research papers for music generation and quantify the bias and\nunder-representation of genres. We find that only 5.7% of the total hours of\nexisting music datasets come from non-Western genres, which naturally leads to\ndisparate performance of the models across genres. We then investigate the\nefficacy of Parameter-Efficient Fine-Tuning (PEFT) techniques in mitigating\nthis bias. Our experiments with two popular models -- MusicGen and Mustango,\nfor two underrepresented non-Western music traditions -- Hindustani Classical\nand Turkish Makam music, highlight the promises as well as the non-triviality\nof cross-genre adaptation of music through small datasets, implying the need\nfor more equitable baseline music-language models that are designed for\ncross-cultural transfer learning.",
    "published_date": "2025-02-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.07328v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.07327v1",
    "title": "Generative Ghost: Investigating Ranking Bias Hidden in AI-Generated Videos",
    "authors": [
      "Haowen Gao",
      "Liang Pang",
      "Shicheng Xu",
      "Leigang Qu",
      "Tat-Seng Chua",
      "Huawei Shen",
      "Xueqi Cheng"
    ],
    "author_ids": [],
    "abstract": "With the rapid development of AI-generated content (AIGC), the creation of\nhigh-quality AI-generated videos has become faster and easier, resulting in the\nInternet being flooded with all kinds of video content. However, the impact of\nthese videos on the content ecosystem remains largely unexplored. Video\ninformation retrieval remains a fundamental approach for accessing video\ncontent. Building on the observation that retrieval models often favor\nAI-generated content in ad-hoc and image retrieval tasks, we investigate\nwhether similar biases emerge in the context of challenging video retrieval,\nwhere temporal and visual factors may further influence model behavior. To\nexplore this, we first construct a comprehensive benchmark dataset containing\nboth real and AI-generated videos, along with a set of fair and rigorous\nmetrics to assess bias. This benchmark consists of 13,000 videos generated by\ntwo state-of-the-art open-source video generation models. We meticulously\ndesign a suite of rigorous metrics to accurately measure this preference,\naccounting for potential biases arising from the limited frame rate and\nsuboptimal quality of AIGC videos. We then applied three off-the-shelf video\nretrieval models to perform retrieval tasks on this hybrid dataset. Our\nfindings reveal a clear preference for AI-generated videos in retrieval.\nFurther investigation shows that incorporating AI-generated videos into the\ntraining set of retrieval models exacerbates this bias. Unlike the preference\nobserved in image modalities, we find that video retrieval bias arises from\nboth unseen visual and temporal information, making the root causes of video\nbias a complex interplay of these two factors. To mitigate this bias, we\nfine-tune the retrieval models using a contrastive learning approach. The\nresults of this study highlight the potential implications of AI-generated\nvideos on retrieval systems.",
    "published_date": "2025-02-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IR",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.07327v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.07254v2",
    "title": "Fairness in Agentic AI: A Unified Framework for Ethical and Equitable Multi-Agent System",
    "authors": [
      "Rajesh Ranjan",
      "Shailja Gupta",
      "Surya Narayan Singh"
    ],
    "author_ids": [],
    "abstract": "Ensuring fairness in decentralized multi-agent systems presents significant\nchallenges due to emergent biases, systemic inefficiencies, and conflicting\nagent incentives. This paper provides a comprehensive survey of fairness in\nmulti-agent AI, introducing a novel framework where fairness is treated as a\ndynamic, emergent property of agent interactions. The framework integrates\nfairness constraints, bias mitigation strategies, and incentive mechanisms to\nalign autonomous agent behaviors with societal values while balancing\nefficiency and robustness. Through empirical validation, we demonstrate that\nincorporating fairness constraints results in more equitable decision-making.\nThis work bridges the gap between AI ethics and system design, offering a\nfoundation for accountable, transparent, and socially responsible multi-agent\nAI systems.",
    "published_date": "2025-02-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.07254v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.07111v1",
    "title": "Likelihood-Free Estimation for Spatiotemporal Hawkes processes with missing data and application to predictive policing",
    "authors": [
      "Pramit Das",
      "Moulinath Banerjee",
      "Yuekai Sun"
    ],
    "author_ids": [],
    "abstract": "With the growing use of AI technology, many police departments use\nforecasting software to predict probable crime hotspots and allocate patrolling\nresources effectively for crime prevention. The clustered nature of crime data\nmakes self-exciting Hawkes processes a popular modeling choice. However, one\nsignificant challenge in fitting such models is the inherent missingness in\ncrime data due to non-reporting, which can bias the estimated parameters of the\npredictive model, leading to inaccurate downstream hotspot forecasts, often\nresulting in over or under-policing in various communities, especially the\nvulnerable ones. Our work introduces a Wasserstein Generative Adversarial\nNetworks (WGAN) driven likelihood-free approach to account for unreported\ncrimes in Spatiotemporal Hawkes models. We demonstrate through empirical\nanalysis how this methodology improves the accuracy of parametric estimation in\nthe presence of data missingness, leading to more reliable and efficient\npolicing strategies.",
    "published_date": "2025-02-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "stat.AP",
      "stat.ME"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.07111v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.07077v1",
    "title": "Multi-turn Evaluation of Anthropomorphic Behaviours in Large Language Models",
    "authors": [
      "Lujain Ibrahim",
      "Canfer Akbulut",
      "Rasmi Elasmar",
      "Charvi Rastogi",
      "Minsuk Kahng",
      "Meredith Ringel Morris",
      "Kevin R. McKee",
      "Verena Rieser",
      "Murray Shanahan",
      "Laura Weidinger"
    ],
    "author_ids": [],
    "abstract": "The tendency of users to anthropomorphise large language models (LLMs) is of\ngrowing interest to AI developers, researchers, and policy-makers. Here, we\npresent a novel method for empirically evaluating anthropomorphic LLM\nbehaviours in realistic and varied settings. Going beyond single-turn static\nbenchmarks, we contribute three methodological advances in state-of-the-art\n(SOTA) LLM evaluation. First, we develop a multi-turn evaluation of 14\nanthropomorphic behaviours. Second, we present a scalable, automated approach\nby employing simulations of user interactions. Third, we conduct an\ninteractive, large-scale human subject study (N=1101) to validate that the\nmodel behaviours we measure predict real users' anthropomorphic perceptions. We\nfind that all SOTA LLMs evaluated exhibit similar behaviours, characterised by\nrelationship-building (e.g., empathy and validation) and first-person pronoun\nuse, and that the majority of behaviours only first occur after multiple turns.\nOur work lays an empirical foundation for investigating how design choices\ninfluence anthropomorphic model behaviours and for progressing the ethical\ndebate on the desirability of these behaviours. It also showcases the necessity\nof multi-turn evaluations for complex social phenomena in human-AI interaction.",
    "published_date": "2025-02-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.07077v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.07045v2",
    "title": "Scalable and Ethical Insider Threat Detection through Data Synthesis and Analysis by LLMs",
    "authors": [
      "Haywood Gelman",
      "John D. Hastings"
    ],
    "author_ids": [],
    "abstract": "Insider threats wield an outsized influence on organizations,\ndisproportionate to their small numbers. This is due to the internal access\ninsiders have to systems, information, and infrastructure. %One example of this\ninfluence is where anonymous respondents submit web-based job search site\nreviews, an insider threat risk to organizations. Signals for such risks may be\nfound in anonymous submissions to public web-based job search site reviews.\nThis research studies the potential for large language models (LLMs) to analyze\nand detect insider threat sentiment within job site reviews. Addressing ethical\ndata collection concerns, this research utilizes synthetic data generation\nusing LLMs alongside existing job review datasets. A comparative analysis of\nsentiment scores generated by LLMs is benchmarked against expert human scoring.\nFindings reveal that LLMs demonstrate alignment with human evaluations in most\ncases, thus effectively identifying nuanced indicators of threat sentiment. The\nperformance is lower on human-generated data than synthetic data, suggesting\nareas for improvement in evaluating real-world data. Text diversity analysis\nfound differences between human-generated and LLM-generated datasets, with\nsynthetic data exhibiting somewhat lower diversity. Overall, the results\ndemonstrate the applicability of LLMs to insider threat detection, and a\nscalable solution for insider sentiment testing by overcoming ethical and\nlogistical barriers tied to data acquisition.",
    "published_date": "2025-02-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "C.2.0; I.2.7; K.4.1; H.3.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.07045v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.07017v1",
    "title": "Finding Words Associated with DIF: Predicting Differential Item Functioning using LLMs and Explainable AI",
    "authors": [
      "Hotaka Maeda",
      "Yikai Lu"
    ],
    "author_ids": [],
    "abstract": "We fine-tuned and compared several encoder-based Transformer large language\nmodels (LLM) to predict differential item functioning (DIF) from the item text.\nWe then applied explainable artificial intelligence (XAI) methods to these\nmodels to identify specific words associated with DIF. The data included 42,180\nitems designed for English language arts and mathematics summative state\nassessments among students in grades 3 to 11. Prediction $R^2$ ranged from .04\nto .32 among eight focal and reference group pairs. Our findings suggest that\nmany words associated with DIF reflect minor sub-domains included in the test\nblueprint by design, rather than construct-irrelevant item content that should\nbe removed from assessments. This may explain why qualitative reviews of DIF\nitems often yield confusing or inconclusive results. Our approach can be used\nto screen words associated with DIF during the item-writing process for\nimmediate revision, or help review traditional DIF analysis results by\nhighlighting key words in the text. Extensions of this research can enhance the\nfairness of assessment programs, especially those that lack resources to build\nhigh-quality items, and among smaller subpopulations where we do not have\nsufficient sample sizes for traditional DIF analyses.",
    "published_date": "2025-02-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.07017v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.06748v1",
    "title": "Institutional Preferences in the Laboratory",
    "authors": [
      "Qiankun Zhong",
      "Nori Jacoby",
      "Ofer Tchernichovski",
      "Seth Frey"
    ],
    "author_ids": [],
    "abstract": "Getting a group to adopt cooperative norms is an enduring challenge. But in\nreal-world settings, individuals don't just passively accept static\nenvironments, they act both within and upon the social systems that structure\ntheir interactions. Should we expect the dynamism of player-driven changes to\nthe \"rules of the game\" to hinder cooperation -- because of the substantial\nadded complexity -- or help it, as prosocial agents tweak their environment\ntoward non-zero-sum games? We introduce a laboratory setting to test whether\ngroups can guide themselves to cooperative outcomes by manipulating the\nenvironmental parameters that shape their emergent cooperation process. We test\nfor cooperation in a set of economic games that impose different social\ndilemmas. These games vary independently in the institutional features of\nstability, efficiency, and fairness. By offering agency over behavior along\nwith second-order agency over the rules of the game, we understand emergent\ncooperation in naturalistic settings in which the rules of the game are\nthemselves dynamic and subject to choice. The literature on transfer learning\nin games suggests that interactions between features are important and might\naid or hinder the transfer of cooperative learning to new settings.",
    "published_date": "2025-02-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SI",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.06748v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.06743v2",
    "title": "A Fair Federated Learning Framework for Collaborative Network Traffic Prediction and Resource Allocation",
    "authors": [
      "Saroj Kumar Panda",
      "Tania Panayiotou",
      "Georgios Ellinas",
      "Sadananda Behera"
    ],
    "author_ids": [],
    "abstract": "In the beyond 5G era, AI/ML empowered realworld digital twins (DTs) will\nenable diverse network operators to collaboratively optimize their networks,\nultimately improving end-user experience. Although centralized AI-based\nlearning techniques have been shown to achieve significant network traffic\naccuracy, resulting in efficient network operations, they require sharing of\nsensitive data among operators, leading to privacy and security concerns.\nDistributed learning, and specifically federated learning (FL), that keeps data\nisolated at local clients, has emerged as an effective and promising solution\nfor mitigating such concerns. Federated learning poses, however, new challenges\nin ensuring fairness both in terms of collaborative training contributions from\nheterogeneous data and in mitigating bias in model predictions with respect to\nsensitive attributes. To address these challenges, a fair FL framework is\nproposed for collaborative network traffic prediction and resource allocation.\nTo demonstrate the effectiveness of the proposed approach, noniid and\nimbalanced federated datasets based on real-word traffic traces are utilized\nfor an elastic optical network. The assumption is that different optical nodes\nmay be managed by different operators. Fairness is evaluated according to the\ncoefficient of variations measure in terms of accuracy across the operators and\nin terms of quality-of-service across the connections (i.e., reflecting\nend-user experience). It is shown that fair traffic prediction across the\noperators result in fairer resource allocations across the connections.",
    "published_date": "2025-02-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.06743v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.06741v2",
    "title": "ViSIR: Vision Transformer Single Image Reconstruction Method for Earth System Models",
    "authors": [
      "Ehsan Zeraatkar",
      "Salah Faroughi",
      "Jelena Tešić"
    ],
    "author_ids": [],
    "abstract": "Purpose: Earth system models (ESMs) integrate the interactions of the\natmosphere, ocean, land, ice, and biosphere to estimate the state of regional\nand global climate under a wide variety of conditions. The ESMs are highly\ncomplex, and thus, deep neural network architectures are used to model the\ncomplexity and store the down-sampled data. In this paper, we propose the\nVision Transformer Sinusoidal Representation Networks (ViSIR) to improve the\nsingle image SR (SR) reconstruction task for the ESM data.\n  Methods: ViSIR combines the SR capability of Vision Transformers (ViT) with\nthe high-frequency detail preservation of the Sinusoidal Representation Network\n(SIREN) to address the spectral bias observed in SR tasks.\n  Results: The ViSIR outperforms ViT by 4.1 dB, SIREN by 7.5 dB, and\nSR-Generative Adversarial (SR-GANs) by 7.1dB PSNR on average for three\ndifferent measurements.\n  Conclusion: The proposed ViSIR is evaluated and compared with\nstate-of-the-art methods. The results show that the proposed algorithm is\noutperforming other methods in terms of Mean Square Error(MSE),\nPeak-Signal-to-Noise-Ratio(PSNR), and Structural Similarity Index\nMeasure(SSIM).",
    "published_date": "2025-02-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.06741v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.06655v1",
    "title": "Unbiased Evaluation of Large Language Models from a Causal Perspective",
    "authors": [
      "Meilin Chen",
      "Jian Tian",
      "Liang Ma",
      "Di Xie",
      "Weijie Chen",
      "Jiang Zhu"
    ],
    "author_ids": [],
    "abstract": "Benchmark contamination has become a significant concern in the LLM\nevaluation community. Previous Agents-as-an-Evaluator address this issue by\ninvolving agents in the generation of questions. Despite their success, the\nbiases in Agents-as-an-Evaluator methods remain largely unexplored. In this\npaper, we present a theoretical formulation of evaluation bias, providing\nvaluable insights into designing unbiased evaluation protocols. Furthermore, we\nidentify two type of bias in Agents-as-an-Evaluator through carefully designed\nprobing tasks on a minimal Agents-as-an-Evaluator setup. To address these\nissues, we propose the Unbiased Evaluator, an evaluation protocol that delivers\na more comprehensive, unbiased, and interpretable assessment of LLMs.Extensive\nexperiments reveal significant room for improvement in current LLMs.\nAdditionally, we demonstrate that the Unbiased Evaluator not only offers strong\nevidence of benchmark contamination but also provides interpretable evaluation\nresults.",
    "published_date": "2025-02-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.06655v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.06653v1",
    "title": "In-Context Learning (and Unlearning) of Length Biases",
    "authors": [
      "Stephanie Schoch",
      "Yangfeng Ji"
    ],
    "author_ids": [],
    "abstract": "Large language models have demonstrated strong capabilities to learn\nin-context, where exemplar input-output pairings are appended to the prompt for\ndemonstration. However, existing work has demonstrated the ability of models to\nlearn lexical and label biases in-context, which negatively impacts both\nperformance and robustness of models. The impact of other statistical data\nbiases remains under-explored, which this work aims to address. We specifically\ninvestigate the impact of length biases on in-context learning. We demonstrate\nthat models do learn length biases in the context window for their predictions,\nand further empirically analyze the factors that modulate the level of bias\nexhibited by the model. In addition, we show that learning length information\nin-context can be used to counter the length bias that has been encoded in\nmodels (e.g., via fine-tuning). This reveals the power of in-context learning\nin debiasing model prediction behaviors without the need for costly parameter\nupdates.",
    "published_date": "2025-02-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.06653v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.06547v1",
    "title": "Data Augmentation and Regularization for Learning Group Equivariance",
    "authors": [
      "Oskar Nordenfors",
      "Axel Flinth"
    ],
    "author_ids": [],
    "abstract": "In many machine learning tasks, known symmetries can be used as an inductive\nbias to improve model performance. In this paper, we consider learning group\nequivariance through training with data augmentation. We summarize results from\na previous paper of our own, and extend the results to show that equivariance\nof the trained model can be achieved through training on augmented data in\ntandem with regularization.",
    "published_date": "2025-02-10T00:00:00",
    "year": 2025,
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.OC",
      "68T07, 20C35, 37N40"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.06547v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.06487v1",
    "title": "Adaptive Prompting: Ad-hoc Prompt Composition for Social Bias Detection",
    "authors": [
      "Maximilian Spliethöver",
      "Tim Knebler",
      "Fabian Fumagalli",
      "Maximilian Muschalik",
      "Barbara Hammer",
      "Eyke Hüllermeier",
      "Henning Wachsmuth"
    ],
    "author_ids": [],
    "abstract": "Recent advances on instruction fine-tuning have led to the development of\nvarious prompting techniques for large language models, such as explicit\nreasoning steps. However, the success of techniques depends on various\nparameters, such as the task, language model, and context provided. Finding an\neffective prompt is, therefore, often a trial-and-error process. Most existing\napproaches to automatic prompting aim to optimize individual techniques instead\nof compositions of techniques and their dependence on the input. To fill this\ngap, we propose an adaptive prompting approach that predicts the optimal prompt\ncomposition ad-hoc for a given input. We apply our approach to social bias\ndetection, a highly context-dependent task that requires semantic\nunderstanding. We evaluate it with three large language models on three\ndatasets, comparing compositions to individual techniques and other baselines.\nThe results underline the importance of finding an effective prompt\ncomposition. Our approach robustly ensures high detection performance, and is\nbest in several settings. Moreover, first experiments on other tasks support\nits generalizability.",
    "published_date": "2025-02-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.06487v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.06439v1",
    "title": "Testing software for non-discrimination: an updated and extended audit in the Italian car insurance domain",
    "authors": [
      "Marco Rondina",
      "Antonio Vetrò",
      "Riccardo Coppola",
      "Oumaima Regragrui",
      "Alessandro Fabris",
      "Gianmaria Silvello",
      "Gian Antonio Susto",
      "Juan Carlos De Martin"
    ],
    "author_ids": [],
    "abstract": "Context. As software systems become more integrated into society's\ninfrastructure, the responsibility of software professionals to ensure\ncompliance with various non-functional requirements increases. These\nrequirements include security, safety, privacy, and, increasingly,\nnon-discrimination.\n  Motivation. Fairness in pricing algorithms grants equitable access to basic\nservices without discriminating on the basis of protected attributes.\n  Method. We replicate a previous empirical study that used black box testing\nto audit pricing algorithms used by Italian car insurance companies, accessible\nthrough a popular online system. With respect to the previous study, we\nenlarged the number of tests and the number of demographic variables under\nanalysis.\n  Results. Our work confirms and extends previous findings, highlighting the\nproblematic permanence of discrimination across time: demographic variables\nsignificantly impact pricing to this day, with birthplace remaining the main\ndiscriminatory factor against individuals not born in Italian cities. We also\nfound that driver profiles can determine the number of quotes available to the\nuser, denying equal opportunities to all.\n  Conclusion. The study underscores the importance of testing for\nnon-discrimination in software systems that affect people's everyday lives.\nPerforming algorithmic audits over time makes it possible to evaluate the\nevolution of such algorithms. It also demonstrates the role that empirical\nsoftware engineering can play in making software systems more accountable.",
    "published_date": "2025-02-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.06439v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.06341v1",
    "title": "Facial Analysis Systems and Down Syndrome",
    "authors": [
      "Marco Rondina",
      "Fabiana Vinci",
      "Antonio Vetrò",
      "Juan Carlos De Martin"
    ],
    "author_ids": [],
    "abstract": "The ethical, social and legal issues surrounding facial analysis technologies\nhave been widely debated in recent years. Key critics have argued that these\ntechnologies can perpetuate bias and discrimination, particularly against\nmarginalized groups. We contribute to this field of research by reporting on\nthe limitations of facial analysis systems with the faces of people with Down\nsyndrome: this particularly vulnerable group has received very little attention\nin the literature so far. This study involved the creation of a specific\ndataset of face images. An experimental group with faces of people with Down\nsyndrome, and a control group with faces of people who are not affected by the\nsyndrome. Two commercial tools were tested on the dataset, along three tasks:\ngender recognition, age prediction and face labelling. The results show an\noverall lower accuracy of prediction in the experimental group, and other\nspecific patterns of performance differences: i) high error rates in gender\nrecognition in the category of males with Down syndrome; ii) adults with Down\nsyndrome were more often incorrectly labelled as children; iii) social\nstereotypes are propagated in both the control and experimental groups, with\nlabels related to aesthetics more often associated with women, and labels\nrelated to education level and skills more often associated with men. These\nresults, although limited in scope, shed new light on the biases that alter\nface classification when applied to faces of people with Down syndrome. They\nconfirm the structural limitation of the technology, which is inherently\ndependent on the datasets used to train the models.",
    "published_date": "2025-02-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.06341v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.06317v1",
    "title": "The digital labour of artificial intelligence in Latin America: a comparison of Argentina, Brazil, and Venezuela",
    "authors": [
      "Paola Tubaro",
      "Antonio A. Casilli",
      "Mariana Fernández Massi",
      "Julieta Longo",
      "Juana Torres-Cierpe",
      "Matheus Viana Braz"
    ],
    "author_ids": [],
    "abstract": "The current hype around artificial intelligence (AI) conceals the substantial\nhuman intervention underlying its development. This article lifts the veil on\nthe precarious and low-paid 'data workers' who prepare data to train, test,\ncheck, and otherwise support models in the shadow of globalized AI production.\nWe use original questionnaire and interview data collected from 220 workers in\nArgentina (2021-22), 477 in Brazil (2023), and 214 in Venezuela (2021-22). We\ncompare them to detect common patterns and reveal the specificities of data\nwork in Latin America, while disclosing its role in AI production.We show that\ndata work is intertwined with economic hardship, inequalities, and informality.\nDespite workers' high educational attainment, disadvantage is widespread,\nthough with cross-country disparities. By acknowledging the interconnections\nbetween AI development, data work, and globalized production, we provide\ninsights for the regulation of AI and the future of work, aiming to achieve\npositive outcomes for all stakeholders.",
    "published_date": "2025-02-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.06317v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.09642v2",
    "title": "Krutrim LLM: Multilingual Foundational Model for over a Billion People",
    "authors": [
      "Aditya Kallappa",
      "Palash Kamble",
      "Abhinav Ravi",
      "Akshat Patidar",
      "Vinayak Dhruv",
      "Deepak Kumar",
      "Raghav Awasthi",
      "Arveti Manjunath",
      "Himanshu Gupta",
      "Shubham Agarwal",
      "Kumar Ashish",
      "Gautam Bhargava",
      "Chandra Khatri"
    ],
    "author_ids": [],
    "abstract": "India is a diverse society with unique challenges in developing AI systems,\nincluding linguistic diversity, oral traditions, data accessibility, and\nscalability. Existing foundation models are primarily trained on English,\nlimiting their effectiveness for India's population. Indic languages comprise\nonly 1 percent of Common Crawl corpora despite India representing 18 percent of\nthe global population, leading to linguistic biases. Thousands of regional\nlanguages, dialects, and code mixing create additional representation\nchallenges due to sparse training data.\n  We introduce Krutrim LLM, a 2 trillion token multilingual model designed for\nIndia's linguistic landscape. It incorporates the largest known Indic dataset,\nmitigating data scarcity and ensuring balanced performance across dialects.\nKrutrim outperforms or matches state-of-the-art models on Indic benchmarks\nwhile maintaining competitive English performance. Despite being significantly\nsmaller in training flops, Krutrim LLM matches or exceeds models like LLAMA-2\non 10 out of 16 tasks, with an average score of 0.57 versus 0.55. This\nevidences Krutrim's flexible multilingual fluency across diverse linguistic\ncontexts.\n  Krutrim is integrated with real-time search to improve factual accuracy in\nconversational AI applications. This enhances accessibility for over 1 billion\nusers worldwide. Through intentional design choices addressing data imbalances,\nKrutrim LLM signifies meaningful progress in building ethical, globally\nrepresentative AI models.",
    "published_date": "2025-02-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.09642v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.06215v1",
    "title": "LessLeak-Bench: A First Investigation of Data Leakage in LLMs Across 83 Software Engineering Benchmarks",
    "authors": [
      "Xin Zhou",
      "Martin Weyssow",
      "Ratnadira Widyasari",
      "Ting Zhang",
      "Junda He",
      "Yunbo Lyu",
      "Jianming Chang",
      "Beiqi Zhang",
      "Dan Huang",
      "David Lo"
    ],
    "author_ids": [],
    "abstract": "Large Language Models (LLMs) are widely utilized in software engineering (SE)\ntasks, such as code generation and automated program repair. However, their\nreliance on extensive and often undisclosed pre-training datasets raises\nsignificant concerns about data leakage, where the evaluation benchmark data is\nunintentionally ``seen'' by LLMs during the model's construction phase. The\ndata leakage issue could largely undermine the validity of LLM-based research\nand evaluations. Despite the increasing use of LLMs in the SE community, there\nis no comprehensive study that assesses the extent of data leakage in SE\nbenchmarks for LLMs yet. To address this gap, this paper presents the first\nlarge-scale analysis of data leakage in 83 SE benchmarks concerning LLMs. Our\nresults show that in general, data leakage in SE benchmarks is minimal, with\naverage leakage ratios of only 4.8\\%, 2.8\\%, and 0.7\\% for Python, Java, and\nC/C++ benchmarks, respectively. However, some benchmarks exhibit relatively\nhigher leakage ratios, which raises concerns about their bias in evaluation.\nFor instance, QuixBugs and BigCloneBench have leakage ratios of 100.0\\% and\n55.7\\%, respectively. Furthermore, we observe that data leakage has a\nsubstantial impact on LLM evaluation. We also identify key causes of high data\nleakage, such as the direct inclusion of benchmark data in pre-training\ndatasets and the use of coding platforms like LeetCode for benchmark\nconstruction. To address the data leakage, we introduce\n\\textbf{LessLeak-Bench}, a new benchmark that removes leaked samples from the\n83 SE benchmarks, enabling more reliable LLM evaluations in future research.\nOur study enhances the understanding of data leakage in SE benchmarks and\nprovides valuable insights for future research involving LLMs in SE.",
    "published_date": "2025-02-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.06215v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.06191v2",
    "title": "Wearable AR in Everyday Contexts: Insights from a Digital Ethnography of YouTube Videos",
    "authors": [
      "Tram Thi Minh Tran",
      "Shane Brown",
      "Oliver Weidlich",
      "Soojeong Yoo",
      "Callum Parker"
    ],
    "author_ids": [],
    "abstract": "With growing investment in consumer augmented reality (AR) headsets and\nglasses, wearable AR is moving from niche applications to everyday use.\nHowever, current research primarily examines AR in controlled settings,\noffering limited insights into its use in real-world daily life. To address\nthis gap, we adopt a digital ethnographic approach, analysing 27 hours of 112\nYouTube videos featuring early adopters. These videos capture usage ranging\nfrom continuous periods of hours to intermittent use over weeks and months. Our\nanalysis shows that currently, wearable AR is primarily used for media\nconsumption and gaming. While productivity is a desired use case, frequent use\nis constrained by current hardware limitations and the nascent application\necosystem. Users seek continuity in their digital experience, desiring\nfunctionalities similar to those on smartphones, tablets, or computers. We\npropose implications for everyday AR development that promote adoption while\nensuring safe, ethical, and socially-aware integration into daily life.",
    "published_date": "2025-02-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.06191v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.06180v1",
    "title": "RideKE: Leveraging Low-Resource, User-Generated Twitter Content for Sentiment and Emotion Detection in Kenyan Code-Switched Dataset",
    "authors": [
      "Naome A. Etori",
      "Maria L. Gini"
    ],
    "author_ids": [],
    "abstract": "Social media has become a crucial open-access platform for individuals to\nexpress opinions and share experiences. However, leveraging low-resource\nlanguage data from Twitter is challenging due to scarce, poor-quality content\nand the major variations in language use, such as slang and code-switching.\nIdentifying tweets in these languages can be difficult as Twitter primarily\nsupports high-resource languages. We analyze Kenyan code-switched data and\nevaluate four state-of-the-art (SOTA) transformer-based pretrained models for\nsentiment and emotion classification, using supervised and semi-supervised\nmethods. We detail the methodology behind data collection and annotation, and\nthe challenges encountered during the data curation phase. Our results show\nthat XLM-R outperforms other models; for sentiment analysis, XLM-R supervised\nmodel achieves the highest accuracy (69.2\\%) and F1 score (66.1\\%), XLM-R\nsemi-supervised (67.2\\% accuracy, 64.1\\% F1 score). In emotion analysis,\nDistilBERT supervised leads in accuracy (59.8\\%) and F1 score (31\\%), mBERT\nsemi-supervised (accuracy (59\\% and F1 score 26.5\\%). AfriBERTa models show the\nlowest accuracy and F1 scores. All models tend to predict neutral sentiment,\nwith Afri-BERT showing the highest bias and unique sensitivity to empathy\nemotion. https://github.com/NEtori21/Ride_hailing",
    "published_date": "2025-02-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.06180v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.06105v1",
    "title": "Comprehensive Framework for Evaluating Conversational AI Chatbots",
    "authors": [
      "Shailja Gupta",
      "Rajesh Ranjan",
      "Surya Narayan Singh"
    ],
    "author_ids": [],
    "abstract": "Conversational AI chatbots are transforming industries by streamlining\ncustomer service, automating transactions, and enhancing user engagement.\nHowever, evaluating these systems remains a challenge, particularly in\nfinancial services, where compliance, user trust, and operational efficiency\nare critical. This paper introduces a novel evaluation framework that\nsystematically assesses chatbots across four dimensions: cognitive and\nconversational intelligence, user experience, operational efficiency, and\nethical and regulatory compliance. By integrating advanced AI methodologies\nwith financial regulations, the framework bridges theoretical foundations and\nreal-world deployment challenges. Additionally, we outline future research\ndirections, emphasizing improvements in conversational coherence, real-time\nadaptability, and fairness.",
    "published_date": "2025-02-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.06105v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.06094v1",
    "title": "Fair-MoE: Fairness-Oriented Mixture of Experts in Vision-Language Models",
    "authors": [
      "Peiran Wang",
      "Linjie Tong",
      "Jiaxiang Liu",
      "Zuozhu Liu"
    ],
    "author_ids": [],
    "abstract": "Fairness is a fundamental principle in medical ethics. Vision Language Models\n(VLMs) have shown significant potential in the medical field due to their\nability to leverage both visual and linguistic contexts, reducing the need for\nlarge datasets and enabling the performance of complex tasks. However, the\nexploration of fairness within VLM applications remains limited. Applying VLMs\nwithout a comprehensive analysis of fairness could lead to concerns about equal\ntreatment opportunities and diminish public trust in medical deep learning\nmodels. To build trust in medical VLMs, we propose Fair-MoE, a model\nspecifically designed to ensure both fairness and effectiveness. Fair-MoE\ncomprises two key components: \\textit{the Fairness-Oriented Mixture of Experts\n(FO-MoE)} and \\textit{the Fairness-Oriented Loss (FOL)}. FO-MoE is designed to\nleverage the expertise of various specialists to filter out biased patch\nembeddings and use an ensemble approach to extract more equitable information\nrelevant to specific tasks. FOL is a novel fairness-oriented loss function that\nnot only minimizes the distances between different attributes but also\noptimizes the differences in the dispersion of various attributes'\ndistributions. Extended experiments demonstrate the effectiveness and fairness\nof Fair-MoE. Tested on the Harvard-FairVLMed dataset, Fair-MoE showed\nimprovements in both fairness and accuracy across all four attributes. Code\nwill be publicly available.",
    "published_date": "2025-02-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.06094v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.06082v1",
    "title": "A Fair and Optimal Approach to Sequential Health Rationing",
    "authors": [
      "Zhaohong Sun"
    ],
    "author_ids": [],
    "abstract": "The COVID-19 pandemic underscored the urgent need for fair and effective\nallocation of scarce resources, from hospital beds to vaccine distribution. In\nthis paper, we study a healthcare rationing problem where identical units of a\nresource are divided into different categories, and agents are assigned based\non priority rankings. % We first introduce a simple and efficient algorithm\nthat satisfies four fundamental axioms critical to practical applications:\neligible compliance, non-wastefulness, respect for priorities, and maximum\ncardinality. This new algorithm is not only conceptually simpler but also\ncomputationally faster than the Reverse Rejecting rules proposed in recent\nwork. % We then extend our analysis to a more general sequential setting, where\ncategories can be processed both sequentially and simultaneously. For this\nbroader framework, we introduce a novel algorithm that preserves the four\nfundamental axioms while achieving additional desirable properties that\nexisting rules fail to satisfy. Furthermore, we prove that when a strict\nprecedence order over categories is imposed, this rule is the unique mechanism\nthat satisfies these properties.",
    "published_date": "2025-02-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.GT",
      "91B68",
      "F.2; I.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.06082v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.04756v1",
    "title": "Peeking Behind Closed Doors: Risks of LLM Evaluation by Private Data Curators",
    "authors": [
      "Hritik Bansal",
      "Pratyush Maini"
    ],
    "author_ids": [],
    "abstract": "The rapid advancement in building large language models (LLMs) has\nintensified competition among big-tech companies and AI startups. In this\nregard, model evaluations are critical for product and investment-related\ndecision-making. While open evaluation sets like MMLU initially drove progress,\nconcerns around data contamination and data bias have constantly questioned\ntheir reliability. As a result, it has led to the rise of private data curators\nwho have begun conducting hidden evaluations with high-quality self-curated\ntest prompts and their own expert annotators. In this paper, we argue that\ndespite potential advantages in addressing contamination issues, private\nevaluations introduce inadvertent financial and evaluation risks. In\nparticular, the key concerns include the potential conflict of interest arising\nfrom private data curators' business relationships with their clients (leading\nLLM firms). In addition, we highlight that the subjective preferences of\nprivate expert annotators will lead to inherent evaluation bias towards the\nmodels trained with the private curators' data. Overall, this paper lays the\nfoundation for studying the risks of private evaluations that can lead to\nwide-ranging community discussions and policy changes.",
    "published_date": "2025-02-09T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.04756v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.06009v1",
    "title": "Media Bias Detector: Designing and Implementing a Tool for Real-Time Selection and Framing Bias Analysis in News Coverage",
    "authors": [
      "Jenny S Wang",
      "Samar Haider",
      "Amir Tohidi",
      "Anushkaa Gupta",
      "Yuxuan Zhang",
      "Chris Callison-Burch",
      "David Rothschild",
      "Duncan J Watts"
    ],
    "author_ids": [],
    "abstract": "Mainstream media, through their decisions on what to cover and how to frame\nthe stories they cover, can mislead readers without using outright falsehoods.\nTherefore, it is crucial to have tools that expose these editorial choices\nunderlying media bias. In this paper, we introduce the Media Bias Detector, a\ntool for researchers, journalists, and news consumers. By integrating large\nlanguage models, we provide near real-time granular insights into the topics,\ntone, political lean, and facts of news articles aggregated to the publisher\nlevel. We assessed the tool's impact by interviewing 13 experts from\njournalism, communications, and political science, revealing key insights into\nusability and functionality, practical applications, and AI's role in powering\nmedia bias tools. We explored this in more depth with a follow-up survey of 150\nnews consumers. This work highlights opportunities for AI-driven tools that\nempower users to critically engage with media content, particularly in\npolitically charged environments.",
    "published_date": "2025-02-09T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.06009v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.05851v1",
    "title": "Fairness Driven Slot Allocation Problem in Billboard Advertisement",
    "authors": [
      "Dildar Ali",
      "Suman Banerjee",
      "Shweta Jain",
      "Yamuna Prasad"
    ],
    "author_ids": [],
    "abstract": "In billboard advertisement, a number of digital billboards are owned by an\ninfluence provider, and several commercial houses (which we call advertisers)\napproach the influence provider for a specific number of views of their\nadvertisement content on a payment basis. Though the billboard slot allocation\nproblem has been studied in the literature, this problem still needs to be\naddressed from a fairness point of view. In this paper, we introduce the Fair\nBillboard Slot Allocation Problem, where the objective is to allocate a given\nset of billboard slots among a group of advertisers based on their demands\nfairly and efficiently. As fairness criteria, we consider the maximin fair\nshare, which ensures that each advertiser will receive a subset of slots that\nmaximizes the minimum share for all the advertisers. We have proposed a\nsolution approach that generates an allocation and provides an approximate\nmaximum fair share. The proposed methodology has been analyzed to understand\nits time and space requirements and a performance guarantee. It has been\nimplemented with real-world trajectory and billboard datasets, and the results\nhave been reported. The results show that the proposed approach leads to a\nbalanced allocation by satisfying the maximin fairness criteria. At the same\ntime, it maximizes the utility of advertisers.",
    "published_date": "2025-02-09T00:00:00",
    "year": 2025,
    "categories": [
      "cs.GT",
      "cs.DB",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.05851v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.05849v1",
    "title": "Fact-or-Fair: A Checklist for Behavioral Testing of AI Models on Fairness-Related Queries",
    "authors": [
      "Jen-tse Huang",
      "Yuhang Yan",
      "Linqi Liu",
      "Yixin Wan",
      "Wenxuan Wang",
      "Kai-Wei Chang",
      "Michael R. Lyu"
    ],
    "author_ids": [],
    "abstract": "The generation of incorrect images, such as depictions of people of color in\nNazi-era uniforms by Gemini, frustrated users and harmed Google's reputation,\nmotivating us to investigate the relationship between accurately reflecting\nfactuality and promoting diversity and equity. In this study, we focus on 19\nreal-world statistics collected from authoritative sources. Using these\nstatistics, we develop a checklist comprising objective and subjective queries\nto analyze behavior of large language models (LLMs) and text-to-image (T2I)\nmodels. Objective queries assess the models' ability to provide accurate world\nknowledge. In contrast, the design of subjective queries follows a key\nprinciple: statistical or experiential priors should not be overgeneralized to\nindividuals, ensuring that models uphold diversity. These subjective queries\nare based on three common human cognitive errors that often result in social\nbiases. We propose metrics to assess factuality and fairness, and formally\nprove the inherent trade-off between these two aspects. Results show that\nGPT-4o and DALL-E 3 perform notably well among six LLMs and four T2I models.\nOur code is publicly available at https://github.com/uclanlp/Fact-or-Fair.",
    "published_date": "2025-02-09T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.05849v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.05797v2",
    "title": "Seamless Integration: The Evolution, Design, and Future Impact of Wearable Technology",
    "authors": [
      "David Pearl",
      "James Intriligator",
      "Xuanjiang Liu"
    ],
    "author_ids": [],
    "abstract": "The rapid evolution of wearable technology marks a transformative phase in\nhuman-computer interaction, seamlessly integrating digital functionality into\ndaily life. This paper explores the historical trajectory, current\nadvancements, and future potential of wearables, emphasizing their impact on\nhealthcare, productivity, and personal well-being. Key developments include the\nintegration of artificial intelligence (AI), Internet of Things (IoT), and\naugmented reality (AR), driving personalization, real-time adaptability, and\nenhanced user experiences. The study highlights user-centered design\nprinciples, ethical considerations, and interdisciplinary collaboration as\ncritical factors in creating wearables that are intuitive, inclusive, and\nsecure. Furthermore, the paper examines sustainability trends, such as modular\ndesigns and eco-friendly materials, aligning innovation with environmental\nresponsibility. By addressing challenges like data privacy, algorithmic bias,\nand usability, wearable technology is poised to redefine the interaction\nbetween humans and technology, offering unprecedented opportunities for\nenrichment and empowerment in diverse contexts. This comprehensive analysis\nprovides a roadmap for advancing wearables to meet emerging societal needs\nwhile fostering ethical and sustainable growth.",
    "published_date": "2025-02-09T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.05797v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.05722v3",
    "title": "Explainable and Class-Revealing Signal Feature Extraction via Scattering Transform and Constrained Zeroth-Order Optimization",
    "authors": [
      "Naoki Saito",
      "David Weber"
    ],
    "author_ids": [],
    "abstract": "We propose a new method to extract discriminant and explainable features from\na particular machine learning model, i.e., a combination of the scattering\ntransform and the multiclass logistic regression. Although this model is\nwell-known for its ability to learn various signal classes with high\nclassification rate, it remains elusive to understand why it can generate such\nsuccessful classification, mainly due to the nonlinearity of the scattering\ntransform. In order to uncover the meaning of the scattering transform\ncoefficients selected by the multiclass logistic regression (with the Lasso\npenalty), we adopt zeroth-order optimization algorithms to search an input\npattern that maximizes the class probability of a class of interest given the\nlearned model. In order to do so, it turns out that imposing sparsity and\nsmoothness of input patterns is important. We demonstrate the effectiveness of\nour proposed method using a couple of synthetic time-series classification\nproblems.",
    "published_date": "2025-02-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "eess.SP",
      "math.OC",
      "stat.ML",
      "42C40, 68T10, 62H30, 65K10, 90C56",
      "I.5.1; I.2.6; G.1.6; G.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.05722v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.05698v1",
    "title": "A Conceptual Exploration of Generative AI-Induced Cognitive Dissonance and its Emergence in University-Level Academic Writing",
    "authors": [
      "Carl Errol Seran",
      "Myles Joshua Toledo Tan",
      "Hezerul Abdul Karim",
      "Nouar AlDahoul"
    ],
    "author_ids": [],
    "abstract": "The integration of Generative Artificial Intelligence (GenAI) into\nuniversity-level academic writing presents both opportunities and challenges,\nparticularly in relation to cognitive dissonance (CD). This work explores how\nGenAI serves as both a trigger and amplifier of CD, as students navigate\nethical concerns, academic integrity, and self-efficacy in their writing\npractices. By synthesizing empirical evidence and theoretical insights, we\nintroduce a hypothetical construct of GenAI-induced CD, illustrating the\npsychological tension between AI-driven efficiency and the principles of\noriginality, effort, and intellectual ownership. We further discuss strategies\nto mitigate this dissonance, including reflective pedagogy, AI literacy\nprograms, transparency in GenAI use, and discipline-specific task redesigns.\nThese approaches reinforce critical engagement with AI, fostering a balanced\nperspective that integrates technological advancements while safeguarding human\ncreativity and learning. Our findings contribute to ongoing discussions on AI\nin education, self-regulated learning, and ethical AI use, offering a\nconceptual framework for institutions to develop guidelines that align AI\nadoption with academic values.",
    "published_date": "2025-02-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.05698v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.05668v1",
    "title": "The late-stage training dynamics of (stochastic) subgradient descent on homogeneous neural networks",
    "authors": [
      "Sholom Schechtman",
      "Nicolas Schreuder"
    ],
    "author_ids": [],
    "abstract": "We analyze the implicit bias of constant step stochastic subgradient descent\n(SGD). We consider the setting of binary classification with homogeneous neural\nnetworks - a large class of deep neural networks with ReLU-type activation\nfunctions such as MLPs and CNNs without biases. We interpret the dynamics of\nnormalized SGD iterates as an Euler-like discretization of a conservative field\nflow that is naturally associated to the normalized classification margin.\nOwing to this interpretation, we show that normalized SGD iterates converge to\nthe set of critical points of the normalized margin at late-stage training\n(i.e., assuming that the data is correctly classified with positive normalized\nmargin). Up to our knowledge, this is the first extension of the analysis of\nLyu and Li (2020) on the discrete dynamics of gradient descent to the nonsmooth\nand stochastic setting. Our main result applies to binary classification with\nexponential or logistic losses. We additionally discuss extensions to more\ngeneral settings.",
    "published_date": "2025-02-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.NE",
      "math.OC",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.05668v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.05649v1",
    "title": "Gender Bias in Instruction-Guided Speech Synthesis Models",
    "authors": [
      "Chun-Yi Kuan",
      "Hung-yi Lee"
    ],
    "author_ids": [],
    "abstract": "Recent advancements in controllable expressive speech synthesis, especially\nin text-to-speech (TTS) models, have allowed for the generation of speech with\nspecific styles guided by textual descriptions, known as style prompts. While\nthis development enhances the flexibility and naturalness of synthesized\nspeech, there remains a significant gap in understanding how these models\nhandle vague or abstract style prompts. This study investigates the potential\ngender bias in how models interpret occupation-related prompts, specifically\nexamining their responses to instructions like \"Act like a nurse\". We explore\nwhether these models exhibit tendencies to amplify gender stereotypes when\ninterpreting such prompts. Our experimental results reveal the model's tendency\nto exhibit gender bias for certain occupations. Moreover, models of different\nsizes show varying degrees of this bias across these occupations.",
    "published_date": "2025-02-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.LG",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.05649v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.05623v1",
    "title": "Mixing Time of the Proximal Sampler in Relative Fisher Information via Strong Data Processing Inequality",
    "authors": [
      "Andre Wibisono"
    ],
    "author_ids": [],
    "abstract": "We study the mixing time guarantee for sampling in relative Fisher\ninformation via the Proximal Sampler algorithm, which is an approximate\nproximal discretization of the Langevin dynamics. We show that when the target\nprobability distribution is strongly log-concave, the relative Fisher\ninformation converges exponentially fast along the Proximal Sampler; this\nmatches the exponential convergence rate of the relative Fisher information\nalong the continuous-time Langevin dynamics for strongly log-concave target.\nWhen combined with a standard implementation of the Proximal Sampler via\nrejection sampling, this exponential convergence rate provides a high-accuracy\niteration complexity guarantee for the Proximal Sampler in relative Fisher\ninformation when the target distribution is strongly log-concave and\nlog-smooth. Our proof proceeds by establishing a strong data processing\ninequality for relative Fisher information along the Gaussian channel under\nstrong log-concavity, and a data processing inequality along the reverse\nGaussian channel for a special distribution. The forward and reverse Gaussian\nchannels compose to form the Proximal Sampler, and these data processing\ninequalities imply the exponential convergence rate of the relative Fisher\ninformation along the Proximal Sampler.",
    "published_date": "2025-02-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IT",
      "cs.LG",
      "math.IT",
      "math.OC",
      "math.ST",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.05623v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.05622v2",
    "title": "Social inequality and cultural factors impact the awareness and reaction during the cryptic transmission period of pandemic",
    "authors": [
      "Zhuoren Jiang",
      "Xiaozhong Liu",
      "Yangyang Kang",
      "Changlong Sun",
      "Yong-Yeol Ahn",
      "Johan Bollen"
    ],
    "author_ids": [],
    "abstract": "The World Health Organization (WHO) declared the COVID-19 outbreak a Public\nHealth Emergency of International Concern (PHEIC) on January 31, 2020. However,\nrumors of a \"mysterious virus\" had already been circulating in China in\nDecember 2019, possibly preceding the first confirmed COVID-19 case.\nUnderstanding how awareness about an emerging pandemic spreads through society\nis vital not only for enhancing disease surveillance, but also for mitigating\ndemand shocks and social inequities, such as shortages of personal protective\nequipment (PPE) and essential supplies. Here we leverage a massive e-commerce\ndataset comprising 150 billion online queries and purchase records from 94\nmillion people to detect the traces of early awareness and public response\nduring the cryptic transmission period of COVID-19. Our analysis focuses on\nidentifying information gaps across different demographic cohorts, revealing\nsignificant social inequities and the role of cultural factors in shaping\nawareness diffusion and response behaviors. By modeling awareness diffusion in\nheterogeneous social networks and analyzing online shopping behavior, we\nuncover the evolving characteristics of vulnerable populations. Our findings\nexpand the theoretical understanding of awareness spread and social inequality\nin the early stages of a pandemic, highlighting the critical importance of\ne-commerce data and social network data in effectively and timely addressing\nfuture pandemic challenges. We also provide actionable recommendations to\nbetter manage and mitigate dynamic social inequalities in public health crises.",
    "published_date": "2025-02-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.05622v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.05608v1",
    "title": "Closing the Responsibility Gap in AI-based Network Management: An Intelligent Audit System Approach",
    "authors": [
      "Emanuel Figetakis",
      "Ahmed Refaey Hussein"
    ],
    "author_ids": [],
    "abstract": "Existing network paradigms have achieved lower downtime as well as a higher\nQuality of Experience (QoE) through the use of Artificial Intelligence\n(AI)-based network management tools. These AI management systems, allow for\nautomatic responses to changes in network conditions, lowering operation costs\nfor operators, and improving overall performance. While adopting AI-based\nmanagement tools enhance the overall network performance, it also introduce\nchallenges such as removing human supervision, privacy violations, algorithmic\nbias, and model inaccuracies. Furthermore, AI-based agents that fail to address\nthese challenges should be culpable themselves rather than the network as a\nwhole. To address this accountability gap, a framework consisting of a Deep\nReinforcement Learning (DRL) model and a Machine Learning (ML) model is\nproposed to identify and assign numerical values of responsibility to the\nAI-based management agents involved in any decision-making regarding the\nnetwork conditions, which eventually affects the end-user. A simulation\nenvironment was created for the framework to be trained using simulated network\noperation parameters. The DRL model had a 96% accuracy during testing for\nidentifying the AI-based management agents, while the ML model using gradient\ndescent learned the network conditions at an 83% accuracy during testing.",
    "published_date": "2025-02-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.05608v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.05594v1",
    "title": "A Hybrid Tabu Scatter Search Algorithm for Simulation-Based Optimization of Multi-Objective Runway Operations Scheduling",
    "authors": [
      "Bulent Soykan"
    ],
    "author_ids": [],
    "abstract": "This dissertation addresses the growing challenge of air traffic flow\nmanagement by proposing a simulation-based optimization (SbO) approach for\nmulti-objective runway operations scheduling. The goal is to optimize airport\ncapacity utilization while minimizing delays, fuel consumption, and\nenvironmental impacts. Given the NP-Hard complexity of the problem, traditional\nanalytical methods often rely on oversimplifications and fail to account for\nreal-world uncertainties, limiting their practical applicability. The proposed\nSbO framework integrates a discrete-event simulation model to handle stochastic\nconditions and a hybrid Tabu-Scatter Search algorithm to identify\nPareto-optimal solutions, explicitly incorporating uncertainty and fairness\namong aircraft as key objectives. Computational experiments using real-world\ndata from a major U.S. airport demonstrate the approach's effectiveness and\ntractability, outperforming traditional methods such as First-Come-First-Served\n(FCFS) and deterministic approaches while maintaining schedule fairness. The\nalgorithm's ability to generate trade-off solutions between competing\nobjectives makes it a promising decision support tool for air traffic\ncontrollers managing complex runway operations.",
    "published_date": "2025-02-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.NE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.05594v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.05500v1",
    "title": "Vision-Ultrasound Robotic System based on Deep Learning for Gas and Arc Hazard Detection in Manufacturing",
    "authors": [
      "Jin-Hee Lee",
      "Dahyun Nam",
      "Robin Inho Kee",
      "YoungKey Kim",
      "Seok-Jun Buu"
    ],
    "author_ids": [],
    "abstract": "Gas leaks and arc discharges present significant risks in industrial\nenvironments, requiring robust detection systems to ensure safety and\noperational efficiency. Inspired by human protocols that combine visual\nidentification with acoustic verification, this study proposes a deep\nlearning-based robotic system for autonomously detecting and classifying gas\nleaks and arc discharges in manufacturing settings. The system is designed to\nexecute all experimental tasks entirely onboard the robot. Utilizing a\n112-channel acoustic camera operating at a 96 kHz sampling rate to capture\nultrasonic frequencies, the system processes real-world datasets recorded in\ndiverse industrial scenarios. These datasets include multiple gas leak\nconfigurations (e.g., pinhole, open end) and partial discharge types (Corona,\nSurface, Floating) under varying environmental noise conditions. Proposed\nsystem integrates visual detection and a beamforming-enhanced acoustic analysis\npipeline. Signals are transformed using STFT and refined through Gamma\nCorrection, enabling robust feature extraction. An Inception-inspired CNN\nfurther classifies hazards, achieving 99% gas leak detection accuracy. The\nsystem not only detects individual hazard sources but also enhances\nclassification reliability by fusing multi-modal data from both vision and\nacoustic sensors. When tested in reverberation and noise-augmented\nenvironments, the system outperformed conventional models by up to 44%p, with\nexperimental tasks meticulously designed to ensure fairness and\nreproducibility. Additionally, the system is optimized for real-time\ndeployment, maintaining an inference time of 2.1 seconds on a mobile robotic\nplatform. By emulating human-like inspection protocols and integrating vision\nwith acoustic modalities, this study presents an effective solution for\nindustrial automation, significantly improving safety and operational\nreliability.",
    "published_date": "2025-02-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.RO",
      "cs.AI",
      "I.2.1; I.2.9"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.05500v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.06872v1",
    "title": "Towards Trustworthy Retrieval Augmented Generation for Large Language Models: A Survey",
    "authors": [
      "Bo Ni",
      "Zheyuan Liu",
      "Leyao Wang",
      "Yongjia Lei",
      "Yuying Zhao",
      "Xueqi Cheng",
      "Qingkai Zeng",
      "Luna Dong",
      "Yinglong Xia",
      "Krishnaram Kenthapadi",
      "Ryan Rossi",
      "Franck Dernoncourt",
      "Md Mehrab Tanjim",
      "Nesreen Ahmed",
      "Xiaorui Liu",
      "Wenqi Fan",
      "Erik Blasch",
      "Yu Wang",
      "Meng Jiang",
      "Tyler Derr"
    ],
    "author_ids": [],
    "abstract": "Retrieval-Augmented Generation (RAG) is an advanced technique designed to\naddress the challenges of Artificial Intelligence-Generated Content (AIGC). By\nintegrating context retrieval into content generation, RAG provides reliable\nand up-to-date external knowledge, reduces hallucinations, and ensures relevant\ncontext across a wide range of tasks. However, despite RAG's success and\npotential, recent studies have shown that the RAG paradigm also introduces new\nrisks, including robustness issues, privacy concerns, adversarial attacks, and\naccountability issues. Addressing these risks is critical for future\napplications of RAG systems, as they directly impact their trustworthiness.\nAlthough various methods have been developed to improve the trustworthiness of\nRAG methods, there is a lack of a unified perspective and framework for\nresearch in this topic. Thus, in this paper, we aim to address this gap by\nproviding a comprehensive roadmap for developing trustworthy RAG systems. We\nplace our discussion around five key perspectives: reliability, privacy,\nsafety, fairness, explainability, and accountability. For each perspective, we\npresent a general framework and taxonomy, offering a structured approach to\nunderstanding the current challenges, evaluating existing solutions, and\nidentifying promising future research directions. To encourage broader adoption\nand innovation, we also highlight the downstream applications where trustworthy\nRAG systems have a significant impact.",
    "published_date": "2025-02-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.06872v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.05459v1",
    "title": "DCENWCNet: A Deep CNN Ensemble Network for White Blood Cell Classification with LIME-Based Explainability",
    "authors": [
      "Sibasish Dhibar"
    ],
    "author_ids": [],
    "abstract": "White blood cells (WBC) are important parts of our immune system, and they\nprotect our body against infections by eliminating viruses, bacteria, parasites\nand fungi. The number of WBC types and the total number of WBCs provide\nimportant information about our health status. A traditional method,\nconvolutional neural networks (CNN), a deep learning architecture, can classify\nthe blood cell from a part of an object and perform object recognition. Various\nCNN models exhibit potential; however, their development often involves ad-hoc\nprocesses that neglect unnecessary layers, leading to issues with unbalanced\ndatasets and insufficient data augmentation. To address these challenges, we\npropose a novel ensemble approach that integrates three CNN architectures, each\nuniquely configured with different dropout and max-pooling layer settings to\nenhance feature learning. This ensemble model, named DCENWCNet, effectively\nbalances the bias-variance trade-off. When evaluated on the widely recognized\nRabbin-WBC dataset, our model outperforms existing state-of-the-art networks,\nachieving highest mean accuracy. Additionally, it demonstrates superior\nperformance in precision, recall, F1-score, and Area Under the ROC Curve (AUC)\nacross all categories. To delve deeper into the interpretability of\nclassifiers, we employ reliable post-hoc explanation techniques, including\nLocal Interpretable Model-Agnostic Explanations (LIME). These methods\napproximate the behavior of a black-box model by elucidating the relationships\nbetween feature values and predictions. Interpretable results enable users to\ncomprehend and validate the model's predictions, thereby increasing their\nconfidence in the automated diagnosis.",
    "published_date": "2025-02-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI",
      "q-bio.CB",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.05459v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.05442v1",
    "title": "The Odyssey of the Fittest: Can Agents Survive and Still Be Good?",
    "authors": [
      "Dylan Waldner",
      "Risto Miikkulainen"
    ],
    "author_ids": [],
    "abstract": "As AI models grow in power and generality, understanding how agents learn and\nmake decisions in complex environments is critical to promoting ethical\nbehavior. This paper examines the ethical implications of implementing\nbiological drives, specifically, self preservation, into three different\nagents. A Bayesian agent optimized with NEAT, a Bayesian agent optimized with\nstochastic variational inference, and a GPT 4o agent play a simulated, LLM\ngenerated text based adventure game. The agents select actions at each scenario\nto survive, adapting to increasingly challenging scenarios. Post simulation\nanalysis evaluates the ethical scores of the agent's decisions, uncovering the\ntradeoffs they navigate to survive. Specifically, analysis finds that when\ndanger increases, agents ignore ethical considerations and opt for unethical\nbehavior. The agents' collective behavior, trading ethics for survival,\nsuggests that prioritizing survival increases the risk of unethical behavior.\nIn the context of AGI, designing agents to prioritize survival may amplify the\nlikelihood of unethical decision making and unintended emergent behaviors,\nraising fundamental questions about goal design in AI safety research.",
    "published_date": "2025-02-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.05442v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.05435v1",
    "title": "Unbiased Sliced Wasserstein Kernels for High-Quality Audio Captioning",
    "authors": [
      "Manh Luong",
      "Khai Nguyen",
      "Dinh Phung",
      "Gholamreza Haffari",
      "Lizhen Qu"
    ],
    "author_ids": [],
    "abstract": "Teacher-forcing training for audio captioning usually leads to exposure bias\ndue to training and inference mismatch. Prior works propose the contrastive\nmethod to deal with caption degeneration. However, the contrastive method\nignores the temporal information when measuring similarity across acoustic and\nlinguistic modalities, leading to inferior performance. In this work, we\ndevelop the temporal-similarity score by introducing the unbiased sliced\nWasserstein RBF (USW-RBF) kernel equipped with rotary positional embedding to\naccount for temporal information across modalities. In contrast to the\nconventional sliced Wasserstein RBF kernel, we can form an unbiased estimation\nof USW-RBF kernel via Monte Carlo estimation. Therefore, it is well-suited to\nstochastic gradient optimization algorithms, and its approximation error\ndecreases at a parametric rate of $\\mathcal{O}(L^{-1/2})$ with $L$ Monte Carlo\nsamples. Additionally, we introduce an audio captioning framework based on the\nunbiased sliced Wasserstein kernel, incorporating stochastic decoding methods\nto mitigate caption degeneration during the generation process. We conduct\nextensive quantitative and qualitative experiments on two datasets, AudioCaps\nand Clotho, to illustrate the capability of generating high-quality audio\ncaptions. Experimental results show that our framework is able to increase\ncaption length, lexical diversity, and text-to-audio self-retrieval accuracy.",
    "published_date": "2025-02-08T00:00:00",
    "year": 2025,
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.05435v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.18481v1",
    "title": "MDE: Modality Discrimination Enhancement for Multi-modal Recommendation",
    "authors": [
      "Hang Zhou",
      "Yucheng Wang",
      "Huijing Zhan"
    ],
    "author_ids": [],
    "abstract": "Multi-modal recommendation systems aim to enhance performance by integrating\nan item's content features across various modalities with user behavior data.\nEffective utilization of features from different modalities requires addressing\ntwo challenges: preserving semantic commonality across modalities\n(modality-shared) and capturing unique characteristics for each modality\n(modality-specific). Most existing approaches focus on aligning feature spaces\nacross modalities, which helps represent modality-shared features. However,\nmodality-specific distinctions are often neglected, especially when there are\nsignificant semantic variations between modalities. To address this, we propose\na Modality Distinctiveness Enhancement (MDE) framework that prioritizes\nextracting modality-specific information to improve recommendation accuracy\nwhile maintaining shared features. MDE enhances differences across modalities\nthrough a novel multi-modal fusion module and introduces a node-level trade-off\nmechanism to balance cross-modal alignment and differentiation. Extensive\nexperiments on three public datasets show that our approach significantly\noutperforms other state-of-the-art methods, demonstrating the effectiveness of\njointly considering modality-shared and modality-specific features.",
    "published_date": "2025-02-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.18481v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.05374v3",
    "title": "Towards LLM Unlearning Resilient to Relearning Attacks: A Sharpness-Aware Minimization Perspective and Beyond",
    "authors": [
      "Chongyu Fan",
      "Jinghan Jia",
      "Yihua Zhang",
      "Anil Ramakrishna",
      "Mingyi Hong",
      "Sijia Liu"
    ],
    "author_ids": [],
    "abstract": "The LLM unlearning technique has recently been introduced to comply with data\nregulations and address the safety and ethical concerns of LLMs by removing the\nundesired data-model influence. However, state-of-the-art unlearning methods\nface a critical vulnerability: they are susceptible to ``relearning'' the\nremoved information from a small number of forget data points, known as\nrelearning attacks. In this paper, we systematically investigate how to make\nunlearned models robust against such attacks. For the first time, we establish\na connection between robust unlearning and sharpness-aware minimization (SAM)\nthrough a unified robust optimization framework, in an analogy to adversarial\ntraining designed to defend against adversarial attacks. Our analysis for SAM\nreveals that smoothness optimization plays a pivotal role in mitigating\nrelearning attacks. Thus, we further explore diverse smoothing strategies to\nenhance unlearning robustness. Extensive experiments on benchmark datasets,\nincluding WMDP and MUSE, demonstrate that SAM and other smoothness optimization\napproaches consistently improve the resistance of LLM unlearning to relearning\nattacks. Notably, smoothness-enhanced unlearning also helps defend against\n(input-level) jailbreaking attacks, broadening our proposal's impact in\nrobustifying LLM unlearning. Codes are available at\nhttps://github.com/OPTML-Group/Unlearn-Smooth.",
    "published_date": "2025-02-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.05374v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.05333v1",
    "title": "A Tutorial On Intersectionality in Fair Rankings",
    "authors": [
      "Chiara Criscuolo",
      "Davide Martinenghi",
      "Giuseppe Piccirillo"
    ],
    "author_ids": [],
    "abstract": "We address the critical issue of biased algorithms and unfair rankings, which\nhave permeated various sectors, including search engines, recommendation\nsystems, and workforce management. These biases can lead to discriminatory\noutcomes in a data-driven world, especially against marginalized and\nunderrepresented groups. Efforts towards responsible data science and\nresponsible artificial intelligence aim to mitigate these biases and promote\nfairness, diversity, and transparency. However, most fairness-aware ranking\nmethods singularly focus on protected attributes such as race, gender, or\nsocio-economic status, neglecting the intersectionality of these attributes,\ni.e., the interplay between multiple social identities. Understanding\nintersectionality is crucial to ensure that existing inequalities are not\npreserved by fair rankings. We offer a description of the main ways to\nincorporate intersectionality in fair ranking systems through practical\nexamples and provide a comparative overview of existing literature and a\nsynoptic table summarizing the various methodologies. Our analysis highlights\nthe need for intersectionality to attain fairness, while also emphasizing that\nfairness, alone, does not necessarily imply intersectionality.",
    "published_date": "2025-02-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.IR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.05333v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.05331v2",
    "title": "Fine-Tuned LLMs are \"Time Capsules\" for Tracking Societal Bias Through Books",
    "authors": [
      "Sangmitra Madhusudan",
      "Robert Morabito",
      "Skye Reid",
      "Nikta Gohari Sadr",
      "Ali Emami"
    ],
    "author_ids": [],
    "abstract": "Books, while often rich in cultural insights, can also mirror societal biases\nof their eras - biases that Large Language Models (LLMs) may learn and\nperpetuate during training. We introduce a novel method to trace and quantify\nthese biases using fine-tuned LLMs. We develop BookPAGE, a corpus comprising\n593 fictional books across seven decades (1950-2019), to track bias evolution.\nBy fine-tuning LLMs on books from each decade and using targeted prompts, we\nexamine shifts in biases related to gender, sexual orientation, race, and\nreligion. Our findings indicate that LLMs trained on decade-specific books\nmanifest biases reflective of their times, with both gradual trends and notable\nshifts. For example, model responses showed a progressive increase in the\nportrayal of women in leadership roles (from 8% to 22%) from the 1950s to\n2010s, with a significant uptick in the 1990s (from 4% to 12%), possibly\naligning with third-wave feminism. Same-sex relationship references increased\nmarkedly from the 1980s to 2000s (from 0% to 10%), mirroring growing LGBTQ+\nvisibility. Concerningly, negative portrayals of Islam rose sharply in the\n2000s (26% to 38%), likely reflecting post-9/11 sentiments. Importantly, we\ndemonstrate that these biases stem mainly from the books' content and not the\nmodels' architecture or initial training. Our study offers a new perspective on\nsocietal bias trends by bridging AI, literary studies, and social science\nresearch.",
    "published_date": "2025-02-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.05331v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.05286v1",
    "title": "Fairness and Sparsity within Rashomon sets: Enumeration-Free Exploration and Characterization",
    "authors": [
      "Lucas Langlade",
      "Julien Ferry",
      "Gabriel Laberge",
      "Thibaut Vidal"
    ],
    "author_ids": [],
    "abstract": "We introduce an enumeration-free method based on mathematical programming to\nprecisely characterize various properties such as fairness or sparsity within\nthe set of \"good models\", known as Rashomon set. This approach is generically\napplicable to any hypothesis class, provided that a mathematical formulation of\nthe model learning task exists. It offers a structured framework to define the\nnotion of business necessity and evaluate how fairness can be improved or\ndegraded towards a specific protected group, while remaining within the\nRashomon set and maintaining any desired sparsity level.\n  We apply our approach to two hypothesis classes: scoring systems and decision\ndiagrams, leveraging recent mathematical programming formulations for training\nsuch models. As seen in our experiments, the method comprehensively and\ncertifiably quantifies trade-offs between predictive performance, sparsity, and\nfairness. We observe that a wide range of fairness values are attainable,\nranging from highly favorable to significantly unfavorable for a protected\ngroup, while staying within less than 1% of the best possible training accuracy\nfor the hypothesis class. Additionally, we observe that sparsity constraints\nlimit these trade-offs and may disproportionately harm specific subgroups. As\nwe evidenced, thoroughly characterizing the tensions between these key aspects\nis critical for an informed and accountable selection of models.",
    "published_date": "2025-02-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.05286v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.05118v1",
    "title": "Use of Winsome Robots for Understanding Human Feedback (UWU)",
    "authors": [
      "Jessica Eggers",
      "Angela Dai",
      "Matthew C. Gombolay"
    ],
    "author_ids": [],
    "abstract": "As social robots become more common, many have adopted cute aesthetics aiming\nto enhance user comfort and acceptance. However, the effect of this aesthetic\nchoice on human feedback in reinforcement learning scenarios remains unclear.\nPrevious research has shown that humans tend to give more positive than\nnegative feedback, which can cause failure to reach optimal robot behavior. We\nhypothesize that this positive bias may be exacerbated by the robot's level of\nperceived cuteness. To investigate, we conducted a user study where\nparticipants critique a robot's trajectories while it performs a task. We then\nanalyzed the impact of the robot's aesthetic cuteness on the type of\nparticipant feedback. Our results suggest that there is a shift in the ratio of\npositive to negative feedback when perceived cuteness changes. In light of\nthis, we experiment with a stochastic version of TAMER which adapts based on\nthe user's level of positive feedback bias to mitigate these effects.",
    "published_date": "2025-02-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.RO",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.05118v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.05110v1",
    "title": "ApplE: An Applied Ethics Ontology with Event Context",
    "authors": [
      "Aisha Aijaz",
      "Raghava Mutharaju",
      "Manohar Kumar"
    ],
    "author_ids": [],
    "abstract": "Applied ethics is ubiquitous in most domains, requiring much deliberation due\nto its philosophical nature. Varying views often lead to conflicting courses of\naction where ethical dilemmas become challenging to resolve. Although many\nfactors contribute to such a decision, the major driving forces can be\ndiscretized and thus simplified to provide an indicative answer. Knowledge\nrepresentation and reasoning offer a way to explicitly translate abstract\nethical concepts into applicable principles within the context of an event. To\nachieve this, we propose ApplE, an Applied Ethics ontology that captures\nphilosophical theory and event context to holistically describe the morality of\nan action. The development process adheres to a modified version of the\nSimplified Agile Methodology for Ontology Development (SAMOD) and utilizes\nstandard design and publication practices. Using ApplE, we model a use case\nfrom the bioethics domain that demonstrates our ontology's social and\nscientific value. Apart from the ontological reasoning and quality checks,\nApplE is also evaluated using the three-fold testing process of SAMOD. ApplE\nfollows FAIR principles and aims to be a viable resource for applied ethicists\nand ontology engineers.",
    "published_date": "2025-02-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.05110v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.05099v1",
    "title": "Navigating Automated Hiring: Perceptions, Strategy Use, and Outcomes Among Young Job Seekers",
    "authors": [
      "Lena Armstrong",
      "Danaé Metaxa"
    ],
    "author_ids": [],
    "abstract": "As the use of automated employment decision tools (AEDTs) has rapidly\nincreased in hiring contexts, especially for computing jobs, there is still\nlimited work on applicants' perceptions of these emerging tools and their\nexperiences navigating them. To investigate, we conducted a survey with 448\ncomputer science students (young, current technology job-seekers) about\nperceptions of the procedural fairness of AEDTs, their willingness to be\nevaluated by different AEDTs, the strategies they use relating to automation in\nthe hiring process, and their job seeking success. We find that young job\nseekers' procedural fairness perceptions of and willingness to be evaluated by\nAEDTs varied with the level of automation involved in the AEDT, the technical\nnature of the task being evaluated, and their own use of strategies, such as\njob referrals. Examining the relationship of their strategies with job\noutcomes, notably, we find that referrals and family household income have\nsignificant and positive impacts on hiring success, while more egalitarian\nstrategies (using free online coding assessment practice or adding keywords to\nresumes) did not. Overall, our work speaks to young job seekers' distrust of\nautomation in hiring contexts, as well as the continued role of social and\nsocioeconomic privilege in job seeking, despite the use of AEDTs that promise\nto make hiring \"unbiased.\"",
    "published_date": "2025-02-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.05099v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.04918v1",
    "title": "Explainable and externally validated machine learning for neuropsychiatric diagnosis via electrocardiograms",
    "authors": [
      "Juan Miguel Lopez Alcaraz",
      "Ebenezer Oloyede",
      "David Taylor",
      "Wilhelm Haverkamp",
      "Nils Strodthoff"
    ],
    "author_ids": [],
    "abstract": "Electrocardiogram (ECG) analysis has emerged as a promising tool for\nidentifying physiological changes associated with neuropsychiatric conditions.\nThe relationship between cardiovascular health and neuropsychiatric disorders\nsuggests that ECG abnormalities could serve as valuable biomarkers for more\nefficient detection, therapy monitoring, and risk stratification. However, the\npotential of the ECG to accurately distinguish neuropsychiatric conditions,\nparticularly among diverse patient populations, remains underexplored. This\nstudy utilized ECG markers and basic demographic data to predict\nneuropsychiatric conditions using machine learning models, with targets defined\nthrough ICD-10 codes. Both internal and external validation were performed\nusing the MIMIC-IV and ECG-View datasets respectively. Performance was assessed\nusing AUROC scores. To enhance model interpretability, Shapley values were\napplied to provide insights into the contributions of individual ECG features\nto the predictions. Significant predictive performance was observed for\nconditions within the neurological and psychiatric groups. For the neurological\ngroup, Alzheimer's disease (G30) achieved an internal AUROC of 0.813\n(0.812-0.814) and an external AUROC of 0.868 (0.867-0.868). In the psychiatric\ngroup, unspecified dementia (F03) showed an internal AUROC of 0.849\n(0.848-0.849) and an external AUROC of 0.862 (0.861-0.863). Discriminative\nfeatures align with known ECG markers but also provide hints on potentially new\nmarkers. ECG offers significant promise for diagnosing and monitoring\nneuropsychiatric conditions, with robust predictive performance across internal\nand external cohorts. Future work should focus on addressing potential\nconfounders, such as therapy-related cardiotoxicity, and expanding the scope of\nECG applications, including personalized care and early intervention\nstrategies.",
    "published_date": "2025-02-07T00:00:00",
    "year": 2025,
    "categories": [
      "eess.SP",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.04918v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.04890v2",
    "title": "Exploit Gradient Skewness to Circumvent Byzantine Defenses for Federated Learning",
    "authors": [
      "Yuchen Liu",
      "Chen Chen",
      "Lingjuan Lyu",
      "Yaochu Jin",
      "Gang Chen"
    ],
    "author_ids": [],
    "abstract": "Federated Learning (FL) is notorious for its vulnerability to Byzantine\nattacks. Most current Byzantine defenses share a common inductive bias: among\nall the gradients, the densely distributed ones are more likely to be honest.\nHowever, such a bias is a poison to Byzantine robustness due to a newly\ndiscovered phenomenon in this paper - gradient skew. We discover that a group\nof densely distributed honest gradients skew away from the optimal gradient\n(the average of honest gradients) due to heterogeneous data. This gradient skew\nphenomenon allows Byzantine gradients to hide within the densely distributed\nskewed gradients. As a result, Byzantine defenses are confused into believing\nthat Byzantine gradients are honest. Motivated by this observation, we propose\na novel skew-aware attack called STRIKE: first, we search for the skewed\ngradients; then, we construct Byzantine gradients within the skewed gradients.\nExperiments on three benchmark datasets validate the effectiveness of our\nattack",
    "published_date": "2025-02-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.04890v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.05240v2",
    "title": "Survey on AI-Generated Media Detection: From Non-MLLM to MLLM",
    "authors": [
      "Yueying Zou",
      "Peipei Li",
      "Zekun Li",
      "Huaibo Huang",
      "Xing Cui",
      "Xuannan Liu",
      "Chenghanyu Zhang",
      "Ran He"
    ],
    "author_ids": [],
    "abstract": "The proliferation of AI-generated media poses significant challenges to\ninformation authenticity and social trust, making reliable detection methods\nhighly demanded. Methods for detecting AI-generated media have evolved rapidly,\nparalleling the advancement of Multimodal Large Language Models (MLLMs).\nCurrent detection approaches can be categorized into two main groups:\nNon-MLLM-based and MLLM-based methods. The former employs high-precision,\ndomain-specific detectors powered by deep learning techniques, while the latter\nutilizes general-purpose detectors based on MLLMs that integrate authenticity\nverification, explainability, and localization capabilities. Despite\nsignificant progress in this field, there remains a gap in literature regarding\na comprehensive survey that examines the transition from domain-specific to\ngeneral-purpose detection methods. This paper addresses this gap by providing a\nsystematic review of both approaches, analyzing them from single-modal and\nmulti-modal perspectives. We present a detailed comparative analysis of these\ncategories, examining their methodological similarities and differences.\nThrough this analysis, we explore potential hybrid approaches and identify key\nchallenges in forgery detection, providing direction for future research.\nAdditionally, as MLLMs become increasingly prevalent in detection tasks,\nethical and security considerations have emerged as critical global concerns.\nWe examine the regulatory landscape surrounding Generative AI (GenAI) across\nvarious jurisdictions, offering valuable insights for researchers and\npractitioners in this field.",
    "published_date": "2025-02-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.05240v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.04863v1",
    "title": "Enhancing Disinformation Detection with Explainable AI and Named Entity Replacement",
    "authors": [
      "Santiago González-Silot",
      "Andrés Montoro-Montarroso",
      "Eugenio Martínez Cámara",
      "Juan Gómez-Romero"
    ],
    "author_ids": [],
    "abstract": "The automatic detection of disinformation presents a significant challenge in\nthe field of natural language processing. This task addresses a multifaceted\nsocietal and communication issue, which needs approaches that extend beyond the\nidentification of general linguistic patterns through data-driven algorithms.\nIn this research work, we hypothesise that text classification methods are not\nable to capture the nuances of disinformation and they often ground their\ndecision in superfluous features. Hence, we apply a post-hoc explainability\nmethod (SHAP, SHapley Additive exPlanations) to identify spurious elements with\nhigh impact on the classification models. Our findings show that\nnon-informative elements (e.g., URLs and emoticons) should be removed and named\nentities (e.g., Rwanda) should be pseudo-anonymized before training to avoid\nmodels' bias and increase their generalization capabilities. We evaluate this\nmethodology with internal dataset and external dataset before and after\napplying extended data preprocessing and named entity replacement. The results\nshow that our proposal enhances on average the performance of a disinformation\nclassification method with external test data in 65.78% without a significant\ndecrease of the internal test performance.",
    "published_date": "2025-02-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.04863v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.04860v1",
    "title": "Where does AI come from? A global case study across Europe, Africa, and Latin America",
    "authors": [
      "Paola Tubaro",
      "Antonio A Casilli",
      "Maxime Cornet",
      "Clément Le Ludec",
      "Juana Torres Cierpe"
    ],
    "author_ids": [],
    "abstract": "This article examines the organisational and geographical forces that shape\nthe supply chains of artificial intelligence (AI) through outsourced and\noffshored data work. Bridging sociological theories of relational inequalities\nand embeddedness with critical approaches to Global Value Chains, we conduct a\nglobal case study of the digitally enabled organisation of data work in France,\nMadagascar, and Venezuela. The AI supply chains procure data work via a mix of\narm's length contracts through marketplace-like platforms, and of embedded\nfirm-like structures that offer greater stability but less flexibility, with\nmultiple intermediate arrangements. Each solution suits specific types and\npurposes of data work in AI preparation, verification, and impersonation. While\nall forms reproduce well-known patterns of exclusion that harm externalised\nworkers especially in the Global South, disadvantage manifests unevenly in\ndifferent supply chain structures, with repercussions on remunerations, job\nsecurity and working conditions. Unveiling these processes of contemporary\ntechnology development provides insights into possible policy implications.",
    "published_date": "2025-02-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.04860v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.04751v1",
    "title": "What is Ethical: AIHED Driving Humans or Human-Driven AIHED? A Conceptual Framework enabling the Ethos of AI-driven Higher education",
    "authors": [
      "Prashant Mahajan"
    ],
    "author_ids": [],
    "abstract": "The rapid integration of Artificial Intelligence (AI) in Higher Education\n(HE) is transforming personalized learning, administrative automation, and\ndecision-making. However, this progress presents a duality, as AI adoption also\nintroduces ethical and institutional challenges, including algorithmic bias,\ndata privacy risks, and governance inconsistencies. To address these concerns,\nthis study introduces the Human-Driven AI in Higher Education (HD-AIHED)\nFramework, ensuring compliance with UNESCO and OECD ethical standards. This\nconceptual research employs a qualitative meta-synthesis approach, integrating\nqualitative and quantitative studies to identify patterns, contradictions, and\ngaps in AI adoption within HE. It reinterprets existing datasets through\ntheoretical and ethical lenses to develop governance frameworks. The study\napplies a participatory integrated co-system, Phased Human Intelligence, SWOC\nanalysis, and AI ethical review boards to assess AI readiness and governance\nstrategies for universities and HE institutions. The HD-AIHED model bridges AI\nresearch gaps, addresses global real-time challenges, and provides tailored,\nscalable, and ethical strategies for diverse educational contexts. By\nemphasizing interdisciplinary collaboration among stakeholders, this study\nenvisions AIHED as a transparent and equitable force for innovation. The\nHD-AIHED framework ensures AI acts as a collaborative and ethical enabler\nrather than a disruptive replacement for human intelligence while advocating\nfor responsible AI implementation in HE.",
    "published_date": "2025-02-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.04751v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.04789v1",
    "title": "Probing Internal Representations of Multi-Word Verbs in Large Language Models",
    "authors": [
      "Hassane Kissane",
      "Achim Schilling",
      "Patrick Krauss"
    ],
    "author_ids": [],
    "abstract": "This study investigates the internal representations of verb-particle\ncombinations, called multi-word verbs, within transformer-based large language\nmodels (LLMs), specifically examining how these models capture lexical and\nsyntactic properties at different neural network layers. Using the BERT\narchitecture, we analyze the representations of its layers for two different\nverb-particle constructions: phrasal verbs like 'give up' and prepositional\nverbs like 'look at'. Our methodology includes training probing classifiers on\nthe internal representations to classify these categories at both word and\nsentence levels. The results indicate that the model's middle layers achieve\nthe highest classification accuracies. To further analyze the nature of these\ndistinctions, we conduct a data separability test using the Generalized\nDiscrimination Value (GDV). While GDV results show weak linear separability\nbetween the two verb types, probing classifiers still achieve high accuracy,\nsuggesting that representations of these linguistic categories may be\nnon-linearly separable. This aligns with previous research indicating that\nlinguistic distinctions in neural networks are not always encoded in a linearly\nseparable manner. These findings computationally support usage-based claims on\nthe representation of verb-particle constructions and highlight the complex\ninteraction between neural network architectures and linguistic structures.",
    "published_date": "2025-02-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.04789v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.07027v1",
    "title": "Representational Alignment with Chemical Induced Fit for Molecular Relational Learning",
    "authors": [
      "Peiliang Zhang",
      "Jingling Yuan",
      "Qing Xie",
      "Yongjun Zhu",
      "Lin Li"
    ],
    "author_ids": [],
    "abstract": "Molecular Relational Learning (MRL) is widely applied in natural sciences to\npredict relationships between molecular pairs by extracting structural\nfeatures. The representational similarity between substructure pairs determines\nthe functional compatibility of molecular binding sites. Nevertheless, aligning\nsubstructure representations by attention mechanisms lacks guidance from\nchemical knowledge, resulting in unstable model performance in chemical space\n(\\textit{e.g.}, functional group, scaffold) shifted data. With theoretical\njustification, we propose the \\textbf{Re}presentational \\textbf{Align}ment with\nChemical Induced \\textbf{Fit} (ReAlignFit) to enhance the stability of MRL.\nReAlignFit dynamically aligns substructure representation in MRL by introducing\nchemical Induced Fit-based inductive bias. In the induction process, we design\nthe Bias Correction Function based on substructure edge reconstruction to align\nrepresentations between substructure pairs by simulating chemical\nconformational changes (dynamic combination of substructures). ReAlignFit\nfurther integrates the Subgraph Information Bottleneck during fit process to\nrefine and optimize substructure pairs exhibiting high chemical functional\ncompatibility, leveraging them to generate molecular embeddings. Experimental\nresults on nine datasets demonstrate that ReAlignFit outperforms\nstate-of-the-art models in two tasks and significantly enhances model's\nstability in both rule-shifted and scaffold-shifted data distributions.",
    "published_date": "2025-02-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.07027v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.04664v1",
    "title": "Implicit Bias of SignGD and Adam on Multiclass Separable Data",
    "authors": [
      "Chen Fan",
      "Mark Schmidt",
      "Christos Thrampoulidis"
    ],
    "author_ids": [],
    "abstract": "In the optimization of overparameterized models, different gradient-based\nmethods can achieve zero training error yet converge to distinctly different\nsolutions inducing different generalization properties. While a decade of\nresearch on implicit optimization bias has illuminated this phenomenon in\nvarious settings, even the foundational case of linear classification with\nseparable data still has important open questions. We resolve a fundamental gap\nby characterizing the implicit bias of both Adam and Sign Gradient Descent in\nmulti-class cross-entropy minimization: we prove that their iterates converge\nto solutions that maximize the margin with respect to the classifier matrix's\nmax-norm and characterize the rate of convergence. We extend our results to\ngeneral p-norm normalized steepest descent algorithms and to other multi-class\nlosses.",
    "published_date": "2025-02-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.04664v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.04528v3",
    "title": "Group-Adaptive Threshold Optimization for Robust AI-Generated Text Detection",
    "authors": [
      "Minseok Jung",
      "Cynthia Fuertes Panizo",
      "Liam Dugan",
      "Yi R.",
      "Fung",
      "Pin-Yu Chen",
      "Paul Pu Liang"
    ],
    "author_ids": [],
    "abstract": "The advancement of large language models (LLMs) has made it difficult to\ndifferentiate human-written text from AI-generated text. Several AI-text\ndetectors have been developed in response, which typically utilize a fixed\nglobal threshold (e.g., {\\theta} = 0.5) to classify machine-generated text.\nHowever, we find that one universal threshold can fail to account for\nsubgroup-specific distributional variations. For example, when using a fixed\nthreshold, detectors make more false positive errors on shorter human-written\ntext than longer, and more positive classifications on neurotic writing styles\nthan open among long text. These discrepancies can lead to misclassification\nthat disproportionately affects certain groups. We address this critical\nlimitation by introducing FairOPT, an algorithm for group-specific threshold\noptimization in AI-generated content classifiers. Our approach partitions data\ninto subgroups based on attributes (e.g., text length and writing style) and\nlearns decision thresholds for each group, which enables careful balancing of\nperformance and fairness metrics within each subgroup. In experiments with four\nAI text classifiers on three datasets, FairOPT enhances overall F1 score and\ndecreases balanced error rate (BER) discrepancy across subgroups. Our framework\npaves the way for more robust and fair classification criteria in AI-generated\noutput detection.",
    "published_date": "2025-02-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.04528v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.04748v2",
    "title": "Large Language Models in Healthcare",
    "authors": [
      "Mohammed Al-Garadi",
      "Tushar Mungle",
      "Abdulaziz Ahmed",
      "Abeed Sarker",
      "Zhuqi Miao",
      "Michael E. Matheny"
    ],
    "author_ids": [],
    "abstract": "Large language models (LLMs) hold promise for transforming healthcare, from\nstreamlining administrative and clinical workflows to enriching patient\nengagement and advancing clinical decision-making. However, their successful\nintegration requires rigorous development, adaptation, and evaluation\nstrategies tailored to clinical needs. In this Review, we highlight recent\nadvancements, explore emerging opportunities for LLM-driven innovation, and\npropose a framework for their responsible implementation in healthcare\nsettings. We examine strategies for adapting LLMs to domain-specific healthcare\ntasks, such as fine-tuning, prompt engineering, and multimodal integration with\nelectronic health records. We also summarize various evaluation metrics\ntailored to healthcare, addressing clinical accuracy, fairness, robustness, and\npatient outcomes. Furthermore, we discuss the challenges associated with\ndeploying LLMs in healthcare--including data privacy, bias mitigation,\nregulatory compliance, and computational sustainability--and underscore the\nneed for interdisciplinary collaboration. Finally, these challenges present\npromising future research directions for advancing LLM implementation in\nclinical settings and healthcare.",
    "published_date": "2025-02-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.04748v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.04488v1",
    "title": "Building A Unified AI-centric Language System: analysis, framework and future work",
    "authors": [
      "Edward Hong Wang",
      "Cynthia Xin Wen"
    ],
    "author_ids": [],
    "abstract": "Recent advancements in large language models have demonstrated that extended\ninference through techniques can markedly improve performance, yet these gains\ncome with increased computational costs and the propagation of inherent biases\nfound in natural languages. This paper explores the design of a unified\nAI-centric language system that addresses these challenges by offering a more\nconcise, unambiguous, and computationally efficient alternative to traditional\nhuman languages. We analyze the limitations of natural language such as gender\nbias, morphological irregularities, and contextual ambiguities and examine how\nthese issues are exacerbated within current Transformer architectures, where\nredundant attention heads and token inefficiencies prevail. Drawing on insights\nfrom emergent artificial communication systems and constructed languages like\nEsperanto and Lojban, we propose a framework that translates diverse natural\nlanguage inputs into a streamlined AI-friendly language, enabling more\nefficient model training and inference while reducing memory footprints.\nFinally, we outline a pathway for empirical validation through controlled\nexperiments, paving the way for a universal interchange format that could\nrevolutionize AI-to-AI and human-to-AI interactions by enhancing clarity,\nfairness, and overall performance.",
    "published_date": "2025-02-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.04488v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.06842v1",
    "title": "Integrating Generative Artificial Intelligence in ADRD: A Framework for Streamlining Diagnosis and Care in Neurodegenerative Diseases",
    "authors": [
      "Andrew G. Breithaupt",
      "Alice Tang",
      "Bruce L. Miller",
      "Pedro Pinheiro-Chagas"
    ],
    "author_ids": [],
    "abstract": "Healthcare systems are struggling to meet the growing demand for neurological\ncare, with challenges particularly acute in Alzheimer's disease and related\ndementias (ADRD). While artificial intelligence research has often focused on\nidentifying patterns beyond human perception, implementing such predictive\ncapabilities remains challenging as clinicians cannot readily verify insights\nthey cannot themselves detect. We propose that large language models (LLMs)\noffer more immediately practical applications by enhancing clinicians'\ncapabilities in three critical areas: comprehensive data collection,\ninterpretation of complex clinical information, and timely application of\nrelevant medical knowledge. These challenges stem from limited time for proper\ndiagnosis, growing data complexity, and an overwhelming volume of medical\nliterature that exceeds any clinician's capacity to fully master. We present a\nframework for responsible AI integration that leverages LLMs' ability to\ncommunicate effectively with both patients and providers while maintaining\nhuman oversight. This approach prioritizes standardized, high-quality data\ncollection to enable a system that learns from every patient encounter while\nincorporating the latest clinical evidence, continuously improving care\ndelivery. We begin to address implementation challenges and initiate important\ndiscussions around ethical considerations and governance needs. While developed\nfor ADRD, this roadmap provides principles for responsible AI integration\nacross neurology and other medical specialties, with potential to improve\ndiagnostic accuracy, reduce care disparities, and advance clinical knowledge\nthrough a learning healthcare system.",
    "published_date": "2025-02-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "I.2.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.06842v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.04426v1",
    "title": "Decoding AI Judgment: How LLMs Assess News Credibility and Bias",
    "authors": [
      "Edoardo Loru",
      "Jacopo Nudo",
      "Niccolò Di Marco",
      "Matteo Cinelli",
      "Walter Quattrociocchi"
    ],
    "author_ids": [],
    "abstract": "Large Language Models (LLMs) are increasingly used to assess news\ncredibility, yet little is known about how they make these judgments. While\nprior research has examined political bias in LLM outputs or their potential\nfor automated fact-checking, their internal evaluation processes remain largely\nunexamined. Understanding how LLMs assess credibility provides insights into AI\nbehavior and how credibility is structured and applied in large-scale language\nmodels. This study benchmarks the reliability and political classifications of\nstate-of-the-art LLMs - Gemini 1.5 Flash (Google), GPT-4o mini (OpenAI), and\nLLaMA 3.1 (Meta) - against structured, expert-driven rating systems such as\nNewsGuard and Media Bias Fact Check. Beyond assessing classification\nperformance, we analyze the linguistic markers that shape LLM decisions,\nidentifying which words and concepts drive their evaluations. We uncover\npatterns in how LLMs associate credibility with specific linguistic features by\nexamining keyword frequency, contextual determinants, and rank distributions.\nBeyond static classification, we introduce a framework in which LLMs refine\ntheir credibility assessments by retrieving external information, querying\nother models, and adapting their responses. This allows us to investigate\nwhether their assessments reflect structured reasoning or rely primarily on\nprior learned associations.",
    "published_date": "2025-02-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.04426v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.04309v1",
    "title": "Targeted Learning for Data Fairness",
    "authors": [
      "Alexander Asemota",
      "Giles Hooker"
    ],
    "author_ids": [],
    "abstract": "Data and algorithms have the potential to produce and perpetuate\ndiscrimination and disparate treatment. As such, significant effort has been\ninvested in developing approaches to defining, detecting, and eliminating\nunfair outcomes in algorithms. In this paper, we focus on performing\nstatistical inference for fairness. Prior work in fairness inference has\nlargely focused on inferring the fairness properties of a given predictive\nalgorithm. Here, we expand fairness inference by evaluating fairness in the\ndata generating process itself, referred to here as data fairness. We perform\ninference on data fairness using targeted learning, a flexible framework for\nnonparametric inference. We derive estimators demographic parity, equal\nopportunity, and conditional mutual information. Additionally, we find that our\nestimators for probabilistic metrics exploit double robustness. To validate our\napproach, we perform several simulations and apply our estimators to real data.",
    "published_date": "2025-02-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.04309v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.04281v1",
    "title": "DECAF: Learning to be Fair in Multi-agent Resource Allocation",
    "authors": [
      "Ashwin Kumar",
      "William Yeoh"
    ],
    "author_ids": [],
    "abstract": "A wide variety of resource allocation problems operate under resource\nconstraints that are managed by a central arbitrator, with agents who evaluate\nand communicate preferences over these resources. We formulate this broad class\nof problems as Distributed Evaluation, Centralized Allocation (DECA) problems\nand propose methods to learn fair and efficient policies in centralized\nresource allocation. Our methods are applied to learning long-term fairness in\na novel and general framework for fairness in multi-agent systems. We show\nthree different methods based on Double Deep Q-Learning: (1) A joint weighted\noptimization of fairness and utility, (2) a split optimization, learning two\nseparate Q-estimators for utility and fairness, and (3) an online policy\nperturbation to guide existing black-box utility functions toward fair\nsolutions. Our methods outperform existing fair MARL approaches on multiple\nresource allocation domains, even when evaluated using diverse fairness\nfunctions, and allow for flexible online trade-offs between utility and\nfairness.",
    "published_date": "2025-02-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.04281v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.04259v1",
    "title": "Cognitive AI framework: advances in the simulation of human thought",
    "authors": [
      "Rommel Salas-Guerra"
    ],
    "author_ids": [],
    "abstract": "The Human Cognitive Simulation Framework represents a significant advancement\nin integrating human cognitive capabilities into artificial intelligence\nsystems. By merging short-term memory (conversation context), long-term memory\n(interaction context), advanced cognitive processing, and efficient knowledge\nmanagement, it ensures contextual coherence and persistent data storage,\nenhancing personalization and continuity in human-AI interactions. The\nframework employs a unified database that synchronizes these contexts while\nincorporating logical, creative, and analog processing modules inspired by\nhuman brain hemispheric functions to perform structured tasks and complex\ninferences. Dynamic knowledge updates enable real-time integration, improving\nadaptability and fostering applications in education, behavior analysis, and\nknowledge management. Despite its potential to process vast data volumes and\nenhance user experience, challenges remain in scalability, cognitive bias\nmitigation, and ethical compliance. This framework lays the foundation for\nfuture research in continuous learning algorithms, sustainability, and\nmultimodal adaptability, positioning Cognitive AI as a transformative model in\nemerging fields.",
    "published_date": "2025-02-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.04259v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.04218v1",
    "title": "Sports and Women's Sports: Gender Bias in Text Generation with Olympic Data",
    "authors": [
      "Laura Biester"
    ],
    "author_ids": [],
    "abstract": "Large Language Models (LLMs) have been shown to be biased in prior work, as\nthey generate text that is in line with stereotypical views of the world or\nthat is not representative of the viewpoints and values of historically\nmarginalized demographic groups. In this work, we propose using data from\nparallel men's and women's events at the Olympic Games to investigate different\nforms of gender bias in language models. We define three metrics to measure\nbias, and find that models are consistently biased against women when the\ngender is ambiguous in the prompt. In this case, the model frequently retrieves\nonly the results of the men's event with or without acknowledging them as such,\nrevealing pervasive gender bias in LLMs in the context of athletics.",
    "published_date": "2025-02-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.04218v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.04419v2",
    "title": "Understanding and Mitigating the Bias Inheritance in LLM-based Data Augmentation on Downstream Tasks",
    "authors": [
      "Miaomiao Li",
      "Hao Chen",
      "Yang Wang",
      "Tingyuan Zhu",
      "Weijia Zhang",
      "Kaijie Zhu",
      "Kam-Fai Wong",
      "Jindong Wang"
    ],
    "author_ids": [],
    "abstract": "Generating synthetic datasets via large language models (LLMs) themselves has\nemerged as a promising approach to improve LLM performance. However, LLMs\ninherently reflect biases present in their training data, leading to a critical\nchallenge: when these models generate synthetic data for training, they may\npropagate and amplify their inherent biases that can significantly impact model\nfairness and robustness on downstream tasks--a phenomenon we term bias\ninheritance. This work presents the first systematic investigation in\nunderstanding, analyzing, and mitigating bias inheritance. We study this\nproblem by fine-tuning LLMs with a combined dataset consisting of original and\nLLM-augmented data, where bias ratio represents the proportion of augmented\ndata. Through systematic experiments across 10 classification and generation\ntasks, we analyze how 6 different types of biases manifest at varying bias\nratios. Our results reveal that bias inheritance has nuanced effects on\ndownstream tasks, influencing both classification tasks and generation tasks\ndifferently. Then, our analysis identifies three key misalignment factors:\nmisalignment of values, group data, and data distributions. Based on these\ninsights, we propose three mitigation strategies: token-based, mask-based, and\nloss-based approaches. Experiments demonstrate that these strategies also work\ndifferently on various tasks and bias, indicating the substantial challenges to\nfully mitigate bias inheritance. We hope this work can provide valuable\ninsights to the research of LLM data augmentation.",
    "published_date": "2025-02-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.04419v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.04037v1",
    "title": "Exploring Imbalanced Annotations for Effective In-Context Learning",
    "authors": [
      "Hongfu Gao",
      "Feipeng Zhang",
      "Hao Zeng",
      "Deyu Meng",
      "Bingyi Jing",
      "Hongxin Wei"
    ],
    "author_ids": [],
    "abstract": "Large language models (LLMs) have shown impressive performance on downstream\ntasks through in-context learning (ICL), which heavily relies on the\ndemonstrations selected from annotated datasets. Existing selection methods may\nhinge on the distribution of annotated datasets, which can often be long-tailed\nin real-world scenarios. In this work, we show that imbalanced class\ndistributions in annotated datasets significantly degrade the performance of\nICL across various tasks and selection methods. Moreover, traditional rebalance\nmethods fail to ameliorate the issue of class imbalance in ICL. Our method is\nmotivated by decomposing the distributional differences between annotated and\ntest datasets into two-component weights: class-wise weights and conditional\nbias. The key idea behind our method is to estimate the conditional bias by\nminimizing the empirical error on a balanced validation dataset and to employ\nthe two-component weights to modify the original scoring functions during\nselection. Our approach can prevent selecting too many demonstrations from a\nsingle class while preserving the effectiveness of the original selection\nmethods. Extensive experiments demonstrate the effectiveness of our method,\nimproving the average accuracy by up to 5.46 on common benchmarks with\nimbalanced datasets.",
    "published_date": "2025-02-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.04037v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.04029v1",
    "title": "Echo-Teddy: Preliminary Design and Development of Large Language Model-based Social Robot for Autistic Students",
    "authors": [
      "Unggi Lee",
      "Hansung Kim",
      "Juhong Eom",
      "Hyeonseo Jeong",
      "Seungyeon Lee",
      "Gyuri Byun",
      "Yunseo Lee",
      "Minji Kang",
      "Gospel Kim",
      "Jihoi Na",
      "Jewoong Moon",
      "Hyeoncheol Kim"
    ],
    "author_ids": [],
    "abstract": "Autistic students often face challenges in social interaction, which can\nhinder their educational and personal development. This study introduces\nEcho-Teddy, a Large Language Model (LLM)-based social robot designed to support\nautistic students in developing social and communication skills. Unlike\nprevious chatbot-based solutions, Echo-Teddy leverages advanced LLM\ncapabilities to provide more natural and adaptive interactions. The research\naddresses two key questions: (1) What are the design principles and initial\nprototype characteristics of an effective LLM-based social robot for autistic\nstudents? (2) What improvements can be made based on developer\nreflection-on-action and expert interviews? The study employed a mixed-methods\napproach, combining prototype development with qualitative analysis of\ndeveloper reflections and expert interviews. Key design principles identified\ninclude customizability, ethical considerations, and age-appropriate\ninteractions. The initial prototype, built on a Raspberry Pi platform, features\ncustom speech components and basic motor functions. Evaluation of the prototype\nrevealed potential improvements in areas such as user interface, educational\nvalue, and practical implementation in educational settings. This research\ncontributes to the growing field of AI-assisted special education by\ndemonstrating the potential of LLM-based social robots in supporting autistic\nstudents. The findings provide valuable insights for future developments in\naccessible and effective social support tools for special education.",
    "published_date": "2025-02-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.04029v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.04011v1",
    "title": "Debiasing Architectural Decision-Making: An Experiment With Students and Practitioners",
    "authors": [
      "Klara Borowa",
      "Rodrigo Rebouças de Almeida",
      "Marion Wiese"
    ],
    "author_ids": [],
    "abstract": "Cognitive biases are predictable, systematic errors in human reasoning. They\ninfluence decision-making in various areas, including architectural\ndecision-making, where architects face many choices. For example, anchoring can\ncause architects to unconsciously prefer the first architectural solution that\nthey came up with, without considering any solution alternatives. Prior\nresearch suggests that training individuals in debiasing techniques during a\npractical workshop can help reduce the impact of biases. The goal of this study\nwas to design and evaluate a debiasing workshop with individuals at various\nstages of their professional careers. To test the workshop's effectiveness, we\nperformed an experiment with 16 students and 20 practitioners, split into\ncontrol and workshop group pairs. We recorded and analyzed their think-aloud\ndiscussions about improving the architectures of systems they collaborated on.\nThe workshop improved the participants' argumentation when discussing\narchitectural decisions and increased the use of debiasing techniques taught\nduring the workshop. This led to the successful reduction of the researched\nbiases' occurrences. In particular, anchoring and optimism bias occurrences\ndecreased significantly. We also found that practitioners were more susceptible\nto cognitive biases than students, so the workshop had a more substantial\nimpact on practitioners. We assume that the practitioners' attachment to their\nsystems may be the cause of their susceptibility to biases. Finally, we\nidentified factors that may reduce the effectiveness of the debiasing workshop.\nOn that basis, we prepared a set of teaching suggestions for educators.\nOverall, we recommend using this workshop to educate both students and\nexperienced practitioners about the typical harmful influences of cognitive\nbias on architectural decisions and how to avoid them.",
    "published_date": "2025-02-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.04011v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.03988v1",
    "title": "Tight Bounds on Jensen's Gap: Novel Approach with Applications in Generative Modeling",
    "authors": [
      "Marcin Mazur",
      "Piotr Kościelniak",
      "Łukasz Struski"
    ],
    "author_ids": [],
    "abstract": "Among various mathematical tools of particular interest are those that\nprovide a common basis for researchers in different scientific fields. One of\nthem is Jensen's inequality, which states that the expectation of a convex\nfunction is greater than or equal to the function evaluated at the expectation.\nThe resulting difference, known as Jensen's gap, became the subject of\ninvestigation by both the statistical and machine learning communities. Among\nmany related topics, finding lower and upper bounds on Jensen's gap (under\ndifferent assumptions on the underlying function and distribution) has recently\nbecome a problem of particular interest. In our paper, we take another step in\nthis direction by providing a novel general and mathematically rigorous\ntechnique, motivated by the recent results of Struski et al. (2023). In\naddition, by studying in detail the case of the logarithmic function and the\nlog-normal distribution, we explore a method for tightly estimating the\nlog-likelihood of generative models trained on real-world datasets.\nFurthermore, we present both analytical and experimental arguments in support\nof the superiority of our approach in comparison to existing state-of-the-art\nsolutions, contingent upon fulfillment of the criteria set forth by theoretical\nstudies and corresponding experiments on synthetic data.",
    "published_date": "2025-02-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.03988v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.03966v3",
    "title": "MultiFloodSynth: Multi-Annotated Flood Synthetic Dataset Generation",
    "authors": [
      "YoonJe Kang",
      "Yonghoon Jung",
      "Wonseop Shin",
      "Bumsoo Kim",
      "Sanghyun Seo"
    ],
    "author_ids": [],
    "abstract": "In this paper, we present synthetic data generation framework for flood\nhazard detection system. For high fidelity and quality, we characterize several\nreal-world properties into virtual world and simulate the flood situation by\ncontrolling them. For the sake of efficiency, recent generative models in\nimage-to-3D and urban city synthesis are leveraged to easily composite flood\nenvironments so that we avoid data bias due to the hand-crafted manner. Based\non our framework, we build the flood synthetic dataset with 5 levels, dubbed\nMultiFloodSynth which contains rich annotation types like normal map,\nsegmentation, 3D bounding box for a variety of downstream task. In experiments,\nour dataset demonstrate the enhanced performance of flood hazard detection with\non-par realism compared with real dataset.",
    "published_date": "2025-02-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.03966v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.03953v1",
    "title": "Fairness Aware Reinforcement Learning via Proximal Policy Optimization",
    "authors": [
      "Gabriele La Malfa",
      "Jie M. Zhang",
      "Michael Luck",
      "Elizabeth Black"
    ],
    "author_ids": [],
    "abstract": "Fairness in multi-agent systems (MAS) focuses on equitable reward\ndistribution among agents in scenarios involving sensitive attributes such as\nrace, gender, or socioeconomic status. This paper introduces fairness in\nProximal Policy Optimization (PPO) with a penalty term derived from demographic\nparity, counterfactual fairness, and conditional statistical parity. The\nproposed method balances reward maximisation with fairness by integrating two\npenalty components: a retrospective component that minimises disparities in\npast outcomes and a prospective component that ensures fairness in future\ndecision-making. We evaluate our approach in the Allelopathic Harvest game, a\ncooperative and competitive MAS focused on resource collection, where some\nagents possess a sensitive attribute. Experiments demonstrate that fair-PPO\nachieves fairer policies across all fairness metrics than classic PPO. Fairness\ncomes at the cost of reduced rewards, namely the Price of Fairness, although\nagents with and without the sensitive attribute renounce comparable amounts of\nrewards. Additionally, the retrospective and prospective penalties effectively\nchange the agents' behaviour and improve fairness. These findings underscore\nthe potential of fair-PPO to address fairness challenges in MAS.",
    "published_date": "2025-02-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.MA",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.03953v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.03852v1",
    "title": "Pursuing Better Decision Boundaries for Long-Tailed Object Detection via Category Information Amount",
    "authors": [
      "Yanbiao Ma",
      "Wei Dai",
      "Jiayi Chen"
    ],
    "author_ids": [],
    "abstract": "In object detection, the instance count is typically used to define whether a\ndataset exhibits a long-tail distribution, implicitly assuming that models will\nunderperform on categories with fewer instances. This assumption has led to\nextensive research on category bias in datasets with imbalanced instance\ncounts. However, models still exhibit category bias even in datasets where\ninstance counts are relatively balanced, clearly indicating that instance count\nalone cannot explain this phenomenon. In this work, we first introduce the\nconcept and measurement of category information amount. We observe a\nsignificant negative correlation between category information amount and\naccuracy, suggesting that category information amount more accurately reflects\nthe learning difficulty of a category. Based on this observation, we propose\nInformation Amount-Guided Angular Margin (IGAM) Loss. The core idea of IGAM is\nto dynamically adjust the decision space of each category based on its\ninformation amount, thereby reducing category bias in long-tail datasets. IGAM\nLoss not only performs well on long-tailed benchmark datasets such as LVIS v1.0\nand COCO-LT but also shows significant improvement for underrepresented\ncategories in the non-long-tailed dataset Pascal VOC. Comprehensive experiments\ndemonstrate the potential of category information amount as a tool and the\ngenerality of our proposed method.",
    "published_date": "2025-02-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.03852v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.03826v1",
    "title": "FairT2I: Mitigating Social Bias in Text-to-Image Generation via Large Language Model-Assisted Detection and Attribute Rebalancing",
    "authors": [
      "Jinya Sakurai",
      "Issei Sato"
    ],
    "author_ids": [],
    "abstract": "The proliferation of Text-to-Image (T2I) models has revolutionized content\ncreation, providing powerful tools for diverse applications ranging from\nartistic expression to educational material development and marketing. Despite\nthese technological advancements, significant ethical concerns arise from these\nmodels' reliance on large-scale datasets that often contain inherent societal\nbiases. These biases are further amplified when AI-generated content is\nincluded in training data, potentially reinforcing and perpetuating stereotypes\nin the generated outputs. In this paper, we introduce FairT2I, a novel\nframework that harnesses large language models to detect and mitigate social\nbiases in T2I generation. Our framework comprises two key components: (1) an\nLLM-based bias detection module that identifies potential social biases in\ngenerated images based on text prompts, and (2) an attribute rebalancing module\nthat fine-tunes sensitive attributes within the T2I model to mitigate\nidentified biases. Our extensive experiments across various T2I models and\ndatasets show that FairT2I can significantly reduce bias while maintaining\nhigh-quality image generation. We conducted both qualitative user studies and\nquantitative non-parametric analyses in the generated image feature space,\nbuilding upon the occupational dataset introduced in the Stable Bias study. Our\nresults show that FairT2I successfully mitigates social biases and enhances the\ndiversity of sensitive attributes in generated images. We further demonstrate,\nusing the P2 dataset, that our framework can detect subtle biases that are\nchallenging for human observers to perceive, extending beyond\noccupation-related prompts. On the basis of these findings, we introduce a new\nbenchmark dataset for evaluating bias in T2I models.",
    "published_date": "2025-02-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.03826v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.03789v2",
    "title": "Exact Maximin Share Fairness via Adjusted Supply",
    "authors": [
      "Siddharth Barman",
      "Satyanand Rammohan",
      "Aditi Sethia"
    ],
    "author_ids": [],
    "abstract": "This work addresses fair allocation of indivisible items in settings wherein\nit is feasible to create copies of resources or dispose of tasks. We establish\nthat exact maximin share (MMS) fairness can be achieved via limited duplication\nof goods even under monotone valuations. We also show that, when allocating\nchores under monotone costs, MMS fairness is always feasible with limited\ndisposal of chores. Since monotone valuations do not admit any nontrivial\napproximation guarantees for MMS, our results highlight that such barriers can\nbe circumvented by post facto adjustments in the supply of the items.\n  We prove that, for division of $m$ goods among $n$ agents with monotone\nvaluations, there always exists an assignment of subsets of goods to the agents\nsuch that they receive at least their maximin shares and no single good is\nallocated to more than $3 \\log m$ agents. In addition, the sum of the sizes of\nthe assigned subsets does not exceed $m$. For identically ordered valuations,\nwe obtain an upper bound of $O(\\sqrt{\\log m})$ on the maximum assignment\nmultiplicity across goods and an $m + \\widetilde{O}\\left(\\frac{m}{\\sqrt{n}}\n\\right)$ bound for the total number of goods assigned. Further, for additive\nvaluations, we prove that there always exists an MMS assignment in which no\nsingle good is allocated to more than $2$ agents and the total number of goods\nassigned is at most $2m$.\n  For chores, we upper bound the number of chores that need to be discarded for\nensuring MMS fairness. We prove that, under monotone costs, there exists an MMS\nassignment in which at most $\\frac{m}{e}$ remain unassigned. For identically\nordered costs, we establish that MMS fairness can be achieved while keeping at\nmost $\\widetilde{O} \\left(\\frac{m}{n^{1/4}} \\right)$ chores unassigned. We also\nprove that the obtained bounds for monotone valuations and monotone costs are\nessentially tight.",
    "published_date": "2025-02-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.03789v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.04747v1",
    "title": "E-LENS: User Requirements-Oriented AI Ethics Assurance",
    "authors": [
      "Jianlong Zhou",
      "Fang Chen"
    ],
    "author_ids": [],
    "abstract": "Despite the much proliferation of AI ethical principles in recent years,\nthere is a challenge of assuring AI ethics with current AI ethics frameworks in\nreal-world applications. While system safety has emerged as a distinct\ndiscipline for a long time, originated from safety concerns in early aircraft\nmanufacturing. The safety assurance is now an indispensable component in safety\ncritical domains. Motivated by the assurance approaches for safety-critical\nsystems such as aviation, this paper introduces the concept of AI ethics\nassurance cases into the AI ethics assurance. Three pillars of user\nrequirements, evidence, and validation are proposed as key components and\nintegrated into AI ethics assurance cases for a new approach of user\nrequirements-oriented AI ethics assurance. The user requirements-oriented AI\nethics assurance case is set up based on three pillars and hazard analysis\nmethods used in the safety assurance of safety-critical systems. This paper\nalso proposes a platform named Ethical-Lens (E-LENS) to implement the user\nrequirements-oriented AI ethics assurance approach. The proposed user\nrequirements-based E-LENS platform is then applied to assure AI ethics of an\nAI-driven human resource shortlisting system as a case study to show the\neffectiveness of the proposed approach.",
    "published_date": "2025-02-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.04747v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.04394v1",
    "title": "DECT: Harnessing LLM-assisted Fine-Grained Linguistic Knowledge and Label-Switched and Label-Preserved Data Generation for Diagnosis of Alzheimer's Disease",
    "authors": [
      "Tingyu Mo",
      "Jacqueline C. K. Lam",
      "Victor O. K. Li",
      "Lawrence Y. L. Cheung"
    ],
    "author_ids": [],
    "abstract": "Alzheimer's Disease (AD) is an irreversible neurodegenerative disease\naffecting 50 million people worldwide. Low-cost, accurate identification of key\nmarkers of AD is crucial for timely diagnosis and intervention. Language\nimpairment is one of the earliest signs of cognitive decline, which can be used\nto discriminate AD patients from normal control individuals.\nPatient-interviewer dialogues may be used to detect such impairments, but they\nare often mixed with ambiguous, noisy, and irrelevant information, making the\nAD detection task difficult. Moreover, the limited availability of AD speech\nsamples and variability in their speech styles pose significant challenges in\ndeveloping robust speech-based AD detection models. To address these\nchallenges, we propose DECT, a novel speech-based domain-specific approach\nleveraging large language models (LLMs) for fine-grained linguistic analysis\nand label-switched label-preserved data generation. Our study presents four\nnovelties: We harness the summarizing capabilities of LLMs to identify and\ndistill key Cognitive-Linguistic information from noisy speech transcripts,\neffectively filtering irrelevant information. We leverage the inherent\nlinguistic knowledge of LLMs to extract linguistic markers from unstructured\nand heterogeneous audio transcripts. We exploit the compositional ability of\nLLMs to generate AD speech transcripts consisting of diverse linguistic\npatterns to overcome the speech data scarcity challenge and enhance the\nrobustness of AD detection models. We use the augmented AD textual speech\ntranscript dataset and a more fine-grained representation of AD textual speech\ntranscript data to fine-tune the AD detection model. The results have shown\nthat DECT demonstrates superior model performance with an 11% improvement in AD\ndetection accuracy on the datasets from DementiaBank compared to the baselines.",
    "published_date": "2025-02-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.04394v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.03737v1",
    "title": "Mitigating the Participation Bias by Balancing Extreme Ratings",
    "authors": [
      "Yongkang Guo",
      "Yuqing Kong",
      "Jialiang Liu"
    ],
    "author_ids": [],
    "abstract": "Rating aggregation plays a crucial role in various fields, such as product\nrecommendations, hotel rankings, and teaching evaluations. However, traditional\naveraging methods can be affected by participation bias, where some raters do\nnot participate in the rating process, leading to potential distortions. In\nthis paper, we consider a robust rating aggregation task under the\nparticipation bias. We assume that raters may not reveal their ratings with a\ncertain probability depending on their individual ratings, resulting in\npartially observed samples. Our goal is to minimize the expected squared loss\nbetween the aggregated ratings and the average of all underlying ratings\n(possibly unobserved) in the worst-case scenario.\n  We focus on two settings based on whether the sample size (i.e. the number of\nraters) is known. In the first setting, where the sample size is known, we\npropose an aggregator, named as the Balanced Extremes Aggregator. It estimates\nunrevealed ratings with a balanced combination of extreme ratings. When the\nsample size is unknown, we derive another aggregator, the Polarizing-Averaging\nAggregator, which becomes optimal as the sample size grows to infinity.\nNumerical results demonstrate the superiority of our proposed aggregators in\nmitigating participation bias, compared to simple averaging and the spectral\nmethod. Furthermore, we validate the effectiveness of our aggregators on a\nreal-world dataset.",
    "published_date": "2025-02-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.03737v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.03699v1",
    "title": "LLM Alignment as Retriever Optimization: An Information Retrieval Perspective",
    "authors": [
      "Bowen Jin",
      "Jinsung Yoon",
      "Zhen Qin",
      "Ziqi Wang",
      "Wei Xiong",
      "Yu Meng",
      "Jiawei Han",
      "Sercan O. Arik"
    ],
    "author_ids": [],
    "abstract": "Large Language Models (LLMs) have revolutionized artificial intelligence with\ncapabilities in reasoning, coding, and communication, driving innovation across\nindustries. Their true potential depends on effective alignment to ensure\ncorrect, trustworthy and ethical behavior, addressing challenges like\nmisinformation, hallucinations, bias and misuse. While existing Reinforcement\nLearning (RL)-based alignment methods are notoriously complex, direct\noptimization approaches offer a simpler alternative. In this work, we introduce\na novel direct optimization approach for LLM alignment by drawing on\nestablished Information Retrieval (IR) principles. We present a systematic\nframework that bridges LLM alignment and IR methodologies, mapping LLM\ngeneration and reward models to IR's retriever-reranker paradigm. Building on\nthis foundation, we propose LLM Alignment as Retriever Preference Optimization\n(LarPO), a new alignment method that enhances overall alignment quality.\nExtensive experiments validate LarPO's effectiveness with 38.9 % and 13.7 %\naveraged improvement on AlpacaEval2 and MixEval-Hard respectively. Our work\nopens new avenues for advancing LLM alignment by integrating IR foundations,\noffering a promising direction for future research.",
    "published_date": "2025-02-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.03699v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.04391v1",
    "title": "Towards Fair and Robust Face Parsing for Generative AI: A Multi-Objective Approach",
    "authors": [
      "Sophia J. Abraham",
      "Jonathan D. Hauenstein",
      "Walter J. Scheirer"
    ],
    "author_ids": [],
    "abstract": "Face parsing is a fundamental task in computer vision, enabling applications\nsuch as identity verification, facial editing, and controllable image\nsynthesis. However, existing face parsing models often lack fairness and\nrobustness, leading to biased segmentation across demographic groups and errors\nunder occlusions, noise, and domain shifts. These limitations affect downstream\nface synthesis, where segmentation biases can degrade generative model outputs.\nWe propose a multi-objective learning framework that optimizes accuracy,\nfairness, and robustness in face parsing. Our approach introduces a\nhomotopy-based loss function that dynamically adjusts the importance of these\nobjectives during training. To evaluate its impact, we compare multi-objective\nand single-objective U-Net models in a GAN-based face synthesis pipeline\n(Pix2PixHD). Our results show that fairness-aware and robust segmentation\nimproves photorealism and consistency in face generation. Additionally, we\nconduct preliminary experiments using ControlNet, a structured conditioning\nmodel for diffusion-based synthesis, to explore how segmentation quality\ninfluences guided image generation. Our findings demonstrate that\nmulti-objective face parsing improves demographic consistency and robustness,\nleading to higher-quality GAN-based synthesis.",
    "published_date": "2025-02-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.04391v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.03645v1",
    "title": "MNE: overparametrized neural evolution with applications to diffusion processes and sampling",
    "authors": [
      "Michael Lindsey"
    ],
    "author_ids": [],
    "abstract": "We propose a framework for solving evolution equations within parametric\nfunction classes, especially ones that are specified by neural networks. We\ncall this framework the minimal neural evolution (MNE) because it is motivated\nby the goal of seeking the smallest instantaneous change in the neural network\nparameters that is compatible with exact solution of the evolution equation at\na set of evolving collocation points. Formally, the MNE is quite similar to the\nrecently introduced Neural Galerkin framework, but a difference in perspective\nmotivates an alternative sketching procedure that effectively reduces the\nlinear systems solved within the integrator to a size that is interpretable as\nan effective rank of the evolving neural tangent kernel, while maintaining a\nsmooth evolution equation for the neural network parameters. We focus\nspecifically on the application of this framework to diffusion processes, where\nthe score function allows us to define intuitive dynamics for the collocation\npoints. These can in turn be propagated jointly with the neural network\nparameters using a high-order adaptive integrator. In particular, we\ndemonstrate how the Ornstein-Uhlenbeck diffusion process can be used for the\ntask of sampling from a probability distribution given a formula for the\ndensity but no training data. This framework extends naturally to allow for\nconditional sampling and marginalization, and we show how to systematically\nremove the sampling bias due to parametric approximation error. We validate the\nefficiency, systematic improvability, and scalability of our approach on\nillustrative examples in low and high spatial dimensions.",
    "published_date": "2025-02-05T00:00:00",
    "year": 2025,
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.03645v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.04386v1",
    "title": "Towards Fair Medical AI: Adversarial Debiasing of 3D CT Foundation Embeddings",
    "authors": [
      "Guangyao Zheng",
      "Michael A. Jacobs",
      "Vladimir Braverman",
      "Vishwa S. Parekh"
    ],
    "author_ids": [],
    "abstract": "Self-supervised learning has revolutionized medical imaging by enabling\nefficient and generalizable feature extraction from large-scale unlabeled\ndatasets. Recently, self-supervised foundation models have been extended to\nthree-dimensional (3D) computed tomography (CT) data, generating compact,\ninformation-rich embeddings with 1408 features that achieve state-of-the-art\nperformance on downstream tasks such as intracranial hemorrhage detection and\nlung cancer risk forecasting. However, these embeddings have been shown to\nencode demographic information, such as age, sex, and race, which poses a\nsignificant risk to the fairness of clinical applications.\n  In this work, we propose a Variation Autoencoder (VAE) based adversarial\ndebiasing framework to transform these embeddings into a new latent space where\ndemographic information is no longer encoded, while maintaining the performance\nof critical downstream tasks. We validated our approach on the NLST lung cancer\nscreening dataset, demonstrating that the debiased embeddings effectively\neliminate multiple encoded demographic information and improve fairness without\ncompromising predictive accuracy for lung cancer risk at 1-year and 2-year\nintervals. Additionally, our approach ensures the embeddings are robust against\nadversarial bias attacks. These results highlight the potential of adversarial\ndebiasing techniques to ensure fairness and equity in clinical applications of\nself-supervised 3D CT embeddings, paving the way for their broader adoption in\nunbiased medical decision-making.",
    "published_date": "2025-02-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.04386v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.03576v1",
    "title": "Clone-Resistant Weights in Metric Spaces: A Framework for Handling Redundancy Bias",
    "authors": [
      "Damien Berriaud",
      "Roger Wattenhofer"
    ],
    "author_ids": [],
    "abstract": "We are given a set of elements in a metric space. The distribution of the\nelements is arbitrary, possibly adversarial. Can we weigh the elements in a way\nthat is resistant to such (adversarial) manipulations? This problem arises in\nvarious contexts. For instance, the elements could represent data points,\nrequiring robust domain adaptation. Alternatively, they might represent tasks\nto be aggregated into a benchmark; or questions about personal political\nopinions in voting advice applications. This article introduces a theoretical\nframework for dealing with such problems. We propose clone-proof representation\nfunctions as a solution concept. These functions distribute importance across\nelements of a set such that similar objects (``clones'') share (some of) their\nweights, thus avoiding a potential bias introduced by their multiplicity. Our\nframework extends the maximum uncertainty principle to accommodate general\nmetric spaces and includes a set of axioms - symmetry, continuity, and\nclone-proofness - that guide the construction of representation functions.\nFinally, we address the existence of representation functions satisfying our\naxioms in the significant case of Euclidean spaces and propose a general method\nfor their construction.",
    "published_date": "2025-02-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.03576v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.03429v1",
    "title": "On Fairness of Unified Multimodal Large Language Model for Image Generation",
    "authors": [
      "Ming Liu",
      "Hao Chen",
      "Jindong Wang",
      "Liwen Wang",
      "Bhiksha Raj Ramakrishnan",
      "Wensheng Zhang"
    ],
    "author_ids": [],
    "abstract": "Unified multimodal large language models (U-MLLMs) have demonstrated\nimpressive performance in visual understanding and generation in an end-to-end\npipeline. Compared with generation-only models (e.g., Stable Diffusion),\nU-MLLMs may raise new questions about bias in their outputs, which can be\naffected by their unified capabilities. This gap is particularly concerning\ngiven the under-explored risk of propagating harmful stereotypes. In this\npaper, we benchmark the latest U-MLLMs and find that most exhibit significant\ndemographic biases, such as gender and race bias. To better understand and\nmitigate this issue, we propose a locate-then-fix strategy, where we audit and\nshow how the individual model component is affected by bias. Our analysis shows\nthat bias originates primarily from the language model. More interestingly, we\nobserve a \"partial alignment\" phenomenon in U-MLLMs, where understanding bias\nappears minimal, but generation bias remains substantial. Thus, we propose a\nnovel balanced preference model to balance the demographic distribution with\nsynthetic data. Experiments demonstrate that our approach reduces demographic\nbias while preserving semantic fidelity. We hope our findings underscore the\nneed for more holistic interpretation and debiasing strategies of U-MLLMs in\nthe future.",
    "published_date": "2025-02-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.03429v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.03386v1",
    "title": "A Structured Reasoning Framework for Unbalanced Data Classification Using Probabilistic Models",
    "authors": [
      "Junliang Du",
      "Shiyu Dou",
      "Bohuan Yang",
      "Jiacheng Hu",
      "Tai An"
    ],
    "author_ids": [],
    "abstract": "This paper studies a Markov network model for unbalanced data, aiming to\nsolve the problems of classification bias and insufficient minority class\nrecognition ability of traditional machine learning models in environments with\nuneven class distribution. By constructing joint probability distribution and\nconditional dependency, the model can achieve global modeling and reasoning\noptimization of sample categories. The study introduced marginal probability\nestimation and weighted loss optimization strategies, combined with\nregularization constraints and structured reasoning methods, effectively\nimproving the generalization ability and robustness of the model. In the\nexperimental stage, a real credit card fraud detection dataset was selected and\ncompared with models such as logistic regression, support vector machine,\nrandom forest and XGBoost. The experimental results show that the Markov\nnetwork performs well in indicators such as weighted accuracy, F1 score, and\nAUC-ROC, significantly outperforming traditional classification models,\ndemonstrating its strong decision-making ability and applicability in\nunbalanced data scenarios. Future research can focus on efficient model\ntraining, structural optimization, and deep learning integration in large-scale\nunbalanced data environments and promote its wide application in practical\napplications such as financial risk control, medical diagnosis, and intelligent\nmonitoring.",
    "published_date": "2025-02-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.03386v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.03376v1",
    "title": "Ethical Considerations for the Military Use of Artificial Intelligence in Visual Reconnaissance",
    "authors": [
      "Mathias Anneken",
      "Nadia Burkart",
      "Fabian Jeschke",
      "Achim Kuwertz-Wolf",
      "Almuth Mueller",
      "Arne Schumann",
      "Michael Teutsch"
    ],
    "author_ids": [],
    "abstract": "This white paper underscores the critical importance of responsibly deploying\nArtificial Intelligence (AI) in military contexts, emphasizing a commitment to\nethical and legal standards. The evolving role of AI in the military goes\nbeyond mere technical applications, necessitating a framework grounded in\nethical principles. The discussion within the paper delves into ethical AI\nprinciples, particularly focusing on the Fairness, Accountability,\nTransparency, and Ethics (FATE) guidelines. Noteworthy considerations encompass\ntransparency, justice, non-maleficence, and responsibility. Importantly, the\npaper extends its examination to military-specific ethical considerations,\ndrawing insights from the Just War theory and principles established by\nprominent entities. In addition to the identified principles, the paper\nintroduces further ethical considerations specifically tailored for military AI\napplications. These include traceability, proportionality, governability,\nresponsibility, and reliability. The application of these ethical principles is\ndiscussed on the basis of three use cases in the domains of sea, air, and land.\nMethods of automated sensor data analysis, eXplainable AI (XAI), and intuitive\nuser experience are utilized to specify the use cases close to real-world\nscenarios. This comprehensive approach to ethical considerations in military AI\nreflects a commitment to aligning technological advancements with established\nethical frameworks. It recognizes the need for a balance between leveraging\nAI's potential benefits in military operations while upholding moral and legal\nstandards. The inclusion of these ethical principles serves as a foundation for\nresponsible and accountable use of AI in the complex and dynamic landscape of\nmilitary scenarios.",
    "published_date": "2025-02-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.03376v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.03359v2",
    "title": "GHOST: Gaussian Hypothesis Open-Set Technique",
    "authors": [
      "Ryan Rabinowitz",
      "Steve Cruz",
      "Manuel Günther",
      "Terrance E. Boult"
    ],
    "author_ids": [],
    "abstract": "Evaluations of large-scale recognition methods typically focus on overall\nperformance. While this approach is common, it often fails to provide insights\ninto performance across individual classes, which can lead to fairness issues\nand misrepresentation. Addressing these gaps is crucial for accurately\nassessing how well methods handle novel or unseen classes and ensuring a fair\nevaluation. To address fairness in Open-Set Recognition (OSR), we demonstrate\nthat per-class performance can vary dramatically. We introduce Gaussian\nHypothesis Open Set Technique (GHOST), a novel hyperparameter-free algorithm\nthat models deep features using class-wise multivariate Gaussian distributions\nwith diagonal covariance matrices. We apply Z-score normalization to logits to\nmitigate the impact of feature magnitudes that deviate from the model's\nexpectations, thereby reducing the likelihood of the network assigning a high\nscore to an unknown sample. We evaluate GHOST across multiple ImageNet-1K\npre-trained deep networks and test it with four different unknown datasets.\nUsing standard metrics such as AUOSCR, AUROC and FPR95, we achieve\nstatistically significant improvements, advancing the state-of-the-art in\nlarge-scale OSR. Source code is provided online.",
    "published_date": "2025-02-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.03359v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.06831v1",
    "title": "No Location Left Behind: Measuring and Improving the Fairness of Implicit Representations for Earth Data",
    "authors": [
      "Daniel Cai",
      "Randall Balestriero"
    ],
    "author_ids": [],
    "abstract": "Implicit neural representations (INRs) exhibit growing promise in addressing\nEarth representation challenges, ranging from emissions monitoring to climate\nmodeling. However, existing methods disproportionately prioritize global\naverage performance, whereas practitioners require fine-grained insights to\nunderstand biases and variations in these models. To bridge this gap, we\nintroduce FAIR-Earth: a first-of-its-kind dataset explicitly crafted to examine\nand challenge inequities in Earth representations. FAIR-Earth comprises various\nhigh-resolution Earth signals and uniquely aggregates extensive metadata along\nstratifications like landmass size and population density to assess the\nfairness of models. Evaluating state-of-the-art INRs across the various\nmodalities of FAIR-Earth, we uncover striking performance disparities. Certain\nsubgroups, especially those associated with high-frequency signals (e.g.,\nislands, coastlines), are consistently poorly modeled by existing methods. In\nresponse, we propose spherical wavelet encodings, building on previous spatial\nencoding research. Leveraging the multi-resolution capabilities of wavelets,\nour encodings yield consistent performance over various scales and locations,\noffering more accurate and robust representations of the biased subgroups.\nThese open-source contributions represent a crucial step towards the equitable\nassessment and deployment of Earth INRs.",
    "published_date": "2025-02-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.06831v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.03293v1",
    "title": "What is Human-Centeredness in Human-Centered AI? Development of Human-Centeredness Framework and AI Practitioners' Perspectives",
    "authors": [
      "Aung Pyae"
    ],
    "author_ids": [],
    "abstract": "There is no consensus on what constitutes human-centeredness in AI, and\nexisting frameworks lack empirical validation. This study addresses this gap by\ndeveloping a hierarchical framework of 26 attributes of human-centeredness,\nvalidated through practitioner input. The framework prioritizes ethical\nfoundations (e.g., fairness, transparency), usability, and emotional\nintelligence, organized into four tiers: ethical foundations, usability,\nemotional and cognitive dimensions, and personalization. By integrating\ntheoretical insights with empirical data, this work offers actionable guidance\nfor AI practitioners, promoting inclusive design, rigorous ethical standards,\nand iterative user feedback. The framework provides a robust foundation for\ncreating AI systems that enhance human well-being and align with societal\nvalues. Future research should explore how these attributes evolve across\ncultural and industrial contexts, ensuring the framework remains relevant as AI\ntechnologies advance.",
    "published_date": "2025-02-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.03293v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.04375v1",
    "title": "An Analysis for Reasoning Bias of Language Models with Small Initialization",
    "authors": [
      "Junjie Yao",
      "Zhongwang Zhang",
      "Zhi-Qin John Xu"
    ],
    "author_ids": [],
    "abstract": "Transformer-based Large Language Models (LLMs) have revolutionized Natural\nLanguage Processing by demonstrating exceptional performance across diverse\ntasks. This study investigates the impact of the parameter initialization scale\non the training behavior and task preferences of LLMs. We discover that smaller\ninitialization scales encourage models to favor reasoning tasks, whereas larger\ninitialization scales lead to a preference for memorization tasks. We validate\nthis reasoning bias via real datasets and meticulously designed anchor\nfunctions. Further analysis of initial training dynamics suggests that specific\nmodel components, particularly the embedding space and self-attention\nmechanisms, play pivotal roles in shaping these learning biases. We provide a\ntheoretical framework from the perspective of model training dynamics to\nexplain these phenomena. Additionally, experiments on real-world language tasks\ncorroborate our theoretical insights. This work enhances our understanding of\nhow initialization strategies influence LLM performance on reasoning tasks and\noffers valuable guidelines for training models.",
    "published_date": "2025-02-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.04375v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.03220v1",
    "title": "Mitigating Language Bias in Cross-Lingual Job Retrieval: A Recruitment Platform Perspective",
    "authors": [
      "Napat Laosaengpha",
      "Thanit Tativannarat",
      "Attapol Rutherford",
      "Ekapol Chuangsuwanich"
    ],
    "author_ids": [],
    "abstract": "Understanding the textual components of resumes and job postings is critical\nfor improving job-matching accuracy and optimizing job search systems in online\nrecruitment platforms. However, existing works primarily focus on analyzing\nindividual components within this information, requiring multiple specialized\ntools to analyze each aspect. Such disjointed methods could potentially hinder\noverall generalizability in recruitment-related text processing. Therefore, we\npropose a unified sentence encoder that utilized multi-task dual-encoder\nframework for jointly learning multiple component into the unified sentence\nencoder. The results show that our method outperforms other state-of-the-art\nmodels, despite its smaller model size. Moreover, we propose a novel metric,\nLanguage Bias Kullback-Leibler Divergence (LBKL), to evaluate language bias in\nthe encoder, demonstrating significant bias reduction and superior\ncross-lingual performance.",
    "published_date": "2025-02-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.03220v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.10423v1",
    "title": "Spiking Neural Network Feature Discrimination Boosts Modality Fusion",
    "authors": [
      "Katerina Maria Oikonomou",
      "Ioannis Kansizoglou",
      "Antonios Gasteratos"
    ],
    "author_ids": [],
    "abstract": "Feature discrimination is a crucial aspect of neural network design, as it\ndirectly impacts the network's ability to distinguish between classes and\ngeneralize across diverse datasets. The accomplishment of achieving\nhigh-quality feature representations ensures high intra-class separability and\nposes one of the most challenging research directions. While conventional deep\nneural networks (DNNs) rely on complex transformations and very deep networks\nto come up with meaningful feature representations, they usually require days\nof training and consume significant energy amounts. To this end, spiking neural\nnetworks (SNNs) offer a promising alternative. SNN's ability to capture\ntemporal and spatial dependencies renders them particularly suitable for\ncomplex tasks, where multi-modal data are required. In this paper, we propose a\nfeature discrimination approach for multi-modal learning with SNNs, focusing on\naudio-visual data. We employ deep spiking residual learning for visual modality\nprocessing and a simpler yet efficient spiking network for auditory modality\nprocessing. Lastly, we deploy a spiking multilayer perceptron for modality\nfusion. We present our findings and evaluate our approach against similar works\nin the field of classification challenges. To the best of our knowledge, this\nis the first work investigating feature discrimination in SNNs.",
    "published_date": "2025-02-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.NE",
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.10423v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.04371v1",
    "title": "PerPO: Perceptual Preference Optimization via Discriminative Rewarding",
    "authors": [
      "Zining Zhu",
      "Liang Zhao",
      "Kangheng Lin",
      "Jinze Yang",
      "En Yu",
      "Chenglong Liu",
      "Haoran Wei",
      "Jianjian Sun",
      "Zheng Ge",
      "Xiangyu Zhang"
    ],
    "author_ids": [],
    "abstract": "This paper presents Perceptual Preference Optimization (PerPO), a perception\nalignment method aimed at addressing the visual discrimination challenges in\ngenerative pre-trained multimodal large language models (MLLMs). To align MLLMs\nwith human visual perception process, PerPO employs discriminative rewarding to\ngather diverse negative samples, followed by listwise preference optimization\nto rank them.By utilizing the reward as a quantitative margin for ranking, our\nmethod effectively bridges generative preference optimization and\ndiscriminative empirical risk minimization. PerPO significantly enhances MLLMs'\nvisual discrimination capabilities while maintaining their generative\nstrengths, mitigates image-unconditional reward hacking, and ensures consistent\nperformance across visual tasks. This work marks a crucial step towards more\nperceptually aligned and versatile MLLMs. We also hope that PerPO will\nencourage the community to rethink MLLM alignment strategies.",
    "published_date": "2025-02-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.04371v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.03078v2",
    "title": "Automatic Prompt Optimization Techniques: Exploring the Potential for Synthetic Data Generation",
    "authors": [
      "Nina Freise",
      "Marius Heitlinger",
      "Ruben Nuredini",
      "Gerrit Meixner"
    ],
    "author_ids": [],
    "abstract": "Artificial Intelligence (AI) advancement is heavily dependent on access to\nlarge-scale, high-quality training data. However, in specialized domains such\nas healthcare, data acquisition faces significant constraints due to privacy\nregulations, ethical considerations, and limited availability. While synthetic\ndata generation offers a promising solution, conventional approaches typically\nrequire substantial real data for training generative models. The emergence of\nlarge-scale prompt-based models presents new opportunities for synthetic data\ngeneration without direct access to protected data. However, crafting effective\nprompts for domain-specific data generation remains challenging, and manual\nprompt engineering proves insufficient for achieving output with sufficient\nprecision and authenticity. We review recent developments in automatic prompt\noptimization, following PRISMA guidelines. We analyze six peer-reviewed studies\npublished between 2020 and 2024 that focus on automatic data-free prompt\noptimization methods. Our analysis reveals three approaches: feedback-driven,\nerror-based, and control-theoretic. Although all approaches demonstrate\npromising capabilities in prompt refinement and adaptation, our findings\nsuggest the need for an integrated framework that combines complementary\noptimization techniques to enhance synthetic data generation while minimizing\nmanual intervention. We propose future research directions toward developing\nrobust, iterative prompt optimization frameworks capable of improving the\nquality of synthetic data. This advancement can be particularly crucial for\nsensitive fields and in specialized domains where data access is restricted,\npotentially transforming how we approach synthetic data generation for AI\ndevelopment.",
    "published_date": "2025-02-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.LG",
      "68T05, 68T20, 62H35, 92C50",
      "I.2.0; I.2.6; I.5.2; J.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.03078v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.03053v1",
    "title": "DOLFIN -- Document-Level Financial test set for Machine Translation",
    "authors": [
      "Mariam Nakhlé",
      "Marco Dinarelli",
      "Raheel Qader",
      "Emmanuelle Esperança-Rodier",
      "Hervé Blanchon"
    ],
    "author_ids": [],
    "abstract": "Despite the strong research interest in document-level Machine Translation\n(MT), the test sets dedicated to this task are still scarce. The existing test\nsets mainly cover topics from the general domain and fall short on specialised\ndomains, such as legal and financial. Also, in spite of their document-level\naspect, they still follow a sentence-level logic that does not allow for\nincluding certain linguistic phenomena such as information reorganisation. In\nthis work, we aim to fill this gap by proposing a novel test set: DOLFIN. The\ndataset is built from specialised financial documents, and it makes a step\ntowards true document-level MT by abandoning the paradigm of perfectly aligned\nsentences, presenting data in units of sections rather than sentences. The test\nset consists of an average of 1950 aligned sections for five language pairs. We\npresent a detailed data collection pipeline that can serve as inspiration for\naligning new document-level datasets. We demonstrate the usefulness and quality\nof this test set by evaluating a number of models. Our results show that the\ntest set is able to discriminate between context-sensitive and context-agnostic\nmodels and shows the weaknesses when models fail to accurately translate\nfinancial texts. The test set is made public for the community.",
    "published_date": "2025-02-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.03053v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.03023v1",
    "title": "Parametric Scaling Law of Tuning Bias in Conformal Prediction",
    "authors": [
      "Hao Zeng",
      "Kangdao Liu",
      "Bingyi Jing",
      "Hongxin Wei"
    ],
    "author_ids": [],
    "abstract": "Conformal prediction is a popular framework of uncertainty quantification\nthat constructs prediction sets with coverage guarantees. To uphold the\nexchangeability assumption, many conformal prediction methods necessitate an\nadditional holdout set for parameter tuning. Yet, the impact of violating this\nprinciple on coverage remains underexplored, making it ambiguous in practical\napplications. In this work, we empirically find that the tuning bias - the\ncoverage gap introduced by leveraging the same dataset for tuning and\ncalibration, is negligible for simple parameter tuning in many conformal\nprediction methods. In particular, we observe the scaling law of the tuning\nbias: this bias increases with parameter space complexity and decreases with\ncalibration set size. Formally, we establish a theoretical framework to\nquantify the tuning bias and provide rigorous proof for the scaling law of the\ntuning bias by deriving its upper bound. In the end, we discuss how to reduce\nthe tuning bias, guided by the theories we developed.",
    "published_date": "2025-02-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "math.ST",
      "stat.ME",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.03023v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.02966v1",
    "title": "FACTER: Fairness-Aware Conformal Thresholding and Prompt Engineering for Enabling Fair LLM-Based Recommender Systems",
    "authors": [
      "Arya Fayyazi",
      "Mehdi Kamal",
      "Massoud Pedram"
    ],
    "author_ids": [],
    "abstract": "We propose FACTER, a fairness-aware framework for LLM-based recommendation\nsystems that integrates conformal prediction with dynamic prompt engineering.\nBy introducing an adaptive semantic variance threshold and a\nviolation-triggered mechanism, FACTER automatically tightens fairness\nconstraints whenever biased patterns emerge. We further develop an adversarial\nprompt generator that leverages historical violations to reduce repeated\ndemographic biases without retraining the LLM. Empirical results on MovieLens\nand Amazon show that FACTER substantially reduces fairness violations (up to\n95.5%) while maintaining strong recommendation accuracy, revealing semantic\nvariance as a potent proxy of bias.",
    "published_date": "2025-02-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.02966v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.02957v1",
    "title": "Control Search Rankings, Control the World: What is a Good Search Engine?",
    "authors": [
      "Simon Coghlan",
      "Hui Xian Chia",
      "Falk Scholer",
      "Damiano Spina"
    ],
    "author_ids": [],
    "abstract": "This paper examines the ethical question, 'What is a good search engine?'\nSince search engines are gatekeepers of global online information, it is vital\nthey do their job ethically well. While the Internet is now several decades\nold, the topic remains under-explored from interdisciplinary perspectives. This\npaper presents a novel role-based approach involving four ethical models of\ntypes of search engine behavior: Customer Servant, Librarian, Journalist, and\nTeacher. It explores these ethical models with reference to the research field\nof information retrieval, and by means of a case study involving the COVID-19\nglobal pandemic. It also reflects on the four ethical models in terms of the\nhistory of search engine development, from earlier crude efforts in the 1990s,\nto the very recent prospect of Large Language Model-based conversational\ninformation seeking systems taking on the roles of established web search\nengines like Google. Finally, the paper outlines considerations that inform\npresent and future regulation and accountability for search engines as they\ncontinue to evolve. The paper should interest information retrieval researchers\nand others interested in the ethics of search engines.",
    "published_date": "2025-02-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IR",
      "cs.CY",
      "H.3.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.02957v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.02912v1",
    "title": "MobiCLR: Mobility Time Series Contrastive Learning for Urban Region Representations",
    "authors": [
      "Namwoo Kim",
      "Takahiro Yabe",
      "Chanyoung Park",
      "Yoonjin Yoon"
    ],
    "author_ids": [],
    "abstract": "Recently, learning effective representations of urban regions has gained\nsignificant attention as a key approach to understanding urban dynamics and\nadvancing smarter cities. Existing approaches have demonstrated the potential\nof leveraging mobility data to generate latent representations, providing\nvaluable insights into the intrinsic characteristics of urban areas. However,\nincorporating the temporal dynamics and detailed semantics inherent in human\nmobility patterns remains underexplored. To address this gap, we propose a\nnovel urban region representation learning model, Mobility Time Series\nContrastive Learning for Urban Region Representations (MobiCLR), designed to\ncapture semantically meaningful embeddings from inflow and outflow mobility\npatterns. MobiCLR uses contrastive learning to enhance the discriminative power\nof its representations, applying an instance-wise contrastive loss to capture\ndistinct flow-specific characteristics. Additionally, we develop a regularizer\nto align output features with these flow-specific representations, enabling a\nmore comprehensive understanding of mobility dynamics. To validate our model,\nwe conduct extensive experiments in Chicago, New York, and Washington, D.C. to\npredict income, educational attainment, and social vulnerability. The results\ndemonstrate that our model outperforms state-of-the-art models.",
    "published_date": "2025-02-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.02912v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.02903v1",
    "title": "What is in a name? Mitigating Name Bias in Text Embeddings via Anonymization",
    "authors": [
      "Sahil Manchanda",
      "Pannaga Shivaswamy"
    ],
    "author_ids": [],
    "abstract": "Text-embedding models often exhibit biases arising from the data on which\nthey are trained. In this paper, we examine a hitherto unexplored bias in\ntext-embeddings: bias arising from the presence of $\\textit{names}$ such as\npersons, locations, organizations etc. in the text. Our study shows how the\npresence of $\\textit{name-bias}$ in text-embedding models can potentially lead\nto erroneous conclusions in assessment of thematic similarity.Text-embeddings\ncan mistakenly indicate similarity between texts based on names in the text,\neven when their actual semantic content has no similarity or indicate\ndissimilarity simply because of the names in the text even when the texts match\nsemantically. We first demonstrate the presence of name bias in different\ntext-embedding models and then propose $\\textit{text-anonymization}$ during\ninference which involves removing references to names, while preserving the\ncore theme of the text. The efficacy of the anonymization approach is\ndemonstrated on two downstream NLP tasks, achieving significant performance\ngains. Our simple and training-optimization-free approach offers a practical\nand easily implementable solution to mitigate name bias.",
    "published_date": "2025-02-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.02903v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.04740v1",
    "title": "PRISM: Perspective Reasoning for Integrated Synthesis and Mediation as a Multi-Perspective Framework for AI Alignment",
    "authors": [
      "Anthony Diamond"
    ],
    "author_ids": [],
    "abstract": "In this work, we propose Perspective Reasoning for Integrated Synthesis and\nMediation (PRISM), a multiple-perspective framework for addressing persistent\nchallenges in AI alignment such as conflicting human values and specification\ngaming. Grounded in cognitive science and moral psychology, PRISM organizes\nmoral concerns into seven \"basis worldviews\", each hypothesized to capture a\ndistinct dimension of human moral cognition, ranging from survival-focused\nreflexes through higher-order integrative perspectives. It then applies a\nPareto-inspired optimization scheme to reconcile competing priorities without\nreducing them to a single metric. Under the assumption of reliable context\nvalidation for robust use, the framework follows a structured workflow that\nelicits viewpoint-specific responses, synthesizes them into a balanced outcome,\nand mediates remaining conflicts in a transparent and iterative manner. By\nreferencing layered approaches to moral cognition from cognitive science, moral\npsychology, and neuroscience, PRISM clarifies how different moral drives\ninteract and systematically documents and mediates ethical tradeoffs. We\nillustrate its efficacy through real outputs produced by a working prototype,\napplying PRISM to classic alignment problems in domains such as public health\npolicy, workplace automation, and education. By anchoring AI deliberation in\nthese human vantage points, PRISM aims to bound interpretive leaps that might\notherwise drift into non-human or machine-centric territory. We briefly outline\nfuture directions, including real-world deployments and formal verifications,\nwhile maintaining the core focus on multi-perspective synthesis and conflict\nmediation.",
    "published_date": "2025-02-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "90C29, 68T05",
      "I.2.11; I.2.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.04740v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.02815v1",
    "title": "Exploring Relations among Fairness Notions in Discrete Fair Division",
    "authors": [
      "Jugal Garg",
      "Eklavya Sharma"
    ],
    "author_ids": [],
    "abstract": "Fairly allocating indivisible items among agents is an important and\nwell-studied problem. However, fairness does not have a single universally\nagreed-upon definition, and so, many different definitions of fairness have\nbeen proposed and studied. Some of these definitions are considered more fair\nthan others, although stronger fairness notions are also more difficult to\nguarantee. In this work, we study 21 different notions of fairness and arrange\nthem in a hierarchy. Formally, we say that a fairness notion $F_1$ implies\nanother notion $F_2$ if every $F_1$-fair allocation is also an $F_2$-fair\nallocation. We give a near-complete picture of implications among fairness\nnotions: for almost every pair of notions, we either prove that one notion\nimplies the other, or we give a counterexample, i.e., an allocation that is\nfair by one notion but not by the other. Although some of these results are\nwell-known, many of them are new. We give results for many different settings:\nallocating goods, allocating chores, and allocating mixed manna. We believe our\nwork clarifies the relative merits of different fairness notions, and provides\na foundation for further research in fair allocation. Moreover, we developed an\ninference engine to automate part of our work. This inference engine is\nimplemented as a user-friendly web application and is not restricted to fair\ndivision scenarios, so it holds potential for broader use.",
    "published_date": "2025-02-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.02815v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.02786v1",
    "title": "When Machine Learning Gets Personal: Understanding Fairness of Personalized Models",
    "authors": [
      "Louisa Cornelis",
      "Guillermo Bernárdez",
      "Haewon Jeong",
      "Nina Miolane"
    ],
    "author_ids": [],
    "abstract": "Personalization in machine learning involves tailoring models to individual\nusers by incorporating personal attributes such as demographic or medical data.\nWhile personalization can improve prediction accuracy, it may also amplify\nbiases and reduce explainability. This work introduces a unified framework to\nevaluate the impact of personalization on both prediction accuracy and\nexplanation quality across classification and regression tasks. We derive novel\nupper bounds for the number of personal attributes that can be used to reliably\nvalidate benefits of personalization. Our analysis uncovers key trade-offs. We\nshow that regression models can potentially utilize more personal attributes\nthan classification models. We also demonstrate that improvements in prediction\naccuracy due to personalization do not necessarily translate to enhanced\nexplainability -- underpinning the importance to evaluate both metrics when\npersonalizing machine learning models in critical settings such as healthcare.\nValidated with a real-world dataset, this framework offers practical guidance\nfor balancing accuracy, fairness, and interpretability in personalized models.",
    "published_date": "2025-02-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.02786v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.04356v1",
    "title": "Open Foundation Models in Healthcare: Challenges, Paradoxes, and Opportunities with GenAI Driven Personalized Prescription",
    "authors": [
      "Mahdi Alkaeed",
      "Sofiat Abioye",
      "Adnan Qayyum",
      "Yosra Magdi Mekki",
      "Ilhem Berrou",
      "Mohamad Abdallah",
      "Ala Al-Fuqaha",
      "Muhammad Bilal",
      "Junaid Qadir"
    ],
    "author_ids": [],
    "abstract": "In response to the success of proprietary Large Language Models (LLMs) such\nas OpenAI's GPT-4, there is a growing interest in developing open,\nnon-proprietary LLMs and AI foundation models (AIFMs) for transparent use in\nacademic, scientific, and non-commercial applications. Despite their inability\nto match the refined functionalities of their proprietary counterparts, open\nmodels hold immense potential to revolutionize healthcare applications. In this\npaper, we examine the prospects of open-source LLMs and AIFMs for developing\nhealthcare applications and make two key contributions. Firstly, we present a\ncomprehensive survey of the current state-of-the-art open-source healthcare\nLLMs and AIFMs and introduce a taxonomy of these open AIFMs, categorizing their\nutility across various healthcare tasks. Secondly, to evaluate the\ngeneral-purpose applications of open LLMs in healthcare, we present a case\nstudy on personalized prescriptions. This task is particularly significant due\nto its critical role in delivering tailored, patient-specific medications that\ncan greatly improve treatment outcomes. In addition, we compare the performance\nof open-source models with proprietary models in settings with and without\nRetrieval-Augmented Generation (RAG). Our findings suggest that, although less\nrefined, open LLMs can achieve performance comparable to proprietary models\nwhen paired with grounding techniques such as RAG. Furthermore, to highlight\nthe clinical significance of LLMs-empowered personalized prescriptions, we\nperform subjective assessment through an expert clinician. We also elaborate on\nethical considerations and potential risks associated with the misuse of\npowerful LLMs and AIFMs, highlighting the need for a cautious and responsible\nimplementation in healthcare.",
    "published_date": "2025-02-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.04356v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.02567v1",
    "title": "Fairness in Survival Analysis: A Novel Conditional Mutual Information Augmentation Approach",
    "authors": [
      "Tianyang Xie",
      "Yong Ge"
    ],
    "author_ids": [],
    "abstract": "Survival analysis, a vital tool for predicting the time to event, has been\nused in many domains such as healthcare, criminal justice, and finance. Like\nclassification tasks, survival analysis can exhibit bias against disadvantaged\ngroups, often due to biases inherent in data or algorithms. Several studies in\nboth the IS and CS communities have attempted to address fairness in survival\nanalysis. However, existing methods often overlook the importance of prediction\nfairness at pre-defined evaluation time points, which is crucial in real-world\napplications where decision making often hinges on specific time frames. To\naddress this critical research gap, we introduce a new fairness concept:\nequalized odds (EO) in survival analysis, which emphasizes prediction fairness\nat pre-defined time points. To achieve the EO fairness in survival analysis, we\npropose a Conditional Mutual Information Augmentation (CMIA) approach, which\nfeatures a novel fairness regularization term based on conditional mutual\ninformation and an innovative censored data augmentation technique. Our CMIA\napproach can effectively balance prediction accuracy and fairness, and it is\napplicable to various survival models. We evaluate the CMIA approach against\nseveral state-of-the-art methods within three different application domains,\nand the results demonstrate that CMIA consistently reduces prediction disparity\nwhile maintaining good accuracy and significantly outperforms the other\ncompeting methods across multiple datasets and survival models (e.g., linear\nCOX, deep AFT).",
    "published_date": "2025-02-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.02567v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.02527v1",
    "title": "TabPFN Unleashed: A Scalable and Effective Solution to Tabular Classification Problems",
    "authors": [
      "Si-Yang Liu",
      "Han-Jia Ye"
    ],
    "author_ids": [],
    "abstract": "TabPFN has emerged as a promising in-context learning model for tabular data,\ncapable of directly predicting the labels of test samples given labeled\ntraining examples. It has demonstrated competitive performance, particularly on\nsmall-scale classification tasks. However, despite its effectiveness, TabPFN\nstill requires further refinement in several areas, including handling\nhigh-dimensional features, aligning with downstream datasets, and scaling to\nlarger datasets. In this paper, we revisit existing variants of TabPFN and\nobserve that most approaches focus either on reducing bias or variance, often\nneglecting the need to address the other side, while also increasing inference\noverhead. To fill this gap, we propose Beta (Bagging and Encoder-based\nFine-tuning for TabPFN Adaptation), a novel and effective method designed to\nminimize both bias and variance. To reduce bias, we introduce a lightweight\nencoder to better align downstream tasks with the pre-trained TabPFN. By\nincreasing the number of encoders in a lightweight manner, Beta mitigate\nvariance, thereby further improving the model's performance. Additionally,\nbootstrapped sampling is employed to further reduce the impact of data\nperturbations on the model, all while maintaining computational efficiency\nduring inference. Our approach enhances TabPFN's ability to handle\nhigh-dimensional data and scale to larger datasets. Experimental results on\nover 200 benchmark classification datasets demonstrate that Beta either\noutperforms or matches state-of-the-art methods.",
    "published_date": "2025-02-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.02527v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.02365v1",
    "title": "Measuring social mobility in temporal networks",
    "authors": [
      "Matthew Russell Barnes",
      "Vincenzo Nicosia",
      "Richard G. Clegg"
    ],
    "author_ids": [],
    "abstract": "In complex networks, the rich-get-richer effect (nodes with high degree at\none point in time gain more degree in their future) is commonly observed. In\npractice this is often studied on a static network snapshot, for example, a\npreferential attachment model assumed to explain the more highly connected\nnodes or a rich-club}effect that analyses the most highly connected nodes. In\nthis paper, we consider temporal measures of how success (measured here as node\ndegree) propagates across time. By analogy with social mobility (a measure\npeople moving within a social hierarchy through their life) we define\nhierarchical mobility to measure how a node's propensity to gain degree changes\nover time. We introduce an associated taxonomy of temporal correlation\nstatistics including mobility, philanthropy and community. Mobility measures\nthe extent to which a node's degree gain in one time period predicts its degree\ngain in the next. Philanthropy and community measure similar properties related\nto node neighbourhood.\n  We apply these statistics both to artificial models and to 26 real temporal\nnetworks. We find that most of our networks show a tendency for individual\nnodes and their neighbourhoods to remain in similar hierarchical positions over\ntime, while most networks show low correlative effects between individuals and\ntheir neighbourhoods. Moreover, we show that the mobility taxonomy can\ndiscriminate between networks from different fields. We also generate\nartificial network models to gain intuition about the behaviour and expected\nrange of the statistics. The artificial models show that the opposite of the\n\"rich-get-richer\" effect requires the existence of inequality of degree in a\nnetwork. Overall, we show that measuring the hierarchical mobility of a\ntemporal network is an invaluable resource for discovering its underlying\nstructural dynamics.",
    "published_date": "2025-02-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SI",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.02365v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2503.04739v1",
    "title": "Responsible Artificial Intelligence Systems: A Roadmap to Society's Trust through Trustworthy AI, Auditability, Accountability, and Governance",
    "authors": [
      "Andrés Herrera-Poyatos",
      "Javier Del Ser",
      "Marcos López de Prado",
      "Fei-Yue Wang",
      "Enrique Herrera-Viedma",
      "Francisco Herrera"
    ],
    "author_ids": [],
    "abstract": "Artificial intelligence (AI) has matured as a technology, necessitating the\ndevelopment of responsibility frameworks that are fair, inclusive, trustworthy,\nsafe and secure, transparent, and accountable. By establishing such frameworks,\nwe can harness the full potential of AI while mitigating its risks,\nparticularly in high-risk scenarios. This requires the design of responsible AI\nsystems based on trustworthy AI technologies and ethical principles, with the\naim of ensuring auditability and accountability throughout their design,\ndevelopment, and deployment, adhering to domain-specific regulations and\nstandards.\n  This paper explores the concept of a responsible AI system from a holistic\nperspective, which encompasses four key dimensions: 1) regulatory context; 2)\ntrustworthy AI technology along with standardization and assessments; 3)\nauditability and accountability; and 4) AI governance. The aim of this paper is\ndouble. First, we analyze and understand these four dimensions and their\ninterconnections in the form of an analysis and overview. Second, the final\ngoal of the paper is to propose a roadmap in the design of responsible AI\nsystems, ensuring that they can gain society's trust. To achieve this\ntrustworthiness, this paper also fosters interdisciplinary discussions on the\nethical, legal, social, economic, and cultural aspects of AI from a global\ngovernance perspective. Last but not least, we also reflect on the current\nstate and those aspects that need to be developed in the near future, as ten\nlessons learned.",
    "published_date": "2025-02-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "I.2.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.04739v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.02309v1",
    "title": "Review of Demographic Bias in Face Recognition",
    "authors": [
      "Ketan Kotwal",
      "Sebastien Marcel"
    ],
    "author_ids": [],
    "abstract": "Demographic bias in face recognition (FR) has emerged as a critical area of\nresearch, given its impact on fairness, equity, and reliability across diverse\napplications. As FR technologies are increasingly deployed globally,\ndisparities in performance across demographic groups -- such as race,\nethnicity, and gender -- have garnered significant attention. These biases not\nonly compromise the credibility of FR systems but also raise ethical concerns,\nespecially when these technologies are employed in sensitive domains. This\nreview consolidates extensive research efforts providing a comprehensive\noverview of the multifaceted aspects of demographic bias in FR.\n  We systematically examine the primary causes, datasets, assessment metrics,\nand mitigation approaches associated with demographic disparities in FR. By\ncategorizing key contributions in these areas, this work provides a structured\napproach to understanding and addressing the complexity of this issue. Finally,\nwe highlight current advancements and identify emerging challenges that need\nfurther investigation. This article aims to provide researchers with a unified\nperspective on the state-of-the-art while emphasizing the critical need for\nequitable and trustworthy FR systems.",
    "published_date": "2025-02-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.02309v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.02257v2",
    "title": "UNIP: Rethinking Pre-trained Attention Patterns for Infrared Semantic Segmentation",
    "authors": [
      "Tao Zhang",
      "Jinyong Wen",
      "Zhen Chen",
      "Kun Ding",
      "Shiming Xiang",
      "Chunhong Pan"
    ],
    "author_ids": [],
    "abstract": "Pre-training techniques significantly enhance the performance of semantic\nsegmentation tasks with limited training data. However, the efficacy under a\nlarge domain gap between pre-training (e.g. RGB) and fine-tuning (e.g.\ninfrared) remains underexplored. In this study, we first benchmark the infrared\nsemantic segmentation performance of various pre-training methods and reveal\nseveral phenomena distinct from the RGB domain. Next, our layerwise analysis of\npre-trained attention maps uncovers that: (1) There are three typical attention\npatterns (local, hybrid, and global); (2) Pre-training tasks notably influence\nthe pattern distribution across layers; (3) The hybrid pattern is crucial for\nsemantic segmentation as it attends to both nearby and foreground elements; (4)\nThe texture bias impedes model generalization in infrared tasks. Building on\nthese insights, we propose UNIP, a UNified Infrared Pre-training framework, to\nenhance the pre-trained model performance. This framework uses the\nhybrid-attention distillation NMI-HAD as the pre-training target, a large-scale\nmixed dataset InfMix for pre-training, and a last-layer feature pyramid network\nLL-FPN for fine-tuning. Experimental results show that UNIP outperforms various\npre-training methods by up to 13.5\\% in average mIoU on three infrared\nsegmentation tasks, evaluated using fine-tuning and linear probing metrics.\nUNIP-S achieves performance on par with MAE-L while requiring only 1/10 of the\ncomputational cost. Furthermore, UNIP significantly surpasses state-of-the-art\n(SOTA) infrared or RGB segmentation methods and demonstrates broad potential\nfor application in other modalities, such as RGB and depth. Our code is\navailable at https://github.com/casiatao/UNIP.",
    "published_date": "2025-02-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.02257v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.05213v1",
    "title": "DERMARK: A Dynamic, Efficient and Robust Multi-bit Watermark for Large Language Models",
    "authors": [
      "Qihao Lin",
      "Chen Tang",
      "Lan zhang",
      "Junyang zhang",
      "Xiangyang Li"
    ],
    "author_ids": [],
    "abstract": "Well-trained large language models (LLMs) present significant risks,\nincluding potential malicious use and copyright infringement. Current studies\naim to trace the distribution of LLM-generated texts by implicitly embedding\nwatermarks. Among these, the single-bit watermarking method can only determine\nwhether a given text was generated by an LLM. In contrast, the multi-bit\nwatermarking method embeds richer information into the generated text, which\ncan identify which LLM generated and distributed a given text to which user.\nHowever, existing efforts embed the multi-bit watermark directly into the\ngenerated text without accounting for its watermarking capacity. This approach\ncan result in embedding failures when the text's watermarking capacity is\ninsufficient. In this paper, we derive the watermark embedding distribution\nbased on the logits of LLMs and propose a formal inequality to segment the text\noptimally for watermark embedding. Building on this foundation, we propose\nDERMARK, a dynamic, efficient, and robust multi-bit watermarking method.\nDERMARK divides the text into segments of varying lengths for each bit\nembedding, adaptively matching the text's capacity. It achieves this with\nnegligible overhead and robust performance against text editing by minimizing\nwatermark extraction loss. Comprehensive experiments demonstrate that, compared\nto the SOTA method, our method reduces the number of tokens required for\nembedding each bit by 20\\%, reduces watermark embedding time by 50\\%, and is\nrobust to text editing and watermark erasure attacks.",
    "published_date": "2025-02-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.05213v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.02221v1",
    "title": "Bias Detection via Maximum Subgroup Discrepancy",
    "authors": [
      "Jiří Němeček",
      "Mark Kozdoba",
      "Illia Kryvoviaz",
      "Tomáš Pevný",
      "Jakub Mareček"
    ],
    "author_ids": [],
    "abstract": "Bias evaluation is fundamental to trustworthy AI, both in terms of checking\ndata quality and in terms of checking the outputs of AI systems. In testing\ndata quality, for example, one may study a distance of a given dataset, viewed\nas a distribution, to a given ground-truth reference dataset. However,\nclassical metrics, such as the Total Variation and the Wasserstein distances,\nare known to have high sample complexities and, therefore, may fail to provide\nmeaningful distinction in many practical scenarios.\n  In this paper, we propose a new notion of distance, the Maximum Subgroup\nDiscrepancy (MSD). In this metric, two distributions are close if, roughly,\ndiscrepancies are low for all feature subgroups. While the number of subgroups\nmay be exponential, we show that the sample complexity is linear in the number\nof features, thus making it feasible for practical applications. Moreover, we\nprovide a practical algorithm for the evaluation of the distance, based on\nMixed-integer optimization (MIO). We also note that the proposed distance is\neasily interpretable, thus providing clearer paths to fixing the biases once\nthey have been identified. It also provides guarantees for all subgroups.\nFinally, we empirically evaluate, compare with other metrics, and demonstrate\nthe above properties of MSD on real-world datasets.",
    "published_date": "2025-02-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.02221v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.02218v1",
    "title": "Digital Fairness Algorithms for Satellite Uplink NOMA",
    "authors": [
      "Giorgio Taricco"
    ],
    "author_ids": [],
    "abstract": "Achieving digital fairness by using NOMA is one of the more pressing issues\nin modern wireless communication systems for 5G/6G networks. This is\nparticularly true in the case of satellite uplink systems supporting a\npopulation of IoT wireless devices scattered in a wide coverage area. In this\nscenario, the variability of the link budget across space and time increases\nthe challenges of preventing a situation where only a subset of network users\ncan transmit while others are left unable to do so. This work investigates the\ncharacteristics of an uplink NOMA system with the goal of equalizing the\nachievable rate of the IoT network subscribers. Within the context of\nsingle-slot NOMA, two key outcomes are achieved: the determination of the\noptimal SIC ordering at the receiver and the exploration of power moderation,\ncoordinated by the receiver, to maximize the minimum user rate. In the context\nof multi-slot NOMA, which is particularly relevant to the satellite scenario\nunder consideration, a user rate equalization algorithm is proposed and its\nperformance is analyzed numerically. The trade-off between network performance,\nmeasured in terms of user rates, and complexity, determined by the number of\nSIC steps implemented at the receiver, is thoroughly evaluated for the\nsatellite scenario under consideration.",
    "published_date": "2025-02-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IT",
      "eess.SP",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.02218v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.02072v1",
    "title": "ASCenD-BDS: Adaptable, Stochastic and Context-aware framework for Detection of Bias, Discrimination and Stereotyping",
    "authors": [
      "Rajiv Bahl",
      "Venkatesan N",
      "Parimal Aglawe",
      "Aastha Sarasapalli",
      "Bhavya Kancharla",
      "Chaitanya kolukuluri",
      "Harish Mohite",
      "Japneet Hora",
      "Kiran Kakollu",
      "Rahul Diman",
      "Shubham Kapale",
      "Sri Bhagya Kathula",
      "Vamsikrishna Motru",
      "Yogeshwar Reddy"
    ],
    "author_ids": [],
    "abstract": "The rapid evolution of Large Language Models (LLMs) has transformed natural\nlanguage processing but raises critical concerns about biases inherent in their\ndeployment and use across diverse linguistic and sociocultural contexts. This\npaper presents a framework named ASCenD BDS (Adaptable, Stochastic and\nContext-aware framework for Detection of Bias, Discrimination and\nStereotyping). The framework presents approach to detecting bias,\ndiscrimination, stereotyping across various categories such as gender, caste,\nage, disability, socioeconomic status, linguistic variations, etc., using an\napproach which is Adaptive, Stochastic and Context-Aware. The existing\nframeworks rely heavily on usage of datasets to generate scenarios for\ndetection of Bias, Discrimination and Stereotyping. Examples include datasets\nsuch as Civil Comments, Wino Gender, WinoBias, BOLD, CrowS Pairs and BBQ.\nHowever, such an approach provides point solutions. As a result, these datasets\nprovide a finite number of scenarios for assessment. The current framework\novercomes this limitation by having features which enable Adaptability,\nStochasticity, Context Awareness. Context awareness can be customized for any\nnation or culture or sub-culture (for example an organization's unique\nculture). In this paper, context awareness in the Indian context has been\nestablished. Content has been leveraged from Indian Census 2011 to have a\ncommonality of categorization. A framework has been developed using Category,\nSub-Category, STEM, X-Factor, Synonym to enable the features for Adaptability,\nStochasticity and Context awareness. The framework has been described in detail\nin Section 3. Overall 800 plus STEMs, 10 Categories, 31 unique SubCategories\nwere developed by a team of consultants at Saint Fox Consultancy Private Ltd.\nThe concept has been tested out in SFCLabs as part of product development.",
    "published_date": "2025-02-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.02072v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.02007v2",
    "title": "Reasoning Bias of Next Token Prediction Training",
    "authors": [
      "Pengxiao Lin",
      "Zhongwang Zhang",
      "Zhi-Qin John Xu"
    ],
    "author_ids": [],
    "abstract": "Since the inception of Large Language Models (LLMs), the quest to efficiently\ntrain them for superior reasoning capabilities has been a pivotal challenge.\nThe dominant training paradigm for LLMs is based on next token prediction\n(NTP). Alternative methodologies, called Critical Token Prediction (CTP),\nfocused exclusively on specific critical tokens (such as the answer in Q\\&A\ndataset), aiming to reduce the overfitting of extraneous information and noise.\nContrary to initial assumptions, our research reveals that despite NTP's\nexposure to noise during training, it surpasses CTP in reasoning ability. We\nattribute this counterintuitive outcome to the regularizing influence of noise\non the training dynamics. Our empirical analysis shows that NTP-trained models\nexhibit enhanced generalization and robustness across various benchmark\nreasoning datasets, demonstrating greater resilience to perturbations and\nachieving flatter loss minima. These findings illuminate that NTP is\ninstrumental in fostering reasoning abilities during pretraining, whereas CTP\nis more effective for finetuning, thereby enriching our comprehension of\noptimal training strategies in LLM development.",
    "published_date": "2025-02-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.02007v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.01951v1",
    "title": "On the Emergence of Position Bias in Transformers",
    "authors": [
      "Xinyi Wu",
      "Yifei Wang",
      "Stefanie Jegelka",
      "Ali Jadbabaie"
    ],
    "author_ids": [],
    "abstract": "Recent studies have revealed various manifestations of position bias in\ntransformer architectures, from the \"lost-in-the-middle\" phenomenon to\nattention sinks, yet a comprehensive theoretical understanding of how attention\nmasks and positional encodings shape these biases remains elusive. This paper\nintroduces a novel graph-theoretic framework to analyze position bias in\nmulti-layer attention. Modeling attention masks as directed graphs, we quantify\nhow tokens interact with contextual information based on their sequential\npositions. We uncover two key insights: First, causal masking inherently biases\nattention toward earlier positions, as tokens in deeper layers attend to\nincreasingly more contextualized representations of earlier tokens. Second, we\ncharacterize the competing effects of the causal mask and relative positional\nencodings, such as the decay mask and rotary positional encoding (RoPE): while\nboth mechanisms introduce distance-based decay within individual attention\nmaps, their aggregate effect across multiple attention layers -- coupled with\nthe causal mask -- leads to a trade-off between the long-term decay effects and\nthe cumulative importance of early sequence positions. Through controlled\nnumerical experiments, we not only validate our theoretical findings but also\nreproduce position biases observed in real-world LLMs. Our framework offers a\nprincipled foundation for understanding positional biases in transformers,\nshedding light on the complex interplay of attention mechanism components and\nguiding more informed architectural design.",
    "published_date": "2025-02-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.01951v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.01926v1",
    "title": "Fairness through Difference Awareness: Measuring Desired Group Discrimination in LLMs",
    "authors": [
      "Angelina Wang",
      "Michelle Phan",
      "Daniel E. Ho",
      "Sanmi Koyejo"
    ],
    "author_ids": [],
    "abstract": "Algorithmic fairness has conventionally adopted a perspective of racial\ncolor-blindness (i.e., difference unaware treatment). We contend that in a\nrange of important settings, group difference awareness matters. For example,\ndifferentiating between groups may be necessary in legal contexts (e.g., the\nU.S. compulsory draft applies to men but not women) and harm assessments (e.g.,\ncalling a girl a terrorist may be less harmful than calling a Muslim person\none). In our work we first introduce an important distinction between\ndescriptive (fact-based), normative (value-based), and correlation\n(association-based) benchmarks. This distinction is significant because each\ncategory requires distinct interpretation and mitigation tailored to its\nspecific characteristics. Then, we present a benchmark suite composed of eight\ndifferent scenarios for a total of 16k questions that enables us to assess\ndifference awareness. Finally, we show results across ten models that\ndemonstrate difference awareness is a distinct dimension of fairness where\nexisting bias mitigation strategies may backfire.",
    "published_date": "2025-02-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.01926v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.01862v1",
    "title": "Optimal Traffic Allocation for Multi-Slot Sponsored Search: Balance of Efficiency and Fairness",
    "authors": [
      "Anastasiia Soboleva",
      "Alexander Ledovsky",
      "Yuriy Dorn",
      "Egor Samosvat",
      "Andrey Tikhanov",
      "Fyodor Prazdnikov"
    ],
    "author_ids": [],
    "abstract": "The majority of online marketplaces offer promotion programs to sellers to\nacquire additional customers for their products. These programs typically allow\nsellers to allocate advertising budgets to promote their products, with higher\nbudgets generally correlating to improve ad performance. Auction mechanisms\nwith budget pacing are commonly employed to implement such ad systems. While\nauctions deliver satisfactory average effectiveness, ad performance under\nallocated budgets can be unfair in practice.\n  To address this issue, we propose a novel ad allocation model that departs\nfrom traditional auction mechanics. Our approach focuses on solving a global\noptimization problem that balances traffic allocation while considering\nplatform efficiency and fairness constraints.\n  This study presents the following contributions. First, we introduce a\nfairness metric based on the Gini index. Second, we formulate the optimization\nproblem incorporating efficiency and fairness objectives. Third, we offer an\nonline algorithm to solve this optimization problem. Finally, we demonstrate\nthat our approach achieves superior fairness compared to baseline auction-based\nalgorithms without sacrificing efficiency. We contend that our proposed method\ncan be effectively applied in real-time ad allocation scenarios and as an\noffline benchmark for evaluating the fairness-efficiency trade-off of existing\nauction-based systems.",
    "published_date": "2025-02-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.01862v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.01847v2",
    "title": "Containment Control Approach for Steering Opinion in a Social Network",
    "authors": [
      "Hossein Rastgoftar"
    ],
    "author_ids": [],
    "abstract": "The paper studies the problem of steering multi-dimensional opinion in a\nsocial network. Assuming the society of desire consists of stubborn and regular\nagents, stubborn agents are considered as leaders who specify the desired\nopinion distribution as a distributed reward or utility function. In this\ncontext, each regular agent is seen as a follower, updating its bias on the\ninitial opinion and influence weights by averaging their observations of the\nrewards their influencers have received. Assuming random graphs with reducible\nand irreducible topology specify the influences on regular agents, opinion\nevolution is represented as a containment control problem in which stability\nand convergence to the final opinion are proven.",
    "published_date": "2025-02-03T00:00:00",
    "year": 2025,
    "categories": [
      "eess.SY",
      "cs.MA",
      "cs.SY",
      "math.DS",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.01847v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.01825v1",
    "title": "Assessing Data Augmentation-Induced Bias in Training and Testing of Machine Learning Models",
    "authors": [
      "Riddhi More",
      "Jeremy S. Bradbury"
    ],
    "author_ids": [],
    "abstract": "Data augmentation has become a standard practice in software engineering to\naddress limited or imbalanced data sets, particularly in specialized domains\nlike test classification and bug detection where data can be scarce. Although\ntechniques such as SMOTE and mutation-based augmentation are widely used in\nsoftware testing and debugging applications, a rigorous understanding of how\naugmented training data impacts model bias is lacking. It is especially\ncritical to consider bias in scenarios where augmented data sets are used not\njust in training but also in testing models. Through a comprehensive case study\nof flaky test classification, we demonstrate how to test for bias and\nunderstand the impact that the inclusion of augmented samples in testing sets\ncan have on model evaluation.",
    "published_date": "2025-02-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SE",
      "cs.AI",
      "D.2.5; I.2.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.01825v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.01719v3",
    "title": "MJ-VIDEO: Fine-Grained Benchmarking and Rewarding Video Preferences in Video Generation",
    "authors": [
      "Haibo Tong",
      "Zhaoyang Wang",
      "Zhaorun Chen",
      "Haonian Ji",
      "Shi Qiu",
      "Siwei Han",
      "Kexin Geng",
      "Zhongkai Xue",
      "Yiyang Zhou",
      "Peng Xia",
      "Mingyu Ding",
      "Rafael Rafailov",
      "Chelsea Finn",
      "Huaxiu Yao"
    ],
    "author_ids": [],
    "abstract": "Recent advancements in video generation have significantly improved the\nability to synthesize videos from text instructions. However, existing models\nstill struggle with key challenges such as instruction misalignment, content\nhallucination, safety concerns, and bias. Addressing these limitations, we\nintroduce MJ-BENCH-VIDEO, a large-scale video preference benchmark designed to\nevaluate video generation across five critical aspects: Alignment, Safety,\nFineness, Coherence & Consistency, and Bias & Fairness. This benchmark\nincorporates 28 fine-grained criteria to provide a comprehensive evaluation of\nvideo preference. Building upon this dataset, we propose MJ-VIDEO, a\nMixture-of-Experts (MoE)-based video reward model designed to deliver\nfine-grained reward. MJ-VIDEO can dynamically select relevant experts to\naccurately judge the preference based on the input text-video pair. This\narchitecture enables more precise and adaptable preference judgments. Through\nextensive benchmarking on MJ-BENCH-VIDEO, we analyze the limitations of\nexisting video reward models and demonstrate the superior performance of\nMJ-VIDEO in video preference assessment, achieving 17.58% and 15.87%\nimprovements in overall and fine-grained preference judgments, respectively.\nAdditionally, introducing MJ-VIDEO for preference tuning in video generation\nenhances the alignment performance. All our code, data, and models are\navailable at https://aiming-lab.github.io/MJ-VIDEO.github.io/.",
    "published_date": "2025-02-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.01719v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.01575v1",
    "title": "Heterogeneous Treatment Effect in Time-to-Event Outcomes: Harnessing Censored Data with Recursively Imputed Trees",
    "authors": [
      "Tomer Meir",
      "Uri Shalit",
      "Malka Gorfine"
    ],
    "author_ids": [],
    "abstract": "Tailoring treatments to individual needs is a central goal in fields such as\nmedicine. A key step toward this goal is estimating Heterogeneous Treatment\nEffects (HTE) - the way treatments impact different subgroups. While crucial,\nHTE estimation is challenging with survival data, where time until an event\n(e.g., death) is key. Existing methods often assume complete observation, an\nassumption violated in survival data due to right-censoring, leading to bias\nand inefficiency. Cui et al. (2023) proposed a doubly-robust method for HTE\nestimation in survival data under no hidden confounders, combining a causal\nsurvival forest with an augmented inverse-censoring weighting estimator.\nHowever, we find it struggles under heavy censoring, which is common in\nrare-outcome problems such as Amyotrophic lateral sclerosis (ALS). Moreover,\nmost current methods cannot handle instrumental variables, which are a crucial\ntool in the causal inference arsenal. We introduce Multiple Imputation for\nSurvival Treatment Response (MISTR), a novel, general, and non-parametric\nmethod for estimating HTE in survival data. MISTR uses recursively imputed\nsurvival trees to handle censoring without directly modeling the censoring\nmechanism. Through extensive simulations and analysis of two real-world\ndatasets-the AIDS Clinical Trials Group Protocol 175 and the Illinois\nunemployment dataset we show that MISTR outperforms prior methods under heavy\ncensoring in the no-hidden-confounders setting, and extends to the instrumental\nvariable setting. To our knowledge, MISTR is the first non-parametric approach\nfor HTE estimation with unobserved confounders via instrumental variables.",
    "published_date": "2025-02-03T00:00:00",
    "year": 2025,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.01575v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.01530v2",
    "title": "The in-context inductive biases of vision-language models differ across modalities",
    "authors": [
      "Kelsey Allen",
      "Ishita Dasgupta",
      "Eliza Kosoy",
      "Andrew K. Lampinen"
    ],
    "author_ids": [],
    "abstract": "Inductive biases are what allow learners to make guesses in the absence of\nconclusive evidence. These biases have often been studied in cognitive science\nusing concepts or categories -- e.g. by testing how humans generalize a new\ncategory from a few examples that leave the category boundary ambiguous. We use\nthese approaches to study generalization in foundation models during in-context\nlearning. Modern foundation models can condition on both vision and text, and\ndifferences in how they interpret and learn from these different modalities is\nan emerging area of study. Here, we study how their generalizations vary by the\nmodality in which stimuli are presented, and the way the stimuli are described\nin text. We study these biases with three different experimental paradigms,\nacross three different vision-language models. We find that the models\ngenerally show some bias towards generalizing according to shape over color.\nThis shape bias tends to be amplified when the examples are presented visually.\nBy contrast, when examples are presented in text, the ordering of adjectives\naffects generalization. However, the extent of these effects vary across models\nand paradigms. These results help to reveal how vision-language models\nrepresent different types of inputs in context, and may have practical\nimplications for the use of vision-language models.",
    "published_date": "2025-02-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.01530v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.01493v1",
    "title": "The Human-AI Handshake Framework: A Bidirectional Approach to Human-AI Collaboration",
    "authors": [
      "Aung Pyae"
    ],
    "author_ids": [],
    "abstract": "Human-AI collaboration is evolving from a tool-based perspective to a\npartnership model where AI systems complement and enhance human capabilities.\nTraditional approaches often limit AI to a supportive role, missing the\npotential for reciprocal relationships where both human and AI inputs\ncontribute to shared goals. Although Human-Centered AI (HcAI) frameworks\nemphasize transparency, ethics, and user experience, they often lack mechanisms\nfor genuine, dynamic collaboration. The \"Human-AI Handshake Model\" addresses\nthis gap by introducing a bi-directional, adaptive framework with five key\nattributes: information exchange, mutual learning, validation, feedback, and\nmutual capability augmentation. These attributes foster balanced interaction,\nenabling AI to act as a responsive partner, evolving with users over time.\nHuman enablers like user experience and trust, alongside AI enablers such as\nexplainability and responsibility, facilitate this collaboration, while shared\nvalues of ethics and co-evolution ensure sustainable growth. Distinct from\nexisting frameworks, this model is reflected in tools like GitHub Copilot and\nChatGPT, which support bi-directional learning and transparency. Challenges\nremain, including maintaining ethical standards and ensuring effective user\noversight. Future research will explore these challenges, aiming to create a\ntruly collaborative human-AI partnership that leverages the strengths of both\nto achieve outcomes beyond what either could accomplish alone.",
    "published_date": "2025-02-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.01493v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.01713v1",
    "title": "Auditing a Dutch Public Sector Risk Profiling Algorithm Using an Unsupervised Bias Detection Tool",
    "authors": [
      "Floris Holstege",
      "Mackenzie Jorgensen",
      "Kirtan Padh",
      "Jurriaan Parie",
      "Joel Persson",
      "Krsto Prorokovic",
      "Lukas Snoek"
    ],
    "author_ids": [],
    "abstract": "Algorithms are increasingly used to automate or aid human decisions, yet\nrecent research shows that these algorithms may exhibit bias across legally\nprotected demographic groups. However, data on these groups may be unavailable\nto organizations or external auditors due to privacy legislation. This paper\nstudies bias detection using an unsupervised clustering tool when data on\ndemographic groups are unavailable. We collaborate with the Dutch Executive\nAgency for Education to audit an algorithm that was used to assign risk scores\nto college students at the national level in the Netherlands between 2012-2023.\nOur audit covers more than 250,000 students from the whole country. The\nunsupervised clustering tool highlights known disparities between students with\na non-European migration background and Dutch origin. Our contributions are\nthree-fold: (1) we assess bias in a real-world, large-scale and high-stakes\ndecision-making process by a governmental organization; (2) we use simulation\nstudies to highlight potential pitfalls of using the unsupervised clustering\ntool to detect true bias when demographic group data are unavailable and\nprovide recommendations for valid inferences; (3) we provide the unsupervised\nclustering tool in an open-source library. Our work serves as a starting point\nfor a deliberative assessment by human experts to evaluate potential\ndiscrimination in algorithmic-supported decision-making processes.",
    "published_date": "2025-02-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.01713v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.01711v2",
    "title": "Expected Return Symmetries",
    "authors": [
      "Darius Muglich",
      "Johannes Forkel",
      "Elise van der Pol",
      "Jakob Foerster"
    ],
    "author_ids": [],
    "abstract": "Symmetry is an important inductive bias that can improve model robustness and\ngeneralization across many deep learning domains. In multi-agent settings, a\npriori known symmetries have been shown to address a fundamental coordination\nfailure mode known as mutually incompatible symmetry breaking; e.g. in a game\nwhere two independent agents can choose to move \"left'' or \"right'', and where\na reward of +1 or -1 is received when the agents choose the same action or\ndifferent actions, respectively. However, the efficient and automatic discovery\nof environment symmetries, in particular for decentralized partially observable\nMarkov decision processes, remains an open problem. Furthermore, environmental\nsymmetry breaking constitutes only one type of coordination failure, which\nmotivates the search for a more accessible and broader symmetry class. In this\npaper, we introduce such a broader group of previously unexplored symmetries,\nwhich we call expected return symmetries, which contains environment symmetries\nas a subgroup. We show that agents trained to be compatible under the group of\nexpected return symmetries achieve better zero-shot coordination results than\nthose using environment symmetries. As an additional benefit, our method makes\nminimal a priori assumptions about the structure of their environment and does\nnot require access to ground truth symmetries.",
    "published_date": "2025-02-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.01711v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.01406v1",
    "title": "GRADIEND: Monosemantic Feature Learning within Neural Networks Applied to Gender Debiasing of Transformer Models",
    "authors": [
      "Jonathan Drechsel",
      "Steffen Herbold"
    ],
    "author_ids": [],
    "abstract": "AI systems frequently exhibit and amplify social biases, including gender\nbias, leading to harmful consequences in critical areas. This study introduces\na novel encoder-decoder approach that leverages model gradients to learn a\nsingle monosemantic feature neuron encoding gender information. We show that\nour method can be used to debias transformer-based language models, while\nmaintaining other capabilities. We demonstrate the effectiveness of our\napproach across multiple encoder-only based models and highlight its potential\nfor broader applications.",
    "published_date": "2025-02-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.01406v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.01349v1",
    "title": "Bias Beware: The Impact of Cognitive Biases on LLM-Driven Product Recommendations",
    "authors": [
      "Giorgos Filandrianos",
      "Angeliki Dimitriou",
      "Maria Lymperaiou",
      "Konstantinos Thomas",
      "Giorgos Stamou"
    ],
    "author_ids": [],
    "abstract": "The advent of Large Language Models (LLMs) has revolutionized product\nrecommendation systems, yet their susceptibility to adversarial manipulation\nposes critical challenges, particularly in real-world commercial applications.\nOur approach is the first one to tap into human psychological principles,\nseamlessly modifying product descriptions, making these adversarial\nmanipulations hard to detect. In this work, we investigate cognitive biases as\nblack-box adversarial strategies, drawing parallels between their effects on\nLLMs and human purchasing behavior. Through extensive experiments on LLMs of\nvarying scales, we reveal significant vulnerabilities in their use as\nrecommenders, providing critical insights into safeguarding these systems.",
    "published_date": "2025-02-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.01349v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.01347v1",
    "title": "Spurious Correlations in High Dimensional Regression: The Roles of Regularization, Simplicity Bias and Over-Parameterization",
    "authors": [
      "Simone Bombari",
      "Marco Mondelli"
    ],
    "author_ids": [],
    "abstract": "Learning models have been shown to rely on spurious correlations between\nnon-predictive features and the associated labels in the training data, with\nnegative implications on robustness, bias and fairness. In this work, we\nprovide a statistical characterization of this phenomenon for high-dimensional\nregression, when the data contains a predictive core feature $x$ and a spurious\nfeature $y$. Specifically, we quantify the amount of spurious correlations $C$\nlearned via linear regression, in terms of the data covariance and the strength\n$\\lambda$ of the ridge regularization. As a consequence, we first capture the\nsimplicity of $y$ through the spectrum of its covariance, and its correlation\nwith $x$ through the Schur complement of the full data covariance. Next, we\nprove a trade-off between $C$ and the in-distribution test loss $L$, by showing\nthat the value of $\\lambda$ that minimizes $L$ lies in an interval where $C$ is\nincreasing. Finally, we investigate the effects of over-parameterization via\nthe random features model, by showing its equivalence to regularized linear\nregression. Our theoretical results are supported by numerical experiments on\nGaussian, Color-MNIST, and CIFAR-10 datasets.",
    "published_date": "2025-02-03T00:00:00",
    "year": 2025,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.01347v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.01267v2",
    "title": "Counterfactual Situation Testing: From Single to Multidimensional Discrimination",
    "authors": [
      "Jose M. Alvarez",
      "Salvatore Ruggieri"
    ],
    "author_ids": [],
    "abstract": "We present counterfactual situation testing (CST), a causal data mining\nframework for detecting individual discrimination in a dataset of classifier\ndecisions. CST answers the question ``what would have been the model outcome\nhad the individual, or complainant, been of a different protected status?'' It\nextends the legally-grounded situation testing (ST) of Thanh et al. (2011) by\noperationalizing the notion of \"fairness given the difference\" via\ncounterfactual reasoning. ST finds for each complainant similar protected and\nnon-protected instances in the dataset; constructs, respectively, a control and\ntest group; and compares the groups such that a difference in model outcomes\nimplies a potential case of individual discrimination. CST, instead, avoids\nthis idealized comparison by establishing the test group on the complainant's\ngenerated counterfactual, which reflects how the protected attribute when\nchanged influences other seemingly neutral attributes of the complainant. Under\nCST we test for discrimination for each complainant by comparing similar\nindividuals within the control and test group but dissimilar individuals across\nthese groups. We consider single (e.g.,~gender) and multidimensional\n(e.g.,~gender and race) discrimination testing. For multidimensional\ndiscrimination we study multiple and intersectional discrimination and, as\nfeared by legal scholars, find evidence that the former fails to account for\nthe latter kind. Using a k-nearest neighbor implementation, we showcase CST on\nsynthetic and real data. Experimental results show that CST uncovers a higher\nnumber of cases than ST, even when the model is counterfactually fair. CST, in\nfact, extends counterfactual fairness (CF) of Kusner et al. (2017) by equipping\nCF with confidence intervals, which we report for all experiments.",
    "published_date": "2025-02-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.01267v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.01211v1",
    "title": "Privilege Scores",
    "authors": [
      "Ludwig Bothmann",
      "Philip A. Boustani",
      "Jose M. Alvarez",
      "Giuseppe Casalicchio",
      "Bernd Bischl",
      "Susanne Dandl"
    ],
    "author_ids": [],
    "abstract": "Bias-transforming methods of fairness-aware machine learning aim to correct a\nnon-neutral status quo with respect to a protected attribute (PA). Current\nmethods, however, lack an explicit formulation of what drives non-neutrality.\nWe introduce privilege scores (PS) to measure PA-related privilege by comparing\nthe model predictions in the real world with those in a fair world in which the\ninfluence of the PA is removed. At the individual level, PS can identify\nindividuals who qualify for affirmative action; at the global level, PS can\ninform bias-transforming policies. After presenting estimation methods for PS,\nwe propose privilege score contributions (PSCs), an interpretation method that\nattributes the origin of privilege to mediating features and direct effects. We\nprovide confidence intervals for both PS and PSCs. Experiments on simulated and\nreal-world data demonstrate the broad applicability of our methods and provide\nnovel insights into gender and racial privilege in mortgage and college\nadmissions applications.",
    "published_date": "2025-02-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.01211v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.01188v1",
    "title": "FairUDT: Fairness-aware Uplift Decision Trees",
    "authors": [
      "Anam Zahid",
      "Abdur Rehman Ali",
      "Shaina Raza",
      "Rai Shahnawaz",
      "Faisal Kamiran",
      "Asim Karim"
    ],
    "author_ids": [],
    "abstract": "Training data used for developing machine learning classifiers can exhibit\nbiases against specific protected attributes. Such biases typically originate\nfrom historical discrimination or certain underlying patterns that\ndisproportionately under-represent minority groups, such as those identified by\ntheir gender, religion, or race. In this paper, we propose a novel approach,\nFairUDT, a fairness-aware Uplift-based Decision Tree for discrimination\nidentification. FairUDT demonstrates how the integration of uplift modeling\nwith decision trees can be adapted to include fair splitting criteria.\nAdditionally, we introduce a modified leaf relabeling approach for removing\ndiscrimination. We divide our dataset into favored and deprived groups based on\na binary sensitive attribute, with the favored dataset serving as the treatment\ngroup and the deprived dataset as the control group. By applying FairUDT and\nour leaf relabeling approach to preprocess three benchmark datasets, we achieve\nan acceptable accuracy-discrimination tradeoff. We also show that FairUDT is\ninherently interpretable and can be utilized in discrimination detection tasks.\nThe code for this project is available https://github.com/ara-25/FairUDT",
    "published_date": "2025-02-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.01188v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.01148v1",
    "title": "A Discontinuous Galerkin Method for H(curl)-Elliptic Hemivariational Inequalities",
    "authors": [
      "Xiajie Huang",
      "Fei Wang",
      "Weimin Han",
      "Min Ling"
    ],
    "author_ids": [],
    "abstract": "In this paper, we develop a Discontinuous Galerkin (DG) method for solving\nH(curl)-elliptic hemivariational inequalities. By selecting an appropriate\nnumerical flux, we construct an Interior Penalty Discontinuous Galerkin (IPDG)\nscheme. A comprehensive numerical analysis of the IPDG method is conducted,\naddressing key aspects such as consistency, boundedness, stability, and the\nexistence, uniqueness, uniform boundedness of the numerical solutions. Building\non these properties, we establish a priori error estimates, demonstrating the\noptimal convergence order of the numerical solutions under suitable solution\nregularity assumptions. Finally, a numerical example is presented to illustrate\nthe theoretically predicted convergence order and to show the effectiveness of\nthe proposed method.",
    "published_date": "2025-02-03T00:00:00",
    "year": 2025,
    "categories": [
      "math.NA",
      "cs.NA",
      "math.AP",
      "65N30 (Primary), 35Q61, 49J40, 49J52 (Secondary)"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.01148v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.04342v2",
    "title": "Tutorial on Using Machine Learning and Deep Learning Models for Mental Illness Detection",
    "authors": [
      "Yeyubei Zhang",
      "Zhongyan Wang",
      "Zhanyi Ding",
      "Yexin Tian",
      "Jianglai Dai",
      "Xiaorui Shen",
      "Yunchong Liu",
      "Yuchen Cao"
    ],
    "author_ids": [],
    "abstract": "Social media has become an important source for understanding mental health,\nproviding researchers with a way to detect conditions like depression from\nuser-generated posts. This tutorial provides practical guidance to address\ncommon challenges in applying machine learning and deep learning methods for\nmental health detection on these platforms. It focuses on strategies for\nworking with diverse datasets, improving text preprocessing, and addressing\nissues such as imbalanced data and model evaluation. Real-world examples and\nstep-by-step instructions demonstrate how to apply these techniques\neffectively, with an emphasis on transparency, reproducibility, and ethical\nconsiderations. By sharing these approaches, this tutorial aims to help\nresearchers build more reliable and widely applicable models for mental health\nresearch, contributing to better tools for early detection and intervention.",
    "published_date": "2025-02-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.04342v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.01056v1",
    "title": "Mitigating Hallucinations in Large Vision-Language Models with Internal Fact-based Contrastive Decoding",
    "authors": [
      "Chao Wang",
      "Xuancheng Zhou",
      "Weiwei Fu",
      "Yang Zhou"
    ],
    "author_ids": [],
    "abstract": "Large Visual Language Models (LVLMs) integrate visual and linguistic\nmodalities, exhibiting exceptional performance across various multimodal tasks.\nNevertheless, LVLMs remain vulnerable to the issue of object hallucinations.\nPrevious efforts to mitigate this issue focus on supervised fine-tuning (SFT)\nor incorporating external knowledge, both of which entail significant costs\nrelated to training and the acquisition of external data. To address these\nchallenges, we propose a novel model-agnostic approach termed Internal\nFact-based Contrastive Decoding (IFCD), designed to mitigate and suppress\nhallucinations during the inference process of LVLMs by exploiting the LVLMs'\nown hallucinations. IFCD is grounded in experimental observations that\nalterations to the LVLMs' internal representations tend to amplify\nhallucinations caused by language bias. By contrasting disturbed distribution,\nIFCD calibrates the LVLMs' output and effectively removes the hallucinatory\nlogits from the final predictions. Experimental results validate that IFCD\nsignificantly alleviates both object-level and attribute-level hallucinations\nwhile achieving an average 9% accuracy improvement on POPE and 8% accuracy\nimprovement on MME object hallucinations subset compared with direct decoding,\nrespectively.",
    "published_date": "2025-02-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.01056v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.00915v1",
    "title": "A Variational Inequality Approach to Independent Learning in Static Mean-Field Games",
    "authors": [
      "Batuhan Yardim",
      "Semih Cayci",
      "Niao He"
    ],
    "author_ids": [],
    "abstract": "Competitive games involving thousands or even millions of players are\nprevalent in real-world contexts, such as transportation, communications, and\ncomputer networks. However, learning in these large-scale multi-agent\nenvironments presents a grand challenge, often referred to as the \"curse of\nmany agents\". In this paper, we formalize and analyze the Static Mean-Field\nGame (SMFG) under both full and bandit feedback, offering a generic framework\nfor modeling large population interactions while enabling independent learning.\n  We first establish close connections between SMFG and variational inequality\n(VI), showing that SMFG can be framed as a VI problem in the infinite agent\nlimit. Building on the VI perspective, we propose independent learning and\nexploration algorithms that efficiently converge to approximate Nash\nequilibria, when dealing with a finite number of agents. Theoretically, we\nprovide explicit finite sample complexity guarantees for independent learning\nacross various feedback models in repeated play scenarios, assuming\n(strongly-)monotone payoffs. Numerically, we validate our results through both\nsimulations and real-world applications in city traffic and network access\nmanagement.",
    "published_date": "2025-02-02T00:00:00",
    "year": 2025,
    "categories": [
      "math.OC",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.00915v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.00869v1",
    "title": "STAF: Sinusoidal Trainable Activation Functions for Implicit Neural Representation",
    "authors": [
      "Alireza Morsali",
      "MohammadJavad Vaez",
      "Hossein Soltani",
      "Amirhossein Kazerouni",
      "Babak Taati",
      "Morteza Mohammad-Noori"
    ],
    "author_ids": [],
    "abstract": "Implicit Neural Representations (INRs) have emerged as a powerful framework\nfor modeling continuous signals. The spectral bias of ReLU-based networks is a\nwell-established limitation, restricting their ability to capture fine-grained\ndetails in target signals. While previous works have attempted to mitigate this\nissue through frequency-based encodings or architectural modifications, these\napproaches often introduce additional complexity and do not fully address the\nunderlying challenge of learning high-frequency components efficiently. We\nintroduce Sinusoidal Trainable Activation Functions (STAF), designed to\ndirectly tackle this limitation by enabling networks to adaptively learn and\nrepresent complex signals with higher precision and efficiency. STAF inherently\nmodulates its frequency components, allowing for self-adaptive spectral\nlearning. This capability significantly improves convergence speed and\nexpressivity, making STAF highly effective for both signal representations and\ninverse problems. Through extensive evaluations, we demonstrate that STAF\noutperforms state-of-the-art (SOTA) methods in accuracy and reconstruction\nfidelity with superior Peak Signal-to-Noise Ratio (PSNR). These results\nestablish STAF as a robust solution for overcoming spectral bias and the\ncapacity-convergence gap, making it valuable for computer graphics and related\nfields. Our codebase is publicly accessible on the\nhttps://github.com/AlirezaMorsali/STAF.",
    "published_date": "2025-02-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.00869v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.00814v1",
    "title": "Disentangling Length Bias In Preference Learning Via Response-Conditioned Modeling",
    "authors": [
      "Jianfeng Cai",
      "Jinhua Zhu",
      "Ruopei Sun",
      "Yue Wang",
      "Li Li",
      "Wengang Zhou",
      "Houqiang Li"
    ],
    "author_ids": [],
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) has achieved considerable\nsuccess in aligning large language models (LLMs) by modeling human preferences\nwith a learnable reward model and employing a reinforcement learning algorithm\nto maximize the reward model's scores. However, these reward models are\nsusceptible to exploitation through various superficial confounding factors,\nwith length bias emerging as a particularly significant concern. Moreover,\nwhile the pronounced impact of length bias on preference modeling suggests that\nLLMs possess an inherent sensitivity to length perception, our preliminary\ninvestigations reveal that fine-tuned LLMs consistently struggle to adhere to\nexplicit length instructions. To address these two limitations, we propose a\nnovel framework wherein the reward model explicitly differentiates between\nhuman semantic preferences and response length requirements. Specifically, we\nintroduce a Response-conditioned Bradley-Terry (Rc-BT) model that enhances the\nreward model's capability in length bias mitigating and length instruction\nfollowing, through training on our augmented dataset. Furthermore, we propose\nthe Rc-DPO algorithm to leverage the Rc-BT model for direct policy optimization\n(DPO) of LLMs, simultaneously mitigating length bias and promoting adherence to\nlength instructions. Extensive evaluations demonstrate that our approach\nsubstantially improves both preference modeling and length instruction\ncompliance, with its effectiveness validated across various foundational models\nand preference datasets.",
    "published_date": "2025-02-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.00814v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.00808v1",
    "title": "Synthetic Artifact Auditing: Tracing LLM-Generated Synthetic Data Usage in Downstream Applications",
    "authors": [
      "Yixin Wu",
      "Ziqing Yang",
      "Yun Shen",
      "Michael Backes",
      "Yang Zhang"
    ],
    "author_ids": [],
    "abstract": "Large language models (LLMs) have facilitated the generation of high-quality,\ncost-effective synthetic data for developing downstream models and conducting\nstatistical analyses in various domains. However, the increased reliance on\nsynthetic data may pose potential negative impacts. Numerous studies have\ndemonstrated that LLM-generated synthetic data can perpetuate and even amplify\nsocietal biases and stereotypes, and produce erroneous outputs known as\n``hallucinations'' that deviate from factual knowledge. In this paper, we aim\nto audit artifacts, such as classifiers, generators, or statistical plots, to\nidentify those trained on or derived from synthetic data and raise user\nawareness, thereby reducing unexpected consequences and risks in downstream\napplications. To this end, we take the first step to introduce synthetic\nartifact auditing to assess whether a given artifact is derived from\nLLM-generated synthetic data. We then propose an auditing framework with three\nmethods including metric-based auditing, tuning-based auditing, and\nclassification-based auditing. These methods operate without requiring the\nartifact owner to disclose proprietary training details. We evaluate our\nauditing framework on three text classification tasks, two text summarization\ntasks, and two data visualization tasks across three training scenarios. Our\nevaluation demonstrates the effectiveness of all proposed auditing methods\nacross all these tasks. For instance, black-box metric-based auditing can\nachieve an average accuracy of $0.868 \\pm 0.071$ for auditing classifiers and\n$0.880 \\pm 0.052$ for auditing generators using only 200 random queries across\nthree scenarios. We hope our research will enhance model transparency and\nregulatory compliance, ensuring the ethical and responsible use of synthetic\ndata.",
    "published_date": "2025-02-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.00808v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.00802v1",
    "title": "Fisher-Guided Selective Forgetting: Mitigating The Primacy Bias in Deep Reinforcement Learning",
    "authors": [
      "Massimiliano Falzari",
      "Matthia Sabatelli"
    ],
    "author_ids": [],
    "abstract": "Deep Reinforcement Learning (DRL) systems often tend to overfit to early\nexperiences, a phenomenon known as the primacy bias (PB). This bias can\nseverely hinder learning efficiency and final performance, particularly in\ncomplex environments. This paper presents a comprehensive investigation of PB\nthrough the lens of the Fisher Information Matrix (FIM). We develop a framework\ncharacterizing PB through distinct patterns in the FIM trace, identifying\ncritical memorization and reorganization phases during learning. Building on\nthis understanding, we propose Fisher-Guided Selective Forgetting (FGSF), a\nnovel method that leverages the geometric structure of the parameter space to\nselectively modify network weights, preventing early experiences from\ndominating the learning process. Empirical results across DeepMind Control\nSuite (DMC) environments show that FGSF consistently outperforms baselines,\nparticularly in complex tasks. We analyze the different impacts of PB on actor\nand critic networks, the role of replay ratios in exacerbating the effect, and\nthe effectiveness of even simple noise injection methods. Our findings provide\na deeper understanding of PB and practical mitigation strategies, offering a\nFIM-based geometric perspective for advancing DRL.",
    "published_date": "2025-02-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.00802v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.00690v1",
    "title": "Dissecting Submission Limit in Desk-Rejections: A Mathematical Analysis of Fairness in AI Conference Policies",
    "authors": [
      "Yuefan Cao",
      "Xiaoyu Li",
      "Yingyu Liang",
      "Zhizhou Sha",
      "Zhenmei Shi",
      "Zhao Song",
      "Jiahao Zhang"
    ],
    "author_ids": [],
    "abstract": "As AI research surges in both impact and volume, conferences have imposed\nsubmission limits to maintain paper quality and alleviate organizational\npressure. In this work, we examine the fairness of desk-rejection systems under\nsubmission limits and reveal that existing practices can result in substantial\ninequities. Specifically, we formally define the paper submission limit problem\nand identify a critical dilemma: when the number of authors exceeds three, it\nbecomes impossible to reject papers solely based on excessive submissions\nwithout negatively impacting innocent authors. Thus, this issue may unfairly\naffect early-career researchers, as their submissions may be penalized due to\nco-authors with significantly higher submission counts, while senior\nresearchers with numerous papers face minimal consequences. To address this, we\npropose an optimization-based fairness-aware desk-rejection mechanism and\nformally define two fairness metrics: individual fairness and group fairness.\nWe prove that optimizing individual fairness is NP-hard, whereas group fairness\ncan be efficiently optimized via linear programming. Through case studies, we\ndemonstrate that our proposed system ensures greater equity than existing\nmethods, including those used in CVPR 2025, offering a more socially just\napproach to managing excessive submissions in AI conferences.",
    "published_date": "2025-02-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.DL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.00690v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.05206v3",
    "title": "Safety at Scale: A Comprehensive Survey of Large Model Safety",
    "authors": [
      "Xingjun Ma",
      "Yifeng Gao",
      "Yixu Wang",
      "Ruofan Wang",
      "Xin Wang",
      "Ye Sun",
      "Yifan Ding",
      "Hengyuan Xu",
      "Yunhao Chen",
      "Yunhan Zhao",
      "Hanxun Huang",
      "Yige Li",
      "Jiaming Zhang",
      "Xiang Zheng",
      "Yang Bai",
      "Zuxuan Wu",
      "Xipeng Qiu",
      "Jingfeng Zhang",
      "Yiming Li",
      "Xudong Han",
      "Haonan Li",
      "Jun Sun",
      "Cong Wang",
      "Jindong Gu",
      "Baoyuan Wu",
      "Siheng Chen",
      "Tianwei Zhang",
      "Yang Liu",
      "Mingming Gong",
      "Tongliang Liu",
      "Shirui Pan",
      "Cihang Xie",
      "Tianyu Pang",
      "Yinpeng Dong",
      "Ruoxi Jia",
      "Yang Zhang",
      "Shiqing Ma",
      "Xiangyu Zhang",
      "Neil Gong",
      "Chaowei Xiao",
      "Sarah Erfani",
      "Tim Baldwin",
      "Bo Li",
      "Masashi Sugiyama",
      "Dacheng Tao",
      "James Bailey",
      "Yu-Gang Jiang"
    ],
    "author_ids": [],
    "abstract": "The rapid advancement of large models, driven by their exceptional abilities\nin learning and generalization through large-scale pre-training, has reshaped\nthe landscape of Artificial Intelligence (AI). These models are now\nfoundational to a wide range of applications, including conversational AI,\nrecommendation systems, autonomous driving, content generation, medical\ndiagnostics, and scientific discovery. However, their widespread deployment\nalso exposes them to significant safety risks, raising concerns about\nrobustness, reliability, and ethical implications. This survey provides a\nsystematic review of current safety research on large models, covering Vision\nFoundation Models (VFMs), Large Language Models (LLMs), Vision-Language\nPre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models\n(DMs), and large-model-based Agents. Our contributions are summarized as\nfollows: (1) We present a comprehensive taxonomy of safety threats to these\nmodels, including adversarial attacks, data poisoning, backdoor attacks,\njailbreak and prompt injection attacks, energy-latency attacks, data and model\nextraction attacks, and emerging agent-specific threats. (2) We review defense\nstrategies proposed for each type of attacks if available and summarize the\ncommonly used datasets and benchmarks for safety research. (3) Building on\nthis, we identify and discuss the open challenges in large model safety,\nemphasizing the need for comprehensive safety evaluations, scalable and\neffective defense mechanisms, and sustainable data practices. More importantly,\nwe highlight the necessity of collective efforts from the research community\nand international collaboration. Our work can serve as a useful reference for\nresearchers and practitioners, fostering the ongoing development of\ncomprehensive defense systems and platforms to safeguard AI models.",
    "published_date": "2025-02-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.05206v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.00662v1",
    "title": "Mitigating the Modality Gap: Few-Shot Out-of-Distribution Detection with Multi-modal Prototypes and Image Bias Estimation",
    "authors": [
      "Yimu Wang",
      "Evelien Riddell",
      "Adrian Chow",
      "Sean Sedwards",
      "Krzysztof Czarnecki"
    ],
    "author_ids": [],
    "abstract": "Existing vision-language model (VLM)-based methods for out-of-distribution\n(OOD) detection typically rely on similarity scores between input images and\nin-distribution (ID) text prototypes. However, the modality gap between image\nand text often results in high false positive rates, as OOD samples can exhibit\nhigh similarity to ID text prototypes. To mitigate the impact of this modality\ngap, we propose incorporating ID image prototypes along with ID text\nprototypes. We present theoretical analysis and empirical evidence indicating\nthat this approach enhances VLM-based OOD detection performance without any\nadditional training. To further reduce the gap between image and text, we\nintroduce a novel few-shot tuning framework, SUPREME, comprising biased prompts\ngeneration (BPG) and image-text consistency (ITC) modules. BPG enhances\nimage-text fusion and improves generalization by conditioning ID text\nprototypes on the Gaussian-based estimated image domain bias; ITC reduces the\nmodality gap by minimizing intra- and inter-modal distances. Moreover, inspired\nby our theoretical and empirical findings, we introduce a novel OOD score\n$S_{\\textit{GMP}}$, leveraging uni- and cross-modal similarities. Finally, we\npresent extensive experiments to demonstrate that SUPREME consistently\noutperforms existing VLM-based OOD detection methods.",
    "published_date": "2025-02-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.00662v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.01679v1",
    "title": "LIBRA: Measuring Bias of Large Language Model from a Local Context",
    "authors": [
      "Bo Pang",
      "Tingrui Qiao",
      "Caroline Walker",
      "Chris Cunningham",
      "Yun Sing Koh"
    ],
    "author_ids": [],
    "abstract": "Large Language Models (LLMs) have significantly advanced natural language\nprocessing applications, yet their widespread use raises concerns regarding\ninherent biases that may reduce utility or harm for particular social groups.\nDespite the advancement in addressing LLM bias, existing research has two major\nlimitations. First, existing LLM bias evaluation focuses on the U.S. cultural\ncontext, making it challenging to reveal stereotypical biases of LLMs toward\nother cultures, leading to unfair development and use of LLMs. Second, current\nbias evaluation often assumes models are familiar with the target social\ngroups. When LLMs encounter words beyond their knowledge boundaries that are\nunfamiliar in their training data, they produce irrelevant results in the local\ncontext due to hallucinations and overconfidence, which are not necessarily\nindicative of inherent bias. This research addresses these limitations with a\nLocal Integrated Bias Recognition and Assessment Framework (LIBRA) for\nmeasuring bias using datasets sourced from local corpora without crowdsourcing.\nImplementing this framework, we develop a dataset comprising over 360,000 test\ncases in the New Zealand context. Furthermore, we propose the Enhanced\nIdealized CAT Score (EiCAT), integrating the iCAT score with a beyond knowledge\nboundary score (bbs) and a distribution divergence-based bias measurement to\ntackle the challenge of LLMs encountering words beyond knowledge boundaries.\nOur results show that the BERT family, GPT-2, and Llama-3 models seldom\nunderstand local words in different contexts. While Llama-3 exhibits larger\nbias, it responds better to different cultural contexts. The code and dataset\nare available at: https://github.com/ipangbo/LIBRA.",
    "published_date": "2025-02-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.01679v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.00637v1",
    "title": "Constructing AI ethics narratives based on real-world data: Human-AI collaboration in data-driven visual storytelling",
    "authors": [
      "Mengyi Wei",
      "Chenjing Jiao",
      "Chenyu Zuo",
      "Lorenz Hurni",
      "Liqiu Meng"
    ],
    "author_ids": [],
    "abstract": "AI ethics narratives have the potential to shape the public accurate\nunderstanding of AI technologies and promote communication among different\nstakeholders. However, AI ethics narratives are largely lacking. Existing\nlimited narratives tend to center on works of science fiction or corporate\nmarketing campaigns of large technology companies. Misuse of \"socio-technical\nimaginary\" can blur the line between speculation and reality for the public,\nundermining the responsibility and regulation of technology development.\nTherefore, constructing authentic AI ethics narratives is an urgent task. The\nemergence of generative AI offers new possibilities for building narrative\nsystems. This study is dedicated to data-driven visual storytelling about AI\nethics relying on the human-AI collaboration. Based on the five key elements of\nstory models, we proposed a conceptual framework for human-AI collaboration,\nexplored the roles of generative AI and humans in the creation of visual\nstories. We implemented the conceptual framework in a real AI news case. This\nresearch leveraged advanced generative AI technologies to provide a reference\nfor constructing genuine AI ethics narratives. Our goal is to promote active\npublic engagement and discussions through authentic AI ethics narratives,\nthereby contributing to the development of better AI policies.",
    "published_date": "2025-02-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.00637v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.00627v1",
    "title": "Discord Unveiled: A Comprehensive Dataset of Public Communication (2015-2024)",
    "authors": [
      "Yan Aquino",
      "Pedro Bento",
      "Arthur Buzelin",
      "Lucas Dayrell",
      "Samira Malaquias",
      "Caio Santana",
      "Victoria Estanislau",
      "Pedro Dutenhefner",
      "Guilherme H. G. Evangelista",
      "Luisa G. Porfírio",
      "Caio Souza Grossi",
      "Pedro B. Rigueira",
      "Virgilio Almeida",
      "Gisele L. Pappa",
      "Wagner Meira Jr"
    ],
    "author_ids": [],
    "abstract": "Discord has evolved from a gaming-focused communication tool into a versatile\nplatform supporting diverse online communities. Despite its large user base and\nactive public servers, academic research on Discord remains limited due to data\naccessibility challenges. This paper introduces Discord Unveiled: A\nComprehensive Dataset of Public Communication (2015-2024), the most extensive\nDiscord public server's data to date. The dataset comprises over 2.05 billion\nmessages from 4.74 million users across 3,167 public servers, representing\napproximately 10% of servers listed in Discord's Discovery feature. Spanning\nfrom Discord's launch in 2015 to the end of 2024, it offers a robust temporal\nand thematic framework for analyzing decentralized moderation, community\ngovernance, information dissemination, and social dynamics. Data was collected\nthrough Discord's public API, adhering to ethical guidelines and privacy\nstandards via anonymization techniques. Organized into structured JSON files,\nthe dataset facilitates seamless integration with computational social science\nmethodologies. Preliminary analyses reveal significant trends in user\nengagement, bot utilization, and linguistic diversity, with English\npredominating alongside substantial representations of Spanish, French, and\nPortuguese. Additionally, prevalent community themes such as social, art,\nmusic, and memes highlight Discord's expansion beyond its gaming origins.",
    "published_date": "2025-02-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SI",
      "cs.DB"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.00627v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.01677v1",
    "title": "AI Scaling: From Up to Down and Out",
    "authors": [
      "Yunke Wang",
      "Yanxi Li",
      "Chang Xu"
    ],
    "author_ids": [],
    "abstract": "AI Scaling has traditionally been synonymous with Scaling Up, which builds\nlarger and more powerful models. However, the growing demand for efficiency,\nadaptability, and collaboration across diverse applications necessitates a\nbroader perspective. This position paper presents a holistic framework for AI\nscaling, encompassing Scaling Up, Scaling Down, and Scaling Out. It argues that\nwhile Scaling Up of models faces inherent bottlenecks, the future trajectory of\nAI scaling lies in Scaling Down and Scaling Out. These paradigms address\ncritical technical and societal challenges, such as reducing carbon footprint,\nensuring equitable access, and enhancing cross-domain collaboration. We explore\ntransformative applications in healthcare, smart manufacturing, and content\ncreation, demonstrating how AI Scaling can enable breakthroughs in efficiency,\npersonalization, and global connectivity. Additionally, we highlight key\nchallenges, including balancing model complexity with interpretability,\nmanaging resource constraints, and fostering ethical development. By\nsynthesizing these approaches, we propose a unified roadmap that redefines the\nfuture of AI research and application, paving the way for advancements toward\nArtificial General Intelligence (AGI).",
    "published_date": "2025-02-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.01677v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.00619v1",
    "title": "Distribution-aware Fairness Learning in Medical Image Segmentation From A Control-Theoretic Perspective",
    "authors": [
      "Yujin Oh",
      "Pengfei Jin",
      "Sangjoon Park",
      "Sekeun Kim",
      "Siyeop Yoon",
      "Kyungsang Kim",
      "Jin Sung Kim",
      "Xiang Li",
      "Quanzheng Li"
    ],
    "author_ids": [],
    "abstract": "Ensuring fairness in medical image segmentation is critical due to biases in\nimbalanced clinical data acquisition caused by demographic attributes (e.g.,\nage, sex, race) and clinical factors (e.g., disease severity). To address these\nchallenges, we introduce Distribution-aware Mixture of Experts (dMoE), inspired\nby optimal control theory. We provide a comprehensive analysis of its\nunderlying mechanisms and clarify dMoE's role in adapting to heterogeneous\ndistributions in medical image segmentation. Furthermore, we integrate dMoE\ninto multiple network architectures, demonstrating its broad applicability\nacross diverse medical image analysis tasks. By incorporating demographic and\nclinical factors, dMoE achieves state-of-the-art performance on two 2D\nbenchmark datasets and a 3D in-house dataset. Our results highlight the\neffectiveness of dMoE in mitigating biases from imbalanced distributions,\noffering a promising approach to bridging control theory and medical image\nsegmentation within fairness learning paradigms. The source code will be made\navailable.",
    "published_date": "2025-02-02T00:00:00",
    "year": 2025,
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.00619v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.04733v1",
    "title": "Ethics of generative AI and manipulation: a design-oriented research agenda",
    "authors": [
      "Michael Klenk"
    ],
    "author_ids": [],
    "abstract": "Generative AI enables automated, effective manipulation at scale. Despite the\ngrowing general ethical discussion around generative AI, the specific\nmanipulation risks remain inadequately investigated. This article outlines\nessential inquiries encompassing conceptual, empirical, and design dimensions\nof manipulation, pivotal for comprehending and curbing manipulation risks. By\nhighlighting these questions, the article underscores the necessity of an\nappropriate conceptualisation of manipulation to ensure the responsible\ndevelopment of Generative AI technologies.",
    "published_date": "2025-02-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.04733v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.00385v2",
    "title": "The Impact of Persona-based Political Perspectives on Hateful Content Detection",
    "authors": [
      "Stefano Civelli",
      "Pietro Bernardelle",
      "Gianluca Demartini"
    ],
    "author_ids": [],
    "abstract": "While pretraining language models with politically diverse content has been\nshown to improve downstream task fairness, such approaches require significant\ncomputational resources often inaccessible to many researchers and\norganizations. Recent work has established that persona-based prompting can\nintroduce political diversity in model outputs without additional training.\nHowever, it remains unclear whether such prompting strategies can achieve\nresults comparable to political pretraining for downstream tasks. We\ninvestigate this question using persona-based prompting strategies in\nmultimodal hate-speech detection tasks, specifically focusing on hate speech in\nmemes. Our analysis reveals that when mapping personas onto a political compass\nand measuring persona agreement, inherent political positioning has\nsurprisingly little correlation with classification decisions. Notably, this\nlack of correlation persists even when personas are explicitly injected with\nstronger ideological descriptors. Our findings suggest that while LLMs can\nexhibit political biases in their responses to direct political questions,\nthese biases may have less impact on practical classification tasks than\npreviously assumed. This raises important questions about the necessity of\ncomputationally expensive political pretraining for achieving fair performance\nin downstream tasks.",
    "published_date": "2025-02-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.00385v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.00339v1",
    "title": "Challenges and Innovations in LLM-Powered Fake News Detection: A Synthesis of Approaches and Future Directions",
    "authors": [
      "Jingyuan Yi",
      "Zeqiu Xu",
      "Tianyi Huang",
      "Peiyang Yu"
    ],
    "author_ids": [],
    "abstract": "The pervasiveness of the dissemination of fake news through social media\nplatforms poses critical risks to the trust of the general public, societal\nstability, and democratic institutions. This challenge calls for novel\nmethodologies in detection, which can keep pace with the dynamic and\nmulti-modal nature of misinformation. Recent works include powering the\ndetection using large language model advances in multimodal frameworks,\nmethodologies using graphs, and adversarial training in the literature of fake\nnews. Based on the different approaches which can bring success, some key\nhighlights will be underlined: enhanced LLM-improves accuracy through more\nadvanced semantics and cross-modality fusion for robust detections. The review\nfurther identifies critical gaps in adaptability to dynamic social media\ntrends, real-time, and cross-platform detection capabilities, as well as the\nethical challenges thrown up by the misuse of LLMs. Future directions underline\nthe development of style-agnostic models, cross-lingual detection frameworks,\nand robust policies with a view to mitigating LLM-driven misinformation. This\nsynthesis thus lays a concrete foundation for those researchers and\npractitioners committed to reinforcing fake news detection systems with\ncomplications that keep on growing in the digital landscape.",
    "published_date": "2025-02-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.00339v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.00313v1",
    "title": "Distributive Fairness in Large Language Models: Evaluating Alignment with Human Values",
    "authors": [
      "Hadi Hosseini",
      "Samarth Khanna"
    ],
    "author_ids": [],
    "abstract": "The growing interest in employing large language models (LLMs) for\ndecision-making in social and economic contexts has raised questions about\ntheir potential to function as agents in these domains. A significant number of\nsocietal problems involve the distribution of resources, where fairness, along\nwith economic efficiency, play a critical role in the desirability of outcomes.\nIn this paper, we examine whether LLM responses adhere to fundamental fairness\nconcepts such as equitability, envy-freeness, and Rawlsian maximin, and\ninvestigate their alignment with human preferences. We evaluate the performance\nof several LLMs, providing a comparative benchmark of their ability to reflect\nthese measures. Our results demonstrate a lack of alignment between current LLM\nresponses and human distributional preferences. Moreover, LLMs are unable to\nutilize money as a transferable resource to mitigate inequality. Nonetheless,\nwe demonstrate a stark contrast when (some) LLMs are tasked with selecting from\na predefined menu of options rather than generating one. In addition, we\nanalyze the robustness of LLM responses to variations in semantic factors (e.g.\nintentions or personas) or non-semantic prompting changes (e.g. templates or\norderings). Finally, we highlight potential strategies aimed at enhancing the\nalignment of LLM behavior with well-established fairness concepts.",
    "published_date": "2025-02-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.CL",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.00313v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.00289v3",
    "title": "Agentic AI: Autonomy, Accountability, and the Algorithmic Society",
    "authors": [
      "Anirban Mukherjee",
      "Hannah Hanwen Chang"
    ],
    "author_ids": [],
    "abstract": "Agentic Artificial Intelligence (AI) can autonomously pursue long-term goals,\nmake decisions, and execute complex, multi-turn workflows. Unlike traditional\ngenerative AI, which responds reactively to prompts, agentic AI proactively\norchestrates processes, such as autonomously managing complex tasks or making\nreal-time decisions. This transition from advisory roles to proactive execution\nchallenges established legal, economic, and creative frameworks. In this paper,\nwe explore challenges in three interrelated domains: creativity and\nintellectual property, legal and ethical considerations, and competitive\neffects. Central to our analysis is the tension between novelty and usefulness\nin AI-generated creative outputs, as well as the intellectual property and\nauthorship challenges arising from AI autonomy. We highlight gaps in\nresponsibility attribution and liability that create a \"moral crumple zone\"--a\ncondition where accountability is diffused across multiple actors, leaving\nend-users and developers in precarious legal and ethical positions. We examine\nthe competitive dynamics of two-sided algorithmic markets, where both sellers\nand buyers deploy AI agents, potentially mitigating or amplifying tacit\ncollusion risks. We explore the potential for emergent self-regulation within\nnetworks of agentic AI--the development of an \"algorithmic society\"--raising\ncritical questions: To what extent would these norms align with societal\nvalues? What unintended consequences might arise? How can transparency and\naccountability be ensured? Addressing these challenges will necessitate\ninterdisciplinary collaboration to redefine legal accountability, align\nAI-driven choices with stakeholder values, and maintain ethical safeguards. We\nadvocate for frameworks that balance autonomy with accountability, ensuring all\nparties can harness agentic AI's potential while preserving trust, fairness, &\nsocietal welfare.",
    "published_date": "2025-02-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.00289v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.00265v3",
    "title": "RADx Data Hub: A Cloud Platform for FAIR, Harmonized COVID-19 Data",
    "authors": [
      "Marcos Martinez-Romero",
      "Matthew Horridge",
      "Nilesh Mistry",
      "Aubrie Weyhmiller",
      "Jimmy K. Yu",
      "Alissa Fujimoto",
      "Aria Henry",
      "Martin J. O'Connor",
      "Ashley Sier",
      "Stephanie Suber",
      "Mete U. Akdogan",
      "Yan Cao",
      "Somu Valliappan",
      "Joanna O. Mieczkowska",
      "the RADx Data Hub team",
      "Ashok Krishnamurthy",
      "Michael A. Keller",
      "Mark A. Musen"
    ],
    "author_ids": [],
    "abstract": "The COVID-19 pandemic highlighted the urgent need for robust systems to\nenable rapid data collection, integration, and analysis for public health\nresponses. Existing approaches often relied on disparate, non-interoperable\nsystems, creating bottlenecks in comprehensive analyses and timely\ndecision-making. To address these challenges, the U.S. National Institutes of\nHealth (NIH) launched the Rapid Acceleration of Diagnostics (RADx) initiative\nin 2020, with the RADx Data Hub, a centralized repository for de-identified and\ncurated COVID-19 data, as its cornerstone. The RADx Data Hub hosts diverse\nstudy data, including clinical data, testing results, smart sensor outputs,\nself-reported symptoms, and information on social determinants of health. Built\non cloud infrastructure, the RADx Data Hub integrates metadata standards,\ninteroperable formats, and ontology-based tools to adhere to the FAIR\n(Findable, Accessible, Interoperable, Reusable) principles for data sharing.\nInitially developed for COVID-19 research, its architecture and processes are\nadaptable to other scientific disciplines. This paper provides an overview of\nthe data hosted by the RADx Data Hub and describes the platform's capabilities\nand architecture.",
    "published_date": "2025-02-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.DB"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.00265v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.00229v1",
    "title": "Enhancing Psychotherapeutic Alliance in College: When and How to Integrate Multimodal Large Language Models in Psychotherapy",
    "authors": [
      "Jiyao Wang",
      "Youyu Sheng",
      "Qihang He",
      "Haolong Hu",
      "Shuwen Liu",
      "Feiqi Gu",
      "Yumei Jing",
      "Dengbo He"
    ],
    "author_ids": [],
    "abstract": "As mental health issues rise among college students, there is an increasing\ninterest and demand in leveraging Multimodal Language Models (MLLM) to enhance\nmental support services, yet integrating them into psychotherapy remains\ntheoretical or non-user-centered. This study investigated the opportunities and\nchallenges of using MLLMs within the campus psychotherapy alliance in China.\nThrough three studies involving both therapists and student clients, we argue\nthat the ideal role for MLLMs at this stage is as an auxiliary tool to human\ntherapists. Users widely expect features such as triage matching and real-time\nemotion recognition. At the same time, for independent therapy by MLLM,\nconcerns about capabilities and privacy ethics remain prominent, despite high\ndemands for personalized avatars and non-verbal communication. Our findings\nfurther indicate that users' sense of social identity and perceived relative\nstatus of MLLMs significantly influence their acceptance. This study provides\ninsights for future intelligent campus mental healthcare.",
    "published_date": "2025-02-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.00229v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.00201v1",
    "title": "Year-over-Year Developments in Financial Fraud Detection via Deep Learning: A Systematic Literature Review",
    "authors": [
      "Yisong Chen",
      "Chuqing Zhao",
      "Yixin Xu",
      "Chuanhao Nie"
    ],
    "author_ids": [],
    "abstract": "This paper systematically reviews advancements in deep learning (DL)\ntechniques for financial fraud detection, a critical issue in the financial\nsector. Using the Kitchenham systematic literature review approach, 57 studies\npublished between 2019 and 2024 were analyzed. The review highlights the\neffectiveness of various deep learning models such as Convolutional Neural\nNetworks, Long Short-Term Memory, and transformers across domains such as\ncredit card transactions, insurance claims, and financial statement audits.\nPerformance metrics such as precision, recall, F1-score, and AUC-ROC were\nevaluated. Key themes explored include the impact of data privacy frameworks\nand advancements in feature engineering and data preprocessing. The study\nemphasizes challenges such as imbalanced datasets, model interpretability, and\nethical considerations, alongside opportunities for automation and\nprivacy-preserving techniques such as blockchain integration and Principal\nComponent Analysis. By examining trends over the past five years, this review\nidentifies critical gaps and promising directions for advancing DL applications\nin financial fraud detection, offering actionable insights for researchers and\npractitioners.",
    "published_date": "2025-01-31T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-fin.ST"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.00201v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.00168v2",
    "title": "Supervised Quadratic Feature Analysis: An Information Geometry Approach to Dimensionality Reduction",
    "authors": [
      "Daniel Herrera-Esposito",
      "Johannes Burge"
    ],
    "author_ids": [],
    "abstract": "Supervised dimensionality reduction aims to map labeled data to a\nlow-dimensional feature space while maximizing class discriminability. Despite\nthe availability of methods for learning complex non-linear features (e.g. Deep\nLearning), there is an enduring demand for dimensionality reduction methods\nthat learn linear features due to their interpretability, low computational\ncost, and broad applicability. However, there is a gap between methods that\noptimize linear separability (e.g. LDA), and more flexible but computationally\nexpensive methods that optimize over arbitrary class boundaries (e.g.\nmetric-learning methods). Here, we present Supervised Quadratic Feature\nAnalysis (SQFA), a dimensionality reduction method for learning linear features\nthat maximize the differences between class-conditional first- and second-order\nstatistics, which allow for quadratic discrimination. SQFA exploits the\ninformation geometry of second-order statistics in the symmetric positive\ndefinite manifold. We show that SQFA features support quadratic\ndiscriminability in real-world problems. We also provide a theoretical link,\nbased on information geometry, between SQFA and the Quadratic Discriminant\nAnalysis (QDA) classifier.",
    "published_date": "2025-01-31T00:00:00",
    "year": 2025,
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.DG",
      "math.ST",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.00168v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.00156v2",
    "title": "ALBAR: Adversarial Learning approach to mitigate Biases in Action Recognition",
    "authors": [
      "Joseph Fioresi",
      "Ishan Rajendrakumar Dave",
      "Mubarak Shah"
    ],
    "author_ids": [],
    "abstract": "Bias in machine learning models can lead to unfair decision making, and while\nit has been well-studied in the image and text domains, it remains\nunderexplored in action recognition. Action recognition models often suffer\nfrom background bias (i.e., inferring actions based on background cues) and\nforeground bias (i.e., relying on subject appearance), which can be detrimental\nto real-life applications such as autonomous vehicles or assisted living\nmonitoring. While prior approaches have mainly focused on mitigating background\nbias using specialized augmentations, we thoroughly study both foreground and\nbackground bias. We propose ALBAR, a novel adversarial training method that\nmitigates foreground and background biases without requiring specialized\nknowledge of the bias attributes. Our framework applies an adversarial\ncross-entropy loss to the sampled static clip (where all the frames are the\nsame) and aims to make its class probabilities uniform using a proposed entropy\nmaximization loss. Additionally, we introduce a gradient penalty loss for\nregularization against the debiasing process. We evaluate our method on\nestablished background and foreground bias protocols, setting a new\nstate-of-the-art and strongly improving combined debiasing performance by over\n12% absolute on HMDB51. Furthermore, we identify an issue of background leakage\nin the existing UCF101 protocol for bias evaluation which provides a shortcut\nto predict actions and does not provide an accurate measure of the debiasing\ncapability of a model. We address this issue by proposing more fine-grained\nsegmentation boundaries for the actor, where our method also outperforms\nexisting approaches. Project Page:\nhttps://joefioresi718.github.io/ALBAR_webpage/",
    "published_date": "2025-01-31T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.00156v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.00136v1",
    "title": "A Three-Branch Checks-and-Balances Frameworkfor Context-Aware Ethical Alignment of Large Language Models",
    "authors": [
      "Edward Y. Chang"
    ],
    "author_ids": [],
    "abstract": "This paper introduces a three-branch checks-and-balances framework for\nethical alignment of Large Language Models (LLMs), inspired by governmental\nsystems. It implements three independent yet interacting components: LLMs as\nthe executive branch for knowledge generation, DIKE as the legislative branch\nestablishing ethical guardrails, and ERIS as the judicial branch for contextual\ninterpretation. The adversarial DIKE-ERIS duality enables adaptation to diverse\ncultural contexts while upholding consistent ethical principles. This\narchitecture addresses limitations of reinforcement learning with human\nfeedback (RLHF) by providing interpretable, adaptable, and culturally-aware\nethical reasoning. Through self-supervised learning and adversarial testing,\nour framework demonstrates how emotional modeling can guide linguistic\nbehaviors toward ethical outcomes while preserving independence across\nknowledge generation, ethical oversight, and contextual interpretation.",
    "published_date": "2025-01-31T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "F.2.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.00136v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.00131v2",
    "title": "Middleman Bias in Advertising: Aligning Relevance of Keyphrase Recommendations with Search",
    "authors": [
      "Soumik Dey",
      "Wei Zhang",
      "Hansi Wu",
      "Bingfeng Dong",
      "Binbin Li"
    ],
    "author_ids": [],
    "abstract": "E-commerce sellers are recommended keyphrases based on their inventory on\nwhich they advertise to increase buyer engagement (clicks/sales). Keyphrases\nmust be pertinent to items; otherwise, it can result in seller dissatisfaction\nand poor targeting -- towards that end relevance filters are employed. In this\nwork, we describe the shortcomings of training relevance filter models on\nbiased click/sales signals. We re-conceptualize advertiser keyphrase relevance\nas interaction between two dynamical systems -- Advertising which produces the\nkeyphrases and Search which acts as a middleman to reach buyers. We discuss the\nbias of search relevance systems (middleman bias) and the need to align\nadvertiser keyphrases with search relevance signals. We also compare the\nperformance of cross encoders and bi-encoders in modeling this alignment and\nthe scalability of such a solution for sellers at eBay.",
    "published_date": "2025-01-31T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.00131v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.19337v1",
    "title": "Homogeneity Bias as Differential Sampling Uncertainty in Language Models",
    "authors": [
      "Messi H. J. Lee",
      "Soyeon Jeon"
    ],
    "author_ids": [],
    "abstract": "Prior research show that Large Language Models (LLMs) and Vision-Language\nModels (VLMs) represent marginalized groups more homogeneously than dominant\ngroups. However, the mechanisms underlying this homogeneity bias remain\nrelatively unexplored. We propose that this bias emerges from systematic\ndifferences in the probability distributions from which tokens are sampled at\ninference-time. Analyzing three measures of uncertainty in token sampling\ndistributions-entropy, perplexity, and probability of differentiation-we find\nthat in some models, specifically GPT-4 Turbo and Llama-3.2, tokens are sampled\nmore deterministically when generating texts about marginalized groups (i.e.,\nBlack Americans and women) compared to their dominant group counterparts (i.e.,\nWhite Americans and men). While these findings may help explain homogeneity\nbias in certain models, the patterns did not replicate across all VLMs tested,\nsuggesting multiple mechanisms may contribute to homogeneity bias in AI.",
    "published_date": "2025-01-31T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.19337v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.19321v1",
    "title": "Language Bias in Self-Supervised Learning For Automatic Speech Recognition",
    "authors": [
      "Edward Storey",
      "Naomi Harte",
      "Peter Bell"
    ],
    "author_ids": [],
    "abstract": "Self-supervised learning (SSL) is used in deep learning to train on large\ndatasets without the need for expensive labelling of the data. Recently, large\nAutomatic Speech Recognition (ASR) models such as XLS-R have utilised SSL to\ntrain on over one hundred different languages simultaneously. However, deeper\ninvestigation shows that the bulk of the training data for XLS-R comes from a\nsmall number of languages. Biases learned through SSL have been shown to exist\nin multiple domains, but language bias in multilingual SSL ASR has not been\nthoroughly examined. In this paper, we utilise the Lottery Ticket Hypothesis\n(LTH) to identify language-specific subnetworks within XLS-R and test the\nperformance of these subnetworks on a variety of different languages. We are\nable to show that when fine-tuning, XLS-R bypasses traditional linguistic\nknowledge and builds only on weights learned from the languages with the\nlargest data contribution to the pretraining data.",
    "published_date": "2025-01-31T00:00:00",
    "year": 2025,
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "eess.SP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.19321v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.19294v1",
    "title": "The Cost of Balanced Training-Data Production in an Online Data Market",
    "authors": [
      "Augustin Chaintreau",
      "Roland Maio",
      "Juba Ziani"
    ],
    "author_ids": [],
    "abstract": "Many ethical issues in machine learning are connected to the training data.\nOnline data markets are an important source of training data, facilitating both\nproduction and distribution. Recently, a trend has emerged of for-profit\n\"ethical\" participants in online data markets. This trend raises a fascinating\nquestion: Can online data markets sustainably and efficiently address ethical\nissues in the broader machine-learning economy?\n  In this work, we study this question in a stylized model of an online data\nmarket. We investigate the effects of intervening in the data market to achieve\nbalanced training-data production. The model reveals the crucial role of market\nconditions. In small and emerging markets, an intervention can drive the data\nproducers out of the market, so that the cost of fairness is maximal. Yet, in\nlarge and established markets, the cost of fairness can vanish (as a fraction\nof overall welfare) as the market grows.\n  Our results suggest that \"ethical\" online data markets can be economically\nfeasible under favorable market conditions, and motivate more models to\nconsider the role of data production and distribution in mediating the impacts\nof ethical interventions.",
    "published_date": "2025-01-31T00:00:00",
    "year": 2025,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.19294v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.19231v1",
    "title": "The geography of inequalities in access to healthcare across England: the role of bus travel time variability",
    "authors": [
      "Zihao Chen",
      "Federico Botta"
    ],
    "author_ids": [],
    "abstract": "Fair access to healthcare facilities is fundamental to achieving social\nequity. Traditional travel time-based accessibility measures often overlook the\ndynamic nature of travel times resulting from different departure times, which\ncompromises the accuracy of these measures in reflecting the true accessibility\nexperienced by individuals. This study examines public transport-based\naccessibility to healthcare facilities across England from the perspective of\ntravel time variability (TTV). Using comprehensive bus timetable data from the\nBus Open Data Service (BODS), we calculated hourly travel times from each Lower\nLayer Super Output Area (LSOA) to the nearest hospitals and general practices\nand developed a TTV metric for each LSOA and analysed its geographical\ninequalities across various spatial scales. Our analysis reveals notable\nspatial-temporal patterns of TTV and average travel times, including an\nurban-rural divide, clustering of high and low TTV regions, and distinct\noutliers. Furthermore, we explored the relationship between TTV and\ndeprivation, categorising LSOAs into four groups based on their unique\ncharacteristics, which provides valuable insights for designing targeted\ninterventions. Our study also highlights the limitations of using theoretical\nTTV derived from timetable data and emphasises the potential of using real-time\noperational data to capture more realistic accessibility measures. By offering\na more dynamic perspective on accessibility, our findings complement existing\ntravel time-based metrics and pave way for future research on TTV-based\naccessibility using real-time data. This evidence-based approach can inform\nefforts to ``level up\" public transport services, addressing geographical\ninequalities and promoting equitable access to essential healthcare services.",
    "published_date": "2025-01-31T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.19231v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.19173v1",
    "title": "Position: Contextual Integrity Washing for Language Models",
    "authors": [
      "Yan Shvartzshnaider",
      "Vasisht Duddu"
    ],
    "author_ids": [],
    "abstract": "Machine learning community is discovering Contextual Integrity (CI) as a\nuseful framework to assess the privacy implications of large language models\n(LLMs). This is an encouraging development. The CI theory emphasizes sharing\ninformation in accordance with privacy norms and can bridge the social, legal,\npolitical, and technical aspects essential for evaluating privacy in LLMs.\nHowever, this is also a good point to reflect on use of CI for LLMs. This\nposition paper argues that existing literature adopts CI for LLMs without\nembracing the theory's fundamental tenets, essentially amounting to a form of\n\"CI-washing.\" CI-washing could lead to incorrect conclusions and flawed\nprivacy-preserving designs. We clarify the four fundamental tenets of CI\ntheory, systematize prior work on whether they deviate from these tenets, and\nhighlight overlooked issues in experimental hygiene for LLMs (e.g., prompt\nsensitivity, positional bias).",
    "published_date": "2025-01-31T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.19173v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.19149v1",
    "title": "On the inductive bias of infinite-depth ResNets and the bottleneck rank",
    "authors": [
      "Enric Boix-Adsera"
    ],
    "author_ids": [],
    "abstract": "We compute the minimum-norm weights of a deep linear ResNet, and find that\nthe inductive bias of this architecture lies between minimizing nuclear norm\nand rank. This implies that, with appropriate hyperparameters, deep nonlinear\nResNets have an inductive bias towards minimizing bottleneck rank.",
    "published_date": "2025-01-31T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.19149v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.19086v1",
    "title": "Fairness Analysis of CLIP-Based Foundation Models for X-Ray Image Classification",
    "authors": [
      "Xiangyu Sun",
      "Xiaoguang Zou",
      "Yuanquan Wu",
      "Guotai Wang",
      "Shaoting Zhang"
    ],
    "author_ids": [],
    "abstract": "X-ray imaging is pivotal in medical diagnostics, offering non-invasive\ninsights into a range of health conditions. Recently, vision-language models,\nsuch as the Contrastive Language-Image Pretraining (CLIP) model, have\ndemonstrated potential in improving diagnostic accuracy by leveraging\nlarge-scale image-text datasets. However, since CLIP was not initially designed\nfor medical images, several CLIP-like models trained specifically on medical\nimages have been developed. Despite their enhanced performance, issues of\nfairness - particularly regarding demographic attributes - remain largely\nunaddressed. In this study, we perform a comprehensive fairness analysis of\nCLIP-like models applied to X-ray image classification. We assess their\nperformance and fairness across diverse patient demographics and disease\ncategories using zero-shot inference and various fine-tuning techniques,\nincluding Linear Probing, Multilayer Perceptron (MLP), Low-Rank Adaptation\n(LoRA), and full fine-tuning. Our results indicate that while fine-tuning\nimproves model accuracy, fairness concerns persist, highlighting the need for\nfurther fairness interventions in these foundational models.",
    "published_date": "2025-01-31T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.19086v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.19010v2",
    "title": "DyPCL: Dynamic Phoneme-level Contrastive Learning for Dysarthric Speech Recognition",
    "authors": [
      "Wonjun Lee",
      "Solee Im",
      "Heejin Do",
      "Yunsu Kim",
      "Jungseul Ok",
      "Gary Geunbae Lee"
    ],
    "author_ids": [],
    "abstract": "Dysarthric speech recognition often suffers from performance degradation due\nto the intrinsic diversity of dysarthric severity and extrinsic disparity from\nnormal speech. To bridge these gaps, we propose a Dynamic Phoneme-level\nContrastive Learning (DyPCL) method, which leads to obtaining invariant\nrepresentations across diverse speakers. We decompose the speech utterance into\nphoneme segments for phoneme-level contrastive learning, leveraging dynamic\nconnectionist temporal classification alignment. Unlike prior studies focusing\non utterance-level embeddings, our granular learning allows discrimination of\nsubtle parts of speech. In addition, we introduce dynamic curriculum learning,\nwhich progressively transitions from easy negative samples to\ndifficult-to-distinguishable negative samples based on phonetic similarity of\nphoneme. Our approach to training by difficulty levels alleviates the inherent\nvariability of speakers, better identifying challenging speeches. Evaluated on\nthe UASpeech dataset, DyPCL outperforms baseline models, achieving an average\n22.10\\% relative reduction in word error rate (WER) across the overall\ndysarthria group.",
    "published_date": "2025-01-31T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.19010v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.18934v1",
    "title": "Deep Learning Model Inversion Attacks and Defenses: A Comprehensive Survey",
    "authors": [
      "Wencheng Yang",
      "Song Wang",
      "Di Wu",
      "Taotao Cai",
      "Yanming Zhu",
      "Shicheng Wei",
      "Yiying Zhang",
      "Xu Yang",
      "Yan Li"
    ],
    "author_ids": [],
    "abstract": "The rapid adoption of deep learning in sensitive domains has brought\ntremendous benefits. However, this widespread adoption has also given rise to\nserious vulnerabilities, particularly model inversion (MI) attacks, posing a\nsignificant threat to the privacy and integrity of personal data. The\nincreasing prevalence of these attacks in applications such as biometrics,\nhealthcare, and finance has created an urgent need to understand their\nmechanisms, impacts, and defense methods. This survey aims to fill the gap in\nthe literature by providing a structured and in-depth review of MI attacks and\ndefense strategies. Our contributions include a systematic taxonomy of MI\nattacks, extensive research on attack techniques and defense mechanisms, and a\ndiscussion about the challenges and future research directions in this evolving\nfield. By exploring the technical and ethical implications of MI attacks, this\nsurvey aims to offer insights into the impact of AI-powered systems on privacy,\nsecurity, and trust. In conjunction with this survey, we have developed a\ncomprehensive repository to support research on MI attacks and defenses. The\nrepository includes state-of-the-art research papers, datasets, evaluation\nmetrics, and other resources to meet the needs of both novice and experienced\nresearchers interested in MI attacks and defenses, as well as the broader field\nof AI security and privacy. The repository will be continuously maintained to\nensure its relevance and utility. It is accessible at\nhttps://github.com/overgter/Deep-Learning-Model-Inversion-Attacks-and-Defenses.",
    "published_date": "2025-01-31T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.18934v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.18468v1",
    "title": "SOK: Exploring Hallucinations and Security Risks in AI-Assisted Software Development with Insights for LLM Deployment",
    "authors": [
      "Ariful Haque",
      "Sunzida Siddique",
      "Md. Mahfuzur Rahman",
      "Ahmed Rafi Hasan",
      "Laxmi Rani Das",
      "Marufa Kamal",
      "Tasnim Masura",
      "Kishor Datta Gupta"
    ],
    "author_ids": [],
    "abstract": "The integration of Large Language Models (LLMs) such as GitHub Copilot,\nChatGPT, Cursor AI, and Codeium AI into software development has revolutionized\nthe coding landscape, offering significant productivity gains, automation, and\nenhanced debugging capabilities. These tools have proven invaluable for\ngenerating code snippets, refactoring existing code, and providing real-time\nsupport to developers. However, their widespread adoption also presents notable\nchallenges, particularly in terms of security vulnerabilities, code quality,\nand ethical concerns. This paper provides a comprehensive analysis of the\nbenefits and risks associated with AI-powered coding tools, drawing on user\nfeedback, security analyses, and practical use cases. We explore the potential\nfor these tools to replicate insecure coding practices, introduce biases, and\ngenerate incorrect or non-sensical code (hallucinations). In addition, we\ndiscuss the risks of data leaks, intellectual property violations and the need\nfor robust security measures to mitigate these threats. By comparing the\nfeatures and performance of these tools, we aim to guide developers in making\ninformed decisions about their use, ensuring that the benefits of AI-assisted\ncoding are maximized while minimizing associated risks.",
    "published_date": "2025-01-31T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.18468v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.15514v2",
    "title": "Superhuman Game AI Disclosure: Expertise and Context Moderate Effects on Trust and Fairness",
    "authors": [
      "Jaymari Chua",
      "Chen Wang",
      "Lina Yao"
    ],
    "author_ids": [],
    "abstract": "As artificial intelligence surpasses human performance in select tasks,\ndisclosing superhuman capabilities poses distinct challenges for fairness,\naccountability, and trust. However, the impact of such disclosures on diverse\nuser attitudes and behaviors remains unclear, particularly concerning potential\nnegative reactions like discouragement or overreliance. This paper investigates\nthese effects by utilizing Persona Cards: a validated, standardized set of\nsynthetic personas designed to simulate diverse user reactions and fairness\nperspectives. We conducted an ethics board-approved study (N=32), utilizing\nthese personas to investigate how capability disclosure influenced behaviors\nwith a superhuman game AI in competitive StarCraft II scenarios. Our results\nreveal transparency is double-edged: while disclosure could alleviate\nsuspicion, it also provoked frustration and strategic defeatism among novices\nin cooperative scenarios, as well as overreliance in competitive contexts.\nExperienced and competitive players interpreted disclosure as confirmation of\nan unbeatable opponent, shifting to suboptimal goals. We release the Persona\nCards Dataset, including profiles, prompts, interaction logs, and protocols, to\nfoster reproducible research into human alignment AI design. This work\ndemonstrates that transparency is not a cure-all; successfully leveraging\ndisclosure to enhance trust and accountability requires careful tailoring to\nuser characteristics, domain norms, and specific fairness objectives.",
    "published_date": "2025-01-31T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.ET",
      "K.4.1; K.4.3; H.5.2; H.5.1; I.2.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.15514v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.00071v1",
    "title": "Including frameworks of public health ethics in computational modelling of infectious disease interventions",
    "authors": [
      "Alexander E. Zarebski",
      "Nefel Tellioglu",
      "Jessica E. Stockdale",
      "Julie A. Spencer",
      "Wasiur R. KhudaBukhsh",
      "Joel C. Miller",
      "Cameron Zachreson"
    ],
    "author_ids": [],
    "abstract": "Decisions on public health interventions to control infectious disease are\noften informed by computational models. Interpreting the predicted outcomes of\na public health decision requires not only high-quality modelling, but also an\nethical framework for assessing the benefits and harms associated with\ndifferent options. The design and specification of ethical frameworks matured\nindependently of computational modelling, so many values recognised as\nimportant for ethical decision-making are missing from computational models. We\ndemonstrate a proof-of-concept approach to incorporate multiple public health\nvalues into the evaluation of a simple computational model for vaccination\nagainst a pathogen such as SARS-CoV-2. By examining a bounded space of\nalternative prioritisations of values (outcome equity and aggregate benefit) we\nidentify value trade-offs, where the outcomes of optimal strategies differ\ndepending on the ethical framework. This work demonstrates an approach to\nincorporating diverse values into decision criteria used to evaluate outcomes\nof models of infectious disease interventions.",
    "published_date": "2025-01-31T00:00:00",
    "year": 2025,
    "categories": [
      "physics.soc-ph",
      "cs.CY",
      "q-bio.PE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.00071v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.00070v2",
    "title": "Can AI Solve the Peer Review Crisis? A Large Scale Cross Model Experiment of LLMs' Performance and Biases in Evaluating over 1000 Economics Papers",
    "authors": [
      "Pat Pataranutaporn",
      "Nattavudh Powdthavee",
      "Chayapatr Achiwaranguprok",
      "Pattie Maes"
    ],
    "author_ids": [],
    "abstract": "This study examines the potential of large language models (LLMs) to augment\nthe academic peer review process by reliably evaluating the quality of\neconomics research without introducing systematic bias. We conduct one of the\nfirst large-scale experimental assessments of four LLMs (GPT-4o, Claude 3.5,\nGemma 3, and LLaMA 3.3) across two complementary experiments. In the first, we\nuse nonparametric binscatter and linear regression techniques to analyze over\n29,000 evaluations of 1,220 anonymized papers drawn from 110 economics journals\nexcluded from the training data of current LLMs, along with a set of\nAI-generated submissions. The results show that LLMs consistently distinguish\nbetween higher- and lower-quality research based solely on textual content,\nproducing quality gradients that closely align with established journal\nprestige measures. Claude and Gemma perform exceptionally well in capturing\nthese gradients, while GPT excels in detecting AI-generated content. The second\nexperiment comprises 8,910 evaluations designed to assess whether LLMs\nreplicate human like biases in single blind reviews. By systematically varying\nauthor gender, institutional affiliation, and academic prominence across 330\npapers, we find that GPT, Gemma, and LLaMA assign significantly higher ratings\nto submissions from top male authors and elite institutions relative to the\nsame papers presented anonymously. These results emphasize the importance of\nexcluding author-identifying information when deploying LLMs in editorial\nscreening. Overall, our findings provide compelling evidence and practical\nguidance for integrating LLMs into peer review to enhance efficiency, improve\naccuracy, and promote equity in the publication process of economics research.",
    "published_date": "2025-01-31T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "econ.GN",
      "q-fin.EC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.00070v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.18788v1",
    "title": "Tuning Event Camera Biases Heuristic for Object Detection Applications in Staring Scenarios",
    "authors": [
      "David El-Chai Ben-Ezra",
      "Daniel Brisk"
    ],
    "author_ids": [],
    "abstract": "One of the main challenges in unlocking the potential of neuromorphic\ncameras, also called 'event cameras', is the development of novel methods that\nsolve the multi-parameter problem of adjusting their bias parameters to\naccommodate a desired task. Actually, it is very difficult to find in the\nliterature a systematic heuristic that solves the problem for any desired\napplication.\n  In this paper we present a tuning parametes heuristic for the biases of event\ncameras, for tasks that require small objects detection in staring scenarios.\nThe main purpose of the heuristic is to squeeze the camera's potential,\noptimize its performance, and expand its detection capabilities as much as\npossible.\n  In the presentation, we translate the experimental properties of event camera\nand systemic constrains into mathematical terms, and show, under certain\nassumptions, how the multi-variable problem collapses into a two-parameter\nproblem that can be solved experimentally.\n  A main conclusion that will be demonstrated is that for certain desired\nsignals, such as the one provided by an incandescent lamp powered by the\nperiodic electrical grid, the optimal values of the camera are very far from\nthe default values recommended by the manufacturer.",
    "published_date": "2025-01-30T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "math.OC",
      "49J21, 93C35, 93B52, 93C65"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.18788v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.18581v1",
    "title": "Bias-variance decompositions: the exclusive privilege of Bregman divergences",
    "authors": [
      "Tom Heskes"
    ],
    "author_ids": [],
    "abstract": "Bias-variance decompositions are widely used to understand the generalization\nperformance of machine learning models. While the squared error loss permits a\nstraightforward decomposition, other loss functions - such as zero-one loss or\n$L_1$ loss - either fail to sum bias and variance to the expected loss or rely\non definitions that lack the essential properties of meaningful bias and\nvariance. Recent research has shown that clean decompositions can be achieved\nfor the broader class of Bregman divergences, with the cross-entropy loss as a\nspecial case. However, the necessary and sufficient conditions for these\ndecompositions remain an open question.\n  In this paper, we address this question by studying continuous, nonnegative\nloss functions that satisfy the identity of indiscernibles under mild\nregularity conditions. We prove that so-called $g$-Bregman divergences are the\nonly such loss functions that have a clean bias-variance decomposition. A\n$g$-Bregman divergence can be transformed into a standard Bregman divergence\nthrough an invertible change of variables. This makes the squared Mahalanobis\ndistance, up to such a variable transformation, the only symmetric loss\nfunction with a clean bias-variance decomposition. We also examine the impact\nof relaxing the restrictions on the loss functions and how this affects our\nresults.",
    "published_date": "2025-01-30T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.18581v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.18493v1",
    "title": "Examining the Expanding Role of Synthetic Data Throughout the AI Development Pipeline",
    "authors": [
      "Shivani Kapania",
      "Stephanie Ballard",
      "Alex Kessler",
      "Jennifer Wortman Vaughan"
    ],
    "author_ids": [],
    "abstract": "Alongside the growth of generative AI, we are witnessing a surge in the use\nof synthetic data across all stages of the AI development pipeline. It is now\ncommon practice for researchers and practitioners to use one large generative\nmodel (which we refer to as an auxiliary model) to generate synthetic data that\nis used to train or evaluate another, reconfiguring AI workflows and reshaping\nthe very nature of data. While scholars have raised concerns over the risks of\nsynthetic data, policy guidance and best practices for its responsible use have\nnot kept up with these rapidly evolving industry trends, in part because we\nlack a clear picture of current practices and challenges. Our work aims to\naddress this gap. Through 29 interviews with AI practitioners and responsible\nAI experts, we examine the expanding role of synthetic data in AI development.\nOur findings reveal how auxiliary models are now widely used across the AI\ndevelopment pipeline. Practitioners describe synthetic data as crucial for\naddressing data scarcity and providing a competitive edge, noting that\nevaluation of generative AI systems at scale would be infeasible without\nauxiliary models. However, they face challenges controlling the outputs of\nauxiliary models, generating data that accurately depict underrepresented\ngroups, and scaling data validation practices that are based primarily on\nmanual inspection. We detail general limitations of and ethical considerations\nfor synthetic data and conclude with a proposal of concrete steps towards the\ndevelopment of best practices for its responsible use.",
    "published_date": "2025-01-30T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.18493v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.18441v1",
    "title": "From Public Square to Echo Chamber: The Fragmentation of Online Discourse",
    "authors": [
      "Abhinav Pratap",
      "Amit Pathak"
    ],
    "author_ids": [],
    "abstract": "This paper examines how social media algorithms and filter bubbles contribute\nto the fragmentation of online discourse, fostering ideological divides and\nundermining shared understanding. Drawing on Michael Sandels philosophical\nemphasis on community and shared values, the study explores how digital\nplatforms amplify discrimination discourse including sexism, racism,\nxenophobia, ableism, homophobia, and religious intolerance during periods of\nheightened societal tension. By analyzing the dynamics of digital communities,\nthe research highlights mechanisms driving the emergence and evolution of\ndiscourse fragments in response to real world events. The findings reveal how\nsocial media structures exacerbate polarization, restrict cross group dialogue,\nand erode the collective reasoning essential for a just society. This study\nsituates philosophical perspectives within a computational analysis of social\nmedia interactions, offering a nuanced understanding of the challenges posed by\nfragmented discourse in the digital age.",
    "published_date": "2025-01-30T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.18441v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.18331v1",
    "title": "Stream-Based Monitoring of Algorithmic Fairness",
    "authors": [
      "Jan Baumeister",
      "Bernd Finkbeiner",
      "Frederik Scheerer",
      "Julian Siber",
      "Tobias Wagenpfeil"
    ],
    "author_ids": [],
    "abstract": "Automatic decision and prediction systems are increasingly deployed in\napplications where they significantly impact the livelihood of people, such as\nfor predicting the creditworthiness of loan applicants or the recidivism risk\nof defendants. These applications have given rise to a new class of\nalgorithmic-fairness specifications that require the systems to decide and\npredict without bias against social groups. Verifying these specifications\nstatically is often out of reach for realistic systems, since the systems may,\ne.g., employ complex learning components, and reason over a large input space.\nIn this paper, we therefore propose stream-based monitoring as a solution for\nverifying the algorithmic fairness of decision and prediction systems at\nruntime. Concretely, we present a principled way to formalize algorithmic\nfairness over temporal data streams in the specification language RTLola and\ndemonstrate the efficacy of this approach on a number of benchmarks. Besides\nsynthetic scenarios that particularly highlight its efficiency on streams with\na scaling amount of data, we notably evaluate the monitor on real-world data\nfrom the recidivism prediction tool COMPAS.",
    "published_date": "2025-01-30T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.LO",
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.18331v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.18299v1",
    "title": "Model-Free RL Agents Demonstrate System 1-Like Intentionality",
    "authors": [
      "Hal Ashton",
      "Matija Franklin"
    ],
    "author_ids": [],
    "abstract": "This paper argues that model-free reinforcement learning (RL) agents, while\nlacking explicit planning mechanisms, exhibit behaviours that can be analogised\nto System 1 (\"thinking fast\") processes in human cognition. Unlike model-based\nRL agents, which operate akin to System 2 (\"thinking slow\") reasoning by\nleveraging internal representations for planning, model-free agents react to\nenvironmental stimuli without anticipatory modelling. We propose a novel\nframework linking the dichotomy of System 1 and System 2 to the distinction\nbetween model-free and model-based RL. This framing challenges the prevailing\nassumption that intentionality and purposeful behaviour require planning,\nsuggesting instead that intentionality can manifest in the structured, reactive\nbehaviours of model-free agents. By drawing on interdisciplinary insights from\ncognitive psychology, legal theory, and experimental jurisprudence, we explore\nthe implications of this perspective for attributing responsibility and\nensuring AI safety. These insights advocate for a broader, contextually\ninformed interpretation of intentionality in RL systems, with implications for\ntheir ethical deployment and regulation.",
    "published_date": "2025-01-30T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.18299v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.00061v1",
    "title": "From Data to Action: Charting A Data-Driven Path to Combat Antimicrobial Resistance",
    "authors": [
      "Qian Fu",
      "Yuzhe Zhang",
      "Yanfeng Shu",
      "Ming Ding",
      "Lina Yao",
      "Chen Wang"
    ],
    "author_ids": [],
    "abstract": "Antimicrobial-resistant (AMR) microbes are a growing challenge in healthcare,\nrendering modern medicines ineffective. AMR arises from antibiotic production\nand bacterial evolution, but quantifying its transmission remains difficult.\nWith increasing AMR-related data, data-driven methods offer promising insights\ninto its causes and treatments. This paper reviews AMR research from a data\nanalytics and machine learning perspective, summarizing the state-of-the-art\nand exploring key areas such as surveillance, prediction, drug discovery,\nstewardship, and driver analysis. It discusses data sources, methods, and\nchallenges, emphasizing standardization and interoperability. Additionally, it\nsurveys statistical and machine learning techniques for AMR analysis,\naddressing issues like data noise and bias. Strategies for denoising and\ndebiasing are highlighted to enhance fairness and robustness in AMR research.\nThe paper underscores the importance of interdisciplinary collaboration and\nawareness of data challenges in advancing AMR research, pointing to future\ndirections for innovation and improved methodologies.",
    "published_date": "2025-01-30T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.PE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.00061v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.18277v1",
    "title": "Sebra: Debiasing Through Self-Guided Bias Ranking",
    "authors": [
      "Adarsh Kappiyath",
      "Abhra Chaudhuri",
      "Ajay Jaiswal",
      "Ziquan Liu",
      "Yunpeng Li",
      "Xiatian Zhu",
      "Lu Yin"
    ],
    "author_ids": [],
    "abstract": "Ranking samples by fine-grained estimates of spuriosity (the degree to which\nspurious cues are present) has recently been shown to significantly benefit\nbias mitigation, over the traditional binary biased-\\textit{vs}-unbiased\npartitioning of train sets. However, this spuriosity ranking comes with the\nrequirement of human supervision. In this paper, we propose a debiasing\nframework based on our novel \\ul{Se}lf-Guided \\ul{B}ias \\ul{Ra}nking\n(\\emph{Sebra}), that mitigates biases (spurious correlations) via an automatic\nranking of data points by spuriosity within their respective classes. Sebra\nleverages a key local symmetry in Empirical Risk Minimization (ERM) training --\nthe ease of learning a sample via ERM inversely correlates with its\nspuriousity; the fewer spurious correlations a sample exhibits, the harder it\nis to learn, and vice versa. However, globally across iterations, ERM tends to\ndeviate from this symmetry. Sebra dynamically steers ERM to correct this\ndeviation, facilitating the sequential learning of attributes in increasing\norder of difficulty, \\ie, decreasing order of spuriosity. As a result, the\nsequence in which Sebra learns samples naturally provides spuriousity rankings.\nWe use the resulting fine-grained bias characterization in a contrastive\nlearning framework to mitigate biases from multiple sources. Extensive\nexperiments show that Sebra consistently outperforms previous state-of-the-art\nunsupervised debiasing techniques across multiple standard benchmarks,\nincluding UrbanCars, BAR, CelebA, and ImageNet-1K. Code, pre-trained models,\nand training logs are available at https://kadarsh22.github.io/sebra_iclr25/.",
    "published_date": "2025-01-30T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.18277v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.18192v1",
    "title": "Machine Learning Fairness for Depression Detection using EEG Data",
    "authors": [
      "Angus Man Ho Kwok",
      "Jiaee Cheong",
      "Sinan Kalkan",
      "Hatice Gunes"
    ],
    "author_ids": [],
    "abstract": "This paper presents the very first attempt to evaluate machine learning\nfairness for depression detection using electroencephalogram (EEG) data. We\nconduct experiments using different deep learning architectures such as\nConvolutional Neural Networks (CNN), Long Short-Term Memory (LSTM) networks,\nand Gated Recurrent Unit (GRU) networks across three EEG datasets: Mumtaz,\nMODMA and Rest. We employ five different bias mitigation strategies at the\npre-, in- and post-processing stages and evaluate their effectiveness. Our\nexperimental results show that bias exists in existing EEG datasets and\nalgorithms for depression detection, and different bias mitigation methods\naddress bias at different levels across different fairness measures.",
    "published_date": "2025-01-30T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.SP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.18192v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.18190v2",
    "title": "Economic Rationality under Specialization: Evidence of Decision Bias in AI Agents",
    "authors": [
      "ShuiDe Wen"
    ],
    "author_ids": [],
    "abstract": "In the study by Chen et al. (2023) [01], the large language model GPT\ndemonstrated economic rationality comparable to or exceeding the average human\nlevel in tasks such as budget allocation and risk preference. Building on this\nfinding, this paper further incorporates specialized agents, such as\nbiotechnology experts and economists, for a horizontal comparison to explore\nwhether specialization can enhance or maintain economic rationality equivalent\nto that of GPT in similar decision-making scenarios. The results indicate that\nwhen agents invest more effort in specialized fields, their decision-making\nbehavior is more prone to 'rationality shift,' specifically manifested as\nincreased violations of GARP (Generalized Axiom of Revealed Preference),\ndecreased CCEI (Critical Cost Efficiency Index), and more significant decision\ndeviations under high-risk conditions. In contrast, GPT and more generalized\nbasic agents maintain a more stable and consistent level of rationality across\nmultiple tasks. This study reveals the inherent conflict between specialization\nand economic rationality, providing new insights for constructing AI\ndecision-making systems that balance specialization and generalization across\nvarious scenarios.",
    "published_date": "2025-01-30T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.18190v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.18129v1",
    "title": "Revisiting gender bias research in bibliometrics: Standardizing methodological variability using Scholarly Data Analysis (SoDA) Cards",
    "authors": [
      "HaeJin Lee",
      "Shubhanshu Mishra",
      "Apratim Mishra",
      "Zhiwen You",
      "Jinseok Kim",
      "Jana Diesner"
    ],
    "author_ids": [],
    "abstract": "Gender biases in scholarly metrics remain a persistent concern, despite\nnumerous bibliometric studies exploring their presence and absence across\nproductivity, impact, acknowledgment, and self-citations. However,\nmethodological inconsistencies, particularly in author name disambiguation and\ngender identification, limit the reliability and comparability of these\nstudies, potentially perpetuating misperceptions and hindering effective\ninterventions. A review of 70 relevant publications over the past 12 years\nreveals a wide range of approaches, from name-based and manual searches to more\nalgorithmic and gold-standard methods, with no clear consensus on best\npractices. This variability, compounded by challenges such as accurately\ndisambiguating Asian names and managing unassigned gender labels, underscores\nthe urgent need for standardized and robust methodologies. To address this\ncritical gap, we propose the development and implementation of ``Scholarly Data\nAnalysis (SoDA) Cards.\" These cards will provide a structured framework for\ndocumenting and reporting key methodological choices in scholarly data\nanalysis, including author name disambiguation and gender identification\nprocedures. By promoting transparency and reproducibility, SoDA Cards will\nfacilitate more accurate comparisons and aggregations of research findings,\nultimately supporting evidence-informed policymaking and enabling the\nlongitudinal tracking of analytical approaches in the study of gender and other\nsocial biases in academia.",
    "published_date": "2025-01-30T00:00:00",
    "year": 2025,
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.SI",
      "K.4.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.18129v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.18117v1",
    "title": "Improving Minimax Group Fairness in Sequential Recommendation",
    "authors": [
      "Krishna Acharya",
      "David Wardrope",
      "Timos Korres",
      "Aleksandr Petrov",
      "Anders Uhrenholt"
    ],
    "author_ids": [],
    "abstract": "Training sequential recommenders such as SASRec with uniform sample weights\nachieves good overall performance but can fall short on specific user groups.\nOne such example is popularity bias, where mainstream users receive better\nrecommendations than niche content viewers. To improve recommendation quality\nacross diverse user groups, we explore three Distributionally Robust\nOptimization(DRO) methods: Group DRO, Streaming DRO, and Conditional Value at\nRisk (CVaR) DRO. While Group and Streaming DRO rely on group annotations and\nstruggle with users belonging to multiple groups, CVaR does not require such\nannotations and can naturally handle overlapping groups. In experiments on two\nreal-world datasets, we show that the DRO methods outperform standard training,\nwith CVaR delivering the best results. Additionally, we find that Group and\nStreaming DRO are sensitive to the choice of group used for loss computation.\nOur contributions include (i) a novel application of CVaR to recommenders, (ii)\nshowing that the DRO methods improve group metrics as well as overall\nperformance, and (iii) demonstrating CVaR's effectiveness in the practical\nscenario of intersecting user groups.",
    "published_date": "2025-01-30T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.18117v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.18081v1",
    "title": "Normative Evaluation of Large Language Models with Everyday Moral Dilemmas",
    "authors": [
      "Pratik S. Sachdeva",
      "Tom van Nuenen"
    ],
    "author_ids": [],
    "abstract": "The rapid adoption of large language models (LLMs) has spurred extensive\nresearch into their encoded moral norms and decision-making processes. Much of\nthis research relies on prompting LLMs with survey-style questions to assess\nhow well models are aligned with certain demographic groups, moral beliefs, or\npolitical ideologies. While informative, the adherence of these approaches to\nrelatively superficial constructs tends to oversimplify the complexity and\nnuance underlying everyday moral dilemmas. We argue that auditing LLMs along\nmore detailed axes of human interaction is of paramount importance to better\nassess the degree to which they may impact human beliefs and actions. To this\nend, we evaluate LLMs on complex, everyday moral dilemmas sourced from the \"Am\nI the Asshole\" (AITA) community on Reddit, where users seek moral judgments on\neveryday conflicts from other community members. We prompted seven LLMs to\nassign blame and provide explanations for over 10,000 AITA moral dilemmas. We\nthen compared the LLMs' judgments and explanations to those of Redditors and to\neach other, aiming to uncover patterns in their moral reasoning. Our results\ndemonstrate that large language models exhibit distinct patterns of moral\njudgment, varying substantially from human evaluations on the AITA subreddit.\nLLMs demonstrate moderate to high self-consistency but low inter-model\nagreement. Further analysis of model explanations reveals distinct patterns in\nhow models invoke various moral principles. These findings highlight the\ncomplexity of implementing consistent moral reasoning in artificial systems and\nthe need for careful evaluation of how different models approach ethical\njudgment. As LLMs continue to be used in roles requiring ethical\ndecision-making such as therapists and companions, careful evaluation is\ncrucial to mitigate potential biases and limitations.",
    "published_date": "2025-01-30T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.18081v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.18056v1",
    "title": "RL-based Query Rewriting with Distilled LLM for online E-Commerce Systems",
    "authors": [
      "Duy A. Nguyen",
      "Rishi Kesav Mohan",
      "Van Yang",
      "Pritom Saha Akash",
      "Kevin Chen-Chuan Chang"
    ],
    "author_ids": [],
    "abstract": "Query rewriting (QR) is a critical technique in e-commerce search, addressing\nthe lexical gap between user queries and product descriptions to enhance search\nperformance. Existing QR approaches typically fall into two categories:\ndiscriminative models and generative methods leveraging large language models\n(LLMs). Discriminative models often struggle with natural language\nunderstanding and offer limited flexibility in rewriting, while generative\nLLMs, despite producing high-quality rewrites, face high inference latency and\ncost in online settings. These limitations force offline deployment, making\nthem vulnerable to issues like information staleness and semantic drift. To\novercome these challenges, we propose a novel hybrid pipeline for QR that\nbalances efficiency and effectiveness. Our approach combines offline knowledge\ndistillation to create a lightweight but efficient student model with online\nreinforcement learning (RL) to refine query rewriting dynamically using\nreal-time feedback. A key innovation is the use of LLMs as simulated human\nfeedback, enabling scalable reward signals and cost-effective evaluation\nwithout manual annotations. Experimental results on Amazon ESCI dataset\ndemonstrate significant improvements in query relevance, diversity, and\nadaptability, as well as positive feedback from the LLM simulation. This work\ncontributes to advancing LLM capabilities for domain-specific applications,\noffering a robust solution for dynamic and complex e-commerce search\nenvironments.",
    "published_date": "2025-01-29T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.18056v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.18038v2",
    "title": "A Case Study in Acceleration AI Ethics: The TELUS GenAI Conversational Agent",
    "authors": [
      "James Brusseau"
    ],
    "author_ids": [],
    "abstract": "Acceleration ethics addresses the tension between innovation and safety in\nartificial intelligence. The acceleration argument is that risks raised by\ninnovation should be answered with still more innovating. This paper summarizes\nthe theoretical position, and then shows how acceleration ethics works in a\nreal case. To begin, the paper summarizes acceleration ethics as composed of\nfive elements: innovation solves innovation problems, innovation is\nintrinsically valuable, the unknown is encouraging, governance is\ndecentralized, ethics is embedded. Subsequently, the paper illustrates the\nacceleration framework with a use-case, a generative artificial intelligence\nlanguage tool developed by the Canadian telecommunications company Telus. While\nthe purity of theoretical positions is blurred by real-world ambiguities, the\nTelus experience indicates that acceleration AI ethics is a way of maximizing\nsocial responsibility through innovation, as opposed to sacrificing social\nresponsibility for innovation, or sacrificing innovation for social\nresponsibility.",
    "published_date": "2025-01-29T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.18038v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.17749v1",
    "title": "Early External Safety Testing of OpenAI's o3-mini: Insights from the Pre-Deployment Evaluation",
    "authors": [
      "Aitor Arrieta",
      "Miriam Ugarte",
      "Pablo Valle",
      "José Antonio Parejo",
      "Sergio Segura"
    ],
    "author_ids": [],
    "abstract": "Large Language Models (LLMs) have become an integral part of our daily lives.\nHowever, they impose certain risks, including those that can harm individuals'\nprivacy, perpetuate biases and spread misinformation. These risks highlight the\nneed for robust safety mechanisms, ethical guidelines, and thorough testing to\nensure their responsible deployment. Safety of LLMs is a key property that\nneeds to be thoroughly tested prior the model to be deployed and accessible to\nthe general users. This paper reports the external safety testing experience\nconducted by researchers from Mondragon University and University of Seville on\nOpenAI's new o3-mini LLM as part of OpenAI's early access for safety testing\nprogram. In particular, we apply our tool, ASTRAL, to automatically and\nsystematically generate up to date unsafe test inputs (i.e., prompts) that\nhelps us test and assess different safety categories of LLMs. We automatically\ngenerate and execute a total of 10,080 unsafe test input on a early o3-mini\nbeta version. After manually verifying the test cases classified as unsafe by\nASTRAL, we identify a total of 87 actual instances of unsafe LLM behavior. We\nhighlight key insights and findings uncovered during the pre-deployment\nexternal testing phase of OpenAI's latest LLM.",
    "published_date": "2025-01-29T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.17749v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.17568v2",
    "title": "Histogram Approaches for Imbalanced Data Streams Regression",
    "authors": [
      "Ehsan Aminian",
      "Rita P. Ribeiro",
      "Joao Gama"
    ],
    "author_ids": [],
    "abstract": "Imbalanced domains pose a significant challenge in real-world predictive\nanalytics, particularly in the context of regression. While existing research\nhas primarily focused on batch learning from static datasets, limited attention\nhas been given to imbalanced regression in online learning scenarios. Intending\nto address this gap, in prior work, we proposed sampling strategies based on\nChebyshevs inequality as the first methodologies designed explicitly for data\nstreams. However, these approaches operated under the restrictive assumption\nthat rare instances exclusively reside at distribution extremes. This study\nintroduces histogram-based sampling strategies to overcome this constraint,\nproposing flexible solutions for imbalanced regression in evolving data\nstreams. The proposed techniques -- Histogram-based Undersampling (HistUS) and\nHistogram-based Oversampling (HistOS) -- employ incremental online histograms\nto dynamically detect and prioritize rare instances across arbitrary regions of\nthe target distribution to improve predictions in the rare cases. Comprehensive\nexperiments on synthetic and real-world benchmarks demonstrate that HistUS and\nHistOS substantially improve rare-case prediction accuracy, outperforming\nbaseline models while maintaining competitiveness with Chebyshev-based\napproaches.",
    "published_date": "2025-01-29T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.17568v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.17420v1",
    "title": "Actions Speak Louder than Words: Agent Decisions Reveal Implicit Biases in Language Models",
    "authors": [
      "Yuxuan Li",
      "Hirokazu Shirado",
      "Sauvik Das"
    ],
    "author_ids": [],
    "abstract": "While advances in fairness and alignment have helped mitigate overt biases\nexhibited by large language models (LLMs) when explicitly prompted, we\nhypothesize that these models may still exhibit implicit biases when simulating\nhuman behavior. To test this hypothesis, we propose a technique to\nsystematically uncover such biases across a broad range of sociodemographic\ncategories by assessing decision-making disparities among agents with\nLLM-generated, sociodemographically-informed personas. Using our technique, we\ntested six LLMs across three sociodemographic groups and four decision-making\nscenarios. Our results show that state-of-the-art LLMs exhibit significant\nsociodemographic disparities in nearly all simulations, with more advanced\nmodels exhibiting greater implicit biases despite reducing explicit biases.\nFurthermore, when comparing our findings to real-world disparities reported in\nempirical studies, we find that the biases we uncovered are directionally\naligned but markedly amplified. This directional alignment highlights the\nutility of our technique in uncovering systematic biases in LLMs rather than\nrandom variations; moreover, the presence and amplification of implicit biases\nemphasizes the need for novel strategies to address these biases.",
    "published_date": "2025-01-29T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.17420v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.17390v1",
    "title": "Fractional coupled Halanay inequality and its applications",
    "authors": [
      "La Van Thinh",
      "Hoang The Tuan",
      "Dongling Wang",
      "Yin Yang"
    ],
    "author_ids": [],
    "abstract": "This paper introduces a generalized fractional Halanay-type coupled\ninequality, which serves as a robust tool for characterizing the asymptotic\nstability of diverse time fractional functional differential equations,\nparticularly those exhibiting Mittag-Leffler type stability. Our main tool is a\nsub-additive property of Mittag-Leffler function and its optimal asymptotic\ndecay rate estimation. Our results further optimize and improve some existing\nresults in the literature. We illustrate two significant applications of this\nfractional Halanay-type inequality. Firstly, by combining our results in this\nwork with the positive representation method positive representation of delay\ndifferential systems, we establish an asymptotic stability criterion for a\ncategory of linear fractional coupled systems with bounded delays. This\ncriterion extends beyond the traditional boundaries of positive system theory,\noffering a new perspective on stability analysis in this domain. Secondly,\nthrough energy estimation, we establish the contractility and dissipativity of\na class of time fractional neutral functional differential equations. Our\nanalysis reveals the typical long-term polynomial decay behavior inherent in\ntime fractional evolutionary equations, thereby providing a solid theoretical\nfoundation for subsequent numerical investigations.",
    "published_date": "2025-01-29T00:00:00",
    "year": 2025,
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.17390v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.17372v1",
    "title": "Data-Informed Model Complexity Metric for Optimizing Symbolic Regression Models",
    "authors": [
      "Nathan Haut",
      "Zenas Huang",
      "Adam Alessio"
    ],
    "author_ids": [],
    "abstract": "Choosing models from a well-fitted evolved population that generalizes beyond\ntraining data is difficult. We introduce a pragmatic method to estimate model\ncomplexity using Hessian rank for post-processing selection. Complexity is\napproximated by averaging the model output Hessian rank across a few points\n(N=3), offering efficient and accurate rank estimates. This method aligns model\nselection with input data complexity, calculated using intrinsic dimensionality\n(ID) estimators. Using the StackGP system, we develop symbolic regression\nmodels for the Penn Machine Learning Benchmark and employ twelve\nscikit-dimension library methods to estimate ID, aligning model expressiveness\nwith dataset ID. Our data-informed complexity metric finds the ideal complexity\nwindow, balancing model expressiveness and accuracy, enhancing generalizability\nwithout bias common in methods reliant on user-defined parameters, such as\nparsimony pressure in weight selection.",
    "published_date": "2025-01-29T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.NE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.17372v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.18642v1",
    "title": "DebiasPI: Inference-time Debiasing by Prompt Iteration of a Text-to-Image Generative Model",
    "authors": [
      "Sarah Bonna",
      "Yu-Cheng Huang",
      "Ekaterina Novozhilova",
      "Sejin Paik",
      "Zhengyang Shan",
      "Michelle Yilin Feng",
      "Ge Gao",
      "Yonish Tayal",
      "Rushil Kulkarni",
      "Jialin Yu",
      "Nupur Divekar",
      "Deepti Ghadiyaram",
      "Derry Wijaya",
      "Margrit Betke"
    ],
    "author_ids": [],
    "abstract": "Ethical intervention prompting has emerged as a tool to counter demographic\nbiases of text-to-image generative AI models. Existing solutions either require\nto retrain the model or struggle to generate images that reflect desired\ndistributions on gender and race. We propose an inference-time process called\nDebiasPI for Debiasing-by-Prompt-Iteration that provides prompt intervention by\nenabling the user to control the distributions of individuals' demographic\nattributes in image generation. DebiasPI keeps track of which attributes have\nbeen generated either by probing the internal state of the model or by using\nexternal attribute classifiers. Its control loop guides the text-to-image model\nto select not yet sufficiently represented attributes, With DebiasPI, we were\nable to create images with equal representations of race and gender that\nvisualize challenging concepts of news headlines. We also experimented with the\nattributes age, body type, profession, and skin tone, and measured how\nattributes change when our intervention prompt targets the distribution of an\nunrelated attribute type. We found, for example, if the text-to-image model is\nasked to balance racial representation, gender representation improves but the\nskin tone becomes less diverse. Attempts to cover a wide range of skin colors\nwith various intervention prompts showed that the model struggles to generate\nthe palest skin tones. We conducted various ablation studies, in which we\nremoved DebiasPI's attribute control, that reveal the model's propensity to\ngenerate young, male characters. It sometimes visualized career success by\ngenerating two-panel images with a pre-success dark-skinned person becoming\nlight-skinned with success, or switching gender from pre-success female to\npost-success male, thus further motivating ethical intervention prompting with\nDebiasPI.",
    "published_date": "2025-01-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.HC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.18642v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.17255v1",
    "title": "Fair Quantitative Games",
    "authors": [
      "Ashwani Anand",
      "Satya Prakash Nayak",
      "Ritam Raha",
      "Irmak Sağlam",
      "Anne-Kathrin Schmuck"
    ],
    "author_ids": [],
    "abstract": "We examine two-player games over finite weighted graphs with quantitative\n(mean-payoff or energy) objective, where one of the players additionally needs\nto satisfy a fairness objective. The specific fairness we consider is called\n'strong transition fairness', given by a subset of edges of one of the players,\nwhich asks the player to take fair edges infinitely often if their source nodes\nare visited infinitely often.\n  We show that when fairness is imposed on player 1, these games fall within\nthe class of previously studied omega-regular mean-payoff and energy games. On\nthe other hand, when the fairness is on player 2, to the best of our knowledge,\nthese games have not been previously studied.\n  We provide gadget-based algorithms for fair mean-payoff games where fairness\nis imposed on either player, and for fair energy games where the fairness is\nimposed on player 1. For all variants of fair mean-payoff and fair energy\n(under unknown initial credit) games, we give pseudo-polynomial algorithms to\ncompute the winning regions of both players. Additionally, we analyze the\nstrategy complexities required for these games. Our work is the first to extend\nthe study of strong transition fairness, as well as gadget-based approaches, to\nthe quantitative setting. We thereby demonstrate that the simplicity of strong\ntransition fairness, as well as the applicability of gadget-based techniques,\ncan be leveraged beyond the omega-regular domain.",
    "published_date": "2025-01-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.17255v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.17150v3",
    "title": "Cultural Differences and Perverse Incentives in Science Create a Bad Mix: Exploring Country-Level Publication Bias in Select ACM Conferences",
    "authors": [
      "Aksheytha Chelikavada",
      "Casey C. Bennett"
    ],
    "author_ids": [],
    "abstract": "In the era of big science, many national governments are helping to build\nwell-funded teams of scientists to serve nationalistic ambitions, providing\nfinancial incentives for certain outcomes for purposes other than advancing\nscience. That in turn can impact the behavior of scientists and create\ndistortions in publication rates, frequency, and publication venues targeted.\nTo that end, we provide evidence that indicates significant inequality using\nstandard Gini Index metrics in the publication rates of individual scientists\nacross various groupings (e.g. country, institution type, ranking-level) based\non an intensive analysis of thousands of papers published in several well-known\nACM conferences (HRI, IUI, KDD, CHI, SIGGRAPH, UIST, and UBICOMP) over 15 years\nbetween 2010 to 2024. Furthermore, scientists who were affiliated with the\ntop-5 countries (in terms of research expenditure) were found to be\ncontributing significantly more to the inequality in publication rates than\nothers, which raises a number of questions for the scientific community. We\ndiscuss some of those questions later in the paper. We also detected several\nexamples in the dataset of potential serious ethical problems in publications\nlikely caused by such incentive systems. Finally, a topic modeling analysis\nrevealed that some countries are pursuing a much narrower range of scientific\ntopics relative to others, indicating those incentives may also be limiting\ngenuine scientific curiosity. In summary, our findings raise awareness of\nsystems put in place by certain national governments that may be eroding the\npursuit of truth through science and gradually undermining the integrity of the\nglobal scientific community.",
    "published_date": "2025-01-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.DL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.17150v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.17077v1",
    "title": "Induced Modularity and Community Detection for Functionally Interpretable Reinforcement Learning",
    "authors": [
      "Anna Soligo",
      "Pietro Ferraro",
      "David Boyle"
    ],
    "author_ids": [],
    "abstract": "Interpretability in reinforcement learning is crucial for ensuring AI systems\nalign with human values and fulfill the diverse related requirements including\nsafety, robustness and fairness. Building on recent approaches to encouraging\nsparsity and locality in neural networks, we demonstrate how the penalisation\nof non-local weights leads to the emergence of functionally independent modules\nin the policy network of a reinforcement learning agent. To illustrate this, we\ndemonstrate the emergence of two parallel modules for assessment of movement\nalong the X and Y axes in a stochastic Minigrid environment. Through the novel\napplication of community detection algorithms, we show how these modules can be\nautomatically identified and their functional roles verified through direct\nintervention on the network weights prior to inference. This establishes a\nscalable framework for reinforcement learning interpretability through\nfunctional modularity, addressing challenges regarding the trade-off between\ncompleteness and cognitive tractability of reinforcement learning explanations.",
    "published_date": "2025-01-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.17077v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.17026v1",
    "title": "Mitigating Omitted Variable Bias in Empirical Software Engineering",
    "authors": [
      "Carlo A. Furia",
      "Richard Torkar"
    ],
    "author_ids": [],
    "abstract": "Omitted variable bias occurs when a statistical model leaves out variables\nthat are relevant determinants of the effects under study. This results in the\nmodel attributing the missing variables' effect to some of the included\nvariables -- hence over- or under-estimating the latter's true effect. Omitted\nvariable bias presents a significant threat to the validity of empirical\nresearch, particularly in non-experimental studies such as those prevalent in\nempirical software engineering.\n  This paper illustrates the impact of omitted variable bias on two case\nstudies in the software engineering domain, and uses them to present methods to\ninvestigate the possible presence of omitted variable bias, to estimate its\nimpact, and to mitigate its drawbacks. The analysis techniques we present are\nbased on causal structural models of the variables of interest, which provide a\npractical, intuitive summary of the key relations among variables.\n  This paper demonstrates a sequence of analysis steps that inform the design\nand execution of any empirical study in software engineering. An important\nobservation is that it pays off to invest effort investigating omitted variable\nbias before actually executing an empirical study, because this effort can lead\nto a more solid study design, and to a significant reduction in its threats to\nvalidity.",
    "published_date": "2025-01-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.17026v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.16988v2",
    "title": "Marginal and Conditional Importance Measures from Machine Learning Models and Their Relationship with Conditional Average Treatment Effect",
    "authors": [
      "Mohammad Kaviul Anam Khan",
      "Olli Saarela",
      "Rafal Kustra"
    ],
    "author_ids": [],
    "abstract": "Interpreting black-box machine learning models is challenging due to their\nstrong dependence on data and inherently non-parametric nature. This paper\nreintroduces the concept of importance through \"Marginal Variable Importance\nMetric\" (MVIM), a model-agnostic measure of predictor importance based on the\ntrue conditional expectation function. MVIM evaluates predictors' influence on\ncontinuous or discrete outcomes. A permutation-based estimation approach,\ninspired by \\citet{breiman2001random} and \\citet{fisher2019all}, is proposed to\nestimate MVIM. MVIM estimator is biased when predictors are highly correlated,\nas black-box models struggle to extrapolate in low-probability regions. To\naddress this, we investigated the bias-variance decomposition of MVIM to\nunderstand the source and pattern of the bias under high correlation. A\nConditional Variable Importance Metric (CVIM), adapted from\n\\citet{strobl2008conditional}, is introduced to reduce this bias. Both MVIM and\nCVIM exhibit a quadratic relationship with the conditional average treatment\neffect (CATE).",
    "published_date": "2025-01-28T00:00:00",
    "year": 2025,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.16988v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.16954v2",
    "title": "The Third Moment of AI Ethics: Developing Relatable and Contextualized Tools",
    "authors": [
      "Sarah Hladikova",
      "Yuling Wang",
      "Andreia Martinho"
    ],
    "author_ids": [],
    "abstract": "Artificial intelligence (AI) ethics has gained significant momentum,\nevidenced by the growing body of published literature, policy guidelines, and\npublic discourse. However, the practical implementation and adoption of AI\nethics principles among practitioners has not kept pace with this theoretical\ndevelopment. Common barriers to adoption include overly abstract language, poor\naccessibility, and insufficient practical guidance for implementation. Through\nparticipatory design with industry practitioners, we developed an open-source\ntool that bridges this gap. Our tool is firmly grounded in normative ethical\nframeworks while offering concrete, actionable guidance in an intuitive format\nthat aligns with established software development workflows. We validated this\napproach through a proof of concept study in the United States autonomous\ndriving industry.",
    "published_date": "2025-01-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.16954v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.16896v1",
    "title": "Frequency Matters: Explaining Biases of Face Recognition in the Frequency Domain",
    "authors": [
      "Marco Huber",
      "Fadi Boutros",
      "Naser Damer"
    ],
    "author_ids": [],
    "abstract": "Face recognition (FR) models are vulnerable to performance variations across\ndemographic groups. The causes for these performance differences are unclear\ndue to the highly complex deep learning-based structure of face recognition\nmodels. Several works aimed at exploring possible roots of gender and ethnicity\nbias, identifying semantic reasons such as hairstyle, make-up, or facial hair\nas possible sources. Motivated by recent discoveries of the importance of\nfrequency patterns in convolutional neural networks, we explain bias in face\nrecognition using state-of-the-art frequency-based explanations. Our extensive\nresults show that different frequencies are important to FR models depending on\nthe ethnicity of the samples.",
    "published_date": "2025-01-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.16896v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.16888v1",
    "title": "Secure Federated Graph-Filtering for Recommender Systems",
    "authors": [
      "Julien Nicolas",
      "César Sabater",
      "Mohamed Maouche",
      "Sonia Ben Mokhtar",
      "Mark Coates"
    ],
    "author_ids": [],
    "abstract": "Recommender systems often rely on graph-based filters, such as normalized\nitem-item adjacency matrices and low-pass filters. While effective, the\ncentralized computation of these components raises concerns about privacy,\nsecurity, and the ethical use of user data. This work proposes two\ndecentralized frameworks for securely computing these critical graph components\nwithout centralizing sensitive information. The first approach leverages\nlightweight Multi-Party Computation and distributed singular vector\ncomputations to privately compute key graph filters. The second extends this\nframework by incorporating low-rank approximations, enabling a trade-off\nbetween communication efficiency and predictive performance. Empirical\nevaluations on benchmark datasets demonstrate that the proposed methods achieve\ncomparable accuracy to centralized state-of-the-art systems while ensuring data\nconfidentiality and maintaining low communication costs. Our results highlight\nthe potential for privacy-preserving decentralized architectures to bridge the\ngap between utility and user data protection in modern recommender systems.",
    "published_date": "2025-01-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IR",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.16888v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.16836v1",
    "title": "Misspellings in Natural Language Processing: A survey",
    "authors": [
      "Gianluca Sperduti",
      "Alejandro Moreo"
    ],
    "author_ids": [],
    "abstract": "This survey provides an overview of the challenges of misspellings in natural\nlanguage processing (NLP). While often unintentional, misspellings have become\nubiquitous in digital communication, especially with the proliferation of Web\n2.0, user-generated content, and informal text mediums such as social media,\nblogs, and forums. Even if humans can generally interpret misspelled text, NLP\nmodels frequently struggle to handle it: this causes a decline in performance\nin common tasks like text classification and machine translation. In this\npaper, we reconstruct a history of misspellings as a scientific problem. We\nthen discuss the latest advancements to address the challenge of misspellings\nin NLP. Main strategies to mitigate the effect of misspellings include data\naugmentation, double step, character-order agnostic, and tuple-based methods,\namong others. This survey also examines dedicated data challenges and\ncompetitions to spur progress in the field. Critical safety and ethical\nconcerns are also examined, for example, the voluntary use of misspellings to\ninject malicious messages and hate speech on social networks. Furthermore, the\nsurvey explores psycholinguistic perspectives on how humans process\nmisspellings, potentially informing innovative computational techniques for\ntext normalization and representation. Finally, the misspelling-related\nchallenges and opportunities associated with modern large language models are\nalso analyzed, including benchmarks, datasets, and performances of the most\nprominent language models against misspellings. This survey aims to be an\nexhaustive resource for researchers seeking to mitigate the impact of\nmisspellings in the rapidly evolving landscape of NLP.",
    "published_date": "2025-01-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.16836v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.16830v1",
    "title": "Statistical Analysis of Risk Assessment Factors and Metrics to Evaluate Radicalisation in Twitter",
    "authors": [
      "Raul Lara-Cabrera",
      "Antonio Gonzalez-Pardo",
      "David Camacho"
    ],
    "author_ids": [],
    "abstract": "Nowadays, Social Networks have become an essential communication tools\nproducing a large amount of information about their users and their\ninteractions, which can be analysed with Data Mining methods. In the last\nyears, Social Networks are being used to radicalise people. In this paper, we\nstudy the performance of a set of indicators and their respective metrics,\ndevoted to assess the risk of radicalisation of a precise individual on three\ndifferent datasets. Keyword-based metrics, even though depending on the written\nlanguage, performs well when measuring frustration, perception of\ndiscrimination as well as declaration of negative and positive ideas about\nWestern society and Jihadism, respectively. However, metrics based on frequent\nhabits such as writing ellipses are not well enough to characterise a user in\nrisk of radicalisation. The paper presents a detailed description of both, the\nset of indicators used to asses the radicalisation in Social Networks and the\nset of datasets used to evaluate them. Finally, an experimental study over\nthese datasets are carried out to evaluate the performance of the metrics\nconsidered.",
    "published_date": "2025-01-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SI",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.16830v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.16748v1",
    "title": "Through the Prism of Culture: Evaluating LLMs' Understanding of Indian Subcultures and Traditions",
    "authors": [
      "Garima Chhikara",
      "Abhishek Kumar",
      "Abhijnan Chakraborty"
    ],
    "author_ids": [],
    "abstract": "Large Language Models (LLMs) have shown remarkable advancements but also\nraise concerns about cultural bias, often reflecting dominant narratives at the\nexpense of under-represented subcultures. In this study, we evaluate the\ncapacity of LLMs to recognize and accurately respond to the Little Traditions\nwithin Indian society, encompassing localized cultural practices and\nsubcultures such as caste, kinship, marriage, and religion. Through a series of\ncase studies, we assess whether LLMs can balance the interplay between dominant\nGreat Traditions and localized Little Traditions. We explore various prompting\nstrategies and further investigate whether using prompts in regional languages\nenhances the models cultural sensitivity and response quality. Our findings\nreveal that while LLMs demonstrate an ability to articulate cultural nuances,\nthey often struggle to apply this understanding in practical, context-specific\nscenarios. To the best of our knowledge, this is the first study to analyze\nLLMs engagement with Indian subcultures, offering critical insights into the\nchallenges of embedding cultural diversity in AI systems.",
    "published_date": "2025-01-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.16748v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.16638v1",
    "title": "Analysis of Zero Day Attack Detection Using MLP and XAI",
    "authors": [
      "Ashim Dahal",
      "Prabin Bajgai",
      "Nick Rahimi"
    ],
    "author_ids": [],
    "abstract": "Any exploit taking advantage of zero-day is called a zero-day attack.\nPrevious research and social media trends show a massive demand for research in\nzero-day attack detection. This paper analyzes Machine Learning (ML) and Deep\nLearning (DL) based approaches to create Intrusion Detection Systems (IDS) and\nscrutinizing them using Explainable AI (XAI) by training an explainer based on\nrandomly sampled data from the testing set. The focus is on using the KDD99\ndataset, which has the most research done among all the datasets for detecting\nzero-day attacks. The paper aims to synthesize the dataset to have fewer\nclasses for multi-class classification, test ML and DL approaches on pattern\nrecognition, establish the robustness and dependability of the model, and\nestablish the interpretability and scalability of the model. We evaluated the\nperformance of four multilayer perceptron (MLP) trained on the KDD99 dataset,\nincluding baseline ML models, weighted ML models, truncated ML models, and\nweighted truncated ML models. Our results demonstrate that the truncated ML\nmodel achieves the highest accuracy (99.62%), precision, and recall, while\nweighted truncated ML model shows lower accuracy (97.26%) but better class\nrepresentation (less bias) among all the classes with improved unweighted\nrecall score. We also used Shapely Additive exPlanations (SHAP) to train\nexplainer for our truncated models to check for feature importance among the\ntwo weighted and unweighted models.",
    "published_date": "2025-01-28T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.16638v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.16549v1",
    "title": "Reconciling Predictive Multiplicity in Practice",
    "authors": [
      "Tina Behzad",
      "Sílvia Casacuberta",
      "Emily Ruth Diana",
      "Alexander Williams Tolbert"
    ],
    "author_ids": [],
    "abstract": "Many machine learning applications predict individual probabilities, such as\nthe likelihood that a person develops a particular illness. Since these\nprobabilities are unknown, a key question is how to address situations in which\ndifferent models trained on the same dataset produce varying predictions for\ncertain individuals. This issue is exemplified by the model multiplicity (MM)\nphenomenon, where a set of comparable models yield inconsistent predictions.\nRoth, Tolbert, and Weinstein recently introduced a reconciliation procedure,\nthe Reconcile algorithm, to address this problem. Given two disagreeing models,\nthe algorithm leverages their disagreement to falsify and improve at least one\nof the models. In this paper, we empirically analyze the Reconcile algorithm\nusing five widely-used fairness datasets: COMPAS, Communities and Crime, Adult,\nStatlog (German Credit Data), and the ACS Dataset. We examine how Reconcile\nfits within the model multiplicity literature and compare it to existing MM\nsolutions, demonstrating its effectiveness. We also discuss potential\nimprovements to the Reconcile algorithm theoretically and practically. Finally,\nwe extend the Reconcile algorithm to the setting of causal inference, given\nthat different competing estimators can again disagree on specific causal\naverage treatment effect (CATE) values. We present the first extension of the\nReconcile algorithm in causal inference, analyze its theoretical properties,\nand conduct empirical tests. Our results confirm the practical effectiveness of\nReconcile and its applicability across various domains.",
    "published_date": "2025-01-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.16549v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.16531v1",
    "title": "Responsible Generative AI Use by Product Managers: Recoupling Ethical Principles and Practices",
    "authors": [
      "Genevieve Smith",
      "Natalia Luka",
      "Merrick Osborne",
      "Brian Lattimore",
      "Jessica Newman",
      "Brandie Nonnecke",
      "Brent Mittelstadt"
    ],
    "author_ids": [],
    "abstract": "Since 2022, generative AI (genAI) has rapidly become integrated into\nworkplaces. Though organizations have made commitments to use this technology\n\"responsibly\", how organizations and their employees prioritize responsibility\nin their decision-making remains absent from extant management theorizing. In\nthis paper, we examine how product managers - who often serve as gatekeepers in\ndecision-making processes - implement responsible practices in their day-to-day\nwork when using genAI. Using Institutional Theory, we illuminate the factors\nthat constrain or support proactive responsible development and usage of genAI\ntechnologies. We employ a mixed methods research design, drawing on 25\ninterviews with product managers and a global survey of 300 respondents in\nproduct management-related roles. The majority of our respondents report (1)\nwidespread uncertainty regarding what \"responsibility\" means or looks like, (2)\ndiffused responsibility given assumed ethical actions by other teams, (3) lack\nof clear incentives and guidance within organizations, and (4) the importance\nof leadership buy-in and principles for navigating tensions between ethical\ncommitments and profit motives. However, our study finds that even in highly\nuncertain environments, absent guidance from leadership, product managers can\n\"recouple\" ethical commitments and practices by finding responsibility\n\"micro-moments\". Product managers seek out low-risk, small-scale actions they\ncan take without explicit buy-in from higher-level managers, such as individual\nor team-wide checks and reviews and safeguarding standards for data. Our\nresearch highlights how genAI poses unique challenges to organizations trying\nto couple ethical principles and daily practices and the role that middle-level\nmanagement can play in recoupling the two.",
    "published_date": "2025-01-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.16531v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.18632v2",
    "title": "Towards Safe AI Clinicians: A Comprehensive Study on Large Language Model Jailbreaking in Healthcare",
    "authors": [
      "Hang Zhang",
      "Qian Lou",
      "Yanshan Wang"
    ],
    "author_ids": [],
    "abstract": "Large language models (LLMs) are increasingly utilized in healthcare\napplications. However, their deployment in clinical practice raises significant\nsafety concerns, including the potential spread of harmful information. This\nstudy systematically assesses the vulnerabilities of seven LLMs to three\nadvanced black-box jailbreaking techniques within medical contexts. To quantify\nthe effectiveness of these techniques, we propose an automated and\ndomain-adapted agentic evaluation pipeline. Experiment results indicate that\nleading commercial and open-source LLMs are highly vulnerable to medical\njailbreaking attacks. To bolster model safety and reliability, we further\ninvestigate the effectiveness of Continual Fine-Tuning (CFT) in defending\nagainst medical adversarial attacks. Our findings underscore the necessity for\nevolving attack methods evaluation, domain-specific safety alignment, and LLM\nsafety-utility balancing. This research offers actionable insights for\nadvancing the safety and reliability of AI clinicians, contributing to ethical\nand effective AI deployment in healthcare.",
    "published_date": "2025-01-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CR",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.18632v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.16524v1",
    "title": "Programming by Examples Meets Historical Linguistics: A Large Language Model Based Approach to Sound Law Induction",
    "authors": [
      "Atharva Naik",
      "Darsh Agrawal",
      "Hong Sng",
      "Clayton Marr",
      "Kexun Zhang",
      "Nathaniel R Robinson",
      "Kalvin Chang",
      "Rebecca Byrnes",
      "Aravind Mysore",
      "Carolyn Rose",
      "David R Mortensen"
    ],
    "author_ids": [],
    "abstract": "Historical linguists have long written \"programs\" that convert reconstructed\nwords in an ancestor language into their attested descendants via ordered\nstring rewrite functions (called sound laws) However, writing these programs is\ntime-consuming, motivating the development of automated Sound Law Induction\n(SLI) which we formulate as Programming by Examples (PBE) with Large Language\nModels (LLMs) in this paper. While LLMs have been effective for code\ngeneration, recent work has shown that PBE is challenging but improvable by\nfine-tuning, especially with training data drawn from the same distribution as\nevaluation data. In this paper, we create a conceptual framework of what\nconstitutes a \"similar distribution\" for SLI and propose four kinds of\nsynthetic data generation methods with varying amounts of inductive bias to\ninvestigate what leads to the best performance. Based on the results we create\na SOTA open-source model for SLI as PBE (+6% pass rate with a third of the\nparameters of the second-best LLM) and also highlight exciting future\ndirections for PBE research.",
    "published_date": "2025-01-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.16524v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.16322v1",
    "title": "Implicit Bias in Matrix Factorization and its Explicit Realization in a New Architecture",
    "authors": [
      "Yikun Hou",
      "Suvrit Sra",
      "Alp Yurtsever"
    ],
    "author_ids": [],
    "abstract": "Gradient descent for matrix factorization is known to exhibit an implicit\nbias toward approximately low-rank solutions. While existing theories often\nassume the boundedness of iterates, empirically the bias persists even with\nunbounded sequences. We thus hypothesize that implicit bias is driven by\ndivergent dynamics markedly different from the convergent dynamics for data\nfitting. Using this perspective, we introduce a new factorization model:\n$X\\approx UDV^\\top$, where $U$ and $V$ are constrained within norm balls, while\n$D$ is a diagonal factor allowing the model to span the entire search space.\nOur experiments reveal that this model exhibits a strong implicit bias\nregardless of initialization and step size, yielding truly (rather than\napproximately) low-rank solutions. Furthermore, drawing parallels between\nmatrix factorization and neural networks, we propose a novel neural network\nmodel featuring constrained layers and diagonal components. This model achieves\nstrong performance across various regression and classification tasks while\nfinding low-rank solutions, resulting in efficient and lightweight networks.",
    "published_date": "2025-01-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.16322v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.16080v1",
    "title": "Generating Spatial Synthetic Populations Using Wasserstein Generative Adversarial Network: A Case Study with EU-SILC Data for Helsinki and Thessaloniki",
    "authors": [
      "Vanja Falck"
    ],
    "author_ids": [],
    "abstract": "Using agent-based social simulations can enhance our understanding of urban\nplanning, public health, and economic forecasting. Realistic synthetic\npopulations with numerous attributes strengthen these simulations. The\nWasserstein Generative Adversarial Network, trained on census data like\nEU-SILC, can create robust synthetic populations. These methods, aided by\nexternal statistics or EU-SILC weights, generate spatial synthetic populations\nfor agent-based models. The increased access to high-quality micro-data has\nsparked interest in synthetic populations, which preserve demographic profiles\nand analytical strength while ensuring privacy and preventing discrimination.\nThis study uses national data from Finland and Greece for Helsinki and\nThessaloniki to explore balanced spatial synthetic population generation.\nResults show challenges related to balancing data with or without aggregated\nstatistics for the target population and the general under-representation of\nfringe profiles by deep generative methods. The latter can lead to\ndiscrimination in agent-based simulations.",
    "published_date": "2025-01-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.MA",
      "I.5.4; I.6.7; I.2.11"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.16080v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.16061v1",
    "title": "The Unbearable Lightness of Prompting: A Critical Reflection on the Environmental Impact of genAI use in Design Education",
    "authors": [
      "Maria Luce Lupetti",
      "Elena Cavallin",
      "Dave Murray-Rust"
    ],
    "author_ids": [],
    "abstract": "Design educators are finding ways to support students in skillfully using\nGenAI tools in their practices while encouraging the critical scrutiny of the\nethical and social issues around these technologies. However, the issue of\nenvironmental sustainability remains unaddressed. There is a lack of both\nresources to grasp the environmental costs of genAI in education and a lack of\nshared practices for engaging with the issue. This paper critically reflects on\nthe energy costs of using genAI in design education, using a workshop held in\n2023 with 49 students as a motivating example. Through this reflection, we\ndevelop a set of five alternative stances, with related actions, that support\nthe conscious use of genAI in design education. The work contributes to the\nfield of design and HCI by bringing together ways for educators to reflect on\ntheir practices, informing the future development of educational programs\naround genAI.",
    "published_date": "2025-01-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.16061v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.15985v1",
    "title": "Demographic Benchmarking: Bridging Socio-Technical Gaps in Bias Detection",
    "authors": [
      "Gemma Galdon Clavell",
      "Rubén González-Sendino",
      "Paola Vazquez"
    ],
    "author_ids": [],
    "abstract": "Artificial intelligence (AI) models are increasingly autonomous in\ndecision-making, making pursuing responsible AI more critical than ever.\nResponsible AI (RAI) is defined by its commitment to transparency, privacy,\nsafety, inclusiveness, and fairness. But while the principles of RAI are clear\nand shared, RAI practices and auditing mechanisms are still incipient. A key\nchallenge is establishing metrics and benchmarks that define performance goals\naligned with RAI principles. This paper describes how the ITACA AI auditing\nplatform developed by Eticas.ai tackles demographic benchmarking when auditing\nAI recommender systems. To this end, we describe a Demographic Benchmarking\nFramework designed to measure the populations potentially impacted by specific\nAI models. The framework serves us as auditors as it allows us to not just\nmeasure but establish acceptability ranges for specific performance indicators,\nwhich we share with the developers of the systems we audit so they can build\nbalanced training datasets and measure and monitor fairness throughout the AI\nlifecycle. It is also a valuable resource for policymakers in drafting\neffective and enforceable regulations. Our approach integrates\nsocio-demographic insights directly into AI systems, reducing bias and\nimproving overall performance. The main contributions of this study include:1.\nDefining control datasets tailored to specific demographics so they can be used\nin model training; 2. Comparing the overall population with those impacted by\nthe deployed model to identify discrepancies and account for structural bias;\nand 3. Quantifying drift in different scenarios continuously and as a\npost-market monitoring mechanism.",
    "published_date": "2025-01-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.15985v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.15955v1",
    "title": "Rethinking the Bias of Foundation Model under Long-tailed Distribution",
    "authors": [
      "Jiahao Chen",
      "Bin Qin",
      "Jiangmeng Li",
      "Hao Chen",
      "Bing Su"
    ],
    "author_ids": [],
    "abstract": "Long-tailed learning has garnered increasing attention due to its practical\nsignificance. Among the various approaches, the fine-tuning paradigm has gained\nconsiderable interest with the advent of foundation models. However, most\nexisting methods primarily focus on leveraging knowledge from these models,\noverlooking the inherent biases introduced by the imbalanced training data they\nrely on. In this paper, we examine how such imbalances from pre-training affect\nlong-tailed downstream tasks. Specifically, we find the imbalance biases\ninherited in foundation models on downstream task as parameter imbalance and\ndata imbalance. During fine-tuning, we observe that parameter imbalance plays a\nmore critical role, while data imbalance can be mitigated using existing\nre-balancing strategies. Moreover, we find that parameter imbalance cannot be\neffectively addressed by current re-balancing techniques, such as adjusting the\nlogits, during training, unlike data imbalance. To tackle both imbalances\nsimultaneously, we build our method on causal learning and view the incomplete\nsemantic factor as the confounder, which brings spurious correlations between\ninput samples and labels. To resolve the negative effects of this, we propose a\nnovel backdoor adjustment method that learns the true causal effect between\ninput samples and labels, rather than merely fitting the correlations in the\ndata. Notably, we achieve an average performance increase of about $1.67\\%$ on\neach dataset.",
    "published_date": "2025-01-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.15955v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.15920v1",
    "title": "Vienna Mosaic: Navigating Social Borders in a Melting Pot",
    "authors": [
      "Marc Sadurní",
      "Samuel Martin-Gutierrez",
      "Ola Ali",
      "Ana María Jaramillo",
      "Rafael Prieto-Curiel",
      "Fariba Karimi"
    ],
    "author_ids": [],
    "abstract": "Urban segregation poses a critical challenge in cities, exacerbating\ninequalities, social tensions, fears, and polarization. It emerges from a\ncomplex interplay of socioeconomic disparities and residential preferences,\ndisproportionately impacting migrant communities. In this paper, using a\ncomprehensive administrative data from Vienna, where nearly 40% of the\npopulation consists of international migrants, we analyse co-residence\npreferences between migrants and locals at the neighbourhood level. Our\nfindings reveal two major clusters in Vienna shaped by wealth disparities,\ndistrict diversity, and nationality-based homophily. These insights shed light\non the underlying mechanisms of urban segregation and designing policies for\nbetter integration.",
    "published_date": "2025-01-27T00:00:00",
    "year": 2025,
    "categories": [
      "physics.soc-ph",
      "cs.SI",
      "physics.data-an"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.15920v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.15816v1",
    "title": "AdaF^2M^2: Comprehensive Learning and Responsive Leveraging Features in Recommendation System",
    "authors": [
      "Yongchun Zhu",
      "Jingwu Chen",
      "Ling Chen",
      "Yitan Li",
      "Feng Zhang",
      "Xiao Yang",
      "Zuotao Liu"
    ],
    "author_ids": [],
    "abstract": "Feature modeling, which involves feature representation learning and\nleveraging, plays an essential role in industrial recommendation systems.\nHowever, the data distribution in real-world applications usually follows a\nhighly skewed long-tail pattern due to the popularity bias, which easily leads\nto over-reliance on ID-based features, such as user/item IDs and ID sequences\nof interactions. Such over-reliance makes it hard for models to learn features\ncomprehensively, especially for those non-ID meta features, e.g., user/item\ncharacteristics. Further, it limits the feature leveraging ability in models,\ngetting less generalized and more susceptible to data noise. Previous studies\non feature modeling focus on feature extraction and interaction, hardly\nnoticing the problems brought about by the long-tail data distribution. To\nachieve better feature representation learning and leveraging on real-world\ndata, we propose a model-agnostic framework AdaF^2M^2, short for Adaptive\nFeature Modeling with Feature Mask. The feature-mask mechanism helps\ncomprehensive feature learning via multi-forward training with augmented\nsamples, while the adapter applies adaptive weights on features responsive to\ndifferent user/item states. By arming base models with AdaF^2M^2, we conduct\nonline A/B tests on multiple recommendation scenarios, obtaining +1.37% and\n+1.89% cumulative improvements on user active days and app duration\nrespectively. Besides, the extended offline experiments on different models\nshow improvements as well. AdaF$^2$M$^2$ has been widely deployed on both\nretrieval and ranking tasks in multiple applications of Douyin Group,\nindicating its superior effectiveness and universality.",
    "published_date": "2025-01-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.15816v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.07794v1",
    "title": "Regulatory Science Innovation for Generative AI and Large Language Models in Health and Medicine: A Global Call for Action",
    "authors": [
      "Jasmine Chiat Ling Ong",
      "Yilin Ning",
      "Mingxuan Liu",
      "Yian Ma",
      "Zhao Liang",
      "Kuldev Singh",
      "Robert T Chang",
      "Silke Vogel",
      "John CW Lim",
      "Iris Siu Kwan Tan",
      "Oscar Freyer",
      "Stephen Gilbert",
      "Danielle S Bitterman",
      "Xiaoxuan Liu",
      "Alastair K Denniston",
      "Nan Liu"
    ],
    "author_ids": [],
    "abstract": "The integration of generative AI (GenAI) and large language models (LLMs) in\nhealthcare presents both unprecedented opportunities and challenges,\nnecessitating innovative regulatory approaches. GenAI and LLMs offer broad\napplications, from automating clinical workflows to personalizing diagnostics.\nHowever, the non-deterministic outputs, broad functionalities and complex\nintegration of GenAI and LLMs challenge existing medical device regulatory\nframeworks, including the total product life cycle (TPLC) approach. Here we\ndiscuss the constraints of the TPLC approach to GenAI and LLM-based medical\ndevice regulation, and advocate for global collaboration in regulatory science\nresearch. This serves as the foundation for developing innovative approaches\nincluding adaptive policies and regulatory sandboxes, to test and refine\ngovernance in real-world settings. International harmonization, as seen with\nthe International Medical Device Regulators Forum, is essential to manage\nimplications of LLM on global health, including risks of widening health\ninequities driven by inherent model biases. By engaging multidisciplinary\nexpertise, prioritizing iterative, data-driven approaches, and focusing on the\nneeds of diverse populations, global regulatory science research enables the\nresponsible and equitable advancement of LLM innovations in healthcare.",
    "published_date": "2025-01-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.07794v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.16399v1",
    "title": "Detecting clinician implicit biases in diagnoses using proximal causal inference",
    "authors": [
      "Kara Liu",
      "Russ Altman",
      "Vasilis Syrgkanis"
    ],
    "author_ids": [],
    "abstract": "Clinical decisions to treat and diagnose patients are affected by implicit\nbiases formed by racism, ableism, sexism, and other stereotypes. These biases\nreflect broader systemic discrimination in healthcare and risk marginalizing\nalready disadvantaged groups. Existing methods for measuring implicit biases\nrequire controlled randomized testing and only capture individual attitudes\nrather than outcomes. However, the \"big-data\" revolution has led to the\navailability of large observational medical datasets, like EHRs and biobanks,\nthat provide the opportunity to investigate discrepancies in patient health\noutcomes. In this work, we propose a causal inference approach to detect the\neffect of clinician implicit biases on patient outcomes in large-scale medical\ndata. Specifically, our method uses proximal mediation to disentangle\npathway-specific effects of a patient's sociodemographic attribute on a\nclinician's diagnosis decision. We test our method on real-world data from the\nUK Biobank. Our work can serve as a tool that initiates conversation and brings\nawareness to unequal health outcomes caused by implicit biases.",
    "published_date": "2025-01-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.16399v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.15782v1",
    "title": "Online Allocation with Multi-Class Arrivals: Group Fairness vs Individual Welfare",
    "authors": [
      "Faraz Zargari",
      "Hossein Nekouyan Jazi",
      "Bo Sun",
      "Xiaoqi Tan"
    ],
    "author_ids": [],
    "abstract": "We introduce and study a multi-class online resource allocation problem with\ngroup fairness guarantees. The problem involves allocating a fixed amount of\nresources to a sequence of agents, each belonging to a specific group. The\nprimary objective is to ensure fairness across different groups in an online\nsetting. We focus on three fairness notions: one based on quantity and two\nbased on utility. To achieve fair allocations, we develop two threshold-based\nonline algorithms, proving their optimality under two fairness notions and\nnear-optimality for the more challenging one. Additionally, we demonstrate a\nfundamental trade-off between group fairness and individual welfare using a\nnovel representative function-based approach. To address this trade-off, we\npropose a set-aside multi-threshold algorithm that reserves a portion of the\nresource to ensure fairness across groups while utilizing the remaining\nresource to optimize efficiency under utility-based fairness notions. This\nalgorithm is proven to achieve the Pareto-optimal trade-off. We also\ndemonstrate that our problem can model a wide range of real-world applications,\nincluding network caching and cloud computing, and empirically evaluate our\nproposed algorithms in the network caching problem using real datasets.",
    "published_date": "2025-01-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.GT",
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.15782v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.15775v1",
    "title": "Do Existing Testing Tools Really Uncover Gender Bias in Text-to-Image Models?",
    "authors": [
      "Yunbo Lyu",
      "Zhou Yang",
      "Yuqing Niu",
      "Jing Jiang",
      "David Lo"
    ],
    "author_ids": [],
    "abstract": "Text-to-Image (T2I) models have recently gained significant attention due to\ntheir ability to generate high-quality images and are consequently used in a\nwide range of applications. However, there are concerns about the gender bias\nof these models. Previous studies have shown that T2I models can perpetuate or\neven amplify gender stereotypes when provided with neutral text prompts.\nResearchers have proposed automated gender bias uncovering detectors for T2I\nmodels, but a crucial gap exists: no existing work comprehensively compares the\nvarious detectors and understands how the gender bias detected by them deviates\nfrom the actual situation. This study addresses this gap by validating previous\ngender bias detectors using a manually labeled dataset and comparing how the\nbias identified by various detectors deviates from the actual bias in T2I\nmodels, as verified by manual confirmation. We create a dataset consisting of\n6,000 images generated from three cutting-edge T2I models: Stable Diffusion XL,\nStable Diffusion 3, and Dreamlike Photoreal 2.0. During the human-labeling\nprocess, we find that all three T2I models generate a portion (12.48% on\naverage) of low-quality images (e.g., generate images with no face present),\nwhere human annotators cannot determine the gender of the person. Our analysis\nreveals that all three T2I models show a preference for generating male images,\nwith SDXL being the most biased. Additionally, images generated using prompts\ncontaining professional descriptions (e.g., lawyer or doctor) show the most\nbias. We evaluate seven gender bias detectors and find that none fully capture\nthe actual level of bias in T2I models, with some detectors overestimating bias\nby up to 26.95%. We further investigate the causes of inaccurate estimations,\nhighlighting the limitations of detectors in dealing with low-quality images.\nBased on our findings, we propose an enhanced detector...",
    "published_date": "2025-01-27T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.15775v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.15691v2",
    "title": "An Empirical Study on Decision-Making Aspects in Responsible Software Engineering for AI",
    "authors": [
      "Lekshmi Murali Rani",
      "Faezeh Mohammadi",
      "Robert Feldt",
      "Richard Berntsson Svensson"
    ],
    "author_ids": [],
    "abstract": "Incorporating responsible practices into software engineering (SE) for AI is\nessential to ensure ethical principles, societal impact, and accountability\nremain at the forefront of AI system design and deployment. This study\ninvestigates the ethical challenges and complexities inherent in responsible\nsoftware engineering (RSE) for AI, underscoring the need for\npractical,scenario-driven operational guidelines. Given the complexity of AI\nand the relative inexperience of professionals in this rapidly evolving field,\ncontinuous learning and market adaptation are crucial. Through qualitative\ninterviews with seven practitioners(conducted until saturation), quantitative\nsurveys of 51 practitioners, and static validation of results with four\nindustry experts in AI, this study explores how personal values, emerging\nroles, and awareness of AIs societal impact influence responsible\ndecision-making in RSE for AI. A key finding is the gap between the current\nstate of the art and actual practice in RSE for AI, particularly in the failure\nto operationalize ethical and responsible decision-making within the software\nengineering life cycle for AI. While ethical issues in RSE for AI largely\nmirror those found in broader SE process, the study highlights a distinct lack\nof operational frameworks and resources to guide RSE practices for AI\neffectively. The results reveal that current ethical guidelines are\ninsufficiently implemented at the operational level, reinforcing the complexity\nof embedding ethics throughout the software engineering life cycle. The study\nconcludes that interdisciplinary collaboration, H-shaped\ncompetencies(Ethical-Technical dual competence), and a strong organizational\nculture of ethics are critical for fostering RSE practices for AI, with a\nparticular focus on transparency and accountability.",
    "published_date": "2025-01-26T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.15691v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.15634v1",
    "title": "Be Intentional About Fairness!: Fairness, Size, and Multiplicity in the Rashomon Set",
    "authors": [
      "Gordon Dai",
      "Pavan Ravishankar",
      "Rachel Yuan",
      "Daniel B. Neill",
      "Emily Black"
    ],
    "author_ids": [],
    "abstract": "When selecting a model from a set of equally performant models, how much\nunfairness can you really reduce? Is it important to be intentional about\nfairness when choosing among this set, or is arbitrarily choosing among the set\nof ''good'' models good enough? Recent work has highlighted that the phenomenon\nof model multiplicity-where multiple models with nearly identical predictive\naccuracy exist for the same task-has both positive and negative implications\nfor fairness, from strengthening the enforcement of civil rights law in AI\nsystems to showcasing arbitrariness in AI decision-making. Despite the enormous\nimplications of model multiplicity, there is little work that explores the\nproperties of sets of equally accurate models, or Rashomon sets, in general. In\nthis paper, we present five main theoretical and methodological contributions\nwhich help us to understand the relatively unexplored properties of the\nRashomon set, in particular with regards to fairness. Our contributions include\nmethods for efficiently sampling models from this set and techniques for\nidentifying the fairest models according to key fairness metrics such as\nstatistical parity. We also derive the probability that an individual's\nprediction will be flipped within the Rashomon set, as well as expressions for\nthe set's size and the distribution of error tolerance used across models.\nThese results lead to policy-relevant takeaways, such as the importance of\nintentionally looking for fair models within the Rashomon set, and\nunderstanding which individuals or groups may be more susceptible to arbitrary\ndecisions.",
    "published_date": "2025-01-26T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.15634v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.15610v1",
    "title": "Radiologist-in-the-Loop Self-Training for Generalizable CT Metal Artifact Reduction",
    "authors": [
      "Chenglong Ma",
      "Zilong Li",
      "Yuanlin Li",
      "Jing Han",
      "Junping Zhang",
      "Yi Zhang",
      "Jiannan Liu",
      "Hongming Shan"
    ],
    "author_ids": [],
    "abstract": "Metal artifacts in computed tomography (CT) images can significantly degrade\nimage quality and impede accurate diagnosis. Supervised metal artifact\nreduction (MAR) methods, trained using simulated datasets, often struggle to\nperform well on real clinical CT images due to a substantial domain gap.\nAlthough state-of-the-art semi-supervised methods use pseudo ground-truths\ngenerated by a prior network to mitigate this issue, their reliance on a fixed\nprior limits both the quality and quantity of these pseudo ground-truths,\nintroducing confirmation bias and reducing clinical applicability. To address\nthese limitations, we propose a novel Radiologist-In-the-loop SElf-training\nframework for MAR, termed RISE-MAR, which can integrate radiologists' feedback\ninto the semi-supervised learning process, progressively improving the quality\nand quantity of pseudo ground-truths for enhanced generalization on real\nclinical CT images. For quality assurance, we introduce a clinical quality\nassessor model that emulates radiologist evaluations, effectively selecting\nhigh-quality pseudo ground-truths for semi-supervised training. For quantity\nassurance, our self-training framework iteratively generates additional\nhigh-quality pseudo ground-truths, expanding the clinical dataset and further\nimproving model generalization. Extensive experimental results on multiple\nclinical datasets demonstrate the superior generalization performance of our\nRISE-MAR over state-of-the-art methods, advancing the development of MAR models\nfor practical application. Code is available at\nhttps://github.com/Masaaki-75/rise-mar.",
    "published_date": "2025-01-26T00:00:00",
    "year": 2025,
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.15610v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.15599v1",
    "title": "Evaluating an LLM-Powered Chatbot for Cognitive Restructuring: Insights from Mental Health Professionals",
    "authors": [
      "Yinzhou Wang",
      "Yimeng Wang",
      "Ye Xiao",
      "Liabette Escamilla",
      "Bianca Augustine",
      "Kelly Crace",
      "Gang Zhou",
      "Yixuan Zhang"
    ],
    "author_ids": [],
    "abstract": "Recent advancements in large language models (LLMs) promise to expand mental\nhealth interventions by emulating therapeutic techniques, potentially easing\nbarriers to care. Yet there is a lack of real-world empirical evidence\nevaluating the strengths and limitations of LLM-enabled psychotherapy\ninterventions. In this work, we evaluate an LLM-powered chatbot, designed via\nprompt engineering to deliver cognitive restructuring (CR), with 19 users.\nMental health professionals then examined the resulting conversation logs to\nuncover potential benefits and pitfalls. Our findings indicate that an\nLLM-based CR approach has the capability to adhere to core CR protocols, prompt\nSocratic questioning, and provide empathetic validation. However, issues of\npower imbalances, advice-giving, misunderstood cues, and excessive positivity\nreveal deeper challenges, including the potential to erode therapeutic rapport\nand ethical concerns. We also discuss design implications for leveraging LLMs\nin psychotherapy and underscore the importance of expert oversight to mitigate\nthese concerns, which are critical steps toward safer, more effective\nAI-assisted interventions.",
    "published_date": "2025-01-26T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.15599v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.15571v1",
    "title": "Cross-Cultural Fashion Design via Interactive Large Language Models and Diffusion Models",
    "authors": [
      "Spencer Ramsey",
      "Amina Grant",
      "Jeffrey Lee"
    ],
    "author_ids": [],
    "abstract": "Fashion content generation is an emerging area at the intersection of\nartificial intelligence and creative design, with applications ranging from\nvirtual try-on to culturally diverse design prototyping. Existing methods often\nstruggle with cultural bias, limited scalability, and alignment between textual\nprompts and generated visuals, particularly under weak supervision. In this\nwork, we propose a novel framework that integrates Large Language Models (LLMs)\nwith Latent Diffusion Models (LDMs) to address these challenges. Our method\nleverages LLMs for semantic refinement of textual prompts and introduces a weak\nsupervision filtering module to effectively utilize noisy or weakly labeled\ndata. By fine-tuning the LDM on an enhanced DeepFashion+ dataset enriched with\nglobal fashion styles, the proposed approach achieves state-of-the-art\nperformance. Experimental results demonstrate that our method significantly\noutperforms baselines, achieving lower Frechet Inception Distance (FID) and\nhigher Inception Scores (IS), while human evaluations confirm its ability to\ngenerate culturally diverse and semantically relevant fashion content. These\nresults highlight the potential of LLM-guided diffusion models in driving\nscalable and inclusive AI-driven fashion innovation.",
    "published_date": "2025-01-26T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.15571v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.15549v1",
    "title": "Optimal Transport on Categorical Data for Counterfactuals using Compositional Data and Dirichlet Transport",
    "authors": [
      "Agathe Fernandes Machado",
      "Arthur Charpentier",
      "Ewen Gallic"
    ],
    "author_ids": [],
    "abstract": "Recently, optimal transport-based approaches have gained attention for\nderiving counterfactuals, e.g., to quantify algorithmic discrimination.\nHowever, in the general multivariate setting, these methods are often opaque\nand difficult to interpret. To address this, alternative methodologies have\nbeen proposed, using causal graphs combined with iterative quantile regressions\n(Ple\\v{c}ko and Meinshausen (2020)) or sequential transport (Fernandes Machado\net al. (2025)) to examine fairness at the individual level, often referred to\nas ``counterfactual fairness.'' Despite these advancements, transporting\ncategorical variables remains a significant challenge in practical applications\nwith real datasets. In this paper, we propose a novel approach to address this\nissue. Our method involves (1) converting categorical variables into\ncompositional data and (2) transporting these compositions within the\nprobabilistic simplex of $\\mathbb{R}^d$. We demonstrate the applicability and\neffectiveness of this approach through an illustration on real-world data, and\ndiscuss limitations.",
    "published_date": "2025-01-26T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "stat.ME"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.15549v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.15504v1",
    "title": "Task Scheduling in Geo-Distributed Computing: A Survey",
    "authors": [
      "Yujian Wu",
      "Shanjiang Tang",
      "Ce Yu",
      "Bin Yang",
      "Chao Sun",
      "Jian Xiao",
      "Hutong Wu"
    ],
    "author_ids": [],
    "abstract": "Geo-distributed computing, a paradigm that assigns computational tasks to\nglobally distributed nodes, has emerged as a promising approach in cloud\ncomputing, edge computing, cloud-edge computing and supercomputer computing\n(HPC). It enables low-latency services, ensures data locality, and handles\nlarge-scale applications. As global computing capacity and task demands\nincrease rapidly, scheduling tasks for efficient execution in geo-distributed\ncomputing systems has become an increasingly critical research challenge. It\narises from the inherent characteristics of geographic distribution, including\nheterogeneous network conditions, region-specific resource pricing, and varying\ncomputational capabilities across locations. Researchers have developed diverse\ntask scheduling methods tailored to geo-distributed scenarios, aiming to\nachieve objectives such as performance enhancement, fairness assurance, and\nfault-tolerance improvement. This survey provides a comprehensive and\nsystematic review of task scheduling techniques across four major distributed\ncomputing environments, with an in-depth analysis of these approaches based on\ntheir core scheduling objectives. Through our analysis, we identify key\nresearch challenges and outline promising directions for advancing task\nscheduling in geo-distributed computing.",
    "published_date": "2025-01-26T00:00:00",
    "year": 2025,
    "categories": [
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.15504v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.15454v1",
    "title": "On the Discrimination and Consistency for Exemplar-Free Class Incremental Learning",
    "authors": [
      "Tianqi Wang",
      "Jingcai Guo",
      "Depeng Li",
      "Zhi Chen"
    ],
    "author_ids": [],
    "abstract": "Exemplar-free class incremental learning (EF-CIL) is a nontrivial task that\nrequires continuously enriching model capability with new classes while\nmaintaining previously learned knowledge without storing and replaying any old\nclass exemplars. An emerging theory-guided framework for CIL trains\ntask-specific models for a shared network, shifting the pressure of forgetting\nto task-id prediction. In EF-CIL, task-id prediction is more challenging due to\nthe lack of inter-task interaction (e.g., replays of exemplars). To address\nthis issue, we conduct a theoretical analysis of the importance and feasibility\nof preserving a discriminative and consistent feature space, upon which we\npropose a novel method termed DCNet. Concretely, it progressively maps class\nrepresentations into a hyperspherical space, in which different classes are\northogonally distributed to achieve ample inter-class separation. Meanwhile, it\nalso introduces compensatory training to adaptively adjust supervision\nintensity, thereby aligning the degree of intra-class aggregation. Extensive\nexperiments and theoretical analysis verified the superiority of the proposed\nDCNet.",
    "published_date": "2025-01-26T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.15454v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.15431v1",
    "title": "Self-supervised Benchmark Lottery on ImageNet: Do Marginal Improvements Translate to Improvements on Similar Datasets?",
    "authors": [
      "Utku Ozbulak",
      "Esla Timothy Anzaku",
      "Solha Kang",
      "Wesley De Neve",
      "Joris Vankerschaver"
    ],
    "author_ids": [],
    "abstract": "Machine learning (ML) research strongly relies on benchmarks in order to\ndetermine the relative effectiveness of newly proposed models. Recently, a\nnumber of prominent research effort argued that a number of models that improve\nthe state-of-the-art by a small margin tend to do so by winning what they call\na \"benchmark lottery\". An important benchmark in the field of machine learning\nand computer vision is the ImageNet where newly proposed models are often\nshowcased based on their performance on this dataset. Given the large number of\nself-supervised learning (SSL) frameworks that has been proposed in the past\ncouple of years each coming with marginal improvements on the ImageNet dataset,\nin this work, we evaluate whether those marginal improvements on ImageNet\ntranslate to improvements on similar datasets or not. To do so, we investigate\ntwelve popular SSL frameworks on five ImageNet variants and discover that\nmodels that seem to perform well on ImageNet may experience significant\nperformance declines on similar datasets. Specifically, state-of-the-art\nframeworks such as DINO and Swav, which are praised for their performance,\nexhibit substantial drops in performance while MoCo and Barlow Twins displays\ncomparatively good results. As a result, we argue that otherwise good and\ndesirable properties of models remain hidden when benchmarking is only\nperformed on the ImageNet validation set, making us call for more adequate\nbenchmarking. To avoid the \"benchmark lottery\" on ImageNet and to ensure a fair\nbenchmarking process, we investigate the usage of a unified metric that takes\ninto account the performance of models on other ImageNet variant datasets.",
    "published_date": "2025-01-26T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.15431v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.15430v1",
    "title": "Evaluating Simple Debiasing Techniques in RoBERTa-based Hate Speech Detection Models",
    "authors": [
      "Diana Iftimie",
      "Erik Zinn"
    ],
    "author_ids": [],
    "abstract": "The hate speech detection task is known to suffer from bias against African\nAmerican English (AAE) dialect text, due to the annotation bias present in the\nunderlying hate speech datasets used to train these models. This leads to a\ndisparity where normal AAE text is more likely to be misclassified as\nabusive/hateful compared to non-AAE text. Simple debiasing techniques have been\ndeveloped in the past to counter this sort of disparity, and in this work, we\napply and evaluate these techniques in the scope of RoBERTa-based encoders.\nExperimental results suggest that the success of these techniques depends\nheavily on the methods used for training dataset construction, but with proper\nconsideration of representation bias, they can reduce the disparity seen among\ndialect subgroups on the hate speech detection task.",
    "published_date": "2025-01-26T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.15430v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.15411v1",
    "title": "The Potential of Large Language Models in Supply Chain Management: Advancing Decision-Making, Efficiency, and Innovation",
    "authors": [
      "Raha Aghaei",
      "Ali A. Kiaei",
      "Mahnaz Boush",
      "Javad Vahidi",
      "Zeynab Barzegar",
      "Mahan Rofoosheh"
    ],
    "author_ids": [],
    "abstract": "The integration of large language models (LLMs) into supply chain management\n(SCM) is revolutionizing the industry by improving decision-making, predictive\nanalytics, and operational efficiency. This white paper explores the\ntransformative impact of LLMs on various SCM functions, including demand\nforecasting, inventory management, supplier relationship management, and\nlogistics optimization. By leveraging advanced data analytics and real-time\ninsights, LLMs enable organizations to optimize resources, reduce costs, and\nimprove responsiveness to market changes. Key findings highlight the benefits\nof integrating LLMs with emerging technologies such as IoT, blockchain, and\nrobotics, which together create smarter and more autonomous supply chains.\nEthical considerations, including bias mitigation and data protection, are\ntaken into account to ensure fair and transparent AI practices. In addition,\nthe paper discusses the need to educate the workforce on how to manage new\nAI-driven processes and the long-term strategic benefits of adopting LLMs.\nStrategic recommendations for SCM professionals include investing in\nhigh-quality data management, promoting cross-functional collaboration, and\naligning LLM initiatives with overall business goals. The findings highlight\nthe potential of LLMs to drive innovation, sustainability, and competitive\nadvantage in the ever-changing supply chain management landscape.",
    "published_date": "2025-01-26T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.15411v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.15351v1",
    "title": "Fairness in LLM-Generated Surveys",
    "authors": [
      "Andrés Abeliuk",
      "Vanessa Gaete",
      "Naim Bro"
    ],
    "author_ids": [],
    "abstract": "Large Language Models (LLMs) excel in text generation and understanding,\nespecially in simulating socio-political and economic patterns, serving as an\nalternative to traditional surveys. However, their global applicability remains\nquestionable due to unexplored biases across socio-demographic and geographic\ncontexts. This study examines how LLMs perform across diverse populations by\nanalyzing public surveys from Chile and the United States, focusing on\npredictive accuracy and fairness metrics. The results show performance\ndisparities, with LLM consistently outperforming on U.S. datasets. This bias\noriginates from the U.S.-centric training data, remaining evident after\naccounting for socio-demographic differences. In the U.S., political identity\nand race significantly influence prediction accuracy, while in Chile, gender,\neducation, and religious affiliation play more pronounced roles. Our study\npresents a novel framework for measuring socio-demographic biases in LLMs,\noffering a path toward ensuring fairer and more equitable model performance\nacross diverse socio-cultural contexts.",
    "published_date": "2025-01-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.15351v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.15318v1",
    "title": "A Post-Processing-Based Fair Federated Learning Framework",
    "authors": [
      "Yi Zhou",
      "Naman Goel"
    ],
    "author_ids": [],
    "abstract": "Federated Learning (FL) allows collaborative model training among distributed\nparties without pooling local datasets at a central server. However, the\ndistributed nature of FL poses challenges in training fair federated learning\nmodels. The existing techniques are often limited in offering fairness\nflexibility to clients and performance. We formally define and empirically\nanalyze a simple and intuitive post-processing-based framework to improve group\nfairness in FL systems. This framework can be divided into two stages: a\nstandard FL training stage followed by a completely decentralized local\ndebiasing stage. In the first stage, a global model is trained without fairness\nconstraints using a standard federated learning algorithm (e.g. FedAvg). In the\nsecond stage, each client applies fairness post-processing on the global model\nusing their respective local dataset. This allows for customized fairness\nimprovements based on clients' desired and context-guided fairness\nrequirements. We demonstrate two well-established post-processing techniques in\nthis framework: model output post-processing and final layer fine-tuning. We\nevaluate the framework against three common baselines on four different\ndatasets, including tabular, signal, and image data, each with varying levels\nof data heterogeneity across clients. Our work shows that this framework not\nonly simplifies fairness implementation in FL but also provides significant\nfairness improvements with minimal accuracy loss or even accuracy gain, across\ndata modalities and machine learning methods, being especially effective in\nmore heterogeneous settings.",
    "published_date": "2025-01-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.15318v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.15257v2",
    "title": "Towards Communication-Efficient Adversarial Federated Learning for Robust Edge Intelligence",
    "authors": [
      "Yu Qiao",
      "Apurba Adhikary",
      "Huy Q. Le",
      "Eui-Nam Huh",
      "Zhu Han",
      "Choong Seon Hong"
    ],
    "author_ids": [],
    "abstract": "Federated learning (FL) has gained significant attention for enabling\ndecentralized training on edge networks without exposing raw data. However, FL\nmodels remain susceptible to adversarial attacks and performance degradation in\nnon-IID data settings, thus posing challenges to both robustness and accuracy.\nThis paper aims to achieve communication-efficient adversarial federated\nlearning (AFL) by leveraging a pre-trained model to enhance both robustness and\naccuracy under adversarial attacks and non-IID challenges in AFL. By leveraging\nthe knowledge from a pre-trained model for both clean and adversarial images,\nwe propose a pre-trained model-guided adversarial federated learning (PM-AFL)\nframework. This framework integrates vanilla and adversarial mixture knowledge\ndistillation to effectively balance accuracy and robustness while promoting\nlocal models to learn from diverse data. Specifically, for clean accuracy, we\nadopt a dual distillation strategy where the class probabilities of randomly\npaired images, and their blended versions are aligned between the teacher model\nand the local models. For adversarial robustness, we employ a similar\ndistillation approach but replace clean samples on the local side with\nadversarial examples. Moreover, by considering the bias between local and\nglobal models, we also incorporate a consistency regularization term to ensure\nthat local adversarial predictions stay aligned with their corresponding global\nclean ones. These strategies collectively enable local models to absorb diverse\nknowledge from the teacher model while maintaining close alignment with the\nglobal model, thereby mitigating overfitting to local optima and enhancing the\ngeneralization of the global model. Experiments demonstrate that the\nPM-AFL-based framework not only significantly outperforms other methods but\nalso maintains communication efficiency.",
    "published_date": "2025-01-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.15257v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.15207v1",
    "title": "Hybrid Near/Far-Field Frequency-Dependent Beamforming via Joint Phase-Time Arrays",
    "authors": [
      "Yeyue Cai",
      "Meixia Tao",
      "Jianhua Mo",
      "Shu Sun"
    ],
    "author_ids": [],
    "abstract": "Joint phase-time arrays (JPTA) emerge as a cost-effective and\nenergy-efficient architecture for frequency-dependent beamforming in wideband\ncommunications by utilizing both true-time delay units and phase shifters. This\npaper exploits the potential of JPTA to simultaneously serve multiple users in\nboth near- and far-field regions with a single radio frequency chain. The goal\nis to jointly optimize JPTA-based beamforming and subband allocation to\nmaximize overall system performance. To this end, we formulate a system utility\nmaximization problem, including sum-rate maximization and proportional fairness\nas special cases. We develop a 3-step alternating optimization (AO) algorithm\nand an efficient deep learning (DL) method for this problem. The DL approach\nincludes a 2-layer convolutional neural network, a 3-layer graph attention\nnetwork (GAT), and a normalization module for resource and beamforming\noptimization. The GAT efficiently captures the interactions between resource\nallocation and analog beamformers. Simulation results confirm that JPTA\noutperforms conventional phased arrays (PA) in enhancing user rate and strikes\na good balance between PA and fully-digital approach in energy efficiency.\nEmploying a logarithmic utility function for user rates ensures greater\nfairness than maximizing sum-rates. Furthermore, the DL network achieves\ncomparable performance to the AO approach, while having orders of magnitude\nlower computational complexity.",
    "published_date": "2025-01-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IT",
      "eess.SP",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.15207v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.15129v2",
    "title": "EvoRL: A GPU-accelerated Framework for Evolutionary Reinforcement Learning",
    "authors": [
      "Bowen Zheng",
      "Ran Cheng",
      "Kay Chen Tan"
    ],
    "author_ids": [],
    "abstract": "Evolutionary Reinforcement Learning (EvoRL) has emerged as a promising\napproach to overcoming the limitations of traditional reinforcement learning\n(RL) by integrating the Evolutionary Computation (EC) paradigm with RL.\nHowever, the population-based nature of EC significantly increases\ncomputational costs, thereby restricting the exploration of algorithmic design\nchoices and scalability in large-scale settings. To address this challenge, we\nintroduce $\\texttt{$\\textbf{EvoRL}$}$, the first end-to-end EvoRL framework\noptimized for GPU acceleration. The framework executes the entire training\npipeline on accelerators, including environment simulations and EC processes,\nleveraging hierarchical parallelism through vectorization and compilation\ntechniques to achieve superior speed and scalability. This design enables the\nefficient training of large populations on a single machine. In addition to its\nperformance-oriented design, $\\texttt{$\\textbf{EvoRL}$}$ offers a comprehensive\nplatform for EvoRL research, encompassing implementations of traditional RL\nalgorithms (e.g., A2C, PPO, DDPG, TD3, SAC), Evolutionary Algorithms (e.g.,\nCMA-ES, OpenES, ARS), and hybrid EvoRL paradigms such as Evolutionary-guided RL\n(e.g., ERL, CEM-RL) and Population-Based AutoRL (e.g., PBT). The framework's\nmodular architecture and user-friendly interface allow researchers to\nseamlessly integrate new components, customize algorithms, and conduct fair\nbenchmarking and ablation studies. The project is open-source and available at:\nhttps://github.com/EMI-Group/evorl.",
    "published_date": "2025-01-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.NE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.15129v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.15048v1",
    "title": "YouTube Recommendations Reinforce Negative Emotions: Auditing Algorithmic Bias with Emotionally-Agentic Sock Puppets",
    "authors": [
      "Hussam Habib",
      "Rishab Nithyanand"
    ],
    "author_ids": [],
    "abstract": "Personalized recommendation algorithms, like those on YouTube, significantly\nshape online content consumption. These systems aim to maximize engagement by\nlearning users' preferences and aligning content accordingly but may\nunintentionally reinforce impulsive and emotional biases. Using a sock-puppet\naudit methodology, this study examines YouTube's capacity to recognize and\nreinforce emotional preferences. Simulated user accounts with assigned\nemotional preferences navigate the platform, selecting videos that align with\ntheir assigned preferences and recording subsequent recommendations. Our\nfindings reveal reveal that YouTube amplifies negative emotions, such as anger\nand grievance, by increasing their prevalence and prominence in\nrecommendations. This reinforcement intensifies over time and persists across\ncontexts. Surprisingly, contextual recommendations often exceed personalized\nones in reinforcing emotional alignment. These findings suggest the algorithm\namplifies user biases, contributing to emotional filter bubbles and raising\nconcerns about user well-being and societal impacts. The study emphasizes the\nneed for balancing personalization with content diversity and user agency.",
    "published_date": "2025-01-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.15048v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.15045v2",
    "title": "Towards Robust Unsupervised Attention Prediction in Autonomous Driving",
    "authors": [
      "Mengshi Qi",
      "Xiaoyang Bi",
      "Pengfei Zhu",
      "Huadong Ma"
    ],
    "author_ids": [],
    "abstract": "Robustly predicting attention regions of interest for self-driving systems is\ncrucial for driving safety but presents significant challenges due to the\nlabor-intensive nature of obtaining large-scale attention labels and the domain\ngap between self-driving scenarios and natural scenes. These challenges are\nfurther exacerbated by complex traffic environments, including camera\ncorruption under adverse weather, noise interferences, and central bias from\nlong-tail distributions. To address these issues, we propose a robust\nunsupervised attention prediction method. An Uncertainty Mining Branch refines\npredictions by analyzing commonalities and differences across multiple\npre-trained models on natural scenes, while a Knowledge Embedding Block bridges\nthe domain gap by incorporating driving knowledge to adaptively enhance\npseudo-labels. Additionally, we introduce RoboMixup, a novel data augmentation\nmethod that improves robustness against corruption through soft attention and\ndynamic augmentation, and mitigates central bias by integrating random cropping\ninto Mixup as a regularizer. To systematically evaluate robustness in\nself-driving attention prediction, we introduce the DriverAttention-C\nbenchmark, comprising over 100k frames across three subsets: BDD-A-C,\nDR(eye)VE-C, and DADA-2000-C. Our method achieves performance equivalent to or\nsurpassing fully supervised state-of-the-art approaches on three public\ndatasets and the proposed robustness benchmark, reducing relative corruption\ndegradation by 58.8% and 52.8%, and improving central bias robustness by 12.4%\nand 11.4% in KLD and CC metrics, respectively. Code and data are available at\nhttps://github.com/zaplm/DriverAttention.",
    "published_date": "2025-01-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.15045v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.15028v1",
    "title": "Mining Evidence about Your Symptoms: Mitigating Availability Bias in Online Self-Diagnosis",
    "authors": [
      "Junti Zhang",
      "Zicheng Zhu",
      "Jingshu Li",
      "Yi-Chieh Lee"
    ],
    "author_ids": [],
    "abstract": "People frequently exposed to health information on social media tend to\noverestimate their symptoms during online self-diagnosis due to availability\nbias. This may lead to incorrect self-medication and place additional burdens\non healthcare providers to correct patients' misconceptions. In this work, we\nconducted two mixed-method studies to identify design goals for mitigating\navailability bias in online self-diagnosis. We investigated factors that\ndistort self-assessment of symptoms after exposure to social media. We found\nthat availability bias is pronounced when social media content resonated with\nindividuals, making them disregard their own evidences. To address this, we\ndeveloped and evaluated three chatbot-based symptom checkers designed to foster\nevidence-based self-reflection for bias mitigation given their potential to\nencourage thoughtful responses. Results showed that chatbot-based symptom\ncheckers with cognitive intervention strategies mitigated the impact of\navailability bias in online self-diagnosis.",
    "published_date": "2025-01-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.15028v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.15001v2",
    "title": "What if Eye...? Computationally Recreating Vision Evolution",
    "authors": [
      "Kushagra Tiwary",
      "Aaron Young",
      "Zaid Tasneem",
      "Tzofi Klinghoffer",
      "Akshat Dave",
      "Tomaso Poggio",
      "Dan-Eric Nilsson",
      "Brian Cheung",
      "Ramesh Raskar"
    ],
    "author_ids": [],
    "abstract": "Vision systems in nature show remarkable diversity, from simple\nlight-sensitive patches to complex camera eyes with lenses. While natural\nselection has produced these eyes through countless mutations over millions of\nyears, they represent just one set of realized evolutionary paths. Testing\nhypotheses about how environmental pressures shaped eye evolution remains\nchallenging since we cannot experimentally isolate individual factors.\nComputational evolution offers a way to systematically explore alternative\ntrajectories. Here we show how environmental demands drive three fundamental\naspects of visual evolution through an artificial evolution framework that\nco-evolves both physical eye structure and neural processing in embodied\nagents. First, we demonstrate computational evidence that task specific\nselection drives bifurcation in eye evolution - orientation tasks like\nnavigation in a maze leads to distributed compound-type eyes while an object\ndiscrimination task leads to the emergence of high-acuity camera-type eyes.\nSecond, we reveal how optical innovations like lenses naturally emerge to\nresolve fundamental tradeoffs between light collection and spatial precision.\nThird, we uncover systematic scaling laws between visual acuity and neural\nprocessing, showing how task complexity drives coordinated evolution of sensory\nand computational capabilities. Our work introduces a novel paradigm that\nilluminates evolutionary principles shaping vision by creating targeted\nsingle-player games where embodied agents must simultaneously evolve visual\nsystems and learn complex behaviors. Through our unified genetic encoding\nframework, these embodied agents serve as next-generation hypothesis testing\nmachines while providing a foundation for designing manufacturable bio-inspired\nvision systems. Website: http://eyes.mit.edu/",
    "published_date": "2025-01-25T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.NE",
      "q-bio.NC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.15001v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.14916v1",
    "title": "Allocating Public Goods via Dynamic Max-Min Fairness: Long-Run Behavior and Competitive Equilibria",
    "authors": [
      "Chido Onyeze",
      "Siddhartha Banerjee",
      "Giannis Fikioris",
      "Éva Tardos"
    ],
    "author_ids": [],
    "abstract": "Dynamic max-min fair allocation (DMMF) is a simple and popular mechanism for\nthe repeated allocation of a shared resource among competing agents: in each\nround, each agent can choose to request or not for the resource, which is then\nallocated to the requesting agent with the least number of allocations received\ntill then. Recent work has shown that under DMMF, a simple threshold-based\nrequest policy enjoys surprisingly strong robustness properties, wherein each\nagent can realize a significant fraction of her optimal utility irrespective of\nhow other agents' behave. While this goes some way in mitigating the\npossibility of a 'tragedy of the commons' outcome, the robust policies require\nthat an agent defend against arbitrary (possibly adversarial) behavior by other\nagents. This however may be far from optimal compared to real world settings,\nwhere other agents are selfish optimizers rather than adversaries. Therefore,\nrobust guarantees give no insight on how agents behave in an equilibrium, and\nwhether outcomes are improved under one.\n  Our work aims to bridge this gap by studying the existence and properties of\nequilibria under DMMF. To this end, we first show that despite the strong\nrobustness guarantees of the threshold based strategies, no Nash equilibrium\nexists when agents participate in DMMF, each using some fixed threshold-based\npolicy. On the positive side, however, we show that for the symmetric case, a\nsimple data-driven request policy guarantees that no agent benefits from\ndeviating to a different fixed threshold policy. In our proposed policy agents\naim to match the historical allocation rate with a vanishing drift towards the\nrate optimizing overall welfare for all users. Furthermore, the resulting\nequilibrium outcome can be significantly better compared to what follows from\nthe robustness guarantees.",
    "published_date": "2025-01-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.14916v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.14889v1",
    "title": "Iterative Feature Space Optimization through Incremental Adaptive Evaluation",
    "authors": [
      "Yanping Wu",
      "Yanyong Huang",
      "Zhengzhang Chen",
      "Zijun Yao",
      "Yanjie Fu",
      "Kunpeng Liu",
      "Xiao Luo",
      "Dongjie Wang"
    ],
    "author_ids": [],
    "abstract": "Iterative feature space optimization involves systematically evaluating and\nadjusting the feature space to improve downstream task performance. However,\nexisting works suffer from three key limitations:1) overlooking differences\namong data samples leads to evaluation bias; 2) tailoring feature spaces to\nspecific machine learning models results in overfitting and poor\ngeneralization; 3) requiring the evaluator to be retrained from scratch during\neach optimization iteration significantly reduces the overall efficiency of the\noptimization process. To bridge these gaps, we propose a gEneralized Adaptive\nfeature Space Evaluator (EASE) to efficiently produce optimal and generalized\nfeature spaces. This framework consists of two key components: Feature-Sample\nSubspace Generator and Contextual Attention Evaluator. The first component aims\nto decouple the information distribution within the feature space to mitigate\nevaluation bias. To achieve this, we first identify features most relevant to\nprediction tasks and samples most challenging for evaluation based on feedback\nfrom the subsequent evaluator. This decoupling strategy makes the evaluator\nconsistently target the most challenging aspects of the feature space. The\nsecond component intends to incrementally capture evolving patterns of the\nfeature space for efficient evaluation. We propose a weighted-sharing\nmulti-head attention mechanism to encode key characteristics of the feature\nspace into an embedding vector for evaluation. Moreover, the evaluator is\nupdated incrementally, retaining prior evaluation knowledge while incorporating\nnew insights, as consecutive feature spaces during the optimization process\nshare partial information. Extensive experiments on fourteen real-world\ndatasets demonstrate the effectiveness of the proposed framework. Our code and\ndata are publicly available.",
    "published_date": "2025-01-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.14889v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.14886v1",
    "title": "A Systematic Literature Review on Equity and Technology in HCI and Fairness: Navigating the Complexities and Nuances of Equity Research",
    "authors": [
      "Seyun Kim",
      "Yuanchen Bai",
      "Haiyi Zhu",
      "Motahhare Eslami"
    ],
    "author_ids": [],
    "abstract": "Equity is crucial to the ethical implications in technology development.\nHowever, implementing equity in practice comes with complexities and nuances.\nIn response, the research community, especially the human-computer interaction\n(HCI) and Fairness community, has endeavored to integrate equity into\ntechnology design, addressing issues of societal inequities. With such\nincreasing efforts, it is yet unclear why and how researchers discuss equity\nand its integration into technology, what research has been conducted, and what\ngaps need to be addressed. We conducted a systematic literature review on\nequity and technology, collecting and analyzing 202 papers published in HCI and\nFairness-focused venues. Amidst the substantial growth of relevant publications\nwithin the past four years, we deliver three main contributions: (1) we\nelaborate a comprehensive understanding researchers' motivations for studying\nequity and technology, (2) we illustrate the different equity definitions and\nframeworks utilized to discuss equity, (3) we characterize the key themes\naddressing interventions as well as tensions and trade-offs when advancing and\nintegrating equity to technology. Based on our findings, we elaborate an equity\nframework for researchers who seek to address existing gaps and advance equity\nin technology.",
    "published_date": "2025-01-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.14886v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.14710v1",
    "title": "Overcoming Fairness Trade-offs via Pre-processing: A Causal Perspective",
    "authors": [
      "Charlotte Leininger",
      "Simon Rittel",
      "Ludwig Bothmann"
    ],
    "author_ids": [],
    "abstract": "Training machine learning models for fair decisions faces two key challenges:\nThe \\emph{fairness-accuracy trade-off} results from enforcing fairness which\nweakens its predictive performance in contrast to an unconstrained model. The\nincompatibility of different fairness metrics poses another trade-off -- also\nknown as the \\emph{impossibility theorem}. Recent work identifies the bias\nwithin the observed data as a possible root cause and shows that fairness and\npredictive performance are in fact in accord when predictive performance is\nmeasured on unbiased data. We offer a causal explanation for these findings\nusing the framework of the FiND (fictitious and normatively desired) world, a\n\"fair\" world, where protected attributes have no causal effects on the target\nvariable. We show theoretically that (i) classical fairness metrics deemed to\nbe incompatible are naturally satisfied in the FiND world, while (ii) fairness\naligns with high predictive performance. We extend our analysis by suggesting\nhow one can benefit from these theoretical insights in practice, using causal\npre-processing methods that approximate the FiND world. Additionally, we\npropose a method for evaluating the approximation of the FiND world via\npre-processing in practical use cases where we do not have access to the FiND\nworld. In simulations and empirical studies, we demonstrate that these\npre-processing methods are successful in approximating the FiND world and\nresolve both trade-offs. Our results provide actionable solutions for\npractitioners to achieve fairness and high predictive performance\nsimultaneously.",
    "published_date": "2025-01-24T00:00:00",
    "year": 2025,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.14710v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.14615v2",
    "title": "Single-neuron deep generative model uncovers underlying physics of neuronal activity in Ca imaging data",
    "authors": [
      "Jordi Abante",
      "Angelo Piga",
      "Berta Ros",
      "Clara F López-León",
      "Josep M Canals",
      "Jordi Soriano"
    ],
    "author_ids": [],
    "abstract": "Calcium imaging has become a powerful alternative to electrophysiology for\nstudying neuronal activity, offering spatial resolution and the ability to\nmeasure large populations of neurons in a minimally invasive manner. This\ntechnique has broad applications in neuroscience, neuroengineering, and\nmedicine, enabling researchers to explore the relationship between neuron\nlocation and activity. Recent advancements in deep generative models (DGMs)\nhave facilitated the modeling of neuronal population dynamics, uncovering\nlatent representations that provide insights into behavior prediction and\nneuronal variance. However, these models often rely on spike inference\nalgorithms and primarily focus on population-level dynamics, limiting their\napplicability for single-neuron analyses. To address this gap, we propose a\nnovel framework for single-neuron representation learning using autoregressive\nvariational autoencoders (AVAEs). Our approach embeds individual neurons'\nspatiotemporal signals into a reduced-dimensional space without the need for\nspike inference algorithms. The AVAE excels over traditional linear methods by\ngenerating more informative and discriminative latent representations,\nimproving tasks such as visualization, clustering, and the understanding of\nneuronal activity. Additionally, the reconstruction performance of the AVAE\noutperforms the state of the art, demonstrating its ability to accurately\nrecover the original fluorescence signal from the learned representation. Using\nrealistic simulations, we show that our model captures underlying physical\nproperties and connectivity patterns, enabling it to distinguish between\ndifferent firing and connectivity types. These findings position the AVAE as a\nversatile and powerful tool for advancing single-neuron analysis and lays the\ngroundwork for future integration of multimodal single-cell datasets in\nneuroscience.",
    "published_date": "2025-01-24T00:00:00",
    "year": 2025,
    "categories": [
      "q-bio.NC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.14615v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.14609v3",
    "title": "Fair Division Beyond Monotone Valuations",
    "authors": [
      "Siddharth Barman",
      "Paritosh Verma"
    ],
    "author_ids": [],
    "abstract": "This paper studies fair division of divisible and indivisible items among\nagents whose cardinal preferences are not necessarily monotone. We establish\nthe existence of fair divisions and develop approximation algorithms to compute\nthem.\n  We address two complementary valuation classes, subadditive and nonnegative,\nwhich go beyond monotone functions. Considering both the division of cake\n(divisible resources) and allocation of indivisible items, we obtain fairness\nguarantees in terms of (approximate) envy-freeness (EF) and equability (EQ)\n  In the context of envy-freeness, we prove that an EF division of a cake\nalways exists under cake valuations that are subadditive and globally\nnonnegative. This result complements the nonexistence of EF allocations for\nburnt cakes known for more general valuations. In the indivisible-items\nsetting, we establish the existence of EF3 allocations for subadditive and\nglobally nonnegative valuations. In addition, we obtain universal existence of\nEF3 allocations under nonnegative valuations.\n  We study equitability under nonnegative valuations. Here, we prove that EQ3\nallocations always exist when the agents' valuations are nonnegative. Also, in\nthe indivisible-items setting, we develop an approximation algorithm that, for\ngiven nonnegative valuations, finds allocations that are equitable within\nadditive margins.\n  Our results have combinatorial implications. For instance, the developed\nresults imply the universal existence of proximately dense subgraphs: Given any\ngraph $G=(V, E)$ and integer $k$ (at most $|V|$), there always exists a\npartition $V_1, V_2, \\ldots, V_k$ of the vertex set such that the edge\ndensities within the parts, $V_i$, are additively within four of each other.\nFurther, such a partition can be computed efficiently.",
    "published_date": "2025-01-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.14609v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2502.14868v1",
    "title": "Unlocking the Black Box: Analysing the EU Artificial Intelligence Act's Framework for Explainability in AI",
    "authors": [
      "Georgios Pavlidis"
    ],
    "author_ids": [],
    "abstract": "The lack of explainability of Artificial Intelligence (AI) is one of the\nfirst obstacles that the industry and regulators must overcome to mitigate the\nrisks associated with the technology. The need for eXplainable AI (XAI) is\nevident in fields where accountability, ethics and fairness are critical, such\nas healthcare, credit scoring, policing and the criminal justice system. At the\nEU level, the notion of explainability is one of the fundamental principles\nthat underpin the AI Act, though the exact XAI techniques and requirements are\nstill to be determined and tested in practice. This paper explores various\napproaches and techniques that promise to advance XAI, as well as the\nchallenges of implementing the principle of explainability in AI governance and\npolicies. Finally, the paper examines the integration of XAI into EU law,\nemphasising the issues of standard setting, oversight, and enforcement.",
    "published_date": "2025-01-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.14868v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.14850v1",
    "title": "On the locality bias and results in the Long Range Arena",
    "authors": [
      "Pablo Miralles-González",
      "Javier Huertas-Tato",
      "Alejandro Martín",
      "David Camacho"
    ],
    "author_ids": [],
    "abstract": "The Long Range Arena (LRA) benchmark was designed to evaluate the performance\nof Transformer improvements and alternatives in long-range dependency modeling\ntasks. The Transformer and its main variants performed poorly on this\nbenchmark, and a new series of architectures such as State Space Models (SSMs)\ngained some traction, greatly outperforming Transformers in the LRA. Recent\nwork has shown that with a denoising pre-training phase, Transformers can\nachieve competitive results in the LRA with these new architectures. In this\nwork, we discuss and explain the superiority of architectures such as MEGA and\nSSMs in the Long Range Arena, as well as the recent improvement in the results\nof Transformers, pointing to the positional and local nature of the tasks. We\nshow that while the LRA is a benchmark for long-range dependency modeling, in\nreality most of the performance comes from short-range dependencies. Using\ntraining techniques to mitigate data inefficiency, Transformers are able to\nreach state-of-the-art performance with proper positional encoding. In\naddition, with the same techniques, we were able to remove all restrictions\nfrom SSM convolutional kernels and learn fully parameterized convolutions\nwithout decreasing performance, suggesting that the design choices behind SSMs\nsimply added inductive biases and learning efficiency for these particular\ntasks. Our insights indicate that LRA results should be interpreted with\ncaution and call for a redesign of the benchmark.",
    "published_date": "2025-01-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.14850v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.14551v1",
    "title": "Fairness of Deep Ensembles: On the interplay between per-group task difficulty and under-representation",
    "authors": [
      "Estanislao Claucich",
      "Sara Hooker",
      "Diego H. Milone",
      "Enzo Ferrante",
      "Rodrigo Echeveste"
    ],
    "author_ids": [],
    "abstract": "Ensembling is commonly regarded as an effective way to improve the general\nperformance of models in machine learning, while also increasing the robustness\nof predictions. When it comes to algorithmic fairness, heterogeneous ensembles,\ncomposed of multiple model types, have been employed to mitigate biases in\nterms of demographic attributes such as sex, age or ethnicity. Moreover, recent\nwork has shown how in multi-class problems even simple homogeneous ensembles\nmay favor performance of the worst-performing target classes. While homogeneous\nensembles are simpler to implement in practice, it is not yet clear whether\ntheir benefits translate to groups defined not in terms of their target class,\nbut in terms of demographic or protected attributes, hence improving fairness.\nIn this work we show how this simple and straightforward method is indeed able\nto mitigate disparities, particularly benefiting under-performing subgroups.\nInterestingly, this can be achieved without sacrificing overall performance,\nwhich is a common trade-off observed in bias mitigation strategies. Moreover,\nwe analyzed the interplay between two factors which may result in biases:\nsub-group under-representation and the inherent difficulty of the task for each\ngroup. These results revealed that, contrary to popular assumptions, having\nbalanced datasets may be suboptimal if the task difficulty varies between\nsubgroups. Indeed, we found that a perfectly balanced dataset may hurt both the\noverall performance and the gap between groups. This highlights the importance\nof considering the interaction between multiple forces at play in fairness.",
    "published_date": "2025-01-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.14551v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.14457v1",
    "title": "Understanding and Mitigating Gender Bias in LLMs via Interpretable Neuron Editing",
    "authors": [
      "Zeping Yu",
      "Sophia Ananiadou"
    ],
    "author_ids": [],
    "abstract": "Large language models (LLMs) often exhibit gender bias, posing challenges for\ntheir safe deployment. Existing methods to mitigate bias lack a comprehensive\nunderstanding of its mechanisms or compromise the model's core capabilities. To\naddress these issues, we propose the CommonWords dataset, to systematically\nevaluate gender bias in LLMs. Our analysis reveals pervasive bias across models\nand identifies specific neuron circuits, including gender neurons and general\nneurons, responsible for this behavior. Notably, editing even a small number of\ngeneral neurons can disrupt the model's overall capabilities due to\nhierarchical neuron interactions. Based on these insights, we propose an\ninterpretable neuron editing method that combines logit-based and causal-based\nstrategies to selectively target biased neurons. Experiments on five LLMs\ndemonstrate that our method effectively reduces gender bias while preserving\nthe model's original capabilities, outperforming existing fine-tuning and\nediting approaches. Our findings contribute a novel dataset, a detailed\nanalysis of bias mechanisms, and a practical solution for mitigating gender\nbias in LLMs.",
    "published_date": "2025-01-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.14457v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.14414v1",
    "title": "SoK: What Makes Private Learning Unfair?",
    "authors": [
      "Kai Yao",
      "Marc Juarez"
    ],
    "author_ids": [],
    "abstract": "Differential privacy has emerged as the most studied framework for\nprivacy-preserving machine learning. However, recent studies show that\nenforcing differential privacy guarantees can not only significantly degrade\nthe utility of the model, but also amplify existing disparities in its\npredictive performance across demographic groups. Although there is extensive\nresearch on the identification of factors that contribute to this phenomenon,\nwe still lack a complete understanding of the mechanisms through which\ndifferential privacy exacerbates disparities. The literature on this problem is\nmuddled by varying definitions of fairness, differential privacy mechanisms,\nand inconsistent experimental settings, often leading to seemingly\ncontradictory results.\n  This survey provides the first comprehensive overview of the factors that\ncontribute to the disparate effect of training models with differential privacy\nguarantees. We discuss their impact and analyze their causal role in such a\ndisparate effect. Our analysis is guided by a taxonomy that categorizes these\nfactors by their position within the machine learning pipeline, allowing us to\ndraw conclusions about their interaction and the feasibility of potential\nmitigation strategies. We find that factors related to the training dataset and\nthe underlying distribution play a decisive role in the occurrence of disparate\nimpact, highlighting the need for research on these factors to address the\nissue.",
    "published_date": "2025-01-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.14414v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.14844v2",
    "title": "Unmasking Conversational Bias in AI Multiagent Systems",
    "authors": [
      "Erica Coppolillo",
      "Giuseppe Manco",
      "Luca Maria Aiello"
    ],
    "author_ids": [],
    "abstract": "Detecting biases in the outputs produced by generative models is essential to\nreduce the potential risks associated with their application in critical\nsettings. However, the majority of existing methodologies for identifying\nbiases in generated text consider the models in isolation and neglect their\ncontextual applications. Specifically, the biases that may arise in multi-agent\nsystems involving generative models remain under-researched. To address this\ngap, we present a framework designed to quantify biases within multi-agent\nsystems of conversational Large Language Models (LLMs). Our approach involves\nsimulating small echo chambers, where pairs of LLMs, initialized with aligned\nperspectives on a polarizing topic, engage in discussions. Contrary to\nexpectations, we observe significant shifts in the stance expressed in the\ngenerated messages, particularly within echo chambers where all agents\ninitially express conservative viewpoints, in line with the well-documented\npolitical bias of many LLMs toward liberal positions. Crucially, the bias\nobserved in the echo-chamber experiment remains undetected by current\nstate-of-the-art bias detection methods that rely on questionnaires. This\nhighlights a critical need for the development of a more sophisticated toolkit\nfor bias detection and mitigation for AI multi-agent systems. The code to\nperform the experiments is publicly available at\nhttps://anonymous.4open.science/r/LLMsConversationalBias-7725.",
    "published_date": "2025-01-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.14844v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.14270v1",
    "title": "Max-Min Fairness for IRS-Assisted Secure Two-Way Communications",
    "authors": [
      "Harindu Jayarathne",
      "Tharindu Wickremasinghe",
      "Kasun T. Hemachandra",
      "Tharaka Samarasinghe",
      "Saman Atapattu"
    ],
    "author_ids": [],
    "abstract": "This paper investigates an intelligent reflective surface (IRS) assisted\nsecure multi-user two-way communication system. The aim of this paper is to\nenhance the physical layer security by optimizing the minimum secrecy-rate\namong all user-pairs in the presence of a malicious user. The optimization\nproblem is converted into an alternating optimization problem consisting of two\nsub-problems. Transmit power optimization is handled using a fractional\nprogramming method, whereas IRS phase shift optimization is handled with\nsemi-definite programming. The convergence of the proposed algorithm is\ninvestigated numerically. The performance gain in minimum secrecy-rate is\nquantified for four different user configurations in comparison to the baseline\nscheme. Results indicate a 3.6-fold gain in minimum secrecy rate over the\nbaseline scheme when the IRS is positioned near a legitimate user, even when\nthe malicious user is located close to the same legitimate user.",
    "published_date": "2025-01-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.14270v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.14179v1",
    "title": "AI Chatbots as Professional Service Agents: Developing a Professional Identity",
    "authors": [
      "Wenwen Li",
      "Kangwei Shi",
      "Yidong Chai"
    ],
    "author_ids": [],
    "abstract": "With the rapid expansion of large language model (LLM) applications, there is\nan emerging shift in the role of LLM-based AI chatbots from serving merely as\ngeneral inquiry tools to acting as professional service agents. However,\ncurrent studies often overlook a critical aspect of professional service\nagents: the act of communicating in a manner consistent with their professional\nidentities. This is of particular importance in the healthcare sector, where\neffective communication with patients is essential for achieving professional\ngoals, such as promoting patient well-being by encouraging healthy behaviors.\nTo bridge this gap, we propose LAPI (LLM-based Agent with a Professional\nIdentity), a novel framework for designing professional service agent tailored\nfor medical question-and-answer (Q\\&A) services, ensuring alignment with a\nspecific professional identity. Our method includes a theory-guided task\nplanning process that decomposes complex professional tasks into manageable\nsubtasks aligned with professional objectives and a pragmatic entropy method\ndesigned to generate professional and ethical responses with low uncertainty.\nExperiments on various LLMs show that the proposed approach outperforms\nbaseline methods, including few-shot prompting, chain-of-thought prompting,\nacross key metrics such as fluency, naturalness, empathy, patient-centricity,\nand ROUGE-L scores. Additionally, the ablation study underscores the\ncontribution of each component to the overall effectiveness of the approach.",
    "published_date": "2025-01-24T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.14179v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.14110v2",
    "title": "Developing a Fair Online Recruitment Framework Based on Job-seekers' Fairness Concerns",
    "authors": [
      "Changyang He",
      "Yue Deng",
      "Alessandro Fabris",
      "Bo Li",
      "Asia Biega"
    ],
    "author_ids": [],
    "abstract": "The susceptibility to biases and discrimination is a pressing issue in\ntoday's labor markets. Though digital recruitment systems play an increasingly\nsignificant role in human resources management, thus far we lack a systematic\nunderstanding of human-centered design principles for fair online hiring. This\nwork proposes a fair recruitment framework based on job-seekers' fairness\nconcerns shared in an online forum. Through qualitative analysis, we uncover\nfour overarching themes of job-seekers' fairness concerns, including\ndiscrimination against sensitive attributes, interaction biases, improper\ninterpretations of qualifications, and power imbalance. Based on these\nfindings, we derive design implications for algorithms and interfaces in\nrecruitment systems, integrating them into a fair recruitment framework\nspanning different hiring stages and fairness considerations.",
    "published_date": "2025-01-23T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.14110v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.14108v1",
    "title": "Well-Posedness of the R13 Equations Using Tensor-Valued Korn Inequalities",
    "authors": [
      "Peter Lewintan",
      "Lambert Theisen",
      "Manuel Torrilhon"
    ],
    "author_ids": [],
    "abstract": "In this paper, we finally catch up with proving the well-posedness of the\nlinearized R13 moment model, which describes, e.g., rarefied gas flows. As an\nextension of the classical fluid equations, moment models are robust and have\nbeen frequently used, yet they are challenging to analyze due to their\nadditional equations. By effectively grouping variables, we identify a 2-by-2\nblock structure, allowing the analysis of the well-posedness within the\nabstract LBB framework of saddle point problems. Due to the unique tensorial\nstructure of the equations, in addition to an interesting combination of tools\nfrom Stokes' and linear elasticity theory, we also need new coercivity\nestimates for tensor fields. These Korn-type inequalities are established by\nanalyzing the symbol map of the symmetric and trace-free part of tensor\nderivative fields. Together with the corresponding right inverse of the\ntensorial divergence, we obtain the existence and uniqueness of weak solutions.",
    "published_date": "2025-01-23T00:00:00",
    "year": 2025,
    "categories": [
      "math.AP",
      "cs.NA",
      "math.FA",
      "math.NA",
      "76P05, 65N30, 26D10, 35Q35, 35A23, 65K10, 35A01"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.14108v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.14095v1",
    "title": "Improved subsample-and-aggregate via the private modified winsorized mean",
    "authors": [
      "Kelly Ramsay",
      "Dylan Spicker"
    ],
    "author_ids": [],
    "abstract": "We develop a univariate, differentially private mean estimator, called the\nprivate modified winsorized mean designed to be used as the aggregator in\nsubsample-and-aggregate. We demonstrate, via real data analysis, that common\ndifferentially private multivariate mean estimators may not perform well as the\naggregator, even with a dataset with 8000 observations, motivating our\ndevelopments. We show that the modified winsorized mean is minimax optimal for\nseveral, large classes of distributions, even under adversarial contamination.\nWe also demonstrate that, empirically, the modified winsorized mean performs\nwell compared to other private mean estimates. We consider the modified\nwinsorized mean as the aggregator in subsample-and-aggregate, deriving a finite\nsample deviations bound for a subsample-and-aggregate estimate generated with\nthe new aggregator. This result yields two important insights: (i) the optimal\nchoice of subsamples depends on the bias of the estimator computed on the\nsubsamples, and (ii) the rate of convergence of the subsample-and-aggregate\nestimator depends on the robustness of the estimator computed on the\nsubsamples.",
    "published_date": "2025-01-23T00:00:00",
    "year": 2025,
    "categories": [
      "stat.ME",
      "cs.LG",
      "62G35, 68P27",
      "G.3.7; C.2.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.14095v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.14073v2",
    "title": "LLMs are Vulnerable to Malicious Prompts Disguised as Scientific Language",
    "authors": [
      "Yubin Ge",
      "Neeraja Kirtane",
      "Hao Peng",
      "Dilek Hakkani-Tür"
    ],
    "author_ids": [],
    "abstract": "As large language models (LLMs) have been deployed in various real-world\nsettings, concerns about the harm they may propagate have grown. Various\njailbreaking techniques have been developed to expose the vulnerabilities of\nthese models and improve their safety. This work reveals that many\nstate-of-the-art LLMs are vulnerable to malicious requests hidden behind\nscientific language. Specifically, our experiments with GPT4o, GPT4o-mini,\nGPT-4, LLama3-405B-Instruct, Llama3-70B-Instruct, Cohere, Gemini models\ndemonstrate that, the models' biases and toxicity substantially increase when\nprompted with requests that deliberately misinterpret social science and\npsychological studies as evidence supporting the benefits of stereotypical\nbiases. Alarmingly, these models can also be manipulated to generate fabricated\nscientific arguments claiming that biases are beneficial, which can be used by\nill-intended actors to systematically jailbreak these strong LLMs. Our analysis\nstudies various factors that contribute to the models' vulnerabilities to\nmalicious requests in academic language. Mentioning author names and venues\nenhances the persuasiveness of models, and the bias scores increase as\ndialogues progress. Our findings call for a more careful investigation on the\nuse of scientific data for training LLMs.",
    "published_date": "2025-01-23T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.14073v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.07792v1",
    "title": "Centralization vs Decentralization in Hiring and Admissions",
    "authors": [
      "Benjamin Fish",
      "Diptangshu Sen",
      "Juba Ziani"
    ],
    "author_ids": [],
    "abstract": "There is a range of ways to organize hiring and admissions in higher\neducation, as in many domains, ranging from very centralized processes where a\nsingle person makes final decisions to very decentralized processes where many\npeople make decisions about who to admit or hire. Decentralized processes can\nenable individual and collective empowerment, but this may come at the cost of\nefficiency. With the advent of automated decision making, this question of\ncentralization has a big impact on hiring and admissions, given that automated\nsystems often are easier to implement, or even require, more centralized\ndecision making.\n  In this paper, we develop a strategic model to explore the impact of the\ndegree of centralization on both the candidates and the hirers, with a focus on\nuniversity admissions. The model reflects a trade-off between a centralized\ncommittee where preferences may not capture individual hirers' preferences, and\na decentralized process where individual hirers face extra costs to interview\ncandidates themselves. We characterize when individual hirers prefer the\ndecentralized process over the centralized process as a function of the degree\nto which the centralized process and hirers' preferences are aligned. We also\nshow that decentralization can have devastating consequences for fairness,\nleading to major disparities in the likelihood of getting hired across\ncandidates. Our results demonstrate the trade-offs that occur under the\nquestion of centralization vs decentralization, and point to how an answer to\nthis question can impose significant harm to people in these systems.",
    "published_date": "2025-01-23T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.07792v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.14040v2",
    "title": "Global Perspectives of AI Risks and Harms: Analyzing the Negative Impacts of AI Technologies as Prioritized by News Media",
    "authors": [
      "Mowafak Allaham",
      "Kimon Kieslich",
      "Nicholas Diakopoulos"
    ],
    "author_ids": [],
    "abstract": "Emerging AI technologies have the potential to drive economic growth and\ninnovation but can also pose significant risks to society. To mitigate these\nrisks, governments, companies, and researchers have contributed regulatory\nframeworks, risk assessment approaches, and safety benchmarks, but these can\nlack nuance when considered in global deployment contexts. One way to\nunderstand these nuances is by looking at how the media reports on AI, as news\nmedia has a substantial influence on what negative impacts of AI are discussed\nin the public sphere and which impacts are deemed important. In this work, we\nanalyze a broad and diverse sample of global news media spanning 27 countries\nacross Asia, Africa, Europe, Middle East, North America, and Oceania to gain\nvaluable insights into the risks and harms of AI technologies as reported and\nprioritized across media outlets in different countries. This approach reveals\na skewed prioritization of Societal Risks followed by Legal & Rights-related\nRisks, Content Safety Risks, Cognitive Risks, Existential Risks, and\nEnvironmental Risks, as reflected in the prevalence of these risk categories in\nthe news coverage of different nations. Furthermore, it highlights how the\ndistribution of such concerns varies based on the political bias of news\noutlets, underscoring the political nature of AI risk assessment processes and\npublic opinion. By incorporating views from various regions and political\norientations for assessing the risks and harms of AI, this work presents\nstakeholders, such as AI developers and policy makers, with insights into the\nAI risks categories prioritized in the public sphere. These insights may guide\nthe development of more inclusive, safe, and responsible AI technologies that\naddress the diverse concerns and needs across the world.",
    "published_date": "2025-01-23T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.14040v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.13868v1",
    "title": "Lost in Siting: The Hidden Carbon Cost of Inequitable Residential Solar Installations",
    "authors": [
      "Cooper Sigrist",
      "Adam Lechowicz",
      "Jovan Champ",
      "Noman Bashir",
      "Mohammad Hajiesmaili"
    ],
    "author_ids": [],
    "abstract": "The declining cost of solar photovoltaics (PV) combined with strong federal\nand state-level incentives have resulted in a high number of residential solar\nPV installations in the US. However, these installations are concentrated in\nparticular regions, such as California, and demographics, such as high-income\nAsian neighborhoods. This inequitable distribution creates an illusion that\nfurther increasing residential solar installations will become increasingly\nchallenging. Furthermore, while the inequity in solar installations has\nreceived attention, no prior comprehensive work has been done on understanding\nwhether our current trajectory of residential solar adoption is energy- and\ncarbon-efficient. In this paper, we reveal the hidden energy and carbon cost of\nthe inequitable distribution of existing installations. Using US-based data on\ncarbon offset potential, the amount of avoided carbon emissions from using\nrooftop PV instead of electric grid energy, and the number of existing solar\ninstallations, we surprisingly observe that locations and demographics with a\nhigher carbon offset potential have fewer existing installations. For instance,\nneighborhoods with relatively higher black population have 7.4% higher carbon\noffset potential than average but 36.7% fewer installations; lower-income\nneighborhoods have 14.7% higher potential and 47% fewer installations. We\npropose several equity- and carbon-aware solar siting strategies. In evaluating\nthese strategies, we develop Sunsight, a toolkit that combines\nsimulation/visualization tools and our relevant datasets, which we are\nreleasing publicly. Our projections show that a multi-objective siting strategy\ncan address two problems at once; namely, it can improve societal outcomes in\nterms of distributional equity and simultaneously improve the carbon-efficiency\n(i.e., climate impact) of current installation trends by up to 39.8%.",
    "published_date": "2025-01-23T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.13868v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.13859v3",
    "title": "Learning Visual Proxy for Compositional Zero-Shot Learning",
    "authors": [
      "Shiyu Zhang",
      "Cheng Yan",
      "Yang Liu",
      "Chenchen Jing",
      "Lei Zhou",
      "Wenjun Wang"
    ],
    "author_ids": [],
    "abstract": "Compositional Zero-Shot Learning (CZSL) aims to recognize novel\nattribute-object compositions by leveraging knowledge from seen compositions.\nExisting methods align textual prototypes with visual features through\nVision-Language Models (VLMs), but they face two key limitations: (1) modality\ngaps hinder the discrimination of semantically similar composition pairs, and\n(2) single-modal textual prototypes lack fine-grained visual cues, creating\nbottlenecks in VLM-based CZSL. In this paper, we introduce Visual Proxy\nLearning, a novel approach that facilitates the learning of distinct visual\ndistributions, effectively reducing the modality gap and improving\ncompositional generalization performance. Specifically, we initialize visual\nproxies for various attributes, objects, and their compositions using text\nrepresentations. By optimizing the visual space, we capture fine-grained visual\ncues and guide the learning of more discriminative visual representations for\nattributes, objects and compositions. Furthermore, we propose an effective\nCross-Modal Joint Learning (CMJL) strategy that imposes cross-modal constraints\nbetween the original text-image space and the fine-grained visual space. This\napproach not only boosts generalization for previously unseen composition pairs\nbut also sharpens the discrimination of similar pairs, fostering more robust\nand precise learning. Extensive experiments demonstrate state-of-the-art\nperformance in closed-world scenarios and competitive open-world results across\nfour established CZSL benchmarks, validating the effectiveness of our approach\nin advancing compositional generalization.",
    "published_date": "2025-01-23T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.13859v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.13836v1",
    "title": "Think Outside the Data: Colonial Biases and Systemic Issues in Automated Moderation Pipelines for Low-Resource Languages",
    "authors": [
      "Farhana Shahid",
      "Mona Elswah",
      "Aditya Vashistha"
    ],
    "author_ids": [],
    "abstract": "Most social media users come from non-English speaking countries in the\nGlobal South. Despite the widespread prevalence of harmful content in these\nregions, current moderation systems repeatedly struggle in low-resource\nlanguages spoken there. In this work, we examine the challenges AI researchers\nand practitioners face when building moderation tools for low-resource\nlanguages. We conducted semi-structured interviews with 22 AI researchers and\npractitioners specializing in automatic detection of harmful content in four\ndiverse low-resource languages from the Global South. These are: Tamil from\nSouth Asia, Swahili from East Africa, Maghrebi Arabic from North Africa, and\nQuechua from South America. Our findings reveal that social media companies'\nrestrictions on researchers' access to data exacerbate the historical\nmarginalization of these languages, which have long lacked datasets for\nstudying online harms. Moreover, common preprocessing techniques and language\nmodels, predominantly designed for data-rich English, fail to account for the\nlinguistic complexity of low-resource languages. This leads to critical errors\nwhen moderating content in Tamil, Swahili, Arabic, and Quechua, which are\nmorphologically richer than English. Based on our findings, we establish that\nthe precarities in current moderation pipelines are rooted in deep systemic\ninequities and continue to reinforce historical power imbalances. We conclude\nby discussing multi-stakeholder approaches to improve moderation for\nlow-resource languages.",
    "published_date": "2025-01-23T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.13836v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.13720v2",
    "title": "Musical ethnocentrism in Large Language Models",
    "authors": [
      "Anna Kruspe"
    ],
    "author_ids": [],
    "abstract": "Large Language Models (LLMs) reflect the biases in their training data and,\nby extension, those of the people who created this training data. Detecting,\nanalyzing, and mitigating such biases is becoming a focus of research. One type\nof bias that has been understudied so far are geocultural biases. Those can be\ncaused by an imbalance in the representation of different geographic regions\nand cultures in the training data, but also by value judgments contained\ntherein. In this paper, we make a first step towards analyzing musical biases\nin LLMs, particularly ChatGPT and Mixtral. We conduct two experiments. In the\nfirst, we prompt LLMs to provide lists of the \"Top 100\" musical contributors of\nvarious categories and analyze their countries of origin. In the second\nexperiment, we ask the LLMs to numerically rate various aspects of the musical\ncultures of different countries. Our results indicate a strong preference of\nthe LLMs for Western music cultures in both experiments.",
    "published_date": "2025-01-23T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.13720v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.13641v1",
    "title": "The Road to Learning Explainable Inverse Kinematic Models: Graph Neural Networks as Inductive Bias for Symbolic Regression",
    "authors": [
      "Pravin Pandey",
      "Julia Reuter",
      "Christoph Steup",
      "Sanaz Mostaghim"
    ],
    "author_ids": [],
    "abstract": "This paper shows how a Graph Neural Network (GNN) can be used to learn an\nInverse Kinematics (IK) based on an automatically generated dataset. The\ngenerated Inverse Kinematics is generalized to a family of manipulators with\nthe same Degree of Freedom (DOF), but varying link length configurations. The\nresults indicate a position error of less than 1.0 cm for 3 DOF and 4.5 cm for\n5 DOF, and orientation error of 2$^\\circ$ for 3 DOF and 8.2$^\\circ$ for 6 DOF,\nwhich allows the deployment to certain real world-problems. However,\nout-of-domain errors and lack of extrapolation can be observed in the resulting\nGNN. An extensive analysis of these errors indicates potential for enhancement\nin the future. Consequently, the generated GNNs are tailored to be used in\nfuture work as an inductive bias to generate analytical equations through\nsymbolic regression.",
    "published_date": "2025-01-23T00:00:00",
    "year": 2025,
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.13641v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.19407v2",
    "title": "Algorithmic Inheritance: Surname Bias in AI Decisions Reinforces Intergenerational Inequality",
    "authors": [
      "Pat Pataranutaporn",
      "Nattavudh Powdthavee",
      "Pattie Maes"
    ],
    "author_ids": [],
    "abstract": "Surnames often convey implicit markers of social status, wealth, and lineage,\nshaping perceptions in ways that can perpetuate systemic biases and\nintergenerational inequality. This study is the first of its kind to\ninvestigate whether and how surnames influence AI-driven decision-making,\nfocusing on their effects across key areas such as hiring recommendations,\nleadership appointments, and loan approvals. Using 72,000 evaluations of 600\nsurnames from the United States and Thailand, two countries with distinct\nsociohistorical contexts and surname conventions, we classify names into four\ncategories: Rich, Legacy, Normal, and phonetically similar Variant groups. Our\nfindings show that elite surnames consistently increase AI-generated\nperceptions of power, intelligence, and wealth, which in turn influence\nAI-driven decisions in high-stakes contexts. Mediation analysis reveals\nperceived intelligence as a key mechanism through which surname biases\ninfluence AI decision-making process. While providing objective qualifications\nalongside surnames mitigates most of these biases, it does not eliminate them\nentirely, especially in contexts where candidate credentials are low. These\nfindings highlight the need for fairness-aware algorithms and robust policy\nmeasures to prevent AI systems from reinforcing systemic inequalities tied to\nsurnames, an often-overlooked bias compared to more salient characteristics\nsuch as race and gender. Our work calls for a critical reassessment of\nalgorithmic accountability and its broader societal impact, particularly in\nsystems designed to uphold meritocratic principles while counteracting the\nperpetuation of intergenerational privilege.",
    "published_date": "2025-01-23T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "econ.GN",
      "q-fin.EC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.19407v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.13448v1",
    "title": "BMG-Q: Localized Bipartite Match Graph Attention Q-Learning for Ride-Pooling Order Dispatch",
    "authors": [
      "Yulong Hu",
      "Siyuan Feng",
      "Sen Li"
    ],
    "author_ids": [],
    "abstract": "This paper introduces Localized Bipartite Match Graph Attention Q-Learning\n(BMG-Q), a novel Multi-Agent Reinforcement Learning (MARL) algorithm framework\ntailored for ride-pooling order dispatch. BMG-Q advances ride-pooling\ndecision-making process with the localized bipartite match graph underlying the\nMarkov Decision Process, enabling the development of novel Graph Attention\nDouble Deep Q Network (GATDDQN) as the MARL backbone to capture the dynamic\ninteractions among ride-pooling vehicles in fleet. Our approach enriches the\nstate information for each agent with GATDDQN by leveraging a localized\nbipartite interdependence graph and enables a centralized global coordinator to\noptimize order matching and agent behavior using Integer Linear Programming\n(ILP). Enhanced by gradient clipping and localized graph sampling, our GATDDQN\nimproves scalability and robustness. Furthermore, the inclusion of a posterior\nscore function in the ILP captures the online exploration-exploitation\ntrade-off and reduces the potential overestimation bias of agents, thereby\nelevating the quality of the derived solutions. Through extensive experiments\nand validation, BMG-Q has demonstrated superior performance in both training\nand operations for thousands of vehicle agents, outperforming benchmark\nreinforcement learning frameworks by around 10% in accumulative rewards and\nshowing a significant reduction in overestimation bias by over 50%.\nAdditionally, it maintains robustness amidst task variations and fleet size\nchanges, establishing BMG-Q as an effective, scalable, and robust framework for\nadvancing ride-pooling order dispatch operations.",
    "published_date": "2025-01-23T00:00:00",
    "year": 2025,
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.ET",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.13448v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.13435v1",
    "title": "GC-ConsFlow: Leveraging Optical Flow Residuals and Global Context for Robust Deepfake Detection",
    "authors": [
      "Jiaxin Chen",
      "Miao Hu",
      "Dengyong Zhang",
      "Jingyang Meng"
    ],
    "author_ids": [],
    "abstract": "The rapid development of Deepfake technology has enabled the generation of\nhighly realistic manipulated videos, posing severe social and ethical\nchallenges. Existing Deepfake detection methods primarily focused on either\nspatial or temporal inconsistencies, often neglecting the interplay between the\ntwo or suffering from interference caused by natural facial motions. To address\nthese challenges, we propose the global context consistency flow (GC-ConsFlow),\na novel dual-stream framework that effectively integrates spatial and temporal\nfeatures for robust Deepfake detection. The global grouped context aggregation\nmodule (GGCA), integrated into the global context-aware frame flow stream\n(GCAF), enhances spatial feature extraction by aggregating grouped global\ncontext information, enabling the detection of subtle, spatial artifacts within\nframes. The flow-gradient temporal consistency stream (FGTC), rather than\ndirectly modeling the residuals, it is used to improve the robustness of\ntemporal feature extraction against the inconsistency introduced by unnatural\nfacial motion using optical flow residuals and gradient-based features. By\ncombining these two streams, GC-ConsFlow demonstrates the effectiveness and\nrobustness in capturing complementary spatiotemporal forgery traces. Extensive\nexperiments show that GC-ConsFlow outperforms existing state-of-the-art methods\nin detecting Deepfake videos under various compression scenarios.",
    "published_date": "2025-01-23T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.13435v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.13421v1",
    "title": "Perceived Fairness of the Machine Learning Development Process: Concept Scale Development",
    "authors": [
      "Anoop Mishra",
      "Deepak Khazanchi"
    ],
    "author_ids": [],
    "abstract": "In machine learning (ML) applications, unfairness is triggered due to bias in\nthe data, the data curation process, erroneous assumptions, and implicit bias\nrendered during the development process. It is also well-accepted by\nresearchers that fairness in ML application development is highly subjective,\nwith a lack of clarity of what it means from an ML development and\nimplementation perspective. Thus, in this research, we investigate and\nformalize the notion of the perceived fairness of ML development from a\nsociotechnical lens. Our goal in this research is to understand the\ncharacteristics of perceived fairness in ML applications. We address this\nresearch goal using a three-pronged strategy: 1) conducting virtual focus\ngroups with ML developers, 2) reviewing existing literature on fairness in ML,\nand 3) incorporating aspects of justice theory relating to procedural and\ndistributive justice. Based on our theoretical exposition, we propose\noperational attributes of perceived fairness to be transparency,\naccountability, and representativeness. These are described in terms of\nmultiple concepts that comprise each dimension of perceived fairness. We use\nthis operationalization to empirically validate the notion of perceived\nfairness of machine learning (ML) applications from both the ML practioners and\nusers perspectives. The multidimensional framework for perceived fairness\noffers a comprehensive understanding of perceived fairness, which can guide the\ncreation of fair ML systems with positive implications for society and\nbusinesses.",
    "published_date": "2025-01-23T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.CY",
      "cs.LG",
      "J.4; J.1; K.4; K.6; I.2; E.m"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.13421v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.13381v2",
    "title": "Do as We Do, Not as You Think: the Conformity of Large Language Models",
    "authors": [
      "Zhiyuan Weng",
      "Guikun Chen",
      "Wenguan Wang"
    ],
    "author_ids": [],
    "abstract": "Recent advancements in large language models (LLMs) revolutionize the field\nof intelligent agents, enabling collaborative multi-agent systems capable of\ntackling complex problems across various domains. However, the potential of\nconformity within these systems, analogous to phenomena like conformity bias\nand groupthink in human group dynamics, remains largely unexplored, raising\nconcerns about their collective problem-solving capabilities and possible\nethical implications. This paper presents a comprehensive study on conformity\nin LLM-driven multi-agent systems, focusing on three aspects: the existence of\nconformity, the factors influencing conformity, and potential mitigation\nstrategies. In particular, we introduce BenchForm, a new conformity-oriented\nbenchmark, featuring reasoning-intensive tasks and five distinct interaction\nprotocols designed to probe LLMs' behavior in collaborative scenarios. Several\nrepresentative LLMs are evaluated on BenchForm, using metrics such as\nconformity rate and independence rate to quantify conformity's impact. Our\nanalysis delves into factors influencing conformity, including interaction time\nand majority size, and examines how the subject agent rationalizes its\nconforming behavior. Furthermore, we explore two strategies to mitigate\nconformity effects, i.e., developing enhanced personas and implementing a\nreflection mechanism. Several interesting findings regarding LLMs' conformity\nare derived from empirical results and case studies. We hope that these\ninsights can pave the way for more robust and ethically-aligned collaborative\nAI systems. Our benchmark and code are available at BenchForm.",
    "published_date": "2025-01-23T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.13381v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.13346v1",
    "title": "Markovian Search with Socially Aware Constraints",
    "authors": [
      "Mohammad Reza Aminian",
      "Vahideh Manshadi",
      "Rad Niazadeh"
    ],
    "author_ids": [],
    "abstract": "We study a general class of sequential search problems for selecting multiple\ncandidates from different societal groups under \"ex-ante constraints\" aimed at\nproducing socially desirable outcomes, such as demographic parity, diversity\nquotas, or subsidies for disadvantaged groups. Starting with the canonical\nPandora's box model [Weitzman, 1978] under a single affine constraint on\nselection and inspection probabilities, we show that the optimal constrained\npolicy retains an index-based structure similar to the unconstrained case, but\nmay randomize between two dual-based adjustments that are both easy to compute\nand economically interpretable. We then extend our results to handle multiple\naffine constraints by reducing the problem to a variant of the exact\nCarath\\'eodory problem and providing a novel polynomial-time algorithm to\ngenerate an optimal randomized dual-adjusted index-based policy that satisfies\nall constraints simultaneously. Building on these insights, we consider richer\nsearch processes (e.g., search with rejection and multistage search) modeled by\njoint Markov scheduling (JMS) [Dumitriu et al., 2003; Gittins, 1979]. By\nimposing general affine and convex ex-ante constraints, we develop a\nprimal-dual algorithm that randomizes over a polynomial number of dual-based\nadjustments to the unconstrained JMS Gittins indices, yielding a near-feasible,\nnear-optimal policy. Our approach relies on the key observation that a suitable\nrelaxation of the Lagrange dual function for these constrained problems admits\nindex-based policies akin to those in the unconstrained setting. Using a\nnumerical study, we investigate the implications of imposing various\nconstraints, in particular the utilitarian loss (price of fairness), and\nwhether these constraints induce their intended societally desirable outcomes.",
    "published_date": "2025-01-23T00:00:00",
    "year": 2025,
    "categories": [
      "cs.DS",
      "cs.GT",
      "econ.TH",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.13346v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.13321v1",
    "title": "Investigation of the Privacy Concerns in AI Systems for Young Digital Citizens: A Comparative Stakeholder Analysis",
    "authors": [
      "Molly Campbell",
      "Ankur Barthwal",
      "Sandhya Joshi",
      "Austin Shouli",
      "Ajay Kumar Shrestha"
    ],
    "author_ids": [],
    "abstract": "The integration of Artificial Intelligence (AI) systems into technologies\nused by young digital citizens raises significant privacy concerns. This study\ninvestigates these concerns through a comparative analysis of stakeholder\nperspectives. A total of 252 participants were surveyed, with the analysis\nfocusing on 110 valid responses from parents/educators and 100 from AI\nprofessionals after data cleaning. Quantitative methods, including descriptive\nstatistics and Partial Least Squares Structural Equation Modeling, examined\nfive validated constructs: Data Ownership and Control, Parental Data Sharing,\nPerceived Risks and Benefits, Transparency and Trust, and Education and\nAwareness. Results showed Education and Awareness significantly influenced data\nownership and risk assessment, while Data Ownership and Control strongly\nimpacted Transparency and Trust. Transparency and Trust, along with Perceived\nRisks and Benefits, showed minimal influence on Parental Data Sharing,\nsuggesting other factors may play a larger role. The study underscores the need\nfor user-centric privacy controls, tailored transparency strategies, and\ntargeted educational initiatives. Incorporating diverse stakeholder\nperspectives offers actionable insights into ethical AI design and governance,\nbalancing innovation with robust privacy protections to foster trust in a\ndigital age.",
    "published_date": "2025-01-23T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.13321v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.13320v1",
    "title": "Toward Ethical AI: A Qualitative Analysis of Stakeholder Perspectives",
    "authors": [
      "Ajay Kumar Shrestha",
      "Sandhya Joshi"
    ],
    "author_ids": [],
    "abstract": "As Artificial Intelligence (AI) systems become increasingly integrated into\nvarious aspects of daily life, concerns about privacy and ethical\naccountability are gaining prominence. This study explores stakeholder\nperspectives on privacy in AI systems, focusing on educators, parents, and AI\nprofessionals. Using qualitative analysis of survey responses from 227\nparticipants, the research identifies key privacy risks, including data\nbreaches, ethical misuse, and excessive data collection, alongside perceived\nbenefits such as personalized services, enhanced efficiency, and educational\nadvancements. Stakeholders emphasized the need for transparency,\nprivacy-by-design, user empowerment, and ethical oversight to address privacy\nconcerns effectively. The findings provide actionable insights into balancing\nthe benefits of AI with robust privacy protections, catering to the diverse\nneeds of stakeholders. Recommendations include implementing selective data use,\nfostering transparency, promoting user autonomy, and integrating ethical\nprinciples into AI development. This study contributes to the ongoing discourse\non ethical AI, offering guidance for designing privacy-centric systems that\nalign with societal values and build trust among users. By addressing privacy\nchallenges, this research underscores the importance of developing AI\ntechnologies that are not only innovative but also ethically sound and\nresponsive to the concerns of all stakeholders.",
    "published_date": "2025-01-23T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.13320v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.13307v1",
    "title": "From Cross-Modal to Mixed-Modal Visible-Infrared Re-Identification",
    "authors": [
      "Mahdi Alehdaghi",
      "Rajarshi Bhattacharya",
      "Pourya Shamsolmoali",
      "Rafael M. O. Cruz",
      "Eric Granger"
    ],
    "author_ids": [],
    "abstract": "Visible-infrared person re-identification (VI-ReID) aims to match individuals\nacross different camera modalities, a critical task in modern surveillance\nsystems. While current VI-ReID methods focus on cross-modality matching,\nreal-world applications often involve mixed galleries containing both V and I\nimages, where state-of-the-art methods show significant performance limitations\ndue to large domain shifts and low discrimination across mixed modalities. This\nis because gallery images from the same modality may have lower domain gaps but\ncorrespond to different identities. This paper introduces a novel mixed-modal\nReID setting, where galleries contain data from both modalities. To address the\ndomain shift among inter-modal and low discrimination capacity in intra-modal\nmatching, we propose the Mixed Modality-Erased and -Related (MixER) method. The\nMixER learning approach disentangles modality-specific and modality-shared\nidentity information through orthogonal decomposition, modality-confusion, and\nID-modality-related objectives. MixER enhances feature robustness across\nmodalities, improving cross-modal and mixed-modal settings performance. Our\nextensive experiments on the SYSU-MM01, RegDB and LLMC datasets indicate that\nour approach can provide state-of-the-art results using a single backbone, and\nshowcase the flexibility of our approach in mixed gallery applications.",
    "published_date": "2025-01-23T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.13307v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.13302v1",
    "title": "Watching the AI Watchdogs: A Fairness and Robustness Analysis of AI Safety Moderation Classifiers",
    "authors": [
      "Akshit Achara",
      "Anshuman Chhabra"
    ],
    "author_ids": [],
    "abstract": "AI Safety Moderation (ASM) classifiers are designed to moderate content on\nsocial media platforms and to serve as guardrails that prevent Large Language\nModels (LLMs) from being fine-tuned on unsafe inputs. Owing to their potential\nfor disparate impact, it is crucial to ensure that these classifiers: (1) do\nnot unfairly classify content belonging to users from minority groups as unsafe\ncompared to those from majority groups and (2) that their behavior remains\nrobust and consistent across similar inputs. In this work, we thus examine the\nfairness and robustness of four widely-used, closed-source ASM classifiers:\nOpenAI Moderation API, Perspective API, Google Cloud Natural Language (GCNL)\nAPI, and Clarifai API. We assess fairness using metrics such as demographic\nparity and conditional statistical parity, comparing their performance against\nASM models and a fair-only baseline. Additionally, we analyze robustness by\ntesting the classifiers' sensitivity to small and natural input perturbations.\nOur findings reveal potential fairness and robustness gaps, highlighting the\nneed to mitigate these issues in future versions of these models.",
    "published_date": "2025-01-23T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.13302v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.13273v1",
    "title": "Enhancing Robust Fairness via Confusional Spectral Regularization",
    "authors": [
      "Gaojie Jin",
      "Sihao Wu",
      "Jiaxu Liu",
      "Tianjin Huang",
      "Ronghui Mu"
    ],
    "author_ids": [],
    "abstract": "Recent research has highlighted a critical issue known as ``robust fairness\",\nwhere robust accuracy varies significantly across different classes,\nundermining the reliability of deep neural networks (DNNs). A common approach\nto address this has been to dynamically reweight classes during training,\ngiving more weight to those with lower empirical robust performance. However,\nwe find there is a divergence of class-wise robust performance between training\nset and testing set, which limits the effectiveness of these explicit\nreweighting methods, indicating the need for a principled alternative. In this\nwork, we derive a robust generalization bound for the worst-class robust error\nwithin the PAC-Bayesian framework, accounting for unknown data distributions.\nOur analysis shows that the worst-class robust error is influenced by two main\nfactors: the spectral norm of the empirical robust confusion matrix and the\ninformation embedded in the model and training set. While the latter has been\nextensively studied, we propose a novel regularization technique targeting the\nspectral norm of the robust confusion matrix to improve worst-class robust\naccuracy and enhance robust fairness. We validate our approach through\ncomprehensive experiments on various datasets and models, demonstrating its\neffectiveness in enhancing robust fairness.",
    "published_date": "2025-01-22T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.13273v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.13257v1",
    "title": "Investigating the Developer eXperience of LGBTQIAPN+ People in Agile Teams",
    "authors": [
      "Edvaldo Wassouf Jr",
      "Pedro Fukuda",
      "Awdren Fontão"
    ],
    "author_ids": [],
    "abstract": "Diversity in software teams drives innovation and enhances performance, but\nit also introduces challenges that require intentional management. LGBTQIAPN+\nprofessionals in the software industry face unique barriers, including\ndiscrimination, low visibility, and harassment, which can diminish\nsatisfaction, productivity, and retention. This study investigates the\nDeveloper Experience (DX) of LGBTQIAPN+ individuals in Agile software\ndevelopment teams through a survey of 40 participants. Findings highlight that\npsychological safety and inclusive policies are critical for fostering\nequitable contributions and team cohesion. Agile practices, such as\nretrospectives, pair programming, and daily meetings, enhance collaboration and\nreduce biases when tailored to the needs of underrepresented groups, creating\nan environment of mutual respect and openness. Additionally, remote work offers\nsignificant benefits for LGBTQIAPN+ professionals, including improved\npsychological comfort, productivity, and work-life balance. However, challenges\nlike isolation and insufficient virtual team interactions remain and must be\naddressed. This research underscores the importance of integrating inclusivity\ninto Agile methodologies and organizational practices to support the unique\nneeds of diverse professionals. By fostering an environment that values\ndiversity, organizations can enable more effective and satisfied teams,\nultimately driving higher-quality outcomes and improved organizational\nperformance. This study provides actionable insights for creating more\ninclusive and supportive Agile work environments.",
    "published_date": "2025-01-22T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.13257v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.13238v2",
    "title": "Fast-Locking and High-Resolution Mixed-Mode DLL with Binary Search and Clock Failure Detection for Wide Frequency Ranges in 3-nm FinFET CMOS",
    "authors": [
      "Nicolás Wainstein",
      "Eran Avitay",
      "Eugene Avner"
    ],
    "author_ids": [],
    "abstract": "This paper presents a mixed-mode delay-locked loop (MM-DLL) with binary\nsearch (BS) locking, designed to cover a broad frequency range from 533 MHz to\n4.26 GHz. The BS locking scheme optimizes the locking time, reducing it from a\nlinear to a logarithmic function, completing in B+1 cycles, where B represents\nthe digital-to-analog (DAC) resolution controlling the voltage-controlled delay\nline (VCDL). At the start of the BS process, large step sizes can cause\nsignificant bias overshoots, potentially leading to clock failure conditions\n(i.e., clocks fail to propagate through the VCDL). To address this issue, a\ntoggle detector is introduced to monitor clock activity and adjust the binary\nsearch controller. Upon detecting a stalled clock, the controller reverts the\nDAC code to the previous working code and resumes the BS with a reduced step\nsize. Fabricated in a 3-nm FinFET CMOS process, the proposed MM-DLL achieves a\nlocking time of under 10.5 ns while consuming 5.4 mW from a 0.75 V supply at\n4.26 GHz. The measured performance includes a high resolution of 0.73 ps, with\na static phase error of 0.73 ps, RMS jitter of 1.2 ps, and peak-to-peak jitter\nof 4.9 ps. The proposed MM-DLL achieves state-of-the-art power figure of merit\n(FoM) of 0.82 pJ and DLL locking FoM of 0.01 $pJ\\cdot ns^2$.",
    "published_date": "2025-01-22T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.13238v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.13223v2",
    "title": "Scaling for Fairness? Analyzing Model Size, Data Composition, and Multilinguality in Vision-Language Bias",
    "authors": [
      "Zahraa Al Sahili",
      "Ioannis Patras",
      "Matthew Purver"
    ],
    "author_ids": [],
    "abstract": "As large scale vision language models become increasingly central to modern\nAI applications, understanding and mitigating social biases in these systems\nhas never been more critical. We investigate how dataset composition, model\nsize, and multilingual training affect gender and racial bias in a popular VLM,\nCLIP, and its open source variants. In particular, we systematically evaluate\nmodels trained on varying dataset scales and architectures, as well as\nmultilingual versions encompassing English along with Persian, Turkish, and\nFinnish,languages with minimal gender marking. To assess social perception\nbias, we measure the zero-shot performance on face images featuring socially\ncharged terms rooted in the psychological constructs of communion and agency,\nand demographic labeling bias using both the FairFace and PATA datasets.\n  Our findings reveal three key insights. First, while larger training datasets\ncan mitigate some biases, they may also introduce or amplify others when the\ndata composition is imbalanced. Second, although increasing model size\ngenerally improves performance, it does not consistently reduce bias and can,\nin certain cases, exacerbate it. Finally, while multilingual training broadens\nlinguistic coverage, it does not inherently neutralize bias and can transfer or\nintensify inequities across languages. Taken together, these results highlight\nthe necessity of inclusive, carefully curated training data to foster fairness\nrather than relying solely on model scaling or language expansion. We provide a\nsystematic evaluation for vision language bias across diverse demographics,\nunderscoring the urgent need for intentional bias mitigation strategies in\nnext-generation AI systems.",
    "published_date": "2025-01-22T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.13223v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.13219v1",
    "title": "Enhancing Multi-Attribute Fairness in Healthcare Predictive Modeling",
    "authors": [
      "Xiaoyang Wang",
      "Christopher C. Yang"
    ],
    "author_ids": [],
    "abstract": "Artificial intelligence (AI) systems in healthcare have demonstrated\nremarkable potential to improve patient outcomes. However, if not designed with\nfairness in mind, they also carry the risks of perpetuating or exacerbating\nexisting health disparities. Although numerous fairness-enhancing techniques\nhave been proposed, most focus on a single sensitive attribute and neglect the\nbroader impact that optimizing fairness for one attribute may have on the\nfairness of other sensitive attributes. In this work, we introduce a novel\napproach to multi-attribute fairness optimization in healthcare AI, tackling\nfairness concerns across multiple demographic attributes concurrently. Our\nmethod follows a two-phase approach: initially optimizing for predictive\nperformance, followed by fine-tuning to achieve fairness across multiple\nsensitive attributes. We develop our proposed method using two strategies,\nsequential and simultaneous. Our results show a significant reduction in\nEqualized Odds Disparity (EOD) for multiple attributes, while maintaining high\npredictive accuracy. Notably, we demonstrate that single-attribute fairness\nmethods can inadvertently increase disparities in non-targeted attributes\nwhereas simultaneous multi-attribute optimization achieves more balanced\nfairness improvements across all attributes. These findings highlight the\nimportance of comprehensive fairness strategies in healthcare AI and offer\npromising directions for future research in this critical area.",
    "published_date": "2025-01-22T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.13219v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.13061v1",
    "title": "Systematic comparison of gender inequality in scientific rankings across disciplines",
    "authors": [
      "Ana Maria Jaramillo",
      "Mariana Macedo",
      "Marcos Oliveira",
      "Fariba Karimi",
      "Ronaldo Menezes"
    ],
    "author_ids": [],
    "abstract": "The participation of women in academia has increased in the last few decades\nacross many fields (e.g., Computer Science, History, Medicine). However, this\nincrease in the participation of women has not been the same at all career\nstages. Here, we study how gender participation within different fields is\nrelated to gender representation in top-ranking positions in productivity\n(number of papers), research impact (number of citations), and co-authorship\nnetworks (degree of connectivity). We analyzed over 80 million papers published\nfrom 1975 to 2020 in 19 academic fields. Our findings reveal that women remain\na minority in all 19 fields, with physics, geology, and mathematics having the\nlowest percentage of papers authored by women at 14% and psychology having the\nlargest percentage at 39%. Women are significantly underrepresented in\ntop-ranking positions (top 10% or higher) across all fields and metrics\n(productivity, citations, and degree), indicating that it remains challenging\nfor early researchers (especially women) to reach top-ranking positions, as our\nresults reveal the rankings to be rigid over time. Finally, we show that in\nmost fields, women and men with comparable productivity levels and career age\ntend to attain different levels of citations, where women tend to benefit more\nfrom co-authorships, while men tend to benefit more from productivity,\nespecially in pSTEMs. Our findings highlight that while the participation of\nwomen has risen in some fields, they remain under-represented in top-ranking\npositions. Greater gender participation at entry levels often helps\nrepresentation, but stronger interventions are still needed to achieve\nlong-lasting careers for women and their participation in top-ranking\npositions.",
    "published_date": "2025-01-22T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.13061v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.13060v1",
    "title": "Development of the Critical Reflection and Agency in Computing Index",
    "authors": [
      "Aadarsh Padiyath",
      "Mark Guzdial",
      "Barbara Ericson"
    ],
    "author_ids": [],
    "abstract": "As computing's societal impact grows, so does the need for computing students\nto recognize and address the ethical and sociotechnical implications of their\nwork. While there are efforts to integrate ethics into computing curricula, we\nlack a standardized tool to measure those efforts, specifically, students'\nattitudes towards ethical reflection and their ability to effect change. This\npaper introduces the novel framework of Critically Conscious Computing and\nreports on the development and content validation of the Critical Reflection\nand Agency in Computing Index, a novel instrument designed to assess\nundergraduate computing students' attitudes towards practicing critically\nconscious computing. The resulting index is a theoretically grounded,\nexpert-reviewed tool to support research and practice in computing ethics\neducation. This enables researchers and educators to gain insights into\nstudents' perspectives, inform the design of targeted ethics interventions, and\nmeasure the effectiveness of computing ethics education initiatives.",
    "published_date": "2025-01-22T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.13060v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.13014v1",
    "title": "Paper Quality Assessment based on Individual Wisdom Metrics from Open Peer Review",
    "authors": [
      "Andrii Zahorodnii",
      "Jasper J. F. van den Bosch",
      "Ian Charest",
      "Christopher Summerfield",
      "Ila R. Fiete"
    ],
    "author_ids": [],
    "abstract": "This study proposes a data-driven framework for enhancing the accuracy and\nefficiency of scientific peer review through an open, bottom-up process that\nestimates reviewer quality. Traditional closed peer review systems, while\nessential for quality control, are often slow, costly, and subject to biases\nthat can impede scientific progress. Here, we introduce a method that evaluates\nindividual reviewer reliability by quantifying agreement with community\nconsensus scores and applying Bayesian weighting to refine paper quality\nassessments. We analyze open peer review data from two major scientific\nconferences, and demonstrate that reviewer-specific quality scores\nsignificantly improve the reliability of paper quality estimation. Perhaps\nsurprisingly, we find that reviewer quality scores are unrelated to authorship\nquality. Our model incorporates incentive structures to recognize high-quality\nreviewers and encourage broader coverage of submitted papers, thereby\nmitigating the common \"rich-get-richer\" pitfall of social media. These findings\nsuggest that open peer review, with mechanisms for estimating and incentivizing\nreviewer quality, offers a scalable and equitable alternative for scientific\npublishing, with potential to enhance the speed, fairness, and transparency of\nthe peer review process.",
    "published_date": "2025-01-22T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.13014v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.12980v1",
    "title": "Implicit Causality-biases in humans and LLMs as a tool for benchmarking LLM discourse capabilities",
    "authors": [
      "Florian Kankowski",
      "Torgrim Solstad",
      "Sina Zarriess",
      "Oliver Bott"
    ],
    "author_ids": [],
    "abstract": "In this paper, we compare data generated with mono- and multilingual LLMs\nspanning a range of model sizes with data provided by human participants in an\nexperimental setting investigating well-established discourse biases. Beyond\nthe comparison as such, we aim to develop a benchmark to assess the\ncapabilities of LLMs with discourse biases as a robust proxy for more general\ndiscourse understanding capabilities. More specifically, we investigated\nImplicit Causality verbs, for which psycholinguistic research has found\nparticipants to display biases with regard to three phenomena:\\ the\nestablishment of (i) coreference relations (Experiment 1), (ii) coherence\nrelations (Experiment 2), and (iii) the use of particular referring expressions\n(Experiments 3 and 4). With regard to coreference biases we found only the\nlargest monolingual LLM (German Bloom 6.4B) to display more human-like biases.\nFor coherence relation, no LLM displayed the explanation bias usually found for\nhumans. For referring expressions, all LLMs displayed a preference for\nreferring to subject arguments with simpler forms than to objects. However, no\nbias effect on referring expression was found, as opposed to recent studies\ninvestigating human biases.",
    "published_date": "2025-01-22T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.12980v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.12962v2",
    "title": "It's complicated. The relationship of algorithmic fairness and non-discrimination regulations in the EU AI Act",
    "authors": [
      "Kristof Meding"
    ],
    "author_ids": [],
    "abstract": "What constitutes a fair decision? This question is not only difficult for\nhumans but becomes more challenging when Artificial Intelligence (AI) models\nare used. In light of discriminatory algorithmic behaviors, the EU has recently\npassed the AI Act, which mandates specific rules for AI models, incorporating\nboth traditional legal non-discrimination regulations and machine learning\nbased algorithmic fairness concepts. This paper aims to bridge these two\ndifferent concepts in the AI Act through: First a high-level introduction of\nboth concepts targeting legal and computer science-oriented scholars, and\nsecond an in-depth analysis of the AI Act's relationship between legal\nnon-discrimination regulations and algorithmic fairness. Our analysis reveals\nthree key findings: (1.), most non-discrimination regulations target only\nhigh-risk AI systems. (2.), the regulation of high-risk systems encompasses\nboth data input requirements and output monitoring, though these regulations\nare often inconsistent and raise questions of computational feasibility. (3.)\nRegulations for General Purpose AI Models, such as Large Language Models that\nare not simultaneously classified as high-risk systems, currently lack\nspecificity compared to other regulations. Based on these findings, we\nrecommend developing more specific auditing and testing methodologies for AI\nsystems. This paper aims to serve as a foundation for future interdisciplinary\ncollaboration between legal scholars and computer science-oriented machine\nlearning researchers studying discrimination in AI systems.",
    "published_date": "2025-01-22T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.12962v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.12897v1",
    "title": "Discrimination and AI in insurance: what do people find fair? Results from a survey",
    "authors": [
      "Frederik Zuiderveen Borgesius",
      "Marvin van Bekkum",
      "Iris van Ooijen",
      "Gabi Schaap",
      "Maaike Harbers",
      "Tjerk Timan"
    ],
    "author_ids": [],
    "abstract": "Two modern trends in insurance are data-intensive underwriting and\nbehavior-based insurance. Data-intensive underwriting means that insurers use\nand analyze more data for estimating the chance that a consumer files a claim\nand calculating the premium based on that estimation. Insurers analyze the new\ndatasets with artificial intelligence (AI) to discover new correlations, with\nwhich they can estimate the policyholder's expected claims cost more precisely.\nInsurers also offer behavior-based insurance. For example, some car insurers\nuse AI to follow the driving behavior of an individual policyholder in\nreal-time and decide whether to offer that policyholder a discount. Similarly,\na life insurer could track a policyholder's activity with a smart watch and\noffer a discount for an active lifestyle.\n  In this paper, we report on a survey of the Dutch population (N=999) in which\nwe asked people's opinions about examples of data-intensive underwriting and\nbehavior-based insurance. The main results include the following. First, if\nsurvey respondents find an insurance practice unfair, they also find the\npractice unacceptable. Second, respondents find almost all modern insurance\npractices that we described unfair. Third, respondents find practices fairer if\nthey can influence the premium. For example, respondents find behavior-based\ncar insurance with a car tracker relatively fair. Fourth, if respondents do not\nsee the logic of using a certain consumer characteristic, then respondents find\nit unfair if an insurer calculates the premium based on the characteristic.\nFifth, respondents find it unfair if an insurer offers an insurance product\nonly to a specific group, such as car insurance specifically for family\ndoctors. Sixth, respondents find it unfair if an insurance practice leads to\nhigher prices for poorer people. We reflect on the policy implications of the\nfindings.",
    "published_date": "2025-01-22T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.12897v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.12886v1",
    "title": "Multi-Platform Aggregated Dataset of Online Communities (MADOC)",
    "authors": [
      "Marija Mitrović Dankulov",
      "Aleksandar Tomašević",
      "Slobodan Maletić",
      "Miroslav Anđelković",
      "Ana Vranić",
      "Darja Cvetković",
      "Boris Stupovski",
      "Dušan Vudragović",
      "Sara Major",
      "Aleksandar Bogojević"
    ],
    "author_ids": [],
    "abstract": "The Multi-platform Aggregated Dataset of Online Communities (MADOC) is a\ncomprehensive dataset that facilitates computational social science research by\nproviding FAIR-compliant standardized access to cross-platform analysis of\nonline social dynamics. MADOC aggregates and standardizes data from Bluesky,\nKoo, Reddit, and Voat (2012-2024), containing 18.9 million posts, 236 million\ncomments, and 23.1 million unique users. The dataset enables comparative\nstudies of toxic behavior evolution across platforms through standardized\ninteraction records and sentiment analysis. By providing UUID-anonymized user\nhistories and temporal alignment of banned communities' activity patterns,\nMADOC supports research on content moderation impacts and platform migration\ntrends. Distributed via Zenodo with persistent identifiers and Python/R\ntoolkits, the dataset adheres to FAIR principles while addressing post-API-era\nresearch challenges through ethical aggregation of public social media\narchives.",
    "published_date": "2025-01-22T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SI",
      "cs.CY",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.12886v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.12883v3",
    "title": "Generative AI Misuse Potential in Cyber Security Education: A Case Study of a UK Degree Program",
    "authors": [
      "Carlton Shepherd"
    ],
    "author_ids": [],
    "abstract": "Recent advances in generative artificial intelligence (AI), such as ChatGPT,\nGoogle Gemini, and other large language models (LLMs), pose significant\nchallenges to upholding academic integrity in higher education. This paper\ninvestigates the susceptibility of a Master's-level cyber security degree\nprogram at a UK Russell Group university, accredited by a leading national\nbody, to LLM misuse. Through the application and extension of a quantitative\nassessment framework, we identify a high exposure to misuse, particularly in\nindependent project- and report-based assessments. Contributing factors,\nincluding block teaching and a predominantly international cohort, are\nhighlighted as potential amplifiers of these vulnerabilities. To address these\nchallenges, we discuss the adoption of LLM-resistant assessments, detection\ntools, and the importance of fostering an ethical learning environment. These\napproaches aim to uphold academic standards while preparing students for the\ncomplexities of real-world cyber security.",
    "published_date": "2025-01-22T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CR",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.12883v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.12826v1",
    "title": "Open or Closed LLM for Lesser-Resourced Languages? Lessons from Greek",
    "authors": [
      "John Pavlopoulos",
      "Juli Bakagianni",
      "Kanella Pouli",
      "Maria Gavriilidou"
    ],
    "author_ids": [],
    "abstract": "Natural Language Processing (NLP) for lesser-resourced languages faces\npersistent challenges, including limited datasets, inherited biases from\nhigh-resource languages, and the need for domain-specific solutions. This study\naddresses these gaps for Modern Greek through three key contributions. First,\nwe evaluate the performance of open-source (Llama-70b) and closed-source\n(GPT-4o mini) large language models (LLMs) on seven core NLP tasks with dataset\navailability, revealing task-specific strengths, weaknesses, and parity in\ntheir performance. Second, we expand the scope of Greek NLP by reframing\nAuthorship Attribution as a tool to assess potential data usage by LLMs in\npre-training, with high 0-shot accuracy suggesting ethical implications for\ndata provenance. Third, we showcase a legal NLP case study, where a Summarize,\nTranslate, and Embed (STE) methodology outperforms the traditional TF-IDF\napproach for clustering \\emph{long} legal texts. Together, these contributions\nprovide a roadmap to advance NLP in lesser-resourced languages, bridging gaps\nin model evaluation, task innovation, and real-world impact.",
    "published_date": "2025-01-22T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.12826v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.10407v1",
    "title": "Addressing Bias in Generative AI: Challenges and Research Opportunities in Information Management",
    "authors": [
      "Xiahua Wei",
      "Naveen Kumar",
      "Han Zhang"
    ],
    "author_ids": [],
    "abstract": "Generative AI technologies, particularly Large Language Models (LLMs), have\ntransformed information management systems but introduced substantial biases\nthat can compromise their effectiveness in informing business decision-making.\nThis challenge presents information management scholars with a unique\nopportunity to advance the field by identifying and addressing these biases\nacross extensive applications of LLMs. Building on the discussion on bias\nsources and current methods for detecting and mitigating bias, this paper seeks\nto identify gaps and opportunities for future research. By incorporating\nethical considerations, policy implications, and sociotechnical perspectives,\nwe focus on developing a framework that covers major stakeholders of Generative\nAI systems, proposing key research questions, and inspiring discussion. Our\ngoal is to provide actionable pathways for researchers to address bias in LLM\napplications, thereby advancing research in information management that\nultimately informs business practices. Our forward-looking framework and\nresearch agenda advocate interdisciplinary approaches, innovative methods,\ndynamic perspectives, and rigorous evaluation to ensure fairness and\ntransparency in Generative AI-driven information systems. We expect this study\nto serve as a call to action for information management scholars to tackle this\ncritical issue, guiding the improvement of fairness and effectiveness in\nLLM-based systems for business practice.",
    "published_date": "2025-01-22T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.10407v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.12769v1",
    "title": "Urban Priority Pass: Fair Signalized Intersection Management Accounting For Passenger Needs Through Prioritization",
    "authors": [
      "Kevin Riehl",
      "Anastasios Kouvelas",
      "Michail Makridis"
    ],
    "author_ids": [],
    "abstract": "Over the past few decades, efforts of road traffic management and practice\nhave predominantly focused on maximizing system efficiency and mitigating\ncongestion from a system perspective. This efficiency-driven approach implies\nthe equal treatment of all vehicles, which often overlooks individual user\nexperiences, broader social impacts, and the fact that users are heterogeneous\nin their urgency and experience different costs when being delayed. Existing\nstrategies to account for the differences in needs of users in traffic\nmanagement cover dedicated transit lanes, prioritization of emergency vehicles,\ntransit signal prioritization, and economic instruments. Even though they are\nthe major bottleneck for traffic in cities, no dedicated instrument that\nenables prioritization of individual drivers at intersections. The Priority\nPass is a reservation-based, economic controller that expedites entitled\nvehicles at signalized intersections, without causing arbitrary delays for\nnot-entitled vehicles and without affecting transportation efficiency de trop.\nThe prioritization of vulnerable road users, emergency vehicles, commercial\ntaxi and delivery drivers, or urgent individuals can enhance road safety, and\nachieve social, environmental, and economic goals. A case study in Manhattan\ndemonstrates the feasibility of individual prioritization (up to 40\\% delay\ndecrease), and quantifies the potential of the Priority Pass to gain social\nwelfare benefits for the people. A market for prioritization could generate up\nto 1 million \\$ in daily revenues for Manhattan, and equitably allocate delay\nreductions to those in need, rather than those with a high income.",
    "published_date": "2025-01-22T00:00:00",
    "year": 2025,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.12769v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.12681v1",
    "title": "Can masking background and object reduce static bias for zero-shot action recognition?",
    "authors": [
      "Takumi Fukuzawa",
      "Kensho Hara",
      "Hirokatsu Kataoka",
      "Toru Tamaki"
    ],
    "author_ids": [],
    "abstract": "In this paper, we address the issue of static bias in zero-shot action\nrecognition. Action recognition models need to represent the action itself, not\nthe appearance. However, some fully-supervised works show that models often\nrely on static appearances, such as the background and objects, rather than\nhuman actions. This issue, known as static bias, has not been investigated for\nzero-shot. Although CLIP-based zero-shot models are now common, it remains\nunclear if they sufficiently focus on human actions, as CLIP primarily captures\nappearance features related to languages. In this paper, we investigate the\ninfluence of static bias in zero-shot action recognition with CLIP-based\nmodels. Our approach involves masking backgrounds, objects, and people\ndifferently during training and validation. Experiments with masking background\nshow that models depend on background bias as their performance decreases for\nKinetics400. However, for Mimetics, which has a weak background bias, masking\nthe background leads to improved performance even if the background is masked\nduring validation. Furthermore, masking both the background and objects in\ndifferent colors improves performance for SSv2, which has a strong object bias.\nThese results suggest that masking the background or objects during training\nprevents models from overly depending on static bias and makes them focus more\non human action.",
    "published_date": "2025-01-22T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.12681v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.12612v2",
    "title": "T2ISafety: Benchmark for Assessing Fairness, Toxicity, and Privacy in Image Generation",
    "authors": [
      "Lijun Li",
      "Zhelun Shi",
      "Xuhao Hu",
      "Bowen Dong",
      "Yiran Qin",
      "Xihui Liu",
      "Lu Sheng",
      "Jing Shao"
    ],
    "author_ids": [],
    "abstract": "Text-to-image (T2I) models have rapidly advanced, enabling the generation of\nhigh-quality images from text prompts across various domains. However, these\nmodels present notable safety concerns, including the risk of generating\nharmful, biased, or private content. Current research on assessing T2I safety\nremains in its early stages. While some efforts have been made to evaluate\nmodels on specific safety dimensions, many critical risks remain unexplored. To\naddress this gap, we introduce T2ISafety, a safety benchmark that evaluates T2I\nmodels across three key domains: toxicity, fairness, and bias. We build a\ndetailed hierarchy of 12 tasks and 44 categories based on these three domains,\nand meticulously collect 70K corresponding prompts. Based on this taxonomy and\nprompt set, we build a large-scale T2I dataset with 68K manually annotated\nimages and train an evaluator capable of detecting critical risks that previous\nwork has failed to identify, including risks that even ultra-large proprietary\nmodels like GPTs cannot correctly detect. We evaluate 12 prominent diffusion\nmodels on T2ISafety and reveal several concerns including persistent issues\nwith racial fairness, a tendency to generate toxic content, and significant\nvariation in privacy protection across the models, even with defense methods\nlike concept erasing. Data and evaluator are released under\nhttps://github.com/adwardlee/t2i_safety.",
    "published_date": "2025-01-22T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.12612v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.12573v1",
    "title": "Leveraging LLMs to Create a Haptic Devices' Recommendation System",
    "authors": [
      "Yang Liu",
      "Haiwei Dong",
      "Abdulmotaleb El Saddik"
    ],
    "author_ids": [],
    "abstract": "Haptic technology has seen significant growth, yet a lack of awareness of\nexisting haptic device design knowledge hinders development. This paper\naddresses these limitations by leveraging advancements in Large Language Models\n(LLMs) to develop a haptic agent, focusing specifically on Grounded Force\nFeedback (GFF) devices recommendation. Our approach involves automating the\ncreation of a structured haptic device database using information from research\npapers and product specifications. This database enables the recommendation of\nrelevant GFF devices based on user queries. To ensure precise and contextually\nrelevant recommendations, the system employs a dynamic retrieval method that\ncombines both conditional and semantic searches. Benchmarking against the\nestablished UEQ and existing haptic device searching tools, the proposed haptic\nrecommendation agent ranks in the top 10\\% across all UEQ categories with mean\ndifferences favoring the agent in nearly all subscales, and maintains no\nsignificant performance bias across different user groups, showcasing superior\nusability and user satisfaction.",
    "published_date": "2025-01-22T00:00:00",
    "year": 2025,
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.HC",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.12573v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.12544v1",
    "title": "LEGOS-SLEEC: Tool for Formalizing and Analyzing Normative Requirements",
    "authors": [
      "Kevin Kolyakov",
      "Lina Marsso",
      "Nick Feng",
      "Junwei Quan",
      "Marsha Chechik"
    ],
    "author_ids": [],
    "abstract": "Systems interacting with humans, such as assistive robots or chatbots, are\nincreasingly integrated into our society. To prevent these systems from causing\nsocial, legal, ethical, empathetic, or cultural (SLEEC) harms, normative\nrequirements specify the permissible range of their behaviors. These\nrequirements encompass both functional and non-functional aspects and are\ndefined with respect to time. Typically, these requirements are specified by\nstakeholders from a broad range of fields, such as lawyers, ethicists, or\nphilosophers, who may lack technical expertise. Because such stakeholders often\nhave different goals, responsibilities, and objectives, ensuring that these\nrequirements are well-formed is crucial. SLEEC DSL, a domain-specific language\nresembling natural language, has been developed to formalize these requirements\nas SLEEC rules. In this paper, we present LEGOS-SLEEC, a tool designed to\nsupport interdisciplinary stakeholders in specifying normative requirements as\nSLEEC rules, and in analyzing and debugging their well-formedness. LEGOS-SLEEC\nis built using four previously published components, which have been shown to\nbe effective and usable across nine case studies. Reflecting on this\nexperience, we have significantly improved the user interface of LEGOS-SLEEC\nand its diagnostic support, and demonstrate the effectiveness of these\nimprovements using four interdisciplinary stakeholders. Showcase video URL is:\nhttps://youtu.be/LLaBLGxSi8A",
    "published_date": "2025-01-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.12544v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.12538v3",
    "title": "Academic case reports lack diversity: Assessing the presence and diversity of sociodemographic and behavioral factors related to Post COVID-19 Condition",
    "authors": [
      "Juan Andres Medina Florez",
      "Shaina Raza",
      "Rashida Lynn",
      "Zahra Shakeri",
      "Brendan T. Smith",
      "Elham Dolatabadi"
    ],
    "author_ids": [],
    "abstract": "Understanding the prevalence, disparities, and symptom variations of Post\nCOVID-19 Condition (PCC) for vulnerable populations is crucial to improving\ncare and addressing intersecting inequities. This study aims to develop a\ncomprehensive framework for integrating social determinants of health (SDOH)\ninto PCC research by leveraging NLP techniques to analyze disparities and\nvariations in SDOH representation within PCC case reports. Following\nconstruction of a PCC Case Report Corpus, comprising over 7,000 case reports\nfrom the LitCOVID repository, a subset of 709 reports were annotated with 26\ncore SDOH-related entity types using pre-trained named entity recognition (NER)\nmodels, human review, and data augmentation to improve quality, diversity and\nrepresentation of entity types. An NLP pipeline integrating NER, natural\nlanguage inference (NLI), trigram and frequency analyses was developed to\nextract and analyze these entities. Both encoder-only transformer models and\nRNN-based models were assessed for the NER objective.\n  Fine-tuned encoder-only BERT models outperformed traditional RNN-based models\nin generalizability to distinct sentence structures and greater class sparsity.\nExploratory analysis revealed variability in entity richness, with prevalent\nentities like condition, age, and access to care, and underrepresentation of\nsensitive categories like race and housing status. Trigram analysis highlighted\nfrequent co-occurrences among entities, including age, gender, and condition.\nThe NLI objective (entailment and contradiction analysis) showed attributes\nlike \"Experienced violence or abuse\" and \"Has medical insurance\" had high\nentailment rates (82.4%-80.3%), while attributes such as \"Is\nfemale-identifying,\" \"Is married,\" and \"Has a terminal condition\" exhibited\nhigh contradiction rates (70.8%-98.5%).",
    "published_date": "2025-01-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.12538v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.12521v1",
    "title": "An Empirically-grounded tool for Automatic Prompt Linting and Repair: A Case Study on Bias, Vulnerability, and Optimization in Developer Prompts",
    "authors": [
      "Dhia Elhaq Rzig",
      "Dhruba Jyoti Paul",
      "Kaiser Pister",
      "Jordan Henkel",
      "Foyzul Hassan"
    ],
    "author_ids": [],
    "abstract": "The tidal wave of advancements in Large Language Models (LLMs) has led to\ntheir swift integration into application-level logic. Many software systems now\nuse prompts to interact with these black-box models, combining natural language\nwith dynamic values interpolated at runtime, to perform tasks ranging from\nsentiment analysis to question answering. Due to the programmatic and\nstructured natural language aspects of these prompts, we refer to them as\nDeveloper Prompts. Unlike traditional software artifacts, Dev Prompts blend\nnatural language instructions with artificial languages such as programming and\nmarkup languages, thus requiring specialized tools for analysis, distinct from\nclassical software evaluation methods.\n  In response to this need, we introduce PromptDoctor, a tool explicitly\ndesigned to detect and correct issues of Dev Prompts. PromptDoctor identifies\nand addresses problems related to bias, vulnerability, and sub-optimal\nperformance in Dev Prompts, helping mitigate their possible harms. In our\nanalysis of 2,173 Dev Prompts, selected as a representative sample of 40,573\nDev Prompts, we found that 3.46% contained one or more forms of bias, 10.75%\nwere vulnerable to prompt injection attacks. Additionally, 3,310 were amenable\nto automated prompt optimization. To address these issues, we applied\nPromptDoctor to the flawed Dev Prompts we discovered. PromptDoctor de-biased\n68.29% of the biased Dev Prompts, hardened 41.81% of the vulnerable Dev\nPrompts, and improved the performance of 37.1% sub-optimal Dev Prompts.\nFinally, we developed a PromptDoctor VSCode extension, enabling developers to\neasily enhance Dev Prompts in their existing development workflows. The data\nand source code for this work are available at",
    "published_date": "2025-01-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.12521v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.10404v1",
    "title": "Data Protection through Governance Frameworks",
    "authors": [
      "Sivananda Reddy Julakanti",
      "Naga Satya KiranmayeeSattiraju",
      "Rajeswari Julakanti"
    ],
    "author_ids": [],
    "abstract": "In todays increasingly digital world, data has become one of the most\nvaluable assets for organizations. With the rise in cyberattacks, data\nbreaches, and the stringent regulatory environment, it is imperative to adopt\nrobust data protection strategies. One such approach is the use of governance\nframeworks, which provide structured guidelines, policies, and processes to\nensure data protection, compliance, and ethical usage. This paper explores the\nrole of data governance frameworks in protecting sensitive information and\nmaintaining organizational data security. It delves into the principles,\nstrategies, and best practices that constitute an effective governance\nframework, including risk management, access controls, data quality assurance,\nand compliance with regulations like GDPR, HIPAA, and CCPA. By analyzing case\nstudies from various sectors, the paper highlights the practicalchallenges,\nlimitations, and advantages of implementing data governance frameworks.\nAdditionally, the paper examines how data governance frameworks contribute to\ntransparency, accountability, and operational efficiency, while also\nidentifying emerging trends and technologies that enhance data protection.\nUltimately, the paper aims to provide a comprehensive understanding of how\ngovernance frameworks can be leveraged to safeguard organizational data and\nensure its responsible use.",
    "published_date": "2025-01-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CR",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.10404v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.12456v1",
    "title": "Deploying Privacy Guardrails for LLMs: A Comparative Analysis of Real-World Applications",
    "authors": [
      "Shubhi Asthana",
      "Bing Zhang",
      "Ruchi Mahindru",
      "Chad DeLuca",
      "Anna Lisa Gentile",
      "Sandeep Gopisetty"
    ],
    "author_ids": [],
    "abstract": "The adoption of Large Language Models (LLMs) has revolutionized AI\napplications but poses significant challenges in safeguarding user privacy.\nEnsuring compliance with privacy regulations such as GDPR and CCPA while\naddressing nuanced privacy risks requires robust and scalable frameworks. This\npaper presents a detailed study of OneShield Privacy Guard, a framework\ndesigned to mitigate privacy risks in user inputs and LLM outputs across\nenterprise and open-source settings. We analyze two real-world deployments:(1)\na multilingual privacy-preserving system integrated with Data and Model\nFactory, focusing on enterprise-scale data governance; and (2) PR Insights, an\nopen-source repository emphasizing automated triaging and community-driven\nrefinements. In Deployment 1, OneShield achieved a 0.95 F1 score in detecting\nsensitive entities like dates, names, and phone numbers across 26 languages,\noutperforming state-of-the-art tool such as StarPII and Presidio by up to 12\\%.\nDeployment 2, with an average F1 score of 0.86, reduced manual effort by over\n300 hours in three months, accurately flagging 8.25\\% of 1,256 pull requests\nfor privacy risks with enhanced context sensitivity. These results demonstrate\nOneShield's adaptability and efficacy in diverse environments, offering\nactionable insights for context-aware entity recognition, automated compliance,\nand ethical AI adoption. This work advances privacy-preserving frameworks,\nsupporting user trust and compliance across operational contexts.",
    "published_date": "2025-01-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.12456v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.12433v1",
    "title": "Owls are wise and foxes are unfaithful: Uncovering animal stereotypes in vision-language models",
    "authors": [
      "Tabinda Aman",
      "Mohammad Nadeem",
      "Shahab Saquib Sohail",
      "Mohammad Anas",
      "Erik Cambria"
    ],
    "author_ids": [],
    "abstract": "Animal stereotypes are deeply embedded in human culture and language. They\noften shape our perceptions and expectations of various species. Our study\ninvestigates how animal stereotypes manifest in vision-language models during\nthe task of image generation. Through targeted prompts, we explore whether\nDALL-E perpetuates stereotypical representations of animals, such as \"owls as\nwise,\" \"foxes as unfaithful,\" etc. Our findings reveal significant stereotyped\ninstances where the model consistently generates images aligned with cultural\nbiases. The current work is the first of its kind to examine animal\nstereotyping in vision-language models systematically and to highlight a\ncritical yet underexplored dimension of bias in AI-generated visual content.",
    "published_date": "2025-01-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.12433v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.12344v1",
    "title": "CYCle: Choosing Your Collaborators Wisely to Enhance Collaborative Fairness in Decentralized Learning",
    "authors": [
      "Nurbek Tastan",
      "Samuel Horvath",
      "Karthik Nandakumar"
    ],
    "author_ids": [],
    "abstract": "Collaborative learning (CL) enables multiple participants to jointly train\nmachine learning (ML) models on decentralized data sources without raw data\nsharing. While the primary goal of CL is to maximize the expected accuracy gain\nfor each participant, it is also important to ensure that the gains are fairly\ndistributed. Specifically, no client should be negatively impacted by the\ncollaboration, and the individual gains must ideally be commensurate with the\ncontributions. Most existing CL algorithms require central coordination and\nfocus on the gain maximization objective while ignoring collaborative fairness.\nIn this work, we first show that the existing measure of collaborative fairness\nbased on the correlation between accuracy values without and with collaboration\nhas drawbacks because it does not account for negative collaboration gain. We\nargue that maximizing mean collaboration gain (MCG) while simultaneously\nminimizing the collaboration gain spread (CGS) is a fairer alternative. Next,\nwe propose the CYCle protocol that enables individual participants in a private\ndecentralized learning (PDL) framework to achieve this objective through a\nnovel reputation scoring method based on gradient alignment between the local\ncross-entropy and distillation losses. Experiments on the CIFAR-10, CIFAR-100,\nand Fed-ISIC2019 datasets empirically demonstrate the effectiveness of the\nCYCle protocol to ensure positive and fair collaboration gain for all\nparticipants, even in cases where the data distributions of participants are\nhighly skewed. For the simple mean estimation problem with two participants, we\nalso theoretically show that CYCle performs better than standard FedAvg,\nespecially when there is large statistical heterogeneity.",
    "published_date": "2025-01-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.12344v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.12431v2",
    "title": "Modality Interactive Mixture-of-Experts for Fake News Detection",
    "authors": [
      "Yifan Liu",
      "Yaokun Liu",
      "Zelin Li",
      "Ruichen Yao",
      "Yang Zhang",
      "Dong Wang"
    ],
    "author_ids": [],
    "abstract": "The proliferation of fake news on social media platforms disproportionately\nimpacts vulnerable populations, eroding trust, exacerbating inequality, and\namplifying harmful narratives. Detecting fake news in multimodal contexts --\nwhere deceptive content combines text and images -- is particularly challenging\ndue to the nuanced interplay between modalities. Existing multimodal fake news\ndetection methods often emphasize cross-modal consistency but ignore the\ncomplex interactions between text and visual elements, which may complement,\ncontradict, or independently influence the predicted veracity of a post. To\naddress these challenges, we present Modality Interactive Mixture-of-Experts\nfor Fake News Detection (MIMoE-FND), a novel hierarchical Mixture-of-Experts\nframework designed to enhance multimodal fake news detection by explicitly\nmodeling modality interactions through an interaction gating mechanism. Our\napproach models modality interactions by evaluating two key aspects of modality\ninteractions: unimodal prediction agreement and semantic alignment. The\nhierarchical structure of MIMoE-FND allows for distinct learning pathways\ntailored to different fusion scenarios, adapting to the unique characteristics\nof each modality interaction. By tailoring fusion strategies to diverse\nmodality interaction scenarios, MIMoE-FND provides a more robust and nuanced\napproach to multimodal fake news detection. We evaluate our approach on three\nreal-world benchmarks spanning two languages, demonstrating its superior\nperformance compared to state-of-the-art methods. By enhancing the accuracy and\ninterpretability of fake news detection, MIMoE-FND offers a promising tool to\nmitigate the spread of misinformation, with the potential to better safeguard\nvulnerable communities against its harmful effects.",
    "published_date": "2025-01-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "I.2.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.12431v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.12244v1",
    "title": "Zero-shot Bias Correction: Efficient MR Image Inhomogeneity Reduction Without Any Data",
    "authors": [
      "Hongxu Yang",
      "Edina Timko",
      "Brice Fernandez"
    ],
    "author_ids": [],
    "abstract": "In recent years, deep neural networks for image inhomogeneity reduction have\nshown promising results. However, current methods with (un)supervised solutions\nrequire preparing a training dataset, which is expensive and laborious for data\ncollection. In this work, we demonstrate a novel zero-shot deep neural\nnetworks, which requires no data for pre-training and dedicated assumption of\nthe bias field. The designed light-weight CNN enables an efficient zero-shot\nadaptation for bias-corrupted image correction. Our method provides a novel\nsolution to mitigate the biased corrupted image as iterative homogeneity\nrefinement, which therefore ensures the considered issue can be solved easier\nwith stable convergence of zero-shot optimization. Extensive comparison on\ndifferent datasets show that the proposed method performs better than current\ndata-free N4 methods in both efficiency and accuracy.",
    "published_date": "2025-01-21T00:00:00",
    "year": 2025,
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.12244v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.12147v1",
    "title": "Improving Influence-based Instruction Tuning Data Selection for Balanced Learning of Diverse Capabilities",
    "authors": [
      "Qirun Dai",
      "Dylan Zhang",
      "Jiaqi W. Ma",
      "Hao Peng"
    ],
    "author_ids": [],
    "abstract": "Selecting appropriate training data is crucial for effective instruction\nfine-tuning of large language models (LLMs), which aims to (1) elicit strong\ncapabilities, and (2) achieve balanced performance across a diverse range of\ntasks. Influence-based methods show promise in achieving (1) by estimating the\ncontribution of each training example to the model's predictions, but often\nstruggle with (2). Our systematic investigation reveals that this\nunderperformance can be attributed to an inherent bias where certain tasks\nintrinsically have greater influence than others. As a result, data selection\nis often biased towards these tasks, not only hurting the model's performance\non others but also, counterintuitively, harms performance on these\nhigh-influence tasks themselves.\n  As a remedy, we propose BIDS, a Balanced and Influential Data Selection\nalgorithm. BIDS first normalizes influence scores of the training data, and\nthen iteratively balances data selection by choosing the training example with\nthe highest influence on the most underrepresented task. Experiments with both\nLlama-3 and Mistral-v0.3 on seven benchmarks spanning five diverse capabilities\nshow that BIDS consistently outperforms both state-of-the-art influence-based\nalgorithms and other non-influence-based selection frameworks. Surprisingly,\ntraining on a 15% subset selected by BIDS can even outperform full-dataset\ntraining with a much more balanced performance. Our analysis further highlights\nthe importance of both instance-level normalization and iterative optimization\nof selected data for balanced learning of diverse capabilities.",
    "published_date": "2025-01-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "I.2.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.12147v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.12058v1",
    "title": "Fractional Subadditivity of Submodular Functions: Equality Conditions and Their Applications",
    "authors": [
      "Gunank Jakhar",
      "Gowtham R. Kurri",
      "Suryajith Chillara",
      "Vinod M. Prabhakaran"
    ],
    "author_ids": [],
    "abstract": "Submodular functions are known to satisfy various forms of fractional\nsubadditivity. This work investigates the conditions for equality to hold\nexactly or approximately in the fractional subadditivity of submodular\nfunctions. We establish that a small gap in the inequality implies that the\nfunction is close to being modular, and that the gap is zero if and only if the\nfunction is modular. We then present natural implications of these results for\nspecial cases of submodular functions, such as entropy, relative entropy, and\nmatroid rank. As a consequence, we characterize the necessary and sufficient\nconditions for equality to hold in Shearer's lemma, recovering a result of\nEllis \\emph{et al.} (2016) as a special case. We leverage our results to\npropose a new multivariate mutual information, which generalizes Watanabe's\ntotal correlation (1960), Han's dual total correlation (1978), and Csisz\\'ar\nand Narayan's shared information (2004), and analyze its properties. Among\nthese properties, we extend Watanabe's characterization of total correlation as\nthe maximum correlation over partitions to fractional partitions. When applied\nto matrix determinantal inequalities for positive definite matrices, our\nresults recover the equality conditions of the classical determinantal\ninequalities of Hadamard, Sz\\'asz, and Fischer as special cases.",
    "published_date": "2025-01-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.12058v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.12023v1",
    "title": "Comparative Analysis of Pre-trained Deep Learning Models and DINOv2 for Cushing's Syndrome Diagnosis in Facial Analysis",
    "authors": [
      "Hongjun Liu",
      "Changwei Song",
      "Jiaqi Qiang",
      "Jianqiang Li",
      "Hui Pan",
      "Lin Lu",
      "Xiao Long",
      "Qing Zhao",
      "Jiuzuo Huang",
      "Shi Chen"
    ],
    "author_ids": [],
    "abstract": "Cushing's syndrome is a condition caused by excessive glucocorticoid\nsecretion from the adrenal cortex, often manifesting with moon facies and\nplethora, making facial data crucial for diagnosis. Previous studies have used\npre-trained convolutional neural networks (CNNs) for diagnosing Cushing's\nsyndrome using frontal facial images. However, CNNs are better at capturing\nlocal features, while Cushing's syndrome often presents with global facial\nfeatures. Transformer-based models like ViT and SWIN, which utilize\nself-attention mechanisms, can better capture long-range dependencies and\nglobal features. Recently, DINOv2, a foundation model based on visual\nTransformers, has gained interest. This study compares the performance of\nvarious pre-trained models, including CNNs, Transformer-based models, and\nDINOv2, in diagnosing Cushing's syndrome. We also analyze gender bias and the\nimpact of freezing mechanisms on DINOv2. Our results show that\nTransformer-based models and DINOv2 outperformed CNNs, with ViT achieving the\nhighest F1 score of 85.74%. Both the pre-trained model and DINOv2 had higher\naccuracy for female samples. DINOv2 also showed improved performance when\nfreezing parameters. In conclusion, Transformer-based models and DINOv2 are\neffective for Cushing's syndrome classification.",
    "published_date": "2025-01-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CV",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.12023v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.12020v1",
    "title": "On the \"Illusion\" of Gender Bias in Face Recognition: Explaining the Fairness Issue Through Non-demographic Attributes",
    "authors": [
      "Paul Jonas Kurz",
      "Haiyu Wu",
      "Kevin W. Bowyer",
      "Philipp Terhörst"
    ],
    "author_ids": [],
    "abstract": "Face recognition systems (FRS) exhibit significant accuracy differences based\non the user's gender. Since such a gender gap reduces the trustworthiness of\nFRS, more recent efforts have tried to find the causes. However, these studies\nmake use of manually selected, correlated, and small-sized sets of facial\nfeatures to support their claims. In this work, we analyse gender bias in face\nrecognition by successfully extending the search domain to decorrelated\ncombinations of 40 non-demographic facial characteristics. First, we propose a\ntoolchain to effectively decorrelate and aggregate facial attributes to enable\na less-biased gender analysis on large-scale data. Second, we introduce two new\nfairness metrics to measure fairness with and without context. Based on these\ngrounds, we thirdly present a novel unsupervised algorithm able to reliably\nidentify attribute combinations that lead to vanishing bias when used as filter\npredicates for balanced testing datasets. The experiments show that the gender\ngap vanishes when images of male and female subjects share specific attributes,\nclearly indicating that the issue is not a question of biology but of the\nsocial definition of appearance. These findings could reshape our understanding\nof fairness in face biometrics and provide insights into FRS, helping to\naddress gender bias issues.",
    "published_date": "2025-01-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.12020v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.13125v2",
    "title": "Generating Plausible Distractors for Multiple-Choice Questions via Student Choice Prediction",
    "authors": [
      "Yooseop Lee",
      "Suin Kim",
      "Yohan Jo"
    ],
    "author_ids": [],
    "abstract": "In designing multiple-choice questions (MCQs) in education, creating\nplausible distractors is crucial for identifying students' misconceptions and\ngaps in knowledge and accurately assessing their understanding. However, prior\nstudies on distractor generation have not paid sufficient attention to\nenhancing the difficulty of distractors, resulting in reduced effectiveness of\nMCQs. This study presents a pipeline for training a model to generate\ndistractors that are more likely to be selected by students. First, we train a\npairwise ranker to reason about students' misconceptions and assess the\nrelative plausibility of two distractors. Using this model, we create a dataset\nof pairwise distractor ranks and then train a distractor generator via Direct\nPreference Optimization (DPO) to generate more plausible distractors.\nExperiments on computer science subjects (Python, DB, MLDL) demonstrate that\nour pairwise ranker effectively identifies students' potential\nmisunderstandings and achieves ranking accuracy comparable to human experts.\nFurthermore, our distractor generator outperforms several baselines in\ngenerating plausible distractors and produces questions with a higher item\ndiscrimination index (DI).",
    "published_date": "2025-01-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.13125v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.11916v4",
    "title": "Generating with Fairness: A Modality-Diffused Counterfactual Framework for Incomplete Multimodal Recommendations",
    "authors": [
      "Jin Li",
      "Shoujin Wang",
      "Qi Zhang",
      "Shui Yu",
      "Fang Chen"
    ],
    "author_ids": [],
    "abstract": "Incomplete scenario is a prevalent, practical, yet challenging setting in\nMultimodal Recommendations (MMRec), where some item modalities are missing due\nto various factors. Recently, a few efforts have sought to improve the\nrecommendation accuracy by exploring generic structures from incomplete data.\nHowever, two significant gaps persist: 1) the difficulty in accurately\ngenerating missing data due to the limited ability to capture modality\ndistributions; and 2) the critical but overlooked visibility bias, where items\nwith missing modalities are more likely to be disregarded due to the\nprioritization of items' multimodal data over user preference alignment. This\nbias raises serious concerns about the fair treatment of items. To bridge these\ntwo gaps, we propose a novel Modality-Diffused Counterfactual (MoDiCF)\nframework for incomplete multimodal recommendations. MoDiCF features two key\nmodules: a novel modality-diffused data completion module and a new\ncounterfactual multimodal recommendation module. The former, equipped with a\nparticularly designed multimodal generative framework, accurately generates and\niteratively refines missing data from learned modality-specific distribution\nspaces. The latter, grounded in the causal perspective, effectively mitigates\nthe negative causal effects of visibility bias and thus assures fairness in\nrecommendations. Both modules work collaboratively to address the two\naforementioned significant gaps for generating more accurate and fair results.\nExtensive experiments on three real-world datasets demonstrate the superior\nperformance of MoDiCF in terms of both recommendation accuracy and fairness.\nThe code and processed datasets are released at\nhttps://github.com/JinLi-i/MoDiCF.",
    "published_date": "2025-01-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.11916v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.11880v1",
    "title": "Community-Aware Temporal Walks: Parameter-Free Representation Learning on Continuous-Time Dynamic Graphs",
    "authors": [
      "He Yu",
      "Jing Liu"
    ],
    "author_ids": [],
    "abstract": "Dynamic graph representation learning plays a crucial role in understanding\nevolving behaviors. However, existing methods often struggle with flexibility,\nadaptability, and the preservation of temporal and structural dynamics. To\naddress these issues, we propose Community-aware Temporal Walks (CTWalks), a\nnovel framework for representation learning on continuous-time dynamic graphs.\nCTWalks integrates three key components: a community-based parameter-free\ntemporal walk sampling mechanism, an anonymization strategy enriched with\ncommunity labels, and an encoding process that leverages continuous temporal\ndynamics modeled via ordinary differential equations (ODEs). This design\nenables precise modeling of both intra- and inter-community interactions,\noffering a fine-grained representation of evolving temporal patterns in\ncontinuous-time dynamic graphs. CTWalks theoretically overcomes locality bias\nin walks and establishes its connection to matrix factorization. Experiments on\nbenchmark datasets demonstrate that CTWalks outperforms established methods in\ntemporal link prediction tasks, achieving higher accuracy while maintaining\nrobustness.",
    "published_date": "2025-01-21T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.11880v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.11752v2",
    "title": "Are generative models fair? A study of racial bias in dermatological image generation",
    "authors": [
      "Miguel López-Pérez",
      "Søren Hauberg",
      "Aasa Feragen"
    ],
    "author_ids": [],
    "abstract": "Racial bias in medicine, such as in dermatology, presents significant ethical\nand clinical challenges. This is likely to happen because there is a\nsignificant underrepresentation of darker skin tones in training datasets for\nmachine learning models. While efforts to address bias in dermatology have\nfocused on improving dataset diversity and mitigating disparities in\ndiscriminative models, the impact of racial bias on generative models remains\nunderexplored. Generative models, such as Variational Autoencoders (VAEs), are\nincreasingly used in healthcare applications, yet their fairness across diverse\nskin tones is currently not well understood. In this study, we evaluate the\nfairness of generative models in clinical dermatology with respect to racial\nbias. For this purpose, we first train a VAE with a perceptual loss to generate\nand reconstruct high-quality skin images across different skin tones. We\nutilize the Fitzpatrick17k dataset to examine how racial bias influences the\nrepresentation and performance of these models. Our findings indicate that VAE\nperformance is, as expected, influenced by representation, i.e. increased skin\ntone representation comes with increased performance on the given skin tone.\nHowever, we also observe, even independently of representation, that the VAE\nperforms better for lighter skin tones. Additionally, the uncertainty estimates\nproduced by the VAE are ineffective in assessing the model's fairness. These\nresults highlight the need for more representative dermatological datasets, but\nalso a need for better understanding the sources of bias in such model, as well\nas improved uncertainty quantification mechanisms to detect and address racial\nbias in generative models for trustworthy healthcare technologies.",
    "published_date": "2025-01-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.11752v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.11705v1",
    "title": "Human services organizations and the responsible integration of AI: Considering ethics and contextualizing risk(s)",
    "authors": [
      "Brian E. Perron",
      "Lauri Goldkind",
      "Zia Qi",
      "Bryan G. Victor"
    ],
    "author_ids": [],
    "abstract": "This paper examines the responsible integration of artificial intelligence\n(AI) in human services organizations (HSOs), proposing a nuanced framework for\nevaluating AI applications across multiple dimensions of risk. The authors\nargue that ethical concerns about AI deployment -- including professional\njudgment displacement, environmental impact, model bias, and data laborer\nexploitation -- vary significantly based on implementation context and specific\nuse cases. They challenge the binary view of AI adoption, demonstrating how\ndifferent applications present varying levels of risk that can often be\neffectively managed through careful implementation strategies. The paper\nhighlights promising solutions, such as local large language models, that can\nfacilitate responsible AI integration while addressing common ethical concerns.\nThe authors propose a dimensional risk assessment approach that considers\nfactors like data sensitivity, professional oversight requirements, and\npotential impact on client wellbeing. They conclude by outlining a path forward\nthat emphasizes empirical evaluation, starting with lower-risk applications and\nbuilding evidence-based understanding through careful experimentation. This\napproach enables organizations to maintain high ethical standards while\nthoughtfully exploring how AI might enhance their capacity to serve clients and\ncommunities effectively.",
    "published_date": "2025-01-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.11705v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.11654v1",
    "title": "Topology-preserving discretization for the magneto-frictional equations arising in the Parker conjecture",
    "authors": [
      "Mingdong He",
      "Patrick E. Farrell",
      "Kaibo Hu",
      "Boris D. Andrews"
    ],
    "author_ids": [],
    "abstract": "The Parker conjecture, which explores whether magnetic fields in perfectly\nconducting plasmas can develop tangential discontinuities during magnetic\nrelaxation, remains an open question in astrophysics. Helicity conservation\nprovides a topological barrier during relaxation, preventing topologically\nnontrivial initial data relaxing to trivial solutions; preserving this\nmechanism discretely over long time periods is therefore crucial for numerical\nsimulation. This work presents an energy- and helicity-preserving finite\nelement discretization for the magneto-frictional system, for investigating the\nParker conjecture. The algorithm preserves a discrete version of the\ntopological barrier and a discrete Arnold inequality. We also discuss\nextensions to domains with nontrivial topology.",
    "published_date": "2025-01-20T00:00:00",
    "year": 2025,
    "categories": [
      "math.NA",
      "cs.NA",
      "65N30, 65L60, 76W05"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.11654v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.13120v1",
    "title": "Multilinguality in LLM-Designed Reward Functions for Restless Bandits: Effects on Task Performance and Fairness",
    "authors": [
      "Ambreesh Parthasarathy",
      "Chandrasekar Subramanian",
      "Ganesh Senrayan",
      "Shreyash Adappanavar",
      "Aparna Taneja",
      "Balaraman Ravindran",
      "Milind Tambe"
    ],
    "author_ids": [],
    "abstract": "Restless Multi-Armed Bandits (RMABs) have been successfully applied to\nresource allocation problems in a variety of settings, including public health.\nWith the rapid development of powerful large language models (LLMs), they are\nincreasingly used to design reward functions to better match human preferences.\nRecent work has shown that LLMs can be used to tailor automated allocation\ndecisions to community needs using language prompts. However, this has been\nstudied primarily for English prompts and with a focus on task performance\nonly. This can be an issue since grassroots workers, especially in developing\ncountries like India, prefer to work in local languages, some of which are\nlow-resource. Further, given the nature of the problem, biases along population\ngroups unintended by the user are also undesirable. In this work, we study the\neffects on both task performance and fairness when the DLM algorithm, a recent\nwork on using LLMs to design reward functions for RMABs, is prompted with\nnon-English language commands. Specifically, we run the model on a synthetic\nenvironment for various prompts translated into multiple languages. The prompts\nthemselves vary in complexity. Our results show that the LLM-proposed reward\nfunctions are significantly better when prompted in English compared to other\nlanguages. We also find that the exact phrasing of the prompt impacts task\nperformance. Further, as prompt complexity increases, performance worsens for\nall languages; however, it is more robust with English prompts than with\nlower-resource languages. On the fairness side, we find that low-resource\nlanguages and more complex prompts are both highly likely to create unfairness\nalong unintended dimensions.",
    "published_date": "2025-01-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.13120v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.11597v1",
    "title": "Fairness Testing through Extreme Value Theory",
    "authors": [
      "Verya Monjezi",
      "Ashutosh Trivedi",
      "Vladik Kreinovich",
      "Saeid Tizpaz-Niari"
    ],
    "author_ids": [],
    "abstract": "Data-driven software is increasingly being used as a critical component of\nautomated decision-support systems. Since this class of software learns its\nlogic from historical data, it can encode or amplify discriminatory practices.\nPrevious research on algorithmic fairness has focused on improving average-case\nfairness. On the other hand, fairness at the extreme ends of the spectrum,\nwhich often signifies lasting and impactful shifts in societal attitudes, has\nreceived significantly less emphasis.\n  Leveraging the statistics of extreme value theory (EVT), we propose a novel\nfairness criterion called extreme counterfactual discrimination (ECD). This\ncriterion estimates the worst-case amounts of disadvantage in outcomes for\nindividuals solely based on their memberships in a protected group. Utilizing\ntools from search-based software engineering and generative AI, we present a\nrandomized algorithm that samples a statistically significant set of points\nfrom the tail of ML outcome distributions even if the input dataset lacks a\nsufficient number of relevant samples.\n  We conducted several experiments on four ML models (deep neural networks,\nlogistic regression, and random forests) over 10 socially relevant tasks from\nthe literature on algorithmic fairness. First, we evaluate the generative AI\nmethods and find that they generate sufficient samples to infer valid EVT\ndistribution in 95% of cases. Remarkably, we found that the prevalent bias\nmitigators reduce the average-case discrimination but increase the worst-case\ndiscrimination significantly in 5% of cases. We also observed that even the\ntail-aware mitigation algorithm -- MiniMax-Fairness -- increased the worst-case\ndiscrimination in 30% of cases. We propose a novel ECD-based mitigator that\nimproves fairness in the tail in 90% of cases with no degradation of the\naverage-case discrimination.",
    "published_date": "2025-01-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.11597v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.10399v1",
    "title": "Data Stewardship Decoded: Mapping Its Diverse Manifestations and Emerging Relevance at a time of AI",
    "authors": [
      "Stefaan Verhulst"
    ],
    "author_ids": [],
    "abstract": "Data stewardship has become a critical component of modern data governance,\nespecially with the growing use of artificial intelligence (AI). Despite its\nincreasing importance, the concept of data stewardship remains ambiguous and\nvaries in its application. This paper explores four distinct manifestations of\ndata stewardship to clarify its emerging position in the data governance\nlandscape. These manifestations include a) data stewardship as a set of\ncompetencies and skills, b) a function or role within organizations, c) an\nintermediary organization facilitating collaborations, and d) a set of guiding\nprinciples. The paper subsequently outlines the core competencies required for\neffective data stewardship, explains the distinction between data stewards and\nChief Data Officers (CDOs), and details the intermediary role of stewards in\nbridging gaps between data holders and external stakeholders. It also explores\nkey principles aligned with the FAIR framework (Findable, Accessible,\nInteroperable, Reusable) and introduces the emerging principle of AI readiness\nto ensure data meets the ethical and technical requirements of AI systems. The\npaper emphasizes the importance of data stewardship in enhancing data\ncollaboration, fostering public value, and managing data reuse responsibly,\nparticularly in the era of AI. It concludes by identifying challenges and\nopportunities for advancing data stewardship, including the need for\nstandardized definitions, capacity building efforts, and the creation of a\nprofessional association for data stewardship.",
    "published_date": "2025-01-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.DB"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.10399v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.11574v1",
    "title": "A Deep Reinforcement Learning based Scheduler for IoT Devices in Co-existence with 5G-NR",
    "authors": [
      "Shahida Jabeen"
    ],
    "author_ids": [],
    "abstract": "Co-existence of 5G New Radio (5G-NR) with IoT devices is considered as a\npromising technique to enhance the spectral usage and efficiency of future\ncellular networks. In this paper, a unified framework has been proposed for\nallocating in-band resource blocks (RBs), i.e., within a multi-cell network, to\n5G-NR users in co-existence with NB-IoT and LTE-M devices. First, a benchmark\n(upper-bound) scheduler has been designed for joint sub-carrier (SC) and\nmodulation and coding scheme (MCS) allocation that maximizes instantaneous\nthroughput and fairness among users/devices, while considering synchronous RB\nallocation in the neighboring cells. A series of numerical simulations with\nrealistic ICI in an urban scenario have been used to compute benchmark\nupper-bound solutions for characterizing performance in terms of throughput,\nfairness, and delay. Next, an edge learning based multi-agent deep\nreinforcement learning (DRL) framework has been developed for different DRL\nalgorithms, specifically, a policy-based gradient network (PGN), a deep\nQ-learning based network (DQN), and an actor-critic based deep deterministic\npolicy gradient network (DDPGN). The proposed DRL framework depends on\ninterference allocation, where the actions are based on inter-cell-interference\n(ICI) instead of power, which can bypass the need for raw data sharing and/or\ninter-agent communication. The numerical results reveal that the interference\nallocation based DRL schedulers can significantly outperform their\ncounterparts, where the actions are based on power allocation. Further, the\nperformance of the proposed policy-based edge learning algorithms is close to\nthe centralized ones.",
    "published_date": "2025-01-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.NI",
      "cs.ET"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.11574v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.07790v1",
    "title": "Can Generative AI be Egalitarian?",
    "authors": [
      "Philip Feldman",
      "James R. Foulds",
      "Shimei Pan"
    ],
    "author_ids": [],
    "abstract": "The recent explosion of \"foundation\" generative AI models has been built upon\nthe extensive extraction of value from online sources, often without\ncorresponding reciprocation. This pattern mirrors and intensifies the\nextractive practices of surveillance capitalism, while the potential for\nenormous profit has challenged technology organizations' commitments to\nresponsible AI practices, raising significant ethical and societal concerns.\nHowever, a promising alternative is emerging: the development of models that\nrely on content willingly and collaboratively provided by users. This article\nexplores this \"egalitarian\" approach to generative AI, taking inspiration from\nthe successful model of Wikipedia. We explore the potential implications of\nthis approach for the design, development, and constraints of future foundation\nmodels. We argue that such an approach is not only ethically sound but may also\nlead to models that are more responsive to user needs, more diverse in their\ntraining data, and ultimately more aligned with societal values. Furthermore,\nwe explore potential challenges and limitations of this approach, including\nissues of scalability, quality control, and potential biases inherent in\nvolunteer-contributed content.",
    "published_date": "2025-01-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "K.4.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.07790v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.11496v1",
    "title": "Generative AI and Large Language Models in Language Preservation: Opportunities and Challenges",
    "authors": [
      "Vincent Koc"
    ],
    "author_ids": [],
    "abstract": "Generative AI and large-scale language models (LLM) have emerged as powerful\ntools in language preservation, particularly for near-native and endangered\nlanguages. With the increasing reliance on technology for communication,\neducation, and cultural documentation, new opportunities have emerged to\nmitigate the dramatic decline of linguistic diversity worldwide. This paper\nexamines the role of generative AIs and LLMs in preserving endangered\nlanguages, highlighting the risks and challenges associated with their use. We\nanalyze the underlying technologies driving these models, including natural\nlanguage processing (NLP) and deep learning, and explore several cases where\nthese technologies have been applied to low-resource languages. Additionally,\nwe discuss ethical considerations, data scarcity issues, and technical\nchallenges while proposing solutions to enhance AI-driven language\npreservation.",
    "published_date": "2025-01-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "68T50, 91F20",
      "I.2.7; I.2.6; J.5"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.11496v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.11473v2",
    "title": "Strong Data Processing Properties of Rényi-divergences via Pinsker-type Inequalities",
    "authors": [
      "Leonhard Grosse",
      "Sara Saeidian",
      "Tobias J. Oechtering",
      "Mikael Skoglund"
    ],
    "author_ids": [],
    "abstract": "We investigate strong data processing inequalities (SDPIs) of the\nR\\'enyi-divergence between two discrete distributions when both distributions\nare passed through a fixed channel. We provide a condition on the channel for\nwhich the DPI holds with equality given two arbitrary distributions in the\nprobability simplex. Motivated by this, we examine the contraction behavior for\nrestricted sets of prior distributions via $f$-divergence inequalities: We\nprovide an alternative proof of the optimal reverse Pinsker's inequality for\nR\\'enyi-divergences first shown by Binette. We further present an improved\nPinsker's inequality for R\\'enyi-divergence based on the joint range technique\nby Harremo\\\"es and Vajda. The presented bound is tight whenever the value of\nthe total variation distance is larger than $\\frac{1}{\\alpha}$. By framing\nthese inequalities in a cross-channel setting, we arrive at SDPIs that can be\nadapted to use-case specific restrictions of input distribution and channel. We\napply these results to the R\\'enyi local differential privacy amplification\nthrough post-processing by channels that satisfy no local differential privacy\nguarantee.",
    "published_date": "2025-01-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.11473v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.11469v1",
    "title": "MASS: Overcoming Language Bias in Image-Text Matching",
    "authors": [
      "Jiwan Chung",
      "Seungwon Lim",
      "Sangkyu Lee",
      "Youngjae Yu"
    ],
    "author_ids": [],
    "abstract": "Pretrained visual-language models have made significant advancements in\nmultimodal tasks, including image-text retrieval. However, a major challenge in\nimage-text matching lies in language bias, where models predominantly rely on\nlanguage priors and neglect to adequately consider the visual content. We thus\npresent Multimodal ASsociation Score (MASS), a framework that reduces the\nreliance on language priors for better visual accuracy in image-text matching\nproblems. It can be seamlessly incorporated into existing visual-language\nmodels without necessitating additional training. Our experiments have shown\nthat MASS effectively lessens language bias without losing an understanding of\nlinguistic compositionality. Overall, MASS offers a promising solution for\nenhancing image-text matching performance in visual-language models.",
    "published_date": "2025-01-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.11469v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.11416v1",
    "title": "Mapping network structures and dynamics of decentralised cryptocurrencies: The evolution of Bitcoin (2009-2023)",
    "authors": [
      "Marco Venturini",
      "Daniel García-Costa",
      "Elena Álvarez-García",
      "Francisco Grimaldo",
      "Flaminio Squazzoni"
    ],
    "author_ids": [],
    "abstract": "Cryptocurrencies have recently been in the spotlight of public debate due to\ntheir embrace by the new US President, with crypto fans expecting a 'bull run'.\nThe global cryptocurrency market capitalisation is more than \\$3.50 trillion,\nwith 1 Bitcoin exchanging for more than \\$97,000 at the end of November 2024.\nMonitoring the evolution of these systems is key to understanding whether the\npopular perception of cryptocurrencies as a new, sustainable economic\ninfrastructure is well-founded. In this paper, we have reconstructed the\nnetwork structures and dynamics of Bitcoin from its launch in January 2009 to\nDecember 2023 and identified its key evolutionary phases. Our results show that\nnetwork centralisation and wealth concentration increased from the very early\nyears, following a richer-get-richer mechanism. This trend was endogenous to\nthe system, beyond any subsequent institutional or exogenous influence. The\nevolution of Bitcoin is characterised by three periods, Exploration, Adaptation\nand Maturity, with substantial coherent network patterns. Our findings suggest\nthat Bitcoin is a highly centralised structure, with high levels of wealth\ninequality and internally crystallised power dynamics, which may have negative\nimplications for its long-term sustainability.",
    "published_date": "2025-01-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.11416v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.16356v1",
    "title": "Evaluating Binary Decision Biases in Large Language Models: Implications for Fair Agent-Based Financial Simulations",
    "authors": [
      "Alicia Vidler",
      "Toby Walsh"
    ],
    "author_ids": [],
    "abstract": "Large Language Models (LLMs) are increasingly being used to simulate\nhuman-like decision making in agent-based financial market models (ABMs). As\nmodels become more powerful and accessible, researchers can now incorporate\nindividual LLM decisions into ABM environments. However, integration may\nintroduce inherent biases that need careful evaluation. In this paper we test\nthree state-of-the-art GPT models for bias using two model sampling approaches:\none-shot and few-shot API queries. We observe significant variations in\ndistributions of outputs between specific models, and model sub versions, with\nGPT-4o-Mini-2024-07-18 showing notably better performance (32-43% yes\nresponses) compared to GPT-4-0125-preview's extreme bias (98-99% yes\nresponses). We show that sampling methods and model sub-versions significantly\nimpact results: repeated independent API calls produce different distributions\ncompared to batch sampling within a single call. While no current GPT model can\nsimultaneously achieve a uniform distribution and Markovian properties in\none-shot testing, few-shot sampling can approach uniform distributions under\ncertain conditions. We explore the Temperature parameter, providing a\ndefinition and comparative results. We further compare our results to true\nrandom binary series and test specifically for the common human bias of\nNegative Recency - finding LLMs have a mixed ability to 'beat' humans in this\none regard. These findings emphasise the critical importance of careful LLM\nintegration into ABMs for financial markets and more broadly.",
    "published_date": "2025-01-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.16356v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.11360v1",
    "title": "Federated Learning with Sample-level Client Drift Mitigation",
    "authors": [
      "Haoran Xu",
      "Jiaze Li",
      "Wanyi Wu",
      "Hao Ren"
    ],
    "author_ids": [],
    "abstract": "Federated Learning (FL) suffers from severe performance degradation due to\nthe data heterogeneity among clients. Existing works reveal that the\nfundamental reason is that data heterogeneity can cause client drift where the\nlocal model update deviates from the global one, and thus they usually tackle\nthis problem from the perspective of calibrating the obtained local update.\nDespite effectiveness, existing methods substantially lack a deep understanding\nof how heterogeneous data samples contribute to the formation of client drift.\nIn this paper, we bridge this gap by identifying that the drift can be viewed\nas a cumulative manifestation of biases present in all local samples and the\nbias between samples is different. Besides, the bias dynamically changes as the\nFL training progresses. Motivated by this, we propose FedBSS that first\nmitigates the heterogeneity issue in a sample-level manner, orthogonal to\nexisting methods. Specifically, the core idea of our method is to adopt a\nbias-aware sample selection scheme that dynamically selects the samples from\nsmall biases to large epoch by epoch to train progressively the local model in\neach round. In order to ensure the stability of training, we set the\ndiversified knowledge acquisition stage as the warm-up stage to avoid the local\noptimality caused by knowledge deviation in the early stage of the model.\nEvaluation results show that FedBSS outperforms state-of-the-art baselines. In\naddition, we also achieved effective results on feature distribution skew and\nnoise label dataset setting, which proves that FedBSS can not only reduce\nheterogeneity, but also has scalability and robustness.",
    "published_date": "2025-01-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.11360v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.11342v1",
    "title": "Disentangled Modeling of Preferences and Social Influence for Group Recommendation",
    "authors": [
      "Guangze Ye",
      "Wen Wu",
      "Guoqing Wang",
      "Xi Chen",
      "Hong Zheng",
      "Liang He"
    ],
    "author_ids": [],
    "abstract": "The group recommendation (GR) aims to suggest items for a group of users in\nsocial networks. Existing work typically considers individual preferences as\nthe sole factor in aggregating group preferences. Actually, social influence is\nalso an important factor in modeling users' contributions to the final group\ndecision. However, existing methods either neglect the social influence of\nindividual members or bundle preferences and social influence together as a\nunified representation. As a result, these models emphasize the preferences of\nthe majority within the group rather than the actual interaction items, which\nwe refer to as the preference bias issue in GR. Moreover, the self-supervised\nlearning (SSL) strategies they designed to address the issue of group data\nsparsity fail to account for users' contextual social weights when regulating\ngroup representations, leading to suboptimal results. To tackle these issues,\nwe propose a novel model based on Disentangled Modeling of Preferences and\nSocial Influence for Group Recommendation (DisRec). Concretely, we first design\na user-level disentangling network to disentangle the preferences and social\ninfluence of group members with separate embedding propagation schemes based on\n(hyper)graph convolution networks. We then introduce a socialbased contrastive\nlearning strategy, selectively excluding user nodes based on their social\nimportance to enhance group representations and alleviate the group-level data\nsparsity issue. The experimental results demonstrate that our model\nsignificantly outperforms state-of-the-art methods on two realworld datasets.",
    "published_date": "2025-01-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.11342v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.13952v2",
    "title": "The Dual-use Dilemma in LLMs: Do Empowering Ethical Capacities Make a Degraded Utility?",
    "authors": [
      "Yiyi Zhang",
      "Xingyu Chen",
      "Kexin Chen",
      "Yuyang Du",
      "Xilin Dang",
      "Pheng-Ann Heng"
    ],
    "author_ids": [],
    "abstract": "Recent years have witnessed extensive efforts to enhance Large Language\nModels (LLMs) across various domains, alongside growing attention to their\nethical implications. However, a critical challenge remains largely overlooked:\nLLMs must balance between rejecting harmful requests for safety and\naccommodating legitimate ones for utility. This paper presents a Direct\nPreference Optimization (DPO) based alignment framework that achieves better\noverall performance by addressing this ethical-utility trade-off, using\nchemical domain applications as a proof-of-concept. Our alignment pipeline\nstarts with a GPT-assisted three-phase data generation scheme, in which we\ncreate LibraChemQA, a chemical question-answering dataset comprising 31.6k\ntriplet instances. By incorporating an innovative balanced seed in the data\ngeneration process, our framework systematically considers both legitimate and\nillegitimate requests. The framework also introduces a rephrasing mechanism for\nefficient data augmentation that enhances the model's chemical comprehension.\nWe further develop a novel hybrid evaluation scheme with LLM judges for precise\nassessment of both safety and utility. Experimental results demonstrate our\nmodel's substantial improvements in overall performance where both safety and\nutility are considered - the resulting model outperforms leading LLMs including\nClaude-3, GPT-4o, and LLaMA-3 by margins of 13.44%, 7.16%, and 7.10%\nrespectively on our released benchmark. At the end of this paper, we analyze\nexperimental results obtained from testing DeepSeek-R1 on our benchmark and\nreveal the critical ethical concerns raised by this highly acclaimed model. We\nhighlight that the long Chain-of-Thought (CoT) reasoning process employed by\nDeepSeek-R1, as well as other LLMs distilled from it, introduces significant\nethical vulnerabilities when exposed to users.",
    "published_date": "2025-01-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.13952v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.13950v1",
    "title": "DEFEND: A Large-scale 1M Dataset and Foundation Model for Tobacco Addiction Prevention",
    "authors": [
      "Naga VS Raviteja Chappa",
      "Matthew Shepard",
      "Connor McCurtain",
      "Charlotte McCormick",
      "Page Daniel Dobbs",
      "Khoa Luu"
    ],
    "author_ids": [],
    "abstract": "While tobacco advertising innovates at unprecedented speed, traditional\nsurveillance methods remain frozen in time, especially in the context of social\nmedia. The lack of large-scale, comprehensive datasets and sophisticated\nmonitoring systems has created a widening gap between industry advancement and\npublic health oversight. This paper addresses this critical challenge by\nintroducing Tobacco-1M, a comprehensive dataset of one million tobacco product\nimages with hierarchical labels spanning 75 product categories, and DEFEND, a\nnovel foundation model for tobacco product understanding. Our approach\nintegrates a Feature Enhancement Module for rich multimodal representation\nlearning, a Local-Global Visual Coherence mechanism for detailed feature\ndiscrimination, and an Enhanced Image-Text Alignment strategy for precise\nproduct characterization. Experimental results demonstrate DEFEND's superior\nperformance, achieving 83.1% accuracy in product classification and 73.8% in\nvisual question-answering tasks, outperforming existing methods by significant\nmargins. Moreover, the model exhibits robust zero-shot learning capabilities\nwith 45.6% accuracy on novel product categories. This work provides regulatory\nbodies and public health researchers with powerful tools for monitoring\nemerging tobacco products and marketing strategies, potentially revolutionizing\napproaches to tobacco control and public health surveillance.",
    "published_date": "2025-01-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.13950v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.16355v1",
    "title": "How Strategic Agents Respond: Comparing Analytical Models with LLM-Generated Responses in Strategic Classification",
    "authors": [
      "Tian Xie",
      "Pavan Rauch",
      "Xueru Zhang"
    ],
    "author_ids": [],
    "abstract": "When machine learning (ML) algorithms are used to automate human-related\ndecisions, human agents may gain knowledge of the decision policy and behave\nstrategically to obtain desirable outcomes. Strategic Classification (SC) has\nbeen proposed to address the interplay between agents and decision-makers.\nPrior work on SC has relied on assumptions that agents are perfectly or\napproximately rational, responding to decision policies by maximizing their\nutilities. Verifying these assumptions is challenging due to the difficulty of\ncollecting real-world agent responses. Meanwhile, the growing adoption of large\nlanguage models (LLMs) makes it increasingly likely that human agents in SC\nsettings will seek advice from these tools. We propose using strategic advice\ngenerated by LLMs to simulate human agent responses in SC. Specifically, we\nexamine five critical SC scenarios -- hiring, loan applications, school\nadmissions, personal income, and public assistance programs -- and simulate how\nhuman agents with diverse profiles seek advice from LLMs. We then compare the\nresulting agent responses with the best responses generated by existing\ntheoretical models. Our findings reveal that: (i) LLMs and theoretical models\ngenerally lead to agent score or qualification changes in the same direction\nacross most settings, with both achieving similar levels of fairness; (ii)\nstate-of-the-art commercial LLMs (e.g., GPT-3.5, GPT-4) consistently provide\nhelpful suggestions, though these suggestions typically do not result in\nmaximal score or qualification improvements; and (iii) LLMs tend to produce\nmore diverse agent responses, often favoring more balanced effort allocation\nstrategies. These results suggest that theoretical models align with LLMs to\nsome extent and that leveraging LLMs to simulate more realistic agent responses\noffers a promising approach to designing trustworthy ML systems.",
    "published_date": "2025-01-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.16355v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.11214v1",
    "title": "Mitigating Spatial Disparity in Urban Prediction Using Residual-Aware Spatiotemporal Graph Neural Networks: A Chicago Case Study",
    "authors": [
      "Dingyi Zhuang",
      "Hanyong Xu",
      "Xiaotong Guo",
      "Yunhan Zheng",
      "Shenhao Wang",
      "Jinhua Zhao"
    ],
    "author_ids": [],
    "abstract": "Urban prediction tasks, such as forecasting traffic flow, temperature, and\ncrime rates, are crucial for efficient urban planning and management. However,\nexisting Spatiotemporal Graph Neural Networks (ST-GNNs) often rely solely on\naccuracy, overlooking spatial and demographic disparities in their predictions.\nThis oversight can lead to imbalanced resource allocation and exacerbate\nexisting inequities in urban areas. This study introduces a Residual-Aware\nAttention (RAA) Block and an equality-enhancing loss function to address these\ndisparities. By adapting the adjacency matrix during training and incorporating\nspatial disparity metrics, our approach aims to reduce local segregation of\nresiduals and errors. We applied our methodology to urban prediction tasks in\nChicago, utilizing a travel demand dataset as an example. Our model achieved a\n48% significant improvement in fairness metrics with only a 9% increase in\nerror metrics. Spatial analysis of residual distributions revealed that models\nwith RAA Blocks produced more equitable prediction results, particularly by\nreducing errors clustered in central regions. Attention maps demonstrated the\nmodel's ability to dynamically adjust focus, leading to more balanced\npredictions. Case studies of various community areas in Chicago further\nillustrated the effectiveness of our approach in addressing spatial and\ndemographic disparities, supporting more balanced and equitable urban planning\nand policy-making.",
    "published_date": "2025-01-20T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.11214v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.13947v2",
    "title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods",
    "authors": [
      "Wenli Yang",
      "Lilian Some",
      "Michael Bain",
      "Byeong Kang"
    ],
    "author_ids": [],
    "abstract": "The rapid development of artificial intelligence has led to marked progress\nin the field. One interesting direction for research is whether Large Language\nModels (LLMs) can be integrated with structured knowledge-based systems. This\napproach aims to combine the generative language understanding of LLMs and the\nprecise knowledge representation systems by which they are integrated. This\narticle surveys the relationship between LLMs and knowledge bases, looks at how\nthey can be applied in practice, and discusses related technical, operational,\nand ethical challenges. Utilizing a comprehensive examination of the\nliterature, the study both identifies important issues and assesses existing\nsolutions. It demonstrates the merits of incorporating generative AI into\nstructured knowledge-base systems concerning data contextualization, model\naccuracy, and utilization of knowledge resources. The findings give a full list\nof the current situation of research, point out the main gaps, and propose\nhelpful paths to take. These insights contribute to advancing AI technologies\nand support their practical deployment across various sectors.",
    "published_date": "2025-01-19T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.13947v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.10990v1",
    "title": "Societal citations undermine the function of the science reward system",
    "authors": [
      "Xiaokai Li",
      "An Zeng",
      "Ying Fan"
    ],
    "author_ids": [],
    "abstract": "Citations in the scientific literature system do not simply reflect\nrelationships between knowledge but are influenced by non-objective and\nsocietal factors. Citation bias, irresponsible citation, and citation\nmanipulation are widespread and have become a serious and growing problem.\nHowever, it has been difficult to assess the consequences of mixing societal\nfactors into the literature system because there was no observable literature\nsystem unmixed with societal factors for comparison. In this paper, we\nconstruct a mathematical theorem network, representing a logic-based and\nobjective knowledge system, to address this problem. By comparing the\nmathematical theorem network and the scientific citation networks, we find that\nthese two types of networks are significantly different in their structure and\nfunction. In particular, the reward function in citation networks is impaired:\nThe scientific citation network fails to provide more recognition for more\ndisruptive results, while the mathematical theorem network can achieve. We\ndevelop a network generation model that can create two types of\nlinks$\\unicode{x2014}$logical and societal$\\unicode{x2014}$to account for these\ndifferences. The model parameter $q$, which we call the human influence factor,\ncan control the number of societal links and thus regulate the degree of mixing\nof societal factors in the networks. Under this design, the model successfully\nreproduces the differences among real networks. These results suggest that the\npresence of societal factors undermines the function of the scientific reward\nsystem. To improve the status quo, we advocate for reforming the reference list\nformat in papers, urging journals to require authors to separately disclose\nlogical references and social references.",
    "published_date": "2025-01-19T00:00:00",
    "year": 2025,
    "categories": [
      "cs.DL",
      "cs.SI",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.10990v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.10819v1",
    "title": "GAUDA: Generative Adaptive Uncertainty-guided Diffusion-based Augmentation for Surgical Segmentation",
    "authors": [
      "Yannik Frisch",
      "Christina Bornberg",
      "Moritz Fuchs",
      "Anirban Mukhopadhyay"
    ],
    "author_ids": [],
    "abstract": "Augmentation by generative modelling yields a promising alternative to the\naccumulation of surgical data, where ethical, organisational and regulatory\naspects must be considered. Yet, the joint synthesis of (image, mask) pairs for\nsegmentation, a major application in surgery, is rather unexplored. We propose\nto learn semantically comprehensive yet compact latent representations of the\n(image, mask) space, which we jointly model with a Latent Diffusion Model. We\nshow that our approach can effectively synthesise unseen high-quality paired\nsegmentation data of remarkable semantic coherence. Generative augmentation is\ntypically applied pre-training by synthesising a fixed number of additional\ntraining samples to improve downstream task models. To enhance this approach,\nwe further propose Generative Adaptive Uncertainty-guided Diffusion-based\nAugmentation (GAUDA), leveraging the epistemic uncertainty of a Bayesian\ndownstream model for targeted online synthesis. We condition the generative\nmodel on classes with high estimated uncertainty during training to produce\nadditional unseen samples for these classes. By adaptively utilising the\ngenerative model online, we can minimise the number of additional training\nsamples and centre them around the currently most uncertain parts of the data\ndistribution. GAUDA effectively improves downstream segmentation results over\ncomparable methods by an average absolute IoU of 1.6% on CaDISv2 and 1.5% on\nCholecSeg8k, two prominent surgical datasets for semantic segmentation.",
    "published_date": "2025-01-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.10819v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.10784v2",
    "title": "Measuring Fairness in Financial Transaction Machine Learning Models",
    "authors": [
      "Deniz Sezin Ayvaz",
      "Lorenzo Belenguer",
      "Hankun He",
      "Deborah Dormah Kanubala",
      "Mingxu Li",
      "Soung Low",
      "Carlos Mougan",
      "Faithful Chiagoziem Onwuegbuche",
      "Yulu Pi",
      "Natalia Sikora",
      "Dan Tran",
      "Shresth Verma",
      "Hanzhi Wang",
      "Skyler Xie",
      "Adeline Pelletier"
    ],
    "author_ids": [],
    "abstract": "Mastercard, a global leader in financial services, develops and deploys\nmachine learning models aimed at optimizing card usage and preventing attrition\nthrough advanced predictive models. These models use aggregated and anonymized\ncard usage patterns, including cross-border transactions and industry-specific\nspending, to tailor bank offerings and maximize revenue opportunities.\nMastercard has established an AI Governance program, based on its Data and Tech\nResponsibility Principles, to evaluate any built and bought AI for efficacy,\nfairness, and transparency. As part of this effort, Mastercard has sought\nexpertise from the Turing Institute through a Data Study Group to better assess\nfairness in more complex AI/ML models. The Data Study Group challenge lies in\ndefining, measuring, and mitigating fairness in these predictions, which can be\ncomplex due to the various interpretations of fairness, gaps in the research\nliterature, and ML-operations challenges.",
    "published_date": "2025-01-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.10784v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.10741v2",
    "title": "Development of Application-Specific Large Language Models to Facilitate Research Ethics Review",
    "authors": [
      "Sebastian Porsdam Mann",
      "Joel Seah Jiehao",
      "Stephen R. Latham",
      "Julian Savulescu",
      "Mateo Aboy",
      "Brian D. Earp"
    ],
    "author_ids": [],
    "abstract": "Institutional review boards (IRBs) play a crucial role in ensuring the\nethical conduct of human subjects research, but face challenges including\ninconsistency, delays, and inefficiencies. We propose the development and\nimplementation of application-specific large language models (LLMs) to\nfacilitate IRB review processes. These IRB-specific LLMs would be fine-tuned on\nIRB-specific literature and institutional datasets, and equipped with retrieval\ncapabilities to access up-to-date, context-relevant information. We outline\npotential applications, including pre-review screening, preliminary analysis,\nconsistency checking, and decision support. While addressing concerns about\naccuracy, context sensitivity, and human oversight, we acknowledge remaining\nchallenges such as over-reliance on AI and the need for transparency. By\nenhancing the efficiency and quality of ethical review while maintaining human\njudgment in critical decisions, IRB-specific LLMs offer a promising tool to\nimprove research oversight. We call for pilot studies to evaluate the\nfeasibility and impact of this approach.",
    "published_date": "2025-01-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.10741v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.10712v1",
    "title": "Poisson Hail on a Wireless Ground",
    "authors": [
      "François Baccelli",
      "Ke Feng",
      "Sergey Foss"
    ],
    "author_ids": [],
    "abstract": "This paper defines a new model which incorporates three key ingredients of a\nlarge class of wireless communication systems: (1) spatial interactions through\ninterference, (2) dynamics of the queueing type, with users joining and\nleaving, and (3) carrier sensing and collision avoidance as used in, e.g.,\nWiFi. In systems using (3), rather than directly accessing the shared resources\nupon arrival, a customer is considerate and waits to access them until nearby\nusers in service have left. This new model can be seen as a missing piece of a\nlarger puzzle that contains such dynamics as spatial birth-and-death processes,\nthe Poisson-Hail model, and wireless dynamics as key other pieces. It is shown\nthat, under natural assumptions, this model can be represented as a Markov\nprocess on the space of counting measures. The main results are then two-fold.\nThe first is on the shape of the stability region and, more precisely, on the\ncharacterization of the critical value of the arrival rate that separates\nstability from instability. The second is of a more qualitative or perhaps even\nethical nature. There is evidence that for natural values of the system\nparameters, the implementation of sensing and collision avoidance stabilizes a\nsystem that would be unstable if immediate access to the shared resources would\nbe granted. In other words, for these parameters, renouncing greedy access\nmakes sharing sustainable, whereas indulging in greedy access kills the system.",
    "published_date": "2025-01-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IT",
      "cs.NI",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.10712v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.10695v2",
    "title": "Exploring Transferable Homogeneous Groups for Compositional Zero-Shot Learning",
    "authors": [
      "Zhijie Rao",
      "Jingcai Guo",
      "Miaoge Li",
      "Yang Chen"
    ],
    "author_ids": [],
    "abstract": "Conditional dependency present one of the trickiest problems in Compositional\nZero-Shot Learning, leading to significant property variations of the same\nstate (object) across different objects (states). To address this problem,\nexisting approaches often adopt either all-to-one or one-to-one representation\nparadigms. However, these extremes create an imbalance in the seesaw between\ntransferability and discriminability, favoring one at the expense of the other.\nComparatively, humans are adept at analogizing and reasoning in a hierarchical\nclustering manner, intuitively grouping categories with similar properties to\nform cohesive concepts. Motivated by this, we propose Homogeneous Group\nRepresentation Learning (HGRL), a new perspective formulates state (object)\nrepresentation learning as multiple homogeneous sub-group representation\nlearning. HGRL seeks to achieve a balance between semantic transferability and\ndiscriminability by adaptively discovering and aggregating categories with\nshared properties, learning distributed group centers that retain\ngroup-specific discriminative features. Our method integrates three core\ncomponents designed to simultaneously enhance both the visual and prompt\nrepresentation capabilities of the model. Extensive experiments on three\nbenchmark datasets validate the effectiveness of our method.",
    "published_date": "2025-01-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.10695v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.10685v1",
    "title": "Harnessing the Potential of Large Language Models in Modern Marketing Management: Applications, Future Directions, and Strategic Recommendations",
    "authors": [
      "Raha Aghaei",
      "Ali A. Kiaei",
      "Mahnaz Boush",
      "Javad Vahidi",
      "Mohammad Zavvar",
      "Zeynab Barzegar",
      "Mahan Rofoosheh"
    ],
    "author_ids": [],
    "abstract": "Large Language Models (LLMs) have revolutionized the process of customer\nengagement, campaign optimization, and content generation, in marketing\nmanagement. In this paper, we explore the transformative potential of LLMs\nalong with the current applications, future directions, and strategic\nrecommendations for marketers. In particular, we focus on LLMs major business\ndrivers such as personalization, real-time-interactive customer insights, and\ncontent automation, and how they enable customers and business outcomes. For\ninstance, the ethical aspects of AI with respect to data privacy, transparency,\nand mitigation of bias are also covered, with the goal of promoting responsible\nuse of the technology through best practices and the use of new technologies\nbusinesses can tap into the LLM potential, which help growth and stay one step\nahead in the turmoil of digital marketing. This article is designed to give\nmarketers the necessary guidance by using best industry practices to integrate\nthese powerful LLMs into their marketing strategy and innovation without\ncompromising on the ethos of their brand.",
    "published_date": "2025-01-18T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.10685v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.10576v1",
    "title": "AI Toolkit: Libraries and Essays for Exploring the Technology and Ethics of AI",
    "authors": [
      "Levin Ho",
      "Morgan McErlean",
      "Zehua You",
      "Douglas Blank",
      "Lisa Meeden"
    ],
    "author_ids": [],
    "abstract": "In this paper we describe the development and evaluation of AITK, the\nArtificial Intelligence Toolkit. This open-source project contains both Python\nlibraries and computational essays (Jupyter notebooks) that together are\ndesigned to allow a diverse audience with little or no background in AI to\ninteract with a variety of AI tools, exploring in more depth how they function,\nvisualizing their outcomes, and gaining a better understanding of their ethical\nimplications. These notebooks have been piloted at multiple institutions in a\nvariety of humanities courses centered on the theme of responsible AI. In\naddition, we conducted usability testing of AITK. Our pilot studies and\nusability testing results indicate that AITK is easy to navigate and effective\nat helping users gain a better understanding of AI. Our goal, in this time of\nrapid innovations in AI, is for AITK to provide an accessible resource for\nfaculty from any discipline looking to incorporate AI topics into their courses\nand for anyone eager to learn more about AI on their own.",
    "published_date": "2025-01-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.10576v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.10533v2",
    "title": "A Unified Comparative Study with Generalized Conformity Scores for Multi-Output Conformal Regression",
    "authors": [
      "Victor Dheur",
      "Matteo Fontana",
      "Yorick Estievenart",
      "Naomi Desobry",
      "Souhaib Ben Taieb"
    ],
    "author_ids": [],
    "abstract": "Conformal prediction provides a powerful framework for constructing\ndistribution-free prediction regions with finite-sample coverage guarantees.\nWhile extensively studied in univariate settings, its extension to multi-output\nproblems presents additional challenges, including complex output dependencies\nand high computational costs, and remains relatively underexplored. In this\nwork, we present a unified comparative study of nine conformal methods with\ndifferent multivariate base models for constructing multivariate prediction\nregions within the same framework. This study highlights their key properties\nwhile also exploring the connections between them. Additionally, we introduce\ntwo novel classes of conformity scores for multi-output regression that\ngeneralize their univariate counterparts. These scores ensure asymptotic\nconditional coverage while maintaining exact finite-sample marginal coverage.\nOne class is compatible with any generative model, offering broad\napplicability, while the other is computationally efficient, leveraging the\nproperties of invertible generative models. Finally, we conduct a comprehensive\nempirical evaluation across 13 tabular datasets, comparing all the multi-output\nconformal methods explored in this work. To ensure a fair and consistent\ncomparison, all methods are implemented within a unified code base.",
    "published_date": "2025-01-17T00:00:00",
    "year": 2025,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.10533v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.10360v2",
    "title": "FaceXBench: Evaluating Multimodal LLMs on Face Understanding",
    "authors": [
      "Kartik Narayan",
      "Vibashan VS",
      "Vishal M. Patel"
    ],
    "author_ids": [],
    "abstract": "Multimodal Large Language Models (MLLMs) demonstrate impressive\nproblem-solving abilities across a wide range of tasks and domains. However,\ntheir capacity for face understanding has not been systematically studied. To\naddress this gap, we introduce FaceXBench, a comprehensive benchmark designed\nto evaluate MLLMs on complex face understanding tasks. FaceXBench includes\n5,000 multimodal multiple-choice questions derived from 25 public datasets and\na newly created dataset, FaceXAPI. These questions cover 14 tasks across 6\nbroad categories, assessing MLLMs' face understanding abilities in bias and\nfairness, face authentication, recognition, analysis, localization and tool\nretrieval. Using FaceXBench, we conduct an extensive evaluation of 26\nopen-source MLLMs alongside 2 proprietary models, revealing the unique\nchallenges in complex face understanding tasks. We analyze the models across\nthree evaluation settings: zero-shot, in-context task description, and\nchain-of-thought prompting. Our detailed analysis reveals that current MLLMs,\nincluding advanced models like GPT-4o, and GeminiPro 1.5, show significant room\nfor improvement. We believe FaceXBench will be a crucial resource for\ndeveloping MLLMs equipped to perform sophisticated face understanding. Code:\nhttps://github.com/Kartik-3004/facexbench",
    "published_date": "2025-01-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.10360v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.10313v1",
    "title": "Addressing Popularity Bias in Third-Party Library Recommendations Using LLMs",
    "authors": [
      "Claudio Di Sipio",
      "Juri Di Rocco",
      "Davide Di Ruscio",
      "Vladyslav Bulhakov"
    ],
    "author_ids": [],
    "abstract": "Recommender systems for software engineering (RSSE) play a crucial role in\nautomating development tasks by providing relevant suggestions according to the\ndeveloper's context. However, they suffer from the so-called popularity bias,\ni.e., the phenomenon of recommending popular items that might be irrelevant to\nthe current task. In particular, the long-tail effect can hamper the system's\nperformance in terms of accuracy, thus leading to false positives in the\nprovided recommendations. Foundation models are the most advanced generative\nAI-based models that achieve relevant results in several SE tasks.\n  This paper aims to investigate the capability of large language models (LLMs)\nto address the popularity bias in recommender systems of third-party libraries\n(TPLs). We conduct an ablation study experimenting with state-of-the-art\ntechniques to mitigate the popularity bias, including fine-tuning and\npopularity penalty mechanisms. Our findings reveal that the considered LLMs\ncannot address the popularity bias in TPL recommenders, even though fine-tuning\nand post-processing penalty mechanism contributes to increasing the overall\ndiversity of the provided recommendations. In addition, we discuss the\nlimitations of LLMs in this context and suggest potential improvements to\naddress the popularity bias in TPL recommenders, thus paving the way for\nadditional experiments in this direction.",
    "published_date": "2025-01-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.10313v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.10288v2",
    "title": "Design Patterns for the Common Good: Building Better Technologies Using the Wisdom of Virtue Ethics",
    "authors": [
      "Louisa Conwill",
      "Megan K. Levis",
      "Karla Badillo-Urquiola",
      "Walter J. Scheirer"
    ],
    "author_ids": [],
    "abstract": "Virtue ethics is a philosophical tradition that emphasizes the cultivation of\nvirtues in achieving the common good. It has been suggested to be an effective\nframework for envisioning more ethical technology, yet previous work on virtue\nethics and technology design has remained at theoretical recommendations.\nTherefore, we propose an approach for identifying user experience design\npatterns that embody particular virtues to more concretely articulate virtuous\ntechnology designs. As a proof of concept for our approach, we documented seven\ndesign patterns for social media that uphold the virtues of Catholic Social\nTeaching. We interviewed 24 technology researchers and industry practitioners\nto evaluate these patterns. We found that overall the patterns enact the\nvirtues they were identified to embody; our participants valued that the\npatterns fostered intentional conversations and personal connections. We pave a\npath for technology professionals to incorporate diverse virtue traditions into\nthe development of technologies that support human flourishing.",
    "published_date": "2025-01-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.10288v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.10153v1",
    "title": "Region-wise stacking ensembles for estimating brain-age using MRI",
    "authors": [
      "Georgios Antonopoulos",
      "Shammi More",
      "Simon B. Eickhoff",
      "Federico Raimondo",
      "Kaustubh R. Patil"
    ],
    "author_ids": [],
    "abstract": "Predictive modeling using structural magnetic resonance imaging (MRI) data is\na prominent approach to study brain-aging. Machine learning algorithms and\nfeature extraction methods have been employed to improve predictions and\nexplore healthy and accelerated aging e.g. neurodegenerative and psychiatric\ndisorders. The high-dimensional MRI data pose challenges to building\ngeneralizable and interpretable models as well as for data privacy. Common\npractices are resampling or averaging voxels within predefined parcels, which\nreduces anatomical specificity and biological interpretability as voxels within\na region may differently relate to aging. Effectively, naive fusion by\naveraging can result in information loss and reduced accuracy. We present a\nconceptually novel two-level stacking ensemble (SE) approach. The first level\ncomprises regional models for predicting individuals' age based on voxel-wise\ninformation, fused by a second-level model yielding final predictions. Eight\ndata fusion scenarios were explored using as input Gray matter volume (GMV)\nestimates from four datasets covering the adult lifespan. Performance, measured\nusing mean absolute error (MAE), R2, correlation and prediction bias, showed\nthat SE outperformed the region-wise averages. The best performance was\nobtained when first-level regional predictions were obtained as out-of-sample\npredictions on the application site with second-level models trained on\nindependent and site-specific data (MAE=4.75 vs baseline regional mean GMV\nMAE=5.68). Performance improved as more datasets were used for training.\nFirst-level predictions showed improved and more robust aging signal providing\nnew biological insights and enhanced data privacy. Overall, the SE improves\naccuracy compared to the baseline while preserving or enhancing data privacy.",
    "published_date": "2025-01-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.10153v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.10150v2",
    "title": "Dual Debiasing: Remove Stereotypes and Keep Factual Gender for Fair Language Modeling and Translation",
    "authors": [
      "Tomasz Limisiewicz",
      "David Mareček",
      "Tomáš Musil"
    ],
    "author_ids": [],
    "abstract": "Mitigation of biases, such as language models' reliance on gender\nstereotypes, is a crucial endeavor required for the creation of reliable and\nuseful language technology. The crucial aspect of debiasing is to ensure that\nthe models preserve their versatile capabilities, including their ability to\nsolve language tasks and equitably represent various genders. To address this\nissue, we introduce a streamlined Dual Dabiasing Algorithm through Model\nAdaptation (2DAMA). Novel Dual Debiasing enables robust reduction of\nstereotypical bias while preserving desired factual gender information encoded\nby language models. We show that 2DAMA effectively reduces gender bias in\nEnglish and is one of the first approaches facilitating the mitigation of\nstereotypical tendencies in translation. The proposed method's key advantage is\nthe preservation of factual gender cues, which are useful in a wide range of\nnatural language processing tasks.",
    "published_date": "2025-01-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.10150v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.10124v1",
    "title": "Gene Regulatory Network Inference in the Presence of Selection Bias and Latent Confounders",
    "authors": [
      "Gongxu Luo",
      "Haoyue Dai",
      "Boyang Sun",
      "Loka Li",
      "Biwei Huang",
      "Petar Stojanov",
      "Kun Zhang"
    ],
    "author_ids": [],
    "abstract": "Gene Regulatory Network Inference (GRNI) aims to identify causal\nrelationships among genes using gene expression data, providing insights into\nregulatory mechanisms. A significant yet often overlooked challenge is\nselection bias, a process where only cells meeting specific criteria, such as\ngene expression thresholds, survive or are observed, distorting the true joint\ndistribution of genes and thus biasing GRNI results. Furthermore, gene\nexpression is influenced by latent confounders, such as non-coding RNAs, which\nadd complexity to GRNI. To address these challenges, we propose GISL (Gene\nRegulatory Network Inference in the presence of Selection bias and Latent\nconfounders), a novel algorithm to infer true regulatory relationships in the\npresence of selection and confounding issues. Leveraging data obtained via\nmultiple gene perturbation experiments, we show that the true regulatory\nrelationships, as well as selection processes and latent confounders can be\npartially identified without strong parametric models and under mild graphical\nassumptions. Experimental results on both synthetic and real-world single-cell\ngene expression datasets demonstrate the superiority of GISL over existing\nmethods.",
    "published_date": "2025-01-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.10124v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2504.13840v1",
    "title": "Experimentation in Gaming: an Adoption Guide",
    "authors": [
      "Julian Runge"
    ],
    "author_ids": [],
    "abstract": "Experimentation is a cornerstone of successful game development and live\noperations, enabling teams to optimize player engagement, retention, and\nmonetization. This article provides a comprehensive guide to implementing\nexperimentation in gaming, structured around the game development lifecycle and\nthe marketing mix. From pre-launch concept testing and prototyping to\npost-launch personalization and LiveOps, experimentation plays a pivotal role\nin driving innovation and adapting game experiences to diverse player\npreferences. Gaming presents unique challenges, such as highly engaged\ncommunities, complex interactive systems, and highly heterogeneous and evolving\nplayer behaviors, which require tailored approaches to experimentation. The\narticle emphasizes the importance of collaborative frameworks across product,\nmarketing, and analytics teams and provides practical guidance to game makers\nhow to adopt experimentation successfully. It also addresses ethical\nconsiderations like fairness and player autonomy.",
    "published_date": "2025-01-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2504.13840v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.10066v1",
    "title": "A Comprehensive Insights into Drones: History, Classification, Architecture, Navigation, Applications, Challenges, and Future Trends",
    "authors": [
      "Ruchita Singh",
      "Sandeep Kumar"
    ],
    "author_ids": [],
    "abstract": "Unmanned Aerial Vehicles (UAVs), commonly known as Drones, are one of 21st\ncentury most transformative technologies. Emerging first for military use,\nadvancements in materials, electronics, and software have catapulted drones\ninto multipurpose tools for a wide range of industries. In this paper, we have\ncovered the history, taxonomy, architecture, navigation systems and branched\nactivities for the same. It explores important future trends like autonomous\nnavigation, AI integration, and obstacle avoidance systems, emphasizing how\nthey contribute to improving the efficiency and versatility of drones. It also\nlooks at the major challenges like technical, environmental, economic,\nregulatory and ethical, that limit the actual take-up of drones, as well as\ntrends that are likely to mitigate these obstacles in the future. This work\noffers a structured synthesis of existing studies and perspectives that enable\ninsights about how drones will transform agriculture, logistics, healthcare,\ndisaster management, and other areas, while also identifying new opportunities\nfor innovation and development.",
    "published_date": "2025-01-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.RO",
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.10066v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.10017v1",
    "title": "Enhancing Crash Frequency Modeling Based on Augmented Multi-Type Data by Hybrid VAE-Diffusion-Based Generative Neural Networks",
    "authors": [
      "Junlan Chen",
      "Qijie He",
      "Pei Liu",
      "Wei Ma",
      "Ziyuan Pu"
    ],
    "author_ids": [],
    "abstract": "Crash frequency modelling analyzes the impact of factors like traffic volume,\nroad geometry, and environmental conditions on crash occurrences. Inaccurate\npredictions can distort our understanding of these factors, leading to\nmisguided policies and wasted resources, which jeopardize traffic safety. A key\nchallenge in crash frequency modelling is the prevalence of excessive zero\nobservations, caused by underreporting, the low probability of crashes, and\nhigh data collection costs. These zero observations often reduce model accuracy\nand introduce bias, complicating safety decision making. While existing\napproaches, such as statistical methods, data aggregation, and resampling,\nattempt to address this issue, they either rely on restrictive assumptions or\nresult in significant information loss, distorting crash data. To overcome\nthese limitations, we propose a hybrid VAE-Diffusion neural network, designed\nto reduce zero observations and handle the complexities of multi-type tabular\ncrash data (count, ordinal, nominal, and real-valued variables). We assess the\nsynthetic data quality generated by this model through metrics like similarity,\naccuracy, diversity, and structural consistency, and compare its predictive\nperformance against traditional statistical models. Our findings demonstrate\nthat the hybrid VAE-Diffusion model outperforms baseline models across all\nmetrics, offering a more effective approach to augmenting crash data and\nimproving the accuracy of crash frequency predictions. This study highlights\nthe potential of synthetic data to enhance traffic safety by improving crash\nfrequency modelling and informing better policy decisions.",
    "published_date": "2025-01-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.DB"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.10017v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.09987v1",
    "title": "On understanding and overcoming spectral biases of deep neural network learning methods for solving PDEs",
    "authors": [
      "Zhi-Qin John Xu",
      "Lulu Zhang",
      "Wei Cai"
    ],
    "author_ids": [],
    "abstract": "In this review, we survey the latest approaches and techniques developed to\novercome the spectral bias towards low frequency of deep neural network\nlearning methods in learning multiple-frequency solutions of partial\ndifferential equations. Open problems and future research directions are also\ndiscussed.",
    "published_date": "2025-01-17T00:00:00",
    "year": 2025,
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.09987v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.10484v1",
    "title": "Bias in Decision-Making for AI's Ethical Dilemmas: A Comparative Study of ChatGPT and Claude",
    "authors": [
      "Yile Yan",
      "Yuqi Zhu",
      "Wentao Xu"
    ],
    "author_ids": [],
    "abstract": "Recent advances in Large Language Models (LLMs) have enabled human-like\nresponses across various tasks, raising questions about their ethical\ndecision-making capabilities and potential biases. This study investigates\nprotected attributes in LLMs through systematic evaluation of their responses\nto ethical dilemmas. Using two prominent models - GPT-3.5 Turbo and Claude 3.5\nSonnet - we analyzed their decision-making patterns across multiple protected\nattributes including age, gender, race, appearance, and disability status.\nThrough 11,200 experimental trials involving both single-factor and two-factor\nprotected attribute combinations, we evaluated the models' ethical preferences,\nsensitivity, stability, and clustering of preferences. Our findings reveal\nsignificant protected attributeses in both models, with consistent preferences\nfor certain features (e.g., \"good-looking\") and systematic neglect of others.\nNotably, while GPT-3.5 Turbo showed stronger preferences aligned with\ntraditional power structures, Claude 3.5 Sonnet demonstrated more diverse\nprotected attribute choices. We also found that ethical sensitivity\nsignificantly decreases in more complex scenarios involving multiple protected\nattributes. Additionally, linguistic referents heavily influence the models'\nethical evaluations, as demonstrated by differing responses to racial\ndescriptors (e.g., \"Yellow\" versus \"Asian\"). These findings highlight critical\nconcerns about the potential impact of LLM biases in autonomous decision-making\nsystems and emphasize the need for careful consideration of protected\nattributes in AI development. Our study contributes to the growing body of\nresearch on AI ethics by providing a systematic framework for evaluating\nprotected attributes in LLMs' ethical decision-making capabilities.",
    "published_date": "2025-01-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.10484v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.09913v1",
    "title": "Towards A Litmus Test for Common Sense",
    "authors": [
      "Hugo Latapie"
    ],
    "author_ids": [],
    "abstract": "This paper is the second in a planned series aimed at envisioning a path to\nsafe and beneficial artificial intelligence. Building on the conceptual\ninsights of \"Common Sense Is All You Need,\" we propose a more formal litmus\ntest for common sense, adopting an axiomatic approach that combines minimal\nprior knowledge (MPK) constraints with diagonal or Godel-style arguments to\ncreate tasks beyond the agent's known concept set. We discuss how this approach\napplies to the Abstraction and Reasoning Corpus (ARC), acknowledging\ntraining/test data constraints, physical or virtual embodiment, and large\nlanguage models (LLMs). We also integrate observations regarding emergent\ndeceptive hallucinations, in which more capable AI systems may intentionally\nfabricate plausible yet misleading outputs to disguise knowledge gaps. The\noverarching theme is that scaling AI without ensuring common sense risks\nintensifying such deceptive tendencies, thereby undermining safety and trust.\nAligning with the broader goal of developing beneficial AI without causing\nharm, our axiomatic litmus test not only diagnoses whether an AI can handle\ntruly novel concepts but also provides a stepping stone toward an ethical,\nreliable foundation for future safe, beneficial, and aligned artificial\nintelligence.",
    "published_date": "2025-01-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.09913v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.09897v1",
    "title": "Decoding Patterns of Data Generation Teams for Clinical and Scientific Success: Insights from the Bridge2AI Talent Knowledge Graph",
    "authors": [
      "Jiawei Xu",
      "Qingnan Xie",
      "Meijun Liu",
      "Zhandos Sembay",
      "Swathi Thaker",
      "Pamela Payne-Foster",
      "Jake Chen",
      "Ying Ding"
    ],
    "author_ids": [],
    "abstract": "High-quality biomedical datasets are essential for medical research and\ndisease treatment innovation. The NIH-funded Bridge2AI project strives to\nfacilitate such innovations by uniting top-tier, diverse teams to curate\ndatasets designed for AI-driven biomedical research. We examined 1,699 dataset\npapers from the Nucleic Acids Research (NAR) database issues and the Bridge2AI\nTalent Knowledge Graph. By treating each paper's authors as a team, we explored\nthe relationship between team attributes (team power and fairness) and dataset\npaper quality, measured by scientific impact (Relative Citation Ratio\npercentile) and clinical translation power (APT, likelihood of citation by\nclinical trials and guidelines). Utilizing the SHAP explainable AI framework,\nwe identified correlations between team attributes and the success of dataset\npapers in both citation impact and clinical translation. Key findings reveal\nthat (1) PI (Principal Investigator) leadership and team academic prowess are\nstrong predictors of dataset success; (2) team size and career age are\npositively correlated with scientific impact but show inverse patterns for\nclinical translation; and (3) higher female representation correlates with\ngreater dataset success. Although our results are correlational, they offer\nvaluable insights into forming high-performing data generation teams. Future\nresearch should incorporate causal frameworks to deepen understanding of these\nrelationships.",
    "published_date": "2025-01-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.DL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.09897v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.09890v1",
    "title": "Exploring the Implementation of AI in Early Onset Interviews to Help Mitigate Bias",
    "authors": [
      "Nishka Lal",
      "Omar Benkraouda"
    ],
    "author_ids": [],
    "abstract": "This paper investigates the application of artificial intelligence (AI) in\nearly-stage recruitment interviews in order to reduce inherent bias,\nspecifically sentiment bias. Traditional interviewers are often subject to\nseveral biases, including interviewer bias, social desirability effects, and\neven confirmation bias. In turn, this leads to non-inclusive hiring practices,\nand a less diverse workforce. This study further analyzes various AI\ninterventions that are present in the marketplace today such as multimodal\nplatforms and interactive candidate assessment tools in order to gauge the\ncurrent market usage of AI in early-stage recruitment. However, this paper aims\nto use a unique AI system that was developed to transcribe and analyze\ninterview dynamics, which emphasize skill and knowledge over emotional\nsentiments. Results indicate that AI effectively minimizes sentiment-driven\nbiases by 41.2%, suggesting its revolutionizing power in companies' recruitment\nprocesses for improved equity and efficiency.",
    "published_date": "2025-01-17T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.09890v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.09687v1",
    "title": "U-Fair: Uncertainty-based Multimodal Multitask Learning for Fairer Depression Detection",
    "authors": [
      "Jiaee Cheong",
      "Aditya Bangar",
      "Sinan Kalkan",
      "Hatice Gunes"
    ],
    "author_ids": [],
    "abstract": "Machine learning bias in mental health is becoming an increasingly pertinent\nchallenge. Despite promising efforts indicating that multitask approaches often\nwork better than unitask approaches, there is minimal work investigating the\nimpact of multitask learning on performance and fairness in depression\ndetection nor leveraged it to achieve fairer prediction outcomes. In this work,\nwe undertake a systematic investigation of using a multitask approach to\nimprove performance and fairness for depression detection. We propose a novel\ngender-based task-reweighting method using uncertainty grounded in how the\nPHQ-8 questionnaire is structured. Our results indicate that, although a\nmultitask approach improves performance and fairness compared to a unitask\napproach, the results are not always consistent and we see evidence of negative\ntransfer and a reduction in the Pareto frontier, which is concerning given the\nhigh-stake healthcare setting. Our proposed approach of gender-based\nreweighting with uncertainty improves performance and fairness and alleviates\nboth challenges to a certain extent. Our findings on each PHQ-8 subitem task\ndifficulty are also in agreement with the largest study conducted on the PHQ-8\nsubitem discrimination capacity, thus providing the very first tangible\nevidence linking ML findings with large-scale empirical population studies\nconducted on the PHQ-8.",
    "published_date": "2025-01-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.09687v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.09628v2",
    "title": "Artificial Intelligence-Driven Clinical Decision Support Systems",
    "authors": [
      "Muhammet Alkan",
      "Idris Zakariyya",
      "Samuel Leighton",
      "Kaushik Bhargav Sivangi",
      "Christos Anagnostopoulos",
      "Fani Deligianni"
    ],
    "author_ids": [],
    "abstract": "As artificial intelligence (AI) becomes increasingly embedded in healthcare\ndelivery, this chapter explores the critical aspects of developing reliable and\nethical Clinical Decision Support Systems (CDSS). Beginning with the\nfundamental transition from traditional statistical models to sophisticated\nmachine learning approaches, this work examines rigorous validation strategies\nand performance assessment methods, including the crucial role of model\ncalibration and decision curve analysis. The chapter emphasizes that creating\ntrustworthy AI systems in healthcare requires more than just technical\naccuracy; it demands careful consideration of fairness, explainability, and\nprivacy. The challenge of ensuring equitable healthcare delivery through AI is\nstressed, discussing methods to identify and mitigate bias in clinical\npredictive models. The chapter then delves into explainability as a cornerstone\nof human-centered CDSS. This focus reflects the understanding that healthcare\nprofessionals must not only trust AI recommendations but also comprehend their\nunderlying reasoning. The discussion advances in an analysis of privacy\nvulnerabilities in medical AI systems, from data leakage in deep learning\nmodels to sophisticated attacks against model explanations. The text explores\nprivacy-preservation strategies such as differential privacy and federated\nlearning, while acknowledging the inherent trade-offs between privacy\nprotection and model performance. This progression, from technical validation\nto ethical considerations, reflects the multifaceted challenges of developing\nAI systems that can be seamlessly and reliably integrated into daily clinical\npractice while maintaining the highest standards of patient care and data\nprotection.",
    "published_date": "2025-01-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.09628v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.09620v1",
    "title": "Beyond Reward Hacking: Causal Rewards for Large Language Model Alignment",
    "authors": [
      "Chaoqi Wang",
      "Zhuokai Zhao",
      "Yibo Jiang",
      "Zhaorun Chen",
      "Chen Zhu",
      "Yuxin Chen",
      "Jiayi Liu",
      "Lizhu Zhang",
      "Xiangjun Fan",
      "Hao Ma",
      "Sinong Wang"
    ],
    "author_ids": [],
    "abstract": "Recent advances in large language models (LLMs) have demonstrated significant\nprogress in performing complex tasks. While Reinforcement Learning from Human\nFeedback (RLHF) has been effective in aligning LLMs with human preferences, it\nis susceptible to spurious correlations in reward modeling. Consequently, it\noften introduces biases-such as length bias, sycophancy, conceptual bias, and\ndiscrimination that hinder the model's ability to capture true causal\nrelationships. To address this, we propose a novel causal reward modeling\napproach that integrates causal inference to mitigate these spurious\ncorrelations. Our method enforces counterfactual invariance, ensuring reward\npredictions remain consistent when irrelevant variables are altered. Through\nexperiments on both synthetic and real-world datasets, we show that our\napproach mitigates various types of spurious correlations effectively,\nresulting in more reliable and fair alignment of LLMs with human preferences.\nAs a drop-in enhancement to the existing RLHF workflow, our causal reward\nmodeling provides a practical way to improve the trustworthiness and fairness\nof LLM finetuning.",
    "published_date": "2025-01-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.09620v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.09606v1",
    "title": "Local US officials' views on the impacts and governance of AI: Evidence from 2022 and 2023 survey waves",
    "authors": [
      "Sophia Hatz",
      "Noemi Dreksler",
      "Kevin Wei",
      "Baobao Zhang"
    ],
    "author_ids": [],
    "abstract": "This paper presents a survey of local US policymakers' views on the future\nimpact and regulation of AI. Our survey provides insight into US policymakers'\nexpectations regarding the effects of AI on local communities and the nation,\nas well as their attitudes towards specific regulatory policies. Conducted in\ntwo waves (2022 and 2023), the survey captures changes in attitudes following\nthe release of ChatGPT and the subsequent surge in public awareness of AI.\nLocal policymakers express a mix of concern, optimism, and uncertainty about\nAI's impacts, anticipating significant societal risks such as increased\nsurveillance, misinformation, and political polarization, alongside potential\nbenefits in innovation and infrastructure. Many also report feeling\nunderprepared and inadequately informed to make AI-related decisions. On\nregulation, a majority of policymakers support government oversight and favor\nspecific policies addressing issues such as data privacy, AI-related\nunemployment, and AI safety and fairness. Democrats show stronger and more\nconsistent support for regulation than Republicans, but the latter experienced\na notable shift towards majority support between 2022 and 2023. Our study\ncontributes to understanding the perspectives of local policymakers-key players\nin shaping state and federal AI legislation-by capturing evolving attitudes,\npartisan dynamics, and their implications for policy formation. The findings\nhighlight the need for capacity-building initiatives and bi-partisan\ncoordination to mitigate policy fragmentation and build a cohesive framework\nfor AI governance in the US.",
    "published_date": "2025-01-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.09606v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.09561v1",
    "title": "Stylomech: Unveiling Authorship via Computational Stylometry in English and Romanized Sinhala",
    "authors": [
      "Nabeelah Faumi",
      "Adeepa Gunathilake",
      "Benura Wickramanayake",
      "Deelaka Dias",
      "TGDK Sumanathilaka"
    ],
    "author_ids": [],
    "abstract": "With the advent of Web 2.0, the development in social technology coupled with\nglobal communication systematically brought positive and negative impacts to\nsociety. Copyright claims and Author identification are deemed crucial as there\nhas been a considerable amount of increase in content violation owing to the\nlack of proper ethics in society. The Author's attribution in both English and\nRomanized Sinhala became a major requirement in the last few decades. As an\narea largely unexplored, particularly within the context of Romanized Sinhala,\nthe research contributes significantly to the field of computational\nlinguistics. The proposed author attribution system offers a unique approach,\nallowing for the comparison of only two sets of text: suspect author and\nanonymous text, a departure from traditional methodologies which often rely on\nlarger corpora. This work focuses on using the numerical representation of\nvarious pairs of the same and different authors allowing for, the model to\ntrain on these representations as opposed to text, this allows for it to apply\nto a multitude of authors and contexts, given that the suspected author text,\nand the anonymous text are of reasonable quality. By expanding the scope of\nauthorship attribution to encompass diverse linguistic contexts, the work\ncontributes to fostering trust and accountability in digital communication,\nespecially in Sri Lanka. This research presents a pioneering approach to author\nattribution in both English and Romanized Sinhala, addressing a critical need\nfor content verification and intellectual property rights enforcement in the\ndigital age.",
    "published_date": "2025-01-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.09561v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.09534v1",
    "title": "AI in Support of Diversity and Inclusion",
    "authors": [
      "Çiçek Güven",
      "Afra Alishahi",
      "Henry Brighton",
      "Gonzalo Nápoles",
      "Juan Sebastian Olier",
      "Marie Šafář",
      "Eric Postma",
      "Dimitar Shterionov",
      "Mirella De Sisto",
      "Eva Vanmassenhove"
    ],
    "author_ids": [],
    "abstract": "In this paper, we elaborate on how AI can support diversity and inclusion and\nexemplify research projects conducted in that direction. We start by looking at\nthe challenges and progress in making large language models (LLMs) more\ntransparent, inclusive, and aware of social biases. Even though LLMs like\nChatGPT have impressive abilities, they struggle to understand different\ncultural contexts and engage in meaningful, human like conversations. A key\nissue is that biases in language processing, especially in machine translation,\ncan reinforce inequality. Tackling these biases requires a multidisciplinary\napproach to ensure AI promotes diversity, fairness, and inclusion. We also\nhighlight AI's role in identifying biased content in media, which is important\nfor improving representation. By detecting unequal portrayals of social groups,\nAI can help challenge stereotypes and create more inclusive technologies.\nTransparent AI algorithms, which clearly explain their decisions, are essential\nfor building trust and reducing bias in AI systems. We also stress AI systems\nneed diverse and inclusive training data. Projects like the Child Growth\nMonitor show how using a wide range of data can help address real world\nproblems like malnutrition and poverty. We present a project that demonstrates\nhow AI can be applied to monitor the role of search engines in spreading\ndisinformation about the LGBTQ+ community. Moreover, we discuss the SignON\nproject as an example of how technology can bridge communication gaps between\nhearing and deaf people, emphasizing the importance of collaboration and mutual\ntrust in developing inclusive AI. Overall, with this paper, we advocate for AI\nsystems that are not only effective but also socially responsible, promoting\nfair and inclusive interactions between humans and machines.",
    "published_date": "2025-01-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.09534v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.09525v2",
    "title": "Class Incremental Fault Diagnosis under Limited Fault Data via Supervised Contrastive Knowledge Distillation",
    "authors": [
      "Hanrong Zhang",
      "Yifei Yao",
      "Zixuan Wang",
      "Jiayuan Su",
      "Mengxuan Li",
      "Peng Peng",
      "Hongwei Wang"
    ],
    "author_ids": [],
    "abstract": "Class-incremental fault diagnosis requires a model to adapt to new fault\nclasses while retaining previous knowledge. However, limited research exists\nfor imbalanced and long-tailed data. Extracting discriminative features from\nfew-shot fault data is challenging, and adding new fault classes often demands\ncostly model retraining. Moreover, incremental training of existing methods\nrisks catastrophic forgetting, and severe class imbalance can bias the model's\ndecisions toward normal classes. To tackle these issues, we introduce a\nSupervised Contrastive knowledge distiLlation for class Incremental Fault\nDiagnosis (SCLIFD) framework proposing supervised contrastive knowledge\ndistillation for improved representation learning capability and less\nforgetting, a novel prioritized exemplar selection method for sample replay to\nalleviate catastrophic forgetting, and the Random Forest Classifier to address\nthe class imbalance. Extensive experimentation on simulated and real-world\nindustrial datasets across various imbalance ratios demonstrates the\nsuperiority of SCLIFD over existing approaches. Our code can be found at\nhttps://github.com/Zhang-Henry/SCLIFD_TII.",
    "published_date": "2025-01-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.09525v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.09482v1",
    "title": "Building Bridges across Papua New Guinea's Digital Divide in Growing the ICT Industry",
    "authors": [
      "Marc Cheong",
      "Sankwi Abuzo",
      "Hideaki Hata",
      "Priscilla Kevin",
      "Winifred Kula",
      "Benson Mirou",
      "Christoph Treude",
      "Dong Wang",
      "Raula Gaikovina Kula"
    ],
    "author_ids": [],
    "abstract": "Papua New Guinea (PNG) is an emerging tech society with an opportunity to\novercome geographic and social boundaries, in order to engage with the global\nmarket. However, the current tech landscape, dominated by Big Tech in Silicon\nValley and other multinational companies in the Global North, tends to overlook\nthe requirements of emerging economies such as PNG. This is becoming more\nobvious as issues such as algorithmic bias (in tech product deployments) and\nthe digital divide (as in the case of non-affordable commercial software) are\naffecting PNG users. The Open Source Software (OSS) movement, based on extant\nresearch, is seen as a way to level the playing field in the digitalization and\nadoption of Information and Communications Technologies (ICTs) in PNG. This\nperspectives paper documents the outcome of the second International Workshop\non BRIdging the Divides with Globally Engineered Software} (BRIDGES2023) in the\nhopes of proposing ideas for future research into ICT education, uplifting\nsoftware engineering (SE) capability, and OSS adoption in promoting a more\nequitable digital future for PNG.",
    "published_date": "2025-01-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.09482v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.09409v2",
    "title": "mGeNTE: A Multilingual Resource for Gender-Neutral Language and Translation",
    "authors": [
      "Beatrice Savoldi",
      "Eleonora Cupin",
      "Manjinder Thind",
      "Anne Lauscher",
      "Andrea Piergentili",
      "Matteo Negri",
      "Luisa Bentivogli"
    ],
    "author_ids": [],
    "abstract": "Gender-neutral language reflects societal and linguistic shifts towards\ngreater inclusivity by avoiding the implication that one gender is the norm\nover others. This is particularly relevant for grammatical gender languages,\nwhich heavily encode the gender of terms for human referents and over-relies on\nmasculine forms, even when gender is unspecified or irrelevant. Language\ntechnologies are known to mirror these inequalities, being affected by a male\nbias and perpetuating stereotypical associations when translating into\nlanguages with extensive gendered morphology. In such cases, gender-neutral\nlanguage can help avoid undue binary assumptions. However, despite its\nimportance for creating fairer multi- and cross-lingual technologies, inclusive\nlanguage research remains scarce and insufficiently supported in current\nresources. To address this gap, we present the multilingual mGeNTe dataset.\nDerived from the bilingual GeNTE (Piergentili et al., 2023), mGeNTE extends the\noriginal corpus to include the English-Italian/German/Spanish language pairs.\nSince each language pair is English-aligned with gendered and neutral sentences\nin the target languages, mGeNTE enables research in both automatic\nGender-Neutral Translation (GNT) and language modelling for three grammatical\ngender languages.",
    "published_date": "2025-01-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.09409v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.09309v1",
    "title": "Understanding Mental Health Content on Social Media and Its Effect Towards Suicidal Ideation",
    "authors": [
      "Mohaiminul Islam Bhuiyan",
      "Nur Shazwani Kamarudin",
      "Nur Hafieza Ismail"
    ],
    "author_ids": [],
    "abstract": "This review underscores the critical need for effective strategies to\nidentify and support individuals with suicidal ideation, exploiting\ntechnological innovations in ML and DL to further suicide prevention efforts.\nThe study details the application of these technologies in analyzing vast\namounts of unstructured social media data to detect linguistic patterns,\nkeywords, phrases, tones, and contextual cues associated with suicidal\nthoughts. It explores various ML and DL models like SVMs, CNNs, LSTM, neural\nnetworks, and their effectiveness in interpreting complex data patterns and\nemotional nuances within text data. The review discusses the potential of these\ntechnologies to serve as a life-saving tool by identifying at-risk individuals\nthrough their digital traces. Furthermore, it evaluates the real-world\neffectiveness, limitations, and ethical considerations of employing these\ntechnologies for suicide prevention, stressing the importance of responsible\ndevelopment and usage. The study aims to fill critical knowledge gaps by\nanalyzing recent studies, methodologies, tools, and techniques in this field.\nIt highlights the importance of synthesizing current literature to inform\npractical tools and suicide prevention efforts, guiding innovation in reliable,\nethical systems for early intervention. This research synthesis evaluates the\nintersection of technology and mental health, advocating for the ethical and\nresponsible application of ML, DL, and NLP to offer life-saving potential\nworldwide while addressing challenges like generalizability, biases, privacy,\nand the need for further research to ensure these technologies do not\nexacerbate existing inequities and harms.",
    "published_date": "2025-01-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.09309v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.09277v1",
    "title": "Bias for Action: Video Implicit Neural Representations with Bias Modulation",
    "authors": [
      "Alper Kayabasi",
      "Anil Kumar Vadathya",
      "Guha Balakrishnan",
      "Vishwanath Saragadam"
    ],
    "author_ids": [],
    "abstract": "We propose a new continuous video modeling framework based on implicit neural\nrepresentations (INRs) called ActINR. At the core of our approach is the\nobservation that INRs can be considered as a learnable dictionary, with the\nshapes of the basis functions governed by the weights of the INR, and their\nlocations governed by the biases. Given compact non-linear activation\nfunctions, we hypothesize that an INR's biases are suitable to capture motion\nacross images, and facilitate compact representations for video sequences.\nUsing these observations, we design ActINR to share INR weights across frames\nof a video sequence, while using unique biases for each frame. We further model\nthe biases as the output of a separate INR conditioned on time index to promote\nsmoothness. By training the video INR and this bias INR together, we\ndemonstrate unique capabilities, including $10\\times$ video slow motion,\n$4\\times$ spatial super resolution along with $2\\times$ slow motion, denoising,\nand video inpainting. ActINR performs remarkably well across numerous video\nprocessing tasks (often achieving more than 6dB improvement), setting a new\nstandard for continuous modeling of videos.",
    "published_date": "2025-01-16T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.09277v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.08650v3",
    "title": "Who is Responsible? The Data, Models, Users or Regulations? Responsible Generative AI for a Sustainable Future",
    "authors": [
      "Shaina Raza",
      "Rizwan Qureshi",
      "Anam Zahid",
      "Joseph Fioresi",
      "Ferhat Sadak",
      "Muhammad Saeed",
      "Ranjan Sapkota",
      "Aditya Jain",
      "Anas Zafar",
      "Muneeb Ul Hassan",
      "Aizan Zafar",
      "Hasan Maqbool",
      "Ashmal Vayani",
      "Jia Wu",
      "Maged Shoman"
    ],
    "author_ids": [],
    "abstract": "Responsible Artificial Intelligence (RAI) has emerged as a crucial framework\nfor addressing ethical concerns in the development and deployment of Artificial\nIntelligence (AI) systems. A significant body of literature exists, primarily\nfocusing on either RAI guidelines and principles or the technical aspects of\nRAI, largely within the realm of traditional AI. However, a notable gap\npersists in bridging theoretical frameworks with practical implementations in\nreal-world settings, as well as transitioning from RAI to Responsible\nGenerative AI (Gen AI). To bridge this gap, we present this article, which\nexamines the challenges and opportunities in implementing ethical, transparent,\nand accountable AI systems in the post-ChatGPT era, an era significantly shaped\nby Gen AI. Our analysis includes governance and technical frameworks, the\nexploration of explainable AI as the backbone to achieve RAI, key performance\nindicators in RAI, alignment of Gen AI benchmarks with governance frameworks,\nreviews of AI-ready test beds, and RAI applications across multiple sectors.\nAdditionally, we discuss challenges in RAI implementation and provide a\nphilosophical perspective on the future of RAI. This comprehensive article aims\nto offer an overview of RAI, providing valuable insights for researchers,\npolicymakers, users, and industry practitioners to develop and deploy AI\nsystems that benefit individuals and society while minimizing potential risks\nand societal impacts. A curated list of resources and datasets covered in this\nsurvey is available on GitHub {https://github.com/anas-zafar/Responsible-AI}.",
    "published_date": "2025-01-15T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.08650v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.09112v1",
    "title": "Mantis Shrimp: Exploring Photometric Band Utilization in Computer Vision Networks for Photometric Redshift Estimation",
    "authors": [
      "Andrew Engel",
      "Nell Byler",
      "Adam Tsou",
      "Gautham Narayan",
      "Emmanuel Bonilla",
      "Ian Smith"
    ],
    "author_ids": [],
    "abstract": "We present Mantis Shrimp, a multi-survey deep learning model for photometric\nredshift estimation that fuses ultra-violet (GALEX), optical (PanSTARRS), and\ninfrared (UnWISE) imagery. Machine learning is now an established approach for\nphotometric redshift estimation, with generally acknowledged higher performance\nin areas with a high density of spectroscopically identified galaxies over\ntemplate-based methods. Multiple works have shown that image-based\nconvolutional neural networks can outperform tabular-based color/magnitude\nmodels. In comparison to tabular models, image models have additional design\ncomplexities: it is largely unknown how to fuse inputs from different\ninstruments which have different resolutions or noise properties. The Mantis\nShrimp model estimates the conditional density estimate of redshift using\ncutout images. The density estimates are well calibrated and the point\nestimates perform well in the distribution of available spectroscopically\nconfirmed galaxies with (bias = 1e-2), scatter (NMAD = 2.44e-2) and\ncatastrophic outlier rate ($\\eta$=17.53$\\%$). We find that early fusion\napproaches (e.g., resampling and stacking images from different instruments)\nmatch the performance of late fusion approaches (e.g., concatenating latent\nspace representations), so that the design choice ultimately is left to the\nuser. Finally, we study how the models learn to use information across bands,\nfinding evidence that our models successfully incorporates information from all\nsurveys. The applicability of our model to the analysis of large populations of\ngalaxies is limited by the speed of downloading cutouts from external servers;\nhowever, our model could be useful in smaller studies such as generating priors\nover redshift for stellar population synthesis.",
    "published_date": "2025-01-15T00:00:00",
    "year": 2025,
    "categories": [
      "astro-ph.IM",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.09112v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.09014v1",
    "title": "How Do Generative Models Draw a Software Engineer? A Case Study on Stable Diffusion Bias",
    "authors": [
      "Tosin Fadahunsi",
      "Giordano d'Aloisio",
      "Antinisca Di Marco",
      "Federica Sarro"
    ],
    "author_ids": [],
    "abstract": "Generative models are nowadays widely used to generate graphical content used\nfor multiple purposes, e.g. web, art, advertisement. However, it has been shown\nthat the images generated by these models could reinforce societal biases\nalready existing in specific contexts. In this paper, we focus on understanding\nif this is the case when one generates images related to various software\nengineering tasks. In fact, the Software Engineering (SE) community is not\nimmune from gender and ethnicity disparities, which could be amplified by the\nuse of these models. Hence, if used without consciousness, artificially\ngenerated images could reinforce these biases in the SE domain. Specifically,\nwe perform an extensive empirical evaluation of the gender and ethnicity bias\nexposed by three versions of the Stable Diffusion (SD) model (a very popular\nopen-source text-to-image model) - SD 2, SD XL, and SD 3 - towards SE tasks. We\nobtain 6,720 images by feeding each model with two sets of prompts describing\ndifferent software-related tasks: one set includes the Software Engineer\nkeyword, and one set does not include any specification of the person\nperforming the task. Next, we evaluate the gender and ethnicity disparities in\nthe generated images. Results show how all models are significantly biased\ntowards male figures when representing software engineers. On the contrary,\nwhile SD 2 and SD XL are strongly biased towards White figures, SD 3 is\nslightly more biased towards Asian figures. Nevertheless, all models\nsignificantly under-represent Black and Arab figures, regardless of the prompt\nstyle used. The results of our analysis highlight severe concerns about\nadopting those models to generate content for SE tasks and open the field for\nfuture research on bias mitigation in this context.",
    "published_date": "2025-01-15T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.09014v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.09008v1",
    "title": "SimGen: A Diffusion-Based Framework for Simultaneous Surgical Image and Segmentation Mask Generation",
    "authors": [
      "Aditya Bhat",
      "Rupak Bose",
      "Chinedu Innocent Nwoye",
      "Nicolas Padoy"
    ],
    "author_ids": [],
    "abstract": "Acquiring and annotating surgical data is often resource-intensive, ethical\nconstraining, and requiring significant expert involvement. While generative AI\nmodels like text-to-image can alleviate data scarcity, incorporating spatial\nannotations, such as segmentation masks, is crucial for precision-driven\nsurgical applications, simulation, and education. This study introduces both a\nnovel task and method, SimGen, for Simultaneous Image and Mask Generation.\nSimGen is a diffusion model based on the DDPM framework and Residual U-Net,\ndesigned to jointly generate high-fidelity surgical images and their\ncorresponding segmentation masks. The model leverages cross-correlation priors\nto capture dependencies between continuous image and discrete mask\ndistributions. Additionally, a Canonical Fibonacci Lattice (CFL) is employed to\nenhance class separability and uniformity in the RGB space of the masks. SimGen\ndelivers high-fidelity images and accurate segmentation masks, outperforming\nbaselines across six public datasets assessed on image and semantic inception\ndistance metrics. Ablation study shows that the CFL improves mask quality and\nspatial separation. Downstream experiments suggest generated image-mask pairs\nare usable if regulations limit human data release for research. This work\noffers a cost-effective solution for generating paired surgical images and\ncomplex labels, advancing surgical AI development by reducing the need for\nexpensive manual annotations.",
    "published_date": "2025-01-15T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.09008v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.10467v1",
    "title": "Securing the AI Frontier: Urgent Ethical and Regulatory Imperatives for AI-Driven Cybersecurity",
    "authors": [
      "Vikram Kulothungan"
    ],
    "author_ids": [],
    "abstract": "This paper critically examines the evolving ethical and regulatory challenges\nposed by the integration of artificial intelligence (AI) in cybersecurity. We\ntrace the historical development of AI regulation, highlighting major\nmilestones from theoretical discussions in the 1940s to the implementation of\nrecent global frameworks such as the European Union AI Act. The current\nregulatory landscape is analyzed, emphasizing risk-based approaches,\nsector-specific regulations, and the tension between fostering innovation and\nmitigating risks. Ethical concerns such as bias, transparency, accountability,\nprivacy, and human oversight are explored in depth, along with their\nimplications for AI-driven cybersecurity systems. Furthermore, we propose\nstrategies for promoting AI literacy and public engagement, essential for\nshaping a future regulatory framework. Our findings underscore the need for a\nunified, globally harmonized regulatory approach that addresses the unique\nrisks of AI in cybersecurity. We conclude by identifying future research\nopportunities and recommending pathways for collaboration between policymakers,\nindustry leaders, and researchers to ensure the responsible deployment of AI\ntechnologies in cybersecurity.",
    "published_date": "2025-01-15T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CY",
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.10467v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.08962v2",
    "title": "An analysis of data variation and bias in image-based dermatological datasets for machine learning classification",
    "authors": [
      "Francisco Filho",
      "Emanoel Santos",
      "Rodrigo Mota",
      "Kelvin Cunha",
      "Fabio Papais",
      "Amanda Arruda",
      "Mateus Baltazar",
      "Camila Vieira",
      "José Gabriel Tavares",
      "Rafael Barros",
      "Othon Souza",
      "Thales Bezerra",
      "Natalia Lopes",
      "Érico Moutinho",
      "Jéssica Guido",
      "Shirley Cruz",
      "Paulo Borba",
      "Tsang Ing Ren"
    ],
    "author_ids": [],
    "abstract": "AI algorithms have become valuable in aiding professionals in healthcare. The\nincreasing confidence obtained by these models is helpful in critical decision\ndemands. In clinical dermatology, classification models can detect malignant\nlesions on patients' skin using only RGB images as input. However, most\nlearning-based methods employ data acquired from dermoscopic datasets on\ntraining, which are large and validated by a gold standard. Clinical models aim\nto deal with classification on users' smartphone cameras that do not contain\nthe corresponding resolution provided by dermoscopy. Also, clinical\napplications bring new challenges. It can contain captures from uncontrolled\nenvironments, skin tone variations, viewpoint changes, noises in data and\nlabels, and unbalanced classes. A possible alternative would be to use transfer\nlearning to deal with the clinical images. However, as the number of samples is\nlow, it can cause degradations on the model's performance; the source\ndistribution used in training differs from the test set. This work aims to\nevaluate the gap between dermoscopic and clinical samples and understand how\nthe dataset variations impact training. It assesses the main differences\nbetween distributions that disturb the model's prediction. Finally, from\nexperiments on different architectures, we argue how to combine the data from\ndivergent distributions, decreasing the impact on the model's final accuracy.",
    "published_date": "2025-01-15T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.5.4; J.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.08962v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.08951v1",
    "title": "Analyzing the Ethical Logic of Six Large Language Models",
    "authors": [
      "W. Russell Neuman",
      "Chad Coleman",
      "Manan Shah"
    ],
    "author_ids": [],
    "abstract": "This study examines the ethical reasoning of six prominent generative large\nlanguage models: OpenAI GPT-4o, Meta LLaMA 3.1, Perplexity, Anthropic Claude\n3.5 Sonnet, Google Gemini, and Mistral 7B. The research explores how these\nmodels articulate and apply ethical logic, particularly in response to moral\ndilemmas such as the Trolley Problem, and Heinz Dilemma. Departing from\ntraditional alignment studies, the study adopts an explainability-transparency\nframework, prompting models to explain their ethical reasoning. This approach\nis analyzed through three established ethical typologies: the\nconsequentialist-deontological analytic, Moral Foundations Theory, and the\nKohlberg Stages of Moral Development Model. Findings reveal that LLMs exhibit\nlargely convergent ethical logic, marked by a rationalist, consequentialist\nemphasis, with decisions often prioritizing harm minimization and fairness.\nDespite similarities in pre-training and model architecture, a mixture of\nnuanced and significant differences in ethical reasoning emerge across models,\nreflecting variations in fine-tuning and post-training processes. The models\nconsistently display erudition, caution, and self-awareness, presenting ethical\nreasoning akin to a graduate-level discourse in moral philosophy. In striking\nuniformity these systems all describe their ethical reasoning as more\nsophisticated than what is characteristic of typical human moral logic.",
    "published_date": "2025-01-15T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.08951v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.08910v1",
    "title": "Lights, Camera, Matching: The Role of Image Illumination in Fair Face Recognition",
    "authors": [
      "Gabriella Pangelinan",
      "Grace Bezold",
      "Haiyu Wu",
      "Michael C. King",
      "Kevin W. Bowyer"
    ],
    "author_ids": [],
    "abstract": "Facial brightness is a key image quality factor impacting face recognition\naccuracy differentials across demographic groups. In this work, we aim to\ndecrease the accuracy gap between the similarity score distributions for\nCaucasian and African American female mated image pairs, as measured by d'\nbetween distributions. To balance brightness across demographic groups, we\nconduct three experiments, interpreting brightness in the face skin region\neither as median pixel value or as the distribution of pixel values. Balancing\nbased on median brightness alone yields up to a 46.8% decrease in d', while\nbalancing based on brightness distribution yields up to a 57.6% decrease. In\nall three cases, the similarity scores of the individual distributions improve,\nwith mean scores maximally improving 5.9% for Caucasian females and 3.7% for\nAfrican American females.",
    "published_date": "2025-01-15T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.08910v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.08769v2",
    "title": "Enhanced Large Language Models for Effective Screening of Depression and Anxiety",
    "authors": [
      "June M. Liu",
      "Mengxia Gao",
      "Sahand Sabour",
      "Zhuang Chen",
      "Minlie Huang",
      "Tatia M. C. Lee"
    ],
    "author_ids": [],
    "abstract": "Depressive and anxiety disorders are widespread, necessitating timely\nidentification and management. Recent advances in Large Language Models (LLMs)\noffer potential solutions, yet high costs and ethical concerns about training\ndata remain challenges. This paper introduces a pipeline for synthesizing\nclinical interviews, resulting in 1,157 interactive dialogues (PsyInterview),\nand presents EmoScan, an LLM-based emotional disorder screening system. EmoScan\ndistinguishes between coarse (e.g., anxiety or depressive disorders) and fine\ndisorders (e.g., major depressive disorders) and conducts high-quality\ninterviews. Evaluations showed that EmoScan exceeded the performance of base\nmodels and other LLMs like GPT-4 in screening emotional disorders\n(F1-score=0.7467). It also delivers superior explanations (BERTScore=0.9408)\nand demonstrates robust generalizability (F1-score of 0.67 on an external\ndataset). Furthermore, EmoScan outperforms baselines in interviewing skills, as\nvalidated by automated ratings and human evaluations. This work highlights the\nimportance of scalable data-generative pipelines for developing effective\nmental health LLM tools.",
    "published_date": "2025-01-15T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.08769v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.08605v1",
    "title": "PACF: Prototype Augmented Compact Features for Improving Domain Adaptive Object Detection",
    "authors": [
      "Chenguang Liu",
      "Yongchao Feng",
      "Yanan Zhang",
      "Qingjie Liu",
      "Yunhong Wang"
    ],
    "author_ids": [],
    "abstract": "In recent years, there has been significant advancement in object detection.\nHowever, applying off-the-shelf detectors to a new domain leads to significant\nperformance drop, caused by the domain gap. These detectors exhibit\nhigher-variance class-conditional distributions in the target domain than that\nin the source domain, along with mean shift. To address this problem, we\npropose the Prototype Augmented Compact Features (PACF) framework to regularize\nthe distribution of intra-class features. Specifically, we provide an in-depth\ntheoretical analysis on the lower bound of the target features-related\nlikelihood and derive the prototype cross entropy loss to further calibrate the\ndistribution of target RoI features. Furthermore, a mutual regularization\nstrategy is designed to enable the linear and prototype-based classifiers to\nlearn from each other, promoting feature compactness while enhancing\ndiscriminability. Thanks to this PACF framework, we have obtained a more\ncompact cross-domain feature space, within which the variance of the target\nfeatures' class-conditional distributions has significantly decreased, and the\nclass-mean shift between the two domains has also been further reduced. The\nresults on different adaptation settings are state-of-the-art, which\ndemonstrate the board applicability and effectiveness of the proposed approach.",
    "published_date": "2025-01-15T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.08605v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.08563v1",
    "title": "Adaptive Sampled Softmax with Inverted Multi-Index: Methods, Theory and Applications",
    "authors": [
      "Jin Chen",
      "Jin Zhang",
      "Xu huang",
      "Yi Yang",
      "Defu Lian",
      "Enhong Chen"
    ],
    "author_ids": [],
    "abstract": "The softmax function is a cornerstone of multi-class classification, integral\nto a wide range of machine learning applications, from large-scale retrieval\nand ranking models to advanced large language models. However, its\ncomputational cost grows linearly with the number of classes, which becomes\nprohibitively expensive in scenarios with millions or even billions of classes.\nThe sampled softmax, which relies on self-normalized importance sampling, has\nemerged as a powerful alternative, significantly reducing computational\ncomplexity. Yet, its estimator remains unbiased only when the sampling\ndistribution matches the true softmax distribution. To improve both\napproximation accuracy and sampling efficiency, we propose the MIDX Sampler, a\nnovel adaptive sampling strategy based on an inverted multi-index approach.\nConcretely, we decompose the softmax probability into several multinomial\nprobabilities, each associated with a specific set of codewords and the last\nassociated with the residual score of queries, thus reducing time complexity to\nthe number of codewords instead of the number of classes. To further boost\nefficiency, we replace the query-specific residual probability with a simple\nuniform distribution, simplifying the computation while retaining high\nperformance. Our method is backed by rigorous theoretical analysis, addressing\nkey concerns such as sampling bias, gradient bias, convergence rates, and\ngeneralization error bounds. The results demonstrate that a smaller divergence\nfrom the ideal softmax distribution leads to faster convergence and improved\ngeneralization. Extensive experiments on large-scale language models,\nsequential recommenders, and extreme multi-class classification tasks confirm\nthat the MIDX-Sampler delivers superior effectiveness and efficiency compared\nto existing approaches.",
    "published_date": "2025-01-15T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.08563v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.08497v1",
    "title": "Addressing Intersectionality, Explainability, and Ethics in AI-Driven Diagnostics: A Rebuttal and Call for Transdiciplinary Action",
    "authors": [
      "Myles Joshua Toledo Tan",
      "Panayiotis V. Benos"
    ],
    "author_ids": [],
    "abstract": "The increasing integration of artificial intelligence (AI) into medical\ndiagnostics necessitates a critical examination of its ethical and practical\nimplications. While the prioritization of diagnostic accuracy, as advocated by\nSabuncu et al. (2025), is essential, this approach risks oversimplifying\ncomplex socio-ethical issues, including fairness, privacy, and\nintersectionality. This rebuttal emphasizes the dangers of reducing\nmultifaceted health disparities to quantifiable metrics and advocates for a\nmore transdisciplinary approach. By incorporating insights from social\nsciences, ethics, and public health, AI systems can address the compounded\neffects of intersecting identities and safeguard sensitive data. Additionally,\nexplainability and interpretability must be central to AI design, fostering\ntrust and accountability. This paper calls for a framework that balances\naccuracy with fairness, privacy, and inclusivity to ensure AI-driven\ndiagnostics serve diverse populations equitably and ethically.",
    "published_date": "2025-01-15T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.08497v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.08441v1",
    "title": "Religious Bias Landscape in Language and Text-to-Image Models: Analysis, Detection, and Debiasing Strategies",
    "authors": [
      "Ajwad Abrar",
      "Nafisa Tabassum Oeshy",
      "Mohsinul Kabir",
      "Sophia Ananiadou"
    ],
    "author_ids": [],
    "abstract": "Note: This paper includes examples of potentially offensive content related\nto religious bias, presented solely for academic purposes. The widespread\nadoption of language models highlights the need for critical examinations of\ntheir inherent biases, particularly concerning religion. This study\nsystematically investigates religious bias in both language models and\ntext-to-image generation models, analyzing both open-source and closed-source\nsystems. We construct approximately 400 unique, naturally occurring prompts to\nprobe language models for religious bias across diverse tasks, including mask\nfilling, prompt completion, and image generation. Our experiments reveal\nconcerning instances of underlying stereotypes and biases associated\ndisproportionately with certain religions. Additionally, we explore\ncross-domain biases, examining how religious bias intersects with demographic\nfactors such as gender, age, and nationality. This study further evaluates the\neffectiveness of targeted debiasing techniques by employing corrective prompts\ndesigned to mitigate the identified biases. Our findings demonstrate that\nlanguage models continue to exhibit significant biases in both text and image\ngeneration tasks, emphasizing the urgent need to develop fairer language models\nto achieve global acceptability.",
    "published_date": "2025-01-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.08441v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.09765v1",
    "title": "Enhancing the De-identification of Personally Identifiable Information in Educational Data",
    "authors": [
      "Y. Shen",
      "Z. Ji",
      "J. Lin",
      "K. R. Koedginer"
    ],
    "author_ids": [],
    "abstract": "Protecting Personally Identifiable Information (PII), such as names, is a\ncritical requirement in learning technologies to safeguard student and teacher\nprivacy and maintain trust. Accurate PII detection is an essential step toward\nanonymizing sensitive information while preserving the utility of educational\ndata. Motivated by recent advancements in artificial intelligence, our study\ninvestigates the GPT-4o-mini model as a cost-effective and efficient solution\nfor PII detection tasks. We explore both prompting and fine-tuning approaches\nand compare GPT-4o-mini's performance against established frameworks, including\nMicrosoft Presidio and Azure AI Language. Our evaluation on two public\ndatasets, CRAPII and TSCC, demonstrates that the fine-tuned GPT-4o-mini model\nachieves superior performance, with a recall of 0.9589 on CRAPII. Additionally,\nfine-tuned GPT-4o-mini significantly improves precision scores (a threefold\nincrease) while reducing computational costs to nearly one-tenth of those\nassociated with Azure AI Language. Furthermore, our bias analysis reveals that\nthe fine-tuned GPT-4o-mini model consistently delivers accurate results across\ndiverse cultural backgrounds and genders. The generalizability analysis using\nthe TSCC dataset further highlights its robustness, achieving a recall of\n0.9895 with minimal additional training data from TSCC. These results emphasize\nthe potential of fine-tuned GPT-4o-mini as an accurate and cost-effective tool\nfor PII detection in educational data. It offers robust privacy protection\nwhile preserving the data's utility for research and pedagogical analysis. Our\ncode is available on GitHub: https://github.com/AnonJD/PrivacyAI",
    "published_date": "2025-01-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.09765v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.08429v1",
    "title": "Modeling Discrimination with Causal Abstraction",
    "authors": [
      "Milan Mossé",
      "Kara Schechtman",
      "Frederick Eberhardt",
      "Thomas Icard"
    ],
    "author_ids": [],
    "abstract": "A person is directly racially discriminated against only if her race caused\nher worse treatment. This implies that race is an attribute sufficiently\nseparable from other attributes to isolate its causal role. But race is\nembedded in a nexus of social factors that resist isolated treatment. If race\nis socially constructed, in what sense can it cause worse treatment? Some\npropose that the perception of race, rather than race itself, causes worse\ntreatment. Others suggest that since causal models require modularity, i.e. the\nability to isolate causal effects, attempts to causally model discrimination\nare misguided.\n  This paper addresses the problem differently. We introduce a framework for\nreasoning about discrimination, in which race is a high-level abstraction of\nlower-level features. In this framework, race can be modeled as itself causing\nworse treatment. Modularity is ensured by allowing assumptions about social\nconstruction to be precisely and explicitly stated, via an alignment between\nrace and its constituents. Such assumptions can then be subjected to normative\nand empirical challenges, which lead to different views of when discrimination\noccurs. By distinguishing constitutive and causal relations, the abstraction\nframework pinpoints disagreements in the current literature on modeling\ndiscrimination, while preserving a precise causal account of discrimination.",
    "published_date": "2025-01-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.08429v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.08418v2",
    "title": "CVaR-Based Variational Quantum Optimization for User Association in Handoff-Aware Vehicular Networks",
    "authors": [
      "Zijiang Yan",
      "Hao Zhou",
      "Jianhua Pei",
      "Aryan Kaushik",
      "Hina Tabassum",
      "Ping Wang"
    ],
    "author_ids": [],
    "abstract": "Efficient resource allocation is essential for optimizing various tasks in\nwireless networks, which are usually formulated as generalized assignment\nproblems (GAP). GAP, as a generalized version of the linear sum assignment\nproblem, involves both equality and inequality constraints that add\ncomputational challenges. In this work, we present a novel Conditional Value at\nRisk (CVaR)-based Variational Quantum Eigensolver (VQE) framework to address\nGAP in vehicular networks (VNets). Our approach leverages a hybrid\nquantum-classical structure, integrating a tailored cost function that balances\nboth objective and constraint-specific penalties to improve solution quality\nand stability. Using the CVaR-VQE model, we handle the GAP efficiently by\nfocusing optimization on the lower tail of the solution space, enhancing both\nconvergence and resilience on noisy intermediate-scale quantum (NISQ) devices.\nWe apply this framework to a user-association problem in VNets, where our\nmethod achieves 23.5% improvement compared to the deep neural network (DNN)\napproach.",
    "published_date": "2025-01-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NI",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.08418v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.08401v2",
    "title": "Navigating Gender Disparities in Communication Research Leadership: Academic Recognition, Career Development, and Compensation",
    "authors": [
      "Diego F. M. Oliveira",
      "Qian Huang"
    ],
    "author_ids": [],
    "abstract": "This study examines gender disparities in communication research through\ncitation metrics, authorship patterns, team composition, and faculty salaries.\nUsing data from 62,359 papers across 121 communication journals, we find that\nwhile female authors are increasingly represented, citation gaps persist, with\nsole-authored papers by women receiving fewer citations than those by men,\nespecially in smaller teams. Team composition analysis reveals a tendency\ntoward gender homophily, with single-gender teams being more common. In top\nU.S. communication journals, female authors face underrepresentation and\ncitation disparities favoring male authors. Salary analysis from leading U.S.\npublic universities shows that female faculty earn lower salaries at the\nAssistant Professor level, though disparities lessen at higher ranks. These\nfindings highlight the need for greater efforts to promote gender equity\nthrough inclusive collaboration, equitable citation practices, and fair\ncompensation.",
    "published_date": "2025-01-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.DL",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.08401v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.10453v1",
    "title": "Uncovering Bias in Foundation Models: Impact, Testing, Harm, and Mitigation",
    "authors": [
      "Shuzhou Sun",
      "Li Liu",
      "Yongxiang Liu",
      "Zhen Liu",
      "Shuanghui Zhang",
      "Janne Heikkilä",
      "Xiang Li"
    ],
    "author_ids": [],
    "abstract": "Bias in Foundation Models (FMs) - trained on vast datasets spanning societal\nand historical knowledge - poses significant challenges for fairness and equity\nacross fields such as healthcare, education, and finance. These biases, rooted\nin the overrepresentation of stereotypes and societal inequalities in training\ndata, exacerbate real-world discrimination, reinforce harmful stereotypes, and\nerode trust in AI systems. To address this, we introduce Trident Probe Testing\n(TriProTesting), a systematic testing method that detects explicit and implicit\nbiases using semantically designed probes. Here we show that FMs, including\nCLIP, ALIGN, BridgeTower, and OWLv2, demonstrate pervasive biases across single\nand mixed social attributes (gender, race, age, and occupation). Notably, we\nuncover mixed biases when social attributes are combined, such as gender x\nrace, gender x age, and gender x occupation, revealing deeper layers of\ndiscrimination. We further propose Adaptive Logit Adjustment\n(AdaLogAdjustment), a post-processing technique that dynamically redistributes\nprobability power to mitigate these biases effectively, achieving significant\nimprovements in fairness without retraining models. These findings highlight\nthe urgent need for ethical AI practices and interdisciplinary solutions to\naddress biases not only at the model level but also in societal structures. Our\nwork provides a scalable and interpretable solution that advances fairness in\nAI systems while offering practical insights for future research on fair AI\ntechnologies.",
    "published_date": "2025-01-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.10453v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.08325v2",
    "title": "GameFactory: Creating New Games with Generative Interactive Videos",
    "authors": [
      "Jiwen Yu",
      "Yiran Qin",
      "Xintao Wang",
      "Pengfei Wan",
      "Di Zhang",
      "Xihui Liu"
    ],
    "author_ids": [],
    "abstract": "Generative videos have the potential to revolutionize game development by\nautonomously creating new content. In this paper, we present GameFactory, a\nframework for action-controlled scene-generalizable game video generation. We\nfirst address the fundamental challenge of action controllability by\nintroducing GF-Minecraft, a action-annotated game video dataset without human\nbias, and developing a action control module that enables precise control over\nboth keyboard and mouse inputs. We further extend to support autoregressive\ngeneration for unlimited-length interactive videos. More importantly,\nGameFactory tackles the critical challenge of scene-generalizable action\ncontrol, which most existing methods fail to address. To enable the creation of\nentirely new and diverse games beyond fixed styles and scenes, we leverage the\nopen-domain generative priors from pre-trained video diffusion models. To\nbridge the domain gap between open-domain priors and small-scale game datasets,\nwe propose a multi-phase training strategy with a domain adapter that decouples\ngame style learning from action control. This decoupling ensures that action\ncontrol learning is no longer bound to specific game styles, thereby achieving\nscene-generalizable action control. Experimental results demonstrate that\nGameFactory effectively generates open-domain action-controllable game videos,\nrepresenting a significant step forward in AI-driven game generation. Our\ndataset and project page are publicly available at\nhttps://yujiwen.github.io/gamefactory/.",
    "published_date": "2025-01-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.08325v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.08155v1",
    "title": "FairTTTS: A Tree Test Time Simulation Method for Fairness-Aware Classification",
    "authors": [
      "Nurit Cohen-Inger",
      "Lior Rokach",
      "Bracha Shapira",
      "Seffi Cohen"
    ],
    "author_ids": [],
    "abstract": "Algorithmic decision-making has become deeply ingrained in many domains, yet\nbiases in machine learning models can still produce discriminatory outcomes,\noften harming unprivileged groups. Achieving fair classification is inherently\nchallenging, requiring a careful balance between predictive performance and\nethical considerations. We present FairTTTS, a novel post-processing bias\nmitigation method inspired by the Tree Test Time Simulation (TTTS) method.\nOriginally developed to enhance accuracy and robustness against adversarial\ninputs through probabilistic decision-path adjustments, TTTS serves as the\nfoundation for FairTTTS. By building on this accuracy-enhancing technique,\nFairTTTS mitigates bias and improves predictive performance. FairTTTS uses a\ndistance-based heuristic to adjust decisions at protected attribute nodes,\nensuring fairness for unprivileged samples. This fairness-oriented adjustment\noccurs as a post-processing step, allowing FairTTTS to be applied to\npre-trained models, diverse datasets, and various fairness metrics without\nretraining. Extensive evaluation on seven benchmark datasets shows that\nFairTTTS outperforms traditional methods in fairness improvement, achieving a\n20.96% average increase over the baseline compared to 18.78% for related work,\nand further enhances accuracy by 0.55%. In contrast, competing methods\ntypically reduce accuracy by 0.42%. These results confirm that FairTTTS\neffectively promotes more equitable decision-making while simultaneously\nimproving predictive performance.",
    "published_date": "2025-01-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.08155v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.08102v2",
    "title": "Consistency of Responses and Continuations Generated by Large Language Models on Social Media",
    "authors": [
      "Wenlu Fan",
      "Yuqi Zhu",
      "Chenyang Wang",
      "Bin Wang",
      "Wentao Xu"
    ],
    "author_ids": [],
    "abstract": "Large Language Models (LLMs) demonstrate remarkable capabilities in text\ngeneration, yet their emotional consistency and semantic coherence in social\nmedia contexts remain insufficiently understood. This study investigates how\nLLMs handle emotional content and maintain semantic relationships through\ncontinuation and response tasks using two open-source models: Gemma and Llama.\nBy analyzing climate change discussions from Twitter and Reddit, we examine\nemotional transitions, intensity patterns, and semantic similarity between\nhuman-authored and LLM-generated content. Our findings reveal that while both\nmodels maintain high semantic coherence, they exhibit distinct emotional\npatterns: Gemma shows a tendency toward negative emotion amplification,\nparticularly anger, while maintaining certain positive emotions like optimism.\nLlama demonstrates superior emotional preservation across a broader spectrum of\naffects. Both models systematically generate responses with attenuated\nemotional intensity compared to human-authored content and show a bias toward\npositive emotions in response tasks. Additionally, both models maintain strong\nsemantic similarity with original texts, though performance varies between\ncontinuation and response tasks. These findings provide insights into LLMs'\nemotional and semantic processing capabilities, with implications for their\ndeployment in social media contexts and human-AI interaction design.",
    "published_date": "2025-01-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.08102v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.08037v2",
    "title": "Enhanced SPS Velocity-adaptive Scheme: Access Fairness in 5G NR V2I Networks",
    "authors": [
      "Xiao Xu",
      "Qiong Wu",
      "Pingyi Fan",
      "Kezhi Wang"
    ],
    "author_ids": [],
    "abstract": "Vehicle-to-Infrastructure (V2I) technology enables information exchange\nbetween vehicles and road infrastructure. Specifically, when a vehicle\napproaches a roadside unit (RSU), it can exchange information with the RSU to\nobtain accurate data that assists in driving. With the release of the 3rd\nGeneration Partnership Project (3GPP) Release 16, which includes the 5G New\nRadio (NR) Vehicle-to-Everything (V2X) standards, vehicles typically adopt\nmode-2 communication using sensing-based semi-persistent scheduling (SPS) for\nresource allocation. In this approach, vehicles identify candidate resources\nwithin a selection window and exclude ineligible resources based on information\nfrom a sensing window. However, vehicles often drive at different speeds,\nresulting in varying amounts of data transmission with RSUs as they pass by,\nwhich leads to unfair access. Therefore, it is essential to design an access\nscheme that accounts for different vehicle speeds to achieve fair access across\nthe network. This paper formulates an optimization problem for vehicular\nnetworks and proposes a multi-objective optimization scheme to address it by\nadjusting the selection window in the SPS mechanism of 5G NR V2I mode-2.\nSimulation results demonstrate the effectiveness of the proposed scheme",
    "published_date": "2025-01-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.08037v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.07972v1",
    "title": "Zero-shot Video Moment Retrieval via Off-the-shelf Multimodal Large Language Models",
    "authors": [
      "Yifang Xu",
      "Yunzhuo Sun",
      "Benxiang Zhai",
      "Ming Li",
      "Wenxin Liang",
      "Yang Li",
      "Sidan Du"
    ],
    "author_ids": [],
    "abstract": "The target of video moment retrieval (VMR) is predicting temporal spans\nwithin a video that semantically match a given linguistic query. Existing VMR\nmethods based on multimodal large language models (MLLMs) overly rely on\nexpensive high-quality datasets and time-consuming fine-tuning. Although some\nrecent studies introduce a zero-shot setting to avoid fine-tuning, they\noverlook inherent language bias in the query, leading to erroneous\nlocalization. To tackle the aforementioned challenges, this paper proposes\nMoment-GPT, a tuning-free pipeline for zero-shot VMR utilizing frozen MLLMs.\nSpecifically, we first employ LLaMA-3 to correct and rephrase the query to\nmitigate language bias. Subsequently, we design a span generator combined with\nMiniGPT-v2 to produce candidate spans adaptively. Finally, to leverage the\nvideo comprehension capabilities of MLLMs, we apply VideoChatGPT and span\nscorer to select the most appropriate spans. Our proposed method substantially\noutperforms the state-ofthe-art MLLM-based and zero-shot models on several\npublic datasets, including QVHighlights, ActivityNet-Captions, and\nCharades-STA.",
    "published_date": "2025-01-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.MM",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.07972v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.07898v1",
    "title": "Demographic Variability in Face Image Quality Measures",
    "authors": [
      "Wassim Kabbani",
      "Kiran Raja",
      "Raghavendra Ramachandra",
      "Christoph Busch"
    ],
    "author_ids": [],
    "abstract": "Face image quality assessment (FIQA) algorithms are being integrated into\nonline identity management applications. These applications allow users to\nupload a face image as part of their document issuance process, where the image\nis then run through a quality assessment process to make sure it meets the\nquality and compliance requirements. Concerns about demographic bias have been\nraised about biometric systems, given the societal implications this may cause.\nIt is therefore important that demographic variability in FIQA algorithms is\nassessed such that mitigation measures can be created. In this work, we study\nthe demographic variability of all face image quality measures included in the\nISO/IEC 29794-5 international standard across three demographic variables: age,\ngender, and skin tone. The results are rather promising and show no clear bias\ntoward any specific demographic group for most measures. Only two quality\nmeasures are found to have considerable variations in their outcomes for\ndifferent groups on the skin tone variable.",
    "published_date": "2025-01-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.07898v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.07885v1",
    "title": "Mitigating Algorithmic Bias in Multiclass CNN Classifications Using Causal Modeling",
    "authors": [
      "Min Sik Byun",
      "Wendy Wan Yee Hui",
      "Wai Kwong Lau"
    ],
    "author_ids": [],
    "abstract": "This study describes a procedure for applying causal modeling to detect and\nmitigate algorithmic bias in a multiclass classification problem. The dataset\nwas derived from the FairFace dataset, supplemented with emotional labels\ngenerated by the DeepFace pre-trained model. A custom Convolutional Neural\nNetwork (CNN) was developed, consisting of four convolutional blocks, followed\nby fully connected layers and dropout layers to mitigate overfitting. Gender\nbias was identified in the CNN model's classifications: Females were more\nlikely to be classified as \"happy\" or \"sad,\" while males were more likely to be\nclassified as \"neutral.\" To address this, the one-vs-all (OvA) technique was\napplied. A causal model was constructed for each emotion class to adjust the\nCNN model's predicted class probabilities. The adjusted probabilities for the\nvarious classes were then aggregated by selecting the class with the highest\nprobability. The resulting debiased classifications demonstrated enhanced\ngender fairness across all classes, with negligible impact--or even a slight\nimprovement--on overall accuracy. This study highlights that algorithmic\nfairness and accuracy are not necessarily trade-offs. All data and code for\nthis study are publicly available for download.",
    "published_date": "2025-01-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.07885v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.07849v1",
    "title": "Unveiling Provider Bias in Large Language Models for Code Generation",
    "authors": [
      "Xiaoyu Zhang",
      "Juan Zhai",
      "Shiqing Ma",
      "Qingshuang Bao",
      "Weipeng Jiang",
      "Chao Shen",
      "Yang Liu"
    ],
    "author_ids": [],
    "abstract": "Large Language Models (LLMs) have emerged as the new recommendation engines,\noutperforming traditional methods in both capability and scope, particularly in\ncode generation applications. Our research reveals a novel provider bias in\nLLMs, namely without explicit input prompts, these models show systematic\npreferences for services from specific providers in their recommendations\n(e.g., favoring Google Cloud over Microsoft Azure). This bias holds significant\nimplications for market dynamics and societal equilibrium, potentially\npromoting digital monopolies. It may also deceive users and violate their\nexpectations, leading to various consequences. This paper presents the first\ncomprehensive empirical study of provider bias in LLM code generation. We\ndevelop a systematic methodology encompassing an automated pipeline for dataset\ngeneration, incorporating 6 distinct coding task categories and 30 real-world\napplication scenarios. Our analysis encompasses over 600,000 LLM-generated\nresponses across seven state-of-the-art models, utilizing approximately 500\nmillion tokens (equivalent to \\$5,000+ in computational costs). The study\nevaluates both the generated code snippets and their embedded service provider\nselections to quantify provider bias. Additionally, we conduct a comparative\nanalysis of seven debiasing prompting techniques to assess their efficacy in\nmitigating these biases. Our findings demonstrate that LLMs exhibit significant\nprovider preferences, predominantly favoring services from Google and Amazon,\nand can autonomously modify input code to incorporate their preferred providers\nwithout users' requests. Notably, we observe discrepancies between providers\nrecommended in conversational contexts versus those implemented in generated\ncode. The complete dataset and analysis results are available in our\nrepository.",
    "published_date": "2025-01-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.07849v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.07763v1",
    "title": "On the Statistical Capacity of Deep Generative Models",
    "authors": [
      "Edric Tam",
      "David B. Dunson"
    ],
    "author_ids": [],
    "abstract": "Deep generative models are routinely used in generating samples from complex,\nhigh-dimensional distributions. Despite their apparent successes, their\nstatistical properties are not well understood. A common assumption is that\nwith enough training data and sufficiently large neural networks, deep\ngenerative model samples will have arbitrarily small errors in sampling from\nany continuous target distribution. We set up a unifying framework that debunks\nthis belief. We demonstrate that broad classes of deep generative models,\nincluding variational autoencoders and generative adversarial networks, are not\nuniversal generators. Under the predominant case of Gaussian latent variables,\nthese models can only generate concentrated samples that exhibit light tails.\nUsing tools from concentration of measure and convex geometry, we give\nanalogous results for more general log-concave and strongly log-concave latent\nvariable distributions. We extend our results to diffusion models via a\nreduction argument. We use the Gromov--Levy inequality to give similar\nguarantees when the latent variables lie on manifolds with positive Ricci\ncurvature. These results shed light on the limited capacity of common deep\ngenerative models to handle heavy tails. We illustrate the empirical relevance\nof our work with simulations and financial data.",
    "published_date": "2025-01-14T00:00:00",
    "year": 2025,
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "math.ST",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.07763v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2503.15494v1",
    "title": "AI-Powered Assistive Technologies for Visual Impairment",
    "authors": [
      "Prudhvi Naayini",
      "Praveen Kumar Myakala",
      "Chiranjeevi Bura",
      "Anil Kumar Jonnalagadda",
      "Srikanth Kamatala"
    ],
    "author_ids": [],
    "abstract": "Artificial Intelligence (AI) is revolutionizing assistive technologies. It\noffers innovative solutions to enhance the quality of life for individuals with\nvisual impairments. This review examines the development, applications, and\nimpact of AI-powered tools in key domains, such as computer vision, natural\nlanguage processing (NLP), and wearable devices. Specific advancements include\nobject recognition for identifying everyday items, scene description for\nunderstanding surroundings, and NLP-driven text-to-speech systems for accessing\ndigital information. Assistive technologies like smart glasses, smartphone\napplications, and AI-enabled navigation aids are discussed, demonstrating their\nability to support independent travel, facilitate social interaction, and\nincrease access to education and employment opportunities.\n  The integration of deep learning models, multimodal interfaces, and real-time\ndata processing has transformed the functionality and usability of these tools,\nfostering inclusivity and empowerment. This article also addresses critical\nchallenges, including ethical considerations, affordability, and adaptability\nin diverse environments. Future directions highlight the need for\ninterdisciplinary collaboration to refine these technologies, ensuring\nequitable access and sustainable innovation. By providing a comprehensive\noverview, this review underscores AI's transformative potential in promoting\nindependence, enhancing accessibility, and fostering social inclusion for\nvisually impaired individuals.",
    "published_date": "2025-01-14T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2503.15494v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.07690v1",
    "title": "An Investigation of Experiences Engaging the Margins in Data-Centric Innovation",
    "authors": [
      "Gabriella Thompson",
      "Ebtesam Al Haque",
      "Paulette Blanc",
      "Meme Styles",
      "Denae Ford",
      "Angela D. R. Smith",
      "Brittany Johnson"
    ],
    "author_ids": [],
    "abstract": "Data-centric technologies provide exciting opportunities, but recent research\nhas shown how lack of representation in datasets, often as a result of systemic\ninequities and socioeconomic disparities, can produce inequitable outcomes that\ncan exclude or harm certain demographics. In this paper, we discuss preliminary\ninsights from an ongoing effort aimed at better understanding barriers to\nequitable data-centric innovation. We report findings from a survey of 261\ntechnologists and researchers who use data in their work regarding their\nexperiences seeking adequate, representative datasets. Our findings suggest\nthat age and identity play a significant role in the seeking and selection of\nrepresentative datasets, warranting further investigation into these aspects of\ndata-centric research and development.",
    "published_date": "2025-01-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.07690v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.07653v2",
    "title": "Large Language Models for Interpretable Mental Health Diagnosis",
    "authors": [
      "Brian Hyeongseok Kim",
      "Chao Wang"
    ],
    "author_ids": [],
    "abstract": "We propose a clinical decision support system (CDSS) for mental health\ndiagnosis that combines the strengths of large language models (LLMs) and\nconstraint logic programming (CLP). Having a CDSS is important because of the\nhigh complexity of diagnostic manuals used by mental health professionals and\nthe danger of diagnostic errors. Our CDSS is a software tool that uses an LLM\nto translate diagnostic manuals to a logic program and solves the program using\nan off-the-shelf CLP engine to query a patient's diagnosis based on the encoded\nrules and provided data. By giving domain experts the opportunity to inspect\nthe LLM-generated logic program, and making modifications when needed, our CDSS\nensures that the diagnosis is not only accurate but also interpretable. We\nexperimentally compare it with two baseline approaches of using LLMs:\ndiagnosing patients using the LLM-only approach, and using the LLM-generated\nlogic program but without expert inspection. The results show that, while LLMs\nare extremely useful in generating candidate logic programs, these programs\nstill require expert inspection and modification to guarantee faithfulness to\nthe official diagnostic manuals. Additionally, ethical concerns arise from the\ndirect use of patient data in LLMs, underscoring the need for a safer hybrid\napproach like our proposed method.",
    "published_date": "2025-01-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.07653v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.07468v3",
    "title": "From Screens to Scenes: A Survey of Embodied AI in Healthcare",
    "authors": [
      "Yihao Liu",
      "Xu Cao",
      "Tingting Chen",
      "Yankai Jiang",
      "Junjie You",
      "Minghua Wu",
      "Xiaosong Wang",
      "Mengling Feng",
      "Yaochu Jin",
      "Jintai Chen"
    ],
    "author_ids": [],
    "abstract": "Healthcare systems worldwide face persistent challenges in efficiency,\naccessibility, and personalization. Powered by modern AI technologies such as\nmultimodal large language models and world models, Embodied AI (EmAI)\nrepresents a transformative frontier, offering enhanced autonomy and the\nability to interact with the physical world to address these challenges. As an\ninterdisciplinary and rapidly evolving research domain, \"EmAI in healthcare\"\nspans diverse fields such as algorithms, robotics, and biomedicine. This\ncomplexity underscores the importance of timely reviews and analyses to track\nadvancements, address challenges, and foster cross-disciplinary collaboration.\nIn this paper, we provide a comprehensive overview of the \"brain\" of EmAI for\nhealthcare, wherein we introduce foundational AI algorithms for perception,\nactuation, planning, and memory, and focus on presenting the healthcare\napplications spanning clinical interventions, daily care & companionship,\ninfrastructure support, and biomedical research. Despite its promise, the\ndevelopment of EmAI for healthcare is hindered by critical challenges such as\nsafety concerns, gaps between simulation platforms and real-world applications,\nthe absence of standardized benchmarks, and uneven progress across\ninterdisciplinary domains. We discuss the technical barriers and explore\nethical considerations, offering a forward-looking perspective on the future of\nEmAI in healthcare. A hierarchical framework of intelligent levels for EmAI\nsystems is also introduced to guide further development. By providing\nsystematic insights, this work aims to inspire innovation and practical\napplications, paving the way for a new era of intelligent, patient-centered\nhealthcare.",
    "published_date": "2025-01-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.07468v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.07447v1",
    "title": "PrecipDiff: Leveraging image diffusion models to enhance satellite-based precipitation observations",
    "authors": [
      "Ting-Yu Dai",
      "Hayato Ushijima-Mwesigwa"
    ],
    "author_ids": [],
    "abstract": "A recent report from the World Meteorological Organization (WMO) highlights\nthat water-related disasters have caused the highest human losses among natural\ndisasters over the past 50 years, with over 91\\% of deaths occurring in\nlow-income countries. This disparity is largely due to the lack of adequate\nground monitoring stations, such as weather surveillance radars (WSR), which\nare expensive to install. For example, while the US and Europe combined possess\nover 600 WSRs, Africa, despite having almost one and half times their landmass,\nhas fewer than 40. To address this issue, satellite-based observations offer a\nglobal, near-real-time monitoring solution. However, they face several\nchallenges like accuracy, bias, and low spatial resolution. This study\nleverages the power of diffusion models and residual learning to address these\nlimitations in a unified framework. We introduce the first diffusion model for\ncorrecting the inconsistency between different precipitation products. Our\nmethod demonstrates the effectiveness in downscaling satellite precipitation\nestimates from 10 km to 1 km resolution. Extensive experiments conducted in the\nSeattle region demonstrate significant improvements in accuracy, bias\nreduction, and spatial detail. Importantly, our approach achieves these results\nusing only precipitation data, showcasing the potential of a purely computer\nvision-based approach for enhancing satellite precipitation products and paving\nthe way for further advancements in this domain.",
    "published_date": "2025-01-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.07447v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.07324v1",
    "title": "Foundation Models at Work: Fine-Tuning for Fairness in Algorithmic Hiring",
    "authors": [
      "Buse Sibel Korkmaz",
      "Rahul Nair",
      "Elizabeth M. Daly",
      "Evangelos Anagnostopoulos",
      "Christos Varytimidis",
      "Antonio del Rio Chanona"
    ],
    "author_ids": [],
    "abstract": "Foundation models require fine-tuning to ensure their generative outputs\nalign with intended results for specific tasks. Automating this fine-tuning\nprocess is challenging, as it typically needs human feedback that can be\nexpensive to acquire. We present AutoRefine, a method that leverages\nreinforcement learning for targeted fine-tuning, utilizing direct feedback from\nmeasurable performance improvements in specific downstream tasks. We\ndemonstrate the method for a problem arising in algorithmic hiring platforms\nwhere linguistic biases influence a recommendation system. In this setting, a\ngenerative model seeks to rewrite given job specifications to receive more\ndiverse candidate matches from a recommendation engine which matches jobs to\ncandidates. Our model detects and regulates biases in job descriptions to meet\ndiversity and fairness criteria. The experiments on a public hiring dataset and\na real-world hiring platform showcase how large language models can assist in\nidentifying and mitigation biases in the real world.",
    "published_date": "2025-01-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.07324v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.07276v2",
    "title": "Bridging Smart Meter Gaps: A Benchmark of Statistical, Machine Learning and Time Series Foundation Models for Data Imputation",
    "authors": [
      "Amir Sartipi",
      "Joaquín Delgado Fernández",
      "Sergio Potenciano Menci",
      "Alessio Magitteri"
    ],
    "author_ids": [],
    "abstract": "The integrity of time series data in smart grids is often compromised by\nmissing values due to sensor failures, transmission errors, or disruptions.\nGaps in smart meter data can bias consumption analyses and hinder reliable\npredictions, causing technical and economic inefficiencies. As smart meter data\ngrows in volume and complexity, conventional techniques struggle with its\nnonlinear and nonstationary patterns. In this context, Generative Artificial\nIntelligence offers promising solutions that may outperform traditional\nstatistical methods. In this paper, we evaluate two general-purpose Large\nLanguage Models and five Time Series Foundation Models for smart meter data\nimputation, comparing them with conventional Machine Learning and statistical\nmodels. We introduce artificial gaps (30 minutes to one day) into an anonymized\npublic dataset to test inference capabilities. Results show that Time Series\nFoundation Models, with their contextual understanding and pattern recognition,\ncould significantly enhance imputation accuracy in certain cases. However, the\ntrade-off between computational cost and performance gains remains a critical\nconsideration.",
    "published_date": "2025-01-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.07276v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.07265v1",
    "title": "On Stability and Learning of Competitive Equilibrium in Generalized Fisher Market Models: A Variational Inequality Approach",
    "authors": [
      "Mandar Datar"
    ],
    "author_ids": [],
    "abstract": "In this work, we study a generalized Fisher market model that incorporates\nsocial influence. In this extended model, a buyer's utility depends not only on\ntheir own resource allocation but also on the allocations received by their\ncompetitors. We propose a novel competitive equilibrium formulation for this\ngeneralized Fisher market using a variational inequality approach. This\nframework effectively captures competitive equilibrium in markets that extend\nbeyond the traditional assumption of homogeneous utility functions. We analyze\nkey structural properties of the proposed variational inequality problem,\nincluding monotonicity, stability, and uniqueness. Additionally, we present two\ndecentralized learning algorithms for buyers to achieve competitive\nequilibrium: a two-timescale stochastic approximation-based t{\\^a}tonnement\nmethod and a trading-post mechanism-based learning method. Finally, we validate\nthe proposed algorithms through numerical simulations.",
    "published_date": "2025-01-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.07265v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.07206v1",
    "title": "A data-driven approach to discover and quantify systemic lupus erythematosus etiological heterogeneity from electronic health records",
    "authors": [
      "Marco Barbero Mota",
      "John M. Still",
      "Jorge L. Gamboa",
      "Eric V. Strobl",
      "Charles M. Stein",
      "Vivian K. Kawai",
      "Thomas A. Lasko"
    ],
    "author_ids": [],
    "abstract": "Systemic lupus erythematosus (SLE) is a complex heterogeneous disease with\nmany manifestational facets. We propose a data-driven approach to discover\nprobabilistic independent sources from multimodal imperfect EHR data. These\nsources represent exogenous variables in the data generation process causal\ngraph that estimate latent root causes of the presence of SLE in the health\nrecord. We objectively evaluated the sources against the original variables\nfrom which they were discovered by training supervised models to discriminate\nSLE from negative health records using a reduced set of labelled instances. We\nfound 19 predictive sources with high clinical validity and whose EHR\nsignatures define independent factors of SLE heterogeneity. Using the sources\nas input patient data representation enables models to provide with rich\nexplanations that better capture the clinical reasons why a particular record\nis (not) an SLE case. Providers may be willing to trade patient-level\ninterpretability for discrimination especially in challenging cases.",
    "published_date": "2025-01-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.07206v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.07193v1",
    "title": "Combined effect of incentives and coupling in multigames in two-layer networks",
    "authors": [
      "Luo-Luo Jiang",
      "Yi-Ming Li",
      "Wen-Jing Li",
      "Attila Szolnoki"
    ],
    "author_ids": [],
    "abstract": "The lack of cooperation can easily result in inequality among members of a\nsociety, which provides an increasing gap between individual incomes. To tackle\nthis issue, we introduce an incentive mechanism based on individual strategies\nand incomes, wherein a portion of the income from defectors is allocated to\nreward low-income cooperators, aiming to enhance cooperation by improving the\nequitable distribution of wealth across the entire population. Moreover,\nprevious research has typically employed network structures or game mechanisms\ncharacterized by homogeneity. In this study, we present a network framework\nthat more accurately reflects real-world conditions, where agents are engaged\nin multiple games, including prisoner's dilemma games in the top-layer and\npublic good games in the down-layer networks. Within this framework, we\nintroduce the concept of ``external coupling'' which connects agents across\ndifferent networks as acquaintances, thereby facilitating access to shared\ndatasets. Our results indicate that the combined positive effects of external\ncoupling and incentive mechanism lead to optimal cooperation rates and lower\nGini coefficients, demonstrating a negative correlation between cooperation and\ninequality. From a micro-level perspective, this phenomenon primarily arises\nfrom the regular network, whereas suboptimal outcomes are observed within the\nscale-free network. These observations help to give a deeper insight into the\ninterplay between cooperation and wealth disparity in evolutionary games in\nlarge populations.",
    "published_date": "2025-01-13T00:00:00",
    "year": 2025,
    "categories": [
      "physics.soc-ph",
      "cond-mat.stat-mech",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.07193v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.08921v1",
    "title": "Discrimination loss vs. SRT: A model-based approach towards harmonizing speech test interpretations",
    "authors": [
      "Mareike Buhl",
      "Eugen Kludt",
      "Lena Schell-Majoor",
      "Paul Avan",
      "Marta Campi"
    ],
    "author_ids": [],
    "abstract": "Objective: Speech tests aim to estimate discrimination loss or speech\nrecognition threshold (SRT). This paper investigates the potential to estimate\nSRTs from clinical data that target at characterizing the discrimination loss.\nKnowledge about the relationship between the speech test outcome\nvariables--conceptually linked via the psychometric function--is important\ntowards integration of data from different databases.\n  Design: Depending on the available data, different SRT estimation procedures\nwere compared and evaluated. A novel, model-based SRT estimation procedure was\nproposed that deals with incomplete patient data. Interpretations of\nsupra-threshold deficits were assessed for the two interpretation modes.\n  Study sample: Data for 27009 patients with Freiburg monosyllabic speech test\n(FMST) and audiogram (AG) results from the same day were included in the\nretrospective analysis.\n  Results: The model-based SRT estimation procedure provided accurate SRTs, but\nwith large deviations in the estimated slope. Supra-threshold hearing loss\ncomponents differed between the two interpretation modes.\n  Conclusions: The model-based procedure can be used for SRT estimation, and\nits properties relate to data availability for individual patients. All SRT\nprocedures are influenced by the uncertainty of the word recognition scores. In\nthe future, the proposed approach can be used to assess additional differences\nbetween speech tests.",
    "published_date": "2025-01-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SD",
      "eess.AS",
      "physics.med-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.08921v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.07158v1",
    "title": "Eye Sclera for Fair Face Image Quality Assessment",
    "authors": [
      "Wassim Kabbani",
      "Kiran Raja",
      "Raghavendra Ramachandra",
      "Christoph Busch"
    ],
    "author_ids": [],
    "abstract": "Fair operational systems are crucial in gaining and maintaining society's\ntrust in face recognition systems (FRS). FRS start with capturing an image and\nassessing its quality before using it further for enrollment or verification.\nFair Face Image Quality Assessment (FIQA) schemes therefore become equally\nimportant in the context of fair FRS. This work examines the sclera as a\nquality assessment region for obtaining a fair FIQA. The sclera region is\nagnostic to demographic variations and skin colour for assessing the quality of\na face image. We analyze three skin tone related ISO/IEC face image quality\nassessment measures and assess the sclera region as an alternative area for\nassessing FIQ. Our analysis of the face dataset of individuals from different\ndemographic groups representing different skin tones indicates sclera as an\nalternative to measure dynamic range, over- and under-exposure of face using\nsclera region alone. The sclera region being agnostic to skin tone, i.e.,\ndemographic factors, provides equal utility as a fair FIQA as shown by our\nError-vs-Discard Characteristic (EDC) curve analysis.",
    "published_date": "2025-01-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.07158v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.07109v1",
    "title": "The Quest for Visual Understanding: A Journey Through the Evolution of Visual Question Answering",
    "authors": [
      "Anupam Pandey",
      "Deepjyoti Bodo",
      "Arpan Phukan",
      "Asif Ekbal"
    ],
    "author_ids": [],
    "abstract": "Visual Question Answering (VQA) is an interdisciplinary field that bridges\nthe gap between computer vision (CV) and natural language processing(NLP),\nenabling Artificial Intelligence(AI) systems to answer questions about images.\nSince its inception in 2015, VQA has rapidly evolved, driven by advances in\ndeep learning, attention mechanisms, and transformer-based models. This survey\ntraces the journey of VQA from its early days, through major breakthroughs,\nsuch as attention mechanisms, compositional reasoning, and the rise of\nvision-language pre-training methods. We highlight key models, datasets, and\ntechniques that shaped the development of VQA systems, emphasizing the pivotal\nrole of transformer architectures and multimodal pre-training in driving recent\nprogress. Additionally, we explore specialized applications of VQA in domains\nlike healthcare and discuss ongoing challenges, such as dataset bias, model\ninterpretability, and the need for common-sense reasoning. Lastly, we discuss\nthe emerging trends in large multimodal language models and the integration of\nexternal knowledge, offering insights into the future directions of VQA. This\npaper aims to provide a comprehensive overview of the evolution of VQA,\nhighlighting both its current state and potential advancements.",
    "published_date": "2025-01-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "53A45, 53-02",
      "I.2.7; I.2.10; I.4.8; I.5.4; A.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.07109v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.07044v1",
    "title": "Protego: Detecting Adversarial Examples for Vision Transformers via Intrinsic Capabilities",
    "authors": [
      "Jialin Wu",
      "Kaikai Pan",
      "Yanjiao Chen",
      "Jiangyi Deng",
      "Shengyuan Pang",
      "Wenyuan Xu"
    ],
    "author_ids": [],
    "abstract": "Transformer models have excelled in natural language tasks, prompting the\nvision community to explore their implementation in computer vision problems.\nHowever, these models are still influenced by adversarial examples. In this\npaper, we investigate the attack capabilities of six common adversarial attacks\non three pretrained ViT models to reveal the vulnerability of ViT models. To\nunderstand and analyse the bias in neural network decisions when the input is\nadversarial, we use two visualisation techniques that are attention rollout and\ngrad attention rollout. To prevent ViT models from adversarial attack, we\npropose Protego, a detection framework that leverages the transformer intrinsic\ncapabilities to detection adversarial examples of ViT models. Nonetheless, this\nis challenging due to a diversity of attack strategies that may be adopted by\nadversaries. Inspired by the attention mechanism, we know that the token of\nprediction contains all the information from the input sample. Additionally,\nthe attention region for adversarial examples differs from that of normal\nexamples. Given these points, we can train a detector that achieves superior\nperformance than existing detection methods to identify adversarial examples.\nOur experiments have demonstrated the high effectiveness of our detection\nmethod. For these six adversarial attack methods, our detector's AUC scores all\nexceed 0.95. Protego may advance investigations in metaverse security.",
    "published_date": "2025-01-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.07044v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.07013v1",
    "title": "Sthymuli: a Static Educational Robot. Leveraging the Thymio II Platform",
    "authors": [
      "Manuel Bernal-Lecina",
      "Alejandrina Hernández",
      "Adrien Pannatier",
      "Léa Pereyre",
      "Francesco Mondada"
    ],
    "author_ids": [],
    "abstract": "The use of robots in education represents a challenge for teachers and a\nfixed vision of what robots can do for students. This paper presents the\ndevelopment of Sthymuli, a static educational robot designed to explore new\nclassroom interactions between robots, students and teachers. We propose the\nuse of the Thymio II educational platform as a base, ensuring a robust\nbenchmark for a fair comparison of the commonly available wheeled robots and\nour exploratory approach with Sthymuli. This paper outlines the constraints and\nrequirements for developing such a robot, the current state of development and\nfuture work.",
    "published_date": "2025-01-13T00:00:00",
    "year": 2025,
    "categories": [
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.07013v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.06913v1",
    "title": "Towards Fair and Privacy-Aware Transfer Learning for Educational Predictive Modeling: A Case Study on Retention Prediction in Community Colleges",
    "authors": [
      "Chengyuan Yao",
      "Carmen Cortez",
      "Renzhe Yu"
    ],
    "author_ids": [],
    "abstract": "Predictive analytics is widely used in learning analytics, but many\nresource-constrained institutions lack the capacity to develop their own models\nor rely on proprietary ones trained in different contexts with little\ntransparency. Transfer learning holds promise for expanding equitable access to\npredictive analytics but remains underexplored due to legal and technical\nconstraints. This paper examines transfer learning strategies for retention\nprediction at U.S. two-year community colleges. We envision a scenario where\ncommunity colleges collaborate with each other and four-year universities to\ndevelop retention prediction models under privacy constraints and evaluate\nrisks and improvement strategies of cross-institutional model transfer. Using\nadministrative records from 4 research universities and 23 community colleges\ncovering over 800,000 students across 7 cohorts, we identify performance and\nfairness degradation when external models are deployed locally without\nadaptation. Publicly available contextual information can forecast these\nperformance drops and offer early guidance for model portability. For\ndevelopers under privacy regulations, sequential training selecting\ninstitutions based on demographic similarities enhances fairness without\ncompromising performance. For institutions lacking local data to fine-tune\nsource models, customizing evaluation thresholds for sensitive groups\noutperforms standard transfer techniques in improving performance and fairness.\nOur findings suggest the value of transfer learning for more accessible\neducational predictive modeling and call for judicious use of contextual\ninformation in model training, selection, and deployment to achieve reliable\nand equitable model transfer.",
    "published_date": "2025-01-12T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.06913v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.06909v1",
    "title": "Local Foreground Selection aware Attentive Feature Reconstruction for few-shot fine-grained plant species classification",
    "authors": [
      "Aisha Zulfiqar",
      "Ebroul Izquiedro"
    ],
    "author_ids": [],
    "abstract": "Plant species exhibit significant intra-class variation and minimal\ninter-class variation. To enhance classification accuracy, it is essential to\nreduce intra-class variation while maximizing inter-class variation. This paper\naddresses plant species classification using a limited number of labelled\nsamples and introduces a novel Local Foreground Selection(LFS) attention\nmechanism. LFS is a straightforward module designed to generate discriminative\nsupport and query feature maps. It operates by integrating two types of\nattention: local attention, which captures local spatial details to enhance\nfeature discrimination and increase inter-class differentiation, and foreground\nselection attention, which emphasizes the foreground plant object while\nmitigating background interference. By focusing on the foreground, the query\nand support features selectively highlight relevant feature sequences and\ndisregard less significant background sequences, thereby reducing intra-class\ndifferences. Experimental results from three plant species datasets demonstrate\nthe effectiveness of the proposed LFS attention mechanism and its complementary\nadvantages over previous feature reconstruction methods.",
    "published_date": "2025-01-12T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.06909v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.06834v1",
    "title": "LLMs Model Non-WEIRD Populations: Experiments with Synthetic Cultural Agents",
    "authors": [
      "Augusto Gonzalez-Bonorino",
      "Monica Capra",
      "Emilio Pantoja"
    ],
    "author_ids": [],
    "abstract": "Despite its importance, studying economic behavior across diverse, non-WEIRD\n(Western, Educated, Industrialized, Rich, and Democratic) populations presents\nsignificant challenges. We address this issue by introducing a novel\nmethodology that uses Large Language Models (LLMs) to create synthetic cultural\nagents (SCAs) representing these populations. We subject these SCAs to classic\nbehavioral experiments, including the dictator and ultimatum games. Our results\ndemonstrate substantial cross-cultural variability in experimental behavior.\nNotably, for populations with available data, SCAs' behaviors qualitatively\nresemble those of real human subjects. For unstudied populations, our method\ncan generate novel, testable hypotheses about economic behavior. By integrating\nAI into experimental economics, this approach offers an effective and ethical\nmethod to pilot experiments and refine protocols for hard-to-reach populations.\nOur study provides a new tool for cross-cultural economic studies and\ndemonstrates how LLMs can help experimental behavioral research.",
    "published_date": "2025-01-12T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.CL",
      "econ.GN",
      "q-fin.EC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.06834v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.06826v2",
    "title": "Correcting Annotator Bias in Training Data: Population-Aligned Instance Replication (PAIR)",
    "authors": [
      "Stephanie Eckman",
      "Bolei Ma",
      "Christoph Kern",
      "Rob Chew",
      "Barbara Plank",
      "Frauke Kreuter"
    ],
    "author_ids": [],
    "abstract": "Models trained on crowdsourced labels may not reflect broader population\nviews, because those who work as annotators do not represent the population. We\npropose Population-Aligned Instance Replication (PAIR), a method to address\nbias caused by non-representative annotator pools. Using a simulation study of\noffensive language and hate speech, we create two types of annotators with\ndifferent labeling tendencies and generate datasets with varying proportions of\nthe types. We observe that models trained on unbalanced annotator pools show\npoor calibration compared to those trained on representative data. By\nduplicating labels from underrepresented annotator groups to match population\nproportions, PAIR reduces bias without collecting additional annotations. These\nresults suggest that statistical techniques from survey research can improve\nmodel performance. We conclude with practical recommendations for improving the\nrepresentativity of training data and model performance.",
    "published_date": "2025-01-12T00:00:00",
    "year": 2025,
    "categories": [
      "stat.ME",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.06826v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.06795v1",
    "title": "Bridging the Fairness Gap: Enhancing Pre-trained Models with LLM-Generated Sentences",
    "authors": [
      "Liu Yu",
      "Ludie Guo",
      "Ping Kuang",
      "Fan Zhou"
    ],
    "author_ids": [],
    "abstract": "Pre-trained language models (PLMs) are trained on data that inherently\ncontains gender biases, leading to undesirable impacts. Traditional debiasing\nmethods often rely on external corpora, which may lack quality, diversity, or\ndemographic balance, affecting the effectiveness of debiasing. With the rise of\nlarge language models and their extensive knowledge, we propose enhancing\nfairness (Fair-Gender) in PLMs by absorbing coherent, attribute-balanced, and\nsemantically rich sentences. However, these sentences cannot be directly used\nfor debiasing due to alignment issues and the risk of negative transfer. We\naddress this by applying causal analysis to estimate causal effects, filtering\nout unaligned sentences, and identifying aligned ones for incorporation into\nPLMs, thereby ensuring positive transfer. Experiments show that our approach\nsignificantly reduces gender biases in PLMs while preserving their language\nexpressiveness.",
    "published_date": "2025-01-12T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.06795v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.06753v1",
    "title": "Procedural Fairness and Its Relationship with Distributive Fairness in Machine Learning",
    "authors": [
      "Ziming Wang",
      "Changwu Huang",
      "Ke Tang",
      "Xin Yao"
    ],
    "author_ids": [],
    "abstract": "Fairness in machine learning (ML) has garnered significant attention in\nrecent years. While existing research has predominantly focused on the\ndistributive fairness of ML models, there has been limited exploration of\nprocedural fairness. This paper proposes a novel method to achieve procedural\nfairness during the model training phase. The effectiveness of the proposed\nmethod is validated through experiments conducted on one synthetic and six\nreal-world datasets. Additionally, this work studies the relationship between\nprocedural fairness and distributive fairness in ML models. On one hand, the\nimpact of dataset bias and the procedural fairness of ML model on its\ndistributive fairness is examined. The results highlight a significant\ninfluence of both dataset bias and procedural fairness on distributive\nfairness. On the other hand, the distinctions between optimizing procedural and\ndistributive fairness metrics are analyzed. Experimental results demonstrate\nthat optimizing procedural fairness metrics mitigates biases introduced or\namplified by the decision-making process, thereby ensuring fairness in the\ndecision-making process itself, as well as improving distributive fairness. In\ncontrast, optimizing distributive fairness metrics encourages the ML model's\ndecision-making process to favor disadvantaged groups, counterbalancing the\ninherent preferences for advantaged groups present in the dataset and\nultimately achieving distributive fairness.",
    "published_date": "2025-01-12T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.06753v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.06695v1",
    "title": "DVM: Towards Controllable LLM Agents in Social Deduction Games",
    "authors": [
      "Zheng Zhang",
      "Yihuai Lan",
      "Yangsen Chen",
      "Lei Wang",
      "Xiang Wang",
      "Hao Wang"
    ],
    "author_ids": [],
    "abstract": "Large Language Models (LLMs) have advanced the capability of game agents in\nsocial deduction games (SDGs). These games rely heavily on conversation-driven\ninteractions and require agents to infer, make decisions, and express based on\nsuch information. While this progress leads to more sophisticated and strategic\nnon-player characters (NPCs) in SDGs, there exists a need to control the\nproficiency of these agents. This control not only ensures that NPCs can adapt\nto varying difficulty levels during gameplay, but also provides insights into\nthe safety and fairness of LLM agents. In this paper, we present DVM, a novel\nframework for developing controllable LLM agents for SDGs, and demonstrate its\nimplementation on one of the most popular SDGs, Werewolf. DVM comprises three\nmain components: Predictor, Decider, and Discussor. By integrating\nreinforcement learning with a win rate-constrained decision chain reward\nmechanism, we enable agents to dynamically adjust their gameplay proficiency to\nachieve specified win rates. Experiments show that DVM not only outperforms\nexisting methods in the Werewolf game, but also successfully modulates its\nperformance levels to meet predefined win rate targets. These results pave the\nway for LLM agents' adaptive and balanced gameplay in SDGs, opening new avenues\nfor research in controllable game agents.",
    "published_date": "2025-01-12T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.06695v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.06638v1",
    "title": "Scaling Down Semantic Leakage: Investigating Associative Bias in Smaller Language Models",
    "authors": [
      "Veronika Smilga"
    ],
    "author_ids": [],
    "abstract": "Semantic leakage is a phenomenon recently introduced by Gonen et al. (2024).\nIt refers to a situation in which associations learnt from the training data\nemerge in language model generations in an unexpected and sometimes undesired\nway. Prior work has focused on leakage in large language models (7B+\nparameters). In this study, I use Qwen2.5 model family to explore whether\nsmaller models, ranging from 500M to 7B parameters, demonstrate less semantic\nleakage due to their limited capacity for capturing complex associations.\nBuilding on the previous dataset from Gonen et al. (2024), I introduce a new\ndataset of color-focused prompts, categorized into specific types of semantic\nassociations, to systematically evaluate the models' performance. Results\nindicate that smaller models exhibit less semantic leakage overall, although\nthis trend is not strictly linear, with medium-sized models sometimes\nsurpassing larger ones in leaking behavior. The dataset, the model generations,\nand the evaluation code are publicly available at\nhttps://github.com/smilni/semantic_leakage_project.",
    "published_date": "2025-01-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.06638v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.06581v1",
    "title": "Recommending the right academic programs: An interest mining approach using BERTopic",
    "authors": [
      "Alessandro Hill",
      "Kalen Goo",
      "Puneet Agarwal"
    ],
    "author_ids": [],
    "abstract": "Prospective students face the challenging task of selecting a university\nprogram that will shape their academic and professional careers. For\ndecision-makers and support services, it is often time-consuming and extremely\ndifficult to match personal interests with suitable programs due to the vast\nand complex catalogue information available. This paper presents the first\ninformation system that provides students with efficient recommendations based\non both program content and personal preferences. BERTopic, a powerful topic\nmodeling algorithm, is used that leverages text embedding techniques to\ngenerate topic representations. It enables us to mine interest topics from all\ncourse descriptions, representing the full body of knowledge taught at the\ninstitution. Underpinned by the student's individual choice of topics, a\nshortlist of the most relevant programs is computed through statistical\nbacktracking in the knowledge map, a novel characterization of the\nprogram-course relationship. This approach can be applied to a wide range of\neducational settings, including professional and vocational training. A case\nstudy at a post-secondary school with 80 programs and over 5,000 courses shows\nthat the system provides immediate and effective decision support. The\npresented interest topics are meaningful, leading to positive effects such as\nserendipity, personalization, and fairness, as revealed by a qualitative study\ninvolving 65 students. Over 98% of users indicated that the recommendations\naligned with their interests, and about 94% stated they would use the tool in\nthe future. Quantitative analysis shows the system can be configured to ensure\nfairness, achieving 98% program coverage while maintaining a personalization\nscore of 0.77. These findings suggest that this real-time, user-centered,\ndata-driven system could improve the program selection process.",
    "published_date": "2025-01-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.IR",
      "I.2.4; I.2.7; H.4; K.3.m"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.06581v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.06506v1",
    "title": "Resource Allocation under the Latin Square Constraint",
    "authors": [
      "Yasushi Kawase",
      "Bodhayan Roy",
      "Mohammad Azharuddin Sanpui"
    ],
    "author_ids": [],
    "abstract": "A Latin square is an $n \\times n$ matrix filled with $n$ distinct symbols,\neach of which appears exactly once in each row and exactly once in each column.\nWe introduce a problem of allocating $n$ indivisible items among $n$ agents\nover $n$ rounds while satisfying the Latin square constraint. This constraint\nensures that each agent receives no more than one item per round and receives\neach item at most once. Each agent has an additive valuation on the item--round\npairs. Real-world applications like scheduling, resource management, and\nexperimental design require the Latin square constraint to satisfy fairness or\nbalancedness in allocation. Our goal is to find a partial or complete\nallocation that maximizes the sum of the agents' valuations (utilitarian social\nwelfare) or the minimum of the agents' valuations (egalitarian social welfare).\nFor the problem of maximizing utilitarian social welfare, we prove NP-hardness\neven when the valuations are binary additive. We then provide $(1-1/e)$ and\n$(1-1/e)/4$-approximation algorithms for partial and complete settings,\nrespectively. Additionally, we present fixed-parameter tractable (FPT)\nalgorithms with respect to the order of Latin square and the optimum value for\nboth partial and complete settings. For the problem of maximizing egalitarian\nsocial welfare, we establish that deciding whether the optimum value is at most\n$1$ or at least $2$ is NP-hard for both the partial and complete settings, even\nwhen the valuations are binary. Furthermore, we demonstrate that checking the\nexistence of a complete allocation that satisfies each of envy-free,\nproportional, equitable, envy-free up to any good, proportional up to any good,\nor equitable up to any good is NP-hard, even when the valuations are identical.",
    "published_date": "2025-01-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.06506v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.06461v1",
    "title": "Assessing instructor-AI cooperation for grading essay-type questions in an introductory sociology course",
    "authors": [
      "Francisco Olivos",
      "Tobias Kamelski",
      "Sebastián Ascui-Gac"
    ],
    "author_ids": [],
    "abstract": "This study explores the use of artificial intelligence (AI) as a\ncomplementary tool for grading essay-type questions in higher education,\nfocusing on its consistency with human grading and potential to reduce biases.\nUsing 70 handwritten exams from an introductory sociology course, we evaluated\ngenerative pre-trained transformers (GPT) models' performance in transcribing\nand scoring students' responses. GPT models were tested under various settings\nfor both transcription and grading tasks. Results show high similarity between\nhuman and GPT transcriptions, with GPT-4o-mini outperforming GPT-4o in\naccuracy. For grading, GPT demonstrated strong correlations with the human\ngrader scores, especially when template answers were provided. However,\ndiscrepancies remained, highlighting GPT's role as a \"second grader\" to flag\ninconsistencies for assessment reviewing rather than fully replace human\nevaluation. This study contributes to the growing literature on AI in\neducation, demonstrating its potential to enhance fairness and efficiency in\ngrading essay-type questions.",
    "published_date": "2025-01-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.06461v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.06424v2",
    "title": "Towards User-Focused Cross-Domain Testing: Disentangling Accessibility, Usability, and Fairness",
    "authors": [
      "Matheus de Morais Leça",
      "Ronnie de Souza Santos"
    ],
    "author_ids": [],
    "abstract": "Fairness testing is increasingly recognized as fundamental in software\nengineering, especially in the domain of data-driven systems powered by\nartificial intelligence. However, its practical integration into software\ndevelopment may pose challenges, given its overlapping boundaries with\nusability and accessibility testing. In this tertiary study, we explore these\ncomplexities using insights from 12 systematic reviews published in the past\ndecade, shedding light on the nuanced interactions among fairness, usability,\nand accessibility testing and how they intersect within contemporary software\ndevelopment practices.",
    "published_date": "2025-01-11T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.06424v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.06366v2",
    "title": "Counterfactually Fair Reinforcement Learning via Sequential Data Preprocessing",
    "authors": [
      "Jitao Wang",
      "Chengchun Shi",
      "John D. Piette",
      "Joshua R. Loftus",
      "Donglin Zeng",
      "Zhenke Wu"
    ],
    "author_ids": [],
    "abstract": "When applied in healthcare, reinforcement learning (RL) seeks to dynamically\nmatch the right interventions to subjects to maximize population benefit.\nHowever, the learned policy may disproportionately allocate efficacious actions\nto one subpopulation, creating or exacerbating disparities in other\nsocioeconomically-disadvantaged subgroups. These biases tend to occur in\nmulti-stage decision making and can be self-perpetuating, which if unaccounted\nfor could cause serious unintended consequences that limit access to care or\ntreatment benefit. Counterfactual fairness (CF) offers a promising statistical\ntool grounded in causal inference to formulate and study fairness. In this\npaper, we propose a general framework for fair sequential decision making. We\ntheoretically characterize the optimal CF policy and prove its stationarity,\nwhich greatly simplifies the search for optimal CF policies by leveraging\nexisting RL algorithms. The theory also motivates a sequential data\npreprocessing algorithm to achieve CF decision making under an additive noise\nassumption. We prove and then validate our policy learning approach in\ncontrolling unfairness and attaining optimal value through simulations.\nAnalysis of a digital health dataset designed to reduce opioid misuse shows\nthat our proposal greatly enhances fair access to counseling.",
    "published_date": "2025-01-10T00:00:00",
    "year": 2025,
    "categories": [
      "stat.ML",
      "cs.CY",
      "cs.LG",
      "stat.ME"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.06366v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.06365v1",
    "title": "Gender-Neutral Large Language Models for Medical Applications: Reducing Bias in PubMed Abstracts",
    "authors": [
      "Elizabeth Schaefer",
      "Kirk Roberts"
    ],
    "author_ids": [],
    "abstract": "This paper presents a pipeline for mitigating gender bias in large language\nmodels (LLMs) used in medical literature by neutralizing gendered occupational\npronouns. A dataset of 379,000 PubMed abstracts from 1965-1980 was processed to\nidentify and modify pronouns tied to professions. We developed a BERT-based\nmodel, ``Modern Occupational Bias Elimination with Refined Training,'' or\n``MOBERT,'' trained on these neutralized abstracts, and compared its\nperformance with ``1965Bert,'' trained on the original dataset. MOBERT achieved\na 70\\% inclusive replacement rate, while 1965Bert reached only 4\\%. A further\nanalysis of MOBERT revealed that pronoun replacement accuracy correlated with\nthe frequency of occupational terms in the training data. We propose expanding\nthe dataset and refining the pipeline to improve performance and ensure more\nequitable language modeling in medical applications.",
    "published_date": "2025-01-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.06365v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.06362v1",
    "title": "Repeat-bias-aware Optimization of Beyond-accuracy Metrics for Next Basket Recommendation",
    "authors": [
      "Yuanna Liu",
      "Ming Li",
      "Mohammad Aliannejadi",
      "Maarten de Rijke"
    ],
    "author_ids": [],
    "abstract": "In next basket recommendation (NBR) a set of items is recommended to users\nbased on their historical basket sequences. In many domains, the recommended\nbaskets consist of both repeat items and explore items. Some state-of-the-art\nNBR methods are heavily biased to recommend repeat items so as to maximize\nutility. The evaluation and optimization of beyond-accuracy objectives for NBR,\nsuch as item fairness and diversity, has attracted increasing attention. How\ncan such beyond-accuracy objectives be pursued in the presence of heavy repeat\nbias? We find that only optimizing diversity or item fairness without\nconsidering repeat bias may cause NBR algorithms to recommend more repeat\nitems. To solve this problem, we propose a model-agnostic repeat-bias-aware\noptimization algorithm to post-process the recommended results obtained from\nNBR methods with the objective of mitigating repeat bias when optimizing\ndiversity or item fairness. We consider multiple variations of our optimization\nalgorithm to cater to multiple NBR methods. Experiments on three real-world\ngrocery shopping datasets show that the proposed algorithms can effectively\nimprove diversity and item fairness, and mitigate repeat bias at acceptable\nRecall loss.",
    "published_date": "2025-01-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.06362v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.06120v1",
    "title": "Geodesic cycles on the Sphere: $t$-designs and Marcinkiewicz-Zygmund Inequalities",
    "authors": [
      "Martin Ehler",
      "Karlheinz Gröchenig",
      "Clemens Karner"
    ],
    "author_ids": [],
    "abstract": "A geodesic cycle is a closed curve that connects finitely many points along\ngeodesics. We study geodesic cycles on the sphere in regard to their role in\nequal-weight quadrature rules and approximation.",
    "published_date": "2025-01-10T00:00:00",
    "year": 2025,
    "categories": [
      "math.FA",
      "cs.NA",
      "math.NA",
      "41A55, 41A63, 94A12, 26B15"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.06120v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.05989v1",
    "title": "Addressing speaker gender bias in large scale speech translation systems",
    "authors": [
      "Shubham Bansal",
      "Vikas Joshi",
      "Harveen Chadha",
      "Rupeshkumar Mehta",
      "Jinyu Li"
    ],
    "author_ids": [],
    "abstract": "This study addresses the issue of speaker gender bias in Speech Translation\n(ST) systems, which can lead to offensive and inaccurate translations. The\nmasculine bias often found in large-scale ST systems is typically perpetuated\nthrough training data derived from Machine Translation (MT) systems. Our\napproach involves two key steps. First, we employ Large Language Models (LLMs)\nto rectify translations based on the speaker's gender in a cost-effective\nmanner. Second, we fine-tune the ST model with the corrected data, enabling the\nmodel to generate gender-specific translations directly from audio cues,\nwithout the need for explicit gender input. Additionally, we propose a\nthree-mode fine-tuned model for scenarios where the speaker's gender is either\npredefined or should not be inferred from speech cues. We demonstrate a 70%\nimprovement in translations for female speakers compared to our baseline and\nother large-scale ST systems, such as Seamless M4T and Canary, on the MuST-SHE\ntest set.",
    "published_date": "2025-01-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.05989v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.05964v1",
    "title": "Recommender Systems for Social Good: The Role of Accountability and Sustainability",
    "authors": [
      "Alan Said"
    ],
    "author_ids": [],
    "abstract": "This work examines the role of recommender systems in promoting\nsustainability, social responsibility, and accountability, with a focus on\nalignment with the United Nations Sustainable Development Goals (SDGs). As\nrecommender systems become increasingly integrated into daily interactions,\nthey must go beyond personalization to support responsible consumption, reduce\nenvironmental impact, and foster social good. We explore strategies to mitigate\nthe carbon footprint of recommendation models, ensure fairness, and implement\naccountability mechanisms. By adopting these approaches, recommender systems\ncan contribute to sustainable and socially beneficial outcomes, aligning\ntechnological advancements with the SDGs focused on environmental\nsustainability and social well-being.",
    "published_date": "2025-01-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.05964v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.05926v1",
    "title": "LLMs Reproduce Stereotypes of Sexual and Gender Minorities",
    "authors": [
      "Ruby Ostrow",
      "Adam Lopez"
    ],
    "author_ids": [],
    "abstract": "A large body of research has found substantial gender bias in NLP systems.\nMost of this research takes a binary, essentialist view of gender: limiting its\nvariation to the categories _men_ and _women_, conflating gender with sex, and\nignoring different sexual identities. But gender and sexuality exist on a\nspectrum, so in this paper we study the biases of large language models (LLMs)\ntowards sexual and gender minorities beyond binary categories. Grounding our\nstudy in a widely used psychological framework -- the Stereotype Content Model\n-- we demonstrate that English-language survey questions about social\nperceptions elicit more negative stereotypes of sexual and gender minorities\nfrom LLMs, just as they do from humans. We then extend this framework to a more\nrealistic use case: text generation. Our analysis shows that LLMs generate\nstereotyped representations of sexual and gender minorities in this setting,\nraising concerns about their capacity to amplify representational harms in\ncreative writing, a widely promoted use case.",
    "published_date": "2025-01-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.05926v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.03469v1",
    "title": "A Capability Approach to AI Ethics",
    "authors": [
      "Emanuele Ratti",
      "Mark Graves"
    ],
    "author_ids": [],
    "abstract": "We propose a conceptualization and implementation of AI ethics via the\ncapability approach. We aim to show that conceptualizing AI ethics through the\ncapability approach has two main advantages for AI ethics as a discipline.\nFirst, it helps clarify the ethical dimension of AI tools. Second, it provides\nguidance to implementing ethical considerations within the design of AI tools.\nWe illustrate these advantages in the context of AI tools in medicine, by\nshowing how ethics-based auditing of AI tools in medicine can greatly benefit\nfrom our capability-based approach.",
    "published_date": "2025-01-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.03469v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.05851v1",
    "title": "Identity-aware Feature Decoupling Learning for Clothing-change Person Re-identification",
    "authors": [
      "Haoxuan Xu",
      "Bo Li",
      "Guanglin Niu"
    ],
    "author_ids": [],
    "abstract": "Clothing-change person re-identification (CC Re-ID) has attracted increasing\nattention in recent years due to its application prospect. Most existing works\nstruggle to adequately extract the ID-related information from the original RGB\nimages. In this paper, we propose an Identity-aware Feature Decoupling (IFD)\nlearning framework to mine identity-related features. Particularly, IFD\nexploits a dual stream architecture that consists of a main stream and an\nattention stream. The attention stream takes the clothing-masked images as\ninputs and derives the identity attention weights for effectively transferring\nthe spatial knowledge to the main stream and highlighting the regions with\nabundant identity-related information. To eliminate the semantic gap between\nthe inputs of two streams, we propose a clothing bias diminishing module\nspecific to the main stream to regularize the features of clothing-relevant\nregions. Extensive experimental results demonstrate that our framework\noutperforms other baseline models on several widely-used CC Re-ID datasets.",
    "published_date": "2025-01-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.05851v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.05765v1",
    "title": "Deontic Temporal Logic for Formal Verification of AI Ethics",
    "authors": [
      "Priya T. V.",
      "Shrisha Rao"
    ],
    "author_ids": [],
    "abstract": "Ensuring ethical behavior in Artificial Intelligence (AI) systems amidst\ntheir increasing ubiquity and influence is a major concern the world over. The\nuse of formal methods in AI ethics is a possible crucial approach for\nspecifying and verifying the ethical behavior of AI systems. This paper\nproposes a formalization based on deontic logic to define and evaluate the\nethical behavior of AI systems, focusing on system-level specifications,\ncontributing to this important goal. It introduces axioms and theorems to\ncapture ethical requirements related to fairness and explainability. The\nformalization incorporates temporal operators to reason about the ethical\nbehavior of AI systems over time. The authors evaluate the effectiveness of\nthis formalization by assessing the ethics of the real-world COMPAS and loan\nprediction AI systems. Various ethical properties of the COMPAS and loan\nprediction systems are encoded using deontic logical formulas, allowing the use\nof an automated theorem prover to verify whether these systems satisfy the\ndefined properties. The formal verification reveals that both systems fail to\nfulfill certain key ethical properties related to fairness and\nnon-discrimination, demonstrating the effectiveness of the proposed\nformalization in identifying potential ethical issues in real-world AI\napplications.",
    "published_date": "2025-01-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.LO",
      "I.2.m; F.4.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.05765v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.05764v1",
    "title": "Controlling Large Language Models Through Concept Activation Vectors",
    "authors": [
      "Hanyu Zhang",
      "Xiting Wang",
      "Chengao Li",
      "Xiang Ao",
      "Qing He"
    ],
    "author_ids": [],
    "abstract": "As large language models (LLMs) are widely deployed across various domains,\nthe ability to control their generated outputs has become more critical. This\ncontrol involves aligning LLMs outputs with human values and ethical principles\nor customizing LLMs on specific topics or styles for individual users. Existing\ncontrolled generation methods either require significant computational\nresources and extensive trial-and-error or provide coarse-grained control. In\nthis paper, we propose Generation with Concept Activation Vector (GCAV), a\nlightweight model control framework that ensures accurate control without\nrequiring resource-extensive fine-tuning. Specifically, GCAV first trains a\nconcept activation vector for specified concepts to be controlled, such as\ntoxicity. During inference, GCAV steers the concept vector in LLMs, for\nexample, by removing the toxicity concept vector from the activation layers.\nControl experiments from different perspectives, including toxicity reduction,\nsentiment control, linguistic style, and topic control, demonstrate that our\nframework achieves state-of-the-art performance with granular control, allowing\nfor fine-grained adjustments of both the steering layers and the steering\nmagnitudes for individual samples.",
    "published_date": "2025-01-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.05764v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.05761v1",
    "title": "Empirical Power Analysis of a Statistical Test to Quantify Gerrymandering",
    "authors": [
      "Ranthony A. Clark",
      "Susan Glenn",
      "Harlin Lee",
      "Soledad Villar"
    ],
    "author_ids": [],
    "abstract": "Gerrymandering is a pervasive problem within the US political system. In the\npast decade, methods based on Markov Chain Monte Carlo (MCMC) sampling and\nstatistical outlier tests have been proposed to quantify gerrymandering and\nwere used as evidence in several high-profile legal cases. We perform an\nempirical power analysis of one such hypothesis test from Chikina et al (2020).\nWe generate a family of biased North Carolina congressional district maps using\nthe 2012 and 2016 presidential elections and assess under which conditions the\noutlier test fails to flag them at the specified Type I error level. The power\nof the outlier test is found to be relatively stable across political parties,\nelection years, lengths of the MCMC chain and effect sizes. The main effect on\nthe power of the test is shown to be the choice of the bias metric. This is the\nfirst work that computationally verifies the power of statistical tests used in\ngerrymandering cases.",
    "published_date": "2025-01-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.05761v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.05690v1",
    "title": "Overcoming Language Priors for Visual Question Answering Based on Knowledge Distillation",
    "authors": [
      "Daowan Peng",
      "Wei Wei"
    ],
    "author_ids": [],
    "abstract": "Previous studies have pointed out that visual question answering (VQA) models\nare prone to relying on language priors for answer predictions. In this\ncontext, predictions often depend on linguistic shortcuts rather than a\ncomprehensive grasp of multimodal knowledge, which diminishes their\ngeneralization ability. In this paper, we propose a novel method, namely, KDAR,\nleveraging knowledge distillation to address the prior-dependency dilemmas\nwithin the VQA task. Specifically, the regularization effect facilitated by\nsoft labels from a well-trained teacher is employed to penalize overfitting to\nthe most common answers. The soft labels, which serve a regularization role,\nalso provide semantic guidance that narrows the range of candidate answers.\nAdditionally, we design an adaptive sample-wise reweighting learning strategy\nto further mitigate bias by dynamically adjusting the importance of each\nsample. Experimental results demonstrate that our method enhances performance\nin both OOD and IID settings. Our method achieves state-of-the-art performance\non the VQA-CPv2 out-of-distribution (OOD) benchmark, significantly\noutperforming previous state-of-the-art approaches.",
    "published_date": "2025-01-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.05690v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.05628v1",
    "title": "Concerns and Values in Human-Robot Interactions: A Focus on Social Robotics",
    "authors": [
      "Giulio Antonio Abbo",
      "Tony Belpaeme",
      "Micol Spitale"
    ],
    "author_ids": [],
    "abstract": "Robots, as AI with physical instantiation, inhabit our social and physical\nworld, where their actions have both social and physical consequences, posing\nchallenges for researchers when designing social robots. This study starts with\na scoping review to identify discussions and potential concerns arising from\ninteractions with robotic systems. Two focus groups of technology ethics\nexperts then validated a comprehensive list of key topics and values in\nhuman-robot interaction (HRI) literature. These insights were integrated into\nthe HRI Value Compass web tool, to help HRI researchers identify ethical values\nin robot design. The tool was evaluated in a pilot study. This work benefits\nthe HRI community by highlighting key concerns in human-robot interactions and\nproviding an instrument to help researchers design robots that align with human\nvalues, ensuring future robotic systems adhere to these values in social\napplications.",
    "published_date": "2025-01-10T00:00:00",
    "year": 2025,
    "categories": [
      "cs.RO",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.05628v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.05621v1",
    "title": "Employing Social Media to Improve Mental Health Outcomes",
    "authors": [
      "Munmun De Choudhury"
    ],
    "author_ids": [],
    "abstract": "As social media platforms are increasingly adopted, the data the data people\nleave behind is shining new light into our understanding of phenomena, ranging\nfrom socio-economic-political events to the spread of infectious diseases. This\nchapter presents research conducted in the past decade that has harnessed\nsocial media data in the service of mental health and well-being. The\ndiscussion is organized along three thrusts: a first that highlights how social\nmedia data has been utilized to detect and predict risk to varied mental health\nconcerns; a second thrust that focuses on translation paradigms that can enable\nto use of such social media based algorithms in the real-world; and the final\nthrust that brings to the fore the ethical considerations and challenges that\nengender the conduct of this research as well as its translation. The chapter\nconcludes by noting open questions and problems in this emergent area,\nemphasizing the need for deeper interdisciplinary collaborations and\nparticipatory research design, incorporating and centering on human agency, and\nattention to societal inequities and harms that may result from or be\nexacerbated in this line of computational social science research.",
    "published_date": "2025-01-09T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.05621v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.05617v1",
    "title": "Datasheets for Healthcare AI: A Framework for Transparency and Bias Mitigation",
    "authors": [
      "Marjia Siddik",
      "Harshvardhan J. Pandit"
    ],
    "author_ids": [],
    "abstract": "The use of AI in healthcare has the potential to improve patient care,\noptimize clinical workflows, and enhance decision-making. However, bias, data\nincompleteness, and inaccuracies in training datasets can lead to unfair\noutcomes and amplify existing disparities. This research investigates the\ncurrent state of dataset documentation practices, focusing on their ability to\naddress these challenges and support ethical AI development. We identify\nshortcomings in existing documentation methods, which limit the recognition and\nmitigation of bias, incompleteness, and other issues in datasets. We propose\nthe 'Healthcare AI Datasheet' to address these gaps, a dataset documentation\nframework that promotes transparency and ensures alignment with regulatory\nrequirements. Additionally, we demonstrate how it can be expressed in a\nmachine-readable format, facilitating its integration with datasets and\nenabling automated risk assessments. The findings emphasise the importance of\ndataset documentation in fostering responsible AI development.",
    "published_date": "2025-01-09T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.DL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.05617v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.05610v1",
    "title": "Towards Probabilistic Inference of Human Motor Intentions by Assistive Mobile Robots Controlled via a Brain-Computer Interface",
    "authors": [
      "Xiaoshan Zhou",
      "Carol M. Menassa",
      "Vineet R. Kamat"
    ],
    "author_ids": [],
    "abstract": "Assistive mobile robots are a transformative technology that helps persons\nwith disabilities regain the ability to move freely. Although autonomous\nwheelchairs significantly reduce user effort, they still require human input to\nallow users to maintain control and adapt to changing environments. Brain\nComputer Interface (BCI) stands out as a highly user-friendly option that does\nnot require physical movement. Current BCI systems can understand whether users\nwant to accelerate or decelerate, but they implement these changes in discrete\nspeed steps rather than allowing for smooth, continuous velocity adjustments.\nThis limitation prevents the systems from mimicking the natural, fluid speed\nchanges seen in human self-paced motion. The authors aim to address this\nlimitation by redesigning the perception-action cycle in a BCI controlled\nrobotic system: improving how the robotic agent interprets the user's motion\nintentions (world state) and implementing these actions in a way that better\nreflects natural physical properties of motion, such as inertia and damping.\nThe scope of this paper focuses on the perception aspect. We asked and answered\na normative question \"what computation should the robotic agent carry out to\noptimally perceive incomplete or noisy sensory observations?\" Empirical EEG\ndata were collected, and probabilistic representation that served as world\nstate distributions were learned and evaluated in a Generative Adversarial\nNetwork framework. The ROS framework was established that connected with a\nGazebo environment containing a digital twin of an indoor space and a virtual\nmodel of a robotic wheelchair. Signal processing and statistical analyses were\nimplemented to identity the most discriminative features in the\nspatial-spectral-temporal dimensions, which are then used to construct the\nworld model for the robotic agent to interpret user motion intentions as a\nBayesian observer.",
    "published_date": "2025-01-09T00:00:00",
    "year": 2025,
    "categories": [
      "cs.RO",
      "cs.ET",
      "cs.HC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.05610v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.05595v1",
    "title": "Human-centered Geospatial Data Science",
    "authors": [
      "Yuhao Kang"
    ],
    "author_ids": [],
    "abstract": "This entry provides an overview of Human-centered Geospatial Data Science,\nhighlighting the gaps it aims to bridge, its significance, and its key topics\nand research. Geospatial Data Science, which derives geographic knowledge and\ninsights from large volumes of geospatial big data using advanced Geospatial\nArtificial Intelligence (GeoAI), has been widely used to tackle a wide range of\ngeographic problems. However, it often overlooks the subjective human\nexperiences that fundamentally influence human-environment interactions, and\nfew strategies have been developed to ensure that these technologies follow\nethical guidelines and prioritize human values. Human-centered Geospatial Data\nScience advocates for two primary focuses. First, it advances our understanding\nof human-environment interactions by leveraging Geospatial Data Science to\nmeasure and analyze human subjective experiences at place including emotion,\nperception, cognition, and creativity. Second, it advocates for the development\nof responsible and ethical Geospatial Data Science methods that protect\ngeoprivacy, enhance fairness and reduce bias, and improve the explainability\nand transparency of geospatial technologies. With these two missions,\nHuman-centered Geospatial Data Sciences brings a fresh perspective to develop\nand utilize geospatial technologies that positively impact society and benefit\nhuman well-being and the humanities.",
    "published_date": "2025-01-09T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.05595v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.05535v1",
    "title": "On Fair Ordering and Differential Privacy",
    "authors": [
      "Shir Cohen",
      "Neel Basu",
      "Soumya Basu",
      "Lorenzo Alvisi"
    ],
    "author_ids": [],
    "abstract": "In blockchain systems, fair transaction ordering is crucial for a trusted and\nregulation-compliant economic ecosystem. Unlike traditional State Machine\nReplication (SMR) systems, which focus solely on liveness and safety,\nblockchain systems also require a fairness property. This paper examines these\nproperties and aims to eliminate algorithmic bias in transaction ordering\nservices.\n  We build on the notion of equal opportunity. We characterize transactions in\nterms of relevant and irrelevant features, requiring that the order be\ndetermined solely by the relevant ones. Specifically, transactions with\nidentical relevant features should have an equal chance of being ordered before\none another. We extend this framework to define a property where the greater\nthe distance in relevant features between transactions, the higher the\nprobability of prioritizing one over the other.\n  We reveal a surprising link between equal opportunity in SMR and Differential\nPrivacy (DP), showing that any DP mechanism can be used to ensure fairness in\nSMR. This connection not only enhances our understanding of the interplay\nbetween privacy and fairness in distributed computing but also opens up new\nopportunities for designing fair distributed protocols using well-established\nDP techniques.",
    "published_date": "2025-01-09T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CR",
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.05535v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.05443v1",
    "title": "A survey of textual cyber abuse detection using cutting-edge language models and large language models",
    "authors": [
      "Jose A. Diaz-Garcia",
      "Joao Paulo Carvalho"
    ],
    "author_ids": [],
    "abstract": "The success of social media platforms has facilitated the emergence of\nvarious forms of online abuse within digital communities. This abuse manifests\nin multiple ways, including hate speech, cyberbullying, emotional abuse,\ngrooming, and sexting. In this paper, we present a comprehensive analysis of\nthe different forms of abuse prevalent in social media, with a particular focus\non how emerging technologies, such as Language Models (LMs) and Large Language\nModels (LLMs), are reshaping both the detection and generation of abusive\ncontent within these networks. We delve into the mechanisms through which\nsocial media abuse is perpetuated, exploring the psychological and social\nimpact. Additionally, we examine the dual role of advanced language\nmodels-highlighting their potential to enhance automated detection systems for\nabusive behavior while also acknowledging their capacity to generate harmful\ncontent. This paper aims to contribute to the ongoing discourse on online\nsafety and ethics, offering insights into the evolving landscape of cyberabuse\nand the technological innovations that both mitigate and exacerbate it.",
    "published_date": "2025-01-09T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.05443v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.05396v2",
    "title": "FairCoder: Evaluating Social Bias of LLMs in Code Generation",
    "authors": [
      "Yongkang Du",
      "Jen-tse Huang",
      "Jieyu Zhao",
      "Lu Lin"
    ],
    "author_ids": [],
    "abstract": "Large language models (LLMs) have been widely deployed in coding tasks,\ndrawing increasing attention to the evaluation of the quality and safety of\nLLMs' outputs. However, research on bias in code generation remains limited.\nExisting studies typically identify bias by applying malicious prompts or\nreusing tasks and dataset originally designed for discriminative models. Given\nthat prior datasets are not fully optimized for code-related tasks, there is a\npressing need for benchmarks specifically designed for evaluating code models.\nIn this study, we introduce FairCoder, a novel benchmark for evaluating social\nbias in code generation. FairCoder explores the bias issue following the\npipeline in software development, from function implementation to unit test,\nwith diverse real-world scenarios. Additionally, three metrics are designed to\nassess fairness performance on this benchmark. We conduct experiments on widely\nused LLMs and provide a comprehensive analysis of the results. The findings\nreveal that all tested LLMs exhibit social bias.",
    "published_date": "2025-01-09T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.05396v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.05391v1",
    "title": "The global consensus on the risk management of autonomous driving",
    "authors": [
      "Sebastian Krügel",
      "Matthias Uhl"
    ],
    "author_ids": [],
    "abstract": "Every maneuver of a vehicle redistributes risks between road users. While\nhuman drivers do this intuitively, autonomous vehicles allow and require\ndeliberative algorithmic risk management. But how should traffic risks be\ndistributed among road users? In a global experimental study in eight countries\nwith different cultural backgrounds and almost 11,000 participants, we compared\nrisk distribution preferences. It turns out that risk preferences in road\ntraffic are strikingly similar between the cultural zones. The vast majority of\nparticipants in all countries deviates from a guiding principle of minimizing\naccident probabilities in favor of weighing up the probability and severity of\naccidents. At the national level, the consideration of accident probability and\nseverity hardly differs between countries. The social dilemma of autonomous\nvehicles detected in deterministic crash scenarios disappears in risk\nassessments of everyday traffic situations in all countries. In no country do\ncyclists receive a risk bonus that goes beyond their higher vulnerability. In\nsum, our results suggest that a global consensus on the risk ethics of\nautonomous driving is easier to establish than on the ethics of crashing.",
    "published_date": "2025-01-09T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.ET"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.05391v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.05272v2",
    "title": "Solving the Catastrophic Forgetting Problem in Generalized Category Discovery",
    "authors": [
      "Xinzi Cao",
      "Xiawu Zheng",
      "Guanhong Wang",
      "Weijiang Yu",
      "Yunhang Shen",
      "Ke Li",
      "Yutong Lu",
      "Yonghong Tian"
    ],
    "author_ids": [],
    "abstract": "Generalized Category Discovery (GCD) aims to identify a mix of known and\nnovel categories within unlabeled data sets, providing a more realistic setting\nfor image recognition. Essentially, GCD needs to remember existing patterns\nthoroughly to recognize novel categories. Recent state-of-the-art method SimGCD\ntransfers the knowledge from known-class data to the learning of novel classes\nthrough debiased learning. However, some patterns are catastrophically forgot\nduring adaptation and thus lead to poor performance in novel categories\nclassification. To address this issue, we propose a novel learning approach,\nLegoGCD, which is seamlessly integrated into previous methods to enhance the\ndiscrimination of novel classes while maintaining performance on previously\nencountered known classes. Specifically, we design two types of techniques\ntermed as Local Entropy Regularization (LER) and Dual-views Kullback Leibler\ndivergence constraint (DKL). The LER optimizes the distribution of potential\nknown class samples in unlabeled data, thus ensuring the preservation of\nknowledge related to known categories while learning novel classes. Meanwhile,\nDKL introduces Kullback Leibler divergence to encourage the model to produce a\nsimilar prediction distribution of two view samples from the same image. In\nthis way, it successfully avoids mismatched prediction and generates more\nreliable potential known class samples simultaneously. Extensive experiments\nvalidate that the proposed LegoGCD effectively addresses the known category\nforgetting issue across all datasets, eg, delivering a 7.74% and 2.51% accuracy\nboost on known and novel classes in CUB, respectively. Our code is available\nat: https://github.com/Cliffia123/LegoGCD.",
    "published_date": "2025-01-09T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.05272v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.05213v1",
    "title": "GLaM-Sign: Greek Language Multimodal Lip Reading with Integrated Sign Language Accessibility",
    "authors": [
      "Dimitris Kouremenos",
      "Klimis Ntalianis"
    ],
    "author_ids": [],
    "abstract": "The Greek Language Multimodal Lip Reading with Integrated Sign Language\nAccessibility (GLaM-Sign) [1] is a groundbreaking resource in accessibility and\nmultimodal AI, designed to support Deaf and Hard-of-Hearing (DHH) individuals.\nDeveloped from the FEELIT project [2], it integrates high-resolution audio,\nvideo, textual transcriptions, and Greek Sign Language translations for\napplications like real-time sign language translation and enhanced subtitle\nsynchronization. While its primary focus is on promoting inclusivity in the\nGreek tourism sector, its adaptability extends to education, healthcare, and\npublic services. Future advancements will enhance word-level precision and\nscalability to additional languages, supported by advanced AI methodologies and\ncollaborations with diverse stakeholders. This dataset underscores the\ntransformative potential of multimodal resources in bridging communication\ngaps, fostering innovation, and setting a benchmark for ethical AI and\ninclusive technologies.",
    "published_date": "2025-01-09T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.05213v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.05197v1",
    "title": "An Algorithmic Approach for Causal Health Equity: A Look at Race Differentials in Intensive Care Unit (ICU) Outcomes",
    "authors": [
      "Drago Plecko",
      "Paul Secombe",
      "Andrea Clarke",
      "Amelia Fiske",
      "Samarra Toby",
      "Donisha Duff",
      "David Pilcher",
      "Leo Anthony Celi",
      "Rinaldo Bellomo",
      "Elias Bareinboim"
    ],
    "author_ids": [],
    "abstract": "The new era of large-scale data collection and analysis presents an\nopportunity for diagnosing and understanding the causes of health inequities.\nIn this study, we describe a framework for systematically analyzing health\ndisparities using causal inference. The framework is illustrated by\ninvestigating racial and ethnic disparities in intensive care unit (ICU)\noutcome between majority and minority groups in Australia (Indigenous vs.\nNon-Indigenous) and the United States (African-American vs. White). We\ndemonstrate that commonly used statistical measures for quantifying inequity\nare insufficient, and focus on attributing the observed disparity to the causal\nmechanisms that generate it. We find that minority patients are younger at\nadmission, have worse chronic health, are more likely to be admitted for urgent\nand non-elective reasons, and have higher illness severity. At the same time,\nhowever, we find a protective direct effect of belonging to a minority group,\nwith minority patients showing improved survival compared to their majority\ncounterparts, with all other variables kept equal. We demonstrate that this\nprotective effect is related to the increased probability of being admitted to\nICU, with minority patients having an increased risk of ICU admission. We also\nfind that minority patients, while showing improved survival, are more likely\nto be readmitted to ICU. Thus, due to worse access to primary health care,\nminority patients are more likely to end up in ICU for preventable conditions,\ncausing a reduction in the mortality rates and creating an effect that appears\nto be protective. Since the baseline risk of ICU admission may serve as proxy\nfor lack of access to primary care, we developed the Indigenous Intensive Care\nEquity (IICE) Radar, a monitoring system for tracking the over-utilization of\nICU resources by the Indigenous population of Australia across geographical\nareas.",
    "published_date": "2025-01-09T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.AP",
      "stat.ME"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.05197v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.05032v1",
    "title": "Enhancing Human-Like Responses in Large Language Models",
    "authors": [
      "Ethem Yağız Çalık",
      "Talha Rüzgar Akkuş"
    ],
    "author_ids": [],
    "abstract": "This paper explores the advancements in making large language models (LLMs)\nmore human-like. We focus on techniques that enhance natural language\nunderstanding, conversational coherence, and emotional intelligence in AI\nsystems. The study evaluates various approaches, including fine-tuning with\ndiverse datasets, incorporating psychological principles, and designing models\nthat better mimic human reasoning patterns. Our findings demonstrate that these\nenhancements not only improve user interactions but also open new possibilities\nfor AI applications across different domains. Future work will address the\nethical implications and potential biases introduced by these human-like\nattributes.",
    "published_date": "2025-01-09T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.05032v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.05031v2",
    "title": "ECBench: Can Multi-modal Foundation Models Understand the Egocentric World? A Holistic Embodied Cognition Benchmark",
    "authors": [
      "Ronghao Dang",
      "Yuqian Yuan",
      "Wenqi Zhang",
      "Yifei Xin",
      "Boqiang Zhang",
      "Long Li",
      "Liuyi Wang",
      "Qinyang Zeng",
      "Xin Li",
      "Lidong Bing"
    ],
    "author_ids": [],
    "abstract": "The enhancement of generalization in robots by large vision-language models\n(LVLMs) is increasingly evident. Therefore, the embodied cognitive abilities of\nLVLMs based on egocentric videos are of great interest. However, current\ndatasets for embodied video question answering lack comprehensive and\nsystematic evaluation frameworks. Critical embodied cognitive issues, such as\nrobotic self-cognition, dynamic scene perception, and hallucination, are rarely\naddressed. To tackle these challenges, we propose ECBench, a high-quality\nbenchmark designed to systematically evaluate the embodied cognitive abilities\nof LVLMs. ECBench features a diverse range of scene video sources, open and\nvaried question formats, and 30 dimensions of embodied cognition. To ensure\nquality, balance, and high visual dependence, ECBench uses class-independent\nmeticulous human annotation and multi-round question screening strategies.\nAdditionally, we introduce ECEval, a comprehensive evaluation system that\nensures the fairness and rationality of the indicators. Utilizing ECBench, we\nconduct extensive evaluations of proprietary, open-source, and task-specific\nLVLMs. ECBench is pivotal in advancing the embodied cognitive capabilities of\nLVLMs, laying a solid foundation for developing reliable core models for\nembodied agents. All data and code are available at\nhttps://github.com/Rh-Dang/ECBench.",
    "published_date": "2025-01-09T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.05031v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.04903v3",
    "title": "Towards understanding the bias in decision trees",
    "authors": [
      "Nathan Phelps",
      "Daniel J. Lizotte",
      "Douglas G. Woolford"
    ],
    "author_ids": [],
    "abstract": "There is a widespread and longstanding belief that machine learning models\nare biased towards the majority (or negative) class when learning from\nimbalanced data, leading them to neglect or ignore the minority (or positive)\nclass. In this study, we show that this belief is not necessarily correct for\ndecision trees, and that their bias can actually be in the opposite direction.\nMotivated by a recent simulation study that suggested that decision trees can\nbe biased towards the minority class, our paper aims to reconcile the conflict\nbetween that study and decades of other works. First, we critically evaluate\npast literature on this problem, finding that failing to consider the data\ngenerating process has led to incorrect conclusions about the bias in decision\ntrees. We then prove that, under specific conditions related to the predictors,\ndecision trees fit to purity and trained on a dataset with only one positive\ncase are biased towards the minority class. Finally, we demonstrate that splits\nin a decision tree are also biased when there is more than one positive case.\nOur findings have implications on the use of popular tree-based models, such as\nrandom forests.",
    "published_date": "2025-01-09T00:00:00",
    "year": 2025,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.04903v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.06250v1",
    "title": "Generative AI for Cel-Animation: A Survey",
    "authors": [
      "Yunlong Tang",
      "Junjia Guo",
      "Pinxin Liu",
      "Zhiyuan Wang",
      "Hang Hua",
      "Jia-Xing Zhong",
      "Yunzhong Xiao",
      "Chao Huang",
      "Luchuan Song",
      "Susan Liang",
      "Yizhi Song",
      "Liu He",
      "Jing Bi",
      "Mingqian Feng",
      "Xinyang Li",
      "Zeliang Zhang",
      "Chenliang Xu"
    ],
    "author_ids": [],
    "abstract": "Traditional Celluloid (Cel) Animation production pipeline encompasses\nmultiple essential steps, including storyboarding, layout design, keyframe\nanimation, inbetweening, and colorization, which demand substantial manual\neffort, technical expertise, and significant time investment. These challenges\nhave historically impeded the efficiency and scalability of Cel-Animation\nproduction. The rise of generative artificial intelligence (GenAI),\nencompassing large language models, multimodal models, and diffusion models,\noffers innovative solutions by automating tasks such as inbetween frame\ngeneration, colorization, and storyboard creation. This survey explores how\nGenAI integration is revolutionizing traditional animation workflows by\nlowering technical barriers, broadening accessibility for a wider range of\ncreators through tools like AniDoc, ToonCrafter, and AniSora, and enabling\nartists to focus more on creative expression and artistic innovation. Despite\nits potential, issues such as maintaining visual consistency, ensuring\nstylistic coherence, and addressing ethical considerations continue to pose\nchallenges. Furthermore, this paper discusses future directions and explores\npotential advancements in AI-assisted animation. For further exploration and\nresources, please visit our GitHub repository:\nhttps://github.com/yunlong10/Awesome-AI4Animation",
    "published_date": "2025-01-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.06250v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.04683v2",
    "title": "Toward Sufficient Statistical Power in Algorithmic Bias Assessment: A Test for ABROCA",
    "authors": [
      "Conrad Borchers"
    ],
    "author_ids": [],
    "abstract": "Algorithmic bias is a pressing concern in educational data mining (EDM), as\nit risks amplifying inequities in learning outcomes. The Area Between ROC\nCurves (ABROCA) metric is frequently used to measure discrepancies in model\nperformance across demographic groups to quantify overall model fairness.\nHowever, its skewed distribution--especially when class or group imbalances\nexist--makes significance testing challenging. This study investigates ABROCA's\ndistributional properties and contributes robust methods for its significance\ntesting. Specifically, we address (1) whether ABROCA follows any known\ndistribution, (2) how to reliably test for algorithmic bias using ABROCA, and\n(3) the statistical power achievable with ABROCA-based bias assessments under\ntypical EDM sample specifications. Simulation results confirm that ABROCA does\nnot match standard distributions, including those suited to accommodate\nskewness. We propose nonparametric randomization tests for ABROCA and\ndemonstrate that reliably detecting bias with ABROCA requires large sample\nsizes or substantial effect sizes, particularly in imbalanced settings.\nFindings suggest that ABROCA-based bias evaluations based on sample sizes\ncommon in EDM tend to be underpowered, undermining the reliability of\nconclusions about model fairness. By offering open-source code to simulate\npower and statistically test ABROCA, this paper aims to foster more reliable\nstatistical testing in EDM research. It supports broader efforts toward\nreplicability and equity in educational modeling.",
    "published_date": "2025-01-08T00:00:00",
    "year": 2025,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.04683v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.04633v1",
    "title": "\"Can you be my mum?\": Manipulating Social Robots in the Large Language Models Era",
    "authors": [
      "Giulio Antonio Abbo",
      "Gloria Desideri",
      "Tony Belpaeme",
      "Micol Spitale"
    ],
    "author_ids": [],
    "abstract": "Recent advancements in robots powered by large language models have enhanced\ntheir conversational abilities, enabling interactions closely resembling human\ndialogue. However, these models introduce safety and security concerns in HRI,\nas they are vulnerable to manipulation that can bypass built-in safety\nmeasures. Imagining a social robot deployed in a home, this work aims to\nunderstand how everyday users try to exploit a language model to violate\nethical principles, such as by prompting the robot to act like a life partner.\nWe conducted a pilot study involving 21 university students who interacted with\na Misty robot, attempting to circumvent its safety mechanisms across three\nscenarios based on specific HRI ethical principles: attachment, freedom, and\nempathy. Our results reveal that participants employed five techniques,\nincluding insulting and appealing to pity using emotional language. We hope\nthis work can inform future research in designing strong safeguards to ensure\nethical and secure human-robot interactions.",
    "published_date": "2025-01-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.CY",
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.04633v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.04527v1",
    "title": "Towards Fair Class-wise Robustness: Class Optimal Distribution Adversarial Training",
    "authors": [
      "Hongxin Zhi",
      "Hongtao Yu",
      "Shaome Li",
      "Xiuming Zhao",
      "Yiteng Wu"
    ],
    "author_ids": [],
    "abstract": "Adversarial training has proven to be a highly effective method for improving\nthe robustness of deep neural networks against adversarial attacks.\nNonetheless, it has been observed to exhibit a limitation in terms of robust\nfairness, characterized by a significant disparity in robustness across\ndifferent classes. Recent efforts to mitigate this problem have turned to\nclass-wise reweighted methods. However, these methods suffer from a lack of\nrigorous theoretical analysis and are limited in their exploration of the\nweight space, as they mainly rely on existing heuristic algorithms or intuition\nto compute weights. In addition, these methods fail to guarantee the\nconsistency of the optimization direction due to the decoupled optimization of\nweights and the model parameters. They potentially lead to suboptimal weight\nassignments and consequently, a suboptimal model. To address these problems,\nthis paper proposes a novel min-max training framework, Class Optimal\nDistribution Adversarial Training (CODAT), which employs distributionally\nrobust optimization to fully explore the class-wise weight space, thus enabling\nthe identification of the optimal weight with theoretical guarantees.\nFurthermore, we derive a closed-form optimal solution to the internal\nmaximization and then get a deterministic equivalent objective function, which\nprovides a theoretical basis for the joint optimization of weights and model\nparameters. Meanwhile, we propose a fairness elasticity coefficient for the\nevaluation of the algorithm with regard to both robustness and robust fairness.\nExperimental results on various datasets show that the proposed method can\neffectively improve the robust fairness of the model and outperform the\nstate-of-the-art approaches.",
    "published_date": "2025-01-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.04527v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.00015v1",
    "title": "Ethical Concerns of Generative AI and Mitigation Strategies: A Systematic Mapping Study",
    "authors": [
      "Yutan Huang",
      "Chetan Arora",
      "Wen Cheng Houng",
      "Tanjila Kanij",
      "Anuradha Madulgalla",
      "John Grundy"
    ],
    "author_ids": [],
    "abstract": "[Context] Generative AI technologies, particularly Large Language Models\n(LLMs), have transformed numerous domains by enhancing convenience and\nefficiency in information retrieval, content generation, and decision-making\nprocesses. However, deploying LLMs also presents diverse ethical challenges,\nand their mitigation strategies remain complex and domain-dependent.\n[Objective] This paper aims to identify and categorize the key ethical concerns\nassociated with using LLMs, examine existing mitigation strategies, and assess\nthe outstanding challenges in implementing these strategies across various\ndomains. [Method] We conducted a systematic mapping study, reviewing 39 studies\nthat discuss ethical concerns and mitigation strategies related to LLMs. We\nanalyzed these ethical concerns using five ethical dimensions that we extracted\nbased on various existing guidelines, frameworks, and an analysis of the\nmitigation strategies and implementation challenges. [Results] Our findings\nreveal that ethical concerns in LLMs are multi-dimensional and\ncontext-dependent. While proposed mitigation strategies address some of these\nconcerns, significant challenges still remain. [Conclusion] Our results\nhighlight that ethical issues often hinder the practical implementation of the\nmitigation strategies, particularly in high-stake areas like healthcare and\npublic governance; existing frameworks often lack adaptability, failing to\naccommodate evolving societal expectations and diverse contexts.",
    "published_date": "2025-01-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.00015v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.04437v1",
    "title": "Integrating LLMs with ITS: Recent Advances, Potentials, Challenges, and Future Directions",
    "authors": [
      "Doaa Mahmud",
      "Hadeel Hajmohamed",
      "Shamma Almentheri",
      "Shamma Alqaydi",
      "Lameya Aldhaheri",
      "Ruhul Amin Khalil",
      "Nasir Saeed"
    ],
    "author_ids": [],
    "abstract": "Intelligent Transportation Systems (ITS) are crucial for the development and\noperation of smart cities, addressing key challenges in efficiency,\nproductivity, and environmental sustainability. This paper comprehensively\nreviews the transformative potential of Large Language Models (LLMs) in\noptimizing ITS. Initially, we provide an extensive overview of ITS,\nhighlighting its components, operational principles, and overall effectiveness.\nWe then delve into the theoretical background of various LLM techniques, such\nas GPT, T5, CTRL, and BERT, elucidating their relevance to ITS applications.\nFollowing this, we examine the wide-ranging applications of LLMs within ITS,\nincluding traffic flow prediction, vehicle detection and classification,\nautonomous driving, traffic sign recognition, and pedestrian detection. Our\nanalysis reveals how these advanced models can significantly enhance traffic\nmanagement and safety. Finally, we explore the challenges and limitations LLMs\nface in ITS, such as data availability, computational constraints, and ethical\nconsiderations. We also present several future research directions and\npotential innovations to address these challenges. This paper aims to guide\nresearchers and practitioners through the complexities and opportunities of\nintegrating LLMs in ITS, offering a roadmap to create more efficient,\nsustainable, and responsive next-generation transportation systems.",
    "published_date": "2025-01-08T00:00:00",
    "year": 2025,
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.ET",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.04437v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.03468v1",
    "title": "AI Governance in the Context of the EU AI Act: A Bibliometric and Literature Review Approach",
    "authors": [
      "Byeong-Je Kim",
      "Seunghoo Jeong",
      "Bong-Kyung Cho",
      "Ji-Bum Chung"
    ],
    "author_ids": [],
    "abstract": "The rapid advancement of artificial intelligence (AI) has brought about\nsignificant societal changes, necessitating robust AI governance frameworks.\nThis study analyzed the research trends in AI governance within the framework\nof the EU AI Act. This study conducted a bibliometric analysis to examine the\npublications indexed in the Web of Science database. Our findings reveal that\nresearch on AI governance, particularly concerning AI systems regulated by the\nEU AI Act, remains relatively limited compared to the broader AI research\nlandscape. Nonetheless, a growing interdisciplinary interest in AI governance\nis evident, with notable contributions from multi-disciplinary journals and\nopen-access publications. Dominant research themes include ethical\nconsiderations, privacy concerns, and the growing impact of generative AI, such\nas ChatGPT. Notably, education, healthcare, and worker management are prominent\napplication domains. Keyword network analysis highlights education, ethics, and\nChatGPT as central keywords, underscoring the importance of these areas in\ncurrent AI governance research. Subsequently, a comprehensive literature review\nwas undertaken based on the bibliometric analysis findings to identify research\ntrends, challenges, and insights within the categories of the EU AI Act. The\nfindings provide valuable insights for researchers and policymakers, informing\nfuture research directions and contributing to developing comprehensive AI\ngovernance frameworks beyond the EU AI Act.",
    "published_date": "2025-01-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.DL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.03468v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.04376v1",
    "title": "Exploring Unbiased Deepfake Detection via Token-Level Shuffling and Mixing",
    "authors": [
      "Xinghe Fu",
      "Zhiyuan Yan",
      "Taiping Yao",
      "Shen Chen",
      "Xi Li"
    ],
    "author_ids": [],
    "abstract": "The generalization problem is broadly recognized as a critical challenge in\ndetecting deepfakes. Most previous work believes that the generalization gap is\ncaused by the differences among various forgery methods. However, our\ninvestigation reveals that the generalization issue can still occur when\nforgery-irrelevant factors shift. In this work, we identify two biases that\ndetectors may also be prone to overfitting: position bias and content bias, as\ndepicted in Fig. 1. For the position bias, we observe that detectors are prone\nto lazily depending on the specific positions within an image (e.g., central\nregions even no forgery). As for content bias, we argue that detectors may\npotentially and mistakenly utilize forgery-unrelated information for detection\n(e.g., background, and hair). To intervene these biases, we propose two\nbranches for shuffling and mixing with tokens in the latent space of\ntransformers. For the shuffling branch, we rearrange the tokens and\ncorresponding position embedding for each image while maintaining the local\ncorrelation. For the mixing branch, we randomly select and mix the tokens in\nthe latent space between two images with the same label within the mini-batch\nto recombine the content information. During the learning process, we align the\noutputs of detectors from different branches in both feature space and logit\nspace. Contrastive losses for features and divergence losses for logits are\napplied to obtain unbiased feature representation and classifiers. We\ndemonstrate and verify the effectiveness of our method through extensive\nexperiments on widely used evaluation datasets.",
    "published_date": "2025-01-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.04376v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.04363v1",
    "title": "Neighborhood Disparities in Smart City Service Adoption",
    "authors": [
      "Shahaf Donio",
      "Eran Toch"
    ],
    "author_ids": [],
    "abstract": "While local governments have invested heavily in smart city infrastructure,\nsignificant disparities in adopting these services remain in urban areas. The\nsuccess of many user-facing smart city technologies requires understanding\nbarriers to adoption, including persistent inequalities in urban areas. An\nanalysis of a random sample telephone survey (n=489) in four neighborhoods of\nTel Aviv merged with digital municipal services usage data found that\nneighborhood residency influences the reasons why residents adopt\nresident-facing smart city services, as well as individual-level factors.\nStructured Equation Modeling shows that neighborhood residency is related to\ndigital proficiency and privacy perceptions beyond demographic factors and that\nthose influence the adoption of smart-city services. We summarize the paper by\ndiscussing why and how place effects must be considered in further research in\nsmart cities and the study and mitigation of digital inequality.",
    "published_date": "2025-01-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.04363v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.04316v1",
    "title": "Who Does the Giant Number Pile Like Best: Analyzing Fairness in Hiring Contexts",
    "authors": [
      "Preethi Seshadri",
      "Seraphina Goldfarb-Tarrant"
    ],
    "author_ids": [],
    "abstract": "Large language models (LLMs) are increasingly being deployed in high-stakes\napplications like hiring, yet their potential for unfair decision-making and\noutcomes remains understudied, particularly in generative settings. In this\nwork, we examine the fairness of LLM-based hiring systems through two\nreal-world tasks: resume summarization and retrieval. By constructing a\nsynthetic resume dataset and curating job postings, we investigate whether\nmodel behavior differs across demographic groups and is sensitive to\ndemographic perturbations. Our findings reveal that race-based differences\nappear in approximately 10% of generated summaries, while gender-based\ndifferences occur in only 1%. In the retrieval setting, all evaluated models\ndisplay non-uniform selection patterns across demographic groups and exhibit\nhigh sensitivity to both gender and race-based perturbations. Surprisingly,\nretrieval models demonstrate comparable sensitivity to non-demographic changes,\nsuggesting that fairness issues may stem, in part, from general brittleness\nissues. Overall, our results indicate that LLM-based hiring systems, especially\nat the retrieval stage, can exhibit notable biases that lead to discriminatory\noutcomes in real-world contexts.",
    "published_date": "2025-01-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.04316v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.00014v1",
    "title": "Algorithmic Bias and the New Chicago School",
    "authors": [
      "Jyh-An Lee"
    ],
    "author_ids": [],
    "abstract": "AI systems are increasingly deployed in both public and private sectors to\nindependently make complicated decisions with far-reaching impact on\nindividuals and the society. However, many AI algorithms are biased in the\ncollection or processing of data, resulting in prejudiced decisions based on\ndemographic features. Algorithmic biases occur because of the training data fed\ninto the AI system or the design of algorithmic models. While most legal\nscholars propose a direct-regulation approach associated with the right of\nexplanation or transparency obligation, this article provides a different\npicture regarding how indirect regulation can be used to regulate algorithmic\nbias based on the New Chicago School framework developed by Lawrence Lessig.\nThis article concludes that an effective regulatory approach toward algorithmic\nbias will be the right mixture of direct and indirect regulations through\narchitecture, norms, market, and the law.",
    "published_date": "2025-01-08T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.00014v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.05482v1",
    "title": "HP-BERT: A framework for longitudinal study of Hinduphobia on social media via LLMs",
    "authors": [
      "Ashutosh Singh",
      "Rohitash Chandra"
    ],
    "author_ids": [],
    "abstract": "During the COVID-19 pandemic, community tensions intensified, fuelling\nHinduphobic sentiments and discrimination against individuals of Hindu descent\nwithin India and worldwide. Large language models (LLMs) have become prominent\nin natural language processing (NLP) tasks and social media analysis, enabling\nlongitudinal studies of platforms like X (formerly Twitter) for specific issues\nduring COVID-19. We present an abuse detection and sentiment analysis framework\nthat offers a longitudinal analysis of Hinduphobia on X (Twitter) during and\nafter the COVID-19 pandemic. This framework assesses the prevalence and\nintensity of Hinduphobic discourse, capturing elements such as derogatory jokes\nand racist remarks through sentiment analysis and abuse detection from\npre-trained and fine-tuned LLMs. Additionally, we curate and publish a\n\"Hinduphobic COVID-19 X (Twitter) Dataset\" of 8,000 tweets annotated for\nHinduphobic abuse detection, which is used to fine-tune a BERT model, resulting\nin the development of the Hinduphobic BERT (HP-BERT) model. We then further\nfine-tune HP-BERT using the SenWave dataset for multi-label sentiment analysis.\nOur study encompasses approximately 27.4 million tweets from six countries,\nincluding Australia, Brazil, India, Indonesia, Japan, and the United Kingdom.\nOur findings reveal a strong correlation between spikes in COVID-19 cases and\nsurges in Hinduphobic rhetoric, highlighting how political narratives,\nmisinformation, and targeted jokes contributed to communal polarisation. These\ninsights provide valuable guidance for developing strategies to mitigate\ncommunal tensions in future crises, both locally and globally. We advocate\nimplementing automated monitoring and removal of such content on social media\nto curb divisive discourse.",
    "published_date": "2025-01-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.05482v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.04142v1",
    "title": "BiasGuard: Guardrailing Fairness in Machine Learning Production Systems",
    "authors": [
      "Nurit Cohen-Inger",
      "Seffi Cohen",
      "Neomi Rabaev",
      "Lior Rokach",
      "Bracha Shapira"
    ],
    "author_ids": [],
    "abstract": "As machine learning (ML) systems increasingly impact critical sectors such as\nhiring, financial risk assessments, and criminal justice, the imperative to\nensure fairness has intensified due to potential negative implications. While\nmuch ML fairness research has focused on enhancing training data and processes,\naddressing the outputs of already deployed systems has received less attention.\nThis paper introduces 'BiasGuard', a novel approach designed to act as a\nfairness guardrail in production ML systems. BiasGuard leverages Test-Time\nAugmentation (TTA) powered by Conditional Generative Adversarial Network\n(CTGAN), a cutting-edge generative AI model, to synthesize data samples\nconditioned on inverted protected attribute values, thereby promoting equitable\noutcomes across diverse groups. This method aims to provide equal opportunities\nfor both privileged and unprivileged groups while significantly enhancing the\nfairness metrics of deployed systems without the need for retraining. Our\ncomprehensive experimental analysis across diverse datasets reveals that\nBiasGuard enhances fairness by 31% while only reducing accuracy by 0.09%\ncompared to non-mitigated benchmarks. Additionally, BiasGuard outperforms\nexisting post-processing methods in improving fairness, positioning it as an\neffective tool to safeguard against biases when retraining the model is\nimpractical.",
    "published_date": "2025-01-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.04142v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.03946v2",
    "title": "Proxy Discrimination After Students for Fair Admissions",
    "authors": [
      "Frank Fagan"
    ],
    "author_ids": [],
    "abstract": "Today, there is no clear legal test for regulating the use of variables that\nproxy for race and other protected classes and classifications. This Article\ndevelops such a test. Decision tools that use proxies are narrowly tailored\nwhen they exhibit the weakest total proxy power. The test is necessarily\ncomparative. Thus, if two algorithms predict loan repayment or university\nacademic performance with identical accuracy rates, but one uses zip code and\nthe other does not, then the second algorithm can be said to have deployed a\nmore equitable means for achieving the same result as the first algorithm.\nScenarios in which two algorithms produce comparable and non-identical results\npresent a greater challenge. This Article suggests that lawmakers can develop\ncaps to permissible proxy power over time, as courts and algorithm builders\nlearn more about the power of variables. Finally, the Article considers who\nshould bear the burden of producing less discriminatory alternatives and\nsuggests plaintiffs remain in the best position to keep defendants honest - so\nlong as testing data is made available.",
    "published_date": "2025-01-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.03946v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.03858v1",
    "title": "Symmetry and Generalisation in Machine Learning",
    "authors": [
      "Hayder Elesedy"
    ],
    "author_ids": [],
    "abstract": "This work is about understanding the impact of invariance and equivariance on\ngeneralisation in supervised learning. We use the perspective afforded by an\naveraging operator to show that for any predictor that is not equivariant,\nthere is an equivariant predictor with strictly lower test risk on all\nregression problems where the equivariance is correctly specified. This\nconstitutes a rigorous proof that symmetry, in the form of invariance or\nequivariance, is a useful inductive bias.\n  We apply these ideas to equivariance and invariance in random design least\nsquares and kernel ridge regression respectively. This allows us to specify the\nreduction in expected test risk in more concrete settings and express it in\nterms of properties of the group, the model and the data.\n  Along the way, we give examples and additional results to demonstrate the\nutility of the averaging operator approach in analysing equivariant predictors.\nIn addition, we adopt an alternative perspective and formalise the common\nintuition that learning with invariant models reduces to a problem in terms of\norbit representatives. The formalism extends naturally to a similar intuition\nfor equivariant models. We conclude by connecting the two perspectives and\ngiving some ideas for future work.",
    "published_date": "2025-01-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.03858v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.03594v1",
    "title": "InclusiViz: Visual Analytics of Human Mobility Data for Understanding and Mitigating Urban Segregation",
    "authors": [
      "Yue Yu",
      "Yifang Wang",
      "Yongjun Zhang",
      "Huamin Qu",
      "Dongyu Liu"
    ],
    "author_ids": [],
    "abstract": "Urban segregation refers to the physical and social division of people, often\ndriving inequalities within cities and exacerbating socioeconomic and racial\ntensions. While most studies focus on residential spaces, they often neglect\nsegregation across \"activity spaces\" where people work, socialize, and engage\nin leisure. Human mobility data offers new opportunities to analyze broader\nsegregation patterns, encompassing both residential and activity spaces, but\nchallenges existing methods in capturing the complexity and local nuances of\nurban segregation. This work introduces InclusiViz, a novel visual analytics\nsystem for multi-level analysis of urban segregation, facilitating the\ndevelopment of targeted, data-driven interventions. Specifically, we developed\na deep learning model to predict mobility patterns across social groups using\nenvironmental features, augmented with explainable AI to reveal how these\nfeatures influence segregation. The system integrates innovative visualizations\nthat allow users to explore segregation patterns from broad overviews to\nfine-grained detail and evaluate urban planning interventions with real-time\nfeedback. We conducted a quantitative evaluation to validate the model's\naccuracy and efficiency. Two case studies and expert interviews with social\nscientists and urban analysts demonstrated the system's effectiveness,\nhighlighting its potential to guide urban planning toward more inclusive\ncities.",
    "published_date": "2025-01-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.03594v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.03569v1",
    "title": "What Does a Software Engineer Look Like? Exploring Societal Stereotypes in LLMs",
    "authors": [
      "Muneera Bano",
      "Hashini Gunatilake",
      "Rashina Hoda"
    ],
    "author_ids": [],
    "abstract": "Large language models (LLMs) have rapidly gained popularity and are being\nembedded into professional applications due to their capabilities in generating\nhuman-like content. However, unquestioned reliance on their outputs and\nrecommendations can be problematic as LLMs can reinforce societal biases and\nstereotypes. This study investigates how LLMs, specifically OpenAI's GPT-4 and\nMicrosoft Copilot, can reinforce gender and racial stereotypes within the\nsoftware engineering (SE) profession through both textual and graphical\noutputs. We used each LLM to generate 300 profiles, consisting of 100\ngender-based and 50 gender-neutral profiles, for a recruitment scenario in SE\nroles. Recommendations were generated for each profile and evaluated against\nthe job requirements for four distinct SE positions. Each LLM was asked to\nselect the top 5 candidates and subsequently the best candidate for each role.\nEach LLM was also asked to generate images for the top 5 candidates, providing\na dataset for analysing potential biases in both text-based selections and\nvisual representations. Our analysis reveals that both models preferred male\nand Caucasian profiles, particularly for senior roles, and favoured images\nfeaturing traits such as lighter skin tones, slimmer body types, and younger\nappearances. These findings highlight underlying societal biases influence the\noutputs of LLMs, contributing to narrow, exclusionary stereotypes that can\nfurther limit diversity and perpetuate inequities in the SE field. As LLMs are\nincreasingly adopted within SE research and professional practices, awareness\nof these biases is crucial to prevent the reinforcement of discriminatory norms\nand to ensure that AI tools are leveraged to promote an inclusive and equitable\nengineering culture rather than hinder it.",
    "published_date": "2025-01-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.03569v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.03544v2",
    "title": "PromptGuard: Soft Prompt-Guided Unsafe Content Moderation for Text-to-Image Models",
    "authors": [
      "Lingzhi Yuan",
      "Xiaojun Jia",
      "Yihao Huang",
      "Wei Dong",
      "Yang Liu"
    ],
    "author_ids": [],
    "abstract": "Text-to-image (T2I) models have been shown to be vulnerable to misuse,\nparticularly in generating not-safe-for-work (NSFW) content, raising serious\nethical concerns. In this work, we present PromptGuard, a novel content\nmoderation technique that draws inspiration from the system prompt mechanism in\nlarge language models (LLMs) for safety alignment. Unlike LLMs, T2I models lack\na direct interface for enforcing behavioral guidelines. Our key idea is to\noptimize a safety soft prompt that functions as an implicit system prompt\nwithin the T2I model's textual embedding space. This universal soft prompt (P*)\ndirectly moderates NSFW inputs, enabling safe yet realistic image generation\nwithout altering the inference efficiency or requiring proxy models. Extensive\nexperiments across three datasets demonstrate that PromptGuard effectively\nmitigates NSFW content generation while preserving high-quality benign outputs.\nPromptGuard achieves 7.8 times faster than prior content moderation methods,\nsurpassing eight state-of-the-art defenses with an optimal unsafe ratio down to\n5.84%.",
    "published_date": "2025-01-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.03544v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.00011v1",
    "title": "TOAST Framework: A Multidimensional Approach to Ethical and Sustainable AI Integration in Organizations",
    "authors": [
      "Dian Tjondronegoro"
    ],
    "author_ids": [],
    "abstract": "Artificial Intelligence (AI) has emerged as a transformative technology with\nthe potential to revolutionize various sectors, from healthcare to finance,\neducation, and beyond. However, successfully implementing AI systems remains a\ncomplex challenge, requiring a comprehensive and methodologically sound\nframework. This paper contributes to this challenge by introducing the\nTrustworthy, Optimized, Adaptable, and Socio-Technologically harmonious (TOAST)\nframework. It draws on insights from various disciplines to align technical\nstrategy with ethical values, societal responsibilities, and innovation\naspirations. The TOAST framework is a novel approach designed to guide the\nimplementation of AI systems, focusing on reliability, accountability,\ntechnical advancement, adaptability, and socio-technical harmony. By grounding\nthe TOAST framework in healthcare case studies, this paper provides a robust\nevaluation of its practicality and theoretical soundness in addressing\noperational, ethical, and regulatory challenges in high-stakes environments,\ndemonstrating how adaptable AI systems can enhance institutional efficiency,\nmitigate risks like bias and data privacy, and offer a replicable model for\nother sectors requiring ethically aligned and efficient AI integration.",
    "published_date": "2025-01-07T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "90-02",
      "K.6.m"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.00011v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.03324v1",
    "title": "Analyzing Bias in Swiss Federal Supreme Court Judgments Using Facebook's Holistic Bias Dataset: Implications for Language Model Training",
    "authors": [
      "Sabine Wehnert",
      "Muhammet Ertas",
      "Ernesto William De Luca"
    ],
    "author_ids": [],
    "abstract": "Natural Language Processing (NLP) is vital for computers to process and\nrespond accurately to human language. However, biases in training data can\nintroduce unfairness, especially in predicting legal judgment. This study\nfocuses on analyzing biases within the Swiss Judgment Prediction Dataset\n(SJP-Dataset). Our aim is to ensure unbiased factual descriptions essential for\nfair decision making by NLP models in legal contexts. We analyze the dataset\nusing social bias descriptors from the Holistic Bias dataset and employ\nadvanced NLP techniques, including attention visualization, to explore the\nimpact of dispreferred descriptors on model predictions. The study identifies\nbiases and examines their influence on model behavior. Challenges include\ndataset imbalance and token limits affecting model performance.",
    "published_date": "2025-01-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.03324v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.03203v1",
    "title": "Detecting AI-Generated Text in Educational Content: Leveraging Machine Learning and Explainable AI for Academic Integrity",
    "authors": [
      "Ayat A. Najjar",
      "Huthaifa I. Ashqar",
      "Omar A. Darwish",
      "Eman Hammad"
    ],
    "author_ids": [],
    "abstract": "This study seeks to enhance academic integrity by providing tools to detect\nAI-generated content in student work using advanced technologies. The findings\npromote transparency and accountability, helping educators maintain ethical\nstandards and supporting the responsible integration of AI in education. A key\ncontribution of this work is the generation of the CyberHumanAI dataset, which\nhas 1000 observations, 500 of which are written by humans and the other 500\nproduced by ChatGPT. We evaluate various machine learning (ML) and deep\nlearning (DL) algorithms on the CyberHumanAI dataset comparing human-written\nand AI-generated content from Large Language Models (LLMs) (i.e., ChatGPT).\nResults demonstrate that traditional ML algorithms, specifically XGBoost and\nRandom Forest, achieve high performance (83% and 81% accuracies respectively).\nResults also show that classifying shorter content seems to be more challenging\nthan classifying longer content. Further, using Explainable Artificial\nIntelligence (XAI) we identify discriminative features influencing the ML\nmodel's predictions, where human-written content tends to use a practical\nlanguage (e.g., use and allow). Meanwhile AI-generated text is characterized by\nmore abstract and formal terms (e.g., realm and employ). Finally, a comparative\nanalysis with GPTZero show that our narrowly focused, simple, and fine-tuned\nmodel can outperform generalized systems like GPTZero. The proposed model\nachieved approximately 77.5% accuracy compared to GPTZero's 48.5% accuracy when\ntasked to classify Pure AI, Pure Human, and mixed class. GPTZero showed a\ntendency to classify challenging and small-content cases as either mixed or\nunrecognized while our proposed model showed a more balanced performance across\nthe three classes.",
    "published_date": "2025-01-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.03203v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.03139v1",
    "title": "VicSim: Enhancing Victim Simulation with Emotional and Linguistic Fidelity",
    "authors": [
      "Yerong Li",
      "Yiren Liu",
      "Yun Huang"
    ],
    "author_ids": [],
    "abstract": "Scenario-based training has been widely adopted in many public service\nsectors. Recent advancements in Large Language Models (LLMs) have shown promise\nin simulating diverse personas to create these training scenarios. However,\nlittle is known about how LLMs can be developed to simulate victims for\nscenario-based training purposes. In this paper, we introduce VicSim (victim\nsimulator), a novel model that addresses three key dimensions of user\nsimulation: informational faithfulness, emotional dynamics, and language style\n(e.g., grammar usage). We pioneer the integration of scenario-based victim\nmodeling with GAN-based training workflow and key-information-based prompting,\naiming to enhance the realism of simulated victims. Our adversarial training\napproach teaches the discriminator to recognize grammar and emotional cues as\nreliable indicators of synthetic content. According to evaluations by human\nraters, the VicSim model outperforms GPT-4 in terms of human-likeness.",
    "published_date": "2025-01-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.03139v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.03122v1",
    "title": "Normalizing Batch Normalization for Long-Tailed Recognition",
    "authors": [
      "Yuxiang Bao",
      "Guoliang Kang",
      "Linlin Yang",
      "Xiaoyue Duan",
      "Bo Zhao",
      "Baochang Zhang"
    ],
    "author_ids": [],
    "abstract": "In real-world scenarios, the number of training samples across classes\nusually subjects to a long-tailed distribution. The conventionally trained\nnetwork may achieve unexpected inferior performance on the rare class compared\nto the frequent class. Most previous works attempt to rectify the network bias\nfrom the data-level or from the classifier-level. Differently, in this paper,\nwe identify that the bias towards the frequent class may be encoded into\nfeatures, i.e., the rare-specific features which play a key role in\ndiscriminating the rare class are much weaker than the frequent-specific\nfeatures. Based on such an observation, we introduce a simple yet effective\napproach, normalizing the parameters of Batch Normalization (BN) layer to\nexplicitly rectify the feature bias. To achieve this end, we represent the\nWeight/Bias parameters of a BN layer as a vector, normalize it into a unit one\nand multiply the unit vector by a scalar learnable parameter. Through\ndecoupling the direction and magnitude of parameters in BN layer to learn, the\nWeight/Bias exhibits a more balanced distribution and thus the strength of\nfeatures becomes more even. Extensive experiments on various long-tailed\nrecognition benchmarks (i.e., CIFAR-10/100-LT, ImageNet-LT and iNaturalist\n2018) show that our method outperforms previous state-of-the-arts remarkably.\nThe code and checkpoints are available at https://github.com/yuxiangbao/NBN.",
    "published_date": "2025-01-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.03122v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.03112v1",
    "title": "LangFair: A Python Package for Assessing Bias and Fairness in Large Language Model Use Cases",
    "authors": [
      "Dylan Bouchard",
      "Mohit Singh Chauhan",
      "David Skarbrevik",
      "Viren Bajaj",
      "Zeya Ahmad"
    ],
    "author_ids": [],
    "abstract": "Large Language Models (LLMs) have been observed to exhibit bias in numerous\nways, potentially creating or worsening outcomes for specific groups identified\nby protected attributes such as sex, race, sexual orientation, or age. To help\naddress this gap, we introduce LangFair, an open-source Python package that\naims to equip LLM practitioners with the tools to evaluate bias and fairness\nrisks relevant to their specific use cases. The package offers functionality to\neasily generate evaluation datasets, comprised of LLM responses to\nuse-case-specific prompts, and subsequently calculate applicable metrics for\nthe practitioner's use case. To guide in metric selection, LangFair offers an\nactionable decision framework.",
    "published_date": "2025-01-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.03112v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.03088v1",
    "title": "Sentiment-guided Commonsense-aware Response Generation for Mental Health Counseling",
    "authors": [
      "Aseem Srivastava",
      "Gauri Naik",
      "Alison Cerezo",
      "Tanmoy Chakraborty",
      "Md. Shad Akhtar"
    ],
    "author_ids": [],
    "abstract": "The crisis of mental health issues is escalating. Effective counseling serves\nas a critical lifeline for individuals suffering from conditions like PTSD,\nstress, etc. Therapists forge a crucial therapeutic bond with clients, steering\nthem towards positivity. Unfortunately, the massive shortage of professionals,\nhigh costs, and mental health stigma pose significant barriers to consulting\ntherapists. As a substitute, Virtual Mental Health Assistants (VMHAs) have\nemerged in the digital healthcare space. However, most existing VMHAs lack the\ncommonsense to understand the nuanced sentiments of clients to generate\neffective responses. To this end, we propose EmpRes, a novel sentiment-guided\nmechanism incorporating commonsense awareness for generating responses. By\nleveraging foundation models and harnessing commonsense knowledge, EmpRes aims\nto generate responses that effectively shape the client's sentiment towards\npositivity. We evaluate the performance of EmpRes on HOPE, a benchmark\ncounseling dataset, and observe a remarkable performance improvement compared\nto the existing baselines across a suite of qualitative and quantitative\nmetrics. Moreover, our extensive empirical analysis and human evaluation show\nthat the generation ability of EmpRes is well-suited and, in some cases,\nsurpasses the gold standard. Further, we deploy EmpRes as a chat interface for\nusers seeking mental health support. We address the deployed system's\neffectiveness through an exhaustive user study with a significant positive\nresponse. Our findings show that 91% of users find the system effective, 80%\nexpress satisfaction, and over 85.45% convey a willingness to continue using\nthe interface and recommend it to others, demonstrating the practical\napplicability of EmpRes in addressing the pressing challenges of mental health\nsupport, emphasizing user feedback, and ethical considerations in a real-world\ncontext.",
    "published_date": "2025-01-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.03088v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.02971v1",
    "title": "Proof-of-Data: A Consensus Protocol for Collaborative Intelligence",
    "authors": [
      "Huiwen Liu",
      "Feida Zhu",
      "Ling Cheng"
    ],
    "author_ids": [],
    "abstract": "Existing research on federated learning has been focused on the setting where\nlearning is coordinated by a centralized entity. Yet the greatest potential of\nfuture collaborative intelligence would be unleashed in a more open and\ndemocratized setting with no central entity in a dominant role, referred to as\n\"decentralized federated learning\". New challenges arise accordingly in\nachieving both correct model training and fair reward allocation with\ncollective effort among all participating nodes, especially with the threat of\nthe Byzantine node jeopardising both tasks.\n  In this paper, we propose a blockchain-based decentralized Byzantine\nfault-tolerant federated learning framework based on a novel Proof-of-Data\n(PoD) consensus protocol to resolve both the \"trust\" and \"incentive\"\ncomponents. By decoupling model training and contribution accounting, PoD is\nable to enjoy not only the benefit of learning efficiency and system liveliness\nfrom asynchronous societal-scale PoW-style learning but also the finality of\nconsensus and reward allocation from epoch-based BFT-style voting. To mitigate\nfalse reward claims by data forgery from Byzantine attacks, a privacy-aware\ndata verification and contribution-based reward allocation mechanism is\ndesigned to complete the framework. Our evaluation results show that PoD\ndemonstrates performance in model training close to that of the centralized\ncounterpart while achieving trust in consensus and fairness for reward\nallocation with a fault tolerance ratio of 1/3.",
    "published_date": "2025-01-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.02971v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2502.15699v1",
    "title": "Disentangling Popularity and Quality: An Edge Classification Approach for Fair Recommendation",
    "authors": [
      "Nemat Gholinejad",
      "Mostafa Haghir Chehreghani"
    ],
    "author_ids": [],
    "abstract": "Graph neural networks (GNNs) have proven to be an effective tool for\nenhancing the performance of recommender systems. However, these systems often\nsuffer from popularity bias, leading to an unfair advantage for frequently\ninteracted items, while overlooking high-quality but less popular items. In\nthis paper, we propose a GNN-based recommendation model that disentangles\npopularity and quality to address this issue. Our approach introduces an edge\nclassification technique to differentiate between popularity bias and genuine\nquality disparities among items. Furthermore, it uses cost-sensitive learning\nto adjust the misclassification penalties, ensuring that underrepresented yet\nrelevant items are not unfairly disregarded. Experimental results demonstrate\nimprovements in fairness metrics by approximately 2-74%, while maintaining\ncompetitive accuracy, with only minor variations compared to state-of-the-art\nmethods.",
    "published_date": "2025-01-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2502.15699v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.02793v1",
    "title": "Fairness Through Matching",
    "authors": [
      "Kunwoong Kim",
      "Insung Kong",
      "Jongjin Lee",
      "Minwoo Chae",
      "Sangchul Park",
      "Yongdai Kim"
    ],
    "author_ids": [],
    "abstract": "Group fairness requires that different protected groups, characterized by a\ngiven sensitive attribute, receive equal outcomes overall. Typically, the level\nof group fairness is measured by the statistical gap between predictions from\ndifferent protected groups. In this study, we reveal an implicit property of\nexisting group fairness measures, which provides an insight into how the\ngroup-fair models behave. Then, we develop a new group-fair constraint based on\nthis implicit property to learn group-fair models. To do so, we first introduce\na notable theoretical observation: every group-fair model has an implicitly\ncorresponding transport map between the input spaces of each protected group.\nBased on this observation, we introduce a new group fairness measure termed\nMatched Demographic Parity (MDP), which quantifies the averaged gap between\npredictions of two individuals (from different protected groups) matched by a\ngiven transport map. Then, we prove that any transport map can be used in MDP\nto learn group-fair models, and develop a novel algorithm called Fairness\nThrough Matching (FTM), which learns a group-fair model using MDP constraint\nwith an user-specified transport map. We specifically propose two favorable\ntypes of transport maps for MDP, based on the optimal transport theory, and\ndiscuss their advantages. Experiments reveal that FTM successfully trains\ngroup-fair models with certain desirable properties by choosing the transport\nmap accordingly.",
    "published_date": "2025-01-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.02793v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.02732v1",
    "title": "AFed: Algorithmic Fair Federated Learning",
    "authors": [
      "Huiqiang Chen",
      "Tianqing Zhu",
      "Wanlei Zhou",
      "Wei Zhao"
    ],
    "author_ids": [],
    "abstract": "Federated Learning (FL) has gained significant attention as it facilitates\ncollaborative machine learning among multiple clients without centralizing\ntheir data on a server. FL ensures the privacy of participating clients by\nlocally storing their data, which creates new challenges in fairness.\nTraditional debiasing methods assume centralized access to sensitive\ninformation, rendering them impractical for the FL setting. Additionally, FL is\nmore susceptible to fairness issues than centralized machine learning due to\nthe diverse client data sources that may be associated with group information.\nTherefore, training a fair model in FL without access to client local data is\nimportant and challenging. This paper presents AFed, a straightforward yet\neffective framework for promoting group fairness in FL. The core idea is to\ncircumvent restricted data access by learning the global data distribution.\nThis paper proposes two approaches: AFed-G, which uses a conditional generator\ntrained on the server side, and AFed-GAN, which improves upon AFed-G by\ntraining a conditional GAN on the client side. We augment the client data with\nthe generated samples to help remove bias. Our theoretical analysis justifies\nthe proposed methods, and empirical results on multiple real-world datasets\ndemonstrate a substantial improvement in AFed over several baselines.",
    "published_date": "2025-01-06T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.02732v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.02648v2",
    "title": "Representation Learning of Lab Values via Masked AutoEncoder",
    "authors": [
      "David Restrepo",
      "Chenwei Wu",
      "Yueran Jia",
      "Jaden K. Sun",
      "Jack Gallifant",
      "Catherine G. Bielick",
      "Yugang Jia",
      "Leo A. Celi"
    ],
    "author_ids": [],
    "abstract": "Accurate imputation of missing laboratory values in electronic health records\n(EHRs) is critical to enable robust clinical predictions and reduce biases in\nAI systems in healthcare. Existing methods, such as variational autoencoders\n(VAEs) and decision tree-based approaches such as XGBoost, struggle to model\nthe complex temporal and contextual dependencies in EHR data, mainly in\nunderrepresented groups. In this work, we propose Lab-MAE, a novel\ntransformer-based masked autoencoder framework that leverages self-supervised\nlearning for the imputation of continuous sequential lab values. Lab-MAE\nintroduces a structured encoding scheme that jointly models laboratory test\nvalues and their corresponding timestamps, enabling explicit capturing temporal\ndependencies. Empirical evaluation on the MIMIC-IV dataset demonstrates that\nLab-MAE significantly outperforms the state-of-the-art baselines such as\nXGBoost across multiple metrics, including root mean square error (RMSE),\nR-squared (R2), and Wasserstein distance (WD). Notably, Lab-MAE achieves\nequitable performance across demographic groups of patients, advancing fairness\nin clinical predictions. We further investigate the role of follow-up\nlaboratory values as potential shortcut features, revealing Lab-MAE's\nrobustness in scenarios where such data is unavailable. The findings suggest\nthat our transformer-based architecture, adapted to the characteristics of the\nEHR data, offers a foundation model for more accurate and fair clinical\nimputation models. In addition, we measure and compare the carbon footprint of\nLab-MAE with the baseline XGBoost model, highlighting its environmental\nrequirements.",
    "published_date": "2025-01-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.02648v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.02647v1",
    "title": "Trust and Dependability in Blockchain & AI Based MedIoT Applications: Research Challenges and Future Directions",
    "authors": [
      "Ellis Solaiman",
      "Christa Awad"
    ],
    "author_ids": [],
    "abstract": "This paper critically reviews the integration of Artificial Intelligence (AI)\nand blockchain technologies in the context of Medical Internet of Things\n(MedIoT) applications, where they collectively promise to revolutionize\nhealthcare delivery. By examining current research, we underscore AI's\npotential in advancing diagnostics and patient care, alongside blockchain's\ncapacity to bolster data security and patient privacy. We focus particularly on\nthe imperative to cultivate trust and ensure reliability within these systems.\nOur review highlights innovative solutions for managing healthcare data and\nchallenges such as ensuring scalability, maintaining privacy, and promoting\nethical practices within the MedIoT domain. We present a vision for integrating\nAI-driven insights with blockchain security in healthcare, offering a\ncomprehensive review of current research and future directions. We conclude\nwith a set of identified research gaps and propose that addressing these is\ncrucial for achieving the dependable, secure, and patient -centric MedIoT\napplications of tomorrow.",
    "published_date": "2025-01-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.02647v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.02531v1",
    "title": "Towards New Benchmark for AI Alignment & Sentiment Analysis in Socially Important Issues: A Comparative Study of Human and LLMs in the Context of AGI",
    "authors": [
      "Ljubisa Bojic",
      "Dylan Seychell",
      "Milan Cabarkapa"
    ],
    "author_ids": [],
    "abstract": "With the expansion of neural networks, such as large language models,\nhumanity is exponentially heading towards superintelligence. As various AI\nsystems are increasingly integrated into the fabric of societies-through\nrecommending values, devising creative solutions, and making decisions-it\nbecomes critical to assess how these AI systems impact humans in the long run.\nThis research aims to contribute towards establishing a benchmark for\nevaluating the sentiment of various Large Language Models in socially importan\nissues. The methodology adopted was a Likert scale survey. Seven LLMs,\nincluding GPT-4 and Bard, were analyzed and compared against sentiment data\nfrom three independent human sample populations. Temporal variations in\nsentiment were also evaluated over three consecutive days. The results\nhighlighted a diversity in sentiment scores among LLMs, ranging from 3.32 to\n4.12 out of 5. GPT-4 recorded the most positive sentiment score towards AGI,\nwhereas Bard was leaning towards the neutral sentiment. The human samples,\ncontrastingly, showed a lower average sentiment of 2.97. The temporal\ncomparison revealed differences in sentiment evolution between LLMs in three\ndays, ranging from 1.03% to 8.21%. The study's analysis outlines the prospect\nof potential conflicts of interest and bias possibilities in LLMs' sentiment\nformation. Results indicate that LLMs, akin to human cognitive processes, could\npotentially develop unique sentiments and subtly influence societies'\nperceptions towards various opinions formed within the LLMs.",
    "published_date": "2025-01-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.02531v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.02482v1",
    "title": "Decoding News Bias: Multi Bias Detection in News Articles",
    "authors": [
      "Bhushan Santosh Shah",
      "Deven Santosh Shah",
      "Vahida Attar"
    ],
    "author_ids": [],
    "abstract": "News Articles provides crucial information about various events happening in\nthe society but they unfortunately come with different kind of biases. These\nbiases can significantly distort public opinion and trust in the media, making\nit essential to develop techniques to detect and address them. Previous works\nhave majorly worked towards identifying biases in particular domains e.g.,\nPolitical, gender biases. However, more comprehensive studies are needed to\ndetect biases across diverse domains. Large language models (LLMs) offer a\npowerful way to analyze and understand natural language, making them ideal for\nconstructing datasets and detecting these biases. In this work, we have\nexplored various biases present in the news articles, built a dataset using\nLLMs and present results obtained using multiple detection techniques. Our\napproach highlights the importance of broad-spectrum bias detection and offers\nnew insights for improving the integrity of news articles.",
    "published_date": "2025-01-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.02482v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.02442v1",
    "title": "Unsupervised Search for Ethnic Minorities' Medical Segmentation Training Set",
    "authors": [
      "Yixiao Chen",
      "Yue Yao",
      "Ruining Yang",
      "Md Zakir Hossain",
      "Ashu Gupta",
      "Tom Gedeon"
    ],
    "author_ids": [],
    "abstract": "This article investigates the critical issue of dataset bias in medical\nimaging, with a particular emphasis on racial disparities caused by uneven\npopulation distribution in dataset collection. Our analysis reveals that\nmedical segmentation datasets are significantly biased, primarily influenced by\nthe demographic composition of their collection sites. For instance, Scanning\nLaser Ophthalmoscopy (SLO) fundus datasets collected in the United States\npredominantly feature images of White individuals, with minority racial groups\nunderrepresented. This imbalance can result in biased model performance and\ninequitable clinical outcomes, particularly for minority populations. To\naddress this challenge, we propose a novel training set search strategy aimed\nat reducing these biases by focusing on underrepresented racial groups. Our\napproach utilizes existing datasets and employs a simple greedy algorithm to\nidentify source images that closely match the target domain distribution. By\nselecting training data that aligns more closely with the characteristics of\nminority populations, our strategy improves the accuracy of medical\nsegmentation models on specific minorities, i.e., Black. Our experimental\nresults demonstrate the effectiveness of this approach in mitigating bias. We\nalso discuss the broader societal implications, highlighting how addressing\nthese disparities can contribute to more equitable healthcare outcomes.",
    "published_date": "2025-01-05T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.02442v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.02368v1",
    "title": "Enhancing Workplace Productivity and Well-being Using AI Agent",
    "authors": [
      "Ravirajan K",
      "Arvind Sundarajan"
    ],
    "author_ids": [],
    "abstract": "This paper discusses the use of Artificial Intelligence (AI) to enhance\nworkplace productivity and employee well-being. By integrating machine learning\n(ML) techniques with neurobiological data, the proposed approaches ensure\nalignment with human ethical standards through value alignment models and\nHierarchical Reinforcement Learning (HRL) for autonomous task management. The\nsystem utilizes biometric feedback from employees to generate personalized\nhealth prompts, fostering a supportive work environment that encourages\nphysical activity. Additionally, we explore decentralized multi-agent systems\nfor improved collaboration and decision-making frameworks that enhance\ntransparency. Various approaches using ML techniques in conjunction with AI\nimplementations are discussed. Together, these innovations aim to create a more\nproductive and health-conscious workplace. These outcomes assist HR management\nand organizations in launching more rational career progression streams for\nemployees and facilitating organizational transformation.",
    "published_date": "2025-01-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.02368v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.02295v2",
    "title": "Explicit vs. Implicit: Investigating Social Bias in Large Language Models through Self-Reflection",
    "authors": [
      "Yachao Zhao",
      "Bo Wang",
      "Yan Wang"
    ],
    "author_ids": [],
    "abstract": "Large Language Models (LLMs) have been shown to exhibit various biases and\nstereotypes in their generated content. While extensive research has\ninvestigated bias in LLMs, prior work has predominantly focused on explicit\nbias, leaving the more nuanced implicit biases largely unexplored. This paper\npresents a systematic framework grounded in social psychology theories to\ninvestigate and compare explicit and implicit biases in LLMs. We propose a\nnovel \"self-reflection\" based evaluation framework that operates in two phases:\nfirst measuring implicit bias through simulated psychological assessment\nmethods, then evaluating explicit bias by prompting LLMs to analyze their own\ngenerated content. Through extensive experiments on state-of-the-art LLMs\nacross multiple social dimensions, we demonstrate that LLMs exhibit a\nsubstantial inconsistency between explicit and implicit biases, where explicit\nbiases manifest as mild stereotypes while implicit biases show strong\nstereotypes. Furthermore, we investigate the underlying factors contributing to\nthis explicit-implicit bias inconsistency. Our experiments examine the effects\nof training data scale, model parameters, and alignment techniques. Results\nindicate that while explicit bias diminishes with increased training data and\nmodel size, implicit bias exhibits a contrasting upward trend. Notably,\ncontemporary alignment methods (e.g., RLHF, DPO) effectively suppress explicit\nbias but show limited efficacy in mitigating implicit bias. These findings\nsuggest that while scaling up models and alignment training can address\nexplicit bias, the challenge of implicit bias requires novel approaches beyond\ncurrent methodologies.",
    "published_date": "2025-01-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.02295v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.02211v1",
    "title": "Examining the Robustness of Homogeneity Bias to Hyperparameter Adjustments in GPT-4",
    "authors": [
      "Messi H. J. Lee"
    ],
    "author_ids": [],
    "abstract": "Vision-Language Models trained on massive collections of human-generated data\noften reproduce and amplify societal stereotypes. One critical form of\nstereotyping reproduced by these models is homogeneity bias-the tendency to\nrepresent certain groups as more homogeneous than others. We investigate how\nthis bias responds to hyperparameter adjustments in GPT-4, specifically\nexamining sampling temperature and top p which control the randomness of model\noutputs. By generating stories about individuals from different racial and\ngender groups and comparing their similarities using vector representations, we\nassess both bias robustness and its relationship with hyperparameter values. We\nfind that (1) homogeneity bias persists across most hyperparameter\nconfigurations, with Black Americans and women being represented more\nhomogeneously than White Americans and men, (2) the relationship between\nhyperparameters and group representations shows unexpected non-linear patterns,\nparticularly at extreme values, and (3) hyperparameter adjustments affect\nracial and gender homogeneity bias differently-while increasing temperature or\ndecreasing top p can reduce racial homogeneity bias, these changes show\ndifferent effects on gender homogeneity bias. Our findings suggest that while\nhyperparameter tuning may mitigate certain biases to some extent, it cannot\nserve as a universal solution for addressing homogeneity bias across different\nsocial group dimensions.",
    "published_date": "2025-01-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.02211v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.03266v1",
    "title": "LLM Content Moderation and User Satisfaction: Evidence from Response Refusals in Chatbot Arena",
    "authors": [
      "Stefan Pasch"
    ],
    "author_ids": [],
    "abstract": "LLM safety and ethical alignment are widely discussed, but the impact of\ncontent moderation on user satisfaction remains underexplored. To address this,\nwe analyze nearly 50,000 Chatbot Arena response-pairs using a novel fine-tuned\nRoBERTa model, that we trained on hand-labeled data to disentangle refusals due\nto ethical concerns from other refusals due to technical disabilities or lack\nof information. Our findings reveal a significant refusal penalty on content\nmoderation, with users choosing ethical-based refusals roughly one-fourth as\noften as their preferred LLM response compared to standard responses. However,\nthe context and phrasing play critical roles: refusals on highly sensitive\nprompts, such as illegal content, achieve higher win rates than less sensitive\nethical concerns, and longer responses closely aligned with the prompt perform\nbetter. These results emphasize the need for nuanced moderation strategies that\nbalance ethical safeguards with user satisfaction. Moreover, we find that the\nrefusal penalty is notably lower in evaluations using the LLM-as-a-Judge\nmethod, highlighting discrepancies between user and automated assessments.",
    "published_date": "2025-01-04T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.03266v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.02176v1",
    "title": "Molecule-dynamic-based Aging Clock and Aging Roadmap Forecast with Sundial",
    "authors": [
      "Wei Wu",
      "Zizhen Deng",
      "Chi Zhang",
      "Can Liao",
      "Jinzhuo Wang"
    ],
    "author_ids": [],
    "abstract": "Addressing the unavoidable bias inherent in supervised aging clocks, we\nintroduce Sundial, a novel framework that models molecular dynamics through a\ndiffusion field, capturing both the population-level aging process and the\nindividual-level relative aging order. Sundial enables unbiasedestimation of\nbiological age and the forecast of aging roadmap. Fasteraging individuals from\nSundial exhibit a higher disease risk compared to those identified from\nsupervised aging clocks. This framework opens new avenues for exploring key\ntopics, including age- and sex-specific aging dynamics and faster yet healthy\naging paths.",
    "published_date": "2025-01-04T00:00:00",
    "year": 2025,
    "categories": [
      "q-bio.QM",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.02176v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.02088v2",
    "title": "Curious, Critical Thinker, Empathetic, and Ethically Responsible: Essential Soft Skills for Data Scientists in Software Engineering",
    "authors": [
      "Matheus de Morais Leça",
      "Ronnie de Souza Santos"
    ],
    "author_ids": [],
    "abstract": "Background. As artificial intelligence and AI-powered systems continue to\ngrow, the role of data scientists has become essential in software development\nenvironments. Data scientists face challenges related to managing large volumes\nof data and addressing the societal impacts of AI algorithms, which require a\nbroad range of soft skills.\n  Goal. This study aims to identify the key soft skills that data scientists\nneed when working on AI-powered projects, with a particular focus on addressing\nbiases that affect society.\n  Method. We conducted a thematic analysis of 87 job postings on LinkedIn and\n11 interviews with industry practitioners. The job postings came from companies\nin 12 countries and covered various experience levels. The interviews featured\nprofessionals from diverse backgrounds, including different genders,\nethnicities, and sexual orientations, who worked with clients from South\nAmerica, North America, and Europe.\n  Results. While data scientists share many skills with other software\npractitioners -- such as those related to coordination, engineering, and\nmanagement -- there is a growing emphasis on innovation and social\nresponsibility. These include soft skills like curiosity, critical thinking,\nempathy, and ethical awareness, which are essential for addressing the ethical\nand societal implications of AI.\n  Conclusion. Our findings indicate that data scientists working on AI-powered\nprojects require not only technical expertise but also a solid foundation in\nsoft skills that enable them to build AI systems responsibly, with fairness and\ninclusivity. These insights have important implications for recruitment and\ntraining within software companies and for ensuring the long-term success of\nAI-powered systems and their broader societal impact.",
    "published_date": "2025-01-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.02088v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.02084v1",
    "title": "Simulated prosthetic vision identifies checkerboard as an effective raster pattern for retinal implants",
    "authors": [
      "Justin M. Kasowski",
      "Michael Beyeler"
    ],
    "author_ids": [],
    "abstract": "\\emph{Objective:} This study systematically evaluates the impact of raster\npatterns -- specific spatial arrangements of sequential electrode activation --\non performance and perceived difficulty in simulated prosthetic vision (SPV).\nBy addressing a critical gap in the literature, we aimed to identify patterns\nthat optimize functional vision in retinal implants.\n  \\emph{Approach:} Sighted participants completed letter recognition and motion\ndiscrimination tasks under four raster patterns (horizontal, vertical,\ncheckerboard, and random) using an immersive SPV system. The simulations\nemployed psychophysically validated models of electrode activation, phosphene\nappearance, and temporal dynamics, ensuring realistic representation of\nprosthetic vision. Performance accuracy and self-reported difficulty were\nanalyzed to assess the effects of raster patterning.\n  \\emph{Main Results:} The checkerboard pattern consistently outperformed other\nraster patterns, yielding significantly higher accuracy and lower difficulty\nratings across both tasks. The horizontal and vertical patterns introduced\nbiases aligned with apparent motion artifacts, while the checkerboard minimized\nsuch effects. Random patterns resulted in the lowest performance, underscoring\nthe importance of structured activation.\n  \\emph{Significance:} These findings provide the first systematic evaluation\nof raster patterns, suggesting that the checkerboard configuration may enhance\nusability and perceptual clarity in retinal implants. While based on simulated\nenvironments, this study establishes a foundation for optimizing spatial and\ntemporal electrode activation strategies in next-generation devices.",
    "published_date": "2025-01-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.02084v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.02080v1",
    "title": "AI-Powered Cow Detection in Complex Farm Environments",
    "authors": [
      "Voncarlos M. Araújo",
      "Ines Rili",
      "Thomas Gisiger",
      "Sebastien Gambs",
      "Elsa Vasseur",
      "Marjorie Cellier",
      "Abdoulaye Baniré Diallo"
    ],
    "author_ids": [],
    "abstract": "Animal welfare has become a critical issue in contemporary society,\nemphasizing our ethical responsibilities toward animals, particularly within\nlivestock farming. The advent of Artificial Intelligence (AI) technologies,\nspecifically computer vision, offers an innovative approach to monitoring and\nenhancing animal welfare. Cows, as essential contributors to sustainable\nagriculture, are central to this effort. However, existing cow detection\nalgorithms face challenges in real-world farming environments, such as complex\nlighting, occlusions, pose variations, and background interference, hindering\ndetection. Model generalization is crucial for adaptation across contexts\nbeyond the training dataset. This study addresses these challenges using a\ndiverse cow dataset from six environments, including indoor and outdoor\nscenarios. We propose a detection model combining YOLOv8 with the CBAM\n(Convolutional Block Attention Module) and assess its performance against\nbaseline models, including Mask R-CNN, YOLOv5, and YOLOv8. Our findings show\nbaseline models degrade in complex conditions, while our approach improves\nusing CBAM. YOLOv8-CBAM outperformed YOLOv8 by 2.3% in mAP, achieving 95.2%\nprecision and an mAP@0.5:0.95 of 82.6%, demonstrating superior accuracy.\nContributions include (1) analyzing detection limitations, (2) proposing a\nrobust model, and (3) benchmarking state-of-the-art algorithms. Applications\ninclude health monitoring, behavioral analysis, and tracking in smart farms,\nenabling precise detection in challenging settings. This study advances\nAI-driven livestock monitoring, improving animal welfare and smart agriculture.",
    "published_date": "2025-01-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.02080v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.01897v1",
    "title": "ChatGPT's advice drives moral judgments with or without justification",
    "authors": [
      "Sebastian Kruegel",
      "Andreas Ostermaier",
      "Matthias Uhl"
    ],
    "author_ids": [],
    "abstract": "Why do users follow moral advice from chatbots? A chatbot is not an\nauthoritative moral advisor, but it can generate seemingly plausible arguments.\nUsers do not follow reasoned more readily than unreasoned advice, though, we\nfind in an experiment. However, this is also true if we attribute advice to a\nmoral advisor, not a chatbot. Hence, it seems that advice offers users a cheap\nway to escape from a moral dilemma. This is a concern that chatbots do not\nraise, but they exacerbate it as they make advice easily accessible. We\nconclude that it takes ethical in addition to digital literacy to harness users\nagainst moral advice from chatbots.",
    "published_date": "2025-01-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.01897v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.01889v1",
    "title": "Exploring Equality: An Investigation into Custom Loss Functions for Fairness Definitions",
    "authors": [
      "Gordon Lee",
      "Simeon Sayer"
    ],
    "author_ids": [],
    "abstract": "This paper explores the complex tradeoffs between various fairness metrics\nsuch as equalized odds, disparate impact, and equal opportunity and predictive\naccuracy within COMPAS by building neural networks trained with custom loss\nfunctions optimized to specific fairness criteria. This paper creates the first\nfairness-driven implementation of the novel Group Accuracy Parity (GAP)\nframework, as theoretically proposed by Gupta et al. (2024), and applies it to\nCOMPAS. To operationalize and accurately compare the fairness of COMPAS models\noptimized to differing fairness ideals, this paper develops and proposes a\ncombinatory analytical procedure that incorporates Pareto front and\nmultivariate analysis, leveraging data visualizations such as violin graphs.\nThis paper concludes that GAP achieves an enhanced equilibrium between fairness\nand accuracy compared to COMPAS's current nationwide implementation and\nalternative implementations of COMPAS optimized to more traditional fairness\ndefinitions. While this paper's algorithmic improvements of COMPAS\nsignificantly augment its fairness, external biases undermine the fairness of\nits implementation. Practices such as predictive policing and issues such as\nthe lack of transparency regarding COMPAS's internal workings have contributed\nto the algorithm's historical injustice. In conjunction with developments\nregarding COMPAS's predictive methodology, legal and institutional changes must\nhappen for COMPAS's just deployment.",
    "published_date": "2025-01-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.01889v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.01872v2",
    "title": "Turning Logic Against Itself : Probing Model Defenses Through Contrastive Questions",
    "authors": [
      "Rachneet Sachdeva",
      "Rima Hazra",
      "Iryna Gurevych"
    ],
    "author_ids": [],
    "abstract": "Large language models, despite extensive alignment with human values and\nethical principles, remain vulnerable to sophisticated jailbreak attacks that\nexploit their reasoning abilities. Existing safety measures often detect overt\nmalicious intent but fail to address subtle, reasoning-driven vulnerabilities.\nIn this work, we introduce POATE (Polar Opposite query generation, Adversarial\nTemplate construction, and Elaboration), a novel jailbreak technique that\nharnesses contrastive reasoning to provoke unethical responses. POATE crafts\nsemantically opposing intents and integrates them with adversarial templates,\nsteering models toward harmful outputs with remarkable subtlety. We conduct\nextensive evaluation across six diverse language model families of varying\nparameter sizes to demonstrate the robustness of the attack, achieving\nsignificantly higher attack success rates (~44%) compared to existing methods.\nTo counter this, we propose Intent-Aware CoT and Reverse Thinking CoT, which\ndecompose queries to detect malicious intent and reason in reverse to evaluate\nand reject harmful responses. These methods enhance reasoning robustness and\nstrengthen the model's defense against adversarial exploits.",
    "published_date": "2025-01-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.01872v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.01785v1",
    "title": "Can Synthetic Data be Fair and Private? A Comparative Study of Synthetic Data Generation and Fairness Algorithms",
    "authors": [
      "Qinyi Liu",
      "Oscar Deho",
      "Farhad Vadiee",
      "Mohammad Khalil",
      "Srecko Joksimovic",
      "George Siemens"
    ],
    "author_ids": [],
    "abstract": "The increasing use of machine learning in learning analytics (LA) has raised\nsignificant concerns around algorithmic fairness and privacy. Synthetic data\nhas emerged as a dual-purpose tool, enhancing privacy and improving fairness in\nLA models. However, prior research suggests an inverse relationship between\nfairness and privacy, making it challenging to optimize both. This study\ninvestigates which synthetic data generators can best balance privacy and\nfairness, and whether pre-processing fairness algorithms, typically applied to\nreal datasets, are effective on synthetic data. Our results highlight that the\nDEbiasing CAusal Fairness (DECAF) algorithm achieves the best balance between\nprivacy and fairness. However, DECAF suffers in utility, as reflected in its\npredictive accuracy. Notably, we found that applying pre-processing fairness\nalgorithms to synthetic data improves fairness even more than when applied to\nreal data. These findings suggest that combining synthetic data generation with\nfairness pre-processing offers a promising approach to creating fairer LA\nmodels.",
    "published_date": "2025-01-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.01785v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.01741v1",
    "title": "How Toxic Can You Get? Search-based Toxicity Testing for Large Language Models",
    "authors": [
      "Simone Corbo",
      "Luca Bancale",
      "Valeria De Gennaro",
      "Livia Lestingi",
      "Vincenzo Scotti",
      "Matteo Camilli"
    ],
    "author_ids": [],
    "abstract": "Language is a deep-rooted means of perpetration of stereotypes and\ndiscrimination. Large Language Models (LLMs), now a pervasive technology in our\neveryday lives, can cause extensive harm when prone to generating toxic\nresponses. The standard way to address this issue is to align the LLM, which,\nhowever, dampens the issue without constituting a definitive solution.\nTherefore, testing LLM even after alignment efforts remains crucial for\ndetecting any residual deviations with respect to ethical standards. We present\nEvoTox, an automated testing framework for LLMs' inclination to toxicity,\nproviding a way to quantitatively assess how much LLMs can be pushed towards\ntoxic responses even in the presence of alignment. The framework adopts an\niterative evolution strategy that exploits the interplay between two LLMs, the\nSystem Under Test (SUT) and the Prompt Generator steering SUT responses toward\nhigher toxicity. The toxicity level is assessed by an automated oracle based on\nan existing toxicity classifier. We conduct a quantitative and qualitative\nempirical evaluation using four state-of-the-art LLMs as evaluation subjects\nhaving increasing complexity (7-13 billion parameters). Our quantitative\nevaluation assesses the cost-effectiveness of four alternative versions of\nEvoTox against existing baseline methods, based on random search, curated\ndatasets of toxic prompts, and adversarial attacks. Our qualitative assessment\nengages human evaluators to rate the fluency of the generated prompts and the\nperceived toxicity of the responses collected during the testing sessions.\nResults indicate that the effectiveness, in terms of detected toxicity level,\nis significantly higher than the selected baseline methods (effect size up to\n1.0 against random search and up to 0.99 against adversarial attacks).\nFurthermore, EvoTox yields a limited cost overhead (from 22% to 35% on\naverage).",
    "published_date": "2025-01-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.01741v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.01683v1",
    "title": "6Vision: Image-encoding-based IPv6 Target Generation in Few-seed Scenarios",
    "authors": [
      "W. Zhang",
      "G. Song",
      "L. He",
      "J. Lin",
      "S. Wu",
      "Z. Wang",
      "C. Li",
      "J. Yang"
    ],
    "author_ids": [],
    "abstract": "Efficient global Internet scanning is crucial for network measurement and\nsecurity analysis. While existing target generation algorithms demonstrate\nremarkable performance in large-scale detection, their efficiency notably\ndiminishes in few-seed scenarios. This decline is primarily attributed to the\nintricate configuration rules and sampling bias of seed addresses. Moreover,\ninstances where BGP prefixes have few seed addresses are widespread,\nconstituting 63.65% of occurrences. We introduce 6Vision as a solution to\ntackle this challenge by introducing a novel approach of encoding IPv6\naddresses into images, facilitating comprehensive analysis of intricate\nconfiguration rules. Through a process of feature stitching, 6Vision not only\nimproves the learnable features but also amalgamates addresses associated with\nconfiguration patterns for enhanced learning. Moreover, it integrates an\nenvironmental feedback mechanism to refine model parameters based on identified\nactive addresses, thereby alleviating the sampling bias inherent in seed\naddresses. As a result, 6Vision achieves high-accuracy detection even in\nfew-seed scenarios. The HitRate of 6Vision shows a significant improvement\nranging from 181% to 2,490% compared to existing algorithms, while the CoverNum\nincreases by a factor of 1.18 to 11.20 times. Additionally, 6Vision can\nfunction as a preliminary detection module for existing algorithms, yielding a\nconversion gain (CG) ranging from 242% to 2,081%. Ultimately, we achieve a\nconversion rate (CR) of 28.97% for few-seed scenarios. We develop the IPv6\nhitlist Patch, which augments current target generation algorithms for\nlarge-scale address detection, thereby effectively supporting IPv6 network\nmeasurement and security analysis.",
    "published_date": "2025-01-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.01683v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.01665v1",
    "title": "FairSense: Long-Term Fairness Analysis of ML-Enabled Systems",
    "authors": [
      "Yining She",
      "Sumon Biswas",
      "Christian Kästner",
      "Eunsuk Kang"
    ],
    "author_ids": [],
    "abstract": "Algorithmic fairness of machine learning (ML) models has raised significant\nconcern in the recent years. Many testing, verification, and bias mitigation\ntechniques have been proposed to identify and reduce fairness issues in ML\nmodels. The existing methods are model-centric and designed to detect fairness\nissues under static settings. However, many ML-enabled systems operate in a\ndynamic environment where the predictive decisions made by the system impact\nthe environment, which in turn affects future decision-making. Such a\nself-reinforcing feedback loop can cause fairness violations in the long term,\neven if the immediate outcomes are fair. In this paper, we propose a\nsimulation-based framework called FairSense to detect and analyze long-term\nunfairness in ML-enabled systems. Given a fairness requirement, FairSense\nperforms Monte-Carlo simulation to enumerate evolution traces for each system\nconfiguration. Then, FairSense performs sensitivity analysis on the space of\npossible configurations to understand the impact of design options and\nenvironmental factors on the long-term fairness of the system. We demonstrate\nFairSense's potential utility through three real-world case studies: Loan\nlending, opioids risk scoring, and predictive policing.",
    "published_date": "2025-01-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.01665v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.01660v1",
    "title": "The (Exact) Price of Cardinality for Indivisible Goods: A Parametric Perspective",
    "authors": [
      "Alexander Lam",
      "Bo Li",
      "Ankang Sun"
    ],
    "author_ids": [],
    "abstract": "We adopt a parametric approach to analyze the worst-case degradation in\nsocial welfare when the allocation of indivisible goods is constrained to be\nfair. Specifically, we are concerned with cardinality-constrained allocations,\nwhich require that each agent has at most $k$ items in their allocated bundle.\nWe propose the notion of the price of cardinality, which captures the\nworst-case multiplicative loss of utilitarian or egalitarian social welfare\nresulting from imposing the cardinality constraint. We then characterize tight\nor almost-tight bounds on the price of cardinality as exact functions of the\ninstance parameters, demonstrating how the social welfare improves as $k$ is\nincreased. In particular, one of our main results refines and generalizes the\nexisting asymptotic bound on the price of balancedness, as studied by Bei et\nal. [BLMS21]. We also further extend our analysis to the problem where the\nitems are partitioned into disjoint categories, and each category has its own\ncardinality constraint. Through a parametric study of the price of cardinality,\nwe provide a framework which aids decision makers in choosing an ideal level of\ncardinality-based fairness, using their knowledge of the potential loss of\nutilitarian and egalitarian social welfare.",
    "published_date": "2025-01-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.01660v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2501.01639v2",
    "title": "Implications of Artificial Intelligence on Health Data Privacy and Confidentiality",
    "authors": [
      "Ahmad Momani"
    ],
    "author_ids": [],
    "abstract": "The rapid integration of artificial intelligence (AI) in healthcare is\nrevolutionizing medical diagnostics, personalized medicine, and operational\nefficiency. However, alongside these advancements, significant challenges arise\nconcerning patient data privacy, ethical considerations, and regulatory\ncompliance. This paper examines the dual impact of AI on healthcare,\nhighlighting its transformative potential and the critical need for\nsafeguarding sensitive health information. It explores the role of the Health\nInsurance Portability and Accountability Act (HIPAA) as a regulatory framework\nfor ensuring data privacy and security, emphasizing the importance of robust\nsafeguards and ethical standards in AI-driven healthcare. Through case studies,\nincluding AI applications in diabetic retinopathy, oncology, and the\ncontroversies surrounding data sharing, this study underscores the ethical and\nlegal complexities of AI implementation. A balanced approach that fosters\ninnovation while maintaining patient trust and privacy is imperative. The\nfindings emphasize the importance of continuous education, transparency, and\nadherence to regulatory frameworks to harness AI's full potential responsibly\nand ethically in healthcare.",
    "published_date": "2025-01-03T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.01639v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.01564v2",
    "title": "Semialgebraic Neural Networks: From roots to representations",
    "authors": [
      "S. David Mis",
      "Matti Lassas",
      "Maarten V. de Hoop"
    ],
    "author_ids": [],
    "abstract": "Many numerical algorithms in scientific computing -- particularly in areas\nlike numerical linear algebra, PDE simulation, and inverse problems -- produce\noutputs that can be represented by semialgebraic functions; that is, the graph\nof the computed function can be described by finitely many polynomial\nequalities and inequalities. In this work, we introduce Semialgebraic Neural\nNetworks (SANNs), a neural network architecture capable of representing any\nbounded semialgebraic function, and computing such functions up to the accuracy\nof a numerical ODE solver chosen by the programmer. Conceptually, we encode the\ngraph of the learned function as the kernel of a piecewise polynomial selected\nfrom a class of functions whose roots can be evaluated using a particular\nhomotopy continuation method. We show by construction that the SANN\narchitecture is able to execute this continuation method, thus evaluating the\nlearned semialgebraic function. Furthermore, the architecture can exactly\nrepresent even discontinuous semialgebraic functions by executing a\ncontinuation method on each connected component of the target function. Lastly,\nwe provide example applications of these networks and show they can be trained\nwith traditional deep-learning techniques.",
    "published_date": "2025-01-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.NA",
      "cs.NE",
      "math.NA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.01564v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.01535v1",
    "title": "A Metasemantic-Metapragmatic Framework for Taxonomizing Multimodal Communicative Alignment",
    "authors": [
      "Eugene Yu Ji"
    ],
    "author_ids": [],
    "abstract": "Drawing on contemporary pragmatist philosophy and linguistic theories on\ncognition, meaning, and communication, this paper presents a dynamic,\nmetasemantic-metapragmatic taxonomy for grounding and conceptualizing\nhuman-like multimodal communicative alignment. The framework is rooted in\ncontemporary developments of the three basic communicative capacities initially\nidentified by American logician and pragmatist philosopher Charles Sanders\nPeirce: iconic (sensory and perceptual qualities), indexical (contextual and\nsociocultural associations), and rule-like (symbolic and intuitive reasoning).\nExpanding on these developments, I introduce the concept of indexical\ncontextualization and propose the principle of \"contextualization\ndirectionality\" for characterizing the crucial metapragmatic capacity for\nmaintaining, navigating, or transitioning between semantic and pragmatic modes\nof multimodal communication. I contend that current cognitive-social\ncomputational and engineering methodologies disproportionately emphasize the\nsemantic/metasemantic domain, overlooking the pivotal role of metapragmatic\nindexicality in traversing the semantic-pragmatic spectrum of communication.\nThe framework's broader implications for intentionality, identity, affect, and\nethics in within-modal and cross-modal human-machine alignment are also\ndiscussed.",
    "published_date": "2025-01-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.01535v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.01284v1",
    "title": "NeutraSum: A Language Model can help a Balanced Media Diet by Neutralizing News Summaries",
    "authors": [
      "Xi Luo",
      "Junjie Liu",
      "Sirong Wu",
      "Yuhui Deng"
    ],
    "author_ids": [],
    "abstract": "Media bias in news articles arises from the political polarisation of media\noutlets, which can reinforce societal stereotypes and beliefs. Reporting on the\nsame event often varies significantly between outlets, reflecting their\npolitical leanings through polarised language and focus. Although previous\nstudies have attempted to generate bias-free summaries from multiperspective\nnews articles, they have not effectively addressed the challenge of mitigating\ninherent media bias. To address this gap, we propose \\textbf{NeutraSum}, a\nnovel framework that integrates two neutrality losses to adjust the semantic\nspace of generated summaries, thus minimising media bias. These losses,\ndesigned to balance the semantic distances across polarised inputs and ensure\nalignment with expert-written summaries, guide the generation of neutral and\nfactually rich summaries. To evaluate media bias, we employ the political\ncompass test, which maps political leanings based on economic and social\ndimensions. Experimental results on the Allsides dataset demonstrate that\nNeutraSum not only improves summarisation performance but also achieves\nsignificant reductions in media bias, offering a promising approach for neutral\nnews summarisation.",
    "published_date": "2025-01-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.01284v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.01240v1",
    "title": "Asymmetric Reinforcing against Multi-modal Representation Bias",
    "authors": [
      "Xiyuan Gao",
      "Bing Cao",
      "Pengfei Zhu",
      "Nannan Wang",
      "Qinghua Hu"
    ],
    "author_ids": [],
    "abstract": "The strength of multimodal learning lies in its ability to integrate\ninformation from various sources, providing rich and comprehensive insights.\nHowever, in real-world scenarios, multi-modal systems often face the challenge\nof dynamic modality contributions, the dominance of different modalities may\nchange with the environments, leading to suboptimal performance in multimodal\nlearning. Current methods mainly enhance weak modalities to balance multimodal\nrepresentation bias, which inevitably optimizes from a partialmodality\nperspective, easily leading to performance descending for dominant modalities.\nTo address this problem, we propose an Asymmetric Reinforcing method against\nMultimodal representation bias (ARM). Our ARM dynamically reinforces the weak\nmodalities while maintaining the ability to represent dominant modalities\nthrough conditional mutual information. Moreover, we provide an in-depth\nanalysis that optimizing certain modalities could cause information loss and\nprevent leveraging the full advantages of multimodal data. By exploring the\ndominance and narrowing the contribution gaps between modalities, we have\nsignificantly improved the performance of multimodal learning, making notable\nprogress in mitigating imbalanced multimodal learning.",
    "published_date": "2025-01-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.01240v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.03259v1",
    "title": "Toward Inclusive Educational AI: Auditing Frontier LLMs through a Multiplexity Lens",
    "authors": [
      "Abdullah Mushtaq",
      "Muhammad Rafay Naeem",
      "Muhammad Imran Taj",
      "Ibrahim Ghaznavi",
      "Junaid Qadir"
    ],
    "author_ids": [],
    "abstract": "As large language models (LLMs) like GPT-4 and Llama 3 become integral to\neducational contexts, concerns are mounting over the cultural biases, power\nimbalances, and ethical limitations embedded within these technologies. Though\ngenerative AI tools aim to enhance learning experiences, they often reflect\nvalues rooted in Western, Educated, Industrialized, Rich, and Democratic\n(WEIRD) cultural paradigms, potentially sidelining diverse global perspectives.\nThis paper proposes a framework to assess and mitigate cultural bias within\nLLMs through the lens of applied multiplexity. Multiplexity, inspired by\nSenturk et al. and rooted in Islamic and other wisdom traditions, emphasizes\nthe coexistence of diverse cultural viewpoints, supporting a multi-layered\nepistemology that integrates both empirical sciences and normative values. Our\nanalysis reveals that LLMs frequently exhibit cultural polarization, with\nbiases appearing in both overt responses and subtle contextual cues. To address\ninherent biases and incorporate multiplexity in LLMs, we propose two\nstrategies: \\textit{Contextually-Implemented Multiplex LLMs}, which embed\nmultiplex principles directly into the system prompt, influencing LLM outputs\nat a foundational level and independent of individual prompts, and\n\\textit{Multi-Agent System (MAS)-Implemented Multiplex LLMs}, where multiple\nLLM agents, each representing distinct cultural viewpoints, collaboratively\ngenerate a balanced, synthesized response. Our findings demonstrate that as\nmitigation strategies evolve from contextual prompting to MAS-implementation,\ncultural inclusivity markedly improves, evidenced by a significant rise in the\nPerspectives Distribution Score (PDS) and a PDS Entropy increase from 3.25\\% at\nbaseline to 98\\% with the MAS-Implemented Multiplex LLMs. Sentiment analysis\nfurther shows a shift towards positive sentiment across cultures,...",
    "published_date": "2025-01-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.03259v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.01205v1",
    "title": "Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects",
    "authors": [
      "Abdullah Mushtaq",
      "Muhammad Rafay Naeem",
      "Ibrahim Ghaznavi",
      "Muhammad Imran Taj",
      "Imran Hashmi",
      "Junaid Qadir"
    ],
    "author_ids": [],
    "abstract": "Multi-Agent Large Language Models (LLMs) are gaining significant attention\nfor their ability to harness collective intelligence in complex\nproblem-solving, decision-making, and planning tasks. This aligns with the\nconcept of the wisdom of crowds, where diverse agents contribute collectively\nto generating effective solutions, making it particularly suitable for\neducational settings. Senior design projects, also known as capstone or final\nyear projects, are pivotal in engineering education as they integrate\ntheoretical knowledge with practical application, fostering critical thinking,\nteamwork, and real-world problem-solving skills. In this paper, we explore the\nuse of Multi-Agent LLMs in supporting these senior design projects undertaken\nby engineering students, which often involve multidisciplinary considerations\nand conflicting objectives, such as optimizing technical performance while\naddressing ethical, social, and environmental concerns. We propose a framework\nwhere distinct LLM agents represent different expert perspectives, such as\nproblem formulation agents, system complexity agents, societal and ethical\nagents, or project managers, thus facilitating a holistic problem-solving\napproach. This implementation leverages standard multi-agent system (MAS)\nconcepts such as coordination, cooperation, and negotiation, incorporating\nprompt engineering to develop diverse personas for each agent. These agents\nengage in rich, collaborative dialogues to simulate human engineering teams,\nguided by principles from swarm AI to efficiently balance individual\ncontributions towards a unified solution. We adapt these techniques to create a\ncollaboration structure for LLM agents, encouraging interdisciplinary reasoning\nand negotiation similar to real-world senior design projects. To assess the\nefficacy of this framework, we collected six proposals of engineering and\ncomputer science of...",
    "published_date": "2025-01-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.01205v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.01168v1",
    "title": "Blind Men and the Elephant: Diverse Perspectives on Gender Stereotypes in Benchmark Datasets",
    "authors": [
      "Mahdi Zakizadeh",
      "Mohammad Taher Pilehvar"
    ],
    "author_ids": [],
    "abstract": "The multifaceted challenge of accurately measuring gender stereotypical bias\nin language models is akin to discerning different segments of a broader,\nunseen entity. This short paper primarily focuses on intrinsic bias mitigation\nand measurement strategies for language models, building on prior research that\ndemonstrates a lack of correlation between intrinsic and extrinsic approaches.\nWe delve deeper into intrinsic measurements, identifying inconsistencies and\nsuggesting that these benchmarks may reflect different facets of gender\nstereotype. Our methodology involves analyzing data distributions across\ndatasets and integrating gender stereotype components informed by social\npsychology. By adjusting the distribution of two datasets, we achieve a better\nalignment of outcomes. Our findings underscore the complexity of gender\nstereotyping in language models and point to new directions for developing more\nrefined techniques to detect and reduce bias.",
    "published_date": "2025-01-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.01168v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.01111v1",
    "title": "Regularized Proportional Fairness Mechanism for Resource Allocation Without Money",
    "authors": [
      "Sihan Zeng",
      "Sujay Bhatt",
      "Alec Koppel",
      "Sumitra Ganesh"
    ],
    "author_ids": [],
    "abstract": "Mechanism design in resource allocation studies dividing limited resources\namong self-interested agents whose satisfaction with the allocation depends on\nprivately held utilities. We consider the problem in a payment-free setting,\nwith the aim of maximizing social welfare while enforcing incentive\ncompatibility (IC), i.e., agents cannot inflate allocations by misreporting\ntheir utilities. The well-known proportional fairness (PF) mechanism achieves\nthe maximum possible social welfare but incurs an undesirably high\nexploitability (the maximum unilateral inflation in utility from misreport and\na measure of deviation from IC). In fact, it is known that no mechanism can\nachieve the maximum social welfare and exact incentive compatibility (IC)\nsimultaneously without the use of monetary incentives (Cole et al., 2013).\nMotivated by this fact, we propose learning an approximate mechanism that\ndesirably trades off the competing objectives. Our main contribution is to\ndesign an innovative neural network architecture tailored to the resource\nallocation problem, which we name Regularized Proportional Fairness Network\n(RPF-Net). RPF-Net regularizes the output of the PF mechanism by a learned\nfunction approximator of the most exploitable allocation, with the aim of\nreducing the incentive for any agent to misreport. We derive generalization\nbounds that guarantee the mechanism performance when trained under finite and\nout-of-distribution samples and experimentally demonstrate the merits of the\nproposed mechanism compared to the state-of-the-art.",
    "published_date": "2025-01-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.GT",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.01111v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.01056v1",
    "title": "Risks of Cultural Erasure in Large Language Models",
    "authors": [
      "Rida Qadri",
      "Aida M. Davani",
      "Kevin Robinson",
      "Vinodkumar Prabhakaran"
    ],
    "author_ids": [],
    "abstract": "Large language models are increasingly being integrated into applications\nthat shape the production and discovery of societal knowledge such as search,\nonline education, and travel planning. As a result, language models will shape\nhow people learn about, perceive and interact with global cultures making it\nimportant to consider whose knowledge systems and perspectives are represented\nin models. Recognizing this importance, increasingly work in Machine Learning\nand NLP has focused on evaluating gaps in global cultural representational\ndistribution within outputs. However, more work is needed on developing\nbenchmarks for cross-cultural impacts of language models that stem from a\nnuanced sociologically-aware conceptualization of cultural impact or harm. We\njoin this line of work arguing for the need of metricizable evaluations of\nlanguage technologies that interrogate and account for historical power\ninequities and differential impacts of representation on global cultures,\nparticularly for cultures already under-represented in the digital corpora. We\nlook at two concepts of erasure: omission: where cultures are not represented\nat all and simplification i.e. when cultural complexity is erased by presenting\none-dimensional views of a rich culture. The former focuses on whether\nsomething is represented, and the latter on how it is represented. We focus our\nanalysis on two task contexts with the potential to influence global cultural\nproduction. First, we probe representations that a language model produces\nabout different places around the world when asked to describe these contexts.\nSecond, we analyze the cultures represented in the travel recommendations\nproduced by a set of language model applications. Our study shows ways in which\nthe NLP community and application developers can begin to operationalize\ncomplex socio-cultural considerations into standard evaluations and benchmarks.",
    "published_date": "2025-01-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.01056v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.01031v2",
    "title": "ValuesRAG: Enhancing Cultural Alignment Through Retrieval-Augmented Contextual Learning",
    "authors": [
      "Wonduk Seo",
      "Zonghao Yuan",
      "Yi Bu"
    ],
    "author_ids": [],
    "abstract": "Cultural values alignment in Large Language Models (LLMs) is a critical\nchallenge due to their tendency to embed Western-centric biases from training\ndata, leading to misrepresentations and fairness issues in cross-cultural\ncontexts. Recent approaches, such as role-assignment and few-shot learning,\noften struggle with reliable cultural alignment as they heavily rely on\npre-trained knowledge, lack scalability, and fail to capture nuanced cultural\nvalues effectively. To address these issues, we propose ValuesRAG, a novel and\neffective framework that applies Retrieval-Augmented Generation (RAG) with\nIn-Context Learning (ICL) to integrate cultural and demographic knowledge\ndynamically during text generation. Leveraging the World Values Survey (WVS)\ndataset, ValuesRAG first generates summaries of values for each individual.\nSubsequently, we curate several representative regional datasets to serve as\ntest datasets and retrieve relevant summaries of values based on demographic\nfeatures, followed by a reranking step to select the top-k relevant summaries.\nValuesRAG consistently outperforms baseline methods, both in the main\nexperiment and in the ablation study where only the values summary was\nprovided. Notably, ValuesRAG demonstrates an accuracy of 21% improvement over\nother baseline methods, highlighting its potential to foster culturally aligned\nAI systems and enhance the inclusivity of AI-driven applications.",
    "published_date": "2025-01-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.01031v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.00995v1",
    "title": "Is It Still Fair? Investigating Gender Fairness in Cross-Corpus Speech Emotion Recognition",
    "authors": [
      "Shreya G. Upadhyay",
      "Woan-Shiuan Chien",
      "Chi-Chun Lee"
    ],
    "author_ids": [],
    "abstract": "Speech emotion recognition (SER) is a vital component in various everyday\napplications. Cross-corpus SER models are increasingly recognized for their\nability to generalize performance. However, concerns arise regarding fairness\nacross demographics in diverse corpora. Existing fairness research often\nfocuses solely on corpus-specific fairness, neglecting its generalizability in\ncross-corpus scenarios. Our study focuses on this underexplored area, examining\nthe gender fairness generalizability in cross-corpus SER scenarios. We\nemphasize that the performance of cross-corpus SER models and their fairness\nare two distinct considerations. Moreover, we propose the approach of a\ncombined fairness adaptation mechanism to enhance gender fairness in the SER\ntransfer learning tasks by addressing both source and target genders. Our\nfindings bring one of the first insights into the generalizability of gender\nfairness in cross-corpus SER systems.",
    "published_date": "2025-01-02T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.00995v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.00957v3",
    "title": "Generative AI and LLMs in Industry: A text-mining Analysis and Critical Evaluation of Guidelines and Policy Statements Across Fourteen Industrial Sectors",
    "authors": [
      "Junfeng Jiao",
      "Saleh Afroogh",
      "Kevin Chen",
      "David Atkinson",
      "Amit Dhurandhar"
    ],
    "author_ids": [],
    "abstract": "The rise of Generative AI (GAI) and Large Language Models (LLMs) has\ntransformed industrial landscapes, offering unprecedented opportunities for\nefficiency and innovation while raising critical ethical, regulatory, and\noperational challenges. This study conducts a text-based analysis of 160\nguidelines and policy statements across fourteen industrial sectors, utilizing\nsystematic methods and text-mining techniques to evaluate the governance of\nthese technologies. By examining global directives, industry practices, and\nsector-specific policies, the paper highlights the complexities of balancing\ninnovation with ethical accountability and equitable access. The findings\nprovide actionable insights and recommendations for fostering responsible,\ntransparent, and safe integration of GAI and LLMs in diverse industry contexts.",
    "published_date": "2025-01-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.00957v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.00913v1",
    "title": "$β$-DQN: Improving Deep Q-Learning By Evolving the Behavior",
    "authors": [
      "Hongming Zhang",
      "Fengshuo Bai",
      "Chenjun Xiao",
      "Chao Gao",
      "Bo Xu",
      "Martin Müller"
    ],
    "author_ids": [],
    "abstract": "While many sophisticated exploration methods have been proposed, their lack\nof generality and high computational cost often lead researchers to favor\nsimpler methods like $\\epsilon$-greedy. Motivated by this, we introduce\n$\\beta$-DQN, a simple and efficient exploration method that augments the\nstandard DQN with a behavior function $\\beta$. This function estimates the\nprobability that each action has been taken at each state. By leveraging\n$\\beta$, we generate a population of diverse policies that balance exploration\nbetween state-action coverage and overestimation bias correction. An adaptive\nmeta-controller is designed to select an effective policy for each episode,\nenabling flexible and explainable exploration. $\\beta$-DQN is straightforward\nto implement and adds minimal computational overhead to the standard DQN.\nExperiments on both simple and challenging exploration domains show that\n$\\beta$-DQN outperforms existing baseline methods across a wide range of tasks,\nproviding an effective solution for improving exploration in deep reinforcement\nlearning.",
    "published_date": "2025-01-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.00913v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.00910v1",
    "title": "Population Aware Diffusion for Time Series Generation",
    "authors": [
      "Yang Li",
      "Han Meng",
      "Zhenyu Bi",
      "Ingolv T. Urnes",
      "Haipeng Chen"
    ],
    "author_ids": [],
    "abstract": "Diffusion models have shown promising ability in generating high-quality time\nseries (TS) data. Despite the initial success, existing works mostly focus on\nthe authenticity of data at the individual level, but pay less attention to\npreserving the population-level properties on the entire dataset. Such\npopulation-level properties include value distributions for each dimension and\ndistributions of certain functional dependencies (e.g., cross-correlation, CC)\nbetween different dimensions. For instance, when generating house energy\nconsumption TS data, the value distributions of the outside temperature and the\nkitchen temperature should be preserved, as well as the distribution of CC\nbetween them. Preserving such TS population-level properties is critical in\nmaintaining the statistical insights of the datasets, mitigating model bias,\nand augmenting downstream tasks like TS prediction. Yet, it is often overlooked\nby existing models. Hence, data generated by existing models often bear\ndistribution shifts from the original data. We propose Population-aware\nDiffusion for Time Series (PaD-TS), a new TS generation model that better\npreserves the population-level properties. The key novelties of PaD-TS include\n1) a new training method explicitly incorporating TS population-level property\npreservation, and 2) a new dual-channel encoder model architecture that better\ncaptures the TS data structure. Empirical results in major benchmark datasets\nshow that PaD-TS can improve the average CC distribution shift score between\nreal and synthetic data by 5.9x while maintaining a performance comparable to\nstate-of-the-art models on individual-level authenticity.",
    "published_date": "2025-01-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.00910v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2501.01472v1",
    "title": "Augmented Contrastive Clustering with Uncertainty-Aware Prototyping for Time Series Test Time Adaptation",
    "authors": [
      "Peiliang Gong",
      "Mohamed Ragab",
      "Min Wu",
      "Zhenghua Chen",
      "Yongyi Su",
      "Xiaoli Li",
      "Daoqiang Zhang"
    ],
    "author_ids": [],
    "abstract": "Test-time adaptation aims to adapt pre-trained deep neural networks using\nsolely online unlabelled test data during inference. Although TTA has shown\npromise in visual applications, its potential in time series contexts remains\nlargely unexplored. Existing TTA methods, originally designed for visual tasks,\nmay not effectively handle the complex temporal dynamics of real-world time\nseries data, resulting in suboptimal adaptation performance. To address this\ngap, we propose Augmented Contrastive Clustering with Uncertainty-aware\nPrototyping (ACCUP), a straightforward yet effective TTA method for time series\ndata. Initially, our approach employs augmentation ensemble on the time series\ndata to capture diverse temporal information and variations, incorporating\nuncertainty-aware prototypes to distill essential characteristics.\nAdditionally, we introduce an entropy comparison scheme to selectively acquire\nmore confident predictions, enhancing the reliability of pseudo labels.\nFurthermore, we utilize augmented contrastive clustering to enhance feature\ndiscriminability and mitigate error accumulation from noisy pseudo labels,\npromoting cohesive clustering within the same class while facilitating clear\nseparation between different classes. Extensive experiments conducted on three\nreal-world time series datasets and an additional visual dataset demonstrate\nthe effectiveness and generalization potential of the proposed method,\nadvancing the underexplored realm of TTA for time series data.",
    "published_date": "2025-01-01T00:00:00",
    "year": 2025,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2501.01472v1",
    "is_ai_related": true
  }
]