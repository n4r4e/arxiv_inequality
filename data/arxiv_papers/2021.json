[
  {
    "id": "http://arxiv.org/abs/2201.00849v1",
    "title": "Delving into Sample Loss Curve to Embrace Noisy and Imbalanced Data",
    "authors": [
      "Shenwang Jiang",
      "Jianan Li",
      "Ying Wang",
      "Bo Huang",
      "Zhang Zhang",
      "Tingfa Xu"
    ],
    "author_ids": [],
    "abstract": "Corrupted labels and class imbalance are commonly encountered in practically\ncollected training data, which easily leads to over-fitting of deep neural\nnetworks (DNNs). Existing approaches alleviate these issues by adopting a\nsample re-weighting strategy, which is to re-weight sample by designing\nweighting function. However, it is only applicable for training data containing\nonly either one type of data biases. In practice, however, biased samples with\ncorrupted labels and of tailed classes commonly co-exist in training data. How\nto handle them simultaneously is a key but under-explored problem. In this\npaper, we find that these two types of biased samples, though have similar\ntransient loss, have distinguishable trend and characteristics in loss curves,\nwhich could provide valuable priors for sample weight assignment. Motivated by\nthis, we delve into the loss curves and propose a novel probe-and-allocate\ntraining strategy: In the probing stage, we train the network on the whole\nbiased training data without intervention, and record the loss curve of each\nsample as an additional attribute; In the allocating stage, we feed the\nresulting attribute to a newly designed curve-perception network, named\nCurveNet, to learn to identify the bias type of each sample and assign proper\nweights through meta-learning adaptively. The training speed of meta learning\nalso blocks its application. To solve it, we propose a method named skip layer\nmeta optimization (SLMO) to accelerate training speed by skipping the bottom\nlayers. Extensive synthetic and real experiments well validate the proposed\nmethod, which achieves state-of-the-art performance on multiple challenging\nbenchmarks.",
    "published_date": "2021-12-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2201.00849v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.14971v3",
    "title": "Contrastive Fine-grained Class Clustering via Generative Adversarial Networks",
    "authors": [
      "Yunji Kim",
      "Jung-Woo Ha"
    ],
    "author_ids": [],
    "abstract": "Unsupervised fine-grained class clustering is a practical yet challenging\ntask due to the difficulty of feature representations learning of subtle object\ndetails. We introduce C3-GAN, a method that leverages the categorical inference\npower of InfoGAN with contrastive learning. We aim to learn feature\nrepresentations that encourage a dataset to form distinct cluster boundaries in\nthe embedding space, while also maximizing the mutual information between the\nlatent code and its image observation. Our approach is to train a\ndiscriminator, which is also used for inferring clusters, to optimize the\ncontrastive loss, where image-latent pairs that maximize the mutual information\nare considered as positive pairs and the rest as negative pairs. Specifically,\nwe map the input of a generator, which was sampled from the categorical\ndistribution, to the embedding space of the discriminator and let them act as a\ncluster centroid. In this way, C3-GAN succeeded in learning a\nclustering-friendly embedding space where each cluster is distinctively\nseparable. Experimental results show that C3-GAN achieved the state-of-the-art\nclustering performance on four fine-grained image datasets, while also\nalleviating the mode collapse phenomenon. Code is available at\nhttps://github.com/naver-ai/c3-gan.",
    "published_date": "2021-12-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.14971v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.14801v1",
    "title": "Modeling Prejudice and Its Effect on Societal Prosperity",
    "authors": [
      "Deep Inder Mohan",
      "Arjun Verma",
      "Shrisha Rao"
    ],
    "author_ids": [],
    "abstract": "Existing studies on prejudice, which is important in multi-group dynamics in\nsocieties, focus on the social-psychological knowledge behind the processes\ninvolving prejudice and its propagation. We instead create a multi-agent\nframework that simulates the propagation of prejudice and measures its tangible\nimpact on the prosperity of individuals as well as of larger social structures,\nincluding groups and factions within. Groups in society help us define\nprejudice, and factions represent smaller tight-knit circles of individuals\nwith similar opinions. We model social interactions using the Continuous\nPrisoner's Dilemma (CPD) and a type of agent called a prejudiced agent, whose\ncooperation is affected by a prejudice attribute, updated over time based both\non the agent's own experiences and those of others in its faction. Our\nsimulations show that modeling prejudice as an exclusively out-group phenomenon\ngenerates implicit in-group promotion, which eventually leads to higher\nrelative prosperity of the prejudiced population. This skew in prosperity is\nshown to be correlated to factors such as size difference between groups and\nthe number of prejudiced agents in a group. Although prejudiced agents achieve\nhigher prosperity within prejudiced societies, their presence degrades the\noverall prosperity levels of their societies. Our proposed system model can\nserve as a basis for promoting a deeper understanding of origins, propagation,\nand ramifications of prejudice through rigorous simulative studies grounded in\napt theoretical backgrounds. This can help conduct impactful research on\nprominent social issues such as racism, religious discrimination, and unfair\nimmigrant treatment. This model can also serve as a foundation to study other\nsocio-psychological phenomena in tandem with prejudice such as the distribution\nof wealth, social status, and ethnocentrism in a society.",
    "published_date": "2021-12-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.MA",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.14801v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2112.14466v2",
    "title": "Explainability Is in the Mind of the Beholder: Establishing the Foundations of Explainable Artificial Intelligence",
    "authors": [
      "Kacper Sokol",
      "Peter Flach"
    ],
    "author_ids": [],
    "abstract": "Explainable artificial intelligence and interpretable machine learning are\nresearch domains growing in importance. Yet, the underlying concepts remain\nsomewhat elusive and lack generally agreed definitions. While recent\ninspiration from social sciences has refocused the work on needs and\nexpectations of human recipients, the field still misses a concrete\nconceptualisation. We take steps towards addressing this challenge by reviewing\nthe philosophical and social foundations of human explainability, which we then\ntranslate into the technological realm. In particular, we scrutinise the notion\nof algorithmic black boxes and the spectrum of understanding determined by\nexplanatory processes and explainees' background knowledge. This approach\nallows us to define explainability as (logical) reasoning applied to\ntransparent insights (into, possibly black-box, predictive systems) interpreted\nunder background knowledge and placed within a specific context -- a process\nthat engenders understanding in a selected group of explainees. We then employ\nthis conceptualisation to revisit strategies for evaluating explainability as\nwell as the much disputed trade-off between transparency and predictive power,\nincluding its implications for ante-hoc and post-hoc techniques along with\nfairness and accountability established by explainability. We furthermore\ndiscuss components of the machine learning workflow that may be in need of\ninterpretability, building on a range of ideas from human-centred\nexplainability, with a particular focus on explainees, contrastive statements\nand explanatory processes. Our discussion reconciles and complements current\nresearch to help better navigate open questions -- rather than attempting to\naddress any individual issue -- thus laying a solid foundation for a grounded\ndiscussion and future progress of explainable artificial intelligence and\ninterpretable machine learning.",
    "published_date": "2021-12-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.14466v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.14435v2",
    "title": "EiFFFeL: Enforcing Fairness in Forests by Flipping Leaves",
    "authors": [
      "Seyum Assefa Abebe",
      "Claudio Lucchese",
      "Salvatore Orlando"
    ],
    "author_ids": [],
    "abstract": "Nowadays Machine Learning (ML) techniques are extensively adopted in many\nsocially sensitive systems, thus requiring to carefully study the fairness of\nthe decisions taken by such systems. Many approaches have been proposed to\naddress and to make sure there is no bias against individuals or specific\ngroups which might originally come from biased training datasets or algorithm\ndesign. In this regard, we propose a fairness enforcing approach called\nEiFFFeL:Enforcing Fairness in Forests by Flipping Leaves which exploits\ntree-based or leaf-based post-processing strategies to relabel leaves of\nselected decision trees of a given forest. Experimental results show that our\napproach achieves a user defined group fairness degree without losing a\nsignificant amount of accuracy.",
    "published_date": "2021-12-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.14435v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.14772v1",
    "title": "Deep Graph Clustering via Dual Correlation Reduction",
    "authors": [
      "Yue Liu",
      "Wenxuan Tu",
      "Sihang Zhou",
      "Xinwang Liu",
      "Linxuan Song",
      "Xihong Yang",
      "En Zhu"
    ],
    "author_ids": [],
    "abstract": "Deep graph clustering, which aims to reveal the underlying graph structure\nand divide the nodes into different groups, has attracted intensive attention\nin recent years. However, we observe that, in the process of node encoding,\nexisting methods suffer from representation collapse which tends to map all\ndata into the same representation. Consequently, the discriminative capability\nof the node representation is limited, leading to unsatisfied clustering\nperformance. To address this issue, we propose a novel self-supervised deep\ngraph clustering method termed Dual Correlation Reduction Network (DCRN) by\nreducing information correlation in a dual manner. Specifically, in our method,\nwe first design a siamese network to encode samples. Then by forcing the\ncross-view sample correlation matrix and cross-view feature correlation matrix\nto approximate two identity matrices, respectively, we reduce the information\ncorrelation in the dual-level, thus improving the discriminative capability of\nthe resulting features. Moreover, in order to alleviate representation collapse\ncaused by over-smoothing in GCN, we introduce a propagation regularization term\nto enable the network to gain long-distance information with the shallow\nnetwork structure. Extensive experimental results on six benchmark datasets\ndemonstrate the effectiveness of the proposed DCRN against the existing\nstate-of-the-art methods.",
    "published_date": "2021-12-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.14772v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.14338v1",
    "title": "Socially-Optimal Mechanism Design for Incentivized Online Learning",
    "authors": [
      "Zhiyuan Wang",
      "Lin Gao",
      "Jianwei Huang"
    ],
    "author_ids": [],
    "abstract": "Multi-arm bandit (MAB) is a classic online learning framework that studies\nthe sequential decision-making in an uncertain environment. The MAB framework,\nhowever, overlooks the scenario where the decision-maker cannot take actions\n(e.g., pulling arms) directly. It is a practically important scenario in many\napplications such as spectrum sharing, crowdsensing, and edge computing. In\nthese applications, the decision-maker would incentivize other selfish agents\nto carry out desired actions (i.e., pulling arms on the decision-maker's\nbehalf). This paper establishes the incentivized online learning (IOL)\nframework for this scenario. The key challenge to design the IOL framework lies\nin the tight coupling of the unknown environment learning and asymmetric\ninformation revelation. To address this, we construct a special Lagrangian\nfunction based on which we propose a socially-optimal mechanism for the IOL\nframework. Our mechanism satisfies various desirable properties such as agent\nfairness, incentive compatibility, and voluntary participation. It achieves the\nsame asymptotic performance as the state-of-art benchmark that requires extra\ninformation. Our analysis also unveils the power of crowd in the IOL framework:\na larger agent crowd enables our mechanism to approach more closely the\ntheoretical upper bound of social performance. Numerical results demonstrate\nthe advantages of our mechanism in large-scale edge computing.",
    "published_date": "2021-12-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.14338v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.14328v2",
    "title": "QUIC Throughput and Fairness over Dual Connectivity",
    "authors": [
      "David Hasselquist",
      "Christoffer Lindström",
      "Nikita Korzhitskii",
      "Niklas Carlsson",
      "Andrei Gurtov"
    ],
    "author_ids": [],
    "abstract": "Dual Connectivity (DC) is an important lower-layer feature accelerating the\ntransition from 4G to 5G that also is expected to play an important role in\nstandalone 5G radio networks. However, even though the packet reordering\nintroduced by DC can significantly impact the performance of upper-layer\nprotocols, no prior work has studied the impact of DC on QUIC. In this paper,\nwe present the first such performance study. Using a series of throughput and\nfairness experiments, we show how QUIC is affected by different DC parameters,\nnetwork conditions, and whether the DC implementation aims to improve\nthroughput or reliability. Results for two QUIC implementations (aioquic,\nngtcp2) and two congestion control algorithms (NewReno, CUBIC) are presented\nunder both static and highly time-varying network conditions. Our findings\nprovide network operators with insights and understanding into the impacts of\nsplitting QUIC traffic in a DC environment. With reasonably selected DC\nparameters and increased UDP receive buffers, QUIC over DC performs similarly\nto TCP over DC and achieves optimal fairness under symmetric link conditions\nwhen DC is not used for packet duplication. The insights can help network\noperators provide modern users with better end-to-end service when deploying\nDC.",
    "published_date": "2021-12-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.NI",
      "cs.DC",
      "cs.PF",
      "68M12, 68M14, 68M20, 68M10",
      "C.2.2; C.4; C.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.14328v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2112.14233v5",
    "title": "Multitask Learning and Bandits via Robust Statistics",
    "authors": [
      "Kan Xu",
      "Hamsa Bastani"
    ],
    "author_ids": [],
    "abstract": "Decision-makers often simultaneously face many related but heterogeneous\nlearning problems. For instance, a large retailer may wish to learn product\ndemand at different stores to solve pricing or inventory problems, making it\ndesirable to learn jointly for stores serving similar customers; alternatively,\na hospital network may wish to learn patient risk at different providers to\nallocate personalized interventions, making it desirable to learn jointly for\nhospitals serving similar patient populations. Motivated by real datasets, we\nstudy a natural setting where the unknown parameter in each learning instance\ncan be decomposed into a shared global parameter plus a sparse\ninstance-specific term. We propose a novel two-stage multitask learning\nestimator that exploits this structure in a sample-efficient way, using a\nunique combination of robust statistics (to learn across similar instances) and\nLASSO regression (to debias the results). Our estimator yields improved sample\ncomplexity bounds in the feature dimension $d$ relative to commonly-employed\nestimators; this improvement is exponential for \"data-poor\" instances, which\nbenefit the most from multitask learning. We illustrate the utility of these\nresults for online learning by embedding our multitask estimator within\nsimultaneous contextual bandit algorithms. We specify a dynamic calibration of\nour estimator to appropriately balance the bias-variance tradeoff over time,\nimproving the resulting regret bounds in the context dimension $d$. Finally, we\nillustrate the value of our approach on synthetic and real datasets.",
    "published_date": "2021-12-28T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.ST",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.14233v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.14168v1",
    "title": "A Survey on Gender Bias in Natural Language Processing",
    "authors": [
      "Karolina Stanczak",
      "Isabelle Augenstein"
    ],
    "author_ids": [],
    "abstract": "Language can be used as a means of reproducing and enforcing harmful\nstereotypes and biases and has been analysed as such in numerous research. In\nthis paper, we present a survey of 304 papers on gender bias in natural\nlanguage processing. We analyse definitions of gender and its categories within\nsocial sciences and connect them to formal definitions of gender bias in NLP\nresearch. We survey lexica and datasets applied in research on gender bias and\nthen compare and contrast approaches to detecting and mitigating gender bias.\nWe find that research on gender bias suffers from four core limitations. 1)\nMost research treats gender as a binary variable neglecting its fluidity and\ncontinuity. 2) Most of the work has been conducted in monolingual setups for\nEnglish or other high-resource languages. 3) Despite a myriad of papers on\ngender bias in NLP methods, we find that most of the newly developed algorithms\ndo not test their models for bias and disregard possible ethical considerations\nof their work. 4) Finally, methodologies developed in this line of research are\nfundamentally flawed covering very limited definitions of gender bias and\nlacking evaluation baselines and pipelines. We suggest recommendations towards\novercoming these limitations as a guide for future research.",
    "published_date": "2021-12-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.14168v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.13874v2",
    "title": "Unbiased Parameter Inference for a Class of Partially Observed Levy-Process Models",
    "authors": [
      "Hamza Ruzayqat",
      "Ajay Jasra"
    ],
    "author_ids": [],
    "abstract": "We consider the problem of static Bayesian inference for partially observed\nLevy-process models. We develop a methodology which allows one to infer static\nparameters and some states of the process, without a bias from the\ntime-discretization of the afore-mentioned Levy process. The unbiased method is\nexceptionally amenable to parallel implementation and can be computationally\nefficient relative to competing approaches. We implement the method on S & P\n500 log-return daily data and compare it to some Markov chain Monte Carlo\n(MCMC) algorithm.",
    "published_date": "2021-12-27T00:00:00",
    "year": 2021,
    "categories": [
      "stat.CO",
      "cs.NA",
      "math.NA",
      "stat.ME",
      "65C05, 60H35, 62M05, 62M20, 62F15"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.13874v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2112.13835v1",
    "title": "Unbiased Gradient Estimation in Unrolled Computation Graphs with Persistent Evolution Strategies",
    "authors": [
      "Paul Vicol",
      "Luke Metz",
      "Jascha Sohl-Dickstein"
    ],
    "author_ids": [],
    "abstract": "Unrolled computation graphs arise in many scenarios, including training RNNs,\ntuning hyperparameters through unrolled optimization, and training learned\noptimizers. Current approaches to optimizing parameters in such computation\ngraphs suffer from high variance gradients, bias, slow updates, or large memory\nusage. We introduce a method called Persistent Evolution Strategies (PES),\nwhich divides the computation graph into a series of truncated unrolls, and\nperforms an evolution strategies-based update step after each unroll. PES\neliminates bias from these truncations by accumulating correction terms over\nthe entire sequence of unrolls. PES allows for rapid parameter updates, has low\nmemory usage, is unbiased, and has reasonable variance characteristics. We\nexperimentally demonstrate the advantages of PES compared to several other\nmethods for gradient estimation on synthetic tasks, and show its applicability\nto training learned optimizers and tuning hyperparameters.",
    "published_date": "2021-12-27T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.13835v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.13824v2",
    "title": "Fairness in Repetitive Scheduling",
    "authors": [
      "Danny Hermelin",
      "Hendrik Molter",
      "Rolf Niedermeier",
      "Michael Pinedo",
      "Dvir Shabtay"
    ],
    "author_ids": [],
    "abstract": "Recent research found that fairness plays a key role in customer\nsatisfaction. Therefore, many manufacturing and services industries have become\naware of the need to treat customers fairly. Still, there is a huge lack of\nmodels that enable industries to make operational decisions fairly, such as a\nfair scheduling of the customers' jobs. Our main aim in this research is to\nprovide a unified framework to enable schedulers making fair decisions in\nrepetitive scheduling environments. For doing so, we consider a set of\nrepetitive scheduling problems involving a set of $n$ clients. In each out of\n$q$ consecutive operational periods (e.g. days), each one of the customers\nsubmits a job for processing by an operational system. The scheduler's aim is\nto provide a schedule for each of the $q$ periods such that the quality of\nservice (QoS) received by each of the clients will meet a certain predefined\nthreshold. The QoS of a client may take several different forms, e.g., the\nnumber of days that the customer receives its job later than a given due-date,\nthe number of times the customer receive his preferred time slot for service,\nor the sum of waiting times for service. We analyze the single machine variant\nof the problem for several different definitions of QoS, and classify the\ncomplexity of the corresponding problems using the theories of classical and\nparameterized complexity. We also study the price of fairness, i.e., the loss\nin the system's efficiency that results from the need to provide fair\nsolutions.",
    "published_date": "2021-12-27T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DS",
      "cs.DM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.13824v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2112.13758v1",
    "title": "Bridging the Gap: Using Deep Acoustic Representations to Learn Grounded Language from Percepts and Raw Speech",
    "authors": [
      "Gaoussou Youssouf Kebe",
      "Luke E. Richards",
      "Edward Raff",
      "Francis Ferraro",
      "Cynthia Matuszek"
    ],
    "author_ids": [],
    "abstract": "Learning to understand grounded language, which connects natural language to\npercepts, is a critical research area. Prior work in grounded language\nacquisition has focused primarily on textual inputs. In this work we\ndemonstrate the feasibility of performing grounded language acquisition on\npaired visual percepts and raw speech inputs. This will allow interactions in\nwhich language about novel tasks and environments is learned from end users,\nreducing dependence on textual inputs and potentially mitigating the effects of\ndemographic bias found in widely available speech recognition systems. We\nleverage recent work in self-supervised speech representation models and show\nthat learned representations of speech can make language grounding systems more\ninclusive towards specific groups while maintaining or even increasing general\nperformance.",
    "published_date": "2021-12-27T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.13758v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.13398v5",
    "title": "Long Story Short: Omitted Variable Bias in Causal Machine Learning",
    "authors": [
      "Victor Chernozhukov",
      "Carlos Cinelli",
      "Whitney Newey",
      "Amit Sharma",
      "Vasilis Syrgkanis"
    ],
    "author_ids": [],
    "abstract": "We develop a general theory of omitted variable bias for a wide range of\ncommon causal parameters, including (but not limited to) averages of potential\noutcomes, average treatment effects, average causal derivatives, and policy\neffects from covariate shifts. Our theory applies to nonparametric models,\nwhile naturally allowing for (semi-)parametric restrictions (such as partial\nlinearity) when such assumptions are made. We show how simple plausibility\njudgments on the maximum explanatory power of omitted variables are sufficient\nto bound the magnitude of the bias, thus facilitating sensitivity analysis in\notherwise complex, nonlinear models. Finally, we provide flexible and efficient\nstatistical inference methods for the bounds, which can leverage modern machine\nlearning algorithms for estimation. These results allow empirical researchers\nto perform sensitivity analyses in a flexible class of machine-learned causal\nmodels using very simple, and interpretable, tools. We demonstrate the utility\nof our approach with two empirical examples.",
    "published_date": "2021-12-26T00:00:00",
    "year": 2021,
    "categories": [
      "econ.EM",
      "cs.LG",
      "stat.ME",
      "stat.ML",
      "62G"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.13398v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.13352v1",
    "title": "An Interdisciplinary Approach for the Automated Detection and Visualization of Media Bias in News Articles",
    "authors": [
      "Timo Spinde"
    ],
    "author_ids": [],
    "abstract": "Media coverage has a substantial effect on the public perception of events.\nNevertheless, media outlets are often biased. One way to bias news articles is\nby altering the word choice. The automatic identification of bias by word\nchoice is challenging, primarily due to the lack of gold-standard data sets and\nhigh context dependencies. In this research project, I aim to devise data sets\nand methods to identify media bias. To achieve this, I plan to research methods\nusing natural language processing and deep learning while employing models and\nusing analysis concepts from psychology and linguistics. The first results\nindicate the effectiveness of an interdisciplinary research approach. My vision\nis to devise a system that helps news readers become aware of media coverage\ndifferences caused by bias. So far, my best performing BERT-based model is\npre-trained on a larger corpus consisting of distant labels, indicating that\ndistant supervision has the potential to become a solution for the difficult\ntask of bias detection.",
    "published_date": "2021-12-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.13352v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2201.01180v1",
    "title": "Towards Fair Recommendation in Two-Sided Platforms",
    "authors": [
      "Arpita Biswas",
      "Gourab K Patro",
      "Niloy Ganguly",
      "Krishna P. Gummadi",
      "Abhijnan Chakraborty"
    ],
    "author_ids": [],
    "abstract": "Many online platforms today (such as Amazon, Netflix, Spotify, LinkedIn, and\nAirBnB) can be thought of as two-sided markets with producers and customers of\ngoods and services. Traditionally, recommendation services in these platforms\nhave focused on maximizing customer satisfaction by tailoring the results\naccording to the personalized preferences of individual customers. However, our\ninvestigation reinforces the fact that such customer-centric design of these\nservices may lead to unfair distribution of exposure to the producers, which\nmay adversely impact their well-being. On the other hand, a pure\nproducer-centric design might become unfair to the customers. As more and more\npeople are depending on such platforms to earn a living, it is important to\nensure fairness to both producers and customers. In this work, by mapping a\nfair personalized recommendation problem to a constrained version of the\nproblem of fairly allocating indivisible goods, we propose to provide fairness\nguarantees for both sides. Formally, our proposed {\\em FairRec} algorithm\nguarantees Maxi-Min Share ($\\alpha$-MMS) of exposure for the producers, and\nEnvy-Free up to One Item (EF1) fairness for the customers. Extensive\nevaluations over multiple real-world datasets show the effectiveness of {\\em\nFairRec} in ensuring two-sided fairness while incurring a marginal loss in\noverall recommendation quality. Finally, we present a modification of FairRec\n(named as FairRecPlus) that at the cost of additional computation time,\nimproves the recommendation performance for the customers, while maintaining\nthe same fairness guarantees.",
    "published_date": "2021-12-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2201.01180v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.13217v2",
    "title": "\"We do not appreciate being experimented on\": Developer and Researcher Views on the Ethics of Experiments on Open-Source Projects",
    "authors": [
      "Dror G. Feitelson"
    ],
    "author_ids": [],
    "abstract": "A tenet of open source software development is to accept contributions from\nusers-developers (typically after appropriate vetting). But should this also\ninclude interventions done as part of research on open source development?\nFollowing an incident in which buggy code was submitted to the Linux kernel to\nsee whether it would be caught, we conduct a survey among open source\ndevelopers and empirical software engineering researchers to see what behaviors\nthey think are acceptable. This covers two main issues: the use of publicly\naccessible information, and conducting active experimentation. The survey had\n224 respondents. The results indicate that open-source developers are largely\nopen to research, provided it is done transparently. In other words, many would\nagree to experiments on open-source projects if the subjects were notified and\nprovided informed consent, and in special cases also if only the project\nleaders agree. While researchers generally hold similar opinions, they\nsometimes fail to appreciate certain nuances that are important to developers.\nExamples include observing license restrictions on publishing open-source code\nand safeguarding the code. Conversely, researchers seem to be more concerned\nthan developers about privacy issues. Based on these results, it is recommended\nthat open source repositories and projects address use for research in their\naccess guidelines, and that researchers take care to ask permission also when\nnot formally required to do so. We note too that the open source community\nwants to be heard, so professional societies and IRBs should consult with them\nwhen formulating ethics codes.",
    "published_date": "2021-12-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.13217v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2112.13214v1",
    "title": "NeuronFair: Interpretable White-Box Fairness Testing through Biased Neuron Identification",
    "authors": [
      "Haibin Zheng",
      "Zhiqing Chen",
      "Tianyu Du",
      "Xuhong Zhang",
      "Yao Cheng",
      "Shouling Ji",
      "Jingyi Wang",
      "Yue Yu",
      "Jinyin Chen"
    ],
    "author_ids": [],
    "abstract": "Deep neural networks (DNNs) have demonstrated their outperformance in various\ndomains. However, it raises a social concern whether DNNs can produce reliable\nand fair decisions especially when they are applied to sensitive domains\ninvolving valuable resource allocation, such as education, loan, and\nemployment. It is crucial to conduct fairness testing before DNNs are reliably\ndeployed to such sensitive domains, i.e., generating as many instances as\npossible to uncover fairness violations. However, the existing testing methods\nare still limited from three aspects: interpretability, performance, and\ngeneralizability. To overcome the challenges, we propose NeuronFair, a new DNN\nfairness testing framework that differs from previous work in several key\naspects: (1) interpretable - it quantitatively interprets DNNs' fairness\nviolations for the biased decision; (2) effective - it uses the interpretation\nresults to guide the generation of more diverse instances in less time; (3)\ngeneric - it can handle both structured and unstructured data. Extensive\nevaluations across 7 datasets and the corresponding DNNs demonstrate\nNeuronFair's superior performance. For instance, on structured datasets, it\ngenerates much more instances (~x5.84) and saves more time (with an average\nspeedup of 534.56%) compared with the state-of-the-art methods. Besides, the\ninstances of NeuronFair can also be leveraged to improve the fairness of the\nbiased DNNs, which helps build more fair and trustworthy deep learning systems.",
    "published_date": "2021-12-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.13214v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.13143v3",
    "title": "GREED: A Neural Framework for Learning Graph Distance Functions",
    "authors": [
      "Rishabh Ranjan",
      "Siddharth Grover",
      "Sourav Medya",
      "Venkatesan Chakaravarthy",
      "Yogish Sabharwal",
      "Sayan Ranu"
    ],
    "author_ids": [],
    "abstract": "Among various distance functions for graphs, graph and subgraph edit\ndistances (GED and SED respectively) are two of the most popular and expressive\nmeasures. Unfortunately, exact computations for both are NP-hard. To overcome\nthis computational bottleneck, neural approaches to learn and predict edit\ndistance in polynomial time have received much interest. While considerable\nprogress has been made, there exist limitations that need to be addressed.\nFirst, the efficacy of an approximate distance function lies not only in its\napproximation accuracy, but also in the preservation of its properties. To\nelaborate, although GED is a metric, its neural approximations do not provide\nsuch a guarantee. This prohibits their usage in higher order tasks that rely on\nmetric distance functions, such as clustering or indexing. Second, several\nexisting frameworks for GED do not extend to SED due to SED being asymmetric.\nIn this work, we design a novel siamese graph neural network called GREED,\nwhich through a carefully crafted inductive bias, learns GED and SED in a\nproperty-preserving manner. Through extensive experiments across 10 real graph\ndatasets containing up to 7 million edges, we establish that GREED is not only\nmore accurate than the state of the art, but also up to 3 orders of magnitude\nfaster. Even more significantly, due to preserving the triangle inequality, the\ngenerated embeddings are indexable and consequently, even in a CPU-only\nenvironment, GREED is up to 50 times faster than GPU-powered baselines for\ngraph / subgraph retrieval.",
    "published_date": "2021-12-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.13143v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.13058v1",
    "title": "Tri-Transformer Hawkes Process: Three Heads are better than one",
    "authors": [
      "Zhi-yan Song",
      "Jian-wei Liu",
      "Lu-ning Zhang",
      "Ya-nan Han"
    ],
    "author_ids": [],
    "abstract": "Abstract. Most of the real world data we encounter are asynchronous event\nsequence, so the last decades have been characterized by the implementation of\nvarious point process into the field of social networks,electronic medical\nrecords and financial transactions. At the beginning, Hawkes process and its\nvariants which can simulate simultaneously the self-triggering and mutual\ntriggering patterns between different events in complex sequences in a clear\nand quantitative way are more popular.Later on, with the advances of neural\nnetwork, neural Hawkes process has been proposed one after another, and\ngradually become a research hotspot. The proposal of the transformer Hawkes\nprocess (THP) has gained a huge performance improvement, so a new upsurge of\nthe neural Hawkes process based on transformer is set off. However, THP does\nnot make full use of the information of occurrence time and type of event in\nthe asynchronous event sequence. It simply adds the encoding of event type\nconversion and the location encoding of time conversion to the source encoding.\nAt the same time, the learner built from a single transformer will result in an\ninescapable learning bias. In order to mitigate these problems, we propose a\ntri-transformer Hawkes process (Tri-THP) model, in which the event and time\ninformation are added to the dot-product attention as auxiliary information to\nform a new multihead attention. The effectiveness of the Tri-THP is proved by a\nseries of well-designed experiments on both real world and synthetic data.",
    "published_date": "2021-12-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.13058v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.12843v2",
    "title": "Impact of class imbalance on chest x-ray classifiers: towards better evaluation practices for discrimination and calibration performance",
    "authors": [
      "Candelaria Mosquera",
      "Luciana Ferrer",
      "Diego Milone",
      "Daniel Luna",
      "Enzo Ferrante"
    ],
    "author_ids": [],
    "abstract": "This work aims to analyze standard evaluation practices adopted by the\nresearch community when assessing chest x-ray classifiers, particularly\nfocusing on the impact of class imbalance in such appraisals. Our analysis\nconsiders a comprehensive definition of model performance, covering not only\ndiscriminative performance but also model calibration, a topic of research that\nhas received increasing attention during the last years within the machine\nlearning community. Firstly, we conducted a literature study to analyze common\nscientific practices and confirmed that: (1) even when dealing with highly\nimbalanced datasets, the community tends to use metrics that are dominated by\nthe majority class; and (2) it is still uncommon to include calibration studies\nfor chest x-ray classifiers, albeit its importance in the context of\nhealthcare. Secondly, we perform a systematic experiment on two major chest\nx-ray datasets to explore the behavior of several performance metrics under\ndifferent class ratios and show that widely adopted metrics can conceal the\nperformance in the minority class. Finally, we recommend the inclusion of\ncomplementary metrics to better reflect the system's performance in such\nscenarios. Our study indicates that current evaluation practices adopted by the\nresearch community for chest x-ray computer-aided diagnosis systems may not\nreflect their performance in real clinical scenarios, and suggest alternatives\nto improve this situation.",
    "published_date": "2021-12-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.12843v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.12717v4",
    "title": "Forward Composition Propagation for Explainable Neural Reasoning",
    "authors": [
      "Isel Grau",
      "Gonzalo Nápoles",
      "Marilyn Bello",
      "Yamisleydi Salgueiro",
      "Agnieszka Jastrzebska"
    ],
    "author_ids": [],
    "abstract": "This paper proposes an algorithm called Forward Composition Propagation (FCP)\nto explain the predictions of feed-forward neural networks operating on\nstructured classification problems. In the proposed FCP algorithm, each neuron\nis described by a composition vector indicating the role of each problem\nfeature in that neuron. Composition vectors are initialized using a given input\ninstance and subsequently propagated through the whole network until reaching\nthe output layer. The sign of each composition value indicates whether the\ncorresponding feature excites or inhibits the neuron, while the absolute value\nquantifies its impact. The FCP algorithm is executed on a post-hoc basis, i.e.,\nonce the learning process is completed. Aiming to illustrate the FCP algorithm,\nthis paper develops a case study concerning bias detection in a fairness\nproblem in which the ground truth is known. The simulation results show that\nthe composition values closely align with the expected behavior of protected\nfeatures. The source code and supplementary material for this paper are\navailable at https://github.com/igraugar/fcp.",
    "published_date": "2021-12-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.12717v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.12713v2",
    "title": "Modeling Implicit Bias with Fuzzy Cognitive Maps",
    "authors": [
      "Gonzalo Nápoles",
      "Isel Grau",
      "Leonardo Concepción",
      "Lisa Koutsoviti Koumeri",
      "João Paulo Papa"
    ],
    "author_ids": [],
    "abstract": "This paper presents a Fuzzy Cognitive Map model to quantify implicit bias in\nstructured datasets where features can be numeric or discrete. In our proposal,\nproblem features are mapped to neural concepts that are initially activated by\nexperts when running what-if simulations, whereas weights connecting the neural\nconcepts represent absolute correlation/association patterns between features.\nIn addition, we introduce a new reasoning mechanism equipped with a\nnormalization-like transfer function that prevents neurons from saturating.\nAnother advantage of this new reasoning mechanism is that it can easily be\ncontrolled by regulating nonlinearity when updating neurons' activation values\nin each iteration. Finally, we study the convergence of our model and derive\nanalytical conditions concerning the existence and unicity of fixed-point\nattractors.",
    "published_date": "2021-12-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.12713v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.12521v1",
    "title": "Biases in human mobility data impact epidemic modeling",
    "authors": [
      "Frank Schlosser",
      "Vedran Sekara",
      "Dirk Brockmann",
      "Manuel Garcia-Herranz"
    ],
    "author_ids": [],
    "abstract": "Large-scale human mobility data is a key resource in data-driven policy\nmaking and across many scientific fields. Most recently, mobility data was\nextensively used during the COVID-19 pandemic to study the effects of\ngovernmental policies and to inform epidemic models. Large-scale mobility is\noften measured using digital tools such as mobile phones. However, it remains\nan open question how truthfully these digital proxies represent the actual\ntravel behavior of the general population. Here, we examine mobility datasets\nfrom multiple countries and identify two fundamentally different types of bias\ncaused by unequal access to, and unequal usage of mobile phones. We introduce\nthe concept of data generation bias, a previously overlooked type of bias,\nwhich is present when the amount of data that an individual produces influences\ntheir representation in the dataset. We find evidence for data generation bias\nin all examined datasets in that high-wealth individuals are overrepresented,\nwith the richest 20% contributing over 50% of all recorded trips, substantially\nskewing the datasets. This inequality is consequential, as we find mobility\npatterns of different wealth groups to be structurally different, where the\nmobility networks of high-wealth users are denser and contain more long-range\nconnections. To mitigate the skew, we present a framework to debias data and\nshow how simple techniques can be used to increase representativeness. Using\nour approach we show how biases can severely impact outcomes of dynamic\nprocesses such as epidemic simulations, where biased data incorrectly estimates\nthe severity and speed of disease transmission. Overall, we show that a failure\nto account for biases can have detrimental effects on the results of studies\nand urge researchers and practitioners to account for data-fairness in all\nfuture studies of human mobility.",
    "published_date": "2021-12-23T00:00:00",
    "year": 2021,
    "categories": [
      "physics.soc-ph",
      "cs.CY",
      "q-bio.PE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.12521v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2112.12399v1",
    "title": "Towards identifying optimal biased feedback for various user states and traits in motor imagery BCI",
    "authors": [
      "Jelena Mladenović",
      "Jeremy Frey",
      "Smeety Pramij",
      "Jeremie Mattout",
      "Fabien Lotte"
    ],
    "author_ids": [],
    "abstract": "Objective. Neural self-regulation is necessary for achieving control over\nbrain-computer interfaces (BCIs). This can be an arduous learning process\nespecially for motor imagery BCI. Various training methods were proposed to\nassist users in accomplishing BCI control and increase performance. Notably the\nuse of biased feedback, i.e. non-realistic representation of performance.\nBenefits of biased feedback on performance and learning vary between users\n(e.g. depending on their initial level of BCI control) and remain speculative.\nTo disentangle the speculations, we investigate what personality type, initial\nstate and calibration performance (CP) could benefit from a biased feedback.\nMethods. We conduct an experiment (n=30 for 2 sessions). The feedback provided\nto each group (n=10) is either positively, negatively or not biased. Results.\nStatistical analyses suggest that interactions between bias and: 1) workload,\n2) anxiety, and 3) self-control significantly affect online performance. For\ninstance, low initial workload paired with negative bias is associated to\nhigher peak performances (86%) than without any bias (69%). High anxiety\nrelates negatively to performance no matter the bias (60%), while low anxiety\nmatches best with negative bias (76%). For low CP, learning rate (LR) increases\nwith negative bias only short term (LR=2%) as during the second session it\nseverely drops (LR=-1%). Conclusion. We unveil many interactions between said\nhuman factors and bias. Additionally, we use prediction models to confirm and\nreveal even more interactions. Significance. This paper is a first step towards\nidentifying optimal biased feedback for a personality type, state, and CP in\norder to maximize BCI performance and learning.",
    "published_date": "2021-12-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.12399v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2112.12359v3",
    "title": "Dual Path Structural Contrastive Embeddings for Learning Novel Objects",
    "authors": [
      "Bingbin Li",
      "Elvis Han Cui",
      "Yanan Li",
      "Donghui Wang",
      "Weng Kee Wong"
    ],
    "author_ids": [],
    "abstract": "Learning novel classes from a very few labeled samples has attracted\nincreasing attention in machine learning areas. Recent research on either\nmeta-learning based or transfer-learning based paradigm demonstrates that\ngaining information on a good feature space can be an effective solution to\nachieve favorable performance on few-shot tasks. In this paper, we propose a\nsimple but effective paradigm that decouples the tasks of learning feature\nrepresentations and classifiers and only learns the feature embedding\narchitecture from base classes via the typical transfer-learning training\nstrategy. To maintain both the generalization ability across base and novel\nclasses and discrimination ability within each class, we propose a dual path\nfeature learning scheme that effectively combines structural similarity with\ncontrastive feature construction. In this way, both inner-class alignment and\ninter-class uniformity can be well balanced, and result in improved\nperformance. Experiments on three popular benchmarks show that when\nincorporated with a simple prototype based classifier, our method can still\nachieve promising results for both standard and generalized few-shot problems\nin either an inductive or transductive inference setting.",
    "published_date": "2021-12-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.12359v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.12342v3",
    "title": "Individual and Group-wise Classroom Seating Experience: Effects on Student Engagement in Different Courses",
    "authors": [
      "Nan Gao",
      "Mohammad Saiedur Rahaman",
      "Wei Shao",
      "Kaixin Ji",
      "Flora D. Salim"
    ],
    "author_ids": [],
    "abstract": "Seating location in the classroom can affect student engagement, attention\nand academic performance by providing better visibility, improved movement, and\nparticipation in discussions. Existing studies typically explore how\ntraditional seating arrangements (e.g. grouped tables or traditional rows)\ninfluence students' perceived engagement, without considering group seating\nbehaviours under more flexible seating arrangements. Furthermore, survey-based\nmeasures of student engagement are prone to subjectivity and various response\nbias. Therefore, in this research, we investigate how individual and group-wise\nclassroom seating experiences affect student engagement using wearable\nphysiological sensors. We conducted a field study at a high school and\ncollected survey and wearable data from 23 students in 10 courses over four\nweeks. We aim to answer the following research questions: 1. How does the\nseating proximity between students relate to their perceived learning\nengagement? 2. How do students' group seating behaviours relate to their\nphysiologically-based measures of engagement (i.e. physiological arousal and\nphysiological synchrony)? Experiment results indicate that the individual and\ngroup-wise classroom seating experience is associated with perceived student\nengagement and physiologically-based engagement measured from electrodermal\nactivity. We also find that students who sit close together are more likely to\nhave similar learning engagement and tend to have high physiological synchrony.\nThis research opens up opportunities to explore the implications of flexible\nseating arrangements and has great potential to maximize student engagement by\nsuggesting intelligent seating choices in the future.",
    "published_date": "2021-12-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.12342v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2112.12320v1",
    "title": "Model Selection in Batch Policy Optimization",
    "authors": [
      "Jonathan N. Lee",
      "George Tucker",
      "Ofir Nachum",
      "Bo Dai"
    ],
    "author_ids": [],
    "abstract": "We study the problem of model selection in batch policy optimization: given a\nfixed, partial-feedback dataset and $M$ model classes, learn a policy with\nperformance that is competitive with the policy derived from the best model\nclass. We formalize the problem in the contextual bandit setting with linear\nmodel classes by identifying three sources of error that any model selection\nalgorithm should optimally trade-off in order to be competitive: (1)\napproximation error, (2) statistical complexity, and (3) coverage. The first\ntwo sources are common in model selection for supervised learning, where\noptimally trading-off these properties is well-studied. In contrast, the third\nsource is unique to batch policy optimization and is due to dataset shift\ninherent to the setting. We first show that no batch policy optimization\nalgorithm can achieve a guarantee addressing all three simultaneously,\nrevealing a stark contrast between difficulties in batch policy optimization\nand the positive results available in supervised learning. Despite this\nnegative result, we show that relaxing any one of the three error sources\nenables the design of algorithms achieving near-oracle inequalities for the\nremaining two. We conclude with experiments demonstrating the efficacy of these\nalgorithms.",
    "published_date": "2021-12-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.12320v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.12181v2",
    "title": "Simple and near-optimal algorithms for hidden stratification and multi-group learning",
    "authors": [
      "Christopher Tosh",
      "Daniel Hsu"
    ],
    "author_ids": [],
    "abstract": "Multi-group agnostic learning is a formal learning criterion that is\nconcerned with the conditional risks of predictors within subgroups of a\npopulation. The criterion addresses recent practical concerns such as subgroup\nfairness and hidden stratification. This paper studies the structure of\nsolutions to the multi-group learning problem, and provides simple and\nnear-optimal algorithms for the learning problem.",
    "published_date": "2021-12-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.12181v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.12175v4",
    "title": "Recur, Attend or Convolve? On Whether Temporal Modeling Matters for Cross-Domain Robustness in Action Recognition",
    "authors": [
      "Sofia Broomé",
      "Ernest Pokropek",
      "Boyu Li",
      "Hedvig Kjellström"
    ],
    "author_ids": [],
    "abstract": "Most action recognition models today are highly parameterized, and evaluated\non datasets with appearance-wise distinct classes. It has also been shown that\n2D Convolutional Neural Networks (CNNs) tend to be biased toward texture rather\nthan shape in still image recognition tasks, in contrast to humans. Taken\ntogether, this raises suspicion that large video models partly learn spurious\nspatial texture correlations rather than to track relevant shapes over time to\ninfer generalizable semantics from their movement. A natural way to avoid\nparameter explosion when learning visual patterns over time is to make use of\nrecurrence. Biological vision consists of abundant recurrent circuitry, and is\nsuperior to computer vision in terms of domain shift generalization. In this\narticle, we empirically study whether the choice of low-level temporal modeling\nhas consequences for texture bias and cross-domain robustness. In order to\nenable a light-weight and systematic assessment of the ability to capture\ntemporal structure, not revealed from single frames, we provide the Temporal\nShape (TS) dataset, as well as modified domains of Diving48 allowing for the\ninvestigation of spatial texture bias in video models. The combined results of\nour experiments indicate that sound physical inductive bias such as recurrence\nin temporal modeling may be advantageous when robustness to domain shift is\nimportant for the task.",
    "published_date": "2021-12-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.12175v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.12014v2",
    "title": "Quantifying Gender Biases Towards Politicians on Reddit",
    "authors": [
      "Sara Marjanovic",
      "Karolina Stańczak",
      "Isabelle Augenstein"
    ],
    "author_ids": [],
    "abstract": "Despite attempts to increase gender parity in politics, global efforts have\nstruggled to ensure equal female representation. This is likely tied to\nimplicit gender biases against women in authority. In this work, we present a\ncomprehensive study of gender biases that appear in online political\ndiscussion. To this end, we collect 10 million comments on Reddit in\nconversations about male and female politicians, which enables an exhaustive\nstudy of automatic gender bias detection. We address not only misogynistic\nlanguage, but also other manifestations of bias, like benevolent sexism in the\nform of seemingly positive sentiment and dominance attributed to female\npoliticians, or differences in descriptor attribution. Finally, we conduct a\nmulti-faceted study of gender bias towards politicians investigating both\nlinguistic and extra-linguistic cues. We assess 5 different types of gender\nbias, evaluating coverage, combinatorial, nominal, sentimental, and lexical\nbiases extant in social media language and discourse. Overall, we find that,\ncontrary to previous research, coverage and sentiment biases suggest equal\npublic interest in female politicians. Rather than overt hostile or benevolent\nsexism, the results of the nominal and lexical analyses suggest this interest\nis not as professional or respectful as that expressed about male politicians.\nFemale politicians are often named by their first names and are described in\nrelation to their body, clothing, or family; this is a treatment that is not\nsimilarly extended to men. On the now banned far-right subreddits, this\ndisparity is greatest, though differences in gender biases still appear in the\nright and left-leaning subreddits. We release the curated dataset to the public\nfor future studies.",
    "published_date": "2021-12-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.12014v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.11749v1",
    "title": "Class-aware Sounding Objects Localization via Audiovisual Correspondence",
    "authors": [
      "Di Hu",
      "Yake Wei",
      "Rui Qian",
      "Weiyao Lin",
      "Ruihua Song",
      "Ji-Rong Wen"
    ],
    "author_ids": [],
    "abstract": "Audiovisual scenes are pervasive in our daily life. It is commonplace for\nhumans to discriminatively localize different sounding objects but quite\nchallenging for machines to achieve class-aware sounding objects localization\nwithout category annotations, i.e., localizing the sounding object and\nrecognizing its category. To address this problem, we propose a two-stage\nstep-by-step learning framework to localize and recognize sounding objects in\ncomplex audiovisual scenarios using only the correspondence between audio and\nvision. First, we propose to determine the sounding area via coarse-grained\naudiovisual correspondence in the single source cases. Then visual features in\nthe sounding area are leveraged as candidate object representations to\nestablish a category-representation object dictionary for expressive visual\ncharacter extraction. We generate class-aware object localization maps in\ncocktail-party scenarios and use audiovisual correspondence to suppress silent\nareas by referring to this dictionary. Finally, we employ category-level\naudiovisual consistency as the supervision to achieve fine-grained audio and\nsounding object distribution alignment. Experiments on both realistic and\nsynthesized videos show that our model is superior in localizing and\nrecognizing objects as well as filtering out silent ones. We also transfer the\nlearned audiovisual network into the unsupervised object detection task,\nobtaining reasonable performance.",
    "published_date": "2021-12-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.11749v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.11660v4",
    "title": "AED: An black-box NLP classifier model attacker",
    "authors": [
      "Yueyang Liu",
      "Yan Huang",
      "Zhipeng Cai"
    ],
    "author_ids": [],
    "abstract": "Deep Neural Networks (DNNs) have been successful in solving real-world tasks\nin domains such as connected and automated vehicles, disease, and job hiring.\nHowever, their implications are far-reaching in critical application areas.\nHence, there is a growing concern regarding the potential bias and robustness\nof these DNN models. A transparency and robust model is always demanded in\nhigh-stakes domains where reliability and safety are enforced, such as\nhealthcare and finance. While most studies have focused on adversarial image\nattack scenarios, fewer studies have investigated the robustness of DNN models\nin natural language processing (NLP) due to their adversarial samples are\ndifficult to generate. To address this gap, we propose a word-level NLP\nclassifier attack model called \"AED,\" which stands for Attention mechanism\nenabled post-model Explanation with Density peaks clustering algorithm for\nsynonyms search and substitution. AED aims to test the robustness of NLP DNN\nmodels by interpretability their weaknesses and exploring alternative ways to\noptimize them. By identifying vulnerabilities and providing explanations, AED\ncan help improve the reliability and safety of DNN models in critical\napplication areas such as healthcare and automated transportation. Our\nexperiment results demonstrate that compared with other existing models, AED\ncan effectively generate adversarial examples that can fool the victim model\nwhile maintaining the original meaning of the input.",
    "published_date": "2021-12-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.11660v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.11471v1",
    "title": "Towards a Science of Human-AI Decision Making: A Survey of Empirical Studies",
    "authors": [
      "Vivian Lai",
      "Chacha Chen",
      "Q. Vera Liao",
      "Alison Smith-Renner",
      "Chenhao Tan"
    ],
    "author_ids": [],
    "abstract": "As AI systems demonstrate increasingly strong predictive performance, their\nadoption has grown in numerous domains. However, in high-stakes domains such as\ncriminal justice and healthcare, full automation is often not desirable due to\nsafety, ethical, and legal concerns, yet fully manual approaches can be\ninaccurate and time consuming. As a result, there is growing interest in the\nresearch community to augment human decision making with AI assistance. Besides\ndeveloping AI technologies for this purpose, the emerging field of human-AI\ndecision making must embrace empirical approaches to form a foundational\nunderstanding of how humans interact and work with AI to make decisions. To\ninvite and help structure research efforts towards a science of understanding\nand improving human-AI decision making, we survey recent literature of\nempirical human-subject studies on this topic. We summarize the study design\nchoices made in over 100 papers in three important aspects: (1) decision tasks,\n(2) AI models and AI assistance elements, and (3) evaluation metrics. For each\naspect, we summarize current trends, discuss gaps in current practices of the\nfield, and make a list of recommendations for future research. Our survey\nhighlights the need to develop common frameworks to account for the design and\nresearch spaces of human-AI decision making, so that researchers can make\nrigorous choices in study design, and the research community can build on each\nother's work and produce generalizable scientific knowledge. We also hope this\nsurvey will serve as a bridge for HCI and AI communities to work together to\nmutually shape the empirical science and computational technologies for\nhuman-AI decision making.",
    "published_date": "2021-12-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.HC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.11471v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.11279v3",
    "title": "Differential Parity: Relative Fairness Between Two Sets of Decisions",
    "authors": [
      "Zhe Yu",
      "Xiaoyin Xi"
    ],
    "author_ids": [],
    "abstract": "With AI systems widely applied to assist human in decision-making processes\nsuch as talent hiring, school admission, and loan approval; there is an\nincreasing need to ensure that the decisions made are fair. One major challenge\nfor analyzing fairness in decisions is that the standards are highly subjective\nand contextual -- there is no consensus for what absolute fairness means for\nevery scenario. Not to say that different fairness standards often conflict\nwith each other. To bypass this issue, this work aims to test relative fairness\nin decisions. That is, instead of defining what are ``absolutely'' fair\ndecisions, we propose to test the relative fairness of one decision set against\nanother with differential parity -- the difference between two sets of\ndecisions should be independent from a certain sensitive attribute. This\nproposed differential parity fairness notion has the following benefits: (1) it\navoids the ambiguous and contradictory definition of ``absolutely'' fair\ndecisions; (2) it reveals the relative preference and bias between two decision\nsets; (3) differential parity can serve as a new group fairness notion when a\nreference set of decisions (ground truths) is provided. One limitation for\ndifferential parity is that, it requires the two sets of decisions under\ncomparison to be made on the same data subjects. To overcome this limitation,\nwe propose to utilize a machine learning model to bridge the gap between the\ntwo decisions sets made on difference data and estimate the differential\nparity.",
    "published_date": "2021-12-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.11279v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.11216v1",
    "title": "Value Activation for Bias Alleviation: Generalized-activated Deep Double Deterministic Policy Gradients",
    "authors": [
      "Jiafei Lyu",
      "Yu Yang",
      "Jiangpeng Yan",
      "Xiu Li"
    ],
    "author_ids": [],
    "abstract": "It is vital to accurately estimate the value function in Deep Reinforcement\nLearning (DRL) such that the agent could execute proper actions instead of\nsuboptimal ones. However, existing actor-critic methods suffer more or less\nfrom underestimation bias or overestimation bias, which negatively affect their\nperformance. In this paper, we reveal a simple but effective principle: proper\nvalue correction benefits bias alleviation, where we propose the\ngeneralized-activated weighting operator that uses any non-decreasing function,\nnamely activation function, as weights for better value estimation.\nParticularly, we integrate the generalized-activated weighting operator into\nvalue estimation and introduce a novel algorithm, Generalized-activated Deep\nDouble Deterministic Policy Gradients (GD3). We theoretically show that GD3 is\ncapable of alleviating the potential estimation bias. We interestingly find\nthat simple activation functions lead to satisfying performance with no\nadditional tricks, and could contribute to faster convergence. Experimental\nresults on numerous challenging continuous control tasks show that GD3 with\ntask-specific activation outperforms the common baseline methods. We also\nuncover a fact that fine-tuning the polynomial activation function achieves\nsuperior results on most of the tasks.",
    "published_date": "2021-12-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.11216v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.11041v1",
    "title": "Geometry-Aware Unsupervised Domain Adaptation",
    "authors": [
      "You-Wei Luo",
      "Chuan-Xian Ren",
      "Zi-Ying Chen"
    ],
    "author_ids": [],
    "abstract": "Unsupervised Domain Adaptation (UDA) aims to transfer the knowledge from the\nlabeled source domain to the unlabeled target domain in the presence of dataset\nshift. Most existing methods cannot address the domain alignment and class\ndiscrimination well, which may distort the intrinsic data structure for\ndownstream tasks (e.g., classification). To this end, we propose a novel\ngeometry-aware model to learn the transferability and discriminability\nsimultaneously via nuclear norm optimization. We introduce the domain coherence\nand class orthogonality for UDA from the perspective of subspace geometry. The\ndomain coherence will ensure the model has a larger capacity for learning\nseparable representations, and class orthogonality will minimize the\ncorrelation between clusters to alleviate the misalignment. So, they are\nconsistent and can benefit from each other. Besides, we provide a theoretical\ninsight into the norm-based learning literature in UDA, which ensures the\ninterpretability of our model. We show that the norms of domains and clusters\nare expected to be larger and smaller to enhance the transferability and\ndiscriminability, respectively. Extensive experimental results on standard UDA\ndatasets demonstrate the effectiveness of our theory and model.",
    "published_date": "2021-12-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.11041v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.10993v3",
    "title": "Learning in Random Utility Models Via Online Decision Problems",
    "authors": [
      "Emerson Melo"
    ],
    "author_ids": [],
    "abstract": "This paper studies the Random Utility Model (RUM) in a repeated stochastic\nchoice situation, in which the decision maker is imperfectly informed about the\npayoffs of each available alternative. We develop a gradient-based learning\nalgorithm by embedding the RUM into an online decision problem. We show that a\nlarge class of RUMs are Hannan consistent (\\citet{Hahn1957}); that is, the\naverage difference between the expected payoffs generated by a RUM and that of\nthe best-fixed policy in hindsight goes to zero as the number of periods\nincrease. In addition, we show that our gradient-based algorithm is equivalent\nto the Follow the Regularized Leader (FTRL) algorithm, which is widely used in\nthe machine learning literature to model learning in repeated stochastic choice\nproblems. Thus, we provide an economically grounded optimization framework to\nthe FTRL algorithm. Finally, we apply our framework to study recency bias,\nno-regret learning in normal form games, and prediction markets.",
    "published_date": "2021-12-21T00:00:00",
    "year": 2021,
    "categories": [
      "econ.TH",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.10993v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.10947v1",
    "title": "The entropic barrier is $n$-self-concordant",
    "authors": [
      "Sinho Chewi"
    ],
    "author_ids": [],
    "abstract": "For any convex body $K \\subseteq \\mathbb R^n$, S. Bubeck and R. Eldan\nintroduced the entropic barrier on $K$ and showed that it is a $(1+o(1)) \\,\nn$-self-concordant barrier. In this note, we observe that the optimal bound of\n$n$ on the self-concordance parameter holds as a consequence of the dimensional\nBrascamp-Lieb inequality.",
    "published_date": "2021-12-21T00:00:00",
    "year": 2021,
    "categories": [
      "math.MG",
      "cs.LG",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.10947v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2201.00689v2",
    "title": "CausalMTA: Eliminating the User Confounding Bias for Causal Multi-touch Attribution",
    "authors": [
      "Di Yao",
      "Chang Gong",
      "Lei Zhang",
      "Sheng Chen",
      "Jingping Bi"
    ],
    "author_ids": [],
    "abstract": "Multi-touch attribution (MTA), aiming to estimate the contribution of each\nadvertisement touchpoint in conversion journeys, is essential for budget\nallocation and automatically advertising. Existing methods first train a model\nto predict the conversion probability of the advertisement journeys with\nhistorical data and calculate the attribution of each touchpoint using\ncounterfactual predictions. An assumption of these works is the conversion\nprediction model is unbiased, i.e., it can give accurate predictions on any\nrandomly assigned journey, including both the factual and counterfactual ones.\nNevertheless, this assumption does not always hold as the exposed\nadvertisements are recommended according to user preferences. This confounding\nbias of users would lead to an out-of-distribution (OOD) problem in the\ncounterfactual prediction and cause concept drift in attribution. In this\npaper, we define the causal MTA task and propose CausalMTA to eliminate the\ninfluence of user preferences. It systemically eliminates the confounding bias\nfrom both static and dynamic preferences to learn the conversion prediction\nmodel using historical data. We also provide a theoretical analysis to prove\nCausalMTA can learn an unbiased prediction model with sufficient data.\nExtensive experiments on both public datasets and the impression data in an\ne-commerce company show that CausalMTA not only achieves better prediction\nperformance than the state-of-the-art method but also generates meaningful\nattribution credits across different advertising channels.",
    "published_date": "2021-12-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2201.00689v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.11224v1",
    "title": "Attention-Based Sensor Fusion for Human Activity Recognition Using IMU Signals",
    "authors": [
      "Wenjin Tao",
      "Haodong Chen",
      "Md Moniruzzaman",
      "Ming C. Leu",
      "Zhaozheng Yi",
      "Ruwen Qin"
    ],
    "author_ids": [],
    "abstract": "Human Activity Recognition (HAR) using wearable devices such as smart watches\nembedded with Inertial Measurement Unit (IMU) sensors has various applications\nrelevant to our daily life, such as workout tracking and health monitoring. In\nthis paper, we propose a novel attention-based approach to human activity\nrecognition using multiple IMU sensors worn at different body locations.\nFirstly, a sensor-wise feature extraction module is designed to extract the\nmost discriminative features from individual sensors with Convolutional Neural\nNetworks (CNNs). Secondly, an attention-based fusion mechanism is developed to\nlearn the importance of sensors at different body locations and to generate an\nattentive feature representation. Finally, an inter-sensor feature extraction\nmodule is applied to learn the inter-sensor correlations, which are connected\nto a classifier to output the predicted classes of activities. The proposed\napproach is evaluated using five public datasets and it outperforms\nstate-of-the-art methods on a wide variety of activity categories.",
    "published_date": "2021-12-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "eess.SP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.11224v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.10659v2",
    "title": "REFORM: Reputation Based Fair and Temporal Reward Framework for Crowdsourcing",
    "authors": [
      "Samhita Kanaparthy",
      "Sankarshan Damle",
      "Sujit Gujar"
    ],
    "author_ids": [],
    "abstract": "Crowdsourcing is an effective method to collect data by employing distributed\nhuman population. Researchers introduce appropriate reward mechanisms to\nincentivize agents to report accurately. In particular, this paper focuses on\nPeer-Based Mechanisms (PBMs). We observe that with PBMs, crowdsourcing systems\nmay not be fair, i.e., agents may not receive the deserved rewards despite\ninvesting efforts and reporting truthfully. Unfair rewards for the agents may\ndiscourage participation. This paper aims to build a general framework that\nassures fairness for PBMs in temporal settings, i.e., settings that prefer\nearly reports. Towards this, we introduce two general notions of fairness for\nPBMs, namely gamma-fairness and qualitative fairness. To satisfy these notions,\nour framework provides trustworthy agents with additional chances of pairing.\nWe introduce Temporal Reputation Model (TERM) to quantify agents'\ntrustworthiness across tasks. With TERM as the key constituent, we present our\niterative framework, REFORM, that can adopt the reward scheme of any existing\nPBM. We demonstrate REFORM's significance by deploying the framework with\nRPTSC's reward scheme. Specifically, we prove that REFORM with RPTSC\nconsiderably improves fairness; while incentivizing truthful and early reports.\nWe conduct synthetic simulations and show that our framework provides improved\nfairness over RPTSC.",
    "published_date": "2021-12-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.10659v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2112.10572v5",
    "title": "General Greedy De-bias Learning",
    "authors": [
      "Xinzhe Han",
      "Shuhui Wang",
      "Chi Su",
      "Qingming Huang",
      "Qi Tian"
    ],
    "author_ids": [],
    "abstract": "Neural networks often make predictions relying on the spurious correlations\nfrom the datasets rather than the intrinsic properties of the task of interest,\nfacing sharp degradation on out-of-distribution (OOD) test data. Existing\nde-bias learning frameworks try to capture specific dataset bias by annotations\nbut they fail to handle complicated OOD scenarios. Others implicitly identify\nthe dataset bias by special design low capability biased models or losses, but\nthey degrade when the training and testing data are from the same distribution.\nIn this paper, we propose a General Greedy De-bias learning framework (GGD),\nwhich greedily trains the biased models and the base model. The base model is\nencouraged to focus on examples that are hard to solve with biased models, thus\nremaining robust against spurious correlations in the test stage. GGD largely\nimproves models' OOD generalization ability on various tasks, but sometimes\nover-estimates the bias level and degrades on the in-distribution test. We\nfurther re-analyze the ensemble process of GGD and introduce the Curriculum\nRegularization inspired by curriculum learning, which achieves a good trade-off\nbetween in-distribution and out-of-distribution performance. Extensive\nexperiments on image classification, adversarial question answering, and visual\nquestion answering demonstrate the effectiveness of our method. GGD can learn a\nmore robust base model under the settings of both task-specific biased models\nwith prior knowledge and self-ensemble biased model without prior knowledge.",
    "published_date": "2021-12-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.10572v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.10522v2",
    "title": "Improving Ranking Quality and Fairness in Swiss-System Chess Tournaments",
    "authors": [
      "Pascal Sauer",
      "Ágnes Cseh",
      "Pascal Lenzner"
    ],
    "author_ids": [],
    "abstract": "The International Chess Federation (FIDE) imposes a voluminous and complex\nset of player pairing criteria in Swiss-system chess tournaments and endorses\ncomputer programs that are able to calculate the prescribed pairings. The\npurpose of these formalities is to ensure that players are paired fairly during\nthe tournament and that the final ranking corresponds to the players' true\nstrength order. We contest the official FIDE player pairing routine by\npresenting alternative pairing rules. These can be enforced by computing\nmaximum weight matchings in a carefully designed graph. We demonstrate by\nextensive experiments that a tournament format using our mechanism 1) yields\nfairer pairings in the rounds of the tournament and 2) produces a final ranking\nthat reflects the players' true strengths better than the state-of-the-art FIDE\npairing system.",
    "published_date": "2021-12-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.10522v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2112.10290v1",
    "title": "Distributionally Robust Group Backwards Compatibility",
    "authors": [
      "Martin Bertran",
      "Natalia Martinez",
      "Alex Oesterling",
      "Guillermo Sapiro"
    ],
    "author_ids": [],
    "abstract": "Machine learning models are updated as new data is acquired or new\narchitectures are developed. These updates usually increase model performance,\nbut may introduce backward compatibility errors, where individual users or\ngroups of users see their performance on the updated model adversely affected.\nThis problem can also be present when training datasets do not accurately\nreflect overall population demographics, with some groups having overall lower\nparticipation in the data collection process, posing a significant fairness\nconcern. We analyze how ideas from distributional robustness and minimax\nfairness can aid backward compatibility in this scenario, and propose two\nmethods to directly address this issue. Our theoretical analysis is backed by\nexperimental results on CIFAR-10, CelebA, and Waterbirds, three standard image\nclassification datasets. Code available at github.com/natalialmg/GroupBC",
    "published_date": "2021-12-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.10290v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.10264v1",
    "title": "Exploration-exploitation trade-off for continuous-time episodic reinforcement learning with linear-convex models",
    "authors": [
      "Lukasz Szpruch",
      "Tanut Treetanthiploet",
      "Yufei Zhang"
    ],
    "author_ids": [],
    "abstract": "We develop a probabilistic framework for analysing model-based reinforcement\nlearning in the episodic setting. We then apply it to study finite-time horizon\nstochastic control problems with linear dynamics but unknown coefficients and\nconvex, but possibly irregular, objective function. Using probabilistic\nrepresentations, we study regularity of the associated cost functions and\nestablish precise estimates for the performance gap between applying optimal\nfeedback control derived from estimated and true model parameters. We identify\nconditions under which this performance gap is quadratic, improving the linear\nperformance gap in recent work [X. Guo, A. Hu, and Y. Zhang, arXiv preprint,\narXiv:2104.09311, (2021)], which matches the results obtained for stochastic\nlinear-quadratic problems. Next, we propose a phase-based learning algorithm\nfor which we show how to optimise exploration-exploitation trade-off and\nachieve sublinear regrets in high probability and expectation. When assumptions\nneeded for the quadratic performance gap hold, the algorithm achieves an order\n$\\mathcal{O}(\\sqrt{N} \\ln N)$ high probability regret, in the general case, and\nan order $\\mathcal{O}((\\ln N)^2)$ expected regret, in self-exploration case,\nover $N$ episodes, matching the best possible results from the literature. The\nanalysis requires novel concentration inequalities for correlated\ncontinuous-time observations, which we derive.",
    "published_date": "2021-12-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "math.OC",
      "math.PR",
      "stat.ML",
      "93E35, 62G35, 93E11, 68Q32"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.10264v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.10199v2",
    "title": "Tractable Fragments of the Maximum Nash Welfare Problem",
    "authors": [
      "Jugal Garg",
      "Edin Husić",
      "Aniket Murhekar",
      "László Végh"
    ],
    "author_ids": [],
    "abstract": "We study the problem of maximizing Nash welfare (MNW) while allocating\nindivisible goods to asymmetric agents. The Nash welfare of an allocation is\nthe weighted geometric mean of agents' utilities, and the allocation with\nmaximum Nash welfare is known to satisfy several desirable fairness and\nefficiency properties. However, computing such an MNW allocation is NP-hard,\neven for two agents with identical, additive valuations. Hence, we aim to\nidentify tractable classes that either admit a PTAS, an FPTAS, or an exact\npolynomial-time algorithm.\n  To this end, we design a PTAS for finding an MNW allocation for the case of\nasymmetric agents with identical, additive valuations, thus generalizing a\nsimilar result for symmetric agents. Our techniques can also be adapted to give\na PTAS for the problem of computing the optimal $p$-mean welfare. We also show\nthat an MNW allocation can be computed exactly in polynomial time for identical\nagents with $k$-ary valuations when $k$ is a constant, where every agent has at\nmost $k$ different values for the goods. Next, we consider the special case\nwhere every agent finds at most two goods valuable, and show that this class\nadmits an efficient algorithm, even for general monotone valuations. In\ncontrast, we note that when agents can value three or more goods, maximizing\nNash welfare is NP-hard, even when agents are symmetric and have additive\nvaluations, showing our algorithmic result is essentially tight.\n  Finally, we show that for constantly many asymmetric agents with additive\nvaluations, the MNW problem admits an FPTAS.",
    "published_date": "2021-12-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.10199v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2112.10101v1",
    "title": "ArcFace Knows the Gender, Too!",
    "authors": [
      "Majid Farzaneh"
    ],
    "author_ids": [],
    "abstract": "The main idea of this paper is that if a model can recognize a person, of\ncourse, it must be able to know the gender of that person, too. Therefore,\ninstead of defining a new model for gender classification, this paper uses\nArcFace features to determine gender, based on the facial features. A face\nimage is given to ArcFace and 512 features are obtained for the face. Then,\nwith the help of traditional machine learning models, gender is determined.\nDiscriminative methods such as Support Vector Machine (SVM), Linear\nDiscriminant, and Logistic Regression well demonstrate that the features\nextracted from the ArcFace create a remarkable distinction between the gender\nclasses. Experiments on the Gender Classification Dataset show that SVM with\nGaussian kernel is able to classify gender with an accuracy of 96.4% using\nArcFace features.",
    "published_date": "2021-12-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.10101v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.10089v1",
    "title": "Camera-aware Style Separation and Contrastive Learning for Unsupervised Person Re-identification",
    "authors": [
      "Xue Li",
      "Tengfei Liang",
      "Yi Jin",
      "Tao Wang",
      "Yidong Li"
    ],
    "author_ids": [],
    "abstract": "Unsupervised person re-identification (ReID) is a challenging task without\ndata annotation to guide discriminative learning. Existing methods attempt to\nsolve this problem by clustering extracted embeddings to generate pseudo\nlabels. However, most methods ignore the intra-class gap caused by camera style\nvariance, and some methods are relatively complex and indirect although they\ntry to solve the negative impact of the camera style on feature distribution.\nTo solve this problem, we propose a camera-aware style separation and\ncontrastive learning method (CA-UReID), which directly separates camera styles\nin the feature space with the designed camera-aware attention module. It can\nexplicitly divide the learnable feature into camera-specific and\ncamera-agnostic parts, reducing the influence of different cameras. Moreover,\nto further narrow the gap across cameras, we design a camera-aware contrastive\ncenter loss to learn more discriminative embedding for each identity. Extensive\nexperiments demonstrate the superiority of our method over the state-of-the-art\nmethods on the unsupervised person ReID task.",
    "published_date": "2021-12-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.10089v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.10064v1",
    "title": "Data Augmentation for Mental Health Classification on Social Media",
    "authors": [
      "Gunjan Ansari",
      "Muskan Garg",
      "Chandni Saxena"
    ],
    "author_ids": [],
    "abstract": "The mental disorder of online users is determined using social media posts.\nThe major challenge in this domain is to avail the ethical clearance for using\nthe user generated text on social media platforms. Academic re searchers\nidentified the problem of insufficient and unlabeled data for mental health\nclassification. To handle this issue, we have studied the effect of data\naugmentation techniques on domain specific user generated text for mental\nhealth classification. Among the existing well established data augmentation\ntechniques, we have identified Easy Data Augmentation (EDA), conditional BERT,\nand Back Translation (BT) as the potential techniques for generating additional\ntext to improve the performance of classifiers. Further, three different\nclassifiers Random Forest (RF), Support Vector Machine (SVM) and Logistic\nRegression (LR) are employed for analyzing the impact of data augmentation on\ntwo publicly available social media datasets. The experiments mental results\nshow significant improvements in classifiers performance when trained on the\naugmented data.",
    "published_date": "2021-12-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.10064v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.10047v1",
    "title": "Controlling the Quality of Distillation in Response-Based Network Compression",
    "authors": [
      "Vibhas Vats",
      "David Crandall"
    ],
    "author_ids": [],
    "abstract": "The performance of a distillation-based compressed network is governed by the\nquality of distillation. The reason for the suboptimal distillation of a large\nnetwork (teacher) to a smaller network (student) is largely attributed to the\ngap in the learning capacities of given teacher-student pair. While it is hard\nto distill all the knowledge of a teacher, the quality of distillation can be\ncontrolled to a large extent to achieve better performance. Our experiments\nshow that the quality of distillation is largely governed by the quality of\nteacher's response, which in turn is heavily affected by the presence of\nsimilarity information in its response. A well-trained large capacity teacher\nloses similarity information between classes in the process of learning\nfine-grained discriminative properties for classification. The absence of\nsimilarity information causes the distillation process to be reduced from one\nexample-many class learning to one example-one class learning, thereby\nthrottling the flow of diverse knowledge from the teacher. With the implicit\nassumption that only the instilled knowledge can be distilled, instead of\nfocusing only on the knowledge distilling process, we scrutinize the knowledge\ninculcation process. We argue that for a given teacher-student pair, the\nquality of distillation can be improved by finding the sweet spot between batch\nsize and number of epochs while training the teacher. We discuss the steps to\nfind this sweet spot for better distillation. We also propose the distillation\nhypothesis to differentiate the behavior of the distillation process between\nknowledge distillation and regularization effect. We conduct all our\nexperiments on three different datasets.",
    "published_date": "2021-12-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.10047v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.09970v1",
    "title": "3D Structural Analysis of the Optic Nerve Head to Robustly Discriminate Between Papilledema and Optic Disc Drusen",
    "authors": [
      "Michaël J. A. Girard",
      "Satish K. Panda",
      "Tin Aung Tun",
      "Elisabeth A. Wibroe",
      "Raymond P. Najjar",
      "Aung Tin",
      "Alexandre H. Thiéry",
      "Steffen Hamann",
      "Clare Fraser",
      "Dan Milea"
    ],
    "author_ids": [],
    "abstract": "Purpose: (1) To develop a deep learning algorithm to identify major tissue\nstructures of the optic nerve head (ONH) in 3D optical coherence tomography\n(OCT) scans; (2) to exploit such information to robustly differentiate among\nhealthy, optic disc drusen (ODD), and papilledema ONHs.\n  It was a cross-sectional comparative study with confirmed ODD (105 eyes),\npapilledema due to high intracranial pressure (51 eyes), and healthy controls\n(100 eyes). 3D scans of the ONHs were acquired using OCT, then processed to\nimprove deep-tissue visibility. At first, a deep learning algorithm was\ndeveloped using 984 B-scans (from 130 eyes) in order to identify: major\nneural/connective tissues, and ODD regions. The performance of our algorithm\nwas assessed using the Dice coefficient (DC). In a 2nd step, a classification\nalgorithm (random forest) was designed using 150 OCT volumes to perform 3-class\nclassifications (1: ODD, 2: papilledema, 3: healthy) strictly from their drusen\nand prelamina swelling scores (derived from the segmentations). To assess\nperformance, we reported the area under the receiver operating characteristic\ncurves (AUCs) for each class.\n  Our segmentation algorithm was able to isolate neural and connective tissues,\nand ODD regions whenever present. This was confirmed by an average DC of\n0.93$\\pm$0.03 on the test set, corresponding to good performance.\nClassification was achieved with high AUCs, i.e. 0.99$\\pm$0.01 for the\ndetection of ODD, 0.99 $\\pm$ 0.01 for the detection of papilledema, and\n0.98$\\pm$0.02 for the detection of healthy ONHs.\n  Our AI approach accurately discriminated ODD from papilledema, using a single\nOCT scan. Our classification performance was excellent, with the caveat that\nvalidation in a much larger population is warranted. Our approach may have the\npotential to establish OCT as the mainstay of diagnostic imaging in\nneuro-ophthalmology.",
    "published_date": "2021-12-18T00:00:00",
    "year": 2021,
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.09970v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.09786v3",
    "title": "Distill and De-bias: Mitigating Bias in Face Verification using Knowledge Distillation",
    "authors": [
      "Prithviraj Dhar",
      "Joshua Gleason",
      "Aniket Roy",
      "Carlos D. Castillo",
      "P. Jonathon Phillips",
      "Rama Chellappa"
    ],
    "author_ids": [],
    "abstract": "Face recognition networks generally demonstrate bias with respect to\nsensitive attributes like gender, skintone etc. For gender and skintone, we\nobserve that the regions of the face that a network attends to vary by the\ncategory of an attribute. This might contribute to bias. Building on this\nintuition, we propose a novel distillation-based approach called Distill and\nDe-bias (D&D) to enforce a network to attend to similar face regions,\nirrespective of the attribute category. In D&D, we train a teacher network on\nimages from one category of an attribute; e.g. light skintone. Then distilling\ninformation from the teacher, we train a student network on images of the\nremaining category; e.g., dark skintone. A feature-level distillation loss\nconstrains the student network to generate teacher-like representations. This\nallows the student network to attend to similar face regions for all attribute\ncategories and enables it to reduce bias. We also propose a second distillation\nstep on top of D&D, called D&D++. Here, we distill the `un-biasedness' of the\nD&D network into a new student network, the D&D++ network, while training this\nnew network on all attribute categories; e.g., both light and dark skintones.\nThis helps us train a network that is less biased for an attribute, while\nobtaining higher face verification performance than D&D. We show that D&D++\noutperforms existing baselines in reducing gender and skintone bias on the\nIJB-C dataset, while obtaining higher face verification performance than\nexisting adversarial de-biasing methods. We evaluate the effectiveness of our\nproposed methods on two state-of-the-art face recognition networks: ArcFace and\nCrystalface.",
    "published_date": "2021-12-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.09786v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.09763v2",
    "title": "News-sharing on Twitter reveals emergent fragmentation of media agenda and persistent polarization",
    "authors": [
      "Tomas Cicchini",
      "Sofia Morena del Pozo",
      "Enzo Tagliazucchi",
      "Pablo Balenzuela"
    ],
    "author_ids": [],
    "abstract": "News sharing on social networks reveals how information disseminates among\nusers. This process, constrained by user preferences and social ties, plays a\nkey role in the formation of public opinion. In this work we study news sharing\nof main Argentinian media outlets in Twitter, using bipartite news-user\nnetworks, in order to understand if the emergence of affinity groups is driven\nby the underlying political polarization. We compare the results between an\nelectoral and non-electoral year and between a set of politically active users\nand a control group. We found that users' behavior produces well differentiated\ncommunities of news articles identified by a unique distribution of media\noutlets in all analyzed datasets. In particular, these communities split into\ntwo groups which reflect the dominant ideological polarization in Argentina. We\nalso found that users form two well differentiated groups identified by their\npreferences in media outlets consumption. These two groups of media outlets\ndisplay a bias towards the two main political parties that rule the political\nlife in Argentina. These results reveal consistently that ideological\npolarization is the main driving force shaping the Argentinian news sharing in\nTwitter.",
    "published_date": "2021-12-17T00:00:00",
    "year": 2021,
    "categories": [
      "physics.soc-ph",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.09763v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2112.09745v2",
    "title": "Interpretable Data-Based Explanations for Fairness Debugging",
    "authors": [
      "Romila Pradhan",
      "Jiongli Zhu",
      "Boris Glavic",
      "Babak Salimi"
    ],
    "author_ids": [],
    "abstract": "A wide variety of fairness metrics and eXplainable Artificial Intelligence\n(XAI) approaches have been proposed in the literature to identify bias in\nmachine learning models that are used in critical real-life contexts. However,\nmerely reporting on a model's bias, or generating explanations using existing\nXAI techniques is insufficient to locate and eventually mitigate sources of\nbias. We introduce Gopher, a system that produces compact, interpretable and\ncausal explanations for bias or unexpected model behavior by identifying\ncoherent subsets of the training data that are root-causes for this behavior.\nSpecifically, we introduce the concept of causal responsibility that quantifies\nthe extent to which intervening on training data by removing or updating\nsubsets of it can resolve the bias. Building on this concept, we develop an\nefficient approach for generating the top-k patterns that explain model bias\nthat utilizes techniques from the machine learning (ML) community to\napproximate causal responsibility and uses pruning rules to manage the large\nsearch space for patterns. Our experimental evaluation demonstrates the\neffectiveness of Gopher in generating interpretable explanations for\nidentifying and debugging sources of bias.",
    "published_date": "2021-12-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.DB"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.09745v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2201.03445v1",
    "title": "NILC-Metrix: assessing the complexity of written and spoken language in Brazilian Portuguese",
    "authors": [
      "Sidney Evaldo Leal",
      "Magali Sanches Duran",
      "Carolina Evaristo Scarton",
      "Nathan Siegle Hartmann",
      "Sandra Maria Aluísio"
    ],
    "author_ids": [],
    "abstract": "This paper presents and makes publicly available the NILC-Metrix, a\ncomputational system comprising 200 metrics proposed in studies on discourse,\npsycholinguistics, cognitive and computational linguistics, to assess textual\ncomplexity in Brazilian Portuguese (BP). These metrics are relevant for\ndescriptive analysis and the creation of computational models and can be used\nto extract information from various linguistic levels of written and spoken\nlanguage. The metrics in NILC-Metrix were developed during the last 13 years,\nstarting in 2008 with Coh-Metrix-Port, a tool developed within the scope of the\nPorSimples project. Coh-Metrix-Port adapted some metrics to BP from the\nCoh-Metrix tool that computes metrics related to cohesion and coherence of\ntexts in English. After the end of PorSimples in 2010, new metrics were added\nto the initial 48 metrics of Coh-Metrix-Port. Given the large number of\nmetrics, we present them following an organisation similar to the metrics of\nCoh-Metrix v3.0 to facilitate comparisons made with metrics in Portuguese and\nEnglish. In this paper, we illustrate the potential of NILC-Metrix by\npresenting three applications: (i) a descriptive analysis of the differences\nbetween children's film subtitles and texts written for Elementary School I and\nII (Final Years); (ii) a new predictor of textual complexity for the corpus of\noriginal and simplified texts of the PorSimples project; (iii) a complexity\nprediction model for school grades, using transcripts of children's story\nnarratives told by teenagers. For each application, we evaluate which groups of\nmetrics are more discriminative, showing their contribution for each task.",
    "published_date": "2021-12-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2201.03445v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.09542v5",
    "title": "A Formal Model for Polarization under Confirmation Bias in Social Networks",
    "authors": [
      "Mário S. Alvim",
      "Bernardo Amorim",
      "Sophia Knight",
      "Santiago Quintero",
      "Frank Valencia"
    ],
    "author_ids": [],
    "abstract": "We describe a model for polarization in multi-agent systems based on Esteban\nand Ray's standard family of polarization measures from economics. Agents\nevolve by updating their beliefs (opinions) based on an underlying influence\ngraph, as in the standard DeGroot model for social learning, but under a\nconfirmation bias; i.e., a discounting of opinions of agents with dissimilar\nviews. We show that even under this bias polarization eventually vanishes\n(converges to zero) if the influence graph is strongly-connected. If the\ninfluence graph is a regular symmetric circulation, we determine the unique\nbelief value to which all agents converge. Our more insightful result\nestablishes that, under some natural assumptions, if polarization does not\neventually vanish then either there is a disconnected subgroup of agents, or\nsome agent influences others more than she is influenced. We also prove that\npolarization does not necessarily vanish in weakly-connected graphs under\nconfirmation bias. Furthermore, we show how our model relates to the classic\nDeGroot model for social learning. We illustrate our model with several\nsimulations of a running example about polarization over vaccines and of other\ncase studies. The theoretical results and simulations will provide insight into\nthe phenomenon of polarization.",
    "published_date": "2021-12-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LO",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.09542v5",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2112.09466v4",
    "title": "Fair Active Learning: Solving the Labeling Problem in Insurance",
    "authors": [
      "Romuald Elie",
      "Caroline Hillairet",
      "François Hu",
      "Marc Juillard"
    ],
    "author_ids": [],
    "abstract": "This paper addresses significant obstacles that arise from the widespread use\nof machine learning models in the insurance industry, with a specific focus on\npromoting fairness. The initial challenge lies in effectively leveraging\nunlabeled data in insurance while reducing the labeling effort and emphasizing\ndata relevance through active learning techniques. The paper explores various\nactive learning sampling methodologies and evaluates their impact on both\nsynthetic and real insurance datasets. This analysis highlights the difficulty\nof achieving fair model inferences, as machine learning models may replicate\nbiases and discrimination found in the underlying data. To tackle these\ninterconnected challenges, the paper introduces an innovative fair active\nlearning method. The proposed approach samples informative and fair instances,\nachieving a good balance between model predictive performance and fairness, as\nconfirmed by numerical experiments on insurance datasets.",
    "published_date": "2021-12-17T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.09466v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.09439v1",
    "title": "Matching Social Issues to Technologies for Civic Tech by Association Rule Mining using Weighted Casual Confidence",
    "authors": [
      "Masato Kikuchi",
      "Shun Shiramatsu",
      "Ryota Kozakai",
      "Tadachika Ozono"
    ],
    "author_ids": [],
    "abstract": "More than 80 civic tech communities in Japan are developing information\ntechnology (IT) systems to solve their regional issues. Collaboration among\nsuch communities across different regions assists in solving their problems\nbecause some groups have limited IT knowledge and experience for this purpose.\nOur objective is to realize a civic tech matchmaking system to assist such\ncommunities in finding better partners with IT experience in their issues. In\nthis study, as the first step toward collaboration, we acquire relevant social\nissues and information technologies by association rule mining. To meet our\nchallenge, we supply a questionnaire to members of civic tech communities and\nobtain answers on their faced issues and their available technologies.\nSubsequently, we match the relevant issues and technologies from the answers.\nHowever, most of the issues and technologies in this questionnaire data are\ninfrequent, and there is a significant bias in their occurrence. Here, it is\ndifficult to extract truly relevant issues--technologies combinations with\nexisting interestingness measures. Therefore, we introduce a new measure called\nweighted casual confidence, and show that our measure is effective for mining\nrelevant issues--technologies pairs.",
    "published_date": "2021-12-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.DB"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.09439v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2112.09346v2",
    "title": "Balancing Fairness and Robustness via Partial Invariance",
    "authors": [
      "Moulik Choraria",
      "Ibtihal Ferwana",
      "Ankur Mani",
      "Lav R. Varshney"
    ],
    "author_ids": [],
    "abstract": "The Invariant Risk Minimization (IRM) framework aims to learn invariant\nfeatures from a set of environments for solving the out-of-distribution (OOD)\ngeneralization problem. The underlying assumption is that the causal components\nof the data generating distributions remain constant across the environments or\nalternately, the data \"overlaps\" across environments to find meaningful\ninvariant features. Consequently, when the \"overlap\" assumption does not hold,\nthe set of truly invariant features may not be sufficient for optimal\nprediction performance. Such cases arise naturally in networked settings and\nhierarchical data-generating models, wherein the IRM performance becomes\nsuboptimal. To mitigate this failure case, we argue for a partial invariance\nframework. The key idea is to introduce flexibility into the IRM framework by\npartitioning the environments based on hierarchical differences, while\nenforcing invariance locally within the partitions. We motivate this framework\nin classification settings with causal distribution shifts across environments.\nOur results show the capability of the partial invariant risk minimization to\nalleviate the trade-off between fairness and risk in certain settings.",
    "published_date": "2021-12-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.09346v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.09341v1",
    "title": "Personalized On-Device E-health Analytics with Decentralized Block Coordinate Descent",
    "authors": [
      "Guanhua Ye",
      "Hongzhi Yin",
      "Tong Chen",
      "Miao Xu",
      "Quoc Viet Hung Nguyen",
      "Jiangning Song"
    ],
    "author_ids": [],
    "abstract": "Actuated by the growing attention to personal healthcare and the pandemic,\nthe popularity of E-health is proliferating. Nowadays, enhancement on medical\ndiagnosis via machine learning models has been highly effective in many aspects\nof e-health analytics. Nevertheless, in the classic cloud-based/centralized\ne-health paradigms, all the data will be centrally stored on the server to\nfacilitate model training, which inevitably incurs privacy concerns and high\ntime delay. Distributed solutions like Decentralized Stochastic Gradient\nDescent (D-SGD) are proposed to provide safe and timely diagnostic results\nbased on personal devices. However, methods like D-SGD are subject to the\ngradient vanishing issue and usually proceed slowly at the early training\nstage, thereby impeding the effectiveness and efficiency of training. In\naddition, existing methods are prone to learning models that are biased towards\nusers with dense data, compromising the fairness when providing E-health\nanalytics for minority groups. In this paper, we propose a Decentralized Block\nCoordinate Descent (D-BCD) learning framework that can better optimize deep\nneural network-based models distributed on decentralized devices for E-health\nanalytics. Benchmarking experiments on three real-world datasets illustrate the\neffectiveness and practicality of our proposed D-BCD, where additional\nsimulation study showcases the strong applicability of D-BCD in real-life\nE-health scenarios.",
    "published_date": "2021-12-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.09341v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.09253v2",
    "title": "Logically at Factify 2022: Multimodal Fact Verification",
    "authors": [
      "Jie Gao",
      "Hella-Franziska Hoffmann",
      "Stylianos Oikonomou",
      "David Kiskovski",
      "Anil Bandhakavi"
    ],
    "author_ids": [],
    "abstract": "This paper describes our participant system for the multi-modal fact\nverification (Factify) challenge at AAAI 2022. Despite the recent advance in\ntext based verification techniques and large pre-trained multimodal models\ncross vision and language, very limited work has been done in applying\nmultimodal techniques to automate fact checking process, particularly\nconsidering the increasing prevalence of claims and fake news about images and\nvideos on social media. In our work, the challenge is treated as multimodal\nentailment task and framed as multi-class classification. Two baseline\napproaches are proposed and explored including an ensemble model (combining two\nuni-modal models) and a multi-modal attention network (modeling the interaction\nbetween image and text pair from claim and evidence document). We conduct\nseveral experiments investigating and benchmarking different SoTA pre-trained\ntransformers and vision models in this work. Our best model is ranked first in\nleaderboard which obtains a weighted average F-measure of 0.77 on both\nvalidation and test set. Exploratory analysis of dataset is also carried out on\nthe Factify data set and uncovers salient patterns and issues (e.g., word\noverlapping, visual entailment correlation, source bias) that motivates our\nhypothesis. Finally, we highlight challenges of the task and multimodal dataset\nfor future research.",
    "published_date": "2021-12-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.MM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.09253v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.09195v3",
    "title": "Mitigating the Bias of Centered Objects in Common Datasets",
    "authors": [
      "Gergely Szabo",
      "Andras Horvath"
    ],
    "author_ids": [],
    "abstract": "Convolutional networks are considered shift invariant, but it was\ndemonstrated that their response may vary according to the exact location of\nthe objects. In this paper we will demonstrate that most commonly investigated\ndatasets have a bias, where objects are over-represented at the center of the\nimage during training. This bias and the boundary condition of these networks\ncan have a significant effect on the performance of these architectures and\ntheir accuracy drops significantly as an object approaches the boundary. We\nwill also demonstrate how this effect can be mitigated with data augmentation\ntechniques.",
    "published_date": "2021-12-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.09195v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.09047v1",
    "title": "Citation inequity and gendered citation practices in contemporary physics",
    "authors": [
      "Erin G. Teich",
      "Jason Z. Kim",
      "Christopher W. Lynn",
      "Samantha C. Simon",
      "Andrei A. Klishin",
      "Karol P. Szymula",
      "Pragya Srivastava",
      "Lee C. Bassett",
      "Perry Zurn",
      "Jordan D. Dworkin",
      "Dani S. Bassett"
    ],
    "author_ids": [],
    "abstract": "The historical and contemporary under-attribution of women's contributions to\nscientific scholarship is well-known and well-studied, with effects that are\nfelt today in myriad ways by women scientists. One measure of this\nunder-attribution is the so-called citation gap between men and women: the\nunder-citation of papers authored by women relative to expected rates coupled\nwith a corresponding over-citation of papers authored by men relative to\nexpected rates. We explore the citation gap in contemporary physics, analyzing\nover one million articles published over the last 25 years in 35 physics\njournals that span a wide range of subfields. Using a model that predicts\npapers' expected citation rates according to a set of characteristics separate\nfrom author gender, we find a global bias wherein papers authored by women are\nsignificantly under-cited, and papers authored by men are significantly\nover-cited. Moreover, we find that citation behavior varies along several\ndimensions, such that imbalances differ according to who is citing, where they\nare citing, and how they are citing. Specifically, citation imbalance in favor\nof man-authored papers is highest for papers authored by men, papers published\nin general physics journals, and papers likely to be less familiar to citing\nauthors. Our results suggest that, although deciding which papers to cite is an\nindividual choice, the cumulative effects of these choices needlessly harm a\nsubset of scholars. We discuss several strategies for the mitigation of these\neffects, including conscious behavioral changes at the individual, journal, and\ncommunity levels.",
    "published_date": "2021-12-16T00:00:00",
    "year": 2021,
    "categories": [
      "physics.soc-ph",
      "cs.DL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.09047v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2112.08910v3",
    "title": "Degendering Resumes for Fair Algorithmic Resume Screening",
    "authors": [
      "Prasanna Parasurama",
      "João Sedoc"
    ],
    "author_ids": [],
    "abstract": "We investigate whether it is feasible to remove gendered information from\nresumes to mitigate potential bias in algorithmic resume screening. Using a\ncorpus of 709k resumes from IT firms, we first train a series of models to\nclassify the self-reported gender of the applicant, thereby measuring the\nextent and nature of gendered information encoded in resumes. We then conduct a\nseries of gender obfuscation experiments, where we iteratively remove gendered\ninformation from resumes. Finally, we train a resume screening algorithm and\ninvestigate the trade-off between gender obfuscation and screening algorithm\nperformance. Results show: (1) There is a significant amount of gendered\ninformation in resumes. (2) Lexicon-based gender obfuscation method (i.e.\nremoving tokens that are predictive of gender) can reduce the amount of\ngendered information to a large extent. However, after a certain point, the\nperformance of the resume screening algorithm starts suffering. (3)\nGeneral-purpose gender debiasing methods for NLP models such as removing gender\nsubspace from embeddings are not effective in obfuscating gender.",
    "published_date": "2021-12-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.08910v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.11193v2",
    "title": "There is an elephant in the room: Towards a critique on the use of fairness in biometrics",
    "authors": [
      "Ana Valdivia",
      "Júlia Corbera-Serrajòrdia",
      "Aneta Swianiewicz"
    ],
    "author_ids": [],
    "abstract": "In 2019, the UK's Immigration and Asylum Chamber of the Upper Tribunal\ndismissed an asylum appeal basing the decision on the output of a biometric\nsystem, alongside other discrepancies. The fingerprints of the asylum seeker\nwere found in a biometric database which contradicted the appellant's account.\nThe Tribunal found this evidence unequivocal and denied the asylum claim.\nNowadays, the proliferation of biometric systems is shaping public debates\naround its political, social and ethical implications. Yet whilst concerns\ntowards the racialised use of this technology for migration control have been\non the rise, investment in the biometrics industry and innovation is increasing\nconsiderably. Moreover, fairness has also been recently adopted by biometrics\nto mitigate bias and discrimination on biometrics. However, algorithmic\nfairness cannot distribute justice in scenarios which are broken or intended\npurpose is to discriminate, such as biometrics deployed at the border.\n  In this paper, we offer a critical reading of recent debates about biometric\nfairness and show its limitations drawing on research in fairness in machine\nlearning and critical border studies. Building on previous fairness\ndemonstrations, we prove that biometric fairness criteria are mathematically\nmutually exclusive. Then, the paper moves on illustrating empirically that a\nfair biometric system is not possible by reproducing experiments from previous\nworks. Finally, we discuss the politics of fairness in biometrics by situating\nthe debate at the border. We claim that bias and error rates have different\nimpact on citizens and asylum seekers. Fairness has overshadowed the elephant\nin the room of biometrics, focusing on the demographic biases and ethical\ndiscourses of algorithms rather than examine how these systems reproduce\nhistorical and political injustices.",
    "published_date": "2021-12-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.11193v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.08637v3",
    "title": "Analyzing the Limits of Self-Supervision in Handling Bias in Language",
    "authors": [
      "Lisa Bauer",
      "Karthik Gopalakrishnan",
      "Spandana Gella",
      "Yang Liu",
      "Mohit Bansal",
      "Dilek Hakkani-Tur"
    ],
    "author_ids": [],
    "abstract": "Prompting inputs with natural language task descriptions has emerged as a\npopular mechanism to elicit reasonably accurate outputs from large-scale\ngenerative language models with little to no in-context supervision. This also\nhelps gain insight into how well language models capture the semantics of a\nwide range of downstream tasks purely from self-supervised pre-training on\nmassive corpora of unlabeled text. Such models have naturally also been exposed\nto a lot of undesirable content like racist and sexist language and there is\nlimited work on awareness of models along these dimensions. In this paper, we\ndefine and comprehensively evaluate how well such language models capture the\nsemantics of four tasks for bias: diagnosis, identification, extraction and\nrephrasing. We define three broad classes of task descriptions for these tasks:\nstatement, question, and completion, with numerous lexical variants within each\nclass. We study the efficacy of prompting for each task using these classes and\nthe null task description across several decoding methods and few-shot\nexamples. Our analyses indicate that language models are capable of performing\nthese tasks to widely varying degrees across different bias dimensions, such as\ngender and political affiliation. We believe our work is an important step\ntowards unbiased language models by quantifying the limits of current\nself-supervision objectives at accomplishing such sociologically challenging\ntasks.",
    "published_date": "2021-12-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.08637v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.09738v1",
    "title": "Improving Ethical Outcomes with Machine-in-the-Loop: Broadening Human Understanding of Data Annotations",
    "authors": [
      "Ashis Kumer Biswas",
      "Geeta Verma",
      "Justin Otto Barber"
    ],
    "author_ids": [],
    "abstract": "We introduce a machine-in-the-loop pipeline that aims to address root causes\nof unwanted bias in natural language based supervised machine learning tasks in\nthe education domain. Learning from the experiences of students is foundational\nfor education researchers, and academic administrators. 21st-century skills\nlearned from experience are becoming a core part of college and career\nreadiness as well as the hiring process in the new knowledge economy.\nMinoritized students demonstrate these skills in their daily lives, but\ndocumenting, assessing, and validating these skills is a huge problem for\neducational institutions. As an equity focused online platform, LivedX\ntranslates minoritized students' lived experiences into the 21st century\nskills, issues micro-credentials, and creates personal 21st century skills\nportfolio. To automate the micro credential mining from the natural language\ntexts received from the students' submitted essays, we employed a bag-of-word\nmodel to construct a multi-output classifier. Despite our goal, our model\ninitially exacerbated disparate impact on minoritized students. We used a\nmachine-in-the-loop model development pipeline to address the problem and\nrefine the aforementioned model to ensure fairness in its prediction.",
    "published_date": "2021-12-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.09738v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.08453v1",
    "title": "The Need for Ethical, Responsible, and Trustworthy Artificial Intelligence for Environmental Sciences",
    "authors": [
      "Amy McGovern",
      "Imme Ebert-Uphoff",
      "David John Gagne II",
      "Ann Bostrom"
    ],
    "author_ids": [],
    "abstract": "Given the growing use of Artificial Intelligence (AI) and machine learning\n(ML) methods across all aspects of environmental sciences, it is imperative\nthat we initiate a discussion about the ethical and responsible use of AI. In\nfact, much can be learned from other domains where AI was introduced, often\nwith the best of intentions, yet often led to unintended societal consequences,\nsuch as hard coding racial bias in the criminal justice system or increasing\neconomic inequality through the financial system. A common misconception is\nthat the environmental sciences are immune to such unintended consequences when\nAI is being used, as most data come from observations, and AI algorithms are\nbased on mathematical formulas, which are often seen as objective. In this\narticle, we argue the opposite can be the case. Using specific examples, we\ndemonstrate many ways in which the use of AI can introduce similar consequences\nin the environmental sciences. This article will stimulate discussion and\nresearch efforts in this direction. As a community, we should avoid repeating\nany foreseeable mistakes made in other domains through the introduction of AI.\nIn fact, with proper precautions, AI can be a great tool to help {\\it reduce}\nclimate and environmental injustice. We primarily focus on weather and climate\nexamples but the conclusions apply broadly across the environmental sciences.",
    "published_date": "2021-12-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "K.4.0; I.2.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.08453v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.08268v1",
    "title": "Prescriptive Machine Learning for Automated Decision Making: Challenges and Opportunities",
    "authors": [
      "Eyke Hüllermeier"
    ],
    "author_ids": [],
    "abstract": "Recent applications of machine learning (ML) reveal a noticeable shift from\nits use for predictive modeling in the sense of a data-driven construction of\nmodels mainly used for the purpose of prediction (of ground-truth facts) to its\nuse for prescriptive modeling. What is meant by this is the task of learning a\nmodel that stipulates appropriate decisions about the right course of action in\nreal-world scenarios: Which medical therapy should be applied? Should this\nperson be hired for the job? As argued in this article, prescriptive modeling\ncomes with new technical conditions for learning and new demands regarding\nreliability, responsibility, and the ethics of decision making. Therefore, to\nsupport the data-driven design of decision-making agents that act in a rational\nbut at the same time responsible manner, a rigorous methodological foundation\nof prescriptive ML is needed. The purpose of this short paper is to elaborate\non specific characteristics of prescriptive ML and to highlight some key\nchallenges it implies. Besides, drawing connections to other branches of\ncontemporary AI research, the grounding of prescriptive ML in a (generalized)\ndecision-theoretic framework is advocated.",
    "published_date": "2021-12-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.08268v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.08256v1",
    "title": "Est-ce que vous compute? Code-switching, cultural identity, and AI",
    "authors": [
      "Arianna Falbo",
      "Travis LaCroix"
    ],
    "author_ids": [],
    "abstract": "Cultural code-switching concerns how we adjust our overall behaviours,\nmanners of speaking, and appearance in response to a perceived change in our\nsocial environment. We defend the need to investigate cultural code-switching\ncapacities in artificial intelligence systems. We explore a series of ethical\nand epistemic issues that arise when bringing cultural code-switching to bear\non artificial intelligence. Building upon Dotson's (2014) analysis of\ntestimonial smothering, we discuss how emerging technologies in AI can give\nrise to epistemic oppression, and specifically, a form of self-silencing that\nwe call 'cultural smothering'. By leaving the socio-dynamic features of\ncultural code-switching unaddressed, AI systems risk negatively impacting\nalready-marginalised social groups by widening opportunity gaps and further\nentrenching social inequalities.",
    "published_date": "2021-12-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.08256v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.08237v1",
    "title": "Exposure Inequality in People Recommender Systems: The Long-Term Effects",
    "authors": [
      "Francesco Fabbri",
      "Maria Luisa Croci",
      "Francesco Bonchi",
      "Carlos Castillo"
    ],
    "author_ids": [],
    "abstract": "People recommender systems may affect the exposure that users receive in\nsocial networking platforms, influencing attention dynamics and potentially\nstrengthening pre-existing inequalities that disproportionately affect certain\ngroups.\n  In this paper we introduce a model to simulate the feedback loop created by\nmultiple rounds of interactions between users and a link recommender in a\nsocial network. This allows us to study the long-term consequences of those\nparticular recommendation algorithms. Our model is equipped with several\nparameters to control (i) the level of homophily in the network, (ii) the\nrelative size of the groups, (iii) the choice among several state-of-the-art\nlink recommenders, and (iv) the choice among three different user behavior\nmodels, that decide which recommendations are accepted or rejected.\n  Our extensive experimentation with the proposed model shows that a minority\ngroup, if homophilic enough, can get a disproportionate advantage in exposure\nfrom all link recommenders. Instead, when it is heterophilic, it gets\nunder-exposed. Moreover, while the homophily level of the minority affects the\nspeed of the growth of the disparate exposure, the relative size of the\nminority affects the magnitude of the effect. Finally, link recommenders\nstrengthen exposure inequalities at the individual level, exacerbating the\n\"rich-get-richer\" effect: this happens for both the minority and the majority\nclass and independently of their level of homophily.",
    "published_date": "2021-12-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.08237v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2112.08100v1",
    "title": "Tensor Codes and their Invariants",
    "authors": [
      "Eimear Byrne",
      "Giuseppe Cotardo"
    ],
    "author_ids": [],
    "abstract": "In 1991, Roth introduced a natural generalization of rank metric codes,\nnamely tensor codes. The latter are defined to be subspaces of $r$-tensors\nwhere the ambient space is endowed with the tensor rank as a distance function.\nIn this work, we describe the general class of tensor codes and we study their\ninvariants that correspond to different families of anticodes. In our context,\nan anticode is a perfect space that has some additional properties. A perfect\nspace is one that is spanned by tensors of rank 1. Our use of the anticode\nconcept is motivated by an interest in capturing structural properties of\ntensor codes. In particular, we indentify four different classes of tensor\nanticodes and show how these gives different information on the codes they\ndescribe. We also define the generalized tensor binomial moments and the\ngeneralized tensor weight distribution of a code and establish a bijection\nbetween these invariants. We use the generalized tensor binomial moments to\ndefine the concept of an $i$-tensor BMD code, which is an extremal code in\nrelation to an inequality arising from them. Finally, we give MacWilliams\nidentities for generalized tensor binomial moments.",
    "published_date": "2021-12-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.08100v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2112.07968v1",
    "title": "Science Factionalism: How Group Identity Language Affects Public Engagement with Misinformation and Debunking Narratives on a Popular Q&A Platform in China",
    "authors": [
      "Kaiping Chen",
      "Yepeng Jin",
      "Anqi Shao"
    ],
    "author_ids": [],
    "abstract": "Misinformation and intergroup bias are two pathologies challenging informed\ncitizenship. This paper examines how identity language is used in\nmisinformation and debunking messages about controversial science on Chinese\ndigital public sphere, and their impact on how the public engage with science.\nWe collected an eight-year time series dataset of public discussion (N=6039) on\none of the most controversial science issues in China (GMO) from a popular Q&A\nplatform, Zhihu. We found that both misinformation and debunking messages use a\nsubstantial amount of group identity languages when discussing the\ncontroversial science issue, which we define as science factionalism --\ndiscussion about science is divided by factions that are formed upon science\nattitudes. We found that posts that use science factionalism receive more\ndigital votes and comments, even among the science-savvy community in China.\nScience factionalism also increases the use of negativity in public discourse.\nWe discussed the implications of how science factionalism interacts with the\ndigital attention economy to affect public engagement with science\nmisinformation.",
    "published_date": "2021-12-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC",
      "cs.MM",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.07968v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2112.07868v2",
    "title": "Few-shot Instruction Prompts for Pretrained Language Models to Detect Social Biases",
    "authors": [
      "Shrimai Prabhumoye",
      "Rafal Kocielnik",
      "Mohammad Shoeybi",
      "Anima Anandkumar",
      "Bryan Catanzaro"
    ],
    "author_ids": [],
    "abstract": "Detecting social bias in text is challenging due to nuance, subjectivity, and\ndifficulty in obtaining good quality labeled datasets at scale, especially\ngiven the evolving nature of social biases and society. To address these\nchallenges, we propose a few-shot instruction-based method for prompting\npre-trained language models (LMs). We select a few class-balanced exemplars\nfrom a small support repository that are closest to the query to be labeled in\nthe embedding space. We then provide the LM with instruction that consists of\nthis subset of labeled exemplars, the query text to be classified, a definition\nof bias, and prompt it to make a decision. We demonstrate that large LMs used\nin a few-shot context can detect different types of fine-grained biases with\nsimilar and sometimes superior accuracy to fine-tuned models. We observe that\nthe largest 530B parameter model is significantly more effective in detecting\nsocial bias compared to smaller models (achieving at least 13% improvement in\nAUC metric compared to other models). It also maintains a high AUC (dropping\nless than 2%) when the labeled repository is reduced to as few as $100$\nsamples. Large pretrained language models thus make it easier and quicker to\nbuild new bias detectors.",
    "published_date": "2021-12-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.07868v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.07773v1",
    "title": "Filling gaps in trustworthy development of AI",
    "authors": [
      "Shahar Avin",
      "Haydn Belfield",
      "Miles Brundage",
      "Gretchen Krueger",
      "Jasmine Wang",
      "Adrian Weller",
      "Markus Anderljung",
      "Igor Krawczuk",
      "David Krueger",
      "Jonathan Lebensold",
      "Tegan Maharaj",
      "Noa Zilberman"
    ],
    "author_ids": [],
    "abstract": "The range of application of artificial intelligence (AI) is vast, as is the\npotential for harm. Growing awareness of potential risks from AI systems has\nspurred action to address those risks, while eroding confidence in AI systems\nand the organizations that develop them. A 2019 study found over 80\norganizations that published and adopted \"AI ethics principles'', and more have\njoined since. But the principles often leave a gap between the \"what\" and the\n\"how\" of trustworthy AI development. Such gaps have enabled questionable or\nethically dubious behavior, which casts doubts on the trustworthiness of\nspecific organizations, and the field more broadly. There is thus an urgent\nneed for concrete methods that both enable AI developers to prevent harm and\nallow them to demonstrate their trustworthiness through verifiable behavior.\nBelow, we explore mechanisms (drawn from arXiv:2004.07213) for creating an\necosystem where AI developers can earn trust - if they are trustworthy. Better\nassessment of developer trustworthiness could inform user choice, employee\nactions, investment decisions, legal recourse, and emerging governance regimes.",
    "published_date": "2021-12-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.07773v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.11208v2",
    "title": "Artificial Intelligence Ethics and Safety: practical tools for creating \"good\" models",
    "authors": [
      "Nicholas Kluge Corrêa"
    ],
    "author_ids": [],
    "abstract": "The AI Robotics Ethics Society (AIRES) is a non-profit organization founded\nin 2018 by Aaron Hui to promote awareness and the importance of ethical\nimplementation and regulation of AI. AIRES is now an organization with chapters\nat universities such as UCLA (Los Angeles), USC (University of Southern\nCalifornia), Caltech (California Institute of Technology), Stanford University,\nCornell University, Brown University, and the Pontifical Catholic University of\nRio Grande do Sul (Brazil). AIRES at PUCRS is the first international chapter\nof AIRES, and as such, we are committed to promoting and enhancing the AIRES\nMission. Our mission is to focus on educating the AI leaders of tomorrow in\nethical principles to ensure that AI is created ethically and responsibly. As\nthere are still few proposals for how we should implement ethical principles\nand normative guidelines in the practice of AI system development, the goal of\nthis work is to try to bridge this gap between discourse and praxis. Between\nabstract principles and technical implementation. In this work, we seek to\nintroduce the reader to the topic of AI Ethics and Safety. At the same time, we\npresent several tools to help developers of intelligent systems develop \"good\"\nmodels. This work is a developing guide published in English and Portuguese.\nContributions and suggestions are welcome.",
    "published_date": "2021-12-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.11208v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.07583v3",
    "title": "Reinforcing Semantic-Symmetry for Document Summarization",
    "authors": [
      "Mingyang Song",
      "Liping Jing"
    ],
    "author_ids": [],
    "abstract": "Document summarization condenses a long document into a short version with\nsalient information and accurate semantic descriptions. The main issue is how\nto make the output summary semantically consistent with the input document. To\nreach this goal, recently, researchers have focused on supervised end-to-end\nhybrid approaches, which contain an extractor module and abstractor module.\nAmong them, the extractor identifies the salient sentences from the input\ndocument, and the abstractor generates a summary from the salient sentences.\nThis model successfully keeps the consistency between the generated summary and\nthe reference summary via various strategies (e.g., reinforcement learning).\nThere are two semantic gaps when training the hybrid model (one is between\ndocument and extracted sentences, and the other is between extracted sentences\nand summary). However, they are not explicitly considered in the existing\nmethods, which usually results in a semantic bias of summary. To mitigate the\nabove issue, in this paper, a new \\textbf{r}einforcing\ns\\textbf{e}mantic-\\textbf{sy}mmetry learning \\textbf{m}odel is proposed for\ndocument summarization (\\textbf{ReSyM}). ReSyM introduces a\nsemantic-consistency reward in the extractor to bridge the first gap. A\nsemantic dual-reward is designed to bridge the second gap in the abstractor.\nThe whole document summarization process is implemented via reinforcement\nlearning with a hybrid reward mechanism (combining the above two rewards).\nMoreover, a comprehensive sentence representation learning method is presented\nto sufficiently capture the information from the original document. A series of\nexperiments have been conducted on two wildly used benchmark datasets CNN/Daily\nMail and BigPatent. The results have shown the superiority of ReSyM by\ncomparing it with the state-of-the-art baselines in terms of various evaluation\nmetrics.",
    "published_date": "2021-12-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.07583v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.07467v8",
    "title": "AI Ethics Principles in Practice: Perspectives of Designers and Developers",
    "authors": [
      "Conrad Sanderson",
      "David Douglas",
      "Qinghua Lu",
      "Emma Schleiger",
      "Jon Whittle",
      "Justine Lacey",
      "Glenn Newnham",
      "Stefan Hajkowicz",
      "Cathy Robinson",
      "David Hansen"
    ],
    "author_ids": [],
    "abstract": "As consensus across the various published AI ethics principles is approached,\na gap remains between high-level principles and practical techniques that can\nbe readily adopted to design and develop responsible AI systems. We examine the\npractices and experiences of researchers and engineers from Australia's\nnational scientific research agency (CSIRO), who are involved in designing and\ndeveloping AI systems for many application areas. Semi-structured interviews\nwere used to examine how the practices of the participants relate to and align\nwith a set of high-level AI ethics principles proposed by the Australian\nGovernment. The principles comprise: (1) privacy protection and security, (2)\nreliability and safety, (3) transparency and explainability, (4) fairness, (5)\ncontestability, (6) accountability, (7) human-centred values, (8) human, social\nand environmental wellbeing. Discussions on the gained insights from the\ninterviews include various tensions and trade-offs between the principles, and\nprovide suggestions for implementing each high-level principle. We also present\nsuggestions aiming to enhance associated support mechanisms.",
    "published_date": "2021-12-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "68T01",
      "K.4.1; K.4.2; K.4.3; K.7.4; K.7.m; I.2.m; I.5.m"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.07467v8",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.07447v1",
    "title": "Measuring Fairness with Biased Rulers: A Survey on Quantifying Biases in Pretrained Language Models",
    "authors": [
      "Pieter Delobelle",
      "Ewoenam Kwaku Tokpo",
      "Toon Calders",
      "Bettina Berendt"
    ],
    "author_ids": [],
    "abstract": "An increasing awareness of biased patterns in natural language processing\nresources, like BERT, has motivated many metrics to quantify `bias' and\n`fairness'. But comparing the results of different metrics and the works that\nevaluate with such metrics remains difficult, if not outright impossible. We\nsurvey the existing literature on fairness metrics for pretrained language\nmodels and experimentally evaluate compatibility, including both biases in\nlanguage models as in their downstream tasks. We do this by a mixture of\ntraditional literature survey and correlation analysis, as well as by running\nempirical evaluations. We find that many metrics are not compatible and highly\ndepend on (i) templates, (ii) attribute and target seeds and (iii) the choice\nof embeddings. These results indicate that fairness or bias evaluation remains\nchallenging for contextualized language models, if not at least highly\nsubjective. To improve future comparisons and fairness evaluations, we\nrecommend avoiding embedding-based metrics and focusing on fairness evaluations\nin downstream tasks.",
    "published_date": "2021-12-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.07447v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.07392v2",
    "title": "Do You Think It's Biased? How To Ask For The Perception Of Media Bias",
    "authors": [
      "Timo Spinde",
      "Christina Kreuter",
      "Wolfgang Gaissmaier",
      "Felix Hamborg",
      "Bela Gipp",
      "Helge Giese"
    ],
    "author_ids": [],
    "abstract": "Media coverage possesses a substantial effect on the public perception of\nevents. The way media frames events can significantly alter the beliefs and\nperceptions of our society. Nevertheless, nearly all media outlets are known to\nreport news in a biased way. While such bias can be introduced by altering the\nword choice or omitting information, the perception of bias also varies largely\ndepending on a reader's personal background. Therefore, media bias is a very\ncomplex construct to identify and analyze. Even though media bias has been the\nsubject of many studies, previous assessment strategies are oversimplified,\nlack overlap and empirical evaluation. Thus, this study aims to develop a scale\nthat can be used as a reliable standard to evaluate article bias. To name an\nexample: Intending to measure bias in a news article, should we ask, \"How\nbiased is the article?\" or should we instead ask, \"How did the article treat\nthe American president?\". We conducted a literature search to find 824 relevant\nquestions about text perception in previous research on the topic. In a\nmulti-iterative process, we summarized and condensed these questions\nsemantically to conclude a complete and representative set of possible question\ntypes about bias. The final set consisted of 25 questions with varying\nanswering formats, 17 questions using semantic differentials, and six ratings\nof feelings. We tested each of the questions on 190 articles with overall 663\nparticipants to identify how well the questions measure an article's perceived\nbias. Our results show that 21 final items are suitable and reliable for\nmeasuring the perception of media bias. We publish the final set of questions\non http://bias-question-tree.gipplab.org/.",
    "published_date": "2021-12-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.07392v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.07391v2",
    "title": "TASSY -- A Text Annotation Survey System",
    "authors": [
      "Timo Spinde",
      "Kanishka Sinha",
      "Norman Meuschke",
      "Bela Gipp"
    ],
    "author_ids": [],
    "abstract": "We present a free and open-source tool for creating web-based surveys that\ninclude text annotation tasks. Existing tools offer either text annotation or\nsurvey functionality but not both. Combining the two input types is\nparticularly relevant for investigating a reader's perception of a text which\nalso depends on the reader's background, such as age, gender, and education.\nOur tool caters primarily to the needs of researchers in the Library and\nInformation Sciences, the Social Sciences, and the Humanities who apply Content\nAnalysis to investigate, e.g., media bias, political communication, or fake\nnews.",
    "published_date": "2021-12-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.07391v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.07239v1",
    "title": "Compensating trajectory bias for unsupervised patient stratification using adversarial recurrent neural networks",
    "authors": [
      "Avelino Javer",
      "Owen Parsons",
      "Oliver Carr",
      "Janie Baxter",
      "Christian Diedrich",
      "Eren Elçi",
      "Steffen Schaper",
      "Katrin Coboeken",
      "Robert Dürichen"
    ],
    "author_ids": [],
    "abstract": "Electronic healthcare records are an important source of information which\ncan be used in patient stratification to discover novel disease phenotypes.\nHowever, they can be challenging to work with as data is often sparse and\nirregularly sampled. One approach to solve these limitations is learning dense\nembeddings that represent individual patient trajectories using a recurrent\nneural network autoencoder (RNN-AE). This process can be susceptible to\nunwanted data biases. We show that patient embeddings and clusters using\npreviously proposed RNN-AE models might be impacted by a trajectory bias,\nmeaning that results are dominated by the amount of data contained in each\npatients trajectory, instead of clinically relevant details. We investigate\nthis bias on 2 datasets (from different hospitals) and 2 disease areas as well\nas using different parts of the patient trajectory. Our results using 2\npreviously published baseline methods indicate a particularly strong bias in\ncase of an event-to-end trajectory. We present a method that can overcome this\nissue using an adversarial training scheme on top of a RNN-AE. Our results show\nthat our approach can reduce the trajectory bias in all cases.",
    "published_date": "2021-12-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.07239v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.07209v1",
    "title": "ACE-BERT: Adversarial Cross-modal Enhanced BERT for E-commerce Retrieval",
    "authors": [
      "Boxuan Zhang",
      "Chao Wei",
      "Yan Jin",
      "Weiru Zhang"
    ],
    "author_ids": [],
    "abstract": "Nowadays on E-commerce platforms, products are presented to the customers\nwith multiple modalities. These multiple modalities are significant for a\nretrieval system while providing attracted products for customers. Therefore,\nhow to take into account those multiple modalities simultaneously to boost the\nretrieval performance is crucial. This problem is a huge challenge to us due to\nthe following reasons: (1) the way of extracting patch features with the\npre-trained image model (e.g., CNN-based model) has much inductive bias. It is\ndifficult to capture the efficient information from the product image in\nE-commerce. (2) The heterogeneity of multimodal data makes it challenging to\nconstruct the representations of query text and product including title and\nimage in a common subspace. We propose a novel Adversarial Cross-modal Enhanced\nBERT (ACE-BERT) for efficient E-commerce retrieval. In detail, ACE-BERT\nleverages the patch features and pixel features as image representation. Thus\nthe Transformer architecture can be applied directly to the raw image\nsequences. With the pre-trained enhanced BERT as the backbone network, ACE-BERT\nfurther adopts adversarial learning by adding a domain classifier to ensure the\ndistribution consistency of different modality representations for the purpose\nof narrowing down the representation gap between query and product.\nExperimental results demonstrate that ACE-BERT outperforms the state-of-the-art\napproaches on the retrieval task. It is remarkable that ACE-BERT has already\nbeen deployed in our E-commerce's search engine, leading to 1.46% increase in\nrevenue.",
    "published_date": "2021-12-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.07209v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.07164v1",
    "title": "Contention Based Proportional Fairness (CBPF) Transmission Scheme for Time Slotted Channel Hopping Networks",
    "authors": [
      "Lokesh Bommisetty",
      "TG Venkatesh"
    ],
    "author_ids": [],
    "abstract": "Time Slotted Channel Hopping (TSCH) is a Medium Access Control (MAC) protocol\nintroduced in IEEE802.15.4e standard, addressing low power requirements of the\nInternet of Things (IoT) and Low Power Lossy Networks (LLNs). The 6TiSCH\nOperation sublayer (6top) of IEEE802.15.4e defines the schedule that includes\nsleep, transmit and receive routines of the nodes. However, the design of\nschedule is not specified by the standard. In this paper, we propose a\ncontention based proportional fairness (CBPF) transmission scheme for TSCH\nnetworks to maximize the system throughput addressing fair allocation of\nresources to the nodes. We propose a convex programming based method to achieve\nthe fairness and throughput objectives. We model TSCH MAC as a multichannel\nslotted aloha and analyse it for a schedule given by the 6top layer.\nPerformance metrics like throughput, delay and energy spent per successful\ntransmission are derived and validated through simulations. The proposed CBPF\ntransmission scheme has been implemented in the IoT-LAB public testbed to\nevaluate its performance and to compare with the existing scheduling\nalgorithms.",
    "published_date": "2021-12-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.07164v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2112.07106v2",
    "title": "E-CRF: Embedded Conditional Random Field for Boundary-caused Class Weights Confusion in Semantic Segmentation",
    "authors": [
      "Jie Zhu",
      "Huabin Huang",
      "Banghuai Li",
      "Leye Wang"
    ],
    "author_ids": [],
    "abstract": "Modern semantic segmentation methods devote much effect to adjusting image\nfeature representations to improve the segmentation performance in various\nways, such as architecture design, attention mechnism, etc. However, almost all\nthose methods neglect the particularity of class weights (in the classification\nlayer) in segmentation models. In this paper, we notice that the class weights\nof categories that tend to share many adjacent boundary pixels lack\ndiscrimination, thereby limiting the performance. We call this issue\nBoundary-caused Class Weights Confusion (BCWC). We try to focus on this problem\nand propose a novel method named Embedded Conditional Random Field (E-CRF) to\nalleviate it. E-CRF innovatively fuses the CRF into the CNN network as an\norganic whole for more effective end-to-end optimization. The reasons are two\nfolds. It utilizes CRF to guide the message passing between pixels in\nhigh-level features to purify the feature representation of boundary pixels,\nwith the help of inner pixels belonging to the same object. More importantly,\nit enables optimizing class weights from both scale and direction during\nbackpropagation. We make detailed theoretical analysis to prove it. Besides,\nsuperpixel is integrated into E-CRF and served as an auxiliary to exploit the\nlocal object prior for more reliable message passing. Finally, our proposed\nmethod yields impressive results on ADE20K, Cityscapes, and Pascal Context\ndatasets.",
    "published_date": "2021-12-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.07106v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.07030v4",
    "title": "Clustering with fair-center representation: parameterized approximation algorithms and heuristics",
    "authors": [
      "Suhas Thejaswi",
      "Ameet Gadekar",
      "Bruno Ordozgoiti",
      "Michal Osadnik"
    ],
    "author_ids": [],
    "abstract": "We study a variant of classical clustering formulations in the context of\nalgorithmic fairness, known as diversity-aware clustering. In this variant we\nare given a collection of facility subsets, and a solution must contain at\nleast a specified number of facilities from each subset while simultaneously\nminimizing the clustering objective ($k$-median or $k$-means). We investigate\nthe fixed-parameter tractability of these problems and show several negative\nhardness and inapproximability results, even when we afford exponential running\ntime with respect to some parameters.\n  Motivated by these results we identify natural parameters of the problem, and\npresent fixed-parameter approximation algorithms with approximation ratios\n$\\big(1 + \\frac{2}{e} +\\epsilon \\big)$ and $\\big(1 + \\frac{8}{e}+ \\epsilon\n\\big)$ for diversity-aware $k$-median and diversity-aware $k$-means\nrespectively, and argue that these ratios are essentially tight assuming the\ngap-exponential time hypothesis. We also present a simple and more practical\nbicriteria approximation algorithm with better running time bounds. We finally\npropose efficient and practical heuristics. We evaluate the scalability and\neffectiveness of our methods in a wide variety of rigorously conducted\nexperiments, on both real and synthetic data.",
    "published_date": "2021-12-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DS",
      "math.CO",
      "G.2.1; F.2.0; F.1.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.07030v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2112.06899v2",
    "title": "Locally Fair Partitioning",
    "authors": [
      "Pankaj K. Agarwal",
      "Shao-Heng Ko",
      "Kamesh Munagala",
      "Erin Taylor"
    ],
    "author_ids": [],
    "abstract": "We model the societal task of redistricting political districts as a\npartitioning problem: Given a set of $n$ points in the plane, each belonging to\none of two parties, and a parameter $k$, our goal is to compute a partition\n$\\Pi$ of the plane into regions so that each region contains roughly $\\sigma =\nn/k$ points. $\\Pi$ should satisfy a notion of ''local'' fairness, which is\nrelated to the notion of core, a well-studied concept in cooperative game\ntheory. A region is associated with the majority party in that region, and a\npoint is unhappy in $\\Pi$ if it belongs to the minority party. A group $D$ of\nroughly $\\sigma$ contiguous points is called a deviating group with respect to\n$\\Pi$ if majority of points in $D$ are unhappy in $\\Pi$. The partition $\\Pi$ is\nlocally fair if there is no deviating group with respect to $\\Pi$.\n  This paper focuses on a restricted case when points lie in $1$D. The problem\nis non-trivial even in this case. We consider both adversarial and ''beyond\nworst-case\" settings for this problem. For the former, we characterize the\ninput parameters for which a locally fair partition always exists; we also show\nthat a locally fair partition may not exist for certain parameters. We then\nconsider input models where there are ''runs'' of red and blue points. For such\nclustered inputs, we show that a locally fair partition may not exist for\ncertain values of $\\sigma$, but an approximate locally fair partition exists if\nwe allow some regions to have smaller sizes. We finally present a\npolynomial-time algorithm for computing a locally fair partition if one exists.",
    "published_date": "2021-12-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.06899v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2112.06868v2",
    "title": "Variational autoencoders in the presence of low-dimensional data: landscape and implicit bias",
    "authors": [
      "Frederic Koehler",
      "Viraj Mehta",
      "Chenghui Zhou",
      "Andrej Risteski"
    ],
    "author_ids": [],
    "abstract": "Variational Autoencoders are one of the most commonly used generative models,\nparticularly for image data. A prominent difficulty in training VAEs is data\nthat is supported on a lower-dimensional manifold. Recent work by Dai and Wipf\n(2020) proposes a two-stage training algorithm for VAEs, based on a conjecture\nthat in standard VAE training the generator will converge to a solution with 0\nvariance which is correctly supported on the ground truth manifold. They gave\npartial support for that conjecture by showing that some optima of the VAE loss\ndo satisfy this property, but did not analyze the training dynamics. In this\npaper, we show that for linear encoders/decoders, the conjecture is true-that\nis the VAE training does recover a generator with support equal to the ground\ntruth manifold-and does so due to an implicit bias of gradient descent rather\nthan merely the VAE loss itself. In the nonlinear case, we show that VAE\ntraining frequently learns a higher-dimensional manifold which is a superset of\nthe ground truth manifold.",
    "published_date": "2021-12-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.06868v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.06837v1",
    "title": "Sparse Interventions in Language Models with Differentiable Masking",
    "authors": [
      "Nicola De Cao",
      "Leon Schmid",
      "Dieuwke Hupkes",
      "Ivan Titov"
    ],
    "author_ids": [],
    "abstract": "There has been a lot of interest in understanding what information is\ncaptured by hidden representations of language models (LMs). Typically,\ninterpretation methods i) do not guarantee that the model actually uses the\nencoded information, and ii) do not discover small subsets of neurons\nresponsible for a considered phenomenon. Inspired by causal mediation analysis,\nwe propose a method that discovers within a neural LM a small subset of neurons\nresponsible for a particular linguistic phenomenon, i.e., subsets causing a\nchange in the corresponding token emission probabilities. We use a\ndifferentiable relaxation to approximately search through the combinatorial\nspace. An $L_0$ regularization term ensures that the search converges to\ndiscrete and sparse solutions. We apply our method to analyze subject-verb\nnumber agreement and gender bias detection in LSTMs. We observe that it is fast\nand finds better solutions than the alternative (REINFORCE). Our experiments\nconfirm that each of these phenomenons is mediated through a small subset of\nneurons that do not play any other discernible role.",
    "published_date": "2021-12-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.06837v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.06926v1",
    "title": "Addressing Bias in Active Learning with Depth Uncertainty Networks... or Not",
    "authors": [
      "Chelsea Murray",
      "James U. Allingham",
      "Javier Antorán",
      "José Miguel Hernández-Lobato"
    ],
    "author_ids": [],
    "abstract": "Farquhar et al. [2021] show that correcting for active learning bias with\nunderparameterised models leads to improved downstream performance. For\noverparameterised models such as NNs, however, correction leads either to\ndecreased or unchanged performance. They suggest that this is due to an\n\"overfitting bias\" which offsets the active learning bias. We show that depth\nuncertainty networks operate in a low overfitting regime, much like\nunderparameterised models. They should therefore see an increase in performance\nwith bias correction. Surprisingly, they do not. We propose that this negative\nresult, as well as the results Farquhar et al. [2021], can be explained via the\nlens of the bias-variance decomposition of generalisation error.",
    "published_date": "2021-12-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.06926v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.06733v4",
    "title": "Measuring Context-Word Biases in Lexical Semantic Datasets",
    "authors": [
      "Qianchu Liu",
      "Diana McCarthy",
      "Anna Korhonen"
    ],
    "author_ids": [],
    "abstract": "State-of-the-art pretrained contextualized models (PCM) eg. BERT use tasks\nsuch as WiC and WSD to evaluate their word-in-context representations. This\ninherently assumes that performance in these tasks reflect how well a model\nrepresents the coupled word and context semantics. We question this assumption\nby presenting the first quantitative analysis on the context-word interaction\nbeing tested in major contextual lexical semantic tasks. To achieve this, we\nrun probing baselines on masked input, and propose measures to calculate and\nvisualize the degree of context or word biases in existing datasets. The\nanalysis was performed on both models and humans. Our findings demonstrate that\nmodels are usually not being tested for word-in-context semantics in the same\nway as humans are in these tasks, which helps us better understand the\nmodel-human gap. Specifically, to PCMs, most existing datasets fall into the\nextreme ends (the retrieval-based tasks exhibit strong target word bias while\nWiC-style tasks and WSD show strong context bias); In comparison, humans are\nless biased and achieve much better performance when both word and context are\navailable than with masked input. We recommend our framework for understanding\nand controlling these biases for model interpretation and future task design.",
    "published_date": "2021-12-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.06733v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.06615v3",
    "title": "Quick Order Fairness",
    "authors": [
      "Christian Cachin",
      "Jovana Mićić",
      "Nathalie Steinhauer",
      "Luca Zanolini"
    ],
    "author_ids": [],
    "abstract": "Leader-based protocols for consensus, i.e., atomic broadcast, allow some\nprocesses to unilaterally affect the final order of transactions. This has\nbecome a problem for blockchain networks and decentralized finance because it\nfacilitates front-running and other attacks. To address this, order fairness\nfor payload messages has been introduced recently as a new safety property for\natomic broadcast complementing traditional agreement and liveness. We relate\norder fairness to the standard validity notions for consensus protocols and\nhighlight some limitations with the existing formalization. Based on this, we\nintroduce a new differential order fairness property that fixes these issues.\nWe also present the quick order-fair atomic broadcast protocol that guarantees\npayload message delivery in a differentially fair order and is much more\nefficient than existing order-fair consensus protocols. It works for\nasynchronous and for eventually synchronous networks with optimal resilience,\ntolerating corruptions of up to one third of the processes. Previous solutions\nrequired there to be less than one fourth of faults. Furthermore, our protocol\nincurs only quadratic cost, in terms of amortized message complexity per\ndelivered payload.",
    "published_date": "2021-12-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.06615v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2112.06522v1",
    "title": "Anatomizing Bias in Facial Analysis",
    "authors": [
      "Richa Singh",
      "Puspita Majumdar",
      "Surbhi Mittal",
      "Mayank Vatsa"
    ],
    "author_ids": [],
    "abstract": "Existing facial analysis systems have been shown to yield biased results\nagainst certain demographic subgroups. Due to its impact on society, it has\nbecome imperative to ensure that these systems do not discriminate based on\ngender, identity, or skin tone of individuals. This has led to research in the\nidentification and mitigation of bias in AI systems. In this paper, we\nencapsulate bias detection/estimation and mitigation algorithms for facial\nanalysis. Our main contributions include a systematic review of algorithms\nproposed for understanding bias, along with a taxonomy and extensive overview\nof existing bias mitigation algorithms. We also discuss open challenges in the\nfield of biased facial analysis.",
    "published_date": "2021-12-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.06522v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.06495v1",
    "title": "Max-Min Energy-Efficiency Fair Optimization in STAR-RIS Assisted System",
    "authors": [
      "Ruitianyi Lu",
      "Kehao Wang"
    ],
    "author_ids": [],
    "abstract": "In this paper, we investigate simultaneously transmitting and reflecting\nreconfigurable intelligent surfaces (STAR-RIS), which enables to communicate\nwith users both sides by transmitting signals to users forward and reflecting\nsignals to users backward simultaneously. We consider a communicatio system\nwith a STAR-RIS, a base station (BS) and many users to maximize the minimum\nuser energy efficiency (EE) by jointly optimizing the active beamforming,\ntransmission and reflection coefficients with BS power consumption limited. To\nsolve this optimization problem efficiently, we divide it into two sub-problems\nto optimize the transmitting beamforming matrix and phase shifts of STAR-RIS\nseperately. With the two subproblems fixed, an Alternating Optimization (AO)\nmethod is proposed to solve the maximize minmum user EE fair optimization\nproblem. Numerical results can demonstrate that STAR-RIS behaves better than\ntraditional reflecting-only RIS, and the algorithm we desigend can maximize the\nminum EE problem efficienty to ensure user fairness.",
    "published_date": "2021-12-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IT",
      "cs.SI",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.06495v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2201.06921v1",
    "title": "Can Machine Learning be Moral?",
    "authors": [
      "Miguel Sicart",
      "Irina Shklovski",
      "Mirabelle Jones"
    ],
    "author_ids": [],
    "abstract": "The ethics of Machine Learning has become an unavoidable topic in the AI\nCommunity. The deployment of machine learning systems in multiple social\ncontexts has resulted in a closer ethical scrutiny of the design, development,\nand application of these systems. The AI/ML community has come to terms with\nthe imperative to think about the ethical implications of machine learning, not\nonly as a product but also as a practice (Birhane, 2021; Shen et al. 2021). The\ncritical question that is troubling many debates is what can constitute an\nethically accountable machine learning system. In this paper we explore\npossibilities for ethical evaluation of machine learning methodologies. We\nscrutinize techniques, methods and technical practices in machine learning from\na relational ethics perspective, taking into consideration how machine learning\nsystems are part of the world and how they relate to different forms of agency.\nTaking a page from Phil Agre (1997) we use the notion of a critical technical\npractice as a means of analysis of machine learning approaches. Our radical\nproposal is that supervised learning appears to be the only machine learning\nmethod that is ethically defensible.",
    "published_date": "2021-12-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2201.06921v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.06455v8",
    "title": "Self-Paced Deep Regression Forests with Consideration of Ranking Fairness",
    "authors": [
      "Lili Pan",
      "Mingming Meng",
      "Yazhou Ren",
      "Yali Zheng",
      "Zenglin Xu"
    ],
    "author_ids": [],
    "abstract": "Deep discriminative models (DDMs), e.g. deep regression forests and deep\ndecision forests, have been extensively studied recently to solve problems such\nas facial age estimation, head pose estimation, etc.. Due to a shortage of\nwell-labeled data that does not have noise and imbalanced distribution\nproblems, learning DDMs is always challenging. Existing methods usually tackle\nthese challenges through learning more discriminative features or re-weighting\nsamples. We argue that learning DDMs gradually, from easy to hard, is more\nreasonable, for two reasons. First, this is more consistent with the cognitive\nprocess of human beings. Second, noisy as well as underrepresented examples can\nbe distinguished by virtue of previously learned knowledge. Thus, we resort to\na gradual learning strategy -- self-paced learning (SPL). Then, a natural\nquestion arises: can SPL lead DDMs to achieve more robust and less biased\nsolutions? To answer this question, this paper proposes a new SPL method: easy\nand underrepresented examples first, for learning DDMs. This tackles the\nfundamental ranking and selection problem in SPL from a new perspective:\nfairness. Our idea is fundamental and can be easily combined with a variety of\nDDMs. Extensive experimental results on three computer vision tasks, i.e.,\nfacial age estimation, head pose estimation, and gaze estimation, show our new\nmethod gains considerable performance improvement in both accuracy and\nfairness. Source code is available at https://github.com/learninginvision/SPU.",
    "published_date": "2021-12-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.06455v8",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.06431v2",
    "title": "GM Score: Incorporating inter-class and intra-class generator diversity, discriminability of disentangled representation, and sample fidelity for evaluating GANs",
    "authors": [
      "Harshvardhan GM",
      "Aanchal Sahu",
      "Mahendra Kumar Gourisaria"
    ],
    "author_ids": [],
    "abstract": "While generative adversarial networks (GAN) are popular for their higher\nsample quality as opposed to other generative models like the variational\nautoencoders (VAE) and Boltzmann machines, they suffer from the same difficulty\nof the evaluation of generated samples. Various aspects must be kept in mind,\nsuch as the quality of generated samples, the diversity of classes (within a\nclass and among classes), the use of disentangled latent spaces, agreement of\nsaid evaluation metric with human perception, etc. In this paper, we propose a\nnew score, namely, GM Score, which takes into various factors such as sample\nquality, disentangled representation, intra-class and inter-class diversity,\nand other metrics such as precision, recall, and F1 score are employed for\ndiscriminability of latent space of deep belief network (DBN) and restricted\nBoltzmann machine (RBM). The evaluation is done for different GANs (GAN, DCGAN,\nBiGAN, CGAN, CoupledGAN, LSGAN, SGAN, WGAN, and WGAN Improved) trained on the\nbenchmark MNIST dataset.",
    "published_date": "2021-12-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML",
      "I.2.6"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.06431v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.06409v3",
    "title": "Data Collection and Quality Challenges in Deep Learning: A Data-Centric AI Perspective",
    "authors": [
      "Steven Euijong Whang",
      "Yuji Roh",
      "Hwanjun Song",
      "Jae-Gil Lee"
    ],
    "author_ids": [],
    "abstract": "Data-centric AI is at the center of a fundamental shift in software\nengineering where machine learning becomes the new software, powered by big\ndata and computing infrastructure. Here software engineering needs to be\nre-thought where data becomes a first-class citizen on par with code. One\nstriking observation is that a significant portion of the machine learning\nprocess is spent on data preparation. Without good data, even the best machine\nlearning algorithms cannot perform well. As a result, data-centric AI practices\nare now becoming mainstream. Unfortunately, many datasets in the real world are\nsmall, dirty, biased, and even poisoned. In this survey, we study the research\nlandscape for data collection and data quality primarily for deep learning\napplications. Data collection is important because there is lesser need for\nfeature engineering for recent deep learning approaches, but instead more need\nfor large amounts of data. For data quality, we study data validation,\ncleaning, and integration techniques. Even if the data cannot be fully cleaned,\nwe can still cope with imperfect data during model training using robust model\ntraining techniques. In addition, while bias and fairness have been less\nstudied in traditional data management research, these issues become essential\ntopics in modern machine learning applications. We thus study fairness measures\nand unfairness mitigation techniques that can be applied before, during, or\nafter model training. We believe that the data management community is well\npoised to solve these problems.",
    "published_date": "2021-12-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.06409v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.06402v2",
    "title": "MotionBenchMaker: A Tool to Generate and Benchmark Motion Planning Datasets",
    "authors": [
      "Constantinos Chamzas",
      "Carlos Quintero-Peña",
      "Zachary Kingston",
      "Andreas Orthey",
      "Daniel Rakita",
      "Michael Gleicher",
      "Marc Toussaint",
      "Lydia E. Kavraki"
    ],
    "author_ids": [],
    "abstract": "Recently, there has been a wealth of development in motion planning for\nrobotic manipulation new motion planners are continuously proposed, each with\ntheir own unique strengths and weaknesses. However, evaluating new planners is\nchallenging and researchers often create their own ad-hoc problems for\nbenchmarking, which is time-consuming, prone to bias, and does not directly\ncompare against other state-of-the-art planners. We present MotionBenchMaker,\nan open-source tool to generate benchmarking datasets for realistic robot\nmanipulation problems. MotionBenchMaker is designed to be an extensible,\neasy-to-use tool that allows users to both generate datasets and benchmark them\nby comparing motion planning algorithms. Empirically, we show the benefit of\nusing MotionBenchMaker as a tool to procedurally generate datasets which helps\nin the fair evaluation of planners. We also present a suite of 40 prefabricated\ndatasets, with 5 different commonly used robots in 8 environments, to serve as\na common ground to accelerate motion planning research.",
    "published_date": "2021-12-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.06402v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2112.06390v2",
    "title": "PartGlot: Learning Shape Part Segmentation from Language Reference Games",
    "authors": [
      "Juil Koo",
      "Ian Huang",
      "Panos Achlioptas",
      "Leonidas Guibas",
      "Minhyuk Sung"
    ],
    "author_ids": [],
    "abstract": "We introduce PartGlot, a neural framework and associated architectures for\nlearning semantic part segmentation of 3D shape geometry, based solely on part\nreferential language. We exploit the fact that linguistic descriptions of a\nshape can provide priors on the shape's parts -- as natural language has\nevolved to reflect human perception of the compositional structure of objects,\nessential to their recognition and use. For training, we use the paired\ngeometry / language data collected in the ShapeGlot work for their reference\ngame, where a speaker creates an utterance to differentiate a target shape from\ntwo distractors and the listener has to find the target based on this\nutterance. Our network is designed to solve this target discrimination problem,\ncarefully incorporating a Transformer-based attention module so that the output\nattention can precisely highlight the semantic part or parts described in the\nlanguage. Furthermore, the network operates without any direct supervision on\nthe 3D geometry itself. Surprisingly, we further demonstrate that the learned\npart information is generalizable to shape classes unseen during training. Our\napproach opens the possibility of learning 3D shape parts from language alone,\nwithout the need for large-scale part geometry annotations, thus facilitating\nannotation acquisition.",
    "published_date": "2021-12-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.06390v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.06362v3",
    "title": "Scheduling Servers with Stochastic Bilinear Rewards",
    "authors": [
      "Jung-hun Kim",
      "Milan Vojnovic"
    ],
    "author_ids": [],
    "abstract": "We address a control system optimization problem that arises in multi-class,\nmulti-server queueing system scheduling with uncertainty. In this scenario,\njobs incur holding costs while awaiting completion, and job-server assignments\nyield observable stochastic rewards with unknown mean values. The rewards for\njob-server assignments are assumed to follow a bilinear model with respect to\nfeatures characterizing jobs and servers. Our objective is regret minimization,\naiming to maximize the cumulative reward of job-server assignments over a time\nhorizon while maintaining a bounded total job holding cost, thus ensuring\nqueueing system stability. This problem is motivated by applications in\ncomputing services and online platforms. To address this problem, we propose a\nscheduling algorithm based on weighted proportional fair allocation criteria\naugmented with marginal costs for reward maximization, incorporating a bandit\nstrategy. Our algorithm achieves sub-linear regret and sub-linear mean holding\ncost (and queue length bound) with respect to the time horizon, thus\nguaranteeing queueing system stability. Additionally, we establish stability\nconditions for distributed iterative algorithms for computing allocations,\nwhich are relevant to large-scale system applications. Finally, we validate the\nefficiency of our algorithm through numerical experiments.",
    "published_date": "2021-12-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.DS",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.06362v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.06299v2",
    "title": "Optimal Partitions for Nonparametric Multivariate Entropy Estimation",
    "authors": [
      "Z. Keskin"
    ],
    "author_ids": [],
    "abstract": "Efficient and accurate estimation of multivariate empirical probability\ndistributions is fundamental to the calculation of information-theoretic\nmeasures such as mutual information and transfer entropy. Common techniques\ninclude variations on histogram estimation which, whilst computationally\nefficient, are often unable to precisely capture the probability density of\nsamples with high correlation, kurtosis or fine substructure, especially when\nsample sizes are small. Adaptive partitions, which adjust heuristically to the\nsample, can reduce the bias imparted from the geometry of the histogram itself,\nbut these have commonly focused on the location, scale and granularity of the\npartition, the effects of which are limited for highly correlated\ndistributions. In this paper, I reformulate the differential entropy estimator\nfor the special case of an equiprobable histogram, using a k-d tree to\npartition the sample space into bins of equal probability mass. By doing so, I\nexpose an implicit rotational orientation parameter, which is conjectured to be\nsuboptimally specified in the typical marginal alignment. I propose that the\noptimal orientation minimises the variance of the bin volumes, and demonstrate\nthat improved entropy estimates can be obtained by rotationally aligning the\npartition to the sample distribution accordingly. Such optimal partitions are\nobserved to be more accurate than existing techniques in estimating entropies\nof correlated bivariate Gaussian distributions with known theoretical values,\nacross varying sample sizes (99% CI).",
    "published_date": "2021-12-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.06299v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2112.06288v1",
    "title": "Fairness for Robust Learning to Rank",
    "authors": [
      "Omid Memarrast",
      "Ashkan Rezaei",
      "Rizal Fathony",
      "Brian Ziebart"
    ],
    "author_ids": [],
    "abstract": "While conventional ranking systems focus solely on maximizing the utility of\nthe ranked items to users, fairness-aware ranking systems additionally try to\nbalance the exposure for different protected attributes such as gender or race.\nTo achieve this type of group fairness for ranking, we derive a new ranking\nsystem based on the first principles of distributional robustness. We formulate\na minimax game between a player choosing a distribution over rankings to\nmaximize utility while satisfying fairness constraints against an adversary\nseeking to minimize utility while matching statistics of the training data. We\nshow that our approach provides better utility for highly fair rankings than\nexisting baseline methods.",
    "published_date": "2021-12-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.06288v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.06287v1",
    "title": "Identifying bias in cluster quality metrics",
    "authors": [
      "Martí Renedo-Mirambell",
      "Argimiro Arratia"
    ],
    "author_ids": [],
    "abstract": "We study potential biases of popular cluster quality metrics, such as\nconductance or modularity. We propose a method that uses both stochastic and\npreferential attachment block models construction to generate networks with\npreset community structures, to which quality metrics will be applied. These\nmodels also allow us to generate multi-level structures of varying strength,\nwhich will show if metrics favour partitions into a larger or smaller number of\nclusters. Additionally, we propose another quality metric, the density ratio.\n  We observed that most of the studied metrics tend to favour partitions into a\nsmaller number of big clusters, even when their relative internal and external\nconnectivity are the same. The metrics found to be less biased are modularity\nand density ratio.",
    "published_date": "2021-12-12T00:00:00",
    "year": 2021,
    "categories": [
      "physics.soc-ph",
      "cs.LG",
      "cs.SI",
      "I.5.3; I.5.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.06287v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.06161v1",
    "title": "Semi-supervised Domain Adaptive Structure Learning",
    "authors": [
      "Can Qin",
      "Lichen Wang",
      "Qianqian Ma",
      "Yu Yin",
      "Huan Wang",
      "Yun Fu"
    ],
    "author_ids": [],
    "abstract": "Semi-supervised domain adaptation (SSDA) is quite a challenging problem\nrequiring methods to overcome both 1) overfitting towards poorly annotated data\nand 2) distribution shift across domains. Unfortunately, a simple combination\nof domain adaptation (DA) and semi-supervised learning (SSL) methods often fail\nto address such two objects because of training data bias towards labeled\nsamples. In this paper, we introduce an adaptive structure learning method to\nregularize the cooperation of SSL and DA. Inspired by the multi-views learning,\nour proposed framework is composed of a shared feature encoder network and two\nclassifier networks, trained for contradictory purposes. Among them, one of the\nclassifiers is applied to group target features to improve intra-class density,\nenlarging the gap of categorical clusters for robust representation learning.\nMeanwhile, the other classifier, serviced as a regularizer, attempts to scatter\nthe source features to enhance the smoothness of the decision boundary. The\niterations of target clustering and source expansion make the target features\nbeing well-enclosed inside the dilated boundary of the corresponding source\npoints. For the joint address of cross-domain features alignment and partially\nlabeled data learning, we apply the maximum mean discrepancy (MMD) distance\nminimization and self-training (ST) to project the contradictory structures\ninto a shared view to make the reliable final decision. The experimental\nresults over the standard SSDA benchmarks, including DomainNet and Office-home,\ndemonstrate both the accuracy and robustness of our method over the\nstate-of-the-art approaches.",
    "published_date": "2021-12-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.06161v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.06103v3",
    "title": "Improving Vision Transformers for Incremental Learning",
    "authors": [
      "Pei Yu",
      "Yinpeng Chen",
      "Ying Jin",
      "Zicheng Liu"
    ],
    "author_ids": [],
    "abstract": "This paper proposes a working recipe of using Vision Transformer (ViT) in\nclass incremental learning. Although this recipe only combines existing\ntechniques, developing the combination is not trivial. Firstly, naive\napplication of ViT to replace convolutional neural networks (CNNs) in\nincremental learning results in serious performance degradation. Secondly, we\nnail down three issues of naively using ViT: (a) ViT has very slow convergence\nwhen the number of classes is small, (b) more bias towards new classes is\nobserved in ViT than CNN-based architectures, and (c) the conventional learning\nrate of ViT is too low to learn a good classifier layer. Finally, our solution,\nnamed ViTIL (ViT for Incremental Learning) achieves new state-of-the-art on\nboth CIFAR and ImageNet datasets for all three class incremental learning\nsetups by a clear margin. We believe this advances the knowledge of transformer\nin the incremental learning community. Code will be publicly released.",
    "published_date": "2021-12-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.06103v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.05892v3",
    "title": "COMPOSER: Compositional Reasoning of Group Activity in Videos with Keypoint-Only Modality",
    "authors": [
      "Honglu Zhou",
      "Asim Kadav",
      "Aviv Shamsian",
      "Shijie Geng",
      "Farley Lai",
      "Long Zhao",
      "Ting Liu",
      "Mubbasir Kapadia",
      "Hans Peter Graf"
    ],
    "author_ids": [],
    "abstract": "Group Activity Recognition detects the activity collectively performed by a\ngroup of actors, which requires compositional reasoning of actors and objects.\nWe approach the task by modeling the video as tokens that represent the\nmulti-scale semantic concepts in the video. We propose COMPOSER, a Multiscale\nTransformer based architecture that performs attention-based reasoning over\ntokens at each scale and learns group activity compositionally. In addition,\nprior works suffer from scene biases with privacy and ethical concerns. We only\nuse the keypoint modality which reduces scene biases and prevents acquiring\ndetailed visual data that may contain private or biased information of users.\nWe improve the multiscale representations in COMPOSER by clustering the\nintermediate scale representations, while maintaining consistent cluster\nassignments between scales. Finally, we use techniques such as auxiliary\nprediction and data augmentations tailored to the keypoint signals to aid model\ntraining. We demonstrate the model's strength and interpretability on two\nwidely-used datasets (Volleyball and Collective Activity). COMPOSER achieves up\nto +5.4% improvement with just the keypoint modality. Code is available at\nhttps://github.com/hongluzhou/composer",
    "published_date": "2021-12-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.05892v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.05827v1",
    "title": "Quality-Aware Multimodal Biometric Recognition",
    "authors": [
      "Sobhan Soleymani",
      "Ali Dabouei",
      "Fariborz Taherkhani",
      "Seyed Mehdi Iranmanesh",
      "Jeremy Dawson",
      "Nasser M. Nasrabadi"
    ],
    "author_ids": [],
    "abstract": "We present a quality-aware multimodal recognition framework that combines\nrepresentations from multiple biometric traits with varying quality and number\nof samples to achieve increased recognition accuracy by extracting\ncomplimentary identification information based on the quality of the samples.\nWe develop a quality-aware framework for fusing representations of input\nmodalities by weighting their importance using quality scores estimated in a\nweakly-supervised fashion. This framework utilizes two fusion blocks, each\nrepresented by a set of quality-aware and aggregation networks. In addition to\narchitecture modifications, we propose two task-specific loss functions:\nmultimodal separability loss and multimodal compactness loss. The first loss\nassures that the representations of modalities for a class have comparable\nmagnitudes to provide a better quality estimation, while the multimodal\nrepresentations of different classes are distributed to achieve maximum\ndiscrimination in the embedding space. The second loss, which is considered to\nregularize the network weights, improves the generalization performance by\nregularizing the framework. We evaluate the performance by considering three\nmultimodal datasets consisting of face, iris, and fingerprint modalities. The\nefficacy of the framework is demonstrated through comparison with the\nstate-of-the-art algorithms. In particular, our framework outperforms the rank-\nand score-level fusion of modalities of BIOMDATA by more than 30% for true\nacceptance rate at false acceptance rate of $10^{-4}$.",
    "published_date": "2021-12-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.05827v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.05727v1",
    "title": "Neural Belief Propagation for Scene Graph Generation",
    "authors": [
      "Daqi Liu",
      "Miroslaw Bober",
      "Josef Kittler"
    ],
    "author_ids": [],
    "abstract": "Scene graph generation aims to interpret an input image by explicitly\nmodelling the potential objects and their relationships, which is predominantly\nsolved by the message passing neural network models in previous methods.\nCurrently, such approximation models generally assume the output variables are\ntotally independent and thus ignore the informative structural higher-order\ninteractions. This could lead to the inconsistent interpretations for an input\nimage. In this paper, we propose a novel neural belief propagation method to\ngenerate the resulting scene graph. It employs a structural Bethe approximation\nrather than the mean field approximation to infer the associated marginals. To\nfind a better bias-variance trade-off, the proposed model not only incorporates\npairwise interactions but also higher order interactions into the associated\nscoring function. It achieves the state-of-the-art performance on various\npopular scene graph generation benchmarks.",
    "published_date": "2021-12-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.05727v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.05700v1",
    "title": "A Framework for Fairness: A Systematic Review of Existing Fair AI Solutions",
    "authors": [
      "Brianna Richardson",
      "Juan E. Gilbert"
    ],
    "author_ids": [],
    "abstract": "In a world of daily emerging scientific inquisition and discovery, the\nprolific launch of machine learning across industries comes to little surprise\nfor those familiar with the potential of ML. Neither so should the congruent\nexpansion of ethics-focused research that emerged as a response to issues of\nbias and unfairness that stemmed from those very same applications. Fairness\nresearch, which focuses on techniques to combat algorithmic bias, is now more\nsupported than ever before. A large portion of fairness research has gone to\nproducing tools that machine learning practitioners can use to audit for bias\nwhile designing their algorithms. Nonetheless, there is a lack of application\nof these fairness solutions in practice. This systematic review provides an\nin-depth summary of the algorithmic bias issues that have been defined and the\nfairness solution space that has been proposed. Moreover, this review provides\nan in-depth breakdown of the caveats to the solution space that have arisen\nsince their release and a taxonomy of needs that have been proposed by machine\nlearning practitioners, fairness researchers, and institutional stakeholders.\nThese needs have been organized and addressed to the parties most influential\nto their implementation, which includes fairness researchers, organizations\nthat produce ML algorithms, and the machine learning practitioners themselves.\nThese findings can be used in the future to bridge the gap between\npractitioners and fairness experts and inform the creation of usable fair ML\ntoolkits.",
    "published_date": "2021-12-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.05700v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.05675v2",
    "title": "Assessing the Fairness of AI Systems: AI Practitioners' Processes, Challenges, and Needs for Support",
    "authors": [
      "Michael Madaio",
      "Lisa Egede",
      "Hariharan Subramonyam",
      "Jennifer Wortman Vaughan",
      "Hanna Wallach"
    ],
    "author_ids": [],
    "abstract": "Various tools and practices have been developed to support practitioners in\nidentifying, assessing, and mitigating fairness-related harms caused by AI\nsystems. However, prior research has highlighted gaps between the intended\ndesign of these tools and practices and their use within particular contexts,\nincluding gaps caused by the role that organizational factors play in shaping\nfairness work. In this paper, we investigate these gaps for one such practice:\ndisaggregated evaluations of AI systems, intended to uncover performance\ndisparities between demographic groups. By conducting semi-structured\ninterviews and structured workshops with thirty-three AI practitioners from ten\nteams at three technology companies, we identify practitioners' processes,\nchallenges, and needs for support when designing disaggregated evaluations. We\nfind that practitioners face challenges when choosing performance metrics,\nidentifying the most relevant direct stakeholders and demographic groups on\nwhich to focus, and collecting datasets with which to conduct disaggregated\nevaluations. More generally, we identify impacts on fairness work stemming from\na lack of engagement with direct stakeholders or domain experts, business\nimperatives that prioritize customers over marginalized groups, and the drive\nto deploy AI systems at scale.",
    "published_date": "2021-12-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.05675v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.05630v1",
    "title": "On Fair Selection in the Presence of Implicit and Differential Variance",
    "authors": [
      "Vitalii Emelianov",
      "Nicolas Gast",
      "Krishna P. Gummadi",
      "Patrick Loiseau"
    ],
    "author_ids": [],
    "abstract": "Discrimination in selection problems such as hiring or college admission is\noften explained by implicit bias from the decision maker against disadvantaged\ndemographic groups. In this paper, we consider a model where the decision maker\nreceives a noisy estimate of each candidate's quality, whose variance depends\non the candidate's group -- we argue that such differential variance is a key\nfeature of many selection problems. We analyze two notable settings: in the\nfirst, the noise variances are unknown to the decision maker who simply picks\nthe candidates with the highest estimated quality independently of their group;\nin the second, the variances are known and the decision maker picks candidates\nhaving the highest expected quality given the noisy estimate. We show that both\nbaseline decision makers yield discrimination, although in opposite directions:\nthe first leads to underrepresentation of the low-variance group while the\nsecond leads to underrepresentation of the high-variance group. We study the\neffect on the selection utility of imposing a fairness mechanism that we term\nthe $\\gamma$-rule (it is an extension of the classical four-fifths rule and it\nalso includes demographic parity). In the first setting (with unknown\nvariances), we prove that under mild conditions, imposing the $\\gamma$-rule\nincreases the selection utility -- here there is no trade-off between fairness\nand utility. In the second setting (with known variances), imposing the\n$\\gamma$-rule decreases the utility but we prove a bound on the utility loss\ndue to the fairness mechanism.",
    "published_date": "2021-12-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.05630v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.05547v1",
    "title": "PACMAN: PAC-style bounds accounting for the Mismatch between Accuracy and Negative log-loss",
    "authors": [
      "Matias Vera",
      "Leonardo Rey Vega",
      "Pablo Piantanida"
    ],
    "author_ids": [],
    "abstract": "The ultimate performance of machine learning algorithms for classification\ntasks is usually measured in terms of the empirical error probability (or\naccuracy) based on a testing dataset. Whereas, these algorithms are optimized\nthrough the minimization of a typically different--more convenient--loss\nfunction based on a training set. For classification tasks, this loss function\nis often the negative log-loss that leads to the well-known cross-entropy risk\nwhich is typically better behaved (from a numerical perspective) than the error\nprobability. Conventional studies on the generalization error do not usually\ntake into account the underlying mismatch between losses at training and\ntesting phases. In this work, we introduce an analysis based on point-wise PAC\napproach over the generalization gap considering the mismatch of testing based\non the accuracy metric and training on the negative log-loss. We label this\nanalysis PACMAN. Building on the fact that the mentioned mismatch can be\nwritten as a likelihood ratio, concentration inequalities can be used to\nprovide some insights for the generalization problem in terms of some\npoint-wise PAC bounds depending on some meaningful information-theoretic\nquantities. An analysis of the obtained bounds and a comparison with available\nresults in the literature are also provided.",
    "published_date": "2021-12-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.05547v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.05459v1",
    "title": "Sentiment Analysis on Brazilian Portuguese User Reviews",
    "authors": [
      "Frederico Souza",
      "João Filho"
    ],
    "author_ids": [],
    "abstract": "Sentiment Analysis is one of the most classical and primarily studied natural\nlanguage processing tasks. This problem had a notable advance with the\nproposition of more complex and scalable machine learning models. Despite this\nprogress, the Brazilian Portuguese language still disposes only of limited\nlinguistic resources, such as datasets dedicated to sentiment classification,\nespecially when considering the existence of predefined partitions in training,\ntesting, and validation sets that would allow a more fair comparison of\ndifferent algorithm alternatives. Motivated by these issues, this work analyzes\nthe predictive performance of a range of document embedding strategies,\nassuming the polarity as the system outcome. This analysis includes five\nsentiment analysis datasets in Brazilian Portuguese, unified in a single\ndataset, and a reference partitioning in training, testing, and validation\nsets, both made publicly available through a digital repository. A\ncross-evaluation of dataset-specific models over different contexts is\nconducted to evaluate their generalization capabilities and the feasibility of\nadopting a unique model for addressing all scenarios.",
    "published_date": "2021-12-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.05459v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.05261v3",
    "title": "Equivariant Quantum Graph Circuits",
    "authors": [
      "Péter Mernyei",
      "Konstantinos Meichanetzidis",
      "İsmail İlkan Ceylan"
    ],
    "author_ids": [],
    "abstract": "We investigate quantum circuits for graph representation learning, and\npropose equivariant quantum graph circuits (EQGCs), as a class of parameterized\nquantum circuits with strong relational inductive bias for learning over\ngraph-structured data. Conceptually, EQGCs serve as a unifying framework for\nquantum graph representation learning, allowing us to define several\ninteresting subclasses which subsume existing proposals. In terms of the\nrepresentation power, we prove that the studied subclasses of EQGCs are\nuniversal approximators for functions over the bounded graph domain. This\ntheoretical perspective on quantum graph machine learning methods opens many\ndirections for further work, and could lead to models with capabilities beyond\nthose of classical approaches. We empirically verify the expressive power of\nEQGCs through a dedicated experiment on synthetic data, and additionally\nobserve that the performance of EQGCs scales well with the depth of the model\nand does not suffer from barren plateu issues.",
    "published_date": "2021-12-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "quant-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.05261v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.05194v1",
    "title": "Word Embeddings via Causal Inference: Gender Bias Reducing and Semantic Information Preserving",
    "authors": [
      "Lei Ding",
      "Dengdeng Yu",
      "Jinhan Xie",
      "Wenxing Guo",
      "Shenggang Hu",
      "Meichen Liu",
      "Linglong Kong",
      "Hongsheng Dai",
      "Yanchun Bao",
      "Bei Jiang"
    ],
    "author_ids": [],
    "abstract": "With widening deployments of natural language processing (NLP) in daily life,\ninherited social biases from NLP models have become more severe and\nproblematic. Previous studies have shown that word embeddings trained on\nhuman-generated corpora have strong gender biases that can produce\ndiscriminative results in downstream tasks. Previous debiasing methods focus\nmainly on modeling bias and only implicitly consider semantic information while\ncompletely overlooking the complex underlying causal structure among bias and\nsemantic components. To address these issues, we propose a novel methodology\nthat leverages a causal inference framework to effectively remove gender bias.\nThe proposed method allows us to construct and analyze the complex causal\nmechanisms facilitating gender information flow while retaining oracle semantic\ninformation within word embeddings. Our comprehensive experiments show that the\nproposed method achieves state-of-the-art results in gender-debiasing tasks. In\naddition, our methods yield better performance in word similarity evaluation\nand various extrinsic downstream NLP tasks.",
    "published_date": "2021-12-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.05194v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.05128v2",
    "title": "Fair Community Detection and Structure Learning in Heterogeneous Graphical Models",
    "authors": [
      "Davoud Ataee Tarzanagh",
      "Laura Balzano",
      "Alfred O. Hero"
    ],
    "author_ids": [],
    "abstract": "Inference of community structure in probabilistic graphical models may not be\nconsistent with fairness constraints when nodes have demographic attributes.\nCertain demographics may be over-represented in some detected communities and\nunder-represented in others. This paper defines a novel $\\ell_1$-regularized\npseudo-likelihood approach for fair graphical model selection. In particular,\nwe assume there is some community or clustering structure in the true\nunderlying graph, and we seek to learn a sparse undirected graph and its\ncommunities from the data such that demographic groups are fairly represented\nwithin the communities. In the case when the graph is known a priori, we\nprovide a convex semidefinite programming approach for fair community\ndetection. We establish the statistical consistency of the proposed method for\nboth a Gaussian graphical model and an Ising model for, respectively,\ncontinuous and binary data, proving that our method can recover the graphs and\ntheir fair communities with high probability.",
    "published_date": "2021-12-09T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.05128v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.05120v4",
    "title": "On Convergence of Federated Averaging Langevin Dynamics",
    "authors": [
      "Wei Deng",
      "Qian Zhang",
      "Yi-An Ma",
      "Zhao Song",
      "Guang Lin"
    ],
    "author_ids": [],
    "abstract": "We propose a federated averaging Langevin algorithm (FA-LD) for uncertainty\nquantification and mean predictions with distributed clients. In particular, we\ngeneralize beyond normal posterior distributions and consider a general class\nof models. We develop theoretical guarantees for FA-LD for strongly log-concave\ndistributions with non-i.i.d data and study how the injected noise and the\nstochastic-gradient noise, the heterogeneity of data, and the varying learning\nrates affect the convergence. Such an analysis sheds light on the optimal\nchoice of local updates to minimize communication costs. Important to our\napproach is that the communication efficiency does not deteriorate with the\ninjected noise in the Langevin algorithms. In addition, we examine in our FA-LD\nalgorithm both independent and correlated noise used over different clients. We\nobserve there is a trade-off between the pairs among communication, accuracy,\nand data privacy. As local devices may become inactive in federated networks,\nwe also show convergence results based on different averaging schemes where\nonly partial device updates are available. In such a case, we discover an\nadditional bias that does not decay to zero.",
    "published_date": "2021-12-09T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.05120v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.05084v1",
    "title": "A Survey on Echo Chambers on Social Media: Description, Detection and Mitigation",
    "authors": [
      "Faisal Alatawi",
      "Lu Cheng",
      "Anique Tahir",
      "Mansooreh Karami",
      "Bohan Jiang",
      "Tyler Black",
      "Huan Liu"
    ],
    "author_ids": [],
    "abstract": "Echo chambers on social media are a significant problem that can elicit a\nnumber of negative consequences, most recently affecting the response to\nCOVID-19. Echo chambers promote conspiracy theories about the virus and are\nfound to be linked to vaccine hesitancy, less compliance with mask mandates,\nand the practice of social distancing. Moreover, the problem of echo chambers\nis connected to other pertinent issues like political polarization and the\nspread of misinformation. An echo chamber is defined as a network of users in\nwhich users only interact with opinions that support their pre-existing beliefs\nand opinions, and they exclude and discredit other viewpoints. This survey aims\nto examine the echo chamber phenomenon on social media from a social computing\nperspective and provide a blueprint for possible solutions. We survey the\nrelated literature to understand the attributes of echo chambers and how they\naffect the individual and society at large. Additionally, we show the\nmechanisms, both algorithmic and psychological, that lead to the formation of\necho chambers. These mechanisms could be manifested in two forms: (1) the bias\nof social media's recommender systems and (2) internal biases such as\nconfirmation bias and homophily. While it is immensely challenging to mitigate\ninternal biases, there has been great efforts seeking to mitigate the bias of\nrecommender systems. These recommender systems take advantage of our own biases\nto personalize content recommendations to keep us engaged in order to watch\nmore ads. Therefore, we further investigate different computational approaches\nfor echo chamber detection and prevention, mainly based around recommender\nsystems.",
    "published_date": "2021-12-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI",
      "cs.LG",
      "91D30 68T01",
      "I.2; J.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.05084v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.04975v1",
    "title": "Personalized musically induced emotions of not-so-popular Colombian music",
    "authors": [
      "Juan Sebastián Gómez-Cañón",
      "Perfecto Herrera",
      "Estefanía Cano",
      "Emilia Gómez"
    ],
    "author_ids": [],
    "abstract": "This work presents an initial proof of concept of how Music Emotion\nRecognition (MER) systems could be intentionally biased with respect to\nannotations of musically induced emotions in a political context. In specific,\nwe analyze traditional Colombian music containing politically charged lyrics of\ntwo types: (1) vallenatos and social songs from the \"left-wing\" guerrilla\nFuerzas Armadas Revolucionarias de Colombia (FARC) and (2) corridos from the\n\"right-wing\" paramilitaries Autodefensas Unidas de Colombia (AUC). We train\npersonalized machine learning models to predict induced emotions for three\nusers with diverse political views - we aim at identifying the songs that may\ninduce negative emotions for a particular user, such as anger and fear. To this\nextent, a user's emotion judgements could be interpreted as problematizing data\n- subjective emotional judgments could in turn be used to influence the user in\na human-centered machine learning environment. In short, highly desired\n\"emotion regulation\" applications could potentially deviate to \"emotion\nmanipulation\" - the recent discredit of emotion recognition technologies might\ntranscend ethical issues of diversity and inclusion.",
    "published_date": "2021-12-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SD",
      "cs.HC",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.04975v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.04895v1",
    "title": "Latent Space Explanation by Intervention",
    "authors": [
      "Itai Gat",
      "Guy Lorberbom",
      "Idan Schwartz",
      "Tamir Hazan"
    ],
    "author_ids": [],
    "abstract": "The success of deep neural nets heavily relies on their ability to encode\ncomplex relations between their input and their output. While this property\nserves to fit the training data well, it also obscures the mechanism that\ndrives prediction. This study aims to reveal hidden concepts by employing an\nintervention mechanism that shifts the predicted class based on discrete\nvariational autoencoders. An explanatory model then visualizes the encoded\ninformation from any hidden layer and its corresponding intervened\nrepresentation. By the assessment of differences between the original\nrepresentation and the intervened representation, one can determine the\nconcepts that can alter the class, hence providing interpretability. We\ndemonstrate the effectiveness of our approach on CelebA, where we show various\nvisualizations for bias in the data and suggest different interventions to\nreveal and change bias.",
    "published_date": "2021-12-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.04895v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.04828v2",
    "title": "Avoiding C-hacking when evaluating survival distribution predictions with discrimination measures",
    "authors": [
      "Raphael Sonabend",
      "Andreas Bender",
      "Sebastian Vollmer"
    ],
    "author_ids": [],
    "abstract": "In this paper we consider how to evaluate survival distribution predictions\nwith measures of discrimination. This is a non-trivial problem as\ndiscrimination measures are the most commonly used in survival analysis and yet\nthere is no clear method to derive a risk prediction from a distribution\nprediction. We survey methods proposed in literature and software and consider\ntheir respective advantages and disadvantages. Whilst distributions are\nfrequently evaluated by discrimination measures, we find that the method for\ndoing so is rarely described in the literature and often leads to unfair\ncomparisons. We find that the most robust method of reducing a distribution to\na risk is to sum over the predicted cumulative hazard. We recommend that\nmachine learning survival analysis software implements clear transformations\nbetween distribution and risk predictions in order to allow more transparent\nand accessible model evaluation. The code used in the final experiment is\navailable at https://github.com/RaphaelS1/distribution_discrimination.",
    "published_date": "2021-12-09T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.ST",
      "stat.ME",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.04828v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.04577v1",
    "title": "Gaussian Random Number Generator with Reconfigurable Mean and Variance using Stochastic Magnetic Tunnel Junctions",
    "authors": [
      "Punyashloka Debashis",
      "Hai Li",
      "Dmitri Nikonov",
      "Ian Young"
    ],
    "author_ids": [],
    "abstract": "Generating high-quality random numbers with a Gaussian probability\ndistribution function is an important and resource consuming computational task\nfor many applications in the fields of machine learning and Monte Carlo\nalgorithms. Recently, CMOS-based digital hardware architectures have been\nexplored as specialized Gaussian random number generators (GRNGs). These\nCMOS-based GRNGs have a large area and require entropy sources at their input\nwhich increase the computing cost. Here, we propose a GRNG that works on the\nprinciple of the Boltzmann law in a physical system made from an interconnected\nnetwork of thermally unstable magnetic tunnel junctions. The proposed hardware\ncan produce multi-bit Gaussian random numbers at a gigahertz speed and can be\nconfigured to generate distributions with a desired mean and variance. An\nanalytical derivation of the required interconnection and bias strengths is\nprovided followed by numerical simulations to demonstrate the functionalities\nof the GRNG.",
    "published_date": "2021-12-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.ET",
      "cond-mat.dis-nn",
      "cond-mat.mes-hall"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.04577v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.04453v1",
    "title": "MLP Architectures for Vision-and-Language Modeling: An Empirical Study",
    "authors": [
      "Yixin Nie",
      "Linjie Li",
      "Zhe Gan",
      "Shuohang Wang",
      "Chenguang Zhu",
      "Michael Zeng",
      "Zicheng Liu",
      "Mohit Bansal",
      "Lijuan Wang"
    ],
    "author_ids": [],
    "abstract": "We initiate the first empirical study on the use of MLP architectures for\nvision-and-language (VL) fusion. Through extensive experiments on 5 VL tasks\nand 5 robust VQA benchmarks, we find that: (i) Without pre-training, using MLPs\nfor multimodal fusion has a noticeable performance gap compared to\ntransformers; (ii) However, VL pre-training can help close the performance gap;\n(iii) Instead of heavy multi-head attention, adding tiny one-head attention to\nMLPs is sufficient to achieve comparable performance to transformers. Moreover,\nwe also find that the performance gap between MLPs and transformers is not\nwidened when being evaluated on the harder robust VQA benchmarks, suggesting\nusing MLPs for VL fusion can generalize roughly to a similar degree as using\ntransformers. These results hint that MLPs can effectively learn to align\nvision and text features extracted from lower-level encoders without heavy\nreliance on self-attention. Based on this, we ask an even bolder question: can\nwe have an all-MLP architecture for VL modeling, where both VL fusion and the\nvision encoder are replaced with MLPs? Our result shows that an all-MLP VL\nmodel is sub-optimal compared to state-of-the-art full-featured VL models when\nboth of them get pre-trained. However, pre-training an all-MLP can surprisingly\nachieve a better average score than full-featured transformer models without\npre-training. This indicates the potential of large-scale pre-training of\nMLP-like architectures for VL modeling and inspires the future research\ndirection on simplifying well-established VL modeling with less inductive\ndesign bias. Our code is publicly available at:\nhttps://github.com/easonnie/mlp-vil",
    "published_date": "2021-12-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.04453v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.04359v1",
    "title": "Ethical and social risks of harm from Language Models",
    "authors": [
      "Laura Weidinger",
      "John Mellor",
      "Maribeth Rauh",
      "Conor Griffin",
      "Jonathan Uesato",
      "Po-Sen Huang",
      "Myra Cheng",
      "Mia Glaese",
      "Borja Balle",
      "Atoosa Kasirzadeh",
      "Zac Kenton",
      "Sasha Brown",
      "Will Hawkins",
      "Tom Stepleton",
      "Courtney Biles",
      "Abeba Birhane",
      "Julia Haas",
      "Laura Rimell",
      "Lisa Anne Hendricks",
      "William Isaac",
      "Sean Legassick",
      "Geoffrey Irving",
      "Iason Gabriel"
    ],
    "author_ids": [],
    "abstract": "This paper aims to help structure the risk landscape associated with\nlarge-scale Language Models (LMs). In order to foster advances in responsible\ninnovation, an in-depth understanding of the potential risks posed by these\nmodels is needed. A wide range of established and anticipated risks are\nanalysed in detail, drawing on multidisciplinary expertise and literature from\ncomputer science, linguistics, and social sciences.\n  We outline six specific risk areas: I. Discrimination, Exclusion and\nToxicity, II. Information Hazards, III. Misinformation Harms, V. Malicious\nUses, V. Human-Computer Interaction Harms, VI. Automation, Access, and\nEnvironmental Harms. The first area concerns the perpetuation of stereotypes,\nunfair discrimination, exclusionary norms, toxic language, and lower\nperformance by social group for LMs. The second focuses on risks from private\ndata leaks or LMs correctly inferring sensitive information. The third\naddresses risks arising from poor, false or misleading information including in\nsensitive domains, and knock-on risks such as the erosion of trust in shared\ninformation. The fourth considers risks from actors who try to use LMs to cause\nharm. The fifth focuses on risks specific to LLMs used to underpin\nconversational agents that interact with human users, including unsafe use,\nmanipulation or deception. The sixth discusses the risk of environmental harm,\njob automation, and other challenges that may have a disparate effect on\ndifferent social groups or communities.\n  In total, we review 21 risks in-depth. We discuss the points of origin of\ndifferent risks and point to potential mitigation approaches. Lastly, we\ndiscuss organisational responsibilities in implementing mitigations, and the\nrole of collaboration and participation. We highlight directions for further\nresearch, particularly on expanding the toolkit for assessing and evaluating\nthe outlined risks in LMs.",
    "published_date": "2021-12-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.04359v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.04493v2",
    "title": "Binary Change Guided Hyperspectral Multiclass Change Detection",
    "authors": [
      "Meiqi Hu",
      "Chen Wu",
      "Bo Du",
      "Liangpei Zhang"
    ],
    "author_ids": [],
    "abstract": "Characterized by tremendous spectral information, hyperspectral image is able\nto detect subtle changes and discriminate various change classes for change\ndetection. The recent research works dominated by hyperspectral binary change\ndetection, however, cannot provide fine change classes information. And most\nmethods incorporating spectral unmixing for hyperspectral multiclass change\ndetection (HMCD), yet suffer from the neglection of temporal correlation and\nerror accumulation. In this study, we proposed an unsupervised Binary Change\nGuided hyperspectral multiclass change detection Network (BCG-Net) for HMCD,\nwhich aims at boosting the multiclass change detection result and unmixing\nresult with the mature binary change detection approaches. In BCG-Net, a novel\npartial-siamese united-unmixing module is designed for multi-temporal spectral\nunmixing, and a groundbreaking temporal correlation constraint directed by the\npseudo-labels of binary change detection result is developed to guide the\nunmixing process from the perspective of change detection, encouraging the\nabundance of the unchanged pixels more coherent and that of the changed pixels\nmore accurate. Moreover, an innovative binary change detection rule is put\nforward to deal with the problem that traditional rule is susceptible to\nnumerical values. The iterative optimization of the spectral unmixing process\nand the change detection process is proposed to eliminate the accumulated\nerrors and bias from unmixing result to change detection result. The\nexperimental results demonstrate that our proposed BCG-Net could achieve\ncomparative or even outstanding performance of multiclass change detection\namong the state-of-the-art approaches and gain better spectral unmixing results\nat the same time.",
    "published_date": "2021-12-08T00:00:00",
    "year": 2021,
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.04493v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.04169v2",
    "title": "Equity Promotion in Online Resource Allocation",
    "authors": [
      "Pan Xu",
      "Yifan Xu"
    ],
    "author_ids": [],
    "abstract": "We consider online resource allocation under a typical non-profit setting,\nwhere limited or even scarce resources are administered by a not-for-profit\norganization like a government. We focus on the internal-equity by assuming\nthat arriving requesters are homogeneous in terms of their external factors\nlike demands but heterogeneous for their internal attributes like demographics.\nSpecifically, we associate each arriving requester with one or several groups\nbased on their demographics (i.e., race, gender, and age), and we aim to design\nan equitable distributing strategy such that every group of requesters can\nreceive a fair share of resources proportional to a preset target ratio. We\npresent two LP-based sampling algorithms and investigate them both\ntheoretically (in terms of competitive-ratio analysis) and experimentally based\non real COVID-19 vaccination data maintained by the Minnesota Department of\nHealth. Both theoretical and numerical results show that our LP-based sampling\nstrategies can effectively promote equity, especially when the arrival\npopulation is disproportionately represented, as observed in the early stage of\nthe COVID-19 vaccine rollout.",
    "published_date": "2021-12-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.04169v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.04166v2",
    "title": "Weighted Fairness Notions for Indivisible Items Revisited",
    "authors": [
      "Mithun Chakraborty",
      "Erel Segal-Halevi",
      "Warut Suksompong"
    ],
    "author_ids": [],
    "abstract": "We revisit the setting of fairly allocating indivisible items when agents\nhave different weights representing their entitlements. First, we propose a\nparameterized family of relaxations for weighted envy-freeness and the same for\nweighted proportionality; the parameters indicate whether smaller-weight or\nlarger-weight agents should be given a higher priority. We show that each\nnotion in these families can always be satisfied, but any two cannot\nnecessarily be fulfilled simultaneously. We then introduce an intuitive\nweighted generalization of maximin share fairness and establish the optimal\napproximation of it that can be guaranteed. Furthermore, we characterize the\nimplication relations between the various weighted fairness notions introduced\nin this and prior work, and relate them to the lower and upper quota axioms\nfrom apportionment.",
    "published_date": "2021-12-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT",
      "econ.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.04166v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2112.04013v1",
    "title": "A deep learning model for data-driven discovery of functional connectivity",
    "authors": [
      "Usman Mahmood",
      "Zening Fu",
      "Vince Calhoun",
      "Sergey Plis"
    ],
    "author_ids": [],
    "abstract": "Functional connectivity (FC) studies have demonstrated the overarching value\nof studying the brain and its disorders through the undirected weighted graph\nof fMRI correlation matrix. Most of the work with the FC, however, depends on\nthe way the connectivity is computed, and further depends on the manual\npost-hoc analysis of the FC matrices. In this work we propose a deep learning\narchitecture BrainGNN that learns the connectivity structure as part of\nlearning to classify subjects. It simultaneously applies a graphical neural\nnetwork to this learned graph and learns to select a sparse subset of brain\nregions important to the prediction task. We demonstrate the model's\nstate-of-the-art classification performance on a schizophrenia fMRI dataset and\ndemonstrate how introspection leads to disorder relevant findings. The graphs\nlearned by the model exhibit strong class discrimination and the sparse subset\nof relevant regions are consistent with the schizophrenia literature.",
    "published_date": "2021-12-07T00:00:00",
    "year": 2021,
    "categories": [
      "q-bio.NC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.04013v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.03881v3",
    "title": "Nashian game theory is incompatible with quantum physics",
    "authors": [
      "Michal Baczyk",
      "Ghislain Fourny"
    ],
    "author_ids": [],
    "abstract": "We suggest to look at quantum measurement outcomes not through the lens of\nprobability theory, but instead through decision theory. We introduce an\noriginal game-theoretical framework, model and algorithmic procedure where\nmeasurement scenarios are multiplayer games with a structure all observers\nagree on. Measurement axes and, newly, measurement outcomes are modeled as\ndecisions with nature being an action-minimizing economic agent. We translate\nphysical notions of causality, correlation, counterfactuals, and contextuality\nto particular aspects of game theory. We investigate the causal consistency of\ndynamic games with imperfect information from the quantum perspective and\nconclude that counterfactual dependencies should be distinguished from\ncausation and correlation as a separate phenomenon of its own. Most\nsignificantly, we observe that game theory based on Nash equilibria stands in\ncontradiction with a violation of Bell inequalities. Hence, we propose that\nquantum physics should be analyzed with non-Nashian game theory, the inner\nworkings of which we demonstrate using our proposed model.",
    "published_date": "2021-12-07T00:00:00",
    "year": 2021,
    "categories": [
      "quant-ph",
      "cs.GT",
      "91A35",
      "J.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.03881v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2112.04899v1",
    "title": "Assessing Fairness in the Presence of Missing Data",
    "authors": [
      "Yiliang Zhang",
      "Qi Long"
    ],
    "author_ids": [],
    "abstract": "Missing data are prevalent and present daunting challenges in real data\nanalysis. While there is a growing body of literature on fairness in analysis\nof fully observed data, there has been little theoretical work on investigating\nfairness in analysis of incomplete data. In practice, a popular analytical\napproach for dealing with missing data is to use only the set of complete\ncases, i.e., observations with all features fully observed to train a\nprediction algorithm. However, depending on the missing data mechanism, the\ndistribution of complete cases and the distribution of the complete data may be\nsubstantially different. When the goal is to develop a fair algorithm in the\ncomplete data domain where there are no missing values, an algorithm that is\nfair in the complete case domain may show disproportionate bias towards some\nmarginalized groups in the complete data domain. To fill this significant gap,\nwe study the problem of estimating fairness in the complete data domain for an\narbitrary model evaluated merely using complete cases. We provide upper and\nlower bounds on the fairness estimation error and conduct numerical experiments\nto assess our theoretical results. Our work provides the first known\ntheoretical results on fairness guarantee in analysis of incomplete data.",
    "published_date": "2021-12-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.04899v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.03858v1",
    "title": "Reducing Target Group Bias in Hate Speech Detectors",
    "authors": [
      "Darsh J Shah",
      "Sinong Wang",
      "Han Fang",
      "Hao Ma",
      "Luke Zettlemoyer"
    ],
    "author_ids": [],
    "abstract": "The ubiquity of offensive and hateful content on online fora necessitates the\nneed for automatic solutions that detect such content competently across target\ngroups. In this paper we show that text classification models trained on large\npublicly available datasets despite having a high overall performance, may\nsignificantly under-perform on several protected groups. On the\n\\citet{vidgen2020learning} dataset, we find the accuracy to be 37\\% lower on an\nunder annotated Black Women target group and 12\\% lower on Immigrants, where\nhate speech involves a distinct style. To address this, we propose to perform\ntoken-level hate sense disambiguation, and utilize tokens' hate sense\nrepresentations for detection, modeling more general signals. On two publicly\navailable datasets, we observe that the variance in model accuracy across\ntarget groups drops by at least 30\\%, improving the average target group\nperformance by 4\\% and worst case performance by 13\\%.",
    "published_date": "2021-12-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.03858v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.03638v3",
    "title": "Scaling Structured Inference with Randomization",
    "authors": [
      "Yao Fu",
      "John P. Cunningham",
      "Mirella Lapata"
    ],
    "author_ids": [],
    "abstract": "Deep discrete structured models have seen considerable progress recently, but\ntraditional inference using dynamic programming (DP) typically works with a\nsmall number of states (less than hundreds), which severely limits model\ncapacity. At the same time, across machine learning, there is a recent trend of\nusing randomized truncation techniques to accelerate computations involving\nlarge sums. Here, we propose a family of randomized dynamic programming (RDP)\nalgorithms for scaling structured models to tens of thousands of latent states.\nOur method is widely applicable to classical DP-based inference (partition,\nmarginal, reparameterization, entropy) and different graph structures (chains,\ntrees, and more general hypergraphs). It is also compatible with automatic\ndifferentiation: it can be integrated with neural networks seamlessly and\nlearned with gradient-based optimizers. Our core technique approximates the\nsum-product by restricting and reweighting DP on a small subset of nodes, which\nreduces computation by orders of magnitude. We further achieve low bias and\nvariance via Rao-Blackwellization and importance sampling. Experiments over\ndifferent graphs demonstrate the accuracy and efficiency of our approach.\nFurthermore, when using RDP for training a structured variational autoencoder\nwith a scaled inference network, we achieve better test likelihood than\nbaselines and successfully prevent posterior collapse. code at:\nhttps://github.com/FranxYao/RDP",
    "published_date": "2021-12-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.DS",
      "stat.AP",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.03638v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.03240v1",
    "title": "Analyzing a Carceral Algorithm used by the Pennsylvania Department of Corrections",
    "authors": [
      "Vanessa Massaro",
      "Swarup Dhar",
      "Darakhshan Mir",
      "Nathan C. Ryan"
    ],
    "author_ids": [],
    "abstract": "Scholars have focused on algorithms used during sentencing, bail, and parole,\nbut little work explores what we call carceral algorithms that are used during\nincarceration. This paper is focused on the Pennsylvania Additive\nClassification Tool (PACT) used to classify prisoners' custody levels while\nthey are incarcerated. Algorithms that are used during incarceration warrant\ndeeper attention by scholars because they have the power to enact the lived\nreality of the prisoner. The algorithm in this case determines the likelihood a\nperson would endure additional disciplinary actions, can complete required\nprogramming, and gain experiences that, among other things, are distilled into\nvariables feeding into the parole algorithm. Given such power, examining\nalgorithms used on people currently incarcerated offers a unique analytic view\nto think about the dialectic relationship between data and algorithms. Our\nexamination of the PACT is two-fold and complementary. First, our qualitative\noverview of the historical context surrounding PACT reveals that it is designed\nto prioritize incapacitation and control over rehabilitation. While it closely\ninforms prisoner rehabilitation plans and parole considerations, it is rooted\nin population management for prison securitization. Second, on analyzing data\nfor 146,793 incarcerated people in PA, along with associated metadata related\nto the PACT, we find it is replete with racial bias as well as errors,\nomissions, and inaccuracies. Our findings to date further caution against\ndata-driven criminal justice reforms that rely on pre-existing data\ninfrastructures and expansive, uncritical, data-collection routines.",
    "published_date": "2021-12-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.03240v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2112.03237v5",
    "title": "From Coarse to Fine-grained Concept based Discrimination for Phrase Detection",
    "authors": [
      "Maan Qraitem",
      "Bryan A. Plummer"
    ],
    "author_ids": [],
    "abstract": "Phrase detection requires methods to identify if a phrase is relevant to an\nimage and localize it, if applicable. A key challenge for training more\ndiscriminative detection models is sampling negatives. Sampling techniques from\nprior work focus primarily on hard, often noisy, negatives disregarding the\nbroader distribution of negative samples. Our proposed CFCD-Net addresses this\nthrough two novels methods. First, we generate groups of semantically similar\nwords we call concepts (\\eg, \\{dog, cat, horse\\} and \\ \\{car, truck, SUV\\}),\nand then train our CFCD-Net to discriminate between a region of interest and\nits unrelated concepts. Second, for phrases containing fine-grained\nmutually-exclusive words (\\eg, colors), we force the model to select only one\napplicable phrase for each region using our novel fine-grained module (FGM). We\nevaluate our approach on Flickr30K Entities and RefCOCO+, where we improve mAP\nover the state-of-the-art by 1.5-2 points. When considering only the phrases\naffected by our FGM module, we improve by 3-4 points on both datasets.",
    "published_date": "2021-12-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.03237v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.04417v3",
    "title": "What I Cannot Predict, I Do Not Understand: A Human-Centered Evaluation Framework for Explainability Methods",
    "authors": [
      "Julien Colin",
      "Thomas Fel",
      "Remi Cadene",
      "Thomas Serre"
    ],
    "author_ids": [],
    "abstract": "A multitude of explainability methods and associated fidelity performance\nmetrics have been proposed to help better understand how modern AI systems make\ndecisions. However, much of the current work has remained theoretical --\nwithout much consideration for the human end-user. In particular, it is not yet\nknown (1) how useful current explainability methods are in practice for more\nreal-world scenarios and (2) how well associated performance metrics accurately\npredict how much knowledge individual explanations contribute to a human\nend-user trying to understand the inner-workings of the system. To fill this\ngap, we conducted psychophysics experiments at scale to evaluate the ability of\nhuman participants to leverage representative attribution methods for\nunderstanding the behavior of different image classifiers representing three\nreal-world scenarios: identifying bias in an AI system, characterizing the\nvisual strategy it uses for tasks that are too difficult for an untrained\nnon-expert human observer as well as understanding its failure cases. Our\nresults demonstrate that the degree to which individual attribution methods\nhelp human participants better understand an AI system varied widely across\nthese scenarios. This suggests a critical need for the field to move past\nquantitative improvements of current attribution methods towards the\ndevelopment of complementary approaches that provide qualitatively different\nsources of information to human end-users.",
    "published_date": "2021-12-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.HC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.04417v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.03183v2",
    "title": "Modification-Fair Cluster Editing",
    "authors": [
      "Vincent Froese",
      "Leon Kellerhals",
      "Rolf Niedermeier"
    ],
    "author_ids": [],
    "abstract": "The classic Cluster Editing problem (also known as Correlation Clustering)\nasks to transform a given graph into a disjoint union of cliques (clusters) by\na small number of edge modifications. When applied to vertex-colored graphs\n(the colors representing subgroups), standard algorithms for the NP-hard\nCluster Editing problem may yield solutions that are biased towards subgroups\nof data (e.g., demographic groups), measured in the number of modifications\nincident to the members of the subgroups. We propose a modification fairness\nconstraint which ensures that the number of edits incident to each subgroup is\nproportional to its size. To start with, we study Modification-Fair Cluster\nEditing for graphs with two vertex colors. We show that the problem is NP-hard\neven if one may only insert edges within a subgroup; note that in the classic\n\"non-fair\" setting, this case is trivially polynomial-time solvable. However,\nin the more general editing form, the modification-fair variant remains\nfixed-parameter tractable with respect to the number of edge edits. We\ncomplement these and further theoretical results with an empirical analysis of\nour model on real-world social networks where we find that the price of\nmodification-fairness is surprisingly low, that is, the cost of optimal\nmodification-fair solutions differs from the cost of optimal \"non-fair\"\nsolutions only by a small percentage.",
    "published_date": "2021-12-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DS",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.03183v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.03111v1",
    "title": "Ethics and Creativity in Computer Vision",
    "authors": [
      "Negar Rostamzadeh",
      "Emily Denton",
      "Linda Petrini"
    ],
    "author_ids": [],
    "abstract": "This paper offers a retrospective of what we learnt from organizing the\nworkshop *Ethical Considerations in Creative applications of Computer Vision*\nat CVPR 2021 conference and, prior to that, a series of workshops on *Computer\nVision for Fashion, Art and Design* at ECCV 2018, ICCV 2019, and CVPR 2020. We\nhope this reflection will bring artists and machine learning researchers into\nconversation around the ethical and social dimensions of creative applications\nof computer vision.",
    "published_date": "2021-12-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.03111v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.03057v1",
    "title": "Thinking Beyond Distributions in Testing Machine Learned Models",
    "authors": [
      "Negar Rostamzadeh",
      "Ben Hutchinson",
      "Christina Greer",
      "Vinodkumar Prabhakaran"
    ],
    "author_ids": [],
    "abstract": "Testing practices within the machine learning (ML) community have centered\naround assessing a learned model's predictive performance measured against a\ntest dataset, often drawn from the same distribution as the training dataset.\nWhile recent work on robustness and fairness testing within the ML community\nhas pointed to the importance of testing against distributional shifts, these\nefforts also focus on estimating the likelihood of the model making an error\nagainst a reference dataset/distribution. We argue that this view of testing\nactively discourages researchers and developers from looking into other sources\nof robustness failures, for instance corner cases which may have severe\nundesirable impacts. We draw parallels with decades of work within software\nengineering testing focused on assessing a software system against various\nstress conditions, including corner cases, as opposed to solely focusing on\naverage-case behaviour. Finally, we put forth a set of recommendations to\nbroaden the view of machine learning testing to a rigorous practice.",
    "published_date": "2021-12-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.03057v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.02960v3",
    "title": "Two Wrongs Don't Make a Right: Combating Confirmation Bias in Learning with Label Noise",
    "authors": [
      "Mingcai Chen",
      "Hao Cheng",
      "Yuntao Du",
      "Ming Xu",
      "Wenyu Jiang",
      "Chongjun Wang"
    ],
    "author_ids": [],
    "abstract": "Noisy labels damage the performance of deep networks. For robust learning, a\nprominent two-stage pipeline alternates between eliminating possible incorrect\nlabels and semi-supervised training. However, discarding part of noisy labels\ncould result in a loss of information, especially when the corruption has a\ndependency on data, e.g., class-dependent or instance-dependent. Moreover, from\nthe training dynamics of a representative two-stage method DivideMix, we\nidentify the domination of confirmation bias: pseudo-labels fail to correct a\nconsiderable amount of noisy labels, and consequently, the errors accumulate.\nTo sufficiently exploit information from noisy labels and mitigate wrong\ncorrections, we propose Robust Label Refurbishment (Robust LR) a new hybrid\nmethod that integrates pseudo-labeling and confidence estimation techniques to\nrefurbish noisy labels. We show that our method successfully alleviates the\ndamage of both label noise and confirmation bias. As a result, it achieves\nstate-of-the-art performance across datasets and noise types, namely CIFAR\nunder different levels of synthetic noise and Mini-WebVision and ANIMAL-10N\nwith real-world noise.",
    "published_date": "2021-12-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.02960v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.05277v1",
    "title": "Skeletal Graph Self-Attention: Embedding a Skeleton Inductive Bias into Sign Language Production",
    "authors": [
      "Ben Saunders",
      "Necati Cihan Camgoz",
      "Richard Bowden"
    ],
    "author_ids": [],
    "abstract": "Recent approaches to Sign Language Production (SLP) have adopted spoken\nlanguage Neural Machine Translation (NMT) architectures, applied without\nsign-specific modifications. In addition, these works represent sign language\nas a sequence of skeleton pose vectors, projected to an abstract representation\nwith no inherent skeletal structure. In this paper, we represent sign language\nsequences as a skeletal graph structure, with joints as nodes and both spatial\nand temporal connections as edges. To operate on this graphical structure, we\npropose Skeletal Graph Self-Attention (SGSA), a novel graphical attention layer\nthat embeds a skeleton inductive bias into the SLP model. Retaining the\nskeletal feature representation throughout, we directly apply a spatio-temporal\nadjacency matrix into the self-attention formulation. This provides structure\nand context to each skeletal joint that is not possible when using a\nnon-graphical abstract representation, enabling fluid and expressive sign\nlanguage production. We evaluate our Skeletal Graph Self-Attention architecture\non the challenging RWTH-PHOENIX-Weather-2014T(PHOENIX14T) dataset, achieving\nstate-of-the-art back translation performance with an 8% and 7% improvement\nover competing methods for the dev and test sets.",
    "published_date": "2021-12-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.05277v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.02870v1",
    "title": "A Marketplace for Trading AI Models based on Blockchain and Incentives for IoT Data",
    "authors": [
      "Lam Duc Nguyen",
      "Shashi Raj Pandey",
      "Soret Beatriz",
      "Arne Broering",
      "Petar Popovski"
    ],
    "author_ids": [],
    "abstract": "As Machine Learning (ML) models are becoming increasingly complex, one of the\ncentral challenges is their deployment at scale, such that companies and\norganizations can create value through Artificial Intelligence (AI). An\nemerging paradigm in ML is a federated approach where the learning model is\ndelivered to a group of heterogeneous agents partially, allowing agents to\ntrain the model locally with their own data. However, the problem of valuation\nof models, as well the questions of incentives for collaborative training and\ntrading of data/models, have received limited treatment in the literature. In\nthis paper, a new ecosystem of ML model trading over a trusted Blockchain-based\nnetwork is proposed. The buyer can acquire the model of interest from the ML\nmarket, and interested sellers spend local computations on their data to\nenhance that model's quality. In doing so, the proportional relation between\nthe local data and the quality of trained models is considered, and the\nvaluations of seller's data in training the models are estimated through the\ndistributed Data Shapley Value (DSV). At the same time, the trustworthiness of\nthe entire trading process is provided by the distributed Ledger Technology\n(DLT). Extensive experimental evaluation of the proposed approach shows a\ncompetitive run-time performance, with a 15\\% drop in the cost of execution,\nand fairness in terms of incentives for the participants.",
    "published_date": "2021-12-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.02870v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.02841v2",
    "title": "GETAM: Gradient-weighted Element-wise Transformer Attention Map for Weakly-supervised Semantic segmentation",
    "authors": [
      "Weixuan Sun",
      "Jing Zhang",
      "Zheyuan Liu",
      "Yiran Zhong",
      "Nick Barnes"
    ],
    "author_ids": [],
    "abstract": "Weakly Supervised Semantic Segmentation (WSSS) is challenging, particularly\nwhen image-level labels are used to supervise pixel level prediction. To bridge\ntheir gap, a Class Activation Map (CAM) is usually generated to provide pixel\nlevel pseudo labels. CAMs in Convolutional Neural Networks suffer from partial\nactivation ie, only the most discriminative regions are activated. Transformer\nbased methods, on the other hand, are highly effective at exploring global\ncontext with long range dependency modeling, potentially alleviating the\n\"partial activation\" issue. In this paper, we propose the first transformer\nbased WSSS approach, and introduce the Gradient weighted Element wise\nTransformer Attention Map (GETAM). GETAM shows fine scale activation for all\nfeature map elements, revealing different parts of the object across\ntransformer layers. Further, we propose an activation aware label completion\nmodule to generate high quality pseudo labels. Finally, we incorporate our\nmethods into an end to end framework for WSSS using double backward\npropagation. Extensive experiments on PASCAL VOC and COCO demonstrate that our\nresults beat the state-of-the-art end-to-end approaches by a significant\nmargin, and outperform most multi-stage methods.m most multi-stage methods.",
    "published_date": "2021-12-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.02841v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.02833v1",
    "title": "Two-step Lookahead Bayesian Optimization with Inequality Constraints",
    "authors": [
      "Yunxiang Zhang",
      "Xiangyu Zhang",
      "Peter I. Frazier"
    ],
    "author_ids": [],
    "abstract": "Recent advances in computationally efficient non-myopic Bayesian optimization\n(BO) improve query efficiency over traditional myopic methods like expected\nimprovement while only modestly increasing computational cost. These advances\nhave been largely limited, however, to unconstrained optimization. For\nconstrained optimization, the few existing non-myopic BO methods require heavy\ncomputation. For instance, one existing non-myopic constrained BO method [Lam\nand Willcox, 2017] relies on computationally expensive unreliable brute-force\nderivative-free optimization of a Monte Carlo rollout acquisition function.\nMethods that use the reparameterization trick for more efficient\nderivative-based optimization of non-myopic acquisition functions in the\nunconstrained setting, like sample average approximation and infinitesimal\nperturbation analysis, do not extend: constraints introduce discontinuities in\nthe sampled acquisition function surface that hinder its optimization.\nMoreover, we argue here that being non-myopic is even more important in\nconstrained problems because fear of violating constraints pushes myopic\nmethods away from sampling the boundary between feasible and infeasible\nregions, slowing the discovery of optimal solutions with tight constraints. In\nthis paper, we propose a computationally efficient two-step lookahead\nconstrained Bayesian optimization acquisition function (2-OPT-C) supporting\nboth sequential and batch settings. To enable fast acquisition function\noptimization, we develop a novel likelihood-ratio-based unbiased estimator of\nthe gradient of the two-step optimal acquisition function that does not use the\nreparameterization trick. In numerical experiments, 2-OPT-C typically improves\nquery efficiency by 2x or more over previous methods, and in some cases by 10x\nor more.",
    "published_date": "2021-12-06T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.02833v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.02746v1",
    "title": "Unfairness Despite Awareness: Group-Fair Classification with Strategic Agents",
    "authors": [
      "Andrew Estornell",
      "Sanmay Das",
      "Yang Liu",
      "Yevgeniy Vorobeychik"
    ],
    "author_ids": [],
    "abstract": "The use of algorithmic decision making systems in domains which impact the\nfinancial, social, and political well-being of people has created a demand for\nthese decision making systems to be \"fair\" under some accepted notion of\nequity. This demand has in turn inspired a large body of work focused on the\ndevelopment of fair learning algorithms which are then used in lieu of their\nconventional counterparts. Most analysis of such fair algorithms proceeds from\nthe assumption that the people affected by the algorithmic decisions are\nrepresented as immutable feature vectors. However, strategic agents may possess\nboth the ability and the incentive to manipulate this observed feature vector\nin order to attain a more favorable outcome. We explore the impact that\nstrategic agent behavior could have on fair classifiers and derive conditions\nunder which this behavior leads to fair classifiers becoming less fair than\ntheir conventional counterparts under the same measure of fairness that the\nfair classifier takes into account. These conditions are related to the the way\nin which the fair classifier remedies unfairness on the original unmanipulated\ndata: fair classifiers which remedy unfairness by becoming more selective than\ntheir conventional counterparts are the ones that become less fair than their\ncounterparts when agents are strategic. We further demonstrate that both the\nincreased selectiveness of the fair classifier, and consequently the loss of\nfairness, arises when performing fair learning on domains in which the\nadvantaged group is overrepresented in the region near (and on the beneficial\nside of) the decision boundary of conventional classifiers. Finally, we observe\nexperimentally, using several datasets and learning methods, that this fairness\nreversal is common, and that our theoretical characterization of the fairness\nreversal conditions indeed holds in most such cases.",
    "published_date": "2021-12-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.MA",
      "cs.CY",
      "cs.GT",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.02746v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.02530v1",
    "title": "Exploring and Mitigating Gender Bias in Recommender Systems with Explicit Feedback",
    "authors": [
      "Shrikant Saxena",
      "Shweta Jain"
    ],
    "author_ids": [],
    "abstract": "Recommender systems are indispensable because they influence our day-to-day\nbehavior and decisions by giving us personalized suggestions. Services like\nKindle, Youtube, and Netflix depend heavily on the performance of their\nrecommender systems to ensure that their users have a good experience and to\nincrease revenues. Despite their popularity, it has been shown that recommender\nsystems reproduce and amplify the bias present in the real world. The resulting\nfeedback creates a self-perpetuating loop that deteriorates the user experience\nand results in homogenizing recommendations over time. Further, biased\nrecommendations can also reinforce stereotypes based on gender or ethnicity,\nthus reinforcing the filter bubbles that we live in. In this paper, we address\nthe problem of gender bias in recommender systems with explicit feedback. We\npropose a model to quantify the gender bias present in book rating datasets and\nin the recommendations produced by the recommender systems. Our main\ncontribution is to provide a principled approach to mitigate the bias being\nproduced in the recommendations. We theoretically show that the proposed\napproach provides unbiased recommendations despite biased data. Through\nempirical evaluation on publicly available book rating datasets, we further\nshow that the proposed model can significantly reduce bias without significant\nimpact on accuracy. Our method is model agnostic and can be applied to any\nrecommender system. To demonstrate the performance of our model, we present the\nresults on four recommender algorithms, two from the K-nearest neighbors\nfamily, UserKNN and ItemKNN, and the other two from the matrix factorization\nfamily, Alternating least square and Singular value decomposition.",
    "published_date": "2021-12-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.02530v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2112.02365v2",
    "title": "TransBoost: A Boosting-Tree Kernel Transfer Learning Algorithm for Improving Financial Inclusion",
    "authors": [
      "Yiheng Sun",
      "Tian Lu",
      "Cong Wang",
      "Yuan Li",
      "Huaiyu Fu",
      "Jingran Dong",
      "Yunjie Xu"
    ],
    "author_ids": [],
    "abstract": "The prosperity of mobile and financial technologies has bred and expanded\nvarious kinds of financial products to a broader scope of people, which\ncontributes to advocating financial inclusion. It has non-trivial social\nbenefits of diminishing financial inequality. However, the technical challenges\nin individual financial risk evaluation caused by the distinct characteristic\ndistribution and limited credit history of new users, as well as the\ninexperience of newly-entered companies in handling complex data and obtaining\naccurate labels, impede further promoting financial inclusion. To tackle these\nchallenges, this paper develops a novel transfer learning algorithm (i.e.,\nTransBoost) that combines the merits of tree-based models and kernel methods.\nThe TransBoost is designed with a parallel tree structure and efficient weights\nupdating mechanism with theoretical guarantee, which enables it to excel in\ntackling real-world data with high dimensional features and sparsity in $O(n)$\ntime complexity. We conduct extensive experiments on two public datasets and a\nunique large-scale dataset from Tencent Mobile Payment. The results show that\nthe TransBoost outperforms other state-of-the-art benchmark transfer learning\nalgorithms in terms of prediction accuracy with superior efficiency, shows\nstronger robustness to data sparsity, and provides meaningful model\ninterpretation. Besides, given a financial risk level, the TransBoost enables\nfinancial service providers to serve the largest number of users including\nthose who would otherwise be excluded by other algorithms. That is, the\nTransBoost improves financial inclusion.",
    "published_date": "2021-12-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "q-fin.ST"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.02365v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.02170v1",
    "title": "Counterfactual Fairness in Mortgage Lending via Matching and Randomization",
    "authors": [
      "Sama Ghoba",
      "Nathan Colaner"
    ],
    "author_ids": [],
    "abstract": "Unfairness in mortgage lending has created generational inequality among\nracial and ethnic groups in the US. Many studies address this problem, but most\nexisting work focuses on correlation-based techniques. In our work, we use the\nframework of counterfactual fairness to train fair machine learning models. We\npropose a new causal graph for the variables available in the Home Mortgage\nDisclosure Act (HMDA) data. We use a matching-based approach instead of the\nlatent variable modeling approach, because the former approach does not rely on\nany modeling assumptions. Furthermore, matching provides us with counterfactual\npairs in which the race variable is isolated. We first demonstrate the\nunfairness in mortgage approval and interest rates between African-American and\nnon-Hispanic White sub-populations. Then, we show that having balanced data\nusing matching does not guarantee perfect counterfactual fairness of the\nmachine learning models.",
    "published_date": "2021-12-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.02170v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.02086v2",
    "title": "Data-Free Neural Architecture Search via Recursive Label Calibration",
    "authors": [
      "Zechun Liu",
      "Zhiqiang Shen",
      "Yun Long",
      "Eric Xing",
      "Kwang-Ting Cheng",
      "Chas Leichner"
    ],
    "author_ids": [],
    "abstract": "This paper aims to explore the feasibility of neural architecture search\n(NAS) given only a pre-trained model without using any original training data.\nThis is an important circumstance for privacy protection, bias avoidance, etc.,\nin real-world scenarios. To achieve this, we start by synthesizing usable data\nthrough recovering the knowledge from a pre-trained deep neural network. Then\nwe use the synthesized data and their predicted soft-labels to guide neural\narchitecture search. We identify that the NAS task requires the synthesized\ndata (we target at image domain here) with enough semantics, diversity, and a\nminimal domain gap from the natural images. For semantics, we propose recursive\nlabel calibration to produce more informative outputs. For diversity, we\npropose a regional update strategy to generate more diverse and\nsemantically-enriched synthetic data. For minimal domain gap, we use input and\nfeature-level regularization to mimic the original data distribution in latent\nspace. We instantiate our proposed framework with three popular NAS algorithms:\nDARTS, ProxylessNAS and SPOS. Surprisingly, our results demonstrate that the\narchitectures discovered by searching with our synthetic data achieve accuracy\nthat is comparable to, or even higher than, architectures discovered by\nsearching from the original ones, for the first time, deriving the conclusion\nthat NAS can be done effectively with no need of access to the original or\ncalled natural data if the synthesis method is well designed.",
    "published_date": "2021-12-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.02086v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.02034v1",
    "title": "Could AI Democratise Education? Socio-Technical Imaginaries of an EdTech Revolution",
    "authors": [
      "Sahan Bulathwela",
      "María Pérez-Ortiz",
      "Catherine Holloway",
      "John Shawe-Taylor"
    ],
    "author_ids": [],
    "abstract": "Artificial Intelligence (AI) in Education has been said to have the potential\nfor building more personalised curricula, as well as democratising education\nworldwide and creating a Renaissance of new ways of teaching and learning.\nMillions of students are already starting to benefit from the use of these\ntechnologies, but millions more around the world are not. If this trend\ncontinues, the first delivery of AI in Education could be greater educational\ninequality, along with a global misallocation of educational resources\nmotivated by the current technological determinism narrative. In this paper, we\nfocus on speculating and posing questions around the future of AI in Education,\nwith the aim of starting the pressing conversation that would set the right\nfoundations for the new generation of education that is permeated by\ntechnology. This paper starts by synthesising how AI might change how we learn\nand teach, focusing specifically on the case of personalised learning\ncompanions, and then move to discuss some socio-technical features that will be\ncrucial for avoiding the perils of these AI systems worldwide (and perhaps\nensuring their success). This paper also discusses the potential of using AI\ntogether with free, participatory and democratic resources, such as Wikipedia,\nOpen Educational Resources and open-source tools. We also emphasise the need\nfor collectively designing human-centered, transparent, interactive and\ncollaborative AI-based algorithms that empower and give complete agency to\nstakeholders, as well as support new emerging pedagogies. Finally, we ask what\nwould it take for this educational revolution to provide egalitarian and\nempowering access to education, beyond any political, cultural, language,\ngeographical and learning ability barriers.",
    "published_date": "2021-12-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "stat.ML",
      "K.3.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.02034v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.01917v2",
    "title": "A Structured Dictionary Perspective on Implicit Neural Representations",
    "authors": [
      "Gizem Yüce",
      "Guillermo Ortiz-Jiménez",
      "Beril Besbinar",
      "Pascal Frossard"
    ],
    "author_ids": [],
    "abstract": "Implicit neural representations (INRs) have recently emerged as a promising\nalternative to classical discretized representations of signals. Nevertheless,\ndespite their practical success, we still do not understand how INRs represent\nsignals. We propose a novel unified perspective to theoretically analyse INRs.\nLeveraging results from harmonic analysis and deep learning theory, we show\nthat most INR families are analogous to structured signal dictionaries whose\natoms are integer harmonics of the set of initial mapping frequencies. This\nstructure allows INRs to express signals with an exponentially increasing\nfrequency support using a number of parameters that only grows linearly with\ndepth. We also explore the inductive bias of INRs exploiting recent results\nabout the empirical neural tangent kernel (NTK). Specifically, we show that the\neigenfunctions of the NTK can be seen as dictionary atoms whose inner product\nwith the target signal determines the final performance of their\nreconstruction. In this regard, we reveal that meta-learning has a reshaping\neffect on the NTK analogous to dictionary learning, building dictionary atoms\nas a combination of the examples seen during meta-training. Our results permit\nto design and tune novel INR architectures, but can also be of interest for the\nwider deep learning theory community.",
    "published_date": "2021-12-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.01917v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.01902v1",
    "title": "HS-BAN: A Benchmark Dataset of Social Media Comments for Hate Speech Detection in Bangla",
    "authors": [
      "Nauros Romim",
      "Mosahed Ahmed",
      "Md Saiful Islam",
      "Arnab Sen Sharma",
      "Hriteshwar Talukder",
      "Mohammad Ruhul Amin"
    ],
    "author_ids": [],
    "abstract": "In this paper, we present HS-BAN, a binary class hate speech (HS) dataset in\nBangla language consisting of more than 50,000 labeled comments, including\n40.17% hate and rest are non hate speech. While preparing the dataset a strict\nand detailed annotation guideline was followed to reduce human annotation bias.\nThe HS dataset was also preprocessed linguistically to extract different types\nof slang currently people write using symbols, acronyms, or alternative\nspellings. These slang words were further categorized into traditional and\nnon-traditional slang lists and included in the results of this paper. We\nexplored traditional linguistic features and neural network-based methods to\ndevelop a benchmark system for hate speech detection for the Bangla language.\nOur experimental results show that existing word embedding models trained with\ninformal texts perform better than those trained with formal text. Our\nbenchmark shows that a Bi-LSTM model on top of the FastText informal word\nembedding achieved 86.78% F1-score. We will make the dataset available for\npublic use.",
    "published_date": "2021-12-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.01902v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.01901v1",
    "title": "The Box Size Confidence Bias Harms Your Object Detector",
    "authors": [
      "Johannes Gilg",
      "Torben Teepe",
      "Fabian Herzog",
      "Gerhard Rigoll"
    ],
    "author_ids": [],
    "abstract": "Countless applications depend on accurate predictions with reliable\nconfidence estimates from modern object detectors. It is well known, however,\nthat neural networks including object detectors produce miscalibrated\nconfidence estimates. Recent work even suggests that detectors' confidence\npredictions are biased with respect to object size and position, but it is\nstill unclear how this bias relates to the performance of the affected object\ndetectors. We formally prove that the conditional confidence bias is harming\nthe expected performance of object detectors and empirically validate these\nfindings. Specifically, we demonstrate how to modify the histogram binning\ncalibration to not only avoid performance impairment but also improve\nperformance through conditional confidence calibration. We further find that\nthe confidence bias is also present in detections generated on the training\ndata of the detector, which we leverage to perform our de-biasing without using\nadditional data. Moreover, Test Time Augmentation magnifies this bias, which\nresults in even larger performance gains from our calibration method. Finally,\nwe validate our findings on a diverse set of object detection architectures and\nshow improvements of up to 0.6 mAP and 0.8 mAP50 without extra data or\ntraining.",
    "published_date": "2021-12-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.01901v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.01830v1",
    "title": "Table2Vec: Automated Universal Representation Learning to Encode All-round Data DNA for Benchmarkable and Explainable Enterprise Data Science",
    "authors": [
      "Longbing Cao",
      "Chengzhang Zhu"
    ],
    "author_ids": [],
    "abstract": "Enterprise data typically involves multiple heterogeneous data sources and\nexternal data that respectively record business activities, transactions,\ncustomer demographics, status, behaviors, interactions and communications with\nthe enterprise, and the consumption and feedback of its products, services,\nproduction, marketing, operations, and management, etc. A critical challenge in\nenterprise data science is to enable an effective whole-of-enterprise data\nunderstanding and data-driven discovery and decision-making on all-round\nenterprise DNA. We introduce a neural encoder Table2Vec for automated universal\nrepresentation learning of entities such as customers from all-round enterprise\nDNA with automated data characteristics analysis and data quality augmentation.\nThe learned universal representations serve as representative and benchmarkable\nenterprise data genomes and can be used for enterprise-wide and domain-specific\nlearning tasks. Table2Vec integrates automated universal representation\nlearning on low-quality enterprise data and downstream learning tasks. We\nillustrate Table2Vec in characterizing all-round customer data DNA in an\nenterprise on complex heterogeneous multi-relational big tables to build\nuniversal customer vector representations. The learned universal representation\nof each customer is all-round, representative and benchmarkable to support both\nenterprise-wide and domain-specific learning goals and tasks in enterprise data\nscience. Table2Vec significantly outperforms the existing shallow, boosting and\ndeep learning methods typically used for enterprise analytics. We further\ndiscuss the research opportunities, directions and applications of automated\nuniversal enterprise representation and learning and the learned enterprise\ndata DNA for automated, all-purpose, whole-of-enterprise and ethical machine\nlearning and data science.",
    "published_date": "2021-12-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.01830v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.01723v1",
    "title": "Adversarial Attacks against a Satellite-borne Multispectral Cloud Detector",
    "authors": [
      "Andrew Du",
      "Yee Wei Law",
      "Michele Sasdelli",
      "Bo Chen",
      "Ken Clarke",
      "Michael Brown",
      "Tat-Jun Chin"
    ],
    "author_ids": [],
    "abstract": "Data collected by Earth-observing (EO) satellites are often afflicted by\ncloud cover. Detecting the presence of clouds -- which is increasingly done\nusing deep learning -- is crucial preprocessing in EO applications. In fact,\nadvanced EO satellites perform deep learning-based cloud detection on board the\nsatellites and downlink only clear-sky data to save precious bandwidth. In this\npaper, we highlight the vulnerability of deep learning-based cloud detection\ntowards adversarial attacks. By optimising an adversarial pattern and\nsuperimposing it into a cloudless scene, we bias the neural network into\ndetecting clouds in the scene. Since the input spectra of cloud detectors\ninclude the non-visible bands, we generated our attacks in the multispectral\ndomain. This opens up the potential of multi-objective attacks, specifically,\nadversarial biasing in the cloud-sensitive bands and visual camouflage in the\nvisible bands. We also investigated mitigation strategies against the\nadversarial attacks. We hope our work further builds awareness of the potential\nof adversarial attacks in the EO community.",
    "published_date": "2021-12-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.01723v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.01716v1",
    "title": "Reduced, Reused and Recycled: The Life of a Dataset in Machine Learning Research",
    "authors": [
      "Bernard Koch",
      "Emily Denton",
      "Alex Hanna",
      "Jacob G. Foster"
    ],
    "author_ids": [],
    "abstract": "Benchmark datasets play a central role in the organization of machine\nlearning research. They coordinate researchers around shared research problems\nand serve as a measure of progress towards shared goals. Despite the\nfoundational role of benchmarking practices in this field, relatively little\nattention has been paid to the dynamics of benchmark dataset use and reuse,\nwithin or across machine learning subcommunities. In this paper, we dig into\nthese dynamics. We study how dataset usage patterns differ across machine\nlearning subcommunities and across time from 2015-2020. We find increasing\nconcentration on fewer and fewer datasets within task communities, significant\nadoption of datasets from other tasks, and concentration across the field on\ndatasets that have been introduced by researchers situated within a small\nnumber of elite institutions. Our results have implications for scientific\nevaluation, AI ethics, and equity/access within the field.",
    "published_date": "2021-12-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.CV",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.01716v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.01715v1",
    "title": "Self-Supervised Material and Texture Representation Learning for Remote Sensing Tasks",
    "authors": [
      "Peri Akiva",
      "Matthew Purri",
      "Matthew Leotta"
    ],
    "author_ids": [],
    "abstract": "Self-supervised learning aims to learn image feature representations without\nthe usage of manually annotated labels. It is often used as a precursor step to\nobtain useful initial network weights which contribute to faster convergence\nand superior performance of downstream tasks. While self-supervision allows one\nto reduce the domain gap between supervised and unsupervised learning without\nthe usage of labels, the self-supervised objective still requires a strong\ninductive bias to downstream tasks for effective transfer learning. In this\nwork, we present our material and texture based self-supervision method named\nMATTER (MATerial and TExture Representation Learning), which is inspired by\nclassical material and texture methods. Material and texture can effectively\ndescribe any surface, including its tactile properties, color, and specularity.\nBy extension, effective representation of material and texture can describe\nother semantic classes strongly associated with said material and texture.\nMATTER leverages multi-temporal, spatially aligned remote sensing imagery over\nunchanged regions to learn invariance to illumination and viewing angle as a\nmechanism to achieve consistency of material and texture representation. We\nshow that our self-supervision pre-training method allows for up to 24.22% and\n6.33% performance increase in unsupervised and fine-tuned setups, and up to 76%\nfaster convergence on change detection, land cover classification, and semantic\nsegmentation tasks.",
    "published_date": "2021-12-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.01715v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.01683v1",
    "title": "TransZero: Attribute-guided Transformer for Zero-Shot Learning",
    "authors": [
      "Shiming Chen",
      "Ziming Hong",
      "Yang Liu",
      "Guo-Sen Xie",
      "Baigui Sun",
      "Hao Li",
      "Qinmu Peng",
      "Ke Lu",
      "Xinge You"
    ],
    "author_ids": [],
    "abstract": "Zero-shot learning (ZSL) aims to recognize novel classes by transferring\nsemantic knowledge from seen classes to unseen ones. Semantic knowledge is\nlearned from attribute descriptions shared between different classes, which act\nas strong priors for localizing object attributes that represent discriminative\nregion features, enabling significant visual-semantic interaction. Although\nsome attention-based models have attempted to learn such region features in a\nsingle image, the transferability and discriminative attribute localization of\nvisual features are typically neglected. In this paper, we propose an\nattribute-guided Transformer network, termed TransZero, to refine visual\nfeatures and learn attribute localization for discriminative visual embedding\nrepresentations in ZSL. Specifically, TransZero takes a feature augmentation\nencoder to alleviate the cross-dataset bias between ImageNet and ZSL\nbenchmarks, and improves the transferability of visual features by reducing the\nentangled relative geometry relationships among region features. To learn\nlocality-augmented visual features, TransZero employs a visual-semantic decoder\nto localize the image regions most relevant to each attribute in a given image,\nunder the guidance of semantic attribute information. Then, the\nlocality-augmented visual features and semantic vectors are used to conduct\neffective visual-semantic interaction in a visual-semantic embedding network.\nExtensive experiments show that TransZero achieves the new state of the art on\nthree ZSL benchmarks. The codes are available at:\n\\url{https://github.com/shiming-chen/TransZero}.",
    "published_date": "2021-12-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.01683v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.01513v3",
    "title": "OW-DETR: Open-world Detection Transformer",
    "authors": [
      "Akshita Gupta",
      "Sanath Narayan",
      "K J Joseph",
      "Salman Khan",
      "Fahad Shahbaz Khan",
      "Mubarak Shah"
    ],
    "author_ids": [],
    "abstract": "Open-world object detection (OWOD) is a challenging computer vision problem,\nwhere the task is to detect a known set of object categories while\nsimultaneously identifying unknown objects. Additionally, the model must\nincrementally learn new classes that become known in the next training\nepisodes. Distinct from standard object detection, the OWOD setting poses\nsignificant challenges for generating quality candidate proposals on\npotentially unknown objects, separating the unknown objects from the background\nand detecting diverse unknown objects. Here, we introduce a novel end-to-end\ntransformer-based framework, OW-DETR, for open-world object detection. The\nproposed OW-DETR comprises three dedicated components namely, attention-driven\npseudo-labeling, novelty classification and objectness scoring to explicitly\naddress the aforementioned OWOD challenges. Our OW-DETR explicitly encodes\nmulti-scale contextual information, possesses less inductive bias, enables\nknowledge transfer from known classes to the unknown class and can better\ndiscriminate between unknown objects and background. Comprehensive experiments\nare performed on two benchmarks: MS-COCO and PASCAL VOC. The extensive\nablations reveal the merits of our proposed contributions. Further, our model\noutperforms the recently introduced OWOD approach, ORE, with absolute gains\nranging from 1.8% to 3.3% in terms of unknown recall on MS-COCO. In the case of\nincremental object detection, OW-DETR outperforms the state-of-the-art for all\nsettings on PASCAL VOC. Our code is available at\nhttps://github.com/akshitac8/OW-DETR.",
    "published_date": "2021-12-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.01513v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.01387v1",
    "title": "Generalizing Off-Policy Learning under Sample Selection Bias",
    "authors": [
      "Tobias Hatt",
      "Daniel Tschernutter",
      "Stefan Feuerriegel"
    ],
    "author_ids": [],
    "abstract": "Learning personalized decision policies that generalize to the target\npopulation is of great relevance. Since training data is often not\nrepresentative of the target population, standard policy learning methods may\nyield policies that do not generalize target population. To address this\nchallenge, we propose a novel framework for learning policies that generalize\nto the target population. For this, we characterize the difference between the\ntraining data and the target population as a sample selection bias using a\nselection variable. Over an uncertainty set around this selection variable, we\noptimize the minimax value of a policy to achieve the best worst-case policy\nvalue on the target population. In order to solve the minimax problem, we\nderive an efficient algorithm based on a convex-concave procedure and prove\nconvergence for parametrized spaces of policies such as logistic policies. We\nprove that, if the uncertainty set is well-specified, our policies generalize\nto the target population as they can not do worse than on the training data.\nUsing simulated data and a clinical trial, we demonstrate that, compared to\nstandard policy learning methods, our framework improves the generalizability\nof policies substantially.",
    "published_date": "2021-12-02T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.01387v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.01121v1",
    "title": "\"Just Drive\": Colour Bias Mitigation for Semantic Segmentation in the Context of Urban Driving",
    "authors": [
      "Jack Stelling",
      "Amir Atapour-Abarghouei"
    ],
    "author_ids": [],
    "abstract": "Biases can filter into AI technology without our knowledge. Oftentimes,\nseminal deep learning networks champion increased accuracy above all else. In\nthis paper, we attempt to alleviate biases encountered by semantic segmentation\nmodels in urban driving scenes, via an iteratively trained unlearning\nalgorithm. Convolutional neural networks have been shown to rely on colour and\ntexture rather than geometry. This raises issues when safety-critical\napplications, such as self-driving cars, encounter images with covariate shift\nat test time - induced by variations such as lighting changes or seasonality.\nConceptual proof of bias unlearning has been shown on simple datasets such as\nMNIST. However, the strategy has never been applied to the safety-critical\ndomain of pixel-wise semantic segmentation of highly variable training data -\nsuch as urban scenes. Trained models for both the baseline and bias unlearning\nscheme have been tested for performance on colour-manipulated validation sets\nshowing a disparity of up to 85.50% in mIoU from the original RGB images -\nconfirming segmentation networks strongly depend on the colour information in\nthe training data to make their classification. The bias unlearning scheme\nshows improvements of handling this covariate shift of up to 61% in the best\nobserved case - and performs consistently better at classifying the \"human\" and\n\"vehicle\" classes compared to the baseline model.",
    "published_date": "2021-12-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.01121v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.01021v2",
    "title": "Fighting Fire with Fire: Contrastive Debiasing without Bias-free Data via Generative Bias-transformation",
    "authors": [
      "Yeonsung Jung",
      "Hajin Shim",
      "June Yong Yang",
      "Eunho Yang"
    ],
    "author_ids": [],
    "abstract": "Deep neural networks (DNNs), despite their impressive ability to generalize\nover-capacity networks, often rely heavily on malignant bias as shortcuts\ninstead of task-related information for discriminative tasks. To address this\nproblem, recent studies utilize auxiliary information related to the bias,\nwhich is rarely obtainable in practice, or sift through a handful of bias-free\nsamples for debiasing. However, the success of these methods is not always\nguaranteed due to the unfulfilled presumptions. In this paper, we propose a\nnovel method, Contrastive Debiasing via Generative Bias-transformation (CDvG),\nwhich works without explicit bias labels or bias-free samples. Motivated by our\nobservation that not only discriminative models but also image translation\nmodels tend to focus on the malignant bias, CDvG employs an image translation\nmodel to transform one bias mode into another while preserving the\ntask-relevant information. Additionally, the bias-transformed views are set\nagainst each other through contrastive learning to learn bias-invariant\nrepresentations. Our method demonstrates superior performance compared to prior\napproaches, especially when bias-free samples are scarce or absent.\nFurthermore, CDvG can be integrated with the methods that focus on bias-free\nsamples in a plug-and-play manner for additional enhancements, as demonstrated\nby diverse experimental results.",
    "published_date": "2021-12-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.01021v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.01020v2",
    "title": "Learning Optimal Predictive Checklists",
    "authors": [
      "Haoran Zhang",
      "Quaid Morris",
      "Berk Ustun",
      "Marzyeh Ghassemi"
    ],
    "author_ids": [],
    "abstract": "Checklists are simple decision aids that are often used to promote safety and\nreliability in clinical applications. In this paper, we present a method to\nlearn checklists for clinical decision support. We represent predictive\nchecklists as discrete linear classifiers with binary features and unit\nweights. We then learn globally optimal predictive checklists from data by\nsolving an integer programming problem. Our method allows users to customize\nchecklists to obey complex constraints, including constraints to enforce group\nfairness and to binarize real-valued features at training time. In addition, it\npairs models with an optimality gap that can inform model development and\ndetermine the feasibility of learning sufficiently accurate checklists on a\ngiven dataset. We pair our method with specialized techniques that speed up its\nability to train a predictive checklist that performs well and has a small\noptimality gap. We benchmark the performance of our method on seven clinical\nclassification problems, and demonstrate its practical benefits by training a\nshort-form checklist for PTSD screening. Our results show that our method can\nfit simple predictive checklists that perform well and that can easily be\ncustomized to obey a rich class of custom constraints.",
    "published_date": "2021-12-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.01020v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.00859v1",
    "title": "Are Investors Biased Against Women? Analyzing How Gender Affects Startup Funding in Europe",
    "authors": [
      "Michael Färber",
      "Alexander Klein"
    ],
    "author_ids": [],
    "abstract": "One of the main challenges of startups is to raise capital from investors.\nFor startup founders, it is therefore crucial to know whether investors have a\nbias against women as startup founders and in which way startups face\ndisadvantages due to gender bias. Existing works on gender studies have mainly\nanalyzed the US market. In this paper, we aim to give a more comprehensive\npicture of gender bias in early-stage startup funding. We examine European\nstartups listed on Crunchbase using Semantic Web technologies and analyze how\nthe share of female founders in a founding team affects the funding amount. We\nfind that the relative amount of female founders has a negative impact on the\nfunding raised. Furthermore, we observe that founder characteristics have an\neffect on the funding raised based on the founders' gender. Moreover, we find\nthat gender bias in early-stage funding is less prevalent for serial founders\nwith entrepreneurial experience as female founders benefit three times more\nthan male founders from already having founded a startup. Overall, our study\nsuggests that gender bias exists and is worth to be considered in the context\nof startup funding.",
    "published_date": "2021-12-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.00859v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2112.00854v2",
    "title": "GANORCON: Are Generative Models Useful for Few-shot Segmentation?",
    "authors": [
      "Oindrila Saha",
      "Zezhou Cheng",
      "Subhransu Maji"
    ],
    "author_ids": [],
    "abstract": "Advances in generative modeling based on GANs has motivated the community to\nfind their use beyond image generation and editing tasks. In particular,\nseveral recent works have shown that GAN representations can be re-purposed for\ndiscriminative tasks such as part segmentation, especially when training data\nis limited. But how do these improvements stack-up against recent advances in\nself-supervised learning? Motivated by this we present an alternative approach\nbased on contrastive learning and compare their performance on standard\nfew-shot part segmentation benchmarks. Our experiments reveal that not only do\nthe GAN-based approach offer no significant performance advantage, their\nmulti-step training is complex, nearly an order-of-magnitude slower, and can\nintroduce additional bias. These experiments suggest that the inductive biases\nof generative models, such as their ability to disentangle shape and texture,\nare well captured by standard feed-forward networks trained using contrastive\nlearning. These experiments suggest that the inductive biases present in\ncurrent generative models, such as their ability to disentangle shape and\ntexture, are well captured by standard feed-forward networks trained using\ncontrastive learning.",
    "published_date": "2021-12-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.00854v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.00718v2",
    "title": "Improving GAN Equilibrium by Raising Spatial Awareness",
    "authors": [
      "Jianyuan Wang",
      "Ceyuan Yang",
      "Yinghao Xu",
      "Yujun Shen",
      "Hongdong Li",
      "Bolei Zhou"
    ],
    "author_ids": [],
    "abstract": "The success of Generative Adversarial Networks (GANs) is largely built upon\nthe adversarial training between a generator (G) and a discriminator (D). They\nare expected to reach a certain equilibrium where D cannot distinguish the\ngenerated images from the real ones. However, such an equilibrium is rarely\nachieved in practical GAN training, instead, D almost always surpasses G. We\nattribute one of its sources to the information asymmetry between D and G. We\nobserve that D learns its own visual attention when determining whether an\nimage is real or fake, but G has no explicit clue on which regions to focus on\nfor a particular synthesis. To alleviate the issue of D dominating the\ncompetition in GANs, we aim to raise the spatial awareness of G. Randomly\nsampled multi-level heatmaps are encoded into the intermediate layers of G as\nan inductive bias. Thus G can purposefully improve the synthesis of certain\nimage regions. We further propose to align the spatial awareness of G with the\nattention map induced from D. Through this way we effectively lessen the\ninformation gap between D and G. Extensive results show that our method pushes\nthe two-player game in GANs closer to the equilibrium, leading to a better\nsynthesis performance. As a byproduct, the introduced spatial awareness\nfacilitates interactive editing over the output synthesis. Demo video and code\nare available at https://genforce.github.io/eqgan-sa/.",
    "published_date": "2021-12-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.00718v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.00672v1",
    "title": "Controlling for multiple covariates",
    "authors": [
      "Mark Tygert"
    ],
    "author_ids": [],
    "abstract": "A fundamental problem in statistics is to compare the outcomes attained by\nmembers of subpopulations. This problem arises in the analysis of randomized\ncontrolled trials, in the analysis of A/B tests, and in the assessment of\nfairness and bias in the treatment of sensitive subpopulations, especially when\nmeasuring the effects of algorithms and machine learning. Often the comparison\nmakes the most sense when performed separately for individuals who are similar\naccording to certain characteristics given by the values of covariates of\ninterest; the separate comparisons can also be aggregated in various ways to\ncompare across all values of the covariates. Separating, segmenting, or\nstratifying into those with similar values of the covariates is also known as\n\"conditioning on\" or \"controlling for\" those covariates; controlling for age or\nannual income is common.\n  Two standard methods of controlling for covariates are (1) binning and (2)\nregression modeling. Binning requires making fairly arbitrary, yet frequently\nhighly influential choices, and is unsatisfactorily temperamental in multiple\ndimensions, with multiple covariates. Regression analysis works wonderfully\nwhen there is good reason to believe in a particular parameterized regression\nmodel or classifier (such as logistic regression). Thus, there appears to be no\nextant canonical fully non-parametric regression for the comparison of\nsubpopulations, not while conditioning on multiple specified covariates.\nExisting methods rely on analysts to make choices, and those choices can be\ndebatable; analysts can deceive others or even themselves. The present paper\naims to fill the gap, combining two ingredients: (1) recently developed\nmethodologies for such comparisons that already exist when conditioning on a\nsingle scalar covariate and (2) the Hilbert space-filling curve that maps\ncontinuously from one dimension to multiple dimensions.",
    "published_date": "2021-12-01T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ME",
      "cs.CY",
      "stat.CO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.00672v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.00323v1",
    "title": "Push Stricter to Decide Better: A Class-Conditional Feature Adaptive Framework for Improving Adversarial Robustness",
    "authors": [
      "Jia-Li Yin",
      "Lehui Xie",
      "Wanqing Zhu",
      "Ximeng Liu",
      "Bo-Hao Chen"
    ],
    "author_ids": [],
    "abstract": "In response to the threat of adversarial examples, adversarial training\nprovides an attractive option for enhancing the model robustness by training\nmodels on online-augmented adversarial examples. However, most of the existing\nadversarial training methods focus on improving the robust accuracy by\nstrengthening the adversarial examples but neglecting the increasing shift\nbetween natural data and adversarial examples, leading to a dramatic decrease\nin natural accuracy. To maintain the trade-off between natural and robust\naccuracy, we alleviate the shift from the perspective of feature adaption and\npropose a Feature Adaptive Adversarial Training (FAAT) optimizing the\nclass-conditional feature adaption across natural data and adversarial\nexamples. Specifically, we propose to incorporate a class-conditional\ndiscriminator to encourage the features become (1) class-discriminative and (2)\ninvariant to the change of adversarial attacks. The novel FAAT framework\nenables the trade-off between natural and robust accuracy by generating\nfeatures with similar distribution across natural and adversarial data, and\nachieve higher overall robustness benefited from the class-discriminative\nfeature characteristics. Experiments on various datasets demonstrate that FAAT\nproduces more discriminative features and performs favorably against\nstate-of-the-art methods. Codes are available at\nhttps://github.com/VisionFlow/FAAT.",
    "published_date": "2021-12-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.00323v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.00301v1",
    "title": "Uncertainty in Criminal Justice Algorithms: simulation studies of the Pennsylvania Additive Classification Tool",
    "authors": [
      "Swarup Dhar",
      "Vanessa Massaro",
      "Darakhshan Mir",
      "Nathan C. Ryan"
    ],
    "author_ids": [],
    "abstract": "Much attention has been paid to algorithms related to sentencing, the setting\nof bail, parole decisions and recidivism while less attention has been paid to\ncarceral algorithms, those algorithms used to determine an incarcerated\nindividual's lived experience. In this paper we study one such algorithm, the\nPennsylvania Additive Classification Tool (PACT) that assigns custody levels to\nincarcerated individuals. We analyze the PACT in ways that criminal justice\nalgorithms are often analyzed: namely, we train an accurate machine learning\nmodel for the PACT; we study its fairness across sex, age and race; and we\ndetermine which features are most important. In addition to these conventional\ncomputations, we propose and carry out some new ways to study such algorithms.\nInstead of focusing on the outcomes themselves, we propose shifting our\nattention to the variability in the outcomes, especially because many carceral\nalgorithms are used repeatedly and there can be a propagation of uncertainty.\nBy carrying out several simulations of assigning custody levels, we shine light\non problematic aspects of tools like the PACT.",
    "published_date": "2021-12-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.00301v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.00269v1",
    "title": "Unequal Opportunities in Multi-hop Referral Programs",
    "authors": [
      "Yiguang Zhang",
      "Augustin Chaintreau"
    ],
    "author_ids": [],
    "abstract": "As modern social networks allow for faster and broader interactions with\nfriends and acquaintances, online referral programs that promote sales through\nexisting users are becoming increasingly popular. Because it is all too common\nthat online networks reproduce historical structural bias, members of\ndisadvantaged groups often benefit less from such referral opportunities. For\ninstance, one-hop referral programs that distribute rewards only among pairs of\nfriends or followers may offer less rewards and opportunities to minorities in\nnetworks where it was proved that their degrees is statistically smaller. Here,\nwe examine the fairness of general referral programs, increasingly popular\nforms of marketing in which an existing referrer is encouraged to initiate the\nrecruitment of new referred users over multiple hops. While this clearly\nexpands opportunities for rewards, it remains unclear whether it helps\naddressing fairness concerns, or make them worse. We show, from studying 4\nreal-world networks and performing theoretical analysis on networks created\nwith minority-majority affiliations and homophily, that the change of bias in\nmulti-hop referral programs highly depends on the network structures and the\nreferral strategies. Specifically, under three different constrained referral\nstrategies which limit the number of referrals each person can share to a fixed\nnumber, we show that even with no explicit intention to discriminate and\nwithout access to sensitive attributes such as gender and race, certain\nreferral strategies can still amplify the structural biases further when higher\nhops are allowed. Moreover, when there is no constraint on the number of\nreferrals each person can distribute and when the effect of referral strategies\nis removed, we prove a precise condition under which the bias in 1-hop referral\nprograms is amplified in higher-hop referral programs.",
    "published_date": "2021-12-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.00269v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2112.00229v4",
    "title": "Frequency Fitness Assignment: Optimization without Bias for Good Solutions can be Efficient",
    "authors": [
      "Thomas Weise",
      "Zhize Wu",
      "Xinlu Li",
      "Yan Chen",
      "Jörg Lässig"
    ],
    "author_ids": [],
    "abstract": "A fitness assignment process transforms the features (such as the objective\nvalue) of a candidate solution to a scalar fitness, which then is the basis for\nselection. Under Frequency Fitness Assignment (FFA), the fitness corresponding\nto an objective value is its encounter frequency in selection steps and is\nsubject to minimization. FFA creates algorithms that are not biased towards\nbetter solutions and are invariant under all injective transformations of the\nobjective function value. We investigate the impact of FFA on the performance\nof two theory-inspired, state-of-the-art EAs, the Greedy (2+1) GA and the\nSelf-Adjusting (1+(lambda,lambda)) GA. FFA improves their performance\nsignificantly on some problems that are hard for them. In our experiments, one\nFFA-based algorithm exhibited mean runtimes that appear to be polynomial on the\ntheory-based benchmark problems in our study, including traps, jumps, and\nplateaus. We propose two hybrid approaches that use both direct and FFA-based\noptimization and find that they perform well. All FFA-based algorithms also\nperform better on satisfiability problems than any of the pure algorithm\nvariants.",
    "published_date": "2021-12-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.NE",
      "cs.AI",
      "math.CO",
      "F.2.2, G.3, G.2.1, G.2.3, B.2.2",
      "F.2.2; G.3; G.2.1; G.2.3; B.2.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.00229v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.00126v1",
    "title": "Martingale product estimators for sensitivity analysis in computational statistical physics",
    "authors": [
      "Petr Plechac",
      "Gabriel Stoltz",
      "Ting Wang"
    ],
    "author_ids": [],
    "abstract": "We introduce a new class of estimators for the linear response of steady\nstates of stochastic dynamics. We generalize the likelihood ratio approach and\nformulate the linear response as a product of two martingales, hence the name\n\"martingale product estimators\". We present a systematic derivation of the\nmartingale product estimator, and show how to construct such estimator so its\nbias is consistent with the weak order of the numerical scheme that\napproximates the underlying stochastic differential equation. Motivated by the\nestimation of transport properties in molecular systems, we present a rigorous\nnumerical analysis of the bias and variance for these new estimators in the\ncase of Langevin dynamics. We prove that the variance is uniformly bounded in\ntime and derive a specific form of the estimator for second-order splitting\nschemes for Langevin dynamics. For comparison, we also study the bias and\nvariance of a Green-Kubo estimator, motivated, in part, by its variance growing\nlinearly in time. Presented analysis shows that the new martingale product\nestimators, having uniformly bounded variance in time, offer a competitive\nalternative to the traditional Green-Kubo estimator. We compare on illustrative\nnumerical tests the new estimators with results obtained by the Green-Kubo\nmethod.",
    "published_date": "2021-11-30T00:00:00",
    "year": 2021,
    "categories": [
      "math.NA",
      "cs.NA",
      "math-ph",
      "math.MP",
      "math.PR",
      "65C05, 65C20, 65C40, 60J27, 60J75"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.00126v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2112.00113v1",
    "title": "Beyond Flatland: Pre-training with a Strong 3D Inductive Bias",
    "authors": [
      "Shubhaankar Gupta",
      "Thomas P. O'Connell",
      "Bernhard Egger"
    ],
    "author_ids": [],
    "abstract": "Pre-training on large-scale databases consisting of natural images and then\nfine-tuning them to fit the application at hand, or transfer-learning, is a\npopular strategy in computer vision. However, Kataoka et al., 2020 introduced a\ntechnique to eliminate the need for natural images in supervised deep learning\nby proposing a novel synthetic, formula-based method to generate 2D fractals as\ntraining corpus. Using one synthetically generated fractal for each class, they\nachieved transfer learning results comparable to models pre-trained on natural\nimages. In this project, we take inspiration from their work and build on this\nidea -- using 3D procedural object renders. Since the image formation process\nin the natural world is based on its 3D structure, we expect pre-training with\n3D mesh renders to provide an implicit bias leading to better generalization\ncapabilities in a transfer learning setting and that invariances to 3D rotation\nand illumination are easier to be learned based on 3D data. Similar to the\nprevious work, our training corpus will be fully synthetic and derived from\nsimple procedural strategies; we will go beyond classic data augmentation and\nalso vary illumination and pose which are controllable in our setting and study\ntheir effect on transfer learning capabilities in context to prior work. In\naddition, we will compare the 2D fractal and 3D procedural object networks to\nhuman and non-human primate brain data to learn more about the 2D vs. 3D nature\nof biological vision.",
    "published_date": "2021-11-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.00113v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.00054v3",
    "title": "Task2Sim : Towards Effective Pre-training and Transfer from Synthetic Data",
    "authors": [
      "Samarth Mishra",
      "Rameswar Panda",
      "Cheng Perng Phoo",
      "Chun-Fu Chen",
      "Leonid Karlinsky",
      "Kate Saenko",
      "Venkatesh Saligrama",
      "Rogerio S. Feris"
    ],
    "author_ids": [],
    "abstract": "Pre-training models on Imagenet or other massive datasets of real images has\nled to major advances in computer vision, albeit accompanied with shortcomings\nrelated to curation cost, privacy, usage rights, and ethical issues. In this\npaper, for the first time, we study the transferability of pre-trained models\nbased on synthetic data generated by graphics simulators to downstream tasks\nfrom very different domains. In using such synthetic data for pre-training, we\nfind that downstream performance on different tasks are favored by different\nconfigurations of simulation parameters (e.g. lighting, object pose,\nbackgrounds, etc.), and that there is no one-size-fits-all solution. It is thus\nbetter to tailor synthetic pre-training data to a specific downstream task, for\nbest performance. We introduce Task2Sim, a unified model mapping downstream\ntask representations to optimal simulation parameters to generate synthetic\npre-training data for them. Task2Sim learns this mapping by training to find\nthe set of best parameters on a set of \"seen\" tasks. Once trained, it can then\nbe used to predict best simulation parameters for novel \"unseen\" tasks in one\nshot, without requiring additional training. Given a budget in number of images\nper class, our extensive experiments with 20 diverse downstream tasks show\nTask2Sim's task-adaptive pre-training data results in significantly better\ndownstream performance than non-adaptively choosing simulation parameters on\nboth seen and unseen tasks. It is even competitive with pre-training on real\nimages from Imagenet.",
    "published_date": "2021-11-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.00054v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.15603v1",
    "title": "Human Imperceptible Attacks and Applications to Improve Fairness",
    "authors": [
      "Xinru Hua",
      "Huanzhong Xu",
      "Jose Blanchet",
      "Viet Nguyen"
    ],
    "author_ids": [],
    "abstract": "Modern neural networks are able to perform at least as well as humans in\nnumerous tasks involving object classification and image generation. However,\nsmall perturbations which are imperceptible to humans may significantly degrade\nthe performance of well-trained deep neural networks. We provide a\nDistributionally Robust Optimization (DRO) framework which integrates\nhuman-based image quality assessment methods to design optimal attacks that are\nimperceptible to humans but significantly damaging to deep neural networks.\nThrough extensive experiments, we show that our attack algorithm generates\nbetter-quality (less perceptible to humans) attacks than other state-of-the-art\nhuman imperceptible attack methods. Moreover, we demonstrate that DRO training\nusing our optimally designed human imperceptible attacks can improve group\nfairness in image classification. Towards the end, we provide an algorithmic\nimplementation to speed up DRO training significantly, which could be of\nindependent interest.",
    "published_date": "2021-11-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.15603v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.15487v2",
    "title": "FROB: Few-shot ROBust Model for Classification and Out-of-Distribution Detection",
    "authors": [
      "Nikolaos Dionelis",
      "Mehrdad Yaghoobi",
      "Sotirios A. Tsaftaris"
    ],
    "author_ids": [],
    "abstract": "Nowadays, classification and Out-of-Distribution (OoD) detection in the\nfew-shot setting remain challenging aims due to rarity and the limited samples\nin the few-shot setting, and because of adversarial attacks. Accomplishing\nthese aims is important for critical systems in safety, security, and defence.\nIn parallel, OoD detection is challenging since deep neural network classifiers\nset high confidence to OoD samples away from the training data. To address such\nlimitations, we propose the Few-shot ROBust (FROB) model for classification and\nfew-shot OoD detection. We devise FROB for improved robustness and reliable\nconfidence prediction for few-shot OoD detection. We generate the support\nboundary of the normal class distribution and combine it with few-shot Outlier\nExposure (OE). We propose a self-supervised learning few-shot confidence\nboundary methodology based on generative and discriminative models. The\ncontribution of FROB is the combination of the generated boundary in a\nself-supervised learning manner and the imposition of low confidence at this\nlearned boundary. FROB implicitly generates strong adversarial samples on the\nboundary and forces samples from OoD, including our boundary, to be less\nconfident by the classifier. FROB achieves generalization to unseen OoD with\napplicability to unknown, in the wild, test sets that do not correlate to the\ntraining datasets. To improve robustness, FROB redesigns OE to work even for\nzero-shots. By including our boundary, FROB reduces the threshold linked to the\nmodel's few-shot robustness; it maintains the OoD performance approximately\nindependent of the number of few-shots. The few-shot robustness analysis\nevaluation of FROB on different sets and on One-Class Classification (OCC) data\nshows that FROB achieves competitive performance and outperforms benchmarks in\nterms of robustness to the outlier few-shot sample population and variability.",
    "published_date": "2021-11-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.15487v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.15454v3",
    "title": "Boosting Discriminative Visual Representation Learning with Scenario-Agnostic Mixup",
    "authors": [
      "Siyuan Li",
      "Zicheng Liu",
      "Zedong Wang",
      "Di Wu",
      "Zihan Liu",
      "Stan Z. Li"
    ],
    "author_ids": [],
    "abstract": "Mixup is a well-known data-dependent augmentation technique for DNNs,\nconsisting of two sub-tasks: mixup generation and classification. However, the\nrecent dominant online training method confines mixup to supervised learning\n(SL), and the objective of the generation sub-task is limited to selected\nsample pairs instead of the whole data manifold, which might cause trivial\nsolutions. To overcome such limitations, we comprehensively study the objective\nof mixup generation and propose \\textbf{S}cenario-\\textbf{A}gnostic\n\\textbf{Mix}up (SAMix) for both SL and Self-supervised Learning (SSL)\nscenarios. Specifically, we hypothesize and verify the objective function of\nmixup generation as optimizing local smoothness between two mixed classes\nsubject to global discrimination from other classes. Accordingly, we propose\n$\\eta$-balanced mixup loss for complementary learning of the two\nsub-objectives. Meanwhile, a label-free generation sub-network is designed,\nwhich effectively provides non-trivial mixup samples and improves transferable\nabilities. Moreover, to reduce the computational cost of online training, we\nfurther introduce a pre-trained version, SAMix$^\\mathcal{P}$, achieving more\nfavorable efficiency and generalizability. Extensive experiments on nine SL and\nSSL benchmarks demonstrate the consistent superiority and versatility of SAMix\ncompared with existing methods.",
    "published_date": "2021-11-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.15454v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.15452v1",
    "title": "On the Generalization of Agricultural Drought Classification from Climate Data",
    "authors": [
      "Julia Gottfriedsen",
      "Max Berrendorf",
      "Pierre Gentine",
      "Markus Reichstein",
      "Katja Weigel",
      "Birgit Hassler",
      "Veronika Eyring"
    ],
    "author_ids": [],
    "abstract": "Climate change is expected to increase the likelihood of drought events, with\nsevere implications for food security. Unlike other natural disasters, droughts\nhave a slow onset and depend on various external factors, making drought\ndetection in climate data difficult. In contrast to existing works that rely on\nsimple relative drought indices as ground-truth data, we build upon soil\nmoisture index (SMI) obtained from a hydrological model. This index is directly\nrelated to insufficiently available water to vegetation. Given ERA5-Land\nclimate input data of six months with land use information from MODIS satellite\nobservation, we compare different models with and without sequential inductive\nbias in classifying droughts based on SMI. We use PR-AUC as the evaluation\nmeasure to account for the class imbalance and obtain promising results despite\na challenging time-based split. We further show in an ablation study that the\nmodels retain their predictive capabilities given input data of coarser\nresolutions, as frequently encountered in climate models.",
    "published_date": "2021-11-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.15452v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.15430v4",
    "title": "The Devil is in the Margin: Margin-based Label Smoothing for Network Calibration",
    "authors": [
      "Bingyuan Liu",
      "Ismail Ben Ayed",
      "Adrian Galdran",
      "Jose Dolz"
    ],
    "author_ids": [],
    "abstract": "In spite of the dominant performances of deep neural networks, recent works\nhave shown that they are poorly calibrated, resulting in over-confident\npredictions. Miscalibration can be exacerbated by overfitting due to the\nminimization of the cross-entropy during training, as it promotes the predicted\nsoftmax probabilities to match the one-hot label assignments. This yields a\npre-softmax activation of the correct class that is significantly larger than\nthe remaining activations. Recent evidence from the literature suggests that\nloss functions that embed implicit or explicit maximization of the entropy of\npredictions yield state-of-the-art calibration performances. We provide a\nunifying constrained-optimization perspective of current state-of-the-art\ncalibration losses. Specifically, these losses could be viewed as\napproximations of a linear penalty (or a Lagrangian) imposing equality\nconstraints on logit distances. This points to an important limitation of such\nunderlying equality constraints, whose ensuing gradients constantly push\ntowards a non-informative solution, which might prevent from reaching the best\ncompromise between the discriminative performance and calibration of the model\nduring gradient-based optimization. Following our observations, we propose a\nsimple and flexible generalization based on inequality constraints, which\nimposes a controllable margin on logit distances. Comprehensive experiments on\na variety of image classification, semantic segmentation and NLP benchmarks\ndemonstrate that our method sets novel state-of-the-art results on these tasks\nin terms of network calibration, without affecting the discriminative\nperformance. The code is available at https://github.com/by-liu/MbLS .",
    "published_date": "2021-11-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.15430v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.15414v3",
    "title": "Neuron with Steady Response Leads to Better Generalization",
    "authors": [
      "Qiang Fu",
      "Lun Du",
      "Haitao Mao",
      "Xu Chen",
      "Wei Fang",
      "Shi Han",
      "Dongmei Zhang"
    ],
    "author_ids": [],
    "abstract": "Regularization can mitigate the generalization gap between training and\ninference by introducing inductive bias. Existing works have already proposed\nvarious inductive biases from diverse perspectives. However, none of them\nexplores inductive bias from the perspective of class-dependent response\ndistribution of individual neurons. In this paper, we conduct a substantial\nanalysis of the characteristics of such distribution. Based on the analysis\nresults, we articulate the Neuron Steadiness Hypothesis: the neuron with\nsimilar responses to instances of the same class leads to better\ngeneralization. Accordingly, we propose a new regularization method called\nNeuron Steadiness Regularization (NSR) to reduce neuron intra-class response\nvariance. Based on the Complexity Measure, we theoretically guarantee the\neffectiveness of NSR for improving generalization. We conduct extensive\nexperiments on Multilayer Perceptron, Convolutional Neural Networks, and Graph\nNeural Networks with popular benchmark datasets of diverse domains, which show\nthat our Neuron Steadiness Regularization consistently outperforms the vanilla\nversion of models with significant gain and low additional computational\noverhead.",
    "published_date": "2021-11-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.15414v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.15361v3",
    "title": "Seeking Salient Facial Regions for Cross-Database Micro-Expression Recognition",
    "authors": [
      "Xingxun Jiang",
      "Yuan Zong",
      "Wenming Zheng",
      "Jiateng Liu",
      "Mengting Wei"
    ],
    "author_ids": [],
    "abstract": "Cross-Database Micro-Expression Recognition (CDMER) aims to develop the\nMicro-Expression Recognition (MER) methods with strong domain adaptability,\ni.e., the ability to recognize the Micro-Expressions (MEs) of different\nsubjects captured by different imaging devices in different scenes. The\ndevelopment of CDMER is faced with two key problems: 1) the severe feature\ndistribution gap between the source and target databases; 2) the feature\nrepresentation bottleneck of ME such local and subtle facial expressions. To\nsolve these problems, this paper proposes a novel Transfer Group Sparse\nRegression method, namely TGSR, which aims to 1) optimize the measurement and\nbetter alleviate the difference between the source and target databases, and 2)\nhighlight the valid facial regions to enhance extracted features, by the\noperation of selecting the group features from the raw face feature, where each\nregion is associated with a group of raw face feature, i.e., the salient facial\nregion selection. Compared with previous transfer group sparse methods, our\nproposed TGSR has the ability to select the salient facial regions, which is\neffective in alleviating the aforementioned problems for better performance and\nreducing the computational cost at the same time. We use two public ME\ndatabases, i.e., CASME II and SMIC, to evaluate our proposed TGSR method.\nExperimental results show that our proposed TGSR learns the discriminative and\nexplicable regions, and outperforms most state-of-the-art\nsubspace-learning-based domain-adaptive methods for CDMER.",
    "published_date": "2021-11-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.15361v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.15293v1",
    "title": "The Impact of Considering Human Values during Requirements Engineering Activities",
    "authors": [
      "Harsha Perera",
      "Rashina Hoda",
      "Rifat Ara Shams",
      "Arif Nurwidyantoro",
      "Mojtaba Shahin",
      "Waqar Hussain",
      "Jon Whittle"
    ],
    "author_ids": [],
    "abstract": "Human values, or what people hold important in their life, such as freedom,\nfairness, and social responsibility, often remain unnoticed and unattended\nduring software development. Ignoring values can lead to values violations in\nsoftware that can result in financial losses, reputation damage, and widespread\nsocial and legal implications. However, embedding human values in software is\nnot only non-trivial but also generally an unclear process. Commencing as early\nas during the Requirements Engineering (RE) activities promises to ensure\nfit-for-purpose and quality software products that adhere to human values. But\nwhat is the impact of considering human values explicitly during early RE\nactivities? To answer this question, we conducted a scenario-based survey where\n56 software practitioners contextualised requirements analysis towards a\nproposed mobile application for the homeless and suggested values-laden\nsoftware features accordingly. The suggested features were qualitatively\nanalysed. Results show that explicit considerations of values can help\npractitioners identify applicable values, associate purpose with the features\nthey develop, think outside-the-box, and build connections between software\nfeatures and human values. Finally, drawing from the results and experiences of\nthis study, we propose a scenario-based values elicitation process -- a simple\nfour-step takeaway as a practical implication of this study.",
    "published_date": "2021-11-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.15293v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.15259v2",
    "title": "Privacy-Preserving Decentralized Exchange Marketplaces",
    "authors": [
      "Kavya Govindarajan",
      "Dhinakaran Vinayagamurthy",
      "Praveen Jayachandran",
      "Chester Rebeiro"
    ],
    "author_ids": [],
    "abstract": "Decentralized exchange markets leveraging blockchain have been proposed\nrecently to provide open and equal access to traders, improve transparency and\nreduce systemic risk of centralized exchanges. However, they compromise on the\nprivacy of traders with respect to their asset ownership, account balance,\norder details and their identity. In this paper, we present Rialto, a fully\ndecentralized privacy-preserving exchange marketplace with support for matching\ntrade orders, on-chain settlement and market price discovery. Rialto provides\nconfidentiality of order rates and account balances and unlinkability between\ntraders and their trade orders, while retaining the desirable properties of a\ntraditional marketplace like front-running resilience and market fairness. We\ndefine formal security notions and present a security analysis of the\nmarketplace. We perform a detailed evaluation of our solution, demonstrate that\nit scales well and is suitable for a large class of goods and financial\ninstruments traded in modern exchange markets.",
    "published_date": "2021-11-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CR",
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.15259v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.15060v1",
    "title": "Second-order Approximation of Minimum Discrimination Information in Independent Component Analysis",
    "authors": [
      "YunPeng Li"
    ],
    "author_ids": [],
    "abstract": "Independent Component Analysis (ICA) is intended to recover the mutually\nindependent sources from their linear mixtures, and F astICA is one of the most\nsuccessful ICA algorithms. Although it seems reasonable to improve the\nperformance of F astICA by introducing more nonlinear functions to the\nnegentropy estimation, the original fixed-point method (approximate Newton\nmethod) in F astICA degenerates under this circumstance. To alleviate this\nproblem, we propose a novel method based on the second-order approximation of\nminimum discrimination information (MDI). The joint maximization in our method\nis consisted of minimizing single weighted least squares and seeking unmixing\nmatrix by the fixed-point method. Experimental results validate its efficiency\ncompared with other popular ICA algorithms.",
    "published_date": "2021-11-30T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG",
      "eess.SP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.15060v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.01274v1",
    "title": "The Impact of Data Distribution on Fairness and Robustness in Federated Learning",
    "authors": [
      "Mustafa Safa Ozdayi",
      "Murat Kantarcioglu"
    ],
    "author_ids": [],
    "abstract": "Federated Learning (FL) is a distributed machine learning protocol that\nallows a set of agents to collaboratively train a model without sharing their\ndatasets. This makes FL particularly suitable for settings where data privacy\nis desired. However, it has been observed that the performance of FL is closely\nrelated to the similarity of the local data distributions of agents.\nParticularly, as the data distributions of agents differ, the accuracy of the\ntrained models drop. In this work, we look at how variations in local data\ndistributions affect the fairness and the robustness properties of the trained\nmodels in addition to the accuracy. Our experimental results indicate that, the\ntrained models exhibit higher bias, and become more susceptible to attacks as\nlocal data distributions differ. Importantly, the degradation in the fairness,\nand robustness can be much more severe than the accuracy. Therefore, we reveal\nthat small variations that have little impact on the accuracy could still be\nimportant if the trained model is to be deployed in a fairness/security\ncritical context.",
    "published_date": "2021-11-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.01274v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.14943v1",
    "title": "Morph Detection Enhanced by Structured Group Sparsity",
    "authors": [
      "Poorya Aghdaie",
      "Baaria Chaudhary",
      "Sobhan Soleymani",
      "Jeremy Dawson",
      "Nasser M. Nasrabadi"
    ],
    "author_ids": [],
    "abstract": "In this paper, we consider the challenge of face morphing attacks, which\nsubstantially undermine the integrity of face recognition systems such as those\nadopted for use in border protection agencies. Morph detection can be\nformulated as extracting fine-grained representations, where local\ndiscriminative features are harnessed for learning a hypothesis. To acquire\ndiscriminative features at different granularity as well as a decoupled\nspectral information, we leverage wavelet domain analysis to gain insight into\nthe spatial-frequency content of a morphed face. As such, instead of using\nimages in the RGB domain, we decompose every image into its wavelet sub-bands\nusing 2D wavelet decomposition and a deep supervised feature selection scheme\nis employed to find the most discriminative wavelet sub-bands of input images.\nTo this end, we train a Deep Neural Network (DNN) morph detector using the\ndecomposed wavelet sub-bands of the morphed and bona fide images. In the\ntraining phase, our structured group sparsity-constrained DNN picks the most\ndiscriminative wavelet sub-bands out of all the sub-bands, with which we\nretrain our DNN, resulting in a precise detection of morphed images when\ninference is achieved on a probe image. The efficacy of our deep morph detector\nwhich is enhanced by structured group lasso is validated through experiments on\nthree facial morph image databases, i.e., VISAPP17, LMA, and MorGAN.",
    "published_date": "2021-11-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.14943v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.14887v2",
    "title": "DAFormer: Improving Network Architectures and Training Strategies for Domain-Adaptive Semantic Segmentation",
    "authors": [
      "Lukas Hoyer",
      "Dengxin Dai",
      "Luc Van Gool"
    ],
    "author_ids": [],
    "abstract": "As acquiring pixel-wise annotations of real-world images for semantic\nsegmentation is a costly process, a model can instead be trained with more\naccessible synthetic data and adapted to real images without requiring their\nannotations. This process is studied in unsupervised domain adaptation (UDA).\nEven though a large number of methods propose new adaptation strategies, they\nare mostly based on outdated network architectures. As the influence of recent\nnetwork architectures has not been systematically studied, we first benchmark\ndifferent network architectures for UDA and newly reveal the potential of\nTransformers for UDA semantic segmentation. Based on the findings, we propose a\nnovel UDA method, DAFormer. The network architecture of DAFormer consists of a\nTransformer encoder and a multi-level context-aware feature fusion decoder. It\nis enabled by three simple but crucial training strategies to stabilize the\ntraining and to avoid overfitting to the source domain: While (1) Rare Class\nSampling on the source domain improves the quality of the pseudo-labels by\nmitigating the confirmation bias of self-training toward common classes, (2) a\nThing-Class ImageNet Feature Distance and (3) a learning rate warmup promote\nfeature transfer from ImageNet pretraining. DAFormer represents a major advance\nin UDA. It improves the state of the art by 10.8 mIoU for GTA-to-Cityscapes and\n5.4 mIoU for Synthia-to-Cityscapes and enables learning even difficult classes\nsuch as train, bus, and truck well. The implementation is available at\nhttps://github.com/lhoyer/DAFormer.",
    "published_date": "2021-11-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.14887v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.14690v3",
    "title": "DanceTrack: Multi-Object Tracking in Uniform Appearance and Diverse Motion",
    "authors": [
      "Peize Sun",
      "Jinkun Cao",
      "Yi Jiang",
      "Zehuan Yuan",
      "Song Bai",
      "Kris Kitani",
      "Ping Luo"
    ],
    "author_ids": [],
    "abstract": "A typical pipeline for multi-object tracking (MOT) is to use a detector for\nobject localization, and following re-identification (re-ID) for object\nassociation. This pipeline is partially motivated by recent progress in both\nobject detection and re-ID, and partially motivated by biases in existing\ntracking datasets, where most objects tend to have distinguishing appearance\nand re-ID models are sufficient for establishing associations. In response to\nsuch bias, we would like to re-emphasize that methods for multi-object tracking\nshould also work when object appearance is not sufficiently discriminative. To\nthis end, we propose a large-scale dataset for multi-human tracking, where\nhumans have similar appearance, diverse motion and extreme articulation. As the\ndataset contains mostly group dancing videos, we name it \"DanceTrack\". We\nexpect DanceTrack to provide a better platform to develop more MOT algorithms\nthat rely less on visual discrimination and depend more on motion analysis. We\nbenchmark several state-of-the-art trackers on our dataset and observe a\nsignificant performance drop on DanceTrack when compared against existing\nbenchmarks. The dataset, project code and competition server are released at:\n\\url{https://github.com/DanceTrack}.",
    "published_date": "2021-11-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.14690v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.14581v2",
    "title": "Learning Fair Classifiers with Partially Annotated Group Labels",
    "authors": [
      "Sangwon Jung",
      "Sanghyuk Chun",
      "Taesup Moon"
    ],
    "author_ids": [],
    "abstract": "Recently, fairness-aware learning have become increasingly crucial, but most\nof those methods operate by assuming the availability of fully annotated\ndemographic group labels. We emphasize that such assumption is unrealistic for\nreal-world applications since group label annotations are expensive and can\nconflict with privacy issues. In this paper, we consider a more practical\nscenario, dubbed as Algorithmic Group Fairness with the Partially annotated\nGroup labels (Fair-PG). We observe that the existing methods to achieve group\nfairness perform even worse than the vanilla training, which simply uses full\ndata only with target labels, under Fair-PG. To address this problem, we\npropose a simple Confidence-based Group Label assignment (CGL) strategy that is\nreadily applicable to any fairness-aware learning method. CGL utilizes an\nauxiliary group classifier to assign pseudo group labels, where random labels\nare assigned to low confident samples. We first theoretically show that our\nmethod design is better than the vanilla pseudo-labeling strategy in terms of\nfairness criteria. Then, we empirically show on several benchmark datasets that\nby combining CGL and the state-of-the-art fairness-aware in-processing methods,\nthe target accuracies and the fairness metrics can be jointly improved compared\nto the baselines. Furthermore, we convincingly show that CGL enables to\nnaturally augment the given group-labeled dataset with external target\nlabel-only datasets so that both accuracy and fairness can be improved. Code is\navailable at https://github.com/naver-ai/cgl_fairness.",
    "published_date": "2021-11-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CV",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.14581v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.01281v1",
    "title": "Expose Uncertainty, Instill Distrust, Avoid Explanations: Towards Ethical Guidelines for AI",
    "authors": [
      "Claudio S. Pinhanez"
    ],
    "author_ids": [],
    "abstract": "In this position paper, I argue that the best way to help and protect humans\nusing AI technology is to make them aware of the intrinsic limitations and\nproblems of AI algorithms. To accomplish this, I suggest three ethical\nguidelines to be used in the presentation of results, mandating AI systems to\nexpose uncertainty, to instill distrust, and, contrary to traditional views, to\navoid explanations. The paper does a preliminary discussion of the guidelines\nand provides some arguments for their adoption, aiming to start a debate in the\ncommunity about AI ethics in practice.",
    "published_date": "2021-11-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.01281v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.01282v1",
    "title": "Achieving a Data-driven Risk Assessment Methodology for Ethical AI",
    "authors": [
      "Anna Felländer",
      "Jonathan Rebane",
      "Stefan Larsson",
      "Mattias Wiggberg",
      "Fredrik Heintz"
    ],
    "author_ids": [],
    "abstract": "The AI landscape demands a broad set of legal, ethical, and societal\nconsiderations to be accounted for in order to develop ethical AI (eAI)\nsolutions which sustain human values and rights. Currently, a variety of\nguidelines and a handful of niche tools exist to account for and tackle\nindividual challenges. However, it is also well established that many\norganizations face practical challenges in navigating these considerations from\na risk management perspective. Therefore, new methodologies are needed to\nprovide a well-vetted and real-world applicable structure and path through the\nchecks and balances needed for ethically assessing and guiding the development\nof AI. In this paper we show that a multidisciplinary research approach,\nspanning cross-sectional viewpoints, is the foundation of a pragmatic\ndefinition of ethical and societal risks faced by organizations using AI.\nEqually important is the findings of cross-structural governance for\nimplementing eAI successfully. Based on evidence acquired from our\nmultidisciplinary research investigation, we propose a novel data-driven risk\nassessment methodology, entitled DRESS-eAI. In addition, through the evaluation\nof our methodological implementation, we demonstrate its state-of-the-art\nrelevance as a tool for sustaining human values in the data-driven AI era.",
    "published_date": "2021-11-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.01282v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.14467v2",
    "title": "What Drives Readership? An Online Study on User Interface Types and Popularity Bias Mitigation in News Article Recommendations",
    "authors": [
      "Emanuel Lacic",
      "Leon Fadljevic",
      "Franz Weissenboeck",
      "Stefanie Lindstaedt",
      "Dominik Kowald"
    ],
    "author_ids": [],
    "abstract": "Personalized news recommender systems support readers in finding the right\nand relevant articles in online news platforms. In this paper, we discuss the\nintroduction of personalized, content-based news recommendations on DiePresse,\na popular Austrian online news platform, focusing on two specific aspects: (i)\nuser interface type, and (ii) popularity bias mitigation. Therefore, we\nconducted a two-weeks online study that started in October 2020, in which we\nanalyzed the impact of recommendations on two user groups, i.e., anonymous and\nsubscribed users, and three user interface types, i.e., on a desktop, mobile\nand tablet device. With respect to user interface types, we find that the\nprobability of a recommendation to be seen is the highest for desktop devices,\nwhile the probability of interacting with recommendations is the highest for\nmobile devices. With respect to popularity bias mitigation, we find that\npersonalized, content-based news recommendations can lead to a more balanced\ndistribution of news articles' readership popularity in the case of anonymous\nusers. Apart from that, we find that significant events (e.g., the COVID-19\nlockdown announcement in Austria and the Vienna terror attack) influence the\ngeneral consumption behavior of popular articles for both, anonymous and\nsubscribed users.",
    "published_date": "2021-11-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.14467v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.14348v1",
    "title": "A Causal Approach for Unfair Edge Prioritization and Discrimination Removal",
    "authors": [
      "Pavan Ravishankar",
      "Pranshu Malviya",
      "Balaraman Ravindran"
    ],
    "author_ids": [],
    "abstract": "In budget-constrained settings aimed at mitigating unfairness, like law\nenforcement, it is essential to prioritize the sources of unfairness before\ntaking measures to mitigate them in the real world. Unlike previous works,\nwhich only serve as a caution against possible discrimination and de-bias data\nafter data generation, this work provides a toolkit to mitigate unfairness\nduring data generation, given by the Unfair Edge Prioritization algorithm, in\naddition to de-biasing data after generation, given by the Discrimination\nRemoval algorithm. We assume that a non-parametric Markovian causal model\nrepresentative of the data generation procedure is given. The edges emanating\nfrom the sensitive nodes in the causal graph, such as race, are assumed to be\nthe sources of unfairness. We first quantify Edge Flow in any edge X -> Y,\nwhich is the belief of observing a specific value of Y due to the influence of\na specific value of X along X -> Y. We then quantify Edge Unfairness by\nformulating a non-parametric model in terms of edge flows. We then prove that\ncumulative unfairness towards sensitive groups in a decision, like race in a\nbail decision, is non-existent when edge unfairness is absent. We prove this\nresult for the non-trivial non-parametric model setting when the cumulative\nunfairness cannot be expressed in terms of edge unfairness. We then measure the\nPotential to mitigate the Cumulative Unfairness when edge unfairness is\ndecreased. Based on these measurements, we propose the Unfair Edge\nPrioritization algorithm that can then be used by policymakers. We also propose\nthe Discrimination Removal Procedure that de-biases a data distribution by\neliminating optimization constraints that grow exponentially in the number of\nsensitive attributes and values taken by them. Extensive experiments validate\nthe theorem and specifications used for quantifying the above measures.",
    "published_date": "2021-11-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.14348v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.14346v1",
    "title": "Pessimistic Model Selection for Offline Deep Reinforcement Learning",
    "authors": [
      "Chao-Han Huck Yang",
      "Zhengling Qi",
      "Yifan Cui",
      "Pin-Yu Chen"
    ],
    "author_ids": [],
    "abstract": "Deep Reinforcement Learning (DRL) has demonstrated great potentials in\nsolving sequential decision making problems in many applications. Despite its\npromising performance, practical gaps exist when deploying DRL in real-world\nscenarios. One main barrier is the over-fitting issue that leads to poor\ngeneralizability of the policy learned by DRL. In particular, for offline DRL\nwith observational data, model selection is a challenging task as there is no\nground truth available for performance demonstration, in contrast with the\nonline setting with simulated environments. In this work, we propose a\npessimistic model selection (PMS) approach for offline DRL with a theoretical\nguarantee, which features a provably effective framework for finding the best\npolicy among a set of candidate models. Two refined approaches are also\nproposed to address the potential bias of DRL model in identifying the optimal\npolicy. Numerical studies demonstrated the superior performance of our approach\nover existing methods.",
    "published_date": "2021-11-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE",
      "cs.NE",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.14346v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.14339v1",
    "title": "Heterogeneous Visible-Thermal and Visible-Infrared Face Recognition using Unit-Class Loss and Cross-Modality Discriminator",
    "authors": [
      "Usman Cheema",
      "Mobeen Ahmad",
      "Dongil Han",
      "Seungbin Moon"
    ],
    "author_ids": [],
    "abstract": "Visible-to-thermal face image matching is a challenging variate of\ncross-modality recognition. The challenge lies in the large modality gap and\nlow correlation between visible and thermal modalities. Existing approaches\nemploy image preprocessing, feature extraction, or common subspace projection,\nwhich are independent problems in themselves. In this paper, we propose an\nend-to-end framework for cross-modal face recognition. The proposed algorithm\naims to learn identity-discriminative features from unprocessed facial images\nand identify cross-modal image pairs. A novel Unit-Class Loss is proposed for\npreserving identity information while discarding modality information. In\naddition, a Cross-Modality Discriminator block is proposed for integrating\nimage-pair classification capability into the network. The proposed network can\nbe used to extract modality-independent vector representations or a\nmatching-pair classification for test images. Our cross-modality face\nrecognition experiments on five independent databases demonstrate that the\nproposed method achieves marked improvement over existing state-of-the-art\nmethods.",
    "published_date": "2021-11-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.NE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.14339v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.14264v1",
    "title": "Convergence Analysis For Non Linear System Of Parabolic Variational Inequalities",
    "authors": [
      "Yahya Alnashri"
    ],
    "author_ids": [],
    "abstract": "This work aims to provide a comprehensive and unified numerical analysis for\nnon linear system of parabolic variational inequalities (PVIs) subject to\nDirichlet boundary condition. This analysis enables us to establish an\nexistence of the exact solution to the considered model and to prove the\nconvergence for the approximate solution and its approximate gradient. Our\nresults are applicable for several conforming and non conforming numerical\nschemes.",
    "published_date": "2021-11-28T00:00:00",
    "year": 2021,
    "categories": [
      "math.AP",
      "cs.NA",
      "math.NA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.14264v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.14062v1",
    "title": "P4AI: Approaching AI Ethics through Principlism",
    "authors": [
      "Andre Fu",
      "Elisa Ding",
      "Mahdi S. Hosseini",
      "Konstantinos N. Plataniotis"
    ],
    "author_ids": [],
    "abstract": "The field of computer vision is rapidly evolving, particularly in the context\nof new methods of neural architecture design. These models contribute to (1)\nthe Climate Crisis - increased CO2 emissions and (2) the Privacy Crisis - data\nleakage concerns. To address the often overlooked impact the Computer Vision\n(CV) community has on these crises, we outline a novel ethical framework,\n\\textit{P4AI}: Principlism for AI, an augmented principlistic view of ethical\ndilemmas within AI. We then suggest using P4AI to make concrete recommendations\nto the community to mitigate the climate and privacy crises.",
    "published_date": "2021-11-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.14062v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.14020v2",
    "title": "Local Edge Dynamics and Opinion Polarization",
    "authors": [
      "Nikita Bhalla",
      "Adam Lechowicz",
      "Cameron Musco"
    ],
    "author_ids": [],
    "abstract": "The proliferation of social media platforms, recommender systems, and their\njoint societal impacts have prompted significant interest in opinion formation\nand evolution within social networks. We study how local edge dynamics can\ndrive opinion polarization. In particular, we introduce a variant of the\nclassic Friedkin-Johnsen opinion dynamics, augmented with a simple\ntime-evolving network model. Edges are iteratively added or deleted according\nto simple rules, modeling decisions based on individual preferences and network\nrecommendations.\n  Via simulations on synthetic and real-world graphs, we find that the combined\npresence of two dynamics gives rise to high polarization: 1) confirmation bias\n-- i.e., the preference for nodes to connect to other nodes with similar\nexpressed opinions and 2) friend-of-friend link recommendations, which\nencourage new connections between closely connected nodes. We show that our\nmodel is tractable to theoretical analysis, which helps explain how these local\ndynamics erode connectivity across opinion groups, affecting polarization and a\nrelated measure of disagreement across edges. Finally, we validate our model\nagainst real-world data, showing that our edge dynamics drive the structure of\narbitrary graphs, including random graphs, to more closely resemble real social\nnetworks.",
    "published_date": "2021-11-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.14020v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.13576v1",
    "title": "Job Recommender Systems: A Review",
    "authors": [
      "Corné de Ruijt",
      "Sandjai Bhulai"
    ],
    "author_ids": [],
    "abstract": "This paper provides a review of the job recommender system (JRS) literature\npublished in the past decade (2011-2021). Compared to previous literature\nreviews, we put more emphasis on contributions that incorporate the temporal\nand reciprocal nature of job recommendations. Previous studies on JRS suggest\nthat taking such views into account in the design of the JRS can lead to\nimproved model performance. Also, it may lead to a more uniform distribution of\ncandidates over a set of similar jobs. We also consider the literature from the\nperspective of algorithm fairness. Here we find that this is rarely discussed\nin the literature, and if it is discussed, many authors wrongly assume that\nremoving the discriminatory feature would be sufficient. With respect to the\ntype of models used in JRS, authors frequently label their method as `hybrid'.\nUnfortunately, they thereby obscure what these methods entail. Using existing\nrecommender taxonomies, we split this large class of hybrids into subcategories\nthat are easier to analyse. We further find that data availability, and in\nparticular the availability of click data, has a large impact on the choice of\nmethod and validation. Last, although the generalizability of JRS across\ndifferent datasets is infrequently considered, results suggest that error\nscores may vary across these datasets.",
    "published_date": "2021-11-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.13576v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.13550v1",
    "title": "Using Fictitious Class Representations to Boost Discriminative Zero-Shot Learners",
    "authors": [
      "Mohammed Dabbah",
      "Ran El-yaniv"
    ],
    "author_ids": [],
    "abstract": "Focusing on discriminative zero-shot learning, in this work we introduce a\nnovel mechanism that dynamically augments during training the set of seen\nclasses to produce additional fictitious classes. These fictitious classes\ndiminish the model's tendency to fixate during training on attribute\ncorrelations that appear in the training set but will not appear in newly\nexposed classes. The proposed model is tested within the two formulations of\nthe zero-shot learning framework; namely, generalized zero-shot learning (GZSL)\nand classical zero-shot learning (CZSL). Our model improves the\nstate-of-the-art performance on the CUB dataset and reaches comparable results\non the other common datasets, AWA2 and SUN. We investigate the strengths and\nweaknesses of our method, including the effects of catastrophic forgetting when\ntraining an end-to-end zero-shot model.",
    "published_date": "2021-11-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.13550v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.13266v1",
    "title": "Examining Needs and Opportunities for Supporting Students Who Experience Discrimination",
    "authors": [
      "Yasaman S. Sefidgar",
      "Paula S. Nurius",
      "Amanda Baughan",
      "Lisa A. Elkin",
      "Anind K. Dey",
      "Eve Riskin",
      "Jennifer Mankoff",
      "Margaret E. Morris"
    ],
    "author_ids": [],
    "abstract": "Perceived discrimination is common and consequential. Yet, little support is\navailable to ease handling of these experiences. Addressing this gap, we report\non a need-finding study to guide us in identifying relevant technologies and\ntheir requirements. Specifically, we examined unfolding experiences of\nperceived discrimination among college students and found factors to address in\nproviding meaningful support. We used semi-structured retrospective interviews\nwith 14 students to understand their perceptions, emotions, and coping in\nresponse to discriminatory behaviors within the prior ten-week period. These 14\nstudents were among 90 who provided experience sampling reports of unfair\ntreatment over the same ten-week period. We found that discrimination is more\ndistressing if students face related academic and social struggles or when the\nincident triggers beliefs of inefficacy. We additionally identified patterns of\neffective coping. By grounding the findings in an extended stress processing\nframework, we offer a principled approach to intervention design, which we\nillustrate through incident-specific and proactive intervention paradigms.",
    "published_date": "2021-11-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC",
      "J.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.13266v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.13259v1",
    "title": "Identification of Bias Against People with Disabilities in Sentiment Analysis and Toxicity Detection Models",
    "authors": [
      "Pranav Narayanan Venkit",
      "Shomir Wilson"
    ],
    "author_ids": [],
    "abstract": "Sociodemographic biases are a common problem for natural language processing,\naffecting the fairness and integrity of its applications. Within sentiment\nanalysis, these biases may undermine sentiment predictions for texts that\nmention personal attributes that unbiased human readers would consider neutral.\nSuch discrimination can have great consequences in the applications of\nsentiment analysis both in the public and private sectors. For example,\nincorrect inferences in applications like online abuse and opinion analysis in\nsocial media platforms can lead to unwanted ramifications, such as wrongful\ncensoring, towards certain populations. In this paper, we address the\ndiscrimination against people with disabilities, PWD, done by sentiment\nanalysis and toxicity classification models. We provide an examination of\nsentiment and toxicity analysis models to understand in detail how they\ndiscriminate PWD. We present the Bias Identification Test in Sentiments (BITS),\na corpus of 1,126 sentences designed to probe sentiment analysis models for\nbiases in disability. We use this corpus to demonstrate statistically\nsignificant biases in four widely used sentiment analysis tools (TextBlob,\nVADER, Google Cloud Natural Language API and DistilBERT) and two toxicity\nanalysis models trained to predict toxic comments on Jigsaw challenges (Toxic\ncomment classification and Unintended Bias in Toxic comments). The results show\nthat all exhibit strong negative biases on sentences that mention disability.\nWe publicly release BITS Corpus for others to identify potential biases against\ndisability in any sentiment analysis tools and also to update the corpus to be\nused as a test for other sociodemographic variables as well.",
    "published_date": "2021-11-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.13259v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.13108v2",
    "title": "Combating Unknown Bias with Effective Bias-Conflicting Scoring and Gradient Alignment",
    "authors": [
      "Bowen Zhao",
      "Chen Chen",
      "Qian-Wei Wang",
      "Anfeng He",
      "Shu-Tao Xia"
    ],
    "author_ids": [],
    "abstract": "Models notoriously suffer from dataset biases which are detrimental to\nrobustness and generalization. The identify-emphasize paradigm shows a\npromising effect in dealing with unknown biases. However, we find that it is\nstill plagued by two challenges: A, the quality of the identified\nbias-conflicting samples is far from satisfactory; B, the emphasizing\nstrategies just yield suboptimal performance. In this work, for challenge A, we\npropose an effective bias-conflicting scoring method to boost the\nidentification accuracy with two practical strategies -- peer-picking and\nepoch-ensemble. For challenge B, we point out that the gradient contribution\nstatistics can be a reliable indicator to inspect whether the optimization is\ndominated by bias-aligned samples. Then, we propose gradient alignment, which\nemploys gradient statistics to balance the contributions of the mined\nbias-aligned and bias-conflicting samples dynamically throughout the learning\nprocess, forcing models to leverage intrinsic features to make fair decisions.\nExperiments are conducted on multiple datasets in various settings,\ndemonstrating that the proposed solution can alleviate the impact of unknown\nbiases and achieve state-of-the-art performance.",
    "published_date": "2021-11-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.13108v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.13050v1",
    "title": "The vehicle routing problem with drones and drone speed selection",
    "authors": [
      "Felix Tamke",
      "Udo Buscher"
    ],
    "author_ids": [],
    "abstract": "Joint parcel delivery by trucks and drones has enjoyed significant attention\nfor some time, as the advantages of one delivery method offset the\ndisadvantages of the other. This paper focuses on the vehicle routing problem\nwith drones and drone speed selection (VRPD-DSS), which considers\nspeed-dependent energy consumption and drone-charging in detail. For this\npurpose, we formulate a comprehensive mixed-integer problem that aims to\nminimize the operational costs consisting of fuel consumption costs of the\ntrucks, labor costs for the drivers, and energy costs of the drones. The speed\nat which a drone performs a flight must be selected from a discrete set. We\nintroduce preprocessing steps to eliminate dominated speeds for a flight to\nreduce the problem size and use valid inequalities to accelerate the solution\nprocess. The consideration of speed-dependent energy consumption leads to the\nfact that it is advisable to perform different flights at different speeds and\nnot to consistently operate a drone at maximum speed. Instead, drone speed\nshould be selected to balance drone range and speed of delivery. Our extensive\ncomputational study of a rural real-world setting shows that, by modeling\nenergy consumption realistically, the savings in operational costs compared to\ntruck-only delivery are significant but smaller than those identified in\npreviously published work. Our analysis further reveals that the greatest\nsavings stem from the fact that overall delivery time decreases compared to\ntruck-only delivery, allowing costly truck-driver time to be reduced. The\nadditional energy costs of the drone, however, are largely negligible.",
    "published_date": "2021-11-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DM",
      "math.OC",
      "90B06 (Primary) 90C11 (Secondary)"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.13050v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.13041v1",
    "title": "Non-Asimov Explanations Regulating AI through Transparency",
    "authors": [
      "Chris Reed",
      "Keri Grieman",
      "Joseph Early"
    ],
    "author_ids": [],
    "abstract": "An important part of law and regulation is demanding explanations for actual\nand potential failures. We ask questions like: What happened (or might happen)\nto cause this failure? And why did (or might) it happen? These are disguised\nnormative questions - they really ask what ought to have happened, and how the\nhumans involved ought to have behaved. To answer the normative questions, law\nand regulation seeks a narrative explanation, a story. At present, we seek\nthese kinds of narrative explanation from AI technology, because as humans we\nseek to understand technology's working through constructing a story to explain\nit. Our cultural history makes this inevitable - authors like Asimov, writing\nnarratives about future AI technologies like intelligent robots, have told us\nthat they act in ways explainable by the narrative logic which we use to\nexplain human actions and so they can also be explained to us in those terms.\nThis is, at least currently, not true. This work argues that we can only solve\nthis problem by working from both sides. Technologists will need to find ways\nto tell us stories which law and regulation can use. But law and regulation\nwill also need to accept different kinds of narratives, which tell stories\nabout fundamental legal and regulatory concepts like fairness and\nreasonableness that are different from those we are used to.",
    "published_date": "2021-11-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "K.5.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.13041v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.01239v1",
    "title": "Improving Teacher-Student Interactions in Online Educational Forums using a Markov Chain based Stackelberg Game Model",
    "authors": [
      "Rohith Dwarakanath Vallam",
      "Priyanka Bhatt",
      "Debmalya Mandal",
      "Y Narahari"
    ],
    "author_ids": [],
    "abstract": "With the rapid proliferation of the Internet, the area of education has\nundergone a massive transformation in terms of how students and instructors\ninteract in a classroom. Online learning now takes more than one form,\nincluding the use of technology to enhance a face-to-face class, a hybrid class\nthat combines both face-to-face meetings and online work, and fully online\ncourses. Further, online classrooms are usually composed of an online education\nforum (OEF) where students and instructor discuss open-ended questions for\ngaining better understanding of the subject. However, empirical studies have\nrepeatedly shown that the dropout rates in these online courses are very high\npartly due to the lack of motivation among the enrolled students. We undertake\nan empirical comparison of student behavior in OEFs associated with a\ngraduate-level course during two terms. We identify key parameters dictating\nthe dynamics of OEFs like effective incentive design, student heterogeneity,\nand super-posters phenomenon. Motivated by empirical observations, we propose\nan analytical model based on continuous time Markov chains (CTMCs) to capture\ninstructor-student interactions in an OEF. Using concepts from lumpability of\nCTMCs, we compute steady state and transient probabilities along with expected\nnet-rewards for the instructor and the students. We formulate a mixed-integer\nlinear program which views an OEF as a single-leader-multiple-followers\nStackelberg game. Through simulations, we observe that students exhibit varied\ndegree of non-monotonicity in their participation (with increasing instructor\ninvolvement). We also study the effect of instructor bias and budget on the\nstudent participation levels. Our model exhibits the empirically observed\nsuper-poster phenomenon under certain parameter configurations and recommends\nan optimal plan to the instructor for maximizing student participation in OEFs.",
    "published_date": "2021-11-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.GT",
      "91A80 (Primary) 91-10, 60J28 (Secondary)",
      "J.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.01239v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.12939v2",
    "title": "Probabilistic Impact Score Generation using Ktrain-BERT to Identify Hate Words from Twitter Discussions",
    "authors": [
      "Sourav Das",
      "Prasanta Mandal",
      "Sanjay Chatterji"
    ],
    "author_ids": [],
    "abstract": "Social media has seen a worrying rise in hate speech in recent times.\nBranching to several distinct categories of cyberbullying, gender\ndiscrimination, or racism, the combined label for such derogatory content can\nbe classified as toxic content in general. This paper presents experimentation\nwith a Keras wrapped lightweight BERT model to successfully identify hate\nspeech and predict probabilistic impact score for the same to extract the\nhateful words within sentences. The dataset used for this task is the Hate\nSpeech and Offensive Content Detection (HASOC 2021) data from FIRE 2021 in\nEnglish. Our system obtained a validation accuracy of 82.60%, with a maximum\nF1-Score of 82.68%. Subsequently, our predictive cases performed significantly\nwell in generating impact scores for successful identification of the hate\ntweets as well as the hateful words from tweet pools.",
    "published_date": "2021-11-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "15-04",
      "I.2.7; I.2.6"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.12939v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.12929v2",
    "title": "Unbiased Pairwise Learning to Rank in Recommender Systems",
    "authors": [
      "Yi Ren",
      "Hongyan Tang",
      "Siwen Zhu"
    ],
    "author_ids": [],
    "abstract": "Nowadays, recommender systems already impact almost every facet of peoples\nlives. To provide personalized high quality recommendation results,\nconventional systems usually train pointwise rankers to predict the absolute\nvalue of objectives and leverage a distinct shallow tower to estimate and\nalleviate the impact of position bias. However, with such a training paradigm,\nthe optimization target differs a lot from the ranking metrics valuing the\nrelative order of top ranked items rather than the prediction precision of each\nitem. Moreover, as the existing system tends to recommend more relevant items\nat higher positions, it is difficult for the shallow tower based methods to\nprecisely attribute the user feedback to the impact of position or relevance.\nTherefore, there exists an exciting opportunity for us to get enhanced\nperformance if we manage to solve the aforementioned issues. Unbiased learning\nto rank algorithms, which are verified to model the relative relevance\naccurately based on noisy feedback, are appealing candidates and have already\nbeen applied in many applications with single categorical labels, such as user\nclick signals. Nevertheless, the existing unbiased LTR methods cannot properly\nhandle multiple feedback incorporating both categorical and continuous labels.\nAccordingly, we design a novel unbiased LTR algorithm to tackle the challenges,\nwhich innovatively models position bias in the pairwise fashion and introduces\nthe pairwise trust bias to separate the position bias, trust bias, and user\nrelevance explicitly. Experiment results on public benchmark datasets and\ninternal live traffic show the superior results of the proposed method for both\ncategorical and continuous labels.",
    "published_date": "2021-11-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.12929v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.12925v1",
    "title": "ContourletNet: A Generalized Rain Removal Architecture Using Multi-Direction Hierarchical Representation",
    "authors": [
      "Wei-Ting Chen",
      "Cheng-Che Tsai",
      "Hao-Yu Fang",
      "I-Hsiang Chen",
      "Jian-Jiun Ding",
      "Sy-Yen Kuo"
    ],
    "author_ids": [],
    "abstract": "Images acquired from rainy scenes usually suffer from bad visibility which\nmay damage the performance of computer vision applications. The rainy scenarios\ncan be categorized into two classes: moderate rain and heavy rain scenes.\nModerate rain scene mainly consists of rain streaks while heavy rain scene\ncontains both rain streaks and the veiling effect (similar to haze). Although\nexisting methods have achieved excellent performance on these two cases\nindividually, it still lacks a general architecture to address both heavy rain\nand moderate rain scenarios effectively. In this paper, we construct a\nhierarchical multi-direction representation network by using the contourlet\ntransform (CT) to address both moderate rain and heavy rain scenarios. The CT\ndivides the image into the multi-direction subbands (MS) and the semantic\nsubband (SS). First, the rain streak information is retrieved to the MS based\non the multi-orientation property of the CT. Second, a hierarchical\narchitecture is proposed to reconstruct the background information including\ndamaged semantic information and the veiling effect in the SS. Last, the\nmulti-level subband discriminator with the feedback error map is proposed. By\nthis module, all subbands can be well optimized. This is the first architecture\nthat can address both of the two scenarios effectively. The code is available\nin https://github.com/cctakaet/ContourletNet-BMVC2021.",
    "published_date": "2021-11-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV",
      "eess.SP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.12925v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.12823v2",
    "title": "Fairness for AUC via Feature Augmentation",
    "authors": [
      "Hortense Fong",
      "Vineet Kumar",
      "Anay Mehrotra",
      "Nisheeth K. Vishnoi"
    ],
    "author_ids": [],
    "abstract": "We study fairness in the context of classification where the performance is\nmeasured by the area under the curve (AUC) of the receiver operating\ncharacteristic. AUC is commonly used to measure the performance of prediction\nmodels. The same classifier can have significantly varying AUCs for different\nprotected groups and, in real-world applications, it is often desirable to\nreduce such cross-group differences. We address the problem of how to acquire\nadditional features to most greatly improve AUC for the disadvantaged group. We\ndevelop a novel approach, fairAUC, based on feature augmentation (adding\nfeatures) to mitigate bias between identifiable groups. The approach requires\nonly a few summary statistics to offer provable guarantees on AUC improvement,\nand allows managers flexibility in determining where in the fairness-accuracy\ntradeoff they would like to be. We evaluate fairAUC on synthetic and real-world\ndatasets and find that it significantly improves AUC for the disadvantaged\ngroup relative to benchmarks maximizing overall AUC and minimizing bias between\ngroups.",
    "published_date": "2021-11-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.12823v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.12481v1",
    "title": "It Is Different When Items Are Older: Debiasing Recommendations When Selection Bias and User Preferences Are Dynamic",
    "authors": [
      "Jin Huang",
      "Harrie Oosterhuis",
      "Maarten de Rijke"
    ],
    "author_ids": [],
    "abstract": "User interactions with recommender systems (RSs) are affected by user\nselection bias, e.g., users are more likely to rate popular items (popularity\nbias) or items that they expect to enjoy beforehand (positivity bias). Methods\nexist for mitigating the effects of selection bias in user ratings on the\nevaluation and optimization of RSs. However, these methods treat selection bias\nas static, despite the fact that the popularity of an item may change\ndrastically over time and the fact that user preferences may also change over\ntime. We focus on the age of an item and its effect on selection bias and user\npreferences. Our experimental analysis reveals that the rating behavior of\nusers on the MovieLens dataset is better captured by methods that consider\neffects from the age of item on bias and preferences. We theoretically show\nthat in a dynamic scenario in which both the selection bias and user\npreferences are dynamic, existing debiasing methods are no longer unbiased. To\naddress this limitation, we introduce DebiAsing in the dyNamiC scEnaRio\n(DANCER), a novel debiasing method that extends the inverse propensity scoring\ndebiasing method to account for dynamic selection bias and user preferences.\nOur experimental results indicate that DANCER improves rating prediction\nperformance compared to debiasing methods that incorrectly assume that\nselection bias is static in a dynamic scenario. To the best of our knowledge,\nDANCER is the first debiasing method that accounts for dynamic selection bias\nand user preferences in RSs.",
    "published_date": "2021-11-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.12481v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.12358v2",
    "title": "SPCL: A New Framework for Domain Adaptive Semantic Segmentation via Semantic Prototype-based Contrastive Learning",
    "authors": [
      "Binhui Xie",
      "Mingjia Li",
      "Shuang Li"
    ],
    "author_ids": [],
    "abstract": "Although there is significant progress in supervised semantic segmentation,\nit remains challenging to deploy the segmentation models to unseen domains due\nto domain biases. Domain adaptation can help in this regard by transferring\nknowledge from a labeled source domain to an unlabeled target domain. Previous\nmethods typically attempt to perform the adaptation on global features,\nhowever, the local semantic affiliations accounting for each pixel in the\nfeature space are often ignored, resulting in less discriminability. To solve\nthis issue, we propose a novel semantic prototype-based contrastive learning\nframework for fine-grained class alignment. Specifically, the semantic\nprototypes provide supervisory signals for per-pixel discriminative\nrepresentation learning and each pixel of source and target domains in the\nfeature space is required to reflect the content of the corresponding semantic\nprototype. In this way, our framework is able to explicitly make intra-class\npixel representations closer and inter-class pixel representations further\napart to improve the robustness of the segmentation model as well as alleviate\nthe domain shift problem. Our method is easy to implement and attains superior\nresults compared to state-of-the-art approaches, as is demonstrated with a\nnumber of experiments. The code is publicly available at\nhttps://github.com/BinhuiXie/SPCL.",
    "published_date": "2021-11-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.12358v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.12331v1",
    "title": "An MAP Estimation for Between-Class Variance",
    "authors": [
      "Jiao Han",
      "Yunqi Cai",
      "Lantian Li",
      "Guanyu Li",
      "Dong Wang"
    ],
    "author_ids": [],
    "abstract": "Probabilistic linear discriminant analysis (PLDA) has been widely used in\nopen-set verification tasks, such as speaker verification. A potential issue of\nthis model is that the training set often contains limited number of classes,\nwhich makes the estimation for the between-class variance unreliable. This\nunreliable estimation often leads to degraded generalization. In this paper, we\npresent an MAP estimation for the between-class variance, by employing an\nInverse-Wishart prior. A key problem is that with hierarchical models such as\nPLDA, the prior is placed on the variance of class means while the likelihood\nis based on class members, which makes the posterior inference intractable. We\nderive a simple MAP estimation for such a model, and test it in both PLDA\nscoring and length normalization. In both cases, the MAP-based estimation\ndelivers interesting performance improvement.",
    "published_date": "2021-11-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SD",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.12331v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.12115v1",
    "title": "Algorithmic Fairness in Face Morphing Attack Detection",
    "authors": [
      "Raghavendra Ramachandra",
      "Kiran Raja",
      "Christoph Busch"
    ],
    "author_ids": [],
    "abstract": "Face morphing attacks can compromise Face Recognition System (FRS) by\nexploiting their vulnerability. Face Morphing Attack Detection (MAD) techniques\nhave been developed in recent past to deter such attacks and mitigate risks\nfrom morphing attacks. MAD algorithms, as any other algorithms should treat the\nimages of subjects from different ethnic origins in an equal manner and provide\nnon-discriminatory results. While the promising MAD algorithms are tested for\nrobustness, there is no study comprehensively bench-marking their behaviour\nagainst various ethnicities. In this paper, we study and present a\ncomprehensive analysis of algorithmic fairness of the existing Single\nimage-based Morph Attack Detection (S-MAD) algorithms. We attempt to better\nunderstand the influence of ethnic bias on MAD algorithms and to this extent,\nwe study the performance of MAD algorithms on a newly created dataset\nconsisting of four different ethnic groups. With Extensive experiments using\nsix different S-MAD techniques, we first present benchmark of detection\nperformance and then measure the quantitative value of the algorithmic fairness\nfor each of them using Fairness Discrepancy Rate (FDR). The results indicate\nthe lack of fairness on all six different S-MAD methods when trained and tested\non different ethnic groups suggesting the need for reliable MAD approaches to\nmitigate the algorithmic bias.",
    "published_date": "2021-11-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.12115v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.13760v1",
    "title": "Interpreting Machine Learning Models for Room Temperature Prediction in Non-domestic Buildings",
    "authors": [
      "Jianqiao Mao",
      "Grammenos Ryan"
    ],
    "author_ids": [],
    "abstract": "An ensuing challenge in Artificial Intelligence (AI) is the perceived\ndifficulty in interpreting sophisticated machine learning models, whose\never-increasing complexity makes it hard for such models to be understood,\ntrusted and thus accepted by human beings. The lack, if not complete absence,\nof interpretability for these so-called black-box models can lead to serious\neconomic and ethical consequences, thereby hindering the development and\ndeployment of AI in wider fields, particularly in those involving critical and\nregulatory applications. Yet, the building services industry is a\nhighly-regulated domain requiring transparency and decision-making processes\nthat can be understood and trusted by humans. To this end, the design and\nimplementation of autonomous Heating, Ventilation and Air Conditioning systems\nfor the automatic but concurrently interpretable optimisation of energy\nefficiency and room thermal comfort is of topical interest. This work therefore\npresents an interpretable machine learning model aimed at predicting room\ntemperature in non-domestic buildings, for the purpose of optimising the use of\nthe installed HVAC system. We demonstrate experimentally that the proposed\nmodel can accurately forecast room temperatures eight hours ahead in real-time\nby taking into account historical RT information, as well as additional\nenvironmental and time-series features. In this paper, an enhanced feature\nengineering process is conducted based on the Exploratory Data Analysis\nresults. Furthermore, beyond the commonly used Interpretable Machine Learning\ntechniques, we propose a Permutation Feature-based Frequency Response Analysis\n(PF-FRA) method for quantifying the contributions of the different predictors\nin the frequency domain. Based on the generated reason codes, we find that the\nhistorical RT feature is the dominant factor that has most impact on the model\nprediction.",
    "published_date": "2021-11-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2, H.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.13760v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.11712v1",
    "title": "The Ethics of Biosurveillance",
    "authors": [
      "S. K. Devitt",
      "P. W. J. Baxter",
      "G. Hamilton"
    ],
    "author_ids": [],
    "abstract": "Governments must keep agricultural systems free of pests that threaten\nagricultural production and international trade. Biosecurity surveillance\nalready makes use of a wide range of technologies, such as insect traps and\nlures, geographic information systems, and diagnostic biochemical tests. The\nrise of cheap and usable surveillance technologies such as remotely piloted\naircraft systems (RPAS) presents value conflicts not addressed in international\nbiosurveillance guidelines. The costs of keeping agriculture pest-free include\nprivacy violations and reduced autonomy for farmers. We argue that physical and\ndigital privacy in the age of ubiquitous aerial and ground surveillance is a\nnatural right to allow people to function freely on their land. Surveillance\nmethods must be co-created and justified through using ethically defensible\nprocesses such as discourse theory, value-centred design and responsible\ninnovation to forge a cooperative social contract between diverse stakeholders.\nWe propose an ethical framework for biosurveillance activities that balances\nthe collective benefits for food security with individual privacy: (1)\nestablish the boundaries of a biosurveillance social contract; (2) justify\nsurveillance operations for the farmers, researchers, industry, the public and\nregulators; (3) give decision makers a reasonable measure of control over their\npersonal and agricultural data; and (4) choose surveillance methodologies that\ngive the appropriate information. The benefits of incorporating an ethical\nframework for responsible biosurveillance innovation include increased\nparticipation and accumulated trust over time. Long term trust and cooperation\nwill support food security, producing higher quality data overall and\nmitigating against anticipated information gaps that may emerge due to\ndisrespecting landholder rights",
    "published_date": "2021-11-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "K.4.0; K.4.1; K.4.2; K.4.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.11712v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2112.01252v2",
    "title": "Australia's Approach to AI Governance in Security and Defence",
    "authors": [
      "Susannah Kate Devitt",
      "Damian Copeland"
    ],
    "author_ids": [],
    "abstract": "Australia is a leading AI nation with strong allies and partnerships.\nAustralia has prioritised the development of robotics, AI, and autonomous\nsystems to develop sovereign capability for the military. Australia commits to\nArticle 36 reviews of all new means and methods of warfare to ensure weapons\nand weapons systems are operated within acceptable systems of control.\nAdditionally, Australia has undergone significant reviews of the risks of AI to\nhuman rights and within intelligence organisations and has committed to\nproducing ethics guidelines and frameworks in Security and Defence. Australia\nis committed to OECD's values-based principles for the responsible stewardship\nof trustworthy AI as well as adopting a set of National AI ethics principles.\nWhile Australia has not adopted an AI governance framework specifically for the\nAustralian Defence Organisation (ADO); Defence Science and Technology Group\n(DSTG) has published 'A Method for Ethical AI in Defence' (MEAID) technical\nreport which includes a framework and pragmatic tools for managing ethical and\nlegal risks for military applications of AI. Australia can play a leadership\nrole by integrating legal and ethical considerations into its ADO AI capability\nacquisition process. This requires a policy framework that defines its legal\nand ethical requirements, is informed by Defence industry stakeholders, and\nprovides a practical methodology to integrate legal and ethical risk mitigation\nstrategies into the acquisition process.",
    "published_date": "2021-11-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "K.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.01252v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.11692v1",
    "title": "Status-quo policy gradient in Multi-Agent Reinforcement Learning",
    "authors": [
      "Pinkesh Badjatiya",
      "Mausoom Sarkar",
      "Nikaash Puri",
      "Jayakumar Subramanian",
      "Abhishek Sinha",
      "Siddharth Singh",
      "Balaji Krishnamurthy"
    ],
    "author_ids": [],
    "abstract": "Individual rationality, which involves maximizing expected individual\nreturns, does not always lead to high-utility individual or group outcomes in\nmulti-agent problems. For instance, in multi-agent social dilemmas,\nReinforcement Learning (RL) agents trained to maximize individual rewards\nconverge to a low-utility mutually harmful equilibrium. In contrast, humans\nevolve useful strategies in such social dilemmas. Inspired by ideas from human\npsychology that attribute this behavior to the status-quo bias, we present a\nstatus-quo loss (SQLoss) and the corresponding policy gradient algorithm that\nincorporates this bias in an RL agent. We demonstrate that agents trained with\nSQLoss learn high-utility policies in several social dilemma matrix games\n(Prisoner's Dilemma, Stag Hunt matrix variant, Chicken Game). We show how\nSQLoss outperforms existing state-of-the-art methods to obtain high-utility\npolicies in visual input non-matrix games (Coin Game and Stag Hunt visual input\nvariant) using pre-trained cooperation and defection oracles. Finally, we show\nthat SQLoss extends to a 4-agent setting by demonstrating the emergence of\ncooperative behavior in the popular Braess' paradox.",
    "published_date": "2021-11-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.11692v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.11665v2",
    "title": "RadFusion: Benchmarking Performance and Fairness for Multimodal Pulmonary Embolism Detection from CT and EHR",
    "authors": [
      "Yuyin Zhou",
      "Shih-Cheng Huang",
      "Jason Alan Fries",
      "Alaa Youssef",
      "Timothy J. Amrhein",
      "Marcello Chang",
      "Imon Banerjee",
      "Daniel Rubin",
      "Lei Xing",
      "Nigam Shah",
      "Matthew P. Lungren"
    ],
    "author_ids": [],
    "abstract": "Despite the routine use of electronic health record (EHR) data by\nradiologists to contextualize clinical history and inform image interpretation,\nthe majority of deep learning architectures for medical imaging are unimodal,\ni.e., they only learn features from pixel-level information. Recent research\nrevealing how race can be recovered from pixel data alone highlights the\npotential for serious biases in models which fail to account for demographics\nand other key patient attributes. Yet the lack of imaging datasets which\ncapture clinical context, inclusive of demographics and longitudinal medical\nhistory, has left multimodal medical imaging underexplored. To better assess\nthese challenges, we present RadFusion, a multimodal, benchmark dataset of 1794\npatients with corresponding EHR data and high-resolution computed tomography\n(CT) scans labeled for pulmonary embolism. We evaluate several representative\nmultimodal fusion models and benchmark their fairness properties across\nprotected subgroups, e.g., gender, race/ethnicity, age. Our results suggest\nthat integrating imaging and EHR data can improve classification performance\nand robustness without introducing large disparities in the true positive rate\nbetween population groups.",
    "published_date": "2021-11-23T00:00:00",
    "year": 2021,
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.11665v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.11656v2",
    "title": "Few-Shot Object Detection via Association and DIscrimination",
    "authors": [
      "Yuhang Cao",
      "Jiaqi Wang",
      "Ying Jin",
      "Tong Wu",
      "Kai Chen",
      "Ziwei Liu",
      "Dahua Lin"
    ],
    "author_ids": [],
    "abstract": "Object detection has achieved substantial progress in the last decade.\nHowever, detecting novel classes with only few samples remains challenging,\nsince deep learning under low data regime usually leads to a degraded feature\nspace. Existing works employ a holistic fine-tuning paradigm to tackle this\nproblem, where the model is first pre-trained on all base classes with abundant\nsamples, and then it is used to carve the novel class feature space.\nNonetheless, this paradigm is still imperfect. Durning fine-tuning, a novel\nclass may implicitly leverage the knowledge of multiple base classes to\nconstruct its feature space, which induces a scattered feature space, hence\nviolating the inter-class separability. To overcome these obstacles, we propose\na two-step fine-tuning framework, Few-shot object detection via Association and\nDIscrimination (FADI), which builds up a discriminative feature space for each\nnovel class with two integral steps. 1) In the association step, in contrast to\nimplicitly leveraging multiple base classes, we construct a compact novel class\nfeature space via explicitly imitating a specific base class feature space.\nSpecifically, we associate each novel class with a base class according to\ntheir semantic similarity. After that, the feature space of a novel class can\nreadily imitate the well-trained feature space of the associated base class. 2)\nIn the discrimination step, to ensure the separability between the novel\nclasses and associated base classes, we disentangle the classification branches\nfor base and novel classes. To further enlarge the inter-class separability\nbetween all classes, a set-specialized margin loss is imposed. Extensive\nexperiments on Pascal VOC and MS-COCO datasets demonstrate FADI achieves new\nSOTA performance, significantly improving the baseline in any shot/split by\n+18.7. Notably, the advantage is most announced on extremely few-shot\nscenarios.",
    "published_date": "2021-11-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.11656v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.11542v1",
    "title": "Depth Without the Magic: Inductive Bias of Natural Gradient Descent",
    "authors": [
      "Anna Kerekes",
      "Anna Mészáros",
      "Ferenc Huszár"
    ],
    "author_ids": [],
    "abstract": "In gradient descent, changing how we parametrize the model can lead to\ndrastically different optimization trajectories, giving rise to a surprising\nrange of meaningful inductive biases: identifying sparse classifiers or\nreconstructing low-rank matrices without explicit regularization. This implicit\nregularization has been hypothesised to be a contributing factor to good\ngeneralization in deep learning. However, natural gradient descent is\napproximately invariant to reparameterization, it always follows the same\ntrajectory and finds the same optimum. The question naturally arises: What\nhappens if we eliminate the role of parameterization, which solution will be\nfound, what new properties occur? We characterize the behaviour of natural\ngradient flow in deep linear networks for separable classification under\nlogistic loss and deep matrix factorization. Some of our findings extend to\nnonlinear neural networks with sufficient but finite over-parametrization. We\ndemonstrate that there exist learning problems where natural gradient descent\nfails to generalize, while gradient descent with the right architecture\nperforms well.",
    "published_date": "2021-11-22T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.11542v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.11528v1",
    "title": "On Fair Division with Binary Valuations Respecting Social Networks",
    "authors": [
      "Neeldhara Misra",
      "Debanuj Nayak"
    ],
    "author_ids": [],
    "abstract": "We study the computational complexity of finding fair allocations of\nindivisible goods in the setting where a social network on the agents is given.\nNotions of fairness in this context are \"localized\", that is, agents are only\nconcerned about the bundles allocated to their neighbors, rather than every\nother agent in the system. We comprehensively address the computational\ncomplexity of finding locally envy-free and Pareto efficient allocations in the\nsetting where the agents have binary valuations for the goods and the\nunderlying social network is modeled by an undirected graph. We study the\nproblem in the framework of parameterized complexity.\n  We show that the problem is computationally intractable even in fairly\nrestricted scenarios, for instance, even when the underlying graph is a path.\nWe show NP-hardness for settings where the graph has only two distinct\nvaluations among the agents. We demonstrate W-hardness with respect to the\nnumber of goods or the size of the vertex cover of the underlying graph. We\nalso consider notions of proportionality that respect the structure of the\nunderlying graph and show that two natural versions of this notion have\ndifferent complexities: allocating according to the notion that accounts for\nlocality to the greatest degree turns out to be computationally intractable,\nwhile for other notions, the allocation problem can be modeled as a structured\nILP which can be solved efficiently.",
    "published_date": "2021-11-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT",
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.11528v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.11523v1",
    "title": "Learnable Structural Semantic Readout for Graph Classification",
    "authors": [
      "Dongha Lee",
      "Su Kim",
      "Seonghyeon Lee",
      "Chanyoung Park",
      "Hwanjo Yu"
    ],
    "author_ids": [],
    "abstract": "With the great success of deep learning in various domains, graph neural\nnetworks (GNNs) also become a dominant approach to graph classification. By the\nhelp of a global readout operation that simply aggregates all node (or\nnode-cluster) representations, existing GNN classifiers obtain a graph-level\nrepresentation of an input graph and predict its class label using the\nrepresentation. However, such global aggregation does not consider the\nstructural information of each node, which results in information loss on the\nglobal structure. Particularly, it limits the discrimination power by enforcing\nthe same weight parameters of the classifier for all the node representations;\nin practice, each of them contributes to target classes differently depending\non its structural semantic. In this work, we propose structural semantic\nreadout (SSRead) to summarize the node representations at the position-level,\nwhich allows to model the position-specific weight parameters for\nclassification as well as to effectively capture the graph semantic relevant to\nthe global structure. Given an input graph, SSRead aims to identify\nstructurally-meaningful positions by using the semantic alignment between its\nnodes and structural prototypes, which encode the prototypical features of each\nposition. The structural prototypes are optimized to minimize the alignment\ncost for all training graphs, while the other GNN parameters are trained to\npredict the class labels. Our experimental results demonstrate that SSRead\nsignificantly improves the classification performance and interpretability of\nGNN classifiers while being compatible with a variety of aggregation functions,\nGNN architectures, and learning frameworks.",
    "published_date": "2021-11-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.11523v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.12682v2",
    "title": "CQS: A Formally-Verified Framework for Fair and Abortable Synchronization",
    "authors": [
      "Nikita Koval",
      "Dmitry Khalanskiy",
      "Dan Alistarh"
    ],
    "author_ids": [],
    "abstract": "Writing concurrent code that is both correct and efficient is notoriously\ndifficult. Thus, programmers often prefer to use synchronization abstractions,\nwhich render code simpler and easier to reason about. Despite a wealth of work\non this topic, there is still a gap between the rich semantics provided by\nsynchronization abstractions in modern programming languages -- specifically,\n\\emph{fair} FIFO ordering of synchronization requests and support for\n\\emph{abortable} operations -- and frameworks for implementing it correctly and\nefficiently. Supporting such semantics is critical given the rising popularity\nof constructs for asynchronous programming, such as coroutines, which abort\nfrequently and are cheaper to suspend and resume compared to native threads.\n  This paper introduces a new framework called\n\\texttt{CancellableQueueSynchronizer} (CQS), which enables simple yet efficient\nimplementations of a wide range of fair and abortable synchronization\nprimitives: mutexes, semaphores, barriers, count-down latches, and blocking\npools. Our main contribution is algorithmic, as implementing both fairness and\nabortability efficiently at this level of generality is non-trivial.\nImportantly, all our algorithms, including the CQS framework and the primitives\nbuilt on top of it, come with \\emph{formal proofs} in the Iris framework for\nCoq for many of their properties. These proofs are modular, so it is easy to\nshow correctness for new primitives implemented on top of CQS. From a practical\nperspective, implementation of CQS for native threads on the JVM significantly\nimproves Java's \\texttt{AbstractQueuedSynchronizer}, the only practical\nabstraction offering similar semantics. In sum,\n\\texttt{CancellableQueueSynchronizer} is the first framework to combine\nexpressiveness with formal guarantees and solid practical performance.",
    "published_date": "2021-11-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.PL",
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.12682v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.11859v1",
    "title": "Longitudinal Speech Biomarkers for Automated Alzheimer's Detection",
    "authors": [
      "Jordi Laguarta Soler",
      "Brian Subirana"
    ],
    "author_ids": [],
    "abstract": "We introduce a novel audio processing architecture, the Open Voice Brain\nModel (OVBM), improving detection accuracy for Alzheimer's (AD) longitudinal\ndiscrimination from spontaneous speech. We also outline the OVBM design\nmethodology leading us to such architecture, which in general can incorporate\nmultimodal biomarkers and target simultaneously several diseases and other AI\ntasks. Key in our methodology is the use of multiple biomarkers complementing\neach other, and when two of them uniquely identify different subjects in a\ntarget disease we say they are orthogonal. We illustrate the methodology by\nintroducing 16 biomarkers, three of which are orthogonal, demonstrating\nsimultaneous above state-of-the-art discrimination for apparently unrelated\ndiseases such as AD and COVID-19. Inspired by research conducted at the MIT\nCenter for Brain Minds and Machines, OVBM combines biomarker implementations of\nthe four modules of intelligence: The brain OS chunks and overlaps audio\nsamples and aggregates biomarker features from the sensory stream and cognitive\ncore creating a multi-modal graph neural network of symbolic compositional\nmodels for the target task. We apply it to AD, achieving above state-of-the-art\naccuracy of 93.8% on raw audio, while extracting a subject saliency map that\nlongitudinally tracks relative disease progression using multiple biomarkers,\n16 in the reported AD task. The ultimate aim is to help medical practice by\ndetecting onset and treatment impact so that intervention options can be\nlongitudinally tested. Using the OBVM design methodology, we introduce a novel\nlung and respiratory tract biomarker created using 200,000+ cough samples to\npre-train a model discriminating cough cultural origin. This cough dataset sets\na new benchmark as the largest audio health dataset with 30,000+ subjects\nparticipating in April 2020, demonstrating for the first-time cough cultural\nbias.",
    "published_date": "2021-11-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SD",
      "eess.AS",
      "q-bio.QM",
      "I.2.0; I.2.m"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.11859v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.11316v1",
    "title": "Testing thresholds for high-dimensional sparse random geometric graphs",
    "authors": [
      "Siqi Liu",
      "Sidhanth Mohanty",
      "Tselil Schramm",
      "Elizabeth Yang"
    ],
    "author_ids": [],
    "abstract": "In the random geometric graph model $\\mathsf{Geo}_d(n,p)$, we identify each\nof our $n$ vertices with an independently and uniformly sampled vector from the\n$d$-dimensional unit sphere, and we connect pairs of vertices whose vectors are\n``sufficiently close'', such that the marginal probability of an edge is $p$.\n  We investigate the problem of testing for this latent geometry, or in other\nwords, distinguishing an Erd\\H{o}s-R\\'enyi graph $\\mathsf{G}(n, p)$ from a\nrandom geometric graph $\\mathsf{Geo}_d(n, p)$. It is not too difficult to show\nthat if $d\\to \\infty$ while $n$ is held fixed, the two distributions become\nindistinguishable; we wish to understand how fast $d$ must grow as a function\nof $n$ for indistinguishability to occur.\n  When $p = \\frac{\\alpha}{n}$ for constant $\\alpha$, we prove that if $d \\ge\n\\mathrm{polylog} n$, the total variation distance between the two distributions\nis close to $0$; this improves upon the best previous bound of Brennan,\nBresler, and Nagaraj (2020), which required $d \\gg n^{3/2}$, and further our\nresult is nearly tight, resolving a conjecture of Bubeck, Ding, Eldan, \\&\nR\\'{a}cz (2016) up to logarithmic factors. We also obtain improved upper bounds\non the statistical indistinguishability thresholds in $d$ for the full range of\n$p$ satisfying $\\frac{1}{n}\\le p\\le \\frac{1}{2}$, improving upon the previous\nbounds by polynomial factors.\n  Our analysis uses the Belief Propagation algorithm to characterize the\ndistributions of (subsets of) the random vectors {\\em conditioned on producing\na particular graph}. In this sense, our analysis is connected to the ``cavity\nmethod'' from statistical physics. To analyze this process, we rely on novel\nsharp estimates for the area of the intersection of a random sphere cap with an\narbitrary subset of the sphere, which we prove using optimal transport maps and\nentropy-transport inequalities on the unit sphere.",
    "published_date": "2021-11-22T00:00:00",
    "year": 2021,
    "categories": [
      "math.PR",
      "cs.DM",
      "math.CO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.11316v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.11159v1",
    "title": "Investigating Cross-Linguistic Gender Bias in Hindi-English Across Domains",
    "authors": [
      "Somya Khosla"
    ],
    "author_ids": [],
    "abstract": "Measuring, evaluating and reducing Gender Bias has come to the forefront with\nnewer and improved language embeddings being released every few months. But\ncould this bias vary from domain to domain? We see a lot of work to study these\nbiases in various embedding models but limited work has been done to debias\nIndic languages. We aim to measure and study this bias in Hindi language, which\nis a higher-order language (gendered) with reference to English, a lower-order\nlanguage. To achieve this, we study the variations across domains to quantify\nif domain embeddings allow us some insight into Gender bias for this pair of\nHindi-English model. We will generate embeddings in four different corpora and\ncompare results by implementing different metrics like with pre-trained State\nof the Art Indic-English translation model, which has performed better at many\nNLP tasks than existing models.",
    "published_date": "2021-11-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "I.2.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.11159v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.10983v3",
    "title": "A Semi-Supervised Adaptive Discriminative Discretization Method Improving Discrimination Power of Regularized Naive Bayes",
    "authors": [
      "Shihe Wang",
      "Jianfeng Ren",
      "Ruibin Bai"
    ],
    "author_ids": [],
    "abstract": "Recently, many improved naive Bayes methods have been developed with enhanced\ndiscrimination capabilities. Among them, regularized naive Bayes (RNB) produces\nexcellent performance by balancing the discrimination power and generalization\ncapability. Data discretization is important in naive Bayes. By grouping\nsimilar values into one interval, the data distribution could be better\nestimated. However, existing methods including RNB often discretize the data\ninto too few intervals, which may result in a significant information loss. To\naddress this problem, we propose a semi-supervised adaptive discriminative\ndiscretization framework for naive Bayes, which could better estimate the data\ndistribution by utilizing both labeled data and unlabeled data through\npseudo-labeling techniques. The proposed method also significantly reduces the\ninformation loss during discretization by utilizing an adaptive discriminative\ndiscretization scheme, and hence greatly improves the discrimination power of\nclassifiers. The proposed RNB+, i.e., regularized naive Bayes utilizing the\nproposed discretization framework, is systematically evaluated on a wide range\nof machine-learning datasets. It significantly and consistently outperforms\nstate-of-the-art NB classifiers.",
    "published_date": "2021-11-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.10983v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.10934v2",
    "title": "Privacy-preserving Federated Adversarial Domain Adaption over Feature Groups for Interpretability",
    "authors": [
      "Yan Kang",
      "Yang Liu",
      "Yuezhou Wu",
      "Guoqiang Ma",
      "Qiang Yang"
    ],
    "author_ids": [],
    "abstract": "We present a novel privacy-preserving federated adversarial domain adaptation\napproach ($\\textbf{PrADA}$) to address an under-studied but practical\ncross-silo federated domain adaptation problem, in which the party of the\ntarget domain is insufficient in both samples and features. We address the\nlack-of-feature issue by extending the feature space through vertical federated\nlearning with a feature-rich party and tackle the sample-scarce issue by\nperforming adversarial domain adaptation from the sample-rich source party to\nthe target party. In this work, we focus on financial applications where\ninterpretability is critical. However, existing adversarial domain adaptation\nmethods typically apply a single feature extractor to learn feature\nrepresentations that are low-interpretable with respect to the target task. To\nimprove interpretability, we exploit domain expertise to split the feature\nspace into multiple groups that each holds relevant features, and we learn a\nsemantically meaningful high-order feature from each feature group. In\naddition, we apply a feature extractor (along with a domain discriminator) for\neach feature group to enable a fine-grained domain adaptation. We design a\nsecure protocol that enables performing the PrADA in a secure and efficient\nmanner. We evaluate our approach on two tabular datasets. Experiments\ndemonstrate both the effectiveness and practicality of our approach.",
    "published_date": "2021-11-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.10934v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.10897v1",
    "title": "Health Monitoring of Industrial machines using Scene-Aware Threshold Selection",
    "authors": [
      "Arshdeep Singh",
      "Raju Arvind",
      "Padmanabhan Rajan"
    ],
    "author_ids": [],
    "abstract": "This paper presents an autoencoder based unsupervised approach to identify\nanomaly in an industrial machine using sounds produced by the machine. The\nproposed framework is trained using log-melspectrogram representations of the\nsound signal. In classification, our hypothesis is that the reconstruction\nerror computed for an abnormal machine is larger than that of the a normal\nmachine, since only normal machine sounds are being used to train the\nautoencoder. A threshold is chosen to discriminate between normal and abnormal\nmachines. However, the threshold changes as surrounding conditions vary. To\nselect an appropriate threshold irrespective of the surrounding, we propose a\nscene classification framework, which can classify the underlying surrounding.\nHence, the threshold can be selected adaptively irrespective of the\nsurrounding. The experiment evaluation is performed on MIMII dataset for\nindustrial machines namely fan, pump, valve and slide rail. Our experiment\nanalysis shows that utilizing adaptive threshold, the performance improves\nsignificantly as that obtained using the fixed threshold computed for a given\nsurrounding only.",
    "published_date": "2021-11-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS",
      "eess.SP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.10897v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.10885v1",
    "title": "On Fairness and Stability in Two-Sided Matchings",
    "authors": [
      "Gili Karni",
      "Guy N. Rothblum",
      "Gal Yona"
    ],
    "author_ids": [],
    "abstract": "There are growing concerns that algorithms, which increasingly make or\ninfluence important decisions pertaining to individuals, might produce outcomes\nthat discriminate against protected groups. We study such fairness concerns in\nthe context of a two-sided market, where there are two sets of agents, and each\nagent has preferences over the other set. The goal is producing a matching\nbetween the sets. This setting has been the focus of a rich body of work. The\nseminal work of Gale and Shapley formulated a stability desideratum, and showed\nthat a stable matching always exists and can be found efficiently. We study\nthis question through the lens of metric-based fairness notions (Dwork et al.,\nKim et al.). We formulate appropriate definitions of fairness and stability in\nthe presence of a similarity metric, and ask: does a fair and stable matching\nalways exist? Can such a matching be found in polynomial time? Our\ncontributions are as follows: (1) Composition failures for classical\nalgorithms: We show that composing the Gale-Shapley algorithm with fair\nhospital preferences can produce blatantly unfair outcomes. (2) New algorithms\nfor finding fair and stable matchings: Our main technical contributions are\nefficient new algorithms for finding fair and stable matchings when: (i) the\nhospitals' preferences are fair, and (ii) the fairness metric satisfies a\nstrong \"proto-metric\" condition: the distance between every two doctors is\neither zero or one. In particular, these algorithms also show that, in this\nsetting, fairness and stability are compatible. (3) Barriers for finding fair\nand stable matchings in the general case: We show that if the hospital\npreferences can be unfair, or if the metric fails to satisfy the proto-metric\ncondition, then no algorithm in a natural class can find a fair and stable\nmatching. The natural class includes the classical Gale-Shapley algorithms and\nour new algorithms.",
    "published_date": "2021-11-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.DS",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.10885v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.10745v1",
    "title": "COVID Induced Digital Inequality for Senior Citizens",
    "authors": [
      "Nicky Qiu"
    ],
    "author_ids": [],
    "abstract": "The global pandemic of COVID-19 has fundamentally changed how people\ninteract, especially with the introduction of technology-based measures that\naim at curbing the spread of the virus. As the country that currently\nimplements one of the tightest technology-based COVID prevention policy, China\nhas protected its citizen with a prolonged peaceful time of zero case as well\nas a fast reaction to potential upsurging of the disease. However, such\nmobile-based technology does come with sacrifices, especially for senior\ncitizens who find themselves difficult to adapt to modern technologies. In this\nstudy, we demonstrated the fact that most senior citizens find it difficult to\nuse the health code apps called ''JKM'', to which they responded by cutting\ndown on travel and reducing local commuting to locations where the verification\nof JKM is needed. Such compromise has physical and mental consequences and\nleads to inequalities in infrastructure, social isolation and self-sufficiency.\nAs we illustrated in the paper, such decrease in life quality of senior\ncitizens can be greatly reduced if improvements on the user interactions of the\nJKM can be implemented. To the best of our knowledge, we are the first systemic\nstudy of digital inequality due to mobile-based COVID prevention technologies\nfor senior citizens in China. As similar technologies become widely adopted\naround the world, we wish to shed light on how widened digital inequality\nincreasingly affects the life quality of senior citizens in the pandemic era.",
    "published_date": "2021-11-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.10745v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.10723v1",
    "title": "End-to-end Learning for Fair Ranking Systems",
    "authors": [
      "James Kotary",
      "Ferdinando Fioretto",
      "Pascal Van Hentenryck",
      "Ziwei Zhu"
    ],
    "author_ids": [],
    "abstract": "The learning-to-rank problem aims at ranking items to maximize exposure of\nthose most relevant to a user query. A desirable property of such ranking\nsystems is to guarantee some notion of fairness among specified item groups.\nWhile fairness has recently been considered in the context of learning-to-rank\nsystems, current methods cannot provide guarantees on the fairness of the\nproposed ranking policies.\n  This paper addresses this gap and introduces Smart Predict and Optimize for\nFair Ranking (SPOFR), an integrated optimization and learning framework for\nfairness-constrained learning to rank. The end-to-end SPOFR framework includes\na constrained optimization sub-model and produces ranking policies that are\nguaranteed to satisfy fairness constraints while allowing for fine control of\nthe fairness-utility tradeoff. SPOFR is shown to significantly improve current\nstate-of-the-art fair learning-to-rank systems with respect to established\nperformance metrics.",
    "published_date": "2021-11-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.10723v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.10572v1",
    "title": "Control Analysis of Packet Transmission Algorithms: Study on Fairness and Stability",
    "authors": [
      "Lokesh Bommisetty"
    ],
    "author_ids": [],
    "abstract": "This document is a study of fairness, feedback and stability notions of\ndifferent packet transmission algorithms. We start the discussion with defining\ntwo scalable control algorithms namely primal and dual algorithm. We discuss\nthe dual algorithm model and then understand the fair dual algorithm. Further,\nwe discuss different notions of fairness under fair dual algorithm those\ncorrespond to TCP and RCP congestion control protocols. Feedback parameters are\nanalyzed in each of these fairness algorithms and thus their stability is\nstudied.",
    "published_date": "2021-11-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.NI",
      "cs.NA",
      "cs.PF",
      "math.NA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.10572v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.10476v2",
    "title": "Towards Return Parity in Markov Decision Processes",
    "authors": [
      "Jianfeng Chi",
      "Jian Shen",
      "Xinyi Dai",
      "Weinan Zhang",
      "Yuan Tian",
      "Han Zhao"
    ],
    "author_ids": [],
    "abstract": "Algorithmic decisions made by machine learning models in high-stakes domains\nmay have lasting impacts over time. However, naive applications of standard\nfairness criterion in static settings over temporal domains may lead to delayed\nand adverse effects. To understand the dynamics of performance disparity, we\nstudy a fairness problem in Markov decision processes (MDPs). Specifically, we\npropose return parity, a fairness notion that requires MDPs from different\ndemographic groups that share the same state and action spaces to achieve\napproximately the same expected time-discounted rewards. We first provide a\ndecomposition theorem for return disparity, which decomposes the return\ndisparity of any two MDPs sharing the same state and action spaces into the\ndistance between group-wise reward functions, the discrepancy of group\npolicies, and the discrepancy between state visitation distributions induced by\nthe group policies. Motivated by our decomposition theorem, we propose\nalgorithms to mitigate return disparity via learning a shared group policy with\nstate visitation distributional alignment using integral probability metrics.\nWe conduct experiments to corroborate our results, showing that the proposed\nalgorithm can successfully close the disparity gap while maintaining the\nperformance of policies on two real-world recommender system benchmark\ndatasets.",
    "published_date": "2021-11-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.10476v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.10390v1",
    "title": "The ComMA Dataset V0.2: Annotating Aggression and Bias in Multilingual Social Media Discourse",
    "authors": [
      "Ritesh Kumar",
      "Enakshi Nandi",
      "Laishram Niranjana Devi",
      "Shyam Ratan",
      "Siddharth Singh",
      "Akash Bhagat",
      "Yogesh Dawer"
    ],
    "author_ids": [],
    "abstract": "In this paper, we discuss the development of a multilingual dataset annotated\nwith a hierarchical, fine-grained tagset marking different types of aggression\nand the \"context\" in which they occur. The context, here, is defined by the\nconversational thread in which a specific comment occurs and also the \"type\" of\ndiscursive role that the comment is performing with respect to the previous\ncomment. The initial dataset, being discussed here (and made available as part\nof the ComMA@ICON shared task), consists of a total 15,000 annotated comments\nin four languages - Meitei, Bangla, Hindi, and Indian English - collected from\nvarious social media platforms such as YouTube, Facebook, Twitter and Telegram.\nAs is usual on social media websites, a large number of these comments are\nmultilingual, mostly code-mixed with English. The paper gives a detailed\ndescription of the tagset being used for annotation and also the process of\ndeveloping a multi-label, fine-grained tagset that can be used for marking\ncomments with aggression and bias of various kinds including gender bias,\nreligious intolerance (called communal bias in the tagset), class/caste bias\nand ethnic/racial bias. We also define and discuss the tags that have been used\nfor marking different the discursive role being performed through the comments,\nsuch as attack, defend, etc. We also present a statistical analysis of the\ndataset as well as results of our baseline experiments with developing an\nautomatic aggression identification system using the dataset developed.",
    "published_date": "2021-11-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.10390v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.10339v1",
    "title": "Bi-Mix: Bidirectional Mixing for Domain Adaptive Nighttime Semantic Segmentation",
    "authors": [
      "Guanglei Yang",
      "Zhun Zhong",
      "Hao Tang",
      "Mingli Ding",
      "Nicu Sebe",
      "Elisa Ricci"
    ],
    "author_ids": [],
    "abstract": "In autonomous driving, learning a segmentation model that can adapt to\nvarious environmental conditions is crucial. In particular, copying with severe\nillumination changes is an impelling need, as models trained on daylight data\nwill perform poorly at nighttime. In this paper, we study the problem of Domain\nAdaptive Nighttime Semantic Segmentation (DANSS), which aims to learn a\ndiscriminative nighttime model with a labeled daytime dataset and an unlabeled\ndataset, including coarsely aligned day-night image pairs. To this end, we\npropose a novel Bidirectional Mixing (Bi-Mix) framework for DANSS, which can\ncontribute to both image translation and segmentation adaptation processes.\nSpecifically, in the image translation stage, Bi-Mix leverages the knowledge of\nday-night image pairs to improve the quality of nighttime image relighting. On\nthe other hand, in the segmentation adaptation stage, Bi-Mix effectively\nbridges the distribution gap between day and night domains for adapting the\nmodel to the night domain. In both processes, Bi-Mix simply operates by mixing\ntwo samples without extra hyper-parameters, thus it is easy to implement.\nExtensive experiments on Dark Zurich and Nighttime Driving datasets demonstrate\nthe advantage of the proposed Bi-Mix and show that our approach obtains\nstate-of-the-art performance in DANSS. Our code is available at\nhttps://github.com/ygjwd12345/BiMix.",
    "published_date": "2021-11-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.10339v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.11259v1",
    "title": "Model-agnostic bias mitigation methods with regressor distribution control for Wasserstein-based fairness metrics",
    "authors": [
      "Alexey Miroshnikov",
      "Konstandinos Kotsiopoulos",
      "Ryan Franks",
      "Arjun Ravi Kannan"
    ],
    "author_ids": [],
    "abstract": "This article is a companion paper to our earlier work Miroshnikov et al.\n(2021) on fairness interpretability, which introduces bias explanations. In the\ncurrent work, we propose a bias mitigation methodology based upon the\nconstruction of post-processed models with fairer regressor distributions for\nWasserstein-based fairness metrics. By identifying the list of predictors\ncontributing the most to the bias, we reduce the dimensionality of the problem\nby mitigating the bias originating from those predictors. The post-processing\nmethodology involves reshaping the predictor distributions by balancing the\npositive and negative bias explanations and allows for the regressor bias to\ndecrease. We design an algorithm that uses Bayesian optimization to construct\nthe bias-performance efficient frontier over the family of post-processed\nmodels, from which an optimal model is selected. Our novel methodology performs\noptimization in low-dimensional spaces and avoids expensive model retraining.",
    "published_date": "2021-11-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "math.PR",
      "49Q22, 91A12, 68T01"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.11259v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.10235v1",
    "title": "Interpreting deep urban sound classification using Layer-wise Relevance Propagation",
    "authors": [
      "Marco Colussi",
      "Stavros Ntalampiras"
    ],
    "author_ids": [],
    "abstract": "After constructing a deep neural network for urban sound classification, this\nwork focuses on the sensitive application of assisting drivers suffering from\nhearing loss. As such, clear etiology justifying and interpreting model\npredictions comprise a strong requirement. To this end, we used two different\nrepresentations of audio signals, i.e. Mel and constant-Q spectrograms, while\nthe decisions made by the deep neural network are explained via layer-wise\nrelevance propagation. At the same time, frequency content assigned with high\nrelevance in both feature sets, indicates extremely discriminative information\ncharacterizing the present classification task. Overall, we present an\nexplainable AI framework for understanding deep urban sound classification.",
    "published_date": "2021-11-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SD",
      "cs.LG",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.10235v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.09983v1",
    "title": "Towards Measuring Fairness in Speech Recognition: Casual Conversations Dataset Transcriptions",
    "authors": [
      "Chunxi Liu",
      "Michael Picheny",
      "Leda Sarı",
      "Pooja Chitkara",
      "Alex Xiao",
      "Xiaohui Zhang",
      "Mark Chou",
      "Andres Alvarado",
      "Caner Hazirbas",
      "Yatharth Saraf"
    ],
    "author_ids": [],
    "abstract": "It is well known that many machine learning systems demonstrate bias towards\nspecific groups of individuals. This problem has been studied extensively in\nthe Facial Recognition area, but much less so in Automatic Speech Recognition\n(ASR). This paper presents initial Speech Recognition results on \"Casual\nConversations\" -- a publicly released 846 hour corpus designed to help\nresearchers evaluate their computer vision and audio models for accuracy across\na diverse set of metadata, including age, gender, and skin tone. The entire\ncorpus has been manually transcribed, allowing for detailed ASR evaluations\nacross these metadata. Multiple ASR models are evaluated, including models\ntrained on LibriSpeech, 14,000 hour transcribed, and over 2 million hour\nuntranscribed social media videos. Significant differences in word error rate\nacross gender and skin tone are observed at times for all models. We are\nreleasing human transcripts from the Casual Conversations dataset to encourage\nthe community to develop a variety of techniques to reduce these statistical\nbiases.",
    "published_date": "2021-11-18T00:00:00",
    "year": 2021,
    "categories": [
      "eess.AS",
      "cs.SD"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.09983v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.09964v2",
    "title": "Deep IDA: A Deep Learning Method for Integrative Discriminant Analysis of Multi-View Data with Feature Ranking -- An Application to COVID-19 severity",
    "authors": [
      "Jiuzhou Wang",
      "Sandra E. Safo"
    ],
    "author_ids": [],
    "abstract": "COVID-19 severity is due to complications from SARS-Cov-2 but the clinical\ncourse of the infection varies for individuals, emphasizing the need to better\nunderstand the disease at the molecular level. We use clinical and multiple\nmolecular data (or views) obtained from patients with and without COVID-19 who\nwere (or not) admitted to the intensive care unit to shed light on COVID-19\nseverity. Methods for jointly associating the views and separating the COVID-19\ngroups (i.e., one-step methods) have focused on linear relationships. The\nrelationships between the views and COVID-19 patient groups, however, are too\ncomplex to be understood solely by linear methods. Existing nonlinear one-step\nmethods cannot be used to identify signatures to aid in our understanding of\nthe complexity of the disease. We propose Deep IDA (Integrative Discriminant\nAnalysis) to address analytical challenges in our problem of interest. Deep IDA\nlearns nonlinear projections of two or more views that maximally associate the\nviews and separate the classes in each view, and permits feature ranking for\ninterpretable findings. Our applications demonstrate that Deep IDA has\ncompetitive classification rates compared to other state-of-the-art methods and\nis able to identify molecular signatures that facilitate an understanding of\nCOVID-19 severity.",
    "published_date": "2021-11-18T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.ME"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.09964v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.09933v2",
    "title": "Loss Functions for Discrete Contextual Pricing with Observational Data",
    "authors": [
      "Max Biggs",
      "Ruijiang Gao",
      "Wei Sun"
    ],
    "author_ids": [],
    "abstract": "We study a pricing setting where each customer is offered a contextualized\nprice based on customer and/or product features. Often only historical sales\ndata are available, so we observe whether a customer purchased a product at the\nprice prescribed rather than the customer's true valuation. Such observational\ndata are influenced by historical pricing policies, which introduce\ndifficulties in evaluating the effectiveness of future policies. The goal of\nthis paper is to formulate loss functions that can be used for evaluating\npricing policies directly from observational data, rather than going through an\nintermediate demand estimation stage, which may suffer from bias. To achieve\nthis, we adapt ideas from machine learning with corrupted labels, where we\nconsider each observed purchase decision as a known probabilistic\ntransformation of the customer's valuation. From this transformation, we derive\na class of unbiased loss functions. Within this class, we identify minimum\nvariance estimators and estimators robust to poor demand estimation.\nFurthermore, we show that for contextual pricing, estimators popular in the\noff-policy evaluation literature fall within this class of loss functions. We\noffer managerial insights into scenarios under which these estimators are\neffective.",
    "published_date": "2021-11-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.09933v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.09883v2",
    "title": "Swin Transformer V2: Scaling Up Capacity and Resolution",
    "authors": [
      "Ze Liu",
      "Han Hu",
      "Yutong Lin",
      "Zhuliang Yao",
      "Zhenda Xie",
      "Yixuan Wei",
      "Jia Ning",
      "Yue Cao",
      "Zheng Zhang",
      "Li Dong",
      "Furu Wei",
      "Baining Guo"
    ],
    "author_ids": [],
    "abstract": "Large-scale NLP models have been shown to significantly improve the\nperformance on language tasks with no signs of saturation. They also\ndemonstrate amazing few-shot capabilities like that of human beings. This paper\naims to explore large-scale models in computer vision. We tackle three major\nissues in training and application of large vision models, including training\ninstability, resolution gaps between pre-training and fine-tuning, and hunger\non labelled data. Three main techniques are proposed: 1) a residual-post-norm\nmethod combined with cosine attention to improve training stability; 2) A\nlog-spaced continuous position bias method to effectively transfer models\npre-trained using low-resolution images to downstream tasks with\nhigh-resolution inputs; 3) A self-supervised pre-training method, SimMIM, to\nreduce the needs of vast labeled images. Through these techniques, this paper\nsuccessfully trained a 3 billion-parameter Swin Transformer V2 model, which is\nthe largest dense vision model to date, and makes it capable of training with\nimages of up to 1,536$\\times$1,536 resolution. It set new performance records\non 4 representative vision tasks, including ImageNet-V2 image classification,\nCOCO object detection, ADE20K semantic segmentation, and Kinetics-400 video\naction classification. Also note our training is much more efficient than that\nin Google's billion-level visual models, which consumes 40 times less labelled\ndata and 40 times less training time. Code is available at\n\\url{https://github.com/microsoft/Swin-Transformer}.",
    "published_date": "2021-11-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.09883v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.11175v1",
    "title": "On Generalized Schürmann Entropy Estimators",
    "authors": [
      "Peter Grassberger"
    ],
    "author_ids": [],
    "abstract": "We present a new class of estimators of Shannon entropy for severely\nundersampled discrete distributions. It is based on a generalization of an\nestimator proposed by T. Schuermann, which itself is a generalization of an\nestimator proposed by myself in arXiv:physics/0307138. For a special set of\nparameters they are completely free of bias and have a finite variance,\nsomething with is widely believed to be impossible. We present also detailed\nnumerical tests where we compare them with other recent estimators and with\nexact results, and point out a clash with Bayesian estimators for mutual\ninformation.",
    "published_date": "2021-11-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IT",
      "cond-mat.stat-mech",
      "math.IT",
      "physics.data-an"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.11175v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.09507v1",
    "title": "Assessing Social Determinants-Related Performance Bias of Machine Learning Models: A case of Hyperchloremia Prediction in ICU Population",
    "authors": [
      "Songzi Liu",
      "Yuan Luo"
    ],
    "author_ids": [],
    "abstract": "Machine learning in medicine leverages the wealth of healthcare data to\nextract knowledge, facilitate clinical decision-making, and ultimately improve\ncare delivery. However, ML models trained on datasets that lack demographic\ndiversity could yield suboptimal performance when applied to the\nunderrepresented populations (e.g. ethnic minorities, lower social-economic\nstatus), thus perpetuating health disparity. In this study, we evaluated four\nclassifiers built to predict Hyperchloremia - a condition that often results\nfrom aggressive fluids administration in the ICU population - and compared\ntheir performance in racial, gender, and insurance subgroups. We observed that\nadding social determinants features in addition to the lab-based ones improved\nmodel performance on all patients. The subgroup testing yielded significantly\ndifferent AUC scores in 40 out of the 44 model-subgroup, suggesting disparities\nwhen applying ML models to social determinants subgroups. We urge future\nresearchers to design models that proactively adjust for potential biases and\ninclude subgroup reporting in their studies.",
    "published_date": "2021-11-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.09507v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.14263v1",
    "title": "Randomized Controlled Trials Under Influence: Covariate Factors and Graph-Based Network Interference",
    "authors": [
      "Tassilo Schwarz"
    ],
    "author_ids": [],
    "abstract": "Randomized controlled trials are not only the golden standard in medicine and\nvaccine trials but have spread to many other disciplines like behavioral\neconomics, making it an important interdisciplinary tool for scientists.\n  When designing randomized controlled trials, how to assign participants to\ntreatments becomes a key issue. In particular in the presence of covariate\nfactors, the assignment can significantly influence statistical properties and\nthereby the quality of the trial. Another key issue is the widely popular\nassumption among experimenters that participants do not influence each other --\nwhich is far from reality in a field study and can, if unaccounted for,\ndeteriorate the quality of the trial.\n  We address both issues in our work. After introducing randomized controlled\ntrials bridging terms from different disciplines, we first address the issue of\nparticipant-treatment assignment in the presence of known covariate factors.\nThereby, we review a recent assignment algorithm that achieves good worst-case\nvariance bounds.\n  Second, we address social spillover effects. Therefore, we build a\ncomprehensive graph-based model of influence between participants, for which we\ndesign our own average treatment effect estimator $\\hat \\tau_{net}$. We discuss\nits bias and variance and reduce the problem of variance minimization to a\ncertain instance of minimizing the norm of a matrix-vector product, which has\nbeen considered in literature before. Further, we discuss the role of\ndisconnected components in the model's underlying graph.",
    "published_date": "2021-11-18T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ME",
      "cs.SI",
      "math.PR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.14263v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.09137v1",
    "title": "Two-Face: Adversarial Audit of Commercial Face Recognition Systems",
    "authors": [
      "Siddharth D Jaiswal",
      "Karthikeya Duggirala",
      "Abhisek Dash",
      "Animesh Mukherjee"
    ],
    "author_ids": [],
    "abstract": "Computer vision applications like automated face detection are used for a\nvariety of purposes ranging from unlocking smart devices to tracking potential\npersons of interest for surveillance. Audits of these applications have\nrevealed that they tend to be biased against minority groups which result in\nunfair and concerning societal and political outcomes. Despite multiple studies\nover time, these biases have not been mitigated completely and have in fact\nincreased for certain tasks like age prediction. While such systems are audited\nover benchmark datasets, it becomes necessary to evaluate their robustness for\nadversarial inputs. In this work, we perform an extensive adversarial audit on\nmultiple systems and datasets, making a number of concerning observations -\nthere has been a drop in accuracy for some tasks on CELEBSET dataset since a\nprevious audit. While there still exists a bias in accuracy against individuals\nfrom minority groups for multiple datasets, a more worrying observation is that\nthese biases tend to get exorbitantly pronounced with adversarial inputs toward\nthe minority group. We conclude with a discussion on the broader societal\nimpacts in light of these observations and a few suggestions on how to\ncollectively deal with this issue.",
    "published_date": "2021-11-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.09137v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.09065v3",
    "title": "Sampling To Improve Predictions For Underrepresented Observations In Imbalanced Data",
    "authors": [
      "Rune D. Kjærsgaard",
      "Manja G. Grønberg",
      "Line K. H. Clemmensen"
    ],
    "author_ids": [],
    "abstract": "Data imbalance is common in production data, where controlled production\nsettings require data to fall within a narrow range of variation and data are\ncollected with quality assessment in mind, rather than data analytic insights.\nThis imbalance negatively impacts the predictive performance of models on\nunderrepresented observations. We propose sampling to adjust for this imbalance\nwith the goal of improving the performance of models trained on historical\nproduction data. We investigate the use of three sampling approaches to adjust\nfor imbalance. The goal is to downsample the covariates in the training data\nand subsequently fit a regression model. We investigate how the predictive\npower of the model changes when using either the sampled or the original data\nfor training. We apply our methods on a large biopharmaceutical manufacturing\ndata set from an advanced simulation of penicillin production and find that\nfitting a model using the sampled data gives a small reduction in the overall\npredictive performance, but yields a systematically better performance on\nunderrepresented observations. In addition, the results emphasize the need for\nalternative, fair, and balanced model evaluations.",
    "published_date": "2021-11-17T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.09065v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.11203v1",
    "title": "A Data-Centric Behavioral Machine Learning Platform to Reduce Health Inequalities",
    "authors": [
      "Dexian Tang",
      "Guillem Francès",
      "África Periáñez"
    ],
    "author_ids": [],
    "abstract": "Providing front-line health workers in low- and middle- income countries with\nrecommendations and predictions to improve health outcomes can have a\ntremendous impact on reducing healthcare inequalities, for instance by helping\nto prevent the thousands of maternal and newborn deaths that occur every day.\nTo that end, we are developing a data-centric machine learning platform that\nleverages the behavioral logs from a wide range of mobile health applications\nrunning in those countries. Here we describe the platform architecture,\nfocusing on the details that help us to maximize the quality and organization\nof the data throughout the whole process, from the data ingestion with a\ndata-science purposed software development kit to the data pipelines, feature\nengineering and model management.",
    "published_date": "2021-11-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.11203v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.08878v3",
    "title": "CONFAIR: Configurable and Interpretable Algorithmic Fairness",
    "authors": [
      "Ankit Kulshrestha",
      "Ilya Safro"
    ],
    "author_ids": [],
    "abstract": "The rapid growth of data in the recent years has led to the development of\ncomplex learning algorithms that are often used to make decisions in real\nworld. While the positive impact of the algorithms has been tremendous, there\nis a need to mitigate any bias arising from either training samples or implicit\nassumptions made about the data samples. This need becomes critical when\nalgorithms are used in automated decision making systems that can hugely impact\npeople's lives.\n  Many approaches have been proposed to make learning algorithms fair by\ndetecting and mitigating bias in different stages of optimization. However, due\nto a lack of a universal definition of fairness, these algorithms optimize for\na particular interpretation of fairness which makes them limited for real world\nuse. Moreover, an underlying assumption that is common to all algorithms is the\napparent equivalence of achieving fairness and removing bias. In other words,\nthere is no user defined criteria that can be incorporated into the\noptimization procedure for producing a fair algorithm. Motivated by these\nshortcomings of existing methods, we propose the CONFAIR procedure that\nproduces a fair algorithm by incorporating user constraints into the\noptimization procedure. Furthermore, we make the process interpretable by\nestimating the most predictive features from data. We demonstrate the efficacy\nof our approach on several real world datasets using different fairness\ncriteria.",
    "published_date": "2021-11-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.08878v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.08856v2",
    "title": "Fairness Testing of Deep Image Classification with Adequacy Metrics",
    "authors": [
      "Peixin Zhang",
      "Jingyi Wang",
      "Jun Sun",
      "Xinyu Wang"
    ],
    "author_ids": [],
    "abstract": "As deep image classification applications, e.g., face recognition, become\nincreasingly prevalent in our daily lives, their fairness issues raise more and\nmore concern. It is thus crucial to comprehensively test the fairness of these\napplications before deployment. Existing fairness testing methods suffer from\nthe following limitations: 1) applicability, i.e., they are only applicable for\nstructured data or text without handling the high-dimensional and abstract\ndomain sampling in the semantic level for image classification applications; 2)\nfunctionality, i.e., they generate unfair samples without providing testing\ncriterion to characterize the model's fairness adequacy. To fill the gap, we\npropose DeepFAIT, a systematic fairness testing framework specifically designed\nfor deep image classification applications. DeepFAIT consists of several\nimportant components enabling effective fairness testing of deep image\nclassification applications: 1) a neuron selection strategy to identify the\nfairness-related neurons; 2) a set of multi-granularity adequacy metrics to\nevaluate the model's fairness; 3) a test selection algorithm for fixing the\nfairness issues efficiently. We have conducted experiments on widely adopted\nlarge-scale face recognition applications, i.e., VGGFace and FairFace. The\nexperimental results confirm that our approach can effectively identify the\nfairness-related neurons, characterize the model's fairness, and select the\nmost valuable test cases to mitigate the model's fairness issues.",
    "published_date": "2021-11-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.08856v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.08739v2",
    "title": "GAP Enhancing Semantic Interoperability of Genomic Datasets and Provenance Through Nanopublications",
    "authors": [
      "Matheus Feijoó",
      "Rodrigo Jardim",
      "Sergio Serra",
      "Maria Luiza Campos"
    ],
    "author_ids": [],
    "abstract": "While the publication of datasets in scientific repositories has become\nbroadly recognised, the repositories tend to have increasing semantic-related\nproblems. For instance, they present various data reuse obstacles for\nmachine-actionable processes, especially in biological repositories, hampering\nthe reproducibility of scientific experiments. An example of these shortcomings\nis the GenBank database. We propose GAP, an innovative data model to enhance\nthe semantic data meaning to address these issues. The model focuses on\nconverging related approaches like data provenance, semantic interoperability,\nFAIR principles, and nanopublications. Our experiments include a prototype to\nscrape genomic data and trace them to nanopublications as a proof of concept.\nFor this, (meta)data are stored in a three-level nanopub data model. The first\nlevel is related to a target organism, specifying data in terms of biological\ntaxonomy. The second level focuses on the biological strains of the target, the\ncentral part of our contribution. The strains express information related to\ndeciphered (meta)data of the genetic variations of the genomic material. The\nthird level stores related scientific papers (meta)data. We expect it will\noffer higher data storage flexibility and more extensive interoperability with\nother data sources by incorporating and adopting associated approaches to store\ngenomic data in the proposed model.",
    "published_date": "2021-11-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DB",
      "cs.DL",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.08739v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.08723v1",
    "title": "Who Decides if AI is Fair? The Labels Problem in Algorithmic Auditing",
    "authors": [
      "Abhilash Mishra",
      "Yash Gorana"
    ],
    "author_ids": [],
    "abstract": "Labelled \"ground truth\" datasets are routinely used to evaluate and audit AI\nalgorithms applied in high-stakes settings. However, there do not exist widely\naccepted benchmarks for the quality of labels in these datasets. We provide\nempirical evidence that quality of labels can significantly distort the results\nof algorithmic audits in real-world settings. Using data annotators typically\nhired by AI firms in India, we show that fidelity of the ground truth data can\nlead to spurious differences in performance of ASRs between urban and rural\npopulations. After a rigorous, albeit expensive, label cleaning process, these\ndisparities between groups disappear. Our findings highlight how trade-offs\nbetween label quality and data annotation costs can complicate algorithmic\naudits in practice. They also emphasize the need for development of\nconsensus-driven, widely accepted benchmarks for label quality.",
    "published_date": "2021-11-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.08723v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.08711v1",
    "title": "Two-step adversarial debiasing with partial learning -- medical image case-studies",
    "authors": [
      "Ramon Correa",
      "Jiwoong Jason Jeong",
      "Bhavik Patel",
      "Hari Trivedi",
      "Judy W. Gichoya",
      "Imon Banerjee"
    ],
    "author_ids": [],
    "abstract": "The use of artificial intelligence (AI) in healthcare has become a very\nactive research area in the last few years. While significant progress has been\nmade in image classification tasks, only a few AI methods are actually being\ndeployed in hospitals. A major hurdle in actively using clinical AI models\ncurrently is the trustworthiness of these models. More often than not, these\ncomplex models are black boxes in which promising results are generated.\nHowever, when scrutinized, these models begin to reveal implicit biases during\nthe decision making, such as detecting race and having bias towards ethnic\ngroups and subpopulations. In our ongoing study, we develop a two-step\nadversarial debiasing approach with partial learning that can reduce the racial\ndisparity while preserving the performance of the targeted task. The\nmethodology has been evaluated on two independent medical image case-studies -\nchest X-ray and mammograms, and showed promises in bias reduction while\npreserving the targeted performance.",
    "published_date": "2021-11-16T00:00:00",
    "year": 2021,
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.08711v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.08531v1",
    "title": "Language bias in Visual Question Answering: A Survey and Taxonomy",
    "authors": [
      "Desen Yuan"
    ],
    "author_ids": [],
    "abstract": "Visual question answering (VQA) is a challenging task, which has attracted\nmore and more attention in the field of computer vision and natural language\nprocessing. However, the current visual question answering has the problem of\nlanguage bias, which reduces the robustness of the model and has an adverse\nimpact on the practical application of visual question answering. In this\npaper, we conduct a comprehensive review and analysis of this field for the\nfirst time, and classify the existing methods according to three categories,\nincluding enhancing visual information, weakening language priors, data\nenhancement and training strategies. At the same time, the relevant\nrepresentative methods are introduced, summarized and analyzed in turn. The\ncauses of language bias are revealed and classified. Secondly, this paper\nintroduces the datasets mainly used for testing, and reports the experimental\nresults of various existing methods. Finally, we discuss the possible future\nresearch directions in this field.",
    "published_date": "2021-11-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.08531v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.08466v2",
    "title": "Interpretable and Fair Boolean Rule Sets via Column Generation",
    "authors": [
      "Connor Lawless",
      "Sanjeeb Dash",
      "Oktay Gunluk",
      "Dennis Wei"
    ],
    "author_ids": [],
    "abstract": "This paper considers the learning of Boolean rules in disjunctive normal form\n(DNF, OR-of-ANDs, equivalent to decision rule sets) as an interpretable model\nfor classification. An integer program is formulated to optimally trade\nclassification accuracy for rule simplicity. We also consider the fairness\nsetting and extend the formulation to include explicit constraints on two\ndifferent measures of classification parity: equality of opportunity and\nequalized odds. Column generation (CG) is used to efficiently search over an\nexponential number of candidate rules without the need for heuristic rule\nmining. To handle large data sets, we propose an approximate CG algorithm using\nrandomization. Compared to three recently proposed alternatives, the CG\nalgorithm dominates the accuracy-simplicity trade-off in 8 out of 16 data sets.\nWhen maximized for accuracy, CG is competitive with rule learners designed for\nthis purpose, sometimes finding significantly simpler solutions that are no\nless accurate. Compared to other fair and interpretable classifiers, our method\nis able to find rule sets that meet stricter notions of fairness with a modest\ntrade-off in accuracy.",
    "published_date": "2021-11-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.08466v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.08370v2",
    "title": "Fight Detection from Still Images in the Wild",
    "authors": [
      "Şeymanur Aktı",
      "Ferda Ofli",
      "Muhammad Imran",
      "Hazım Kemal Ekenel"
    ],
    "author_ids": [],
    "abstract": "Detecting fights from still images shared on social media is an important\ntask required to limit the distribution of violent scenes in order to prevent\ntheir negative effects. For this reason, in this study, we address the problem\nof fight detection from still images collected from the web and social media.\nWe explore how well one can detect fights from just a single still image. We\nalso propose a new dataset, named Social Media Fight Images (SMFI), comprising\nreal-world images of fight actions. Results of the extensive experiments on the\nproposed dataset show that fight actions can be recognized successfully from\nstill images. That is, even without exploiting the temporal information, it is\npossible to detect fights with high accuracy by utilizing appearance only. We\nalso perform cross-dataset experiments to evaluate the representation capacity\nof the collected dataset. These experiments indicate that, as in the other\ncomputer vision problems, there exists a dataset bias for the fight recognition\nproblem. Although the methods achieve close to 100% accuracy when trained and\ntested on the same fight dataset, the cross-dataset accuracies are\nsignificantly lower, i.e., around 70% when more representative datasets are\nused for training. SMFI dataset is found to be one of the two most\nrepresentative datasets among the utilized five fight datasets.",
    "published_date": "2021-11-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.08370v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.08313v2",
    "title": "Towards Comprehensive Monocular Depth Estimation: Multiple Heads Are Better Than One",
    "authors": [
      "Shuwei Shao",
      "Ran Li",
      "Zhongcai Pei",
      "Zhong Liu",
      "Weihai Chen",
      "Wentao Zhu",
      "Xingming Wu",
      "Baochang Zhang"
    ],
    "author_ids": [],
    "abstract": "Depth estimation attracts widespread attention in the computer vision\ncommunity. However, it is still quite difficult to recover an accurate depth\nmap using only one RGB image. We observe a phenomenon that existing methods\ntend to fail in different cases, caused by differences in network architecture,\nloss function and so on. In this work, we investigate into the phenomenon and\npropose to integrate the strengths of multiple weak depth predictor to build a\ncomprehensive and accurate depth predictor, which is critical for many\nreal-world applications, e.g., 3D reconstruction. Specifically, we construct\nmultiple base (weak) depth predictors by utilizing different Transformer-based\nand convolutional neural network (CNN)-based architectures. Transformer\nestablishes long-range correlation while CNN preserves local information\nignored by Transformer due to the spatial inductive bias. Therefore, the\ncoupling of Transformer and CNN contributes to the generation of complementary\ndepth estimates, which are essential to achieve a comprehensive depth\npredictor. Then, we design mixers to learn from multiple weak predictions and\nadaptively fuse them into a strong depth estimate. The resultant model, which\nwe refer to as Transformer-assisted depth ensembles (TEDepth). On the standard\nNYU-Depth-v2 and KITTI datasets, we thoroughly explore how the neural ensembles\naffect the depth estimation and demonstrate that our TEDepth achieves better\nresults than previous state-of-the-art approaches. To validate the\ngeneralizability across cameras, we directly apply the models trained on\nNYU-Depth-v2 to the SUN RGB-D dataset without any fine-tuning, and the superior\nresults emphasize its strong generalizability.",
    "published_date": "2021-11-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.08313v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.08221v2",
    "title": "Fairness-aware Online Price Discrimination with Nonparametric Demand Models",
    "authors": [
      "Xi Chen",
      "Jiameng Lyu",
      "Xuan Zhang",
      "Yuan Zhou"
    ],
    "author_ids": [],
    "abstract": "Price discrimination, which refers to the strategy of setting different\nprices for different customer groups, has been widely used in online retailing.\nAlthough it helps boost the collected revenue for online retailers, it might\ncreate serious concerns about fairness, which even violates the regulation and\nlaws. This paper studies the problem of dynamic discriminatory pricing under\nfairness constraints. In particular, we consider a finite selling horizon of\nlength $T$ for a single product with two groups of customers. Each group of\ncustomers has its unknown demand function that needs to be learned. For each\nselling period, the seller determines the price for each group and observes\ntheir purchase behavior. While existing literature mainly focuses on maximizing\nrevenue, ensuring fairness among different customers has not been fully\nexplored in the dynamic pricing literature. This work adopts the fairness\nnotion from Cohen et al. (2022). For price fairness, we propose an optimal\ndynamic pricing policy regarding regret, which enforces the strict price\nfairness constraint. In contrast to the standard $\\sqrt{T}$-type regret in\nonline learning, we show that the optimal regret in our case is\n$\\tilde{O}(T^{4/5})$. We further extend our algorithm to a more general notion\nof fairness, which includes demand fairness as a special case. To handle this\ngeneral class, we propose a soft fairness constraint and develop a dynamic\npricing policy that achieves $\\tilde{O}(T^{4/5})$ regret. We also demonstrate\nthat our algorithmic techniques can be adapted to more general scenarios such\nas fairness among multiple groups of customers.",
    "published_date": "2021-11-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.08221v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.08094v2",
    "title": "LIMEcraft: Handcrafted superpixel selection and inspection for Visual eXplanations",
    "authors": [
      "Weronika Hryniewska",
      "Adrianna Grudzień",
      "Przemysław Biecek"
    ],
    "author_ids": [],
    "abstract": "The increased interest in deep learning applications, and their\nhard-to-detect biases result in the need to validate and explain complex\nmodels. However, current explanation methods are limited as far as both the\nexplanation of the reasoning process and prediction results are concerned. They\nusually only show the location in the image that was important for model\nprediction. The lack of possibility to interact with explanations makes it\ndifficult to verify and understand exactly how the model works. This creates a\nsignificant risk when using the model. The risk is compounded by the fact that\nexplanations do not take into account the semantic meaning of the explained\nobjects. To escape from the trap of static and meaningless explanations, we\npropose a tool and a process called LIMEcraft. LIMEcraft enhances the process\nof explanation by allowing a user to interactively select semantically\nconsistent areas and thoroughly examine the prediction for the image instance\nin case of many image features. Experiments on several models show that our\ntool improves model safety by inspecting model fairness for image pieces that\nmay indicate model bias. The code is available at:\nhttp://github.com/MI2DataLab/LIMEcraft",
    "published_date": "2021-11-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.08094v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.08088v1",
    "title": "Assessing gender bias in medical and scientific masked language models with StereoSet",
    "authors": [
      "Robert Robinson"
    ],
    "author_ids": [],
    "abstract": "NLP systems use language models such as Masked Language Models (MLMs) that\nare pre-trained on large quantities of text such as Wikipedia create\nrepresentations of language. BERT is a powerful and flexible general-purpose\nMLM system developed using unlabeled text. Pre-training on large quantities of\ntext also has the potential to transparently embed the cultural and social\nbiases found in the source text into the MLM system. This study aims to compare\nbiases in general purpose and medical MLMs with the StereoSet bias assessment\ntool. The general purpose MLMs showed significant bias overall, with BERT\nscoring 57 and RoBERTa scoring 61. The category of gender bias is where the\nbest performances were found, with 63 for BERT and 73 for RoBERTa. Performances\nfor profession, race, and religion were similar to the overall bias scores for\nthe general-purpose MLMs.Medical MLMs showed more bias in all categories than\nthe general-purpose MLMs except for SciBERT, which showed a race bias score of\n55, which was superior to the race bias score of 53 for BERT. More gender\n(Medical 54-58 vs. General 63-73) and religious (46-54 vs. 58) biases were\nfound with medical MLMs. This evaluation of four medical MLMs for stereotyped\nassessments about race, gender, religion, and profession showed inferior\nperformance to general-purpose MLMs. These medically focused MLMs differ\nconsiderably in training source data, which is likely the root cause of the\ndifferences in ratings for stereotyped biases from the StereoSet tool.",
    "published_date": "2021-11-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.08088v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.07997v2",
    "title": "Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection",
    "authors": [
      "Maarten Sap",
      "Swabha Swayamdipta",
      "Laura Vianna",
      "Xuhui Zhou",
      "Yejin Choi",
      "Noah A. Smith"
    ],
    "author_ids": [],
    "abstract": "The perceived toxicity of language can vary based on someone's identity and\nbeliefs, but this variation is often ignored when collecting toxic language\ndatasets, resulting in dataset and model biases. We seek to understand the who,\nwhy, and what behind biases in toxicity annotations. In two online studies with\ndemographically and politically diverse participants, we investigate the effect\nof annotator identities (who) and beliefs (why), drawing from social psychology\nresearch about hate speech, free speech, racist beliefs, political leaning, and\nmore. We disentangle what is annotated as toxic by considering posts with three\ncharacteristics: anti-Black language, African American English (AAE) dialect,\nand vulgarity. Our results show strong associations between annotator identity\nand beliefs and their ratings of toxicity. Notably, more conservative\nannotators and those who scored highly on our scale for racist beliefs were\nless likely to rate anti-Black language as toxic, but more likely to rate AAE\nas toxic. We additionally present a case study illustrating how a popular\ntoxicity detection system's ratings inherently reflect only specific beliefs\nand perspectives. Our findings call for contextualizing toxicity labels in\nsocial variables, which raises immense implications for toxic language\nannotation and detection.",
    "published_date": "2021-11-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.07997v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.07889v1",
    "title": "An Outcome Test of Discrimination for Ranked Lists",
    "authors": [
      "Jonathan Roth",
      "Guillaume Saint-Jacques",
      "YinYin Yu"
    ],
    "author_ids": [],
    "abstract": "This paper extends Becker (1957)'s outcome test of discrimination to settings\nwhere a (human or algorithmic) decision-maker produces a ranked list of\ncandidates. Ranked lists are particularly relevant in the context of online\nplatforms that produce search results or feeds, and also arise when human\ndecisionmakers express ordinal preferences over a list of candidates. We show\nthat non-discrimination implies a system of moment inequalities, which\nintuitively impose that one cannot permute the position of a lower-ranked\ncandidate from one group with a higher-ranked candidate from a second group and\nsystematically improve the objective. Moreover, we show that that these moment\ninequalities are the only testable implications of non-discrimination when the\nauditor observes only outcomes and group membership by rank. We show how to\nstatistically test the implied inequalities, and validate our approach in an\napplication using data from LinkedIn.",
    "published_date": "2021-11-15T00:00:00",
    "year": 2021,
    "categories": [
      "econ.EM",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.07889v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.07864v2",
    "title": "Evaluating Metrics for Bias in Word Embeddings",
    "authors": [
      "Sarah Schröder",
      "Alexander Schulz",
      "Philip Kenneweg",
      "Robert Feldhans",
      "Fabian Hinder",
      "Barbara Hammer"
    ],
    "author_ids": [],
    "abstract": "Over the last years, word and sentence embeddings have established as text\npreprocessing for all kinds of NLP tasks and improved the performances\nsignificantly. Unfortunately, it has also been shown that these embeddings\ninherit various kinds of biases from the training data and thereby pass on\nbiases present in society to NLP solutions. Many papers attempted to quantify\nbias in word or sentence embeddings to evaluate debiasing methods or compare\ndifferent embedding models, usually with cosine-based metrics. However, lately\nsome works have raised doubts about these metrics showing that even though such\nmetrics report low biases, other tests still show biases. In fact, there is a\ngreat variety of bias metrics or tests proposed in the literature without any\nconsensus on the optimal solutions. Yet we lack works that evaluate bias\nmetrics on a theoretical level or elaborate the advantages and disadvantages of\ndifferent bias metrics. In this work, we will explore different cosine based\nbias metrics. We formalize a bias definition based on the ideas from previous\nworks and derive conditions for bias metrics. Furthermore, we thoroughly\ninvestigate the existing cosine-based metrics and their limitations to show why\nthese metrics can fail to report biases in some cases. Finally, we propose a\nnew metric, SAME, to address the shortcomings of existing metrics and\nmathematically prove that SAME behaves appropriately.",
    "published_date": "2021-11-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.07864v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.07668v1",
    "title": "Fast Axiomatic Attribution for Neural Networks",
    "authors": [
      "Robin Hesse",
      "Simone Schaub-Meyer",
      "Stefan Roth"
    ],
    "author_ids": [],
    "abstract": "Mitigating the dependence on spurious correlations present in the training\ndataset is a quickly emerging and important topic of deep learning. Recent\napproaches include priors on the feature attribution of a deep neural network\n(DNN) into the training process to reduce the dependence on unwanted features.\nHowever, until now one needed to trade off high-quality attributions,\nsatisfying desirable axioms, against the time required to compute them. This in\nturn either led to long training times or ineffective attribution priors. In\nthis work, we break this trade-off by considering a special class of\nefficiently axiomatically attributable DNNs for which an axiomatic feature\nattribution can be computed with only a single forward/backward pass. We\nformally prove that nonnegatively homogeneous DNNs, here termed\n$\\mathcal{X}$-DNNs, are efficiently axiomatically attributable and show that\nthey can be effortlessly constructed from a wide range of regular DNNs by\nsimply removing the bias term of each layer. Various experiments demonstrate\nthe advantages of $\\mathcal{X}$-DNNs, beating state-of-the-art generic\nattribution methods on regular DNNs for training with attribution priors.",
    "published_date": "2021-11-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.07668v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.07608v1",
    "title": "Property Inference Attacks Against GANs",
    "authors": [
      "Junhao Zhou",
      "Yufei Chen",
      "Chao Shen",
      "Yang Zhang"
    ],
    "author_ids": [],
    "abstract": "While machine learning (ML) has made tremendous progress during the past\ndecade, recent research has shown that ML models are vulnerable to various\nsecurity and privacy attacks. So far, most of the attacks in this field focus\non discriminative models, represented by classifiers. Meanwhile, little\nattention has been paid to the security and privacy risks of generative models,\nsuch as generative adversarial networks (GANs). In this paper, we propose the\nfirst set of training dataset property inference attacks against GANs.\nConcretely, the adversary aims to infer the macro-level training dataset\nproperty, i.e., the proportion of samples used to train a target GAN with\nrespect to a certain attribute. A successful property inference attack can\nallow the adversary to gain extra knowledge of the target GAN's training\ndataset, thereby directly violating the intellectual property of the target\nmodel owner. Also, it can be used as a fairness auditor to check whether the\ntarget GAN is trained with a biased dataset. Besides, property inference can\nserve as a building block for other advanced attacks, such as membership\ninference. We propose a general attack pipeline that can be tailored to two\nattack scenarios, including the full black-box setting and partial black-box\nsetting. For the latter, we introduce a novel optimization framework to\nincrease the attack efficacy. Extensive experiments over four representative\nGAN models on five property inference tasks show that our attacks achieve\nstrong performance. In addition, we show that our attacks can be used to\nenhance the performance of membership inference against GANs.",
    "published_date": "2021-11-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.07608v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.07555v1",
    "title": "Confucius, Cyberpunk and Mr. Science: Comparing AI ethics between China and the EU",
    "authors": [
      "Pascale Fung",
      "Hubert Etienne"
    ],
    "author_ids": [],
    "abstract": "The exponential development and application of artificial intelligence\ntriggered an unprecedented global concern for potential social and ethical\nissues. Stakeholders from different industries, international foundations,\ngovernmental organisations and standards institutions quickly improvised and\ncreated various codes of ethics attempting to regulate AI. A major concern is\nthe large homogeneity and presumed consensualism around these principles. While\nit is true that some ethical doctrines, such as the famous Kantian deontology,\naspire to universalism, they are however not universal in practice. In fact,\nethical pluralism is more about differences in which relevant questions to ask\nrather than different answers to a common question. When people abide by\ndifferent moral doctrines, they tend to disagree on the very approach to an\nissue. Even when people from different cultures happen to agree on a set of\ncommon principles, it does not necessarily mean that they share the same\nunderstanding of these concepts and what they entail. In order to better\nunderstand the philosophical roots and cultural context underlying ethical\nprinciples in AI, we propose to analyse and compare the ethical principles\nendorsed by the Chinese National New Generation Artificial Intelligence\nGovernance Professional Committee (CNNGAIGPC) and those elaborated by the\nEuropean High-level Expert Group on AI (HLEGAI). China and the EU have very\ndifferent political systems and diverge in their cultural heritages. In our\nanalysis, we wish to highlight that principles that seem similar a priori may\nactually have different meanings, derived from different approaches and reflect\ndistinct goals.",
    "published_date": "2021-11-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.07555v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.07545v1",
    "title": "Randomized Classifiers vs Human Decision-Makers: Trustworthy AI May Have to Act Randomly and Society Seems to Accept This",
    "authors": [
      "Gábor Erdélyi",
      "Olivia J. Erdélyi",
      "Vladimir Estivill-Castro"
    ],
    "author_ids": [],
    "abstract": "As \\emph{artificial intelligence} (AI) systems are increasingly involved in\ndecisions affecting our lives, ensuring that automated decision-making is fair\nand ethical has become a top priority. Intuitively, we feel that akin to human\ndecisions, judgments of artificial agents should necessarily be grounded in\nsome moral principles. Yet a decision-maker (whether human or artificial) can\nonly make truly ethical (based on any ethical theory) and fair (according to\nany notion of fairness) decisions if full information on all the relevant\nfactors on which the decision is based are available at the time of\ndecision-making. This raises two problems: (1) In settings, where we rely on AI\nsystems that are using classifiers obtained with supervised learning, some\ninduction/generalization is present and some relevant attributes may not be\npresent even during learning. (2) Modeling such decisions as games reveals that\nany -- however ethical -- pure strategy is inevitably susceptible to\nexploitation.\n  Moreover, in many games, a Nash Equilibrium can only be obtained by using\nmixed strategies, i.e., to achieve mathematically optimal outcomes, decisions\nmust be randomized. In this paper, we argue that in supervised learning\nsettings, there exist random classifiers that perform at least as well as\ndeterministic classifiers, and may hence be the optimal choice in many\ncircumstances. We support our theoretical results with an empirical study\nindicating a positive societal attitude towards randomized artificial\ndecision-makers, and discuss some policy and implementation issues related to\nthe use of random classifiers that relate to and are relevant for current AI\npolicy and standardization initiatives.",
    "published_date": "2021-11-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.07545v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.07478v2",
    "title": "Physics in the Machine: Integrating Physical Knowledge in Autonomous Phase-Mapping",
    "authors": [
      "A. Gilad Kusne",
      "Austin McDannald",
      "Brian DeCost",
      "Corey Oses",
      "Cormac Toher",
      "Stefano Curtarolo",
      "Apurva Mehta",
      "Ichiro Takeuchi"
    ],
    "author_ids": [],
    "abstract": "Application of artificial intelligence (AI), and more specifically machine\nlearning, to the physical sciences has expanded significantly over the past\ndecades. In particular, science-informed AI, also known as scientific AI or\ninductive bias AI, has grown from a focus on data analysis to now controlling\nexperiment design, simulation, execution and analysis in closed-loop autonomous\nsystems. The CAMEO (closed-loop autonomous materials exploration and\noptimization) algorithm employs scientific AI to address two tasks: learning a\nmaterial system's composition-structure relationship and identifying materials\ncompositions with optimal functional properties. By integrating these,\naccelerated materials screening across compositional phase diagrams was\ndemonstrated, resulting in the discovery of a best-in-class phase change memory\nmaterial. Key to this success is the ability to guide subsequent measurements\nto maximize knowledge of the composition-structure relationship, or phase map.\nIn this work we investigate the benefits of incorporating varying levels of\nprior physical knowledge into CAMEO's autonomous phase-mapping. This includes\nthe use of ab-initio phase boundary data from the AFLOW repositories, which has\nbeen shown to optimize CAMEO's search when used as a prior.",
    "published_date": "2021-11-15T00:00:00",
    "year": 2021,
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.07478v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.07448v1",
    "title": "Contrastive Clustering: Toward Unsupervised Bias Reduction for Emotion and Sentiment Classification",
    "authors": [
      "Jared Mowery"
    ],
    "author_ids": [],
    "abstract": "Background: When neural network emotion and sentiment classifiers are used in\npublic health informatics studies, biases present in the classifiers could\nproduce inadvertently misleading results.\n  Objective: This study assesses the impact of bias on COVID-19 topics, and\ndemonstrates an automatic algorithm for reducing bias when applied to COVID-19\nsocial media texts. This could help public health informatics studies produce\nmore timely results during crises, with a reduced risk of misleading results.\n  Methods: Emotion and sentiment classifiers were applied to COVID-19 data\nbefore and after debiasing the classifiers using unsupervised contrastive\nclustering. Contrastive clustering approximates the degree to which tokens\nexhibit a causal versus correlational relationship with emotion or sentiment,\nby contrasting the tokens' relative salience to topics versus emotions or\nsentiments.\n  Results: Contrastive clustering distinguishes correlation from causation for\ntokens with an F1 score of 0.753. Masking bias prone tokens from the classifier\ninput decreases the classifier's overall F1 score by 0.02 (anger) and 0.033\n(negative sentiment), but improves the F1 score for sentences annotated as bias\nprone by 0.155 (anger) and 0.103 (negative sentiment). Averaging across topics,\ndebiasing reduces anger estimates by 14.4% and negative sentiment estimates by\n8.0%.\n  Conclusions: Contrastive clustering reduces algorithmic bias in emotion and\nsentiment classification for social media text pertaining to the COVID-19\npandemic. Public health informatics studies should account for bias, due to its\nprevalence across a range of topics. Further research is needed to improve bias\nreduction techniques and to explore the adverse impact of bias on public health\ninformatics analyses.",
    "published_date": "2021-11-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.07448v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.07370v3",
    "title": "Co-segmentation Inspired Attention Module for Video-based Computer Vision Tasks",
    "authors": [
      "Arulkumar Subramaniam",
      "Jayesh Vaidya",
      "Muhammed Abdul Majeed Ameen",
      "Athira Nambiar",
      "Anurag Mittal"
    ],
    "author_ids": [],
    "abstract": "Video-based computer vision tasks can benefit from estimation of the salient\nregions and interactions between those regions. Traditionally, this has been\ndone by identifying the object regions in the images by utilizing pre-trained\nmodels to perform object detection, object segmentation and/or object pose\nestimation. Although using pre-trained models is a viable approach, it has\nseveral limitations in the need for an exhaustive annotation of object\ncategories, a possible domain gap between datasets, and a bias that is\ntypically present in pre-trained models. In this work, we propose to utilize\nthe common rationale that a sequence of video frames capture a set of common\nobjects and interactions between them, thus a notion of co-segmentation between\nthe video frame features may equip the model with the ability to automatically\nfocus on task-specific salient regions and improve the underlying task's\nperformance in an end-to-end manner. In this regard, we propose a generic\nmodule called ``Co-Segmentation inspired Attention Module'' (COSAM) that can be\nplugged in to any CNN model to promote the notion of co-segmentation based\nattention among a sequence of video frame features. We show the application of\nCOSAM in three video-based tasks namely: 1) Video-based person re-ID, 2) Video\ncaptioning, & 3) Video action classification and demonstrate that COSAM is able\nto capture the task-specific salient regions in video frames, thus leading to\nnotable performance improvements along with interpretable attention maps for a\nvariety of video-based vision tasks, with possible application to other\nvideo-based vision tasks as well.",
    "published_date": "2021-11-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.07370v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.07344v1",
    "title": "Towards Privacy-Preserving Affect Recognition: A Two-Level Deep Learning Architecture",
    "authors": [
      "Jimiama M. Mase",
      "Natalie Leesakul",
      "Fan Yang",
      "Grazziela P. Figueredo",
      "Mercedes Torres Torres"
    ],
    "author_ids": [],
    "abstract": "Automatically understanding and recognising human affective states using\nimages and computer vision can improve human-computer and human-robot\ninteraction. However, privacy has become an issue of great concern, as the\nidentities of people used to train affective models can be exposed in the\nprocess. For instance, malicious individuals could exploit images from users\nand assume their identities. In addition, affect recognition using images can\nlead to discriminatory and algorithmic bias, as certain information such as\nrace, gender, and age could be assumed based on facial features. Possible\nsolutions to protect the privacy of users and avoid misuse of their identities\nare to: (1) extract anonymised facial features, namely action units (AU) from a\ndatabase of images, discard the images and use AUs for processing and training,\nand (2) federated learning (FL) i.e. process raw images in users' local\nmachines (local processing) and send the locally trained models to the main\nprocessing machine for aggregation (central processing). In this paper, we\npropose a two-level deep learning architecture for affect recognition that uses\nAUs in level 1 and FL in level 2 to protect users' identities. The architecture\nconsists of recurrent neural networks to capture the temporal relationships\namongst the features and predict valence and arousal affective states. In our\nexperiments, we evaluate the performance of our privacy-preserving architecture\nusing different variations of recurrent neural networks on RECOLA, a\ncomprehensive multimodal affective database. Our results show state-of-the-art\nperformance of $0.426$ for valence and $0.401$ for arousal using the\nConcordance Correlation Coefficient evaluation metric, demonstrating the\nfeasibility of developing models for affect recognition that are both accurate\nand ensure privacy.",
    "published_date": "2021-11-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.07344v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.07308v1",
    "title": "What Should We Optimize in Participatory Budgeting? An Experimental Study",
    "authors": [
      "Ariel Rosenfeld",
      "Nimrod Talmon"
    ],
    "author_ids": [],
    "abstract": "Participatory Budgeting (PB) is a process in which voters decide how to\nallocate a common budget; most commonly it is done by ordinary people -- in\nparticular, residents of some municipality -- to decide on a fraction of the\nmunicipal budget. From a social choice perspective, existing research on PB\nfocuses almost exclusively on designing computationally-efficient aggregation\nmethods that satisfy certain axiomatic properties deemed \"desirable\" by the\nresearch community. Our work complements this line of research through a user\nstudy (N = 215) involving several experiments aimed at identifying what\npotential voters (i.e., non-experts) deem fair or desirable in simple PB\nsettings. Our results show that some modern PB aggregation techniques greatly\ndiffer from users' expectations, while other, more standard approaches, provide\nmore aligned results. We also identify a few possible discrepancies between\nwhat non-experts consider \\say{desirable} and how they perceive the notion of\n\"fairness\" in the PB context. Taken jointly, our results can be used to help\nthe research community identify appropriate PB aggregation methods to use in\npractice.",
    "published_date": "2021-11-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.07308v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.06928v2",
    "title": "Generalized Nested Rollout Policy Adaptation with Dynamic Bias for Vehicle Routing",
    "authors": [
      "Julien Sentuc",
      "Tristan Cazenave",
      "Jean-Yves Lucas"
    ],
    "author_ids": [],
    "abstract": "In this paper we present an extension of the Nested Rollout Policy Adaptation\nalgorithm (NRPA), namely the Generalized Nested Rollout Policy Adaptation\n(GNRPA), as well as its use for solving some instances of the Vehicle Routing\nProblem. We detail some results obtained on the Solomon instances set which is\na conventional benchmark for the Vehicle Routing Problem (VRP). We show that on\nall instances, GNRPA performs better than NRPA. On some instances, it performs\nbetter than the Google OR Tool module dedicated to VRP.",
    "published_date": "2021-11-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.06928v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.08481v2",
    "title": "PySINDy: A comprehensive Python package for robust sparse system identification",
    "authors": [
      "Alan A. Kaptanoglu",
      "Brian M. de Silva",
      "Urban Fasel",
      "Kadierdan Kaheman",
      "Andy J. Goldschmidt",
      "Jared L. Callaham",
      "Charles B. Delahunt",
      "Zachary G. Nicolaou",
      "Kathleen Champion",
      "Jean-Christophe Loiseau",
      "J. Nathan Kutz",
      "Steven L. Brunton"
    ],
    "author_ids": [],
    "abstract": "Automated data-driven modeling, the process of directly discovering the\ngoverning equations of a system from data, is increasingly being used across\nthe scientific community. PySINDy is a Python package that provides tools for\napplying the sparse identification of nonlinear dynamics (SINDy) approach to\ndata-driven model discovery. In this major update to PySINDy, we implement\nseveral advanced features that enable the discovery of more general\ndifferential equations from noisy and limited data. The library of candidate\nterms is extended for the identification of actuated systems, partial\ndifferential equations (PDEs), and implicit differential equations. Robust\nformulations, including the integral form of SINDy and ensembling techniques,\nare also implemented to improve performance for real-world data. Finally, we\nprovide a range of new optimization algorithms, including several sparse\nregression techniques and algorithms to enforce and promote inequality\nconstraints and stability. Together, these updates enable entirely new SINDy\nmodel discovery capabilities that have not been reported in the literature,\nsuch as constrained PDE identification and ensembling with different sparse\nregression optimizers.",
    "published_date": "2021-11-12T00:00:00",
    "year": 2021,
    "categories": [
      "eess.SY",
      "cs.LG",
      "cs.SY",
      "physics.flu-dyn"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.08481v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.08168v1",
    "title": "Explaining medical AI performance disparities across sites with confounder Shapley value analysis",
    "authors": [
      "Eric Wu",
      "Kevin Wu",
      "James Zou"
    ],
    "author_ids": [],
    "abstract": "Medical AI algorithms can often experience degraded performance when\nevaluated on previously unseen sites. Addressing cross-site performance\ndisparities is key to ensuring that AI is equitable and effective when deployed\non diverse patient populations. Multi-site evaluations are key to diagnosing\nsuch disparities as they can test algorithms across a broader range of\npotential biases such as patient demographics, equipment types, and technical\nparameters. However, such tests do not explain why the model performs worse.\nOur framework provides a method for quantifying the marginal and cumulative\neffect of each type of bias on the overall performance difference when a model\nis evaluated on external data. We demonstrate its usefulness in a case study of\na deep learning model trained to detect the presence of pneumothorax, where our\nframework can help explain up to 60% of the discrepancy in performance across\ndifferent sites with known biases like disease comorbidities and imaging\nparameters.",
    "published_date": "2021-11-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.08168v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.06825v1",
    "title": "Alleviating the transit timing variation bias in transit surveys. I. RIVERS: Method and detection of a pair of resonant super-Earths around Kepler-1705",
    "authors": [
      "A. Leleu",
      "G. Chatel",
      "S. Udry",
      "Y. Alibert",
      "J. -B. Delisle",
      "R. Mardling"
    ],
    "author_ids": [],
    "abstract": "Transit timing variations (TTVs) can provide useful information for systems\nobserved by transit, as they allow us to put constraints on the masses and\neccentricities of the observed planets, or even to constrain the existence of\nnon-transiting companions. However, TTVs can also act as a detection bias that\ncan prevent the detection of small planets in transit surveys that would\notherwise be detected by standard algorithms such as the Boxed Least Square\nalgorithm (BLS) if their orbit was not perturbed. This bias is especially\npresent for surveys with a long baseline, such as Kepler, some of the TESS\nsectors, and the upcoming PLATO mission. Here we introduce a detection method\nthat is robust to large TTVs, and illustrate its use by recovering and\nconfirming a pair of resonant super-Earths with ten-hour TTVs around\nKepler-1705. The method is based on a neural network trained to recover the\ntracks of low-signal-to-noise-ratio(S/N) perturbed planets in river diagrams.\nWe recover the transit parameters of these candidates by fitting the light\ncurve. The individual transit S/N of Kepler-1705b and c are about three times\nlower than all the previously known planets with TTVs of 3 hours or more,\npushing the boundaries in the recovery of these small, dynamically active\nplanets. Recovering this type of object is essential for obtaining a complete\npicture of the observed planetary systems, and solving for a bias not often\ntaken into account in statistical studies of exoplanet populations. In\naddition, TTVs are a means of obtaining mass estimates which can be essential\nfor studying the internal structure of planets discovered by transit surveys.\nFinally, we show that due to the strong orbital perturbations, it is possible\nthat the spin of the outer resonant planet of Kepler-1705 is trapped in a sub-\nor super-synchronous spin-orbit resonance.",
    "published_date": "2021-11-12T00:00:00",
    "year": 2021,
    "categories": [
      "astro-ph.EP",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.06825v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.06780v1",
    "title": "AWD3: Dynamic Reduction of the Estimation Bias",
    "authors": [
      "Dogan C. Cicek",
      "Enes Duran",
      "Baturay Saglam",
      "Kagan Kaya",
      "Furkan B. Mutlu",
      "Suleyman S. Kozat"
    ],
    "author_ids": [],
    "abstract": "Value-based deep Reinforcement Learning (RL) algorithms suffer from the\nestimation bias primarily caused by function approximation and temporal\ndifference (TD) learning. This problem induces faulty state-action value\nestimates and therefore harms the performance and robustness of the learning\nalgorithms. Although several techniques were proposed to tackle, learning\nalgorithms still suffer from this bias. Here, we introduce a technique that\neliminates the estimation bias in off-policy continuous control algorithms\nusing the experience replay mechanism. We adaptively learn the weighting\nhyper-parameter beta in the Weighted Twin Delayed Deep Deterministic Policy\nGradient algorithm. Our method is named Adaptive-WD3 (AWD3). We show through\ncontinuous control environments of OpenAI gym that our algorithm matches or\noutperforms the state-of-the-art off-policy policy gradient learning\nalgorithms.",
    "published_date": "2021-11-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.06780v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.06890v1",
    "title": "Impact of loss functions on the performance of a deep neural network designed to restore low-dose digital mammography",
    "authors": [
      "Hongming Shan",
      "Rodrigo de Barros Vimieiro",
      "Lucas Rodrigues Borges",
      "Marcelo Andrade da Costa Vieira",
      "Ge Wang"
    ],
    "author_ids": [],
    "abstract": "Digital mammography is still the most common imaging tool for breast cancer\nscreening. Although the benefits of using digital mammography for cancer\nscreening outweigh the risks associated with the x-ray exposure, the radiation\ndose must be kept as low as possible while maintaining the diagnostic utility\nof the generated images, thus minimizing patient risks. Many studies\ninvestigated the feasibility of dose reduction by restoring low-dose images\nusing deep neural networks. In these cases, choosing the appropriate training\ndatabase and loss function is crucial and impacts the quality of the results.\nIn this work, a modification of the ResNet architecture, with hierarchical skip\nconnections, is proposed to restore low-dose digital mammography. We compared\nthe restored images to the standard full-dose images. Moreover, we evaluated\nthe performance of several loss functions for this task. For training purposes,\nwe extracted 256,000 image patches from a dataset of 400 images of\nretrospective clinical mammography exams, where different dose levels were\nsimulated to generate low and standard-dose pairs. To validate the network in a\nreal scenario, a physical anthropomorphic breast phantom was used to acquire\nreal low-dose and standard full-dose images in a commercially avaliable\nmammography system, which were then processed through our trained model. An\nanalytical restoration model for low-dose digital mammography, previously\npresented, was used as a benchmark in this work. Objective assessment was\nperformed through the signal-to-noise ratio (SNR) and mean normalized squared\nerror (MNSE), decomposed into residual noise and bias. Results showed that the\nperceptual loss function (PL4) is able to achieve virtually the same noise\nlevels of a full-dose acquisition, while resulting in smaller signal bias\ncompared to other loss functions.",
    "published_date": "2021-11-12T00:00:00",
    "year": 2021,
    "categories": [
      "eess.IV",
      "cs.CV",
      "physics.med-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.06890v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.06689v1",
    "title": "Strategic COVID-19 vaccine distribution can simultaneously elevate social utility and equity",
    "authors": [
      "Lin Chen",
      "Fengli Xu",
      "Zhenyu Han",
      "Kun Tang",
      "Pan Hui",
      "James Evans",
      "Yong Li"
    ],
    "author_ids": [],
    "abstract": "Balancing social utility and equity in distributing limited vaccines\nrepresents a critical policy concern for protecting against the prolonged\nCOVID-19 pandemic. What is the nature of the trade-off between maximizing\ncollective welfare and minimizing disparities between more and less privileged\ncommunities? To evaluate vaccination strategies, we propose a novel epidemic\nmodel that explicitly accounts for both demographic and mobility differences\namong communities and their association with heterogeneous COVID-19 risks, then\ncalibrate it with large-scale data. Using this model, we find that social\nutility and equity can be simultaneously improved when vaccine access is\nprioritized for the most disadvantaged communities, which holds even when such\ncommunities manifest considerable vaccine reluctance. Nevertheless, equity\namong distinct demographic features are in tension due to their complex\ncorrelation in society. We design two behavior-and-demography-aware indices,\ncommunity risk and societal harm, which capture the risks communities face and\nthose they impose on society from not being vaccinated, to inform the design of\ncomprehensive vaccine distribution strategies. Our study provides a framework\nfor uniting utility and equity-based considerations in vaccine distribution,\nand sheds light on how to balance multiple ethical values in complex settings\nfor epidemic control.",
    "published_date": "2021-11-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.06689v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.08460v4",
    "title": "Enabling human-centered AI: A new junction and shared journey between AI and HCI communities",
    "authors": [
      "Wei Xu",
      "Marvin Dainoff"
    ],
    "author_ids": [],
    "abstract": "Artificial intelligence (AI) has brought benefits, but it may also cause harm\nif it is not appropriately developed. Current development is mainly driven by a\n\"technology-centered\" approach, causing many failures. For example, the AI\nIncident Database has documented over a thousand AI-related accidents. To\naddress these challenges, a human-centered AI (HCAI) approach has been promoted\nand has received a growing level of acceptance over the last few years. HCAI\ncalls for combining AI with user experience (UX) design will enable the\ndevelopment of AI systems (e.g., autonomous vehicles, intelligent user\ninterfaces, or intelligent decision-making systems) to achieve its design goals\nsuch as usable/explainable AI, human-controlled AI, and ethical AI. While HCAI\npromotion continues, it has not specifically addressed the collaboration\nbetween AI and human-computer interaction (HCI) communities, resulting in\nuncertainty about what action should be taken by both sides to apply HCAI in\ndeveloping AI systems. This Viewpoint focuses on the collaboration between the\nAI and HCI communities, which leads to nine recommendations for effective\ncollaboration to enable HCAI in developing AI systems.",
    "published_date": "2021-11-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.08460v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.06549v2",
    "title": "Bi-Discriminator Class-Conditional Tabular GAN",
    "authors": [
      "Mohammad Esmaeilpour",
      "Nourhene Chaalia",
      "Adel Abusitta",
      "Francois-Xavier Devailly",
      "Wissem Maazoun",
      "Patrick Cardinal"
    ],
    "author_ids": [],
    "abstract": "This paper introduces a bi-discriminator GAN for synthesizing tabular\ndatasets containing continuous, binary, and discrete columns. Our proposed\napproach employs an adapted preprocessing scheme and a novel conditional term\nfor the generator network to more effectively capture the input sample\ndistributions. Additionally, we implement straightforward yet effective\narchitectures for discriminator networks aiming at providing more\ndiscriminative gradient information to the generator. Our experimental results\non four benchmarking public datasets corroborates the superior performance of\nour GAN both in terms of likelihood fitness metric and machine learning\nefficacy.",
    "published_date": "2021-11-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.06549v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.06382v4",
    "title": "The ZERO Regrets Algorithm: Optimizing over Pure Nash Equilibria via Integer Programming",
    "authors": [
      "Gabriele Dragotto",
      "Rosario Scatamacchia"
    ],
    "author_ids": [],
    "abstract": "Designing efficient algorithms to compute Nash equilibria poses considerable\nchallenges in Algorithmic Game Theory and Optimization. In this work, we employ\ninteger programming techniques to compute Nash equilibria in Integer\nProgramming Games, a class of simultaneous and non-cooperative games where each\nplayer solves a parametrized integer program. We introduce ZERO Regrets, a\ngeneral and efficient cutting plane algorithm to compute, enumerate, and select\nNash equilibria. Our framework leverages the concept of equilibrium inequality,\nan inequality valid for any Nash equilibrium, and the associated equilibrium\nseparation oracle. We evaluate our algorithmic framework on a wide range of\npractical and methodological problems from the literature, providing a solid\nbenchmark against the existing approaches.",
    "published_date": "2021-11-11T00:00:00",
    "year": 2021,
    "categories": [
      "math.OC",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.06382v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.06290v1",
    "title": "Fairness, Integrity, and Privacy in a Scalable Blockchain-based Federated Learning System",
    "authors": [
      "Timon Rückel",
      "Johannes Sedlmeir",
      "Peter Hofmann"
    ],
    "author_ids": [],
    "abstract": "Federated machine learning (FL) allows to collectively train models on\nsensitive data as only the clients' models and not their training data need to\nbe shared. However, despite the attention that research on FL has drawn, the\nconcept still lacks broad adoption in practice. One of the key reasons is the\ngreat challenge to implement FL systems that simultaneously achieve fairness,\nintegrity, and privacy preservation for all participating clients. To\ncontribute to solving this issue, our paper suggests a FL system that\nincorporates blockchain technology, local differential privacy, and\nzero-knowledge proofs. Our implementation of a proof-of-concept with multiple\nlinear regression illustrates that these state-of-the-art technologies can be\ncombined to a FL system that aligns economic incentives, trust, and\nconfidentiality requirements in a scalable and transparent system.",
    "published_date": "2021-11-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.DC",
      "68T01, 68M14",
      "C.2; H.4; J.m"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.06290v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.06207v1",
    "title": "Governance of Ethical and Trustworthy AI Systems: Research Gaps in the ECCOLA Method",
    "authors": [
      "Mamia Agbese",
      "Hanna-Kaisa Alanen",
      "Jani Antikainen",
      "Erika Halme",
      "Hannakaisa Isomäki",
      "Marianna Jantunen",
      "Kai-Kristian Kemell",
      "Rebekah Rousi",
      "Heidi Vainio-Pekka",
      "Ville Vakkuri"
    ],
    "author_ids": [],
    "abstract": "Advances in machine learning (ML) technologies have greatly improved\nArtificial Intelligence (AI) systems. As a result, AI systems have become\nubiquitous, with their application prevalent in virtually all sectors. However,\nAI systems have prompted ethical concerns, especially as their usage crosses\nboundaries in sensitive areas such as healthcare, transportation, and security.\nAs a result, users are calling for better AI governance practices in ethical AI\nsystems. Therefore, AI development methods are encouraged to foster these\npractices. This research analyzes the ECCOLA method for developing ethical and\ntrustworthy AI systems to determine if it enables AI governance in development\nprocesses through ethical practices. The results demonstrate that while ECCOLA\nfully facilitates AI governance in corporate governance practices in all its\nprocesses, some of its practices do not fully foster data governance and\ninformation governance practices. This indicates that the method can be further\nimproved.",
    "published_date": "2021-11-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.06207v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.06142v1",
    "title": "Reducing Data Complexity using Autoencoders with Class-informed Loss Functions",
    "authors": [
      "David Charte",
      "Francisco Charte",
      "Francisco Herrera"
    ],
    "author_ids": [],
    "abstract": "Available data in machine learning applications is becoming increasingly\ncomplex, due to higher dimensionality and difficult classes. There exists a\nwide variety of approaches to measuring complexity of labeled data, according\nto class overlap, separability or boundary shapes, as well as group morphology.\nMany techniques can transform the data in order to find better features, but\nfew focus on specifically reducing data complexity. Most data transformation\nmethods mainly treat the dimensionality aspect, leaving aside the available\ninformation within class labels which can be useful when classes are somehow\ncomplex.\n  This paper proposes an autoencoder-based approach to complexity reduction,\nusing class labels in order to inform the loss function about the adequacy of\nthe generated variables. This leads to three different new feature learners,\nScorer, Skaler and Slicer. They are based on Fisher's discriminant ratio, the\nKullback-Leibler divergence and least-squares support vector machines,\nrespectively. They can be applied as a preprocessing stage for a binary\nclassification problem. A thorough experimentation across a collection of 27\ndatasets and a range of complexity and classification metrics shows that\nclass-informed autoencoders perform better than 4 other popular unsupervised\nfeature extraction techniques, especially when the final objective is using the\ndata for a classification task.",
    "published_date": "2021-11-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.NE",
      "68T07",
      "I.2.6; I.5.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.06142v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.06116v1",
    "title": "Implementation of Ethically Aligned Design with Ethical User stories in SMART terminal Digitalization project: Use case Passenger Flow",
    "authors": [
      "Erika Halme",
      "Mamia Agbese",
      "Hanna-Kaisa Alanen",
      "Jani Antikainen",
      "Marianna Jantunen",
      "Arif Ali Khan",
      "Kai-Kristian Kemell",
      "Ville Vakkuri",
      "Pekka Abrahamsson"
    ],
    "author_ids": [],
    "abstract": "Digitalization and Smart systems are part of our everyday lives today. So far\nthe development has been rapid and all the implications that comes after the\ndeployment has not been able to foresee or even assess during the development,\nespecially when ethics or trustworthiness is concerned. Artificial Intelligence\n(AI) and Autonomous Systems (AS) are the direction that software systems are\ntaking today. It is witnessed in banks, stores, internet and it is proceeding\nto transportation as well as on traveling. Autonomous maritime industry has\nalso taking this direction when taking under development in digitalization on\nfairway and port terminals. AI ethics has advanced profoundly since the machine\nlearning develop during the last decade and is now being implemented in AI\ndevelopment and workflow of software engineers. It is not an easy task and\ntools are needed to make the ethical assessment easier. This paper will review\na research in an industrial setting, where Ethically Aligned Design practice,\nEthical User Stories are used to transfer ethical requirements to ethical user\nstories to form practical solutions for project use. This project is in the\nfield of maritime industry and concentrates on digitalization of port terminals\nand this particular paper focuses on the passenger flow. Results are positive\ntowards the practice of Ethical User Stories, drawn from a large empirical data\nset.",
    "published_date": "2021-11-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.06116v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.06064v1",
    "title": "Fairness-aware Crowdsourcing of IoT Energy Services",
    "authors": [
      "Abdallah Lakhdari",
      "Athman Bouguettaya"
    ],
    "author_ids": [],
    "abstract": "We propose a Novel Fairness-Aware framework for Crowdsourcing Energy Services\n(FACES) to efficiently provision crowdsourced IoT energy services. Typically,\nefficient resource provisioning might incur an unfair resource sharing for some\nrequests. FACES, however, maximizes the utilization of the available energy\nservices by maximizing fairness across all requests. We conduct a set of\npreliminary experiments to assess the effectiveness of the proposed framework\nagainst traditional fairness-aware resource allocation algorithms. Results\ndemonstrate that the IoT energy utilization of FACES is better than FCFS and\nsimilar to Max-min fair scheduling. Experiments also show that better fairness\nis achieved among the provisioned requests using FACES compared toFCFS and\nMax-min fair scheduling.",
    "published_date": "2021-11-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.06064v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2112.01531v1",
    "title": "The MAIEI Learning Community Report",
    "authors": [
      "Brittany Wills",
      "Christina Isaicu",
      "Heather von Stackelberg",
      "Lujain Ibrahim",
      "Matthew Hutson",
      "Mitchel Fleming",
      "Nanditha Narayanamoorthy",
      "Samuel Curtis",
      "Shreyasha Paudel",
      "Sofia Trejo",
      "Tiziana Zevallos",
      "Victoria Martín del Campo",
      "Wilson Lee"
    ],
    "author_ids": [],
    "abstract": "This is a labor of the Learning Community cohort that was convened by MAIEI\nin Winter 2021 to work through and discuss important research issues in the\nfield of AI ethics from a multidisciplinary lens. The community came together\nsupported by facilitators from the MAIEI staff to vigorously debate and explore\nthe nuances of issues like bias, privacy, disinformation, accountability, and\nmore especially examining them from the perspective of industry, civil society,\nacademia, and government.\n  The outcome of these discussions is reflected in the report that you are\nreading now - an exploration of a variety of issues with deep-dive, critical\ncommentary on what has been done, what worked and what didn't, and what remains\nto be done so that we can meaningfully move forward in addressing the societal\nchallenges posed by the deployment of AI systems.\n  The chapters titled \"Design and Techno-isolationism\", \"Facebook and the\nDigital Divide: Perspectives from Myanmar, Mexico, and India\", \"Future of\nWork\", and \"Media & Communications & Ethical Foresight\" will hopefully provide\nwith you novel lenses to explore this domain beyond the usual tropes that are\ncovered in the domain of AI ethics.",
    "published_date": "2021-11-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.01531v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.05944v2",
    "title": "Multi-Objective Optimization for Value-Sensitive and Sustainable Basket Recommendations",
    "authors": [
      "Thomas Asikis"
    ],
    "author_ids": [],
    "abstract": "Sustainable consumption aims to minimize the environmental and societal\nimpact of the use of services and products. Over-consumption of services and\nproducts leads to potential natural resource exhaustion and societal\ninequalities as access to goods and services becomes more challenging. In\neveryday life, a person can simply achieve more sustainable purchases by\ndrastically changing their lifestyle choices and potentially going against\ntheir personal values or wishes. Conversely, achieving sustainable consumption\nwhile accounting for personal values is a more complex task as potential\ntrade-offs arise when trying to satisfy environmental and personal goals. This\narticle focuses on value-sensitive design of recommender systems, which enable\nconsumers to improve the sustainability of their purchases while respecting\npersonal and societal values. Value-sensitive recommendations for sustainable\nconsumption are formalized as a multi-objective optimization problem, where\neach objective represents different sustainability goals and personal values.\nNovel and existing multi-objective algorithms calculate solutions to this\nproblem. The solutions are proposed as personalized sustainable basket\nrecommendations to consumers. These recommendations are evaluated on a\nsynthetic dataset, which comprises three established real-world datasets from\nrelevant scientific and organizational reports. The synthetic dataset contains\nquantitative data on product prices, nutritional values, and environmental\nimpact metrics, such as greenhouse gas emissions and water footprint. The\nrecommended baskets are highly similar to consumer purchased baskets and\naligned with both sustainability goals and personal values relevant to health,\nexpenditure, and taste. Even when consumers would accept only a fraction of\nrecommendations, a considerable reduction of environmental impact is observed.",
    "published_date": "2021-11-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.05944v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.01236v1",
    "title": "Local Justice and the Algorithmic Allocation of Societal Resources",
    "authors": [
      "Sanmay Das"
    ],
    "author_ids": [],
    "abstract": "AI is increasingly used to aid decision-making about the allocation of scarce\nsocietal resources, for example housing for homeless people, organs for\ntransplantation, and food donations. Recently, there have been several\nproposals for how to design objectives for these systems that attempt to\nachieve some combination of fairness, efficiency, incentive compatibility, and\nsatisfactory aggregation of stakeholder preferences. This paper lays out\npossible roles and opportunities for AI in this domain, arguing for a closer\nengagement with the political philosophy literature on local justice, which\nprovides a framework for thinking about how societies have over time framed\nobjectives for such allocation problems. It also discusses how we may be able\nto integrate into this framework the opportunities and risks opened up by the\nubiquity of data and the availability of algorithms that can use them to make\naccurate predictions about the future.",
    "published_date": "2021-11-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.01236v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.05820v2",
    "title": "Multi-Task Neural Processes",
    "authors": [
      "Jiayi Shen",
      "Xiantong Zhen",
      "Marcel Worring",
      "Ling Shao"
    ],
    "author_ids": [],
    "abstract": "Neural processes have recently emerged as a class of powerful neural latent\nvariable models that combine the strengths of neural networks and stochastic\nprocesses. As they can encode contextual data in the network's function space,\nthey offer a new way to model task relatedness in multi-task learning. To study\nits potential, we develop multi-task neural processes, a new variant of neural\nprocesses for multi-task learning. In particular, we propose to explore\ntransferable knowledge from related tasks in the function space to provide\ninductive bias for improving each individual task. To do so, we derive the\nfunction priors in a hierarchical Bayesian inference framework, which enables\neach task to incorporate the shared knowledge provided by related tasks into\nits context of the prediction function. Our multi-task neural processes\nmethodologically expand the scope of vanilla neural processes and provide a new\nway of exploring task relatedness in function spaces for multi-task learning.\nThe proposed multi-task neural processes are capable of learning multiple tasks\nwith limited labeled data and in the presence of domain shift. We perform\nextensive experimental evaluations on several benchmarks for the multi-task\nregression and classification tasks. The results demonstrate the effectiveness\nof multi-task neural processes in transferring useful knowledge among tasks for\nmulti-task learning and superior performance in multi-task classification and\nbrain image segmentation.",
    "published_date": "2021-11-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.05820v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.05816v1",
    "title": "Geometric Bounds on the Fastest Mixing Markov Chain",
    "authors": [
      "Sam Olesker-Taylor",
      "Luca Zanetti"
    ],
    "author_ids": [],
    "abstract": "In the Fastest Mixing Markov Chain problem, we are given a graph $G = (V, E)$\nand desire the discrete-time Markov chain with smallest mixing time $\\tau$\nsubject to having equilibrium distribution uniform on $V$ and non-zero\ntransition probabilities only across edges of the graph.\n  It is well-known that the mixing time $\\tau_\\textsf{RW}$ of the lazy random\nwalk on $G$ is characterised by the edge conductance $\\Phi$ of $G$ via\nCheeger's inequality: $\\Phi^{-1} \\lesssim \\tau_\\textsf{RW} \\lesssim \\Phi^{-2}\n\\log |V|$. Analogously, we characterise the fastest mixing time $\\tau^\\star$\nvia a Cheeger-type inequality but for a different geometric quantity, namely\nthe vertex conductance $\\Psi$ of $G$: $\\Psi^{-1} \\lesssim \\tau^\\star \\lesssim\n\\Psi^{-2} (\\log |V|)^2$.\n  This characterisation forbids fast mixing for graphs with small vertex\nconductance. To bypass this fundamental barrier, we consider Markov chains on\n$G$ with equilibrium distribution which need not be uniform, but rather only\n$\\varepsilon$-close to uniform in total variation. We show that it is always\npossible to construct such a chain with mixing time $\\tau \\lesssim\n\\varepsilon^{-1} (\\operatorname{diam} G)^2 \\log |V|$.\n  Finally, we discuss analogous questions for continuous-time and\ntime-inhomogeneous chains.",
    "published_date": "2021-11-10T00:00:00",
    "year": 2021,
    "categories": [
      "math.PR",
      "cs.DM",
      "math.CO",
      "05C81, 60J10, 60J20, 60J27"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.05816v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.05679v1",
    "title": "Explanatory Analysis and Rectification of the Pitfalls in COVID-19 Datasets",
    "authors": [
      "Samyak Prajapati",
      "Japman Singh Monga",
      "Shaanya Singh",
      "Amrit Raj",
      "Yuvraj Singh Champawat",
      "Chandra Prakash"
    ],
    "author_ids": [],
    "abstract": "Since the onset of the COVID-19 pandemic in 2020, millions of people have\nsuccumbed to this deadly virus. Many attempts have been made to devise an\nautomated method of testing that could detect the virus. Various researchers\naround the globe have proposed deep learning based methodologies to detect the\nCOVID-19 using Chest X-Rays. However, questions have been raised on the\npresence of bias in the publicly available Chest X-Ray datasets which have been\nused by the majority of the researchers. In this paper, we propose a 2 staged\nmethodology to address this topical issue. Two experiments have been conducted\nas a part of stage 1 of the methodology to exhibit the presence of bias in the\ndatasets. Subsequently, an image segmentation, super-resolution and CNN based\npipeline along with different image augmentation techniques have been proposed\nin stage 2 of the methodology to reduce the effect of bias. InceptionResNetV2\ntrained on Chest X-Ray images that were augmented with Histogram Equalization\nfollowed by Gamma Correction when passed through the pipeline proposed in stage\n2, yielded a top accuracy of 90.47% for 3-class (Normal, Pneumonia, and\nCOVID-19) classification task.",
    "published_date": "2021-11-10T00:00:00",
    "year": 2021,
    "categories": [
      "eess.IV",
      "cs.CV",
      "I.4; I.5"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.05679v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.05564v1",
    "title": "Understanding and Mitigating Multi-Sided Exposure Bias in Recommender Systems",
    "authors": [
      "Masoud Mansoury"
    ],
    "author_ids": [],
    "abstract": "Fairness is a critical system-level objective in recommender systems that has\nbeen the subject of extensive recent research. It is especially important in\nmulti-sided recommendation platforms where it may be crucial to optimize\nutilities not just for the end user, but also for other actors such as item\nsellers or producers who desire a fair representation of their items. Existing\nsolutions do not properly address various aspects of multi-sided fairness in\nrecommendations as they may either solely have one-sided view (i.e. improving\nthe fairness only for one side), or do not appropriately measure the fairness\nfor each actor involved in the system. In this thesis, I aim at first\ninvestigating the impact of unfair recommendations on the system and how these\nunfair recommendations can negatively affect major actors in the system. Then,\nI seek to propose solutions to tackle the unfairness of recommendations. I\npropose a rating transformation technique that works as a pre-processing step\nbefore building the recommendation model to alleviate the inherent popularity\nbias in the input data and consequently to mitigate the exposure unfairness for\nitems and suppliers in the recommendation lists. Also, as another solution, I\npropose a general graph-based solution that works as a post-processing approach\nafter recommendation generation for mitigating the multi-sided exposure bias in\nthe recommendation results. For evaluation, I introduce several metrics for\nmeasuring the exposure fairness for items and suppliers, and show that these\nmetrics better capture the fairness properties in the recommendation results. I\nperform extensive experiments to evaluate the effectiveness of the proposed\nsolutions. The experiments on different publicly-available datasets and\ncomparison with various baselines confirm the superiority of the proposed\nsolutions in improving the exposure fairness for items and suppliers.",
    "published_date": "2021-11-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.05564v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.05299v1",
    "title": "Can Information Flows Suggest Targets for Interventions in Neural Circuits?",
    "authors": [
      "Praveen Venkatesh",
      "Sanghamitra Dutta",
      "Neil Mehta",
      "Pulkit Grover"
    ],
    "author_ids": [],
    "abstract": "Motivated by neuroscientific and clinical applications, we empirically\nexamine whether observational measures of information flow can suggest\ninterventions. We do so by performing experiments on artificial neural networks\nin the context of fairness in machine learning, where the goal is to induce\nfairness in the system through interventions. Using our recently developed\n$M$-information flow framework, we measure the flow of information about the\ntrue label (responsible for accuracy, and hence desirable), and separately, the\nflow of information about a protected attribute (responsible for bias, and\nhence undesirable) on the edges of a trained neural network. We then compare\nthe flow magnitudes against the effect of intervening on those edges by\npruning. We show that pruning edges that carry larger information flows about\nthe protected attribute reduces bias at the output to a greater extent. This\ndemonstrates that $M$-information flow can meaningfully suggest targets for\ninterventions, answering the title's question in the affirmative. We also\nevaluate bias-accuracy tradeoffs for different intervention strategies, to\nanalyze how one might use estimates of desirable and undesirable information\nflows (here, accuracy and bias flows) to inform interventions that preserve the\nformer while reducing the latter.",
    "published_date": "2021-11-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IT",
      "cs.AI",
      "cs.LG",
      "math.IT",
      "q-bio.NC",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.05299v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.05149v1",
    "title": "Ethically aligned Deep Learning: Unbiased Facial Aesthetic Prediction",
    "authors": [
      "Michael Danner",
      "Thomas Weber",
      "Leping Peng",
      "Tobias Gerlach",
      "Xueping Su",
      "Matthias Rätsch"
    ],
    "author_ids": [],
    "abstract": "Facial beauty prediction (FBP) aims to develop a machine that automatically\nmakes facial attractiveness assessment. In the past those results were highly\ncorrelated with human ratings, therefore also with their bias in annotating. As\nartificial intelligence can have racist and discriminatory tendencies, the\ncause of skews in the data must be identified. Development of training data and\nAI algorithms that are robust against biased information is a new challenge for\nscientists. As aesthetic judgement usually is biased, we want to take it one\nstep further and propose an Unbiased Convolutional Neural Network for FBP.\nWhile it is possible to create network models that can rate attractiveness of\nfaces on a high level, from an ethical point of view, it is equally important\nto make sure the model is unbiased. In this work, we introduce AestheticNet, a\nstate-of-the-art attractiveness prediction network, which significantly\noutperforms competitors with a Pearson Correlation of 0.9601. Additionally, we\npropose a new approach for generating a bias-free CNN to improve fairness in\nmachine learning.",
    "published_date": "2021-11-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.05149v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.05142v1",
    "title": "The Second-Level Smartphone Divide: A Typology of Smartphone Usage Based on Frequency of Use, Skills, and Types of Activities",
    "authors": [
      "Alexander Wenz",
      "Florian Keusch"
    ],
    "author_ids": [],
    "abstract": "This paper examines inequalities in the usage of smartphone technology based\non five samples of smartphone owners collected in Germany and Austria between\n2016 and 2020. We identify six distinct types of smartphone users by conducting\nlatent class analyses that classify individuals based on their frequency of\nsmartphone use, self-rated smartphone skills, and activities carried out on\ntheir smartphone. The results show that the smartphone usage types differ\nsignificantly by sociodemographic and smartphone-related characteristics: The\ntypes reflecting more frequent and diverse smartphone use are younger, have\nhigher levels of educational attainment, and are more likely to use an iPhone.\nOverall, the composition of the latent classes and their characteristics are\nrobust across samples and time.",
    "published_date": "2021-11-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.05142v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.05111v1",
    "title": "Population Protocols for Graph Class Identification Problems",
    "authors": [
      "Hiroto Yasumi",
      "Fukuhito Ooshita",
      "Michiko Inoue"
    ],
    "author_ids": [],
    "abstract": "In this paper, we focus on graph class identification problems in the\npopulation protocol model. A graph class identification problem aims to decide\nwhether a given communication graph is in the desired class (e.g. whether the\ngiven communication graph is a ring graph). Angluin et al. proposed graph class\nidentification protocols with directed graphs and designated initial states\nunder global fairness [Angluin et al., DCOSS2005]. We consider graph class\nidentification problems for undirected graphs on various assumptions such as\ninitial states of agents, fairness of the execution, and initial knowledge of\nagents. In particular, we focus on lines, rings, $k$-regular graphs, stars,\ntrees, and bipartite graphs. With designated initial states, we propose graph\nclass identification protocols for $k$-regular graphs, and trees under global\nfairness, and propose a graph class identification protocol for stars under\nweak fairness. Moreover, we show that, even if agents know the number of agents\n$n$, there is no graph class identification protocol for lines, rings,\n$k$-regular graphs, trees, or bipartite graphs under weak fairness. On the\nother hand, with arbitrary initial states, we show that there is no graph class\nidentification protocol for lines, rings, $k$-regular graphs, stars, trees, or\nbipartite graphs.",
    "published_date": "2021-11-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.05111v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.05065v2",
    "title": "Structure-Preserving Linear Quadratic Gaussian Balanced Truncation for Port-Hamiltonian Descriptor Systems",
    "authors": [
      "Tobias Breiten",
      "Philipp Schulze"
    ],
    "author_ids": [],
    "abstract": "We present a new balancing-based structure-preserving model reduction\ntechnique for linear port-Hamiltonian descriptor systems. The proposed method\nrelies on a modification of a set of two dual generalized algebraic Riccati\nequations that arise in the context of linear quadratic Gaussian balanced\ntruncation for differential algebraic systems. We derive an a priori error\nbound with respect to a right coprime factorization of the underlying transfer\nfunction thereby allowing for an estimate with respect to the gap metric. We\nfurther theoretically and numerically analyze the influence of the Hamiltonian\nand a change thereof, respectively. With regard to this change of the\nHamiltonian, we provide a novel procedure that is based on a recently\nintroduced Kalman-Yakubovich-Popov inequality for descriptor systems. Numerical\nexamples demonstrate how the quality of reduced-order models can significantly\nbe improved by first computing an extremal solution to this inequality.",
    "published_date": "2021-11-09T00:00:00",
    "year": 2021,
    "categories": [
      "math.OC",
      "cs.NA",
      "math.NA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.05065v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.05059v1",
    "title": "MMD-ReID: A Simple but Effective Solution for Visible-Thermal Person ReID",
    "authors": [
      "Chaitra Jambigi",
      "Ruchit Rawal",
      "Anirban Chakraborty"
    ],
    "author_ids": [],
    "abstract": "Learning modality invariant features is central to the problem of\nVisible-Thermal cross-modal Person Reidentification (VT-ReID), where query and\ngallery images come from different modalities. Existing works implicitly align\nthe modalities in pixel and feature spaces by either using adversarial learning\nor carefully designing feature extraction modules that heavily rely on domain\nknowledge. We propose a simple but effective framework, MMD-ReID, that reduces\nthe modality gap by an explicit discrepancy reduction constraint. MMD-ReID\ntakes inspiration from Maximum Mean Discrepancy (MMD), a widely used\nstatistical tool for hypothesis testing that determines the distance between\ntwo distributions. MMD-ReID uses a novel margin-based formulation to match\nclass-conditional feature distributions of visible and thermal samples to\nminimize intra-class distances while maintaining feature discriminability.\nMMD-ReID is a simple framework in terms of architecture and loss formulation.\nWe conduct extensive experiments to demonstrate both qualitatively and\nquantitatively the effectiveness of MMD-ReID in aligning the marginal and class\nconditional distributions, thus learning both modality-independent and\nidentity-consistent features. The proposed framework significantly outperforms\nthe state-of-the-art methods on SYSU-MM01 and RegDB datasets. Code will be\nreleased at https://github.com/vcl-iisc/MMD-ReID",
    "published_date": "2021-11-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.05059v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.04986v3",
    "title": "Unified Group Fairness on Federated Learning",
    "authors": [
      "Fengda Zhang",
      "Kun Kuang",
      "Yuxuan Liu",
      "Long Chen",
      "Chao Wu",
      "Fei Wu",
      "Jiaxun Lu",
      "Yunfeng Shao",
      "Jun Xiao"
    ],
    "author_ids": [],
    "abstract": "Federated learning (FL) has emerged as an important machine learning paradigm\nwhere a global model is trained based on the private data from distributed\nclients. However, most of existing FL algorithms cannot guarantee the\nperformance fairness towards different groups because of data distribution\nshift over groups. In this paper, we formulate the problem of unified group\nfairness on FL, where the groups can be formed by clients (including existing\nclients and newly added clients) and sensitive attribute(s). To solve this\nproblem, we first propose a general fair federated framework. Then we construct\na unified group fairness risk from the view of federated uncertainty set with\ntheoretical analyses to guarantee unified group fairness on FL. We also develop\nan efficient federated optimization algorithm named Federated Mirror Descent\nAscent with Momentum Acceleration (FMDA-M) with convergence guarantee. We\nvalidate the advantages of the FMDA-M algorithm with various kinds of\ndistribution shift settings in experiments, and the results show that FMDA-M\nalgorithm outperforms the existing fair FL algorithms on unified group\nfairness.",
    "published_date": "2021-11-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.04986v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2112.03003v1",
    "title": "What goes on inside rumour and non-rumour tweets and their reactions: A Psycholinguistic Analyses",
    "authors": [
      "Sabur Butt",
      "Shakshi Sharma",
      "Rajesh Sharma",
      "Grigori Sidorov",
      "Alexander Gelbukh"
    ],
    "author_ids": [],
    "abstract": "In recent years, the problem of rumours on online social media (OSM) has\nattracted lots of attention. Researchers have started investigating from two\nmain directions. First is the descriptive analysis of rumours and secondly,\nproposing techniques to detect (or classify) rumours. In the descriptive line\nof works, where researchers have tried to analyse rumours using NLP approaches,\nthere isnt much emphasis on psycho-linguistics analyses of social media text.\nThese kinds of analyses on rumour case studies are vital for drawing meaningful\nconclusions to mitigate misinformation. For our analysis, we explored the\nPHEME9 rumour dataset (consisting of 9 events), including source tweets (both\nrumour and non-rumour categories) and response tweets. We compared the rumour\nand nonrumour source tweets and then their corresponding reply (response)\ntweets to understand how they differ linguistically for every incident.\nFurthermore, we also evaluated if these features can be used for classifying\nrumour vs. non-rumour tweets through machine learning models. To this end, we\nemployed various classical and ensemble-based approaches. To filter out the\nhighly discriminative psycholinguistic features, we explored the SHAP AI\nExplainability tool. To summarise, this research contributes by performing an\nin-depth psycholinguistic analysis of rumours related to various kinds of\nevents.",
    "published_date": "2021-11-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.03003v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.04858v3",
    "title": "Simple odd $β$-cycle inequalities for binary polynomial optimization",
    "authors": [
      "Alberto Del Pia",
      "Matthias Walter"
    ],
    "author_ids": [],
    "abstract": "We consider the multilinear polytope which arises naturally in binary\npolynomial optimization. Del Pia and Di Gregorio introduced the class of odd\n$\\beta$-cycle inequalities valid for this polytope, showed that these generally\nhave Chv{\\'a}tal rank 2 with respect to the standard relaxation and that,\ntogether with flower inequalities, they yield a perfect formulation for cycle\nhypergraph instances. Moreover, they describe a separation algorithm in case\nthe instance is a cycle hypergraph. We introduce a weaker version, called\nsimple odd $\\beta$-cycle inequalities, for which we establish a strongly\npolynomial-time separation algorithm for arbitrary instances. These\ninequalities still have Chv{\\'a}tal rank 2 in general and still suffice to\ndescribe the multilinear polytope for cycle hypergraphs. Finally, we report\nabout computational results of our prototype implementation. The simple odd\n$\\beta$-cycle inequalities sometimes help to close more of the integrality gap\nin the experiments; however, the preliminary implementation has substantial\ncomputational cost, suggesting room for improvement in the separation\nalgorithm.",
    "published_date": "2021-11-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DM",
      "math.CO",
      "90C57",
      "G.2.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.04858v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.04830v1",
    "title": "Creative Compensation (CC): Future of Jobs with Creative Works in 3D Printing",
    "authors": [
      "Chen Liang",
      "Nahyun Kwon",
      "Jeeeun Kim"
    ],
    "author_ids": [],
    "abstract": "With the continuous growth of online 3D printing community and the\ndemocratization of 3D printers, growing number of users start sharing their own\n3D designs on open platforms, enabling a wide audience to search, download, and\n3D print models for free. Although sharing is mostly for altruistic reasons at\nfirst, open platforms had also created potential job opportunities to\ncompensate creative labors. This paper analyzes new job opportunities emerged\nin online 3D printing social platforms and patterns of seeking compensations,\nand reveals various motivations for posting creative content online. We find\nthat offering exclusive membership through subscriptions, selling final\nproducts or printing services through web stores, and using affiliate links are\nprimary means of earning profits, while there exist gaps between creators'\nexpectations and realities. We show that various socio-economic promises\nemerged, leading to a win-win situation for both creators to gain extra income\nand audiences to have access to more quality content. We also discuss future\nchallenges that need to be addressed, such as ethical use of opensource\ncontent.",
    "published_date": "2021-11-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.04830v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.04673v2",
    "title": "Information-Theoretic Bias Assessment Of Learned Representations Of Pretrained Face Recognition",
    "authors": [
      "Jiazhi Li",
      "Wael Abd-Almageed"
    ],
    "author_ids": [],
    "abstract": "As equality issues in the use of face recognition have garnered a lot of\nattention lately, greater efforts have been made to debiased deep learning\nmodels to improve fairness to minorities. However, there is still no clear\ndefinition nor sufficient analysis for bias assessment metrics. We propose an\ninformation-theoretic, independent bias assessment metric to identify degree of\nbias against protected demographic attributes from learned representations of\npretrained facial recognition systems. Our metric differs from other methods\nthat rely on classification accuracy or examine the differences between ground\ntruth and predicted labels of protected attributes predicted using a shallow\nnetwork. Also, we argue, theoretically and experimentally, that logits-level\nloss is not adequate to explain bias since predictors based on neural networks\nwill always find correlations. Further, we present a synthetic dataset that\nmitigates the issue of insufficient samples in certain cohorts. Lastly, we\nestablish a benchmark metric by presenting advantages in clear discrimination\nand small variation comparing with other metrics, and evaluate the performance\nof different debiased models with the proposed metric.",
    "published_date": "2021-11-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.04673v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.04671v1",
    "title": "Equity and Privacy: More Than Just a Tradeoff",
    "authors": [
      "David Pujol",
      "Ashwin Machanavajjhala"
    ],
    "author_ids": [],
    "abstract": "While the entire field of privacy preserving data analytics is focused on the\nprivacy-utility tradeoff, recent work has shown that privacy preserving data\npublishing can introduce different levels of utility across different\npopulation groups. It is important to understand this new tradeoff between\nprivacy and equity as privacy technology is being deployed in situations where\nthe data products will be used for research and policy making. Will marginal\npopulations see disproportionately less utility from privacy technology? If\nthere is an inequity how can we address it?",
    "published_date": "2021-11-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.04671v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.04597v4",
    "title": "Neyman-Pearson Multi-class Classification via Cost-sensitive Learning",
    "authors": [
      "Ye Tian",
      "Yang Feng"
    ],
    "author_ids": [],
    "abstract": "Most existing classification methods aim to minimize the overall\nmisclassification error rate. However, in applications such as loan default\nprediction, different types of errors can have varying consequences. To address\nthis asymmetry issue, two popular paradigms have been developed: the\nNeyman-Pearson (NP) paradigm and the cost-sensitive (CS) paradigm. Previous\nstudies on the NP paradigm have primarily focused on the binary case, while the\nmulti-class NP problem poses a greater challenge due to its unknown\nfeasibility. In this work, we tackle the multi-class NP problem by establishing\na connection with the CS problem via strong duality and propose two algorithms.\nWe extend the concept of NP oracle inequalities, crucial in binary\nclassifications, to NP oracle properties in the multi-class context. Our\nalgorithms satisfy these NP oracle properties under certain conditions.\nFurthermore, we develop practical algorithms to assess the feasibility and\nstrong duality in multi-class NP problems, which can offer practitioners the\nlandscape of a multi-class NP problem with various target error levels.\nSimulations and real data studies validate the effectiveness of our algorithms.\nTo our knowledge, this is the first study to address the multi-class NP problem\nwith theoretical guarantees. The proposed algorithms have been implemented in\nthe R package \\texttt{npcs}, which is available on CRAN.",
    "published_date": "2021-11-08T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.ME"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.04597v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.04738v1",
    "title": "HEROHE Challenge: assessing HER2 status in breast cancer without immunohistochemistry or in situ hybridization",
    "authors": [
      "Eduardo Conde-Sousa",
      "João Vale",
      "Ming Feng",
      "Kele Xu",
      "Yin Wang",
      "Vincenzo Della Mea",
      "David La Barbera",
      "Ehsan Montahaei",
      "Mahdieh Soleymani Baghshah",
      "Andreas Turzynski",
      "Jacob Gildenblat",
      "Eldad Klaiman",
      "Yiyu Hong",
      "Guilherme Aresta",
      "Teresa Araújo",
      "Paulo Aguiar",
      "Catarina Eloy",
      "António Polónia"
    ],
    "author_ids": [],
    "abstract": "Breast cancer is the most common malignancy in women, being responsible for\nmore than half a million deaths every year. As such, early and accurate\ndiagnosis is of paramount importance. Human expertise is required to diagnose\nand correctly classify breast cancer and define appropriate therapy, which\ndepends on the evaluation of the expression of different biomarkers such as the\ntransmembrane protein receptor HER2. This evaluation requires several steps,\nincluding special techniques such as immunohistochemistry or in situ\nhybridization to assess HER2 status. With the goal of reducing the number of\nsteps and human bias in diagnosis, the HEROHE Challenge was organized, as a\nparallel event of the 16th European Congress on Digital Pathology, aiming to\nautomate the assessment of the HER2 status based only on hematoxylin and eosin\nstained tissue sample of invasive breast cancer. Methods to assess HER2 status\nwere presented by 21 teams worldwide and the results achieved by some of the\nproposed methods open potential perspectives to advance the state-of-the-art.",
    "published_date": "2021-11-08T00:00:00",
    "year": 2021,
    "categories": [
      "q-bio.QM",
      "cs.CV",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.04738v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.04479v3",
    "title": "ExtremeBB: A Database for Large-Scale Research into Online Hate, Harassment, the Manosphere and Extremism",
    "authors": [
      "Anh V. Vu",
      "Lydia Wilson",
      "Yi Ting Chua",
      "Ilia Shumailov",
      "Ross Anderson"
    ],
    "author_ids": [],
    "abstract": "We introduce ExtremeBB, a textual database of over 53.5M posts made by 38.5k\nusers on 12 extremist bulletin board forums promoting online hate, harassment,\nthe manosphere and other forms of extremism. It enables large-scale analyses of\nqualitative and quantitative historical trends going back two decades:\nmeasuring hate speech and toxicity; tracing the evolution of different strands\nof extremist ideology; tracking the relationships between online subcultures,\nextremist behaviours, and real-world violence; and monitoring extremist\ncommunities in near real time. This can shed light not only on the spread of\nproblematic ideologies but also the effectiveness of interventions. ExtremeBB\ncomes with a robust ethical data-sharing regime that allows us to share data\nwith academics worldwide. Since 2020, access has been granted to 49 licensees\nin 16 research groups from 12 institutions.",
    "published_date": "2021-11-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.04479v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.04404v2",
    "title": "Robust and Information-theoretically Safe Bias Classifier against Adversarial Attacks",
    "authors": [
      "Lijia Yu",
      "Xiao-Shan Gao"
    ],
    "author_ids": [],
    "abstract": "In this paper, the bias classifier is introduced, that is, the bias part of a\nDNN with Relu as the activation function is used as a classifier. The work is\nmotivated by the fact that the bias part is a piecewise constant function with\nzero gradient and hence cannot be directly attacked by gradient-based methods\nto generate adversaries, such as FGSM. The existence of the bias classifier is\nproved and an effective training method for the bias classifier is given. It is\nproved that by adding a proper random first-degree part to the bias classifier,\nan information-theoretically safe classifier against the original-model\ngradient attack is obtained in the sense that the attack will generate a\ntotally random attacking direction. This seems to be the first time that the\nconcept of information-theoretically safe classifier is proposed. Several\nattack methods for the bias classifier are proposed and numerical experiments\nare used to show that the bias classifier is more robust than DNNs with similar\nsize against these attacks in most cases.",
    "published_date": "2021-11-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.04404v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.04380v1",
    "title": "Ethics-Based Auditing of Automated Decision-Making Systems: Intervention Points and Policy Implications",
    "authors": [
      "Jakob Mokander",
      "Maria Axente"
    ],
    "author_ids": [],
    "abstract": "Organisations increasingly use automated decision-making systems (ADMS) to\ninform decisions that affect humans and their environment. While the use of\nADMS can improve the accuracy and efficiency of decision-making processes, it\nis also coupled with ethical challenges. Unfortunately, the governance\nmechanisms currently used to oversee human decision-making often fail when\napplied to ADMS. In previous work, we proposed that ethics-based auditing\n(EBA), i.e. a structured process by which ADMS are assessed for consistency\nwith relevant principles or norms, can (a) help organisations verify claims\nabout their ADMS and (b) provide decision-subjects with justifications for the\noutputs produced by ADMS. In this article, we outline the conditions under\nwhich EBA procedures can be feasible and effective in practice. First, we argue\nthat EBA is best understood as a 'soft' yet 'formal' governance mechanism. This\nimplies that the main responsibility of auditors should be to spark ethical\ndeliberation at key intervention points throughout the software development\nprocess and ensure that there is sufficient documentation to respond to\npotential inquiries. Second, we frame ADMS as parts of larger socio-technical\nsystems to demonstrate that to be feasible and effective, EBA procedures must\nlink to intervention points that span all levels of organisational governance\nand all phases of the software lifecycle. The main function of EBA should\ntherefore be to inform, formalise, assess, and interlink existing governance\nstructures. Finally, we discuss the policy implications of our findings. To\nsupport the emergence of feasible and effective EBA procedures, policymakers\nand regulators could provide standardised reporting formats, facilitate\nknowledge exchange, provide guidance on how to resolve normative tensions, and\ncreate an independent body to oversee EBA of ADMS.",
    "published_date": "2021-11-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "C.4; C.5; J.2; K.4; K.5; K.6"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.04380v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.04309v1",
    "title": "Assessing learned features of Deep Learning applied to EEG",
    "authors": [
      "Dung Truong",
      "Scott Makeig",
      "Arnaud Delorme"
    ],
    "author_ids": [],
    "abstract": "Convolutional Neural Networks (CNNs) have achieved impressive performance on\nmany computer vision related tasks, such as object detection, image\nrecognition, image retrieval, etc. These achievements benefit from the CNNs'\noutstanding capability to learn discriminative features with deep layers of\nneuron structures and iterative training process. This has inspired the EEG\nresearch community to adopt CNN in performing EEG classification tasks.\nHowever, CNNs learned features are not immediately interpretable, causing a\nlack of understanding of the CNNs' internal working mechanism. To improve CNN\ninterpretability, CNN visualization methods are applied to translate the\ninternal features into visually perceptible patterns for qualitative analysis\nof CNN layers. Many CNN visualization methods have been proposed in the\nComputer Vision literature to interpret the CNN network structure, operation,\nand semantic concept, yet applications to EEG data analysis have been limited.\nIn this work we use 3 different methods to extract EEG-relevant features from a\nCNN trained on raw EEG data: optimal samples for each classification category,\nactivation maximization, and reverse convolution. We applied these methods to a\nhigh-performing Deep Learning model with state-of-the-art performance for an\nEEG sex classification task, and show that the model features a difference in\nthe theta frequency band. We show that visualization of a CNN model can reveal\ninteresting EEG results. Using these tools, EEG researchers using Deep Learning\ncan better identify the learned EEG features, possibly identifying new class\nrelevant biomarkers.",
    "published_date": "2021-11-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "eess.SP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.04309v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.04271v1",
    "title": "Group-Aware Threshold Adaptation for Fair Classification",
    "authors": [
      "Taeuk Jang",
      "Pengyi Shi",
      "Xiaoqian Wang"
    ],
    "author_ids": [],
    "abstract": "The fairness in machine learning is getting increasing attention, as its\napplications in different fields continue to expand and diversify. To mitigate\nthe discriminated model behaviors between different demographic groups, we\nintroduce a novel post-processing method to optimize over multiple fairness\nconstraints through group-aware threshold adaptation. We propose to learn\nadaptive classification thresholds for each demographic group by optimizing the\nconfusion matrix estimated from the probability distribution of a\nclassification model output. As we only need an estimated probability\ndistribution of model output instead of the classification model structure, our\npost-processing model can be applied to a wide range of classification models\nand improve fairness in a model-agnostic manner and ensure privacy. This even\nallows us to post-process existing fairness methods to further improve the\ntrade-off between accuracy and fairness. Moreover, our model has low\ncomputational cost. We provide rigorous theoretical analysis on the convergence\nof our optimization algorithm and the trade-off between accuracy and fairness\nof our method. Our method theoretically enables a better upper bound in near\noptimality than existing method under same condition. Experimental results\ndemonstrate that our method outperforms state-of-the-art methods and obtains\nthe result that is closest to the theoretical accuracy-fairness trade-off\nboundary.",
    "published_date": "2021-11-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.04271v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.04217v1",
    "title": "Access Management in Joint Sensing and Communication Systems: Efficiency versus Fairness",
    "authors": [
      "Trung Thanh Nguyen",
      "Khaled Elbassioni",
      "Nguyen Cong Luong",
      "Dusit Niyato",
      "Dong In Kim"
    ],
    "author_ids": [],
    "abstract": "In this paper, we consider a distributed joint sensing and communication\n(DJSC) system in which multiple radar sensors are deployed. Each sensor is\nequipped with a sensing function and a communication function, and thus it is a\nJSC node. The JSC nodes are able to perform sensing their surrounding\nenvironments, e.g., weather conditions or available spectrum. Furthermore, they\ncan cooperatively detect and track a common target. The information, i.e., of\nthe environment and target, collected by the JSC nodes is transmitted to a base\nstation (BS) for further processing. As such, different aspects of the target\nto be viewed simultaneously, which significantly improves the performance of\nthe target detection and tracking. However, both the sensing function and\ncommunication function require a certain amount of bandwidth for their\noperations, and deploying multiple JSC nodes may consume a large amount of\nbandwidth. Therefore, we investigate the bandwidth allocation problem for the\nDJSC system. In particular, we aim to optimize the bandwidth allocation to the\nsensing function and the communication function of the JSC nodes. To improve\nthe allocation efficiency while benefiting the spatial diversity advantage of\nthe DJSC systems, the objective is to maximize the sum of sensing performances,\ni.e., estimation rates, communication performances, i.e., communication data\nrates, and fairnesses of all the users. The optimization problem is non-convex\nand difficult to be solved. For this, we propose a fully polynomial time\napproximation algorithm, and we prove that the approximation algorithm can\nguarantee a near-optimal solution with an accuracy bound of $\\epsilon$.\nFurthermore, we propose to use a heuristic algorithm with lower complexity. The\nsimulation results show that both the proposed algorithms are able to achieve\nthe solutions close to the optimum in a computationally efficient fashion.",
    "published_date": "2021-11-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.04217v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.04158v1",
    "title": "A Word on Machine Ethics: A Response to Jiang et al. (2021)",
    "authors": [
      "Zeerak Talat",
      "Hagen Blix",
      "Josef Valvoda",
      "Maya Indira Ganesh",
      "Ryan Cotterell",
      "Adina Williams"
    ],
    "author_ids": [],
    "abstract": "Ethics is one of the longest standing intellectual endeavors of humanity. In\nrecent years, the fields of AI and NLP have attempted to wrangle with how\nlearning systems that interact with humans should be constrained to behave\nethically. One proposal in this vein is the construction of morality models\nthat can take in arbitrary text and output a moral judgment about the situation\ndescribed. In this work, we focus on a single case study of the recently\nproposed Delphi model and offer a critique of the project's proposed method of\nautomating morality judgments. Through an audit of Delphi, we examine broader\nissues that would be applicable to any similar attempt. We conclude with a\ndiscussion of how machine ethics could usefully proceed, by focusing on current\nand near-future uses of technology, in a way that centers around transparency,\ndemocratic values, and allows for straightforward accountability.",
    "published_date": "2021-11-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.04158v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.04095v2",
    "title": "Iterative Causal Discovery in the Possible Presence of Latent Confounders and Selection Bias",
    "authors": [
      "Raanan Y. Rohekar",
      "Shami Nisimov",
      "Yaniv Gurwicz",
      "Gal Novik"
    ],
    "author_ids": [],
    "abstract": "We present a sound and complete algorithm, called iterative causal discovery\n(ICD), for recovering causal graphs in the presence of latent confounders and\nselection bias. ICD relies on the causal Markov and faithfulness assumptions\nand recovers the equivalence class of the underlying causal graph. It starts\nwith a complete graph, and consists of a single iterative stage that gradually\nrefines this graph by identifying conditional independence (CI) between\nconnected nodes. Independence and causal relations entailed after any iteration\nare correct, rendering ICD anytime. Essentially, we tie the size of the CI\nconditioning set to its distance on the graph from the tested nodes, and\nincrease this value in the successive iteration. Thus, each iteration refines a\ngraph that was recovered by previous iterations having smaller conditioning\nsets -- a higher statistical power -- which contributes to stability. We\ndemonstrate empirically that ICD requires significantly fewer CI tests and\nlearns more accurate causal graphs compared to FCI, FCI+, and RFCI algorithms\n(code is available at https://github.com/IntelLabs/causality-lab).",
    "published_date": "2021-11-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ME",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.04095v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.03984v2",
    "title": "Proposing an Interactive Audit Pipeline for Visual Privacy Research",
    "authors": [
      "Jasmine DeHart",
      "Chenguang Xu",
      "Lisa Egede",
      "Christan Grant"
    ],
    "author_ids": [],
    "abstract": "In an ideal world, deployed machine learning models will enhance our society.\nWe hope that those models will provide unbiased and ethical decisions that will\nbenefit everyone. However, this is not always the case; issues arise during the\ndata preparation process throughout the steps leading to the models'\ndeployment. The continued use of biased datasets and processes will adversely\ndamage communities and increase the cost of fixing the problem later. In this\nwork, we walk through the decision-making process that a researcher should\nconsider before, during, and after a system deployment to understand the\nbroader impacts of their research in the community. Throughout this paper, we\ndiscuss fairness, privacy, and ownership issues in the machine learning\npipeline; we assert the need for a responsible human-over-the-loop methodology\nto bring accountability into the machine learning pipeline, and finally,\nreflect on the need to explore research agendas that have harmful societal\nimpacts. We examine visual privacy research and draw lessons that can apply\nbroadly to artificial intelligence. Our goal is to systematically analyze the\nmachine learning pipeline for visual privacy and bias issues. We hope to raise\nstakeholder (e.g., researchers, modelers, corporations) awareness as these\nissues propagate in this pipeline's various machine learning phases.",
    "published_date": "2021-11-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.03984v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.03910v1",
    "title": "FAIR Metadata: A Community-driven Vocabulary Application",
    "authors": [
      "Christopher B. Rauch",
      "Mat Kelly",
      "John A. Kunze",
      "Jane Greenberg"
    ],
    "author_ids": [],
    "abstract": "FAIR metadata is critical to supporting FAIR data overall. Transparency,\ncommunity engagement, and flexibility are key aspects of FAIR that apply to\nmetadata. This paper presents YAMZ (Yet Another Metadata Zoo), a\ncommunity-driven vocabulary application that supports FAIR. The history ofYAMZ\nand its original features are reviewed, followed by a presentation of recent\ninnovations and a discussion of how YAMZ supports FAIR principles. The\nconclusion identifies next steps and key outputs.",
    "published_date": "2021-11-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DL",
      "cs.IR",
      "H.3.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.03910v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.03874v1",
    "title": "Towards Calibrated Model for Long-Tailed Visual Recognition from Prior Perspective",
    "authors": [
      "Zhengzhuo Xu",
      "Zenghao Chai",
      "Chun Yuan"
    ],
    "author_ids": [],
    "abstract": "Real-world data universally confronts a severe class-imbalance problem and\nexhibits a long-tailed distribution, i.e., most labels are associated with\nlimited instances. The na\\\"ive models supervised by such datasets would prefer\ndominant labels, encounter a serious generalization challenge and become poorly\ncalibrated. We propose two novel methods from the prior perspective to\nalleviate this dilemma. First, we deduce a balance-oriented data augmentation\nnamed Uniform Mixup (UniMix) to promote mixup in long-tailed scenarios, which\nadopts advanced mixing factor and sampler in favor of the minority. Second,\nmotivated by the Bayesian theory, we figure out the Bayes Bias (Bayias), an\ninherent bias caused by the inconsistency of prior, and compensate it as a\nmodification on standard cross-entropy loss. We further prove that both the\nproposed methods ensure the classification calibration theoretically and\nempirically. Extensive experiments verify that our strategies contribute to a\nbetter-calibrated model, and their combination achieves state-of-the-art\nperformance on CIFAR-LT, ImageNet-LT, and iNaturalist 2018.",
    "published_date": "2021-11-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.03874v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.03687v1",
    "title": "AI and Blackness: Towards moving beyond bias and representation",
    "authors": [
      "Christopher L. Dancy",
      "P. Khalil Saucier"
    ],
    "author_ids": [],
    "abstract": "In this paper, we argue that AI ethics must move beyond the concepts of\nrace-based representation and bias, and towards those that probe the deeper\nrelations that impact how these systems are designed, developed, and deployed.\nMany recent discussions on ethical considerations of bias in AI systems have\ncentered on racial bias. We contend that antiblackness in AI requires more of\nan examination of the ontological space that provides a foundation for the\ndesign, development, and deployment of AI systems. We examine what this\ncontention means from the perspective of the sociocultural context in which AI\nsystems are designed, developed, and deployed and focus on intersections with\nanti-Black racism (antiblackness). To bring these multiple perspectives\ntogether and show an example of antiblackness in the face of attempts at\nde-biasing, we discuss results from auditing an existing open-source semantic\nnetwork (ConceptNet). We use this discussion to further contextualize\nantiblackness in design, development, and deployment of AI systems and suggest\nquestions one may ask when attempting to combat antiblackness in AI systems.",
    "published_date": "2021-11-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "I.2.0; J.4.0; K.4.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.03687v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.03638v1",
    "title": "Increasing Fairness in Predictions Using Bias Parity Score Based Loss Function Regularization",
    "authors": [
      "Bhanu Jain",
      "Manfred Huber",
      "Ramez Elmasri"
    ],
    "author_ids": [],
    "abstract": "Increasing utilization of machine learning based decision support systems\nemphasizes the need for resulting predictions to be both accurate and fair to\nall stakeholders. In this work we present a novel approach to increase a Neural\nNetwork model's fairness during training. We introduce a family of fairness\nenhancing regularization components that we use in conjunction with the\ntraditional binary-cross-entropy based accuracy loss. These loss functions are\nbased on Bias Parity Score (BPS), a score that helps quantify bias in the\nmodels with a single number. In the current work we investigate the behavior\nand effect of these regularization components on bias. We deploy them in the\ncontext of a recidivism prediction task as well as on a census-based adult\nincome dataset. The results demonstrate that with a good choice of fairness\nloss function we can reduce the trained model's bias without deteriorating\naccuracy even in unbalanced dataset.",
    "published_date": "2021-11-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.03638v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.03635v1",
    "title": "BBC-Oxford British Sign Language Dataset",
    "authors": [
      "Samuel Albanie",
      "Gül Varol",
      "Liliane Momeni",
      "Hannah Bull",
      "Triantafyllos Afouras",
      "Himel Chowdhury",
      "Neil Fox",
      "Bencie Woll",
      "Rob Cooper",
      "Andrew McParland",
      "Andrew Zisserman"
    ],
    "author_ids": [],
    "abstract": "In this work, we introduce the BBC-Oxford British Sign Language (BOBSL)\ndataset, a large-scale video collection of British Sign Language (BSL). BOBSL\nis an extended and publicly released dataset based on the BSL-1K dataset\nintroduced in previous work. We describe the motivation for the dataset,\ntogether with statistics and available annotations. We conduct experiments to\nprovide baselines for the tasks of sign recognition, sign language alignment,\nand sign language translation. Finally, we describe several strengths and\nlimitations of the data from the perspectives of machine learning and\nlinguistics, note sources of bias present in the dataset, and discuss potential\napplications of BOBSL in the context of sign language technology. The dataset\nis available at https://www.robots.ox.ac.uk/~vgg/data/bobsl/.",
    "published_date": "2021-11-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.03635v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.03514v1",
    "title": "SocialVec: Social Entity Embeddings",
    "authors": [
      "Nir Lotan",
      "Einat Minkov"
    ],
    "author_ids": [],
    "abstract": "This paper introduces SocialVec, a general framework for eliciting social\nworld knowledge from social networks, and applies this framework to Twitter.\nSocialVec learns low-dimensional embeddings of popular accounts, which\nrepresent entities of general interest, based on their co-occurrences patterns\nwithin the accounts followed by individual users, thus modeling entity\nsimilarity in socio-demographic terms. Similar to word embeddings, which\nfacilitate tasks that involve text processing, we expect social entity\nembeddings to benefit tasks of social flavor. We have learned social embeddings\nfor roughly 200,000 popular accounts from a sample of the Twitter network that\nincludes more than 1.3 million users and the accounts that they follow, and\nevaluate the resulting embeddings on two different tasks. The first task\ninvolves the automatic inference of personal traits of users from their social\nmedia profiles. In another study, we exploit SocialVec embeddings for gauging\nthe political bias of news sources in Twitter. In both cases, we prove\nSocialVec embeddings to be advantageous compared with existing entity embedding\nschemes. We will make the SocialVec entity embeddings publicly available to\nsupport further exploration of social world knowledge as reflected in Twitter.",
    "published_date": "2021-11-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.03514v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.06224v1",
    "title": "Occupational Income Inequality of Thailand: A Case Study of Exploratory Data Analysis beyond Gini Coefficient",
    "authors": [
      "Wanetha Sudswong",
      "Anon Plangprasopchok",
      "Chainarong Amornbunchornvej"
    ],
    "author_ids": [],
    "abstract": "Income inequality is an important issue that has to be solved in order to\nmake progress in our society. The study of income inequality is well received\nthrough the Gini coefficient, which is used to measure degrees of inequality in\ngeneral. While this method is effective in several aspects, the Gini\ncoefficient alone inevitably overlooks minority subpopulations (e.g.\noccupations) which results in missing undetected patterns of inequality in\nminority.\n  In this study, the surveys of incomes and occupations from more than 12\nmillions households across Thailand have been analyzed by using both Gini\ncoefficient and network densities of income domination networks to get insight\nregarding the degrees of general and occupational income inequality issues. The\nresults show that, in agricultural provinces, there are less issues in both\ntypes of inequality (low Gini coefficients and network densities), while some\nnon-agricultural provinces face an issue of occupational income inequality\n(high network densities) without any symptom of general income inequality (low\nGini coefficients). Moreover, the results also illustrate the gaps of income\ninequality using estimation statistics, which not only support whether income\ninequality exists, but that we are also able to tell the magnitudes of income\ngaps among occupations. These results cannot be obtained via Gini coefficients\nalone. This work serves as a use case of analyzing income inequality from both\ngeneral population and subpopulations perspectives that can be utilized in\nstudies of other countries.",
    "published_date": "2021-11-05T00:00:00",
    "year": 2021,
    "categories": [
      "econ.GN",
      "cs.CY",
      "q-fin.EC",
      "stat.AP",
      "62P25",
      "K.4.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.06224v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.03284v1",
    "title": "Dialogue Inspectional Summarization with Factual Inconsistency Awareness",
    "authors": [
      "Leilei Gan",
      "Yating Zhang",
      "Kun Kuang",
      "Lin Yuan",
      "Shuo Li",
      "Changlong Sun",
      "Xiaozhong Liu",
      "Fei Wu"
    ],
    "author_ids": [],
    "abstract": "Dialogue summarization has been extensively studied and applied, where the\nprior works mainly focused on exploring superior model structures to align the\ninput dialogue and the output summary. However, for professional dialogues\n(e.g., legal debate and medical diagnosis), semantic/statistical alignment can\nhardly fill the logical/factual gap between input dialogue discourse and\nsummary output with external knowledge. In this paper, we mainly investigate\nthe factual inconsistency problem for Dialogue Inspectional Summarization (DIS)\nunder non-pretraining and pretraining settings. An innovative end-to-end\ndialogue summary generation framework is proposed with two auxiliary tasks:\nExpectant Factual Aspect Regularization (EFAR) and Missing Factual Entity\nDiscrimination (MFED). Comprehensive experiments demonstrate that the proposed\nmodel can generate a more readable summary with accurate coverage of factual\naspects as well as informing the user with potential missing facts detected\nfrom the input dialogue for further human intervention.",
    "published_date": "2021-11-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.03284v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.03174v2",
    "title": "Single-Sample Prophet Inequalities via Greedy-Ordered Selection",
    "authors": [
      "Constantine Caramanis",
      "Paul Dütting",
      "Matthew Faw",
      "Federico Fusco",
      "Philip Lazos",
      "Stefano Leonardi",
      "Orestis Papadigenopoulos",
      "Emmanouil Pountourakis",
      "Rebecca Reiffenhäuser"
    ],
    "author_ids": [],
    "abstract": "We study single-sample prophet inequalities (SSPIs), i.e., prophet\ninequalities where only a single sample from each prior distribution is\navailable. Besides a direct, and optimal, SSPI for the basic single choice\nproblem [Rubinstein et al., 2020], most existing SSPI results were obtained via\nan elegant, but inherently lossy, reduction to order-oblivious secretary (OOS)\npolicies [Azar et al., 2014]. Motivated by this discrepancy, we develop an\nintuitive and versatile greedy-based technique that yields SSPIs directly\nrather than through the reduction to OOSs. Our results can be seen as\ngeneralizing and unifying a number of existing results in the area of prophet\nand secretary problems. Our algorithms significantly improve on the competitive\nguarantees for a number of interesting scenarios (including general matching\nwith edge arrivals, bipartite matching with vertex arrivals, and certain\nmatroids), and capture new settings (such as budget additive combinatorial\nauctions). Complementing our algorithmic results, we also consider mechanism\ndesign variants. Finally, we analyze the power and limitations of different\nSSPI approaches by providing a partial converse to the reduction from SSPI to\nOOS given by Azar et al.",
    "published_date": "2021-11-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DS",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.03174v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.04475v1",
    "title": "Identifying the Leading Factors of Significant Weight Gains Using a New Rule Discovery Method",
    "authors": [
      "Mina Samizadeh",
      "Jessica C Jones-Smith",
      "Bethany Sheridan",
      "Rahmatollah Beheshti"
    ],
    "author_ids": [],
    "abstract": "Overweight and obesity remain a major global public health concern and\nidentifying the individualized patterns that increase the risk of future weight\ngains has a crucial role in preventing obesity and numerous sub-sequent\ndiseases associated with obesity. In this work, we use a rule discovery method\nto study this problem, by presenting an approach that offers genuine\ninterpretability and concurrently optimizes the accuracy(being correct often)\nand support (applying to many samples) of the identified patterns.\nSpecifically, we extend an established subgroup-discovery method to generate\nthe desired rules of type X -> Y and show how top features can be extracted\nfrom the X side, functioning as the best predictors of Y. In our obesity\nproblem, X refers to the extracted features from very large and multi-site EHR\ndata, and Y indicates significant weight gains. Using our method, we also\nextensively compare the differences and inequities in patterns across 22 strata\ndetermined by the individual's gender, age, race, insurance type, neighborhood\ntype, and income level. Through extensive series of experiments, we show new\nand complementary findings regarding the predictors of future dangerous weight\ngains.",
    "published_date": "2021-11-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.04475v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.03015v2",
    "title": "Modeling Techniques for Machine Learning Fairness: A Survey",
    "authors": [
      "Mingyang Wan",
      "Daochen Zha",
      "Ninghao Liu",
      "Na Zou"
    ],
    "author_ids": [],
    "abstract": "Machine learning models are becoming pervasive in high-stakes applications.\nDespite their clear benefits in terms of performance, the models could show\ndiscrimination against minority groups and result in fairness issues in a\ndecision-making process, leading to severe negative impacts on the individuals\nand the society. In recent years, various techniques have been developed to\nmitigate the unfairness for machine learning models. Among them, in-processing\nmethods have drawn increasing attention from the community, where fairness is\ndirectly taken into consideration during model design to induce intrinsically\nfair models and fundamentally mitigate fairness issues in outputs and\nrepresentations. In this survey, we review the current progress of\nin-processing fairness mitigation techniques. Based on where the fairness is\nachieved in the model, we categorize them into explicit and implicit methods,\nwhere the former directly incorporates fairness metrics in training objectives,\nand the latter focuses on refining latent representation learning. Finally, we\nconclude the survey with a discussion of the research challenges in this\ncommunity to motivate future exploration.",
    "published_date": "2021-11-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.03015v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.02893v3",
    "title": "Symmetry-Aware Autoencoders: s-PCA and s-nlPCA",
    "authors": [
      "Simon Kneer",
      "Taraneh Sayadi",
      "Denis Sipp",
      "Peter Schmid",
      "Georgios Rigas"
    ],
    "author_ids": [],
    "abstract": "Nonlinear principal component analysis (NLPCA) via autoencoders has attracted\nattention in the dynamical systems community due to its larger compression rate\nwhen compared to linear principal component analysis (PCA). These model\nreduction methods experience an increase in the dimensionality of the latent\nspace when applied to datasets that exhibit invariant samples due to the\npresence of symmetries. In this study, we introduce a novel machine learning\nembedding for autoencoders, which uses Siamese networks and spatial transformer\nnetworks to account for discrete and continuous symmetries, respectively. The\nSiamese branches autonomously find a fundamental domain to which all samples\nare transformed, without introducing human bias. The spatial transformer\nnetwork discovers the optimal slicing template for continuous translations so\nthat invariant samples are aligned in the homogeneous direction. Thus, the\nproposed symmetry-aware autoencoder is invariant to predetermined input\ntransformations. This embedding can be employed with both linear and nonlinear\nreduction methods, which we term symmetry-aware PCA (s-PCA) and symmetry-aware\nNLPCA (s-NLPCA). We apply the proposed framework to the Kolmogorov flow to\nshowcase the capabilities for a system exhibiting both a continuous symmetry as\nwell as discrete symmetries.",
    "published_date": "2021-11-04T00:00:00",
    "year": 2021,
    "categories": [
      "physics.flu-dyn",
      "cs.LG",
      "math.DS",
      "37E99",
      "I.2.10"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.02893v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.02859v3",
    "title": "Large Scale Diverse Combinatorial Optimization: ESPN Fantasy Football Player Trades",
    "authors": [
      "Aaron Baughman",
      "Daniel Bohm",
      "Micah Forster",
      "Eduardo Morales",
      "Jeff Powell",
      "Shaun McPartlin",
      "Raja Hebbar",
      "Kavitha Yogaraj",
      "Yoshika Chhabra",
      "Sudeep Ghosh",
      "Rukhsan Ul Haq",
      "Arjun Kashyap"
    ],
    "author_ids": [],
    "abstract": "Even skilled fantasy football managers can be disappointed by their\nmid-season rosters as some players inevitably fall short of draft day\nexpectations. Team managers can quickly discover that their team has a low\nscore ceiling even if they start their best active players. A novel and diverse\ncombinatorial optimization system proposes high volume and unique player trades\nbetween complementary teams to balance trade fairness. Several algorithms\ncreate the valuation of each fantasy football player with an ensemble of\ncomputing models: Quantum Support Vector Classifier with Permutation Importance\n(QSVC-PI), Quantum Support Vector Classifier with Accumulated Local Effects\n(QSVC-ALE), Variational Quantum Circuit with Permutation Importance (VQC-PI),\nHybrid Quantum Neural Network with Permutation Importance (HQNN-PI), eXtreme\nGradient Boosting Classifier (XGB), and Subject Matter Expert (SME) rules. The\nvaluation of each player is personalized based on league rules, roster, and\nselections. The cost of trading away a player is related to a team's roster,\nsuch as the depth at a position, slot count, and position importance. Teams are\npaired together for trading based on a cosine dissimilarity score so that teams\ncan offset their strengths and weaknesses. A knapsack 0-1 algorithm computes\noutgoing players for each team. Postprocessors apply analytics and deep\nlearning models to measure 6 different objective measures about each trade.\nOver the 2020 and 2021 National Football League (NFL) seasons, a group of 24\nexperts from IBM and ESPN evaluated trade quality through 10 Football Error\nAnalysis Tool (FEAT) sessions. Our system started with 76.9% of high-quality\ntrades and was deployed for the 2021 season with 97.3% of high-quality trades.\nTo increase trade quantity, our quantum, classical, and rules-based computing\nhave 100% trade uniqueness. We use Qiskit's quantum simulators throughout our\nwork.",
    "published_date": "2021-11-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.02859v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.02825v3",
    "title": "Whistleblower protection in the digital age -- why 'anonymous' is not enough. From technology to a wider view of governance",
    "authors": [
      "Bettina Berendt",
      "Stefan Schiffner"
    ],
    "author_ids": [],
    "abstract": "When technology enters applications and processes with a long tradition of\ncontroversial societal debate, multi-faceted new ethical and legal questions\narise. This paper focusses on the process of whistleblowing, an activity with\nlarge impacts on democracy and business. Computer science can, for the first\ntime in history, provide for truly anonymous communication. We investigate this\nin relation to the values and rights of accountability, fairness and data\nprotection, focusing on opportunities and limitations of the anonymity that can\nbe provided computationally; possible consequences of outsourcing\nwhistleblowing support; and challenges for the interpretation and use of some\nrelevant laws. We conclude that to address these questions, whistleblowing and\nanonymous whistleblowing must rest on three pillars, forming a 'triangle of\nwhistleblowing protection and incentivisation' that combines anonymity in a\nformal and technical sense; whistleblower protection through laws; and other\nnorms and practices including organisational error culture.",
    "published_date": "2021-11-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.02825v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.02719v6",
    "title": "SPEEDEX: A Scalable, Parallelizable, and Economically Efficient Decentralized EXchange",
    "authors": [
      "Geoffrey Ramseyer",
      "Ashish Goel",
      "David Mazières"
    ],
    "author_ids": [],
    "abstract": "SPEEDEX is a decentralized exchange (DEX) that lets participants securely\ntrade assets without giving any single party undue control over the market.\nSPEEDEX offers several advantages over prior DEXes. It achieves high throughput\n-- over 200,000 transactions per second on 48-core servers, even with tens of\nmillions of open offers. SPEEDEX runs entirely within a Layer-1 blockchain, and\nthus achieves its scalability without fragmenting market liquidity between\nmultiple blockchains or rollups. It eliminates internal arbitrage\nopportunities, so that a direct trade from asset $\\mathcal{A}$ to asset\n$\\mathcal{B}$ always receives as good a price as trading through some third\nasset such as USD. Finally, it prevents certain front-running attacks that\nwould otherwise increase the effective bid-ask spread for small traders.\nSPEEDEX's key design insight is its use of an Arrow-Debreu exchange market\nstructure that fixes the valuation of assets for all trades in a given block of\ntransactions. We construct an algorithm, which is both asymptotically efficient\nand empirically practical, that computes these valuations while exactly\npreserving a DEX's financial correctness constraints. Not only does this market\nstructure provide fairness across trades, but it also makes trade operations\ncommutative and hence efficiently parallelizable. SPEEDEX is prototyped but not\nyet merged within the Stellar blockchain, one of the largest Layer-1\nblockchains.",
    "published_date": "2021-11-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.02719v6",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.02550v1",
    "title": "Recommendations to clarify NASA open source requirements",
    "authors": [
      "John D. Haiducek",
      "Thom R. Edwards",
      "Wade Duvall",
      "Sarah R. Cannon",
      "Kai Germaschewski",
      "Jason E. Kooi"
    ],
    "author_ids": [],
    "abstract": "The software community has specific definitions for terms such as \"open\nsource software,\" \"free software,\" and \"permissive license,\" but scientists\nproposing software development efforts to NASA are not always knowledgeable\nabout these definitions. Misunderstandings about the meaning of these terms can\nresult in problems of fairness with solicitations, because scientists who\ninterpret the terms differently than NASA intends may either needlessly limit\nthe scope of their proposed work, or unwittingly propose work that does not\ncomply with software licensing requirements. It is therefore recommended that\nNASA adopt definitions of the above terms that are in line with software\ncommunity usage, that these definitions be communicated as part of\nsolicitations to ensure a common understanding, and that proposals be required\nto identify what software licenses the proposers expect to use.",
    "published_date": "2021-11-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SE",
      "physics.space-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.02550v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.02450v1",
    "title": "Unified 3D Mesh Recovery of Humans and Animals by Learning Animal Exercise",
    "authors": [
      "Kim Youwang",
      "Kim Ji-Yeon",
      "Kyungdon Joo",
      "Tae-Hyun Oh"
    ],
    "author_ids": [],
    "abstract": "We propose an end-to-end unified 3D mesh recovery of humans and quadruped\nanimals trained in a weakly-supervised way. Unlike recent work focusing on a\nsingle target class only, we aim to recover 3D mesh of broader classes with a\nsingle multi-task model. However, there exists no dataset that can directly\nenable multi-task learning due to the absence of both human and animal\nannotations for a single object, e.g., a human image does not have animal pose\nannotations; thus, we have to devise a new way to exploit heterogeneous\ndatasets. To make the unstable disjoint multi-task learning jointly trainable,\nwe propose to exploit the morphological similarity between humans and animals,\nmotivated by animal exercise where humans imitate animal poses. We realize the\nmorphological similarity by semantic correspondences, called sub-keypoint,\nwhich enables joint training of human and animal mesh regression branches.\nBesides, we propose class-sensitive regularization methods to avoid a\nmean-shape bias and to improve the distinctiveness across multi-classes. Our\nmethod performs favorably against recent uni-modal models on various human and\nanimal datasets while being far more compact.",
    "published_date": "2021-11-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.02450v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.02447v1",
    "title": "On the Frequency Bias of Generative Models",
    "authors": [
      "Katja Schwarz",
      "Yiyi Liao",
      "Andreas Geiger"
    ],
    "author_ids": [],
    "abstract": "The key objective of Generative Adversarial Networks (GANs) is to generate\nnew data with the same statistics as the provided training data. However,\nmultiple recent works show that state-of-the-art architectures yet struggle to\nachieve this goal. In particular, they report an elevated amount of high\nfrequencies in the spectral statistics which makes it straightforward to\ndistinguish real and generated images. Explanations for this phenomenon are\ncontroversial: While most works attribute the artifacts to the generator, other\nworks point to the discriminator. We take a sober look at those explanations\nand provide insights on what makes proposed measures against high-frequency\nartifacts effective. To achieve this, we first independently assess the\narchitectures of both the generator and discriminator and investigate if they\nexhibit a frequency bias that makes learning the distribution of high-frequency\ncontent particularly problematic. Based on these experiments, we make the\nfollowing four observations: 1) Different upsampling operations bias the\ngenerator towards different spectral properties. 2) Checkerboard artifacts\nintroduced by upsampling cannot explain the spectral discrepancies alone as the\ngenerator is able to compensate for these artifacts. 3) The discriminator does\nnot struggle with detecting high frequencies per se but rather struggles with\nfrequencies of low magnitude. 4) The downsampling operations in the\ndiscriminator can impair the quality of the training signal it provides. In\nlight of these findings, we analyze proposed measures against high-frequency\nartifacts in state-of-the-art GAN training but find that none of the existing\napproaches can fully resolve spectral artifacts yet. Our results suggest that\nthere is great potential in improving the discriminator and that this could be\nkey to match the distribution of the training data more closely.",
    "published_date": "2021-11-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.02447v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.02326v2",
    "title": "End-to-End Annotator Bias Approximation on Crowdsourced Single-Label Sentiment Analysis",
    "authors": [
      "Gerhard Johann Hagerer",
      "David Szabo",
      "Andreas Koch",
      "Maria Luisa Ripoll Dominguez",
      "Christian Widmer",
      "Maximilian Wich",
      "Hannah Danner",
      "Georg Groh"
    ],
    "author_ids": [],
    "abstract": "Sentiment analysis is often a crowdsourcing task prone to subjective labels\ngiven by many annotators. It is not yet fully understood how the annotation\nbias of each annotator can be modeled correctly with state-of-the-art methods.\nHowever, resolving annotator bias precisely and reliably is the key to\nunderstand annotators' labeling behavior and to successfully resolve\ncorresponding individual misconceptions and wrongdoings regarding the\nannotation task. Our contribution is an explanation and improvement for precise\nneural end-to-end bias modeling and ground truth estimation, which reduces an\nundesired mismatch in that regard of the existing state-of-the-art.\nClassification experiments show that it has potential to improve accuracy in\ncases where each sample is annotated only by one single annotator. We provide\nthe whole source code publicly and release an own domain-specific sentiment\ndataset containing 10,000 sentences discussing organic food products. These are\ncrawled from social media and are singly labeled by 10 non-expert annotators.",
    "published_date": "2021-11-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.HC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.02326v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.02302v3",
    "title": "Selecting the number of clusters, clustering models, and algorithms. A unifying approach based on the quadratic discriminant score",
    "authors": [
      "Luca Coraggio",
      "Pietro Coretto"
    ],
    "author_ids": [],
    "abstract": "Cluster analysis requires many decisions: the clustering method and the\nimplied reference model, the number of clusters and, often, several\nhyper-parameters and algorithms' tunings. In practice, one produces several\npartitions, and a final one is chosen based on validation or selection\ncriteria. There exist an abundance of validation methods that, implicitly or\nexplicitly, assume a certain clustering notion. Moreover, they are often\nrestricted to operate on partitions obtained from a specific method. In this\npaper, we focus on groups that can be well separated by quadratic or linear\nboundaries. The reference cluster concept is defined through the quadratic\ndiscriminant score function and parameters describing clusters' size, center\nand scatter. We develop two cluster-quality criteria called quadratic scores.\nWe show that these criteria are consistent with groups generated from a general\nclass of elliptically-symmetric distributions. The quest for this type of\ngroups is common in applications. The connection with likelihood theory for\nmixture models and model-based clustering is investigated. Based on bootstrap\nresampling of the quadratic scores, we propose a selection rule that allows\nchoosing among many clustering solutions. The proposed method has the\ndistinctive advantage that it can compare partitions that cannot be compared\nwith other state-of-the-art methods. Extensive numerical experiments and the\nanalysis of real data show that, even if some competing methods turn out to be\nsuperior in some setups, the proposed methodology achieves a better overall\nperformance.",
    "published_date": "2021-11-03T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.02302v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.02275v2",
    "title": "Causal-BALD: Deep Bayesian Active Learning of Outcomes to Infer Treatment-Effects from Observational Data",
    "authors": [
      "Andrew Jesson",
      "Panagiotis Tigas",
      "Joost van Amersfoort",
      "Andreas Kirsch",
      "Uri Shalit",
      "Yarin Gal"
    ],
    "author_ids": [],
    "abstract": "Estimating personalized treatment effects from high-dimensional observational\ndata is essential in situations where experimental designs are infeasible,\nunethical, or expensive. Existing approaches rely on fitting deep models on\noutcomes observed for treated and control populations. However, when measuring\nindividual outcomes is costly, as is the case of a tumor biopsy, a\nsample-efficient strategy for acquiring each result is required. Deep Bayesian\nactive learning provides a framework for efficient data acquisition by\nselecting points with high uncertainty. However, existing methods bias training\ndata acquisition towards regions of non-overlapping support between the treated\nand control populations. These are not sample-efficient because the treatment\neffect is not identifiable in such regions. We introduce causal, Bayesian\nacquisition functions grounded in information theory that bias data acquisition\ntowards regions with overlapping support to maximize sample efficiency for\nlearning personalized treatment effects. We demonstrate the performance of the\nproposed acquisition strategies on synthetic and semi-synthetic datasets IHDP\nand CMNIST and their extensions, which aim to simulate common dataset biases\nand pathologies.",
    "published_date": "2021-11-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.02275v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.02251v1",
    "title": "Fair Mutual Exclusion for N Processes (extended version)",
    "authors": [
      "Yousra Hafidi",
      "Jeroen J. A. Keiren",
      "Jan Friso Groote"
    ],
    "author_ids": [],
    "abstract": "Peterson's mutual exclusion algorithm for two processes has been generalized\nto $N$ processes in various ways. As far as we know, no such generalization is\nstarvation free without making any fairness assumptions. In this paper, we\nstudy the generalization of Peterson's algorithm to $N$ processes using a\ntournament tree. Using the mCRL2 language and toolset we prove that it is not\nstarvation free unless weak fairness assumptions are incorporated. Inspired by\nthe counterexample for starvation freedom, we propose a fair $N$-process\ngeneralization of Peterson's algorithm. We use model checking to show that our\nnew algorithm is correct for small $N$. For arbitrary $N$, model checking is\ninfeasible due to the state space explosion problem, and instead, we present a\ngeneral proof that, for $N \\geq 4$, when a process requests access to the\ncritical section, other processes can enter first at most $(N-1)(N-2)$ times.",
    "published_date": "2021-11-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LO",
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.02251v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.02244v1",
    "title": "Exploring Explainable AI in the Financial Sector: Perspectives of Banks and Supervisory Authorities",
    "authors": [
      "Ouren Kuiper",
      "Martin van den Berg",
      "Joost van der Burgt",
      "Stefan Leijnen"
    ],
    "author_ids": [],
    "abstract": "Explainable artificial intelligence (xAI) is seen as a solution to making AI\nsystems less of a black box. It is essential to ensure transparency, fairness,\nand accountability, which are especially paramount in the financial sector. The\naim of this study was a preliminary investigation of the perspectives of\nsupervisory authorities and regulated entities regarding the application of xAI\nin the fi-nancial sector. Three use cases (consumer credit, credit risk, and\nanti-money laundering) were examined using semi-structured interviews at three\nbanks and two supervisory authorities in the Netherlands. We found that for the\ninvestigated use cases a disparity exists between supervisory authorities and\nbanks regarding the desired scope of explainability of AI systems. We argue\nthat the financial sector could benefit from clear differentiation between\ntechnical AI (model) ex-plainability requirements and explainability\nrequirements of the broader AI system in relation to applicable laws and\nregulations.",
    "published_date": "2021-11-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.02244v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.02153v2",
    "title": "Local Structure and effective Dimensionality of Time Series Data Sets",
    "authors": [
      "Monika Doerfler",
      "Franz Luef",
      "Eirik Skrettingland"
    ],
    "author_ids": [],
    "abstract": "The goal of this paper is to develop novel tools for understanding the local\nstructure of systems of functions, e.g. time-series data points, such as the\ntotal correlation function, the Cohen class of the data set, the data operator\nand the average lack of concentration. The Cohen class of the data operator\ngives a time-frequency representation of the data set. Furthermore, we show\nthat the von Neumann entropy of the data operator captures local features of\nthe data set and that it is related to the notion of the effective\ndimensionality. The accumulated Cohen class of the data operator gives us a\nlow-dimensional representation of the data set and we quantify this in terms of\nthe average lack of concentration and the von Neumann entropy of the data\noperator by an application of a Berezin-Lieb inequality. The framework for our\napproach is provided by quantum harmonic analysis.",
    "published_date": "2021-11-03T00:00:00",
    "year": 2021,
    "categories": [
      "math.FA",
      "cs.NA",
      "math-ph",
      "math.MP",
      "math.NA",
      "46N40, 47B93"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.02153v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.02058v1",
    "title": "Rethinking the Image Feature Biases Exhibited by Deep CNN Models",
    "authors": [
      "Dawei Dai",
      "Yutang Li",
      "Huanan Bao",
      "Sy Xia",
      "Guoyin Wang",
      "Xiaoli Ma"
    ],
    "author_ids": [],
    "abstract": "In recent years, convolutional neural networks (CNNs) have been applied\nsuccessfully in many fields. However, such deep neural models are still\nregarded as black box in most tasks. One of the fundamental issues underlying\nthis problem is understanding which features are most influential in image\nrecognition tasks and how they are processed by CNNs. It is widely accepted\nthat CNN models combine low-level features to form complex shapes until the\nobject can be readily classified, however, several recent studies have argued\nthat texture features are more important than other features. In this paper, we\nassume that the importance of certain features varies depending on specific\ntasks, i.e., specific tasks exhibit a feature bias. We designed two\nclassification tasks based on human intuition to train deep neural models to\nidentify anticipated biases. We devised experiments comprising many tasks to\ntest these biases for the ResNet and DenseNet models. From the results, we\nconclude that (1) the combined effect of certain features is typically far more\ninfluential than any single feature; (2) in different tasks, neural models can\nperform different biases, that is, we can design a specific task to make a\nneural model biased toward a specific anticipated feature.",
    "published_date": "2021-11-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.02058v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.02038v5",
    "title": "Fair-SSL: Building fair ML Software with less data",
    "authors": [
      "Joymallya Chakraborty",
      "Suvodeep Majumder",
      "Huy Tu"
    ],
    "author_ids": [],
    "abstract": "Ethical bias in machine learning models has become a matter of concern in the\nsoftware engineering community. Most of the prior software engineering works\nconcentrated on finding ethical bias in models rather than fixing it. After\nfinding bias, the next step is mitigation. Prior researchers mainly tried to\nuse supervised approaches to achieve fairness. However, in the real world,\ngetting data with trustworthy ground truth is challenging and also ground truth\ncan contain human bias. Semi-supervised learning is a machine learning\ntechnique where, incrementally, labeled data is used to generate pseudo-labels\nfor the rest of the data (and then all that data is used for model training).\nIn this work, we apply four popular semi-supervised techniques as\npseudo-labelers to create fair classification models. Our framework, Fair-SSL,\ntakes a very small amount (10%) of labeled data as input and generates\npseudo-labels for the unlabeled data. We then synthetically generate new data\npoints to balance the training data based on class and protected attribute as\nproposed by Chakraborty et al. in FSE 2021. Finally, the classification model\nis trained on the balanced pseudo-labeled data and validated on test data.\nAfter experimenting on ten datasets and three learners, we find that Fair-SSL\nachieves similar performance as three state-of-the-art bias mitigation\nalgorithms. That said, the clear advantage of Fair-SSL is that it requires only\n10% of the labeled training data. To the best of our knowledge, this is the\nfirst SE work where semi-supervised techniques are used to fight against\nethical bias in SE ML models.",
    "published_date": "2021-11-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SE",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.02038v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.01968v2",
    "title": "A Survey on Epistemic (Model) Uncertainty in Supervised Learning: Recent Advances and Applications",
    "authors": [
      "Xinlei Zhou",
      "Han Liu",
      "Farhad Pourpanah",
      "Tieyong Zeng",
      "Xizhao Wang"
    ],
    "author_ids": [],
    "abstract": "Quantifying the uncertainty of supervised learning models plays an important\nrole in making more reliable predictions. Epistemic uncertainty, which usually\nis due to insufficient knowledge about the model, can be reduced by collecting\nmore data or refining the learning models. Over the last few years, scholars\nhave proposed many epistemic uncertainty handling techniques which can be\nroughly grouped into two categories, i.e., Bayesian and ensemble. This paper\nprovides a comprehensive review of epistemic uncertainty learning techniques in\nsupervised learning over the last five years. As such, we, first, decompose the\nepistemic uncertainty into bias and variance terms. Then, a hierarchical\ncategorization of epistemic uncertainty learning techniques along with their\nrepresentative models is introduced. In addition, several applications such as\ncomputer vision (CV) and natural language processing (NLP) are presented,\nfollowed by a discussion on research gaps and possible future research\ndirections.",
    "published_date": "2021-11-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.01968v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.01874v2",
    "title": "Numerical Smoothing with Hierarchical Adaptive Sparse Grids and Quasi-Monte Carlo Methods for Efficient Option Pricing",
    "authors": [
      "Christian Bayer",
      "Chiheb Ben Hammouda",
      "Raúl Tempone"
    ],
    "author_ids": [],
    "abstract": "When approximating the expectations of a functional of a solution to a\nstochastic differential equation, the numerical performance of deterministic\nquadrature methods, such as sparse grid quadrature and quasi-Monte Carlo (QMC)\nmethods, may critically depend on the regularity of the integrand. To overcome\nthis issue and improve the regularity structure of the problem, we consider\ncases in which analytic smoothing (bias-free mollification) cannot be performed\nand introduce a novel numerical smoothing approach by combining a root-finding\nmethod with a one-dimensional numerical integration with respect to a single\nwell-chosen variable. We prove that, under appropriate conditions, the\nresulting function of the remaining variables is highly smooth, potentially\naffording the improved efficiency of adaptive sparse grid quadrature (ASGQ) and\nQMC methods, particularly when combined with hierarchical transformations (ie.,\nthe Brownian bridge and Richardson extrapolation on the weak error). This\napproach facilitates the effective treatment of high dimensionality. Our study\nis motivated by option pricing problems, focusing on dynamics where the\ndiscretization of the asset price is necessary. Based on our analysis and\nnumerical experiments, we demonstrate the advantages of combining numerical\nsmoothing with the ASGQ and QMC methods over these methods without smoothing\nand the Monte Carlo approach. Finally, our approach is generic and can be\napplied to solve a broad class of problems, particularly approximating\ndistribution functions, computing financial Greeks, and estimating risk\nquantities.",
    "published_date": "2021-11-02T00:00:00",
    "year": 2021,
    "categories": [
      "q-fin.CP",
      "cs.CC",
      "cs.CE",
      "cs.NA",
      "math.NA",
      "q-fin.PR",
      "65C05, 65D30, 65D32, 65Y20, 91G20, 91G60"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.01874v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.01872v3",
    "title": "Towards Fairness-Aware Federated Learning",
    "authors": [
      "Yuxin Shi",
      "Han Yu",
      "Cyril Leung"
    ],
    "author_ids": [],
    "abstract": "Recent advances in Federated Learning (FL) have brought large-scale\ncollaborative machine learning opportunities for massively distributed clients\nwith performance and data privacy guarantees. However, most current works focus\non the interest of the central controller in FL,and overlook the interests of\nthe FL clients. This may result in unfair treatment of clients that discourages\nthem from actively participating in the learning process and damages the\nsustainability of the FL ecosystem. Therefore, the topic of ensuring fairness\nin FL is attracting a great deal of research interest. In recent years, diverse\nFairness-Aware FL (FAFL) approaches have been proposed in an effort to achieve\nfairness in FL from different perspectives. However, there is no comprehensive\nsurvey that helps readers gain insight into this interdisciplinary field. This\npaper aims to provide such a survey. By examining the fundamental and\nsimplifying assumptions, as well as the notions of fairness adopted by existing\nliterature in this field, we propose a taxonomy of FAFL approaches covering\nmajor steps in FL, including client selection, optimization, contribution\nevaluation and incentive distribution. In addition, we discuss the main metrics\nfor experimentally evaluating the performance of FAFL approaches, and suggest\npromising future research directions towards FAFL.",
    "published_date": "2021-11-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.01872v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.01705v1",
    "title": "AI Ethics Statements -- Analysis and lessons learnt from NeurIPS Broader Impact Statements",
    "authors": [
      "Carolyn Ashurst",
      "Emmie Hine",
      "Paul Sedille",
      "Alexis Carlier"
    ],
    "author_ids": [],
    "abstract": "Ethics statements have been proposed as a mechanism to increase transparency\nand promote reflection on the societal impacts of published research. In 2020,\nthe machine learning (ML) conference NeurIPS broke new ground by requiring that\nall papers include a broader impact statement. This requirement was removed in\n2021, in favour of a checklist approach. The 2020 statements therefore provide\na unique opportunity to learn from the broader impact experiment: to\ninvestigate the benefits and challenges of this and similar governance\nmechanisms, as well as providing an insight into how ML researchers think about\nthe societal impacts of their own work. Such learning is needed as NeurIPS and\nother venues continue to question and adapt their policies. To enable this, we\nhave created a dataset containing the impact statements from all NeurIPS 2020\npapers, along with additional information such as affiliation type, location\nand subject area, and a simple visualisation tool for exploration. We also\nprovide an initial quantitative analysis of the dataset, covering\nrepresentation, engagement, common themes, and willingness to discuss potential\nharms alongside benefits. We investigate how these vary by geography,\naffiliation type and subject area. Drawing on these findings, we discuss the\npotential benefits and negative outcomes of ethics statement requirements, and\ntheir possible causes and associated challenges. These lead us to several\nlessons to be learnt from the 2020 requirement: (i) the importance of creating\nthe right incentives, (ii) the need for clear expectations and guidance, and\n(iii) the importance of transparency and constructive deliberation. We\nencourage other researchers to use our dataset to provide additional analysis,\nto further our understanding of how researchers responded to this requirement,\nand to investigate the benefits and challenges of this and related mechanisms.",
    "published_date": "2021-11-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.01705v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.01566v3",
    "title": "Strategyproof and Proportionally Fair Facility Location",
    "authors": [
      "Haris Aziz",
      "Alexander Lam",
      "Barton E. Lee",
      "Toby Walsh"
    ],
    "author_ids": [],
    "abstract": "We focus on a simple, one-dimensional collective decision problem (often\nreferred to as the facility location problem) and explore issues of\nstrategyproofness and proportionality-based fairness. We introduce and analyze\na hierarchy of proportionality-based fairness axioms of varying strength:\nIndividual Fair Share (IFS), Unanimous Fair Share (UFS), Proportionality (as in\nFreeman et al, 2021), and Proportional Fairness (PF). For each axiom, we\ncharacterize the family of mechanisms that satisfy the axiom and\nstrategyproofness. We show that imposing strategyproofness renders many of the\naxioms to be equivalent: the family of mechanisms that satisfy proportionality,\nunanimity, and strategyproofness is equivalent to the family of mechanisms that\nsatisfy UFS and strategyproofness, which, in turn, is equivalent to the family\nof mechanisms that satisfy PF and strategyproofness. Furthermore, there is a\nunique such mechanism: the Uniform Phantom mechanism, which is studied in\nFreeman et al. (2021). We also characterize the outcomes of the Uniform Phantom\nmechanism as the unique (pure) equilibrium outcome for any mechanism that\nsatisfies continuity, strict monotonicity, and UFS. Finally, we analyze the\napproximation guarantees, in terms of optimal social welfare and minimum total\ncost, obtained by mechanisms that are strategyproof and satisfy each\nproportionality-based fairness axiom. We show that the Uniform Phantom\nmechanism provides the best approximation of the optimal social welfare (and\nalso minimum total cost) among all mechanisms that satisfy UFS.",
    "published_date": "2021-11-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.MA",
      "econ.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.01566v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.01306v1",
    "title": "On the Current and Emerging Challenges of Developing Fair and Ethical AI Solutions in Financial Services",
    "authors": [
      "Eren Kurshan",
      "Jiahao Chen",
      "Victor Storchan",
      "Hongda Shen"
    ],
    "author_ids": [],
    "abstract": "Artificial intelligence (AI) continues to find more numerous and more\ncritical applications in the financial services industry, giving rise to fair\nand ethical AI as an industry-wide objective. While many ethical principles and\nguidelines have been published in recent years, they fall short of addressing\nthe serious challenges that model developers face when building ethical AI\nsolutions. We survey the practical and overarching issues surrounding model\ndevelopment, from design and implementation complexities, to the shortage of\ntools, and the lack of organizational constructs. We show how practical\nconsiderations reveal the gaps between high-level principles and concrete,\ndeployed AI applications, with the aim of starting industry-wide conversations\ntoward solution approaches.",
    "published_date": "2021-11-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CE",
      "68T01",
      "K.4; K.5.2; I.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.01306v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.01254v3",
    "title": "Unique Games hardness of Quantum Max-Cut, and a conjectured vector-valued Borell's inequality",
    "authors": [
      "Yeongwoo Hwang",
      "Joe Neeman",
      "Ojas Parekh",
      "Kevin Thompson",
      "John Wright"
    ],
    "author_ids": [],
    "abstract": "The Gaussian noise stability of a function $f:\\mathbb{R}^n \\to \\{-1, 1\\}$ is\nthe expected value of $f(\\boldsymbol{x}) \\cdot f(\\boldsymbol{y})$ over\n$\\rho$-correlated Gaussian random variables $\\boldsymbol{x}$ and\n$\\boldsymbol{y}$. Borell's inequality states that for $-1 \\leq \\rho \\leq 0$,\nthis is minimized by the halfspace $f(x) = \\mathrm{sign}(x_1)$. In this work,\nwe generalize this result to hold for functions $f:\\mathbb{R}^n \\to S^{k-1}$\nwhich output $k$-dimensional unit vectors. Our main conjecture, which we call\nthe $\\textit{vector-valued Borell's inequality}$, asserts that the expected\nvalue of $\\langle f(\\boldsymbol{x}), f(\\boldsymbol{y})\\rangle$ is minimized by\nthe function $f(x) = x_{\\leq k} / \\Vert x_{\\leq k} \\Vert$, where $x_{\\leq k} =\n(x_1, \\ldots, x_k)$. We give several pieces of evidence in favor of this\nconjecture, including a proof that it does indeed hold in the special case of\n$n = k$.\n  As an application of this conjecture, we show that it implies several\nhardness of approximation results for a special case of the local Hamiltonian\nproblem related to the anti-ferromagnetic Heisenberg model known as Quantum\nMax-Cut. This can be viewed as a natural quantum analogue of the classical\nMax-Cut problem and has been proposed as a useful testbed for developing\nalgorithms. We show the following, assuming our conjecture:\n  (1) The integrality gap of the basic SDP is $0.498$, matching an existing\nrounding algorithm. Combined with existing results, this shows that the basic\nSDP does not achieve the optimal approximation ratio.\n  (2) It is Unique Games-hard (UG-hard) to compute a\n$(0.956+\\varepsilon)$-approximation to the value of the best product state,\nmatching an existing approximation algorithm.\n  (3) It is UG-hard to compute a $(0.956+\\varepsilon)$-approximation to the\nvalue of the best (possibly entangled) state.",
    "published_date": "2021-11-01T00:00:00",
    "year": 2021,
    "categories": [
      "quant-ph",
      "cs.CC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.01254v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.01235v2",
    "title": "Low-Cost Algorithmic Recourse for Users With Uncertain Cost Functions",
    "authors": [
      "Prateek Yadav",
      "Peter Hase",
      "Mohit Bansal"
    ],
    "author_ids": [],
    "abstract": "People affected by machine learning model decisions may benefit greatly from\naccess to recourses, i.e. suggestions about what features they could change to\nreceive a more favorable decision from the model. Current approaches try to\noptimize for the cost incurred by users when adopting a recourse, but they\nassume that all users share the same cost function. This is an unrealistic\nassumption because users might have diverse preferences about their willingness\nto change certain features. In this work, we introduce a new method for\nidentifying recourse sets for users which does not assume that users'\npreferences are known in advance. We propose an objective function, Expected\nMinimum Cost (EMC), based on two key ideas: (1) when presenting a set of\noptions to a user, there only needs to be one low-cost solution that the user\ncould adopt; (2) when we do not know the user's true cost function, we can\napproximately optimize for user satisfaction by first sampling plausible cost\nfunctions from a distribution, then finding a recourse set that achieves a good\ncost for these samples. We optimize EMC with a novel discrete optimization\nalgorithm, Cost Optimized Local Search (COLS), which is guaranteed to improve\nthe recourse set quality over iterations. Experimental evaluation on popular\nreal-world datasets with simulated users demonstrates that our method satisfies\nup to 25.89 percentage points more users compared to strong baseline methods,\nwhile, the human evaluation shows that our recourses are preferred more than\ntwice as often as the strongest baseline recourses. Finally, using standard\nfairness metrics we show that our method can provide more fair solutions across\ndemographic groups than baselines. We provide our source code at:\nhttps://github.com/prateeky2806/EMC-COLS-recourse",
    "published_date": "2021-11-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.01235v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.01231v1",
    "title": "Switch Point biased Self-Training: Re-purposing Pretrained Models for Code-Switching",
    "authors": [
      "Parul Chopra",
      "Sai Krishna Rallabandi",
      "Alan W Black",
      "Khyathi Raghavi Chandu"
    ],
    "author_ids": [],
    "abstract": "Code-switching (CS), a ubiquitous phenomenon due to the ease of communication\nit offers in multilingual communities still remains an understudied problem in\nlanguage processing. The primary reasons behind this are: (1) minimal efforts\nin leveraging large pretrained multilingual models, and (2) the lack of\nannotated data. The distinguishing case of low performance of multilingual\nmodels in CS is the intra-sentence mixing of languages leading to switch\npoints. We first benchmark two sequence labeling tasks -- POS and NER on 4\ndifferent language pairs with a suite of pretrained models to identify the\nproblems and select the best performing model, char-BERT, among them\n(addressing (1)). We then propose a self training method to repurpose the\nexisting pretrained models using a switch-point bias by leveraging unannotated\ndata (addressing (2)). We finally demonstrate that our approach performs well\non both tasks by reducing the gap between the switch point performance while\nretaining the overall performance on two distinct language pairs in both the\ntasks. Our code is available here:\nhttps://github.com/PC09/EMNLP2021-Switch-Point-biased-Self-Training.",
    "published_date": "2021-11-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.01231v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.01201v2",
    "title": "Unintended Selection: Persistent Qualification Rate Disparities and Interventions",
    "authors": [
      "Reilly Raab",
      "Yang Liu"
    ],
    "author_ids": [],
    "abstract": "Realistically -- and equitably -- modeling the dynamics of group-level\ndisparities in machine learning remains an open problem. In particular, we\ndesire models that do not suppose inherent differences between artificial\ngroups of people -- but rather endogenize disparities by appeal to unequal\ninitial conditions of insular subpopulations. In this paper, agents each have a\nreal-valued feature $X$ (e.g., credit score) informed by a \"true\" binary label\n$Y$ representing qualification (e.g., for a loan). Each agent alternately (1)\nreceives a binary classification label $\\hat{Y}$ (e.g., loan approval) from a\nBayes-optimal machine learning classifier observing $X$ and (2) may update\ntheir qualification $Y$ by imitating successful strategies (e.g., seek a raise)\nwithin an isolated group $G$ of agents to which they belong. We consider the\ndisparity of qualification rates $\\Pr(Y=1)$ between different groups and how\nthis disparity changes subject to a sequence of Bayes-optimal classifiers\nrepeatedly retrained on the global population. We model the evolving\nqualification rates of each subpopulation (group) using the replicator\nequation, which derives from a class of imitation processes. We show that\ndifferences in qualification rates between subpopulations can persist\nindefinitely for a set of non-trivial equilibrium states due to uniformed\nclassifier deployments, even when groups are identical in all aspects except\ninitial qualification densities. We next simulate the effects of commonly\nproposed fairness interventions on this dynamical system along with a new\nfeedback control mechanism capable of permanently eliminating group-level\nqualification rate disparities. We conclude by discussing the limitations of\nour model and findings and by outlining potential future work.",
    "published_date": "2021-11-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GT",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.01201v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.01108v2",
    "title": "Resource-Efficient Federated Learning",
    "authors": [
      "Ahmed M. Abdelmoniem",
      "Atal Narayan Sahu",
      "Marco Canini",
      "Suhaib A. Fahmy"
    ],
    "author_ids": [],
    "abstract": "Federated Learning (FL) enables distributed training by learners using local\ndata, thereby enhancing privacy and reducing communication. However, it\npresents numerous challenges relating to the heterogeneity of the data\ndistribution, device capabilities, and participant availability as deployments\nscale, which can impact both model convergence and bias. Existing FL schemes\nuse random participant selection to improve fairness; however, this can result\nin inefficient use of resources and lower quality training. In this work, we\nsystematically address the question of resource efficiency in FL, showing the\nbenefits of intelligent participant selection, and incorporation of updates\nfrom straggling participants. We demonstrate how these factors enable resource\nefficiency while also improving trained model quality.",
    "published_date": "2021-11-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.01108v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.00826v4",
    "title": "Reproducibility as a Mechanism for Teaching Fairness, Accountability, Confidentiality, and Transparency in Artificial Intelligence",
    "authors": [
      "Ana Lucic",
      "Maurits Bleeker",
      "Sami Jullien",
      "Samarth Bhargav",
      "Maarten de Rijke"
    ],
    "author_ids": [],
    "abstract": "In this work, we explain the setup for a technical, graduate-level course on\nFairness, Accountability, Confidentiality, and Transparency in Artificial\nIntelligence (FACT-AI) at the University of Amsterdam, which teaches FACT-AI\nconcepts through the lens of reproducibility. The focal point of the course is\na group project based on reproducing existing FACT-AI algorithms from top AI\nconferences and writing a corresponding report. In the first iteration of the\ncourse, we created an open source repository with the code implementations from\nthe group projects. In the second iteration, we encouraged students to submit\ntheir group projects to the Machine Learning Reproducibility Challenge,\nresulting in 9 reports from our course being accepted for publication in the\nReScience journal. We reflect on our experience teaching the course over two\nyears, where one year coincided with a global pandemic, and propose guidelines\nfor teaching FACT-AI through reproducibility in graduate-level AI study\nprograms. We hope this can be a useful resource for instructors who want to set\nup similar courses in the future.",
    "published_date": "2021-11-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.00826v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.00814v1",
    "title": "Statistical quantification of confounding bias in predictive modelling",
    "authors": [
      "Tamas Spisak"
    ],
    "author_ids": [],
    "abstract": "The lack of non-parametric statistical tests for confounding bias\nsignificantly hampers the development of robust, valid and generalizable\npredictive models in many fields of research. Here I propose the partial and\nfull confounder tests, which, for a given confounder variable, probe the null\nhypotheses of unconfounded and fully confounded models, respectively. The tests\nprovide a strict control for Type I errors and high statistical power, even for\nnon-normally and non-linearly dependent predictions, often seen in machine\nlearning. Applying the proposed tests on models trained on functional brain\nconnectivity data from the Human Connectome Project and the Autism Brain\nImaging Data Exchange dataset reveals confounders that were previously\nunreported or found to be hard to correct for with state-of-the-art confound\nmitigation approaches. The tests, implemented in the package mlconfound\n(https://mlconfound.readthedocs.io), can aid the assessment and improvement of\nthe generalizability and neurobiological validity of predictive models and,\nthereby, foster the development of clinically useful machine learning\nbiomarkers.",
    "published_date": "2021-11-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "q-bio.QM",
      "stat.ML",
      "G.3; I.2.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.00814v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.00754v1",
    "title": "Few-shot learning with improved local representations via bias rectify module",
    "authors": [
      "Chao Dong",
      "Qi Ye",
      "Wenchao Meng",
      "Kaixiang Yang"
    ],
    "author_ids": [],
    "abstract": "Recent approaches based on metric learning have achieved great progress in\nfew-shot learning. However, most of them are limited to image-level\nrepresentation manners, which fail to properly deal with the intra-class\nvariations and spatial knowledge and thus produce undesirable performance. In\nthis paper we propose a Deep Bias Rectify Network (DBRN) to fully exploit the\nspatial information that exists in the structure of the feature\nrepresentations. We first employ a bias rectify module to alleviate the adverse\nimpact caused by the intra-class variations. bias rectify module is able to\nfocus on the features that are more discriminative for classification by given\ndifferent weights. To make full use of the training data, we design a prototype\naugment mechanism that can make the prototypes generated from the support set\nto be more representative. To validate the effectiveness of our method, we\nconducted extensive experiments on various popular few-shot classification\nbenchmarks and our methods can outperform state-of-the-art methods.",
    "published_date": "2021-11-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.00754v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.00735v1",
    "title": "Calibrating Explore-Exploit Trade-off for Fair Online Learning to Rank",
    "authors": [
      "Yiling Jia",
      "Hongning Wang"
    ],
    "author_ids": [],
    "abstract": "Online learning to rank (OL2R) has attracted great research interests in\nrecent years, thanks to its advantages in avoiding expensive relevance labeling\nas required in offline supervised ranking model learning. Such a solution\nexplores the unknowns (e.g., intentionally present selected results on top\npositions) to improve its relevance estimation. This however triggers concerns\non its ranking fairness: different groups of items might receive differential\ntreatments during the course of OL2R. But existing fair ranking solutions\nusually require the knowledge of result relevance or a performing ranker\nbeforehand, which contradicts with the setting of OL2R and thus cannot be\ndirectly applied to guarantee fairness.\n  In this work, we propose a general framework to achieve fairness defined by\ngroup exposure in OL2R. The key idea is to calibrate exploration and\nexploitation for fairness control, relevance learning and online ranking\nquality. In particular, when the model is exploring a set of results for\nrelevance feedback, we confine the exploration within a subset of random\npermutations, where fairness across groups is maintained while the feedback is\nstill unbiased. Theoretically we prove such a strategy introduces minimum\ndistortion in OL2R's regret to obtain fairness. Extensive empirical analysis is\nperformed on two public learning to rank benchmark datasets to demonstrate the\neffectiveness of the proposed solution compared to existing fair OL2R\nsolutions.",
    "published_date": "2021-11-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.00735v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.00601v1",
    "title": "Explainable Artificial Intelligence for Smart City Application: A Secure and Trusted Platform",
    "authors": [
      "M. Humayn Kabir",
      "Khondokar Fida Hasan",
      "Mohammad Kamrul Hasan",
      "Keyvan Ansari"
    ],
    "author_ids": [],
    "abstract": "Artificial Intelligence (AI) is one of the disruptive technologies that is\nshaping the future. It has growing applications for data-driven decisions in\nmajor smart city solutions, including transportation, education, healthcare,\npublic governance, and power systems. At the same time, it is gaining\npopularity in protecting critical cyber infrastructure from cyber threats,\nattacks, damages, or unauthorized access. However, one of the significant\nissues of those traditional AI technologies (e.g., deep learning) is that the\nrapid progress in complexity and sophistication propelled and turned out to be\nuninterpretable black boxes. On many occasions, it is very challenging to\nunderstand the decision and bias to control and trust systems' unexpected or\nseemingly unpredictable outputs. It is acknowledged that the loss of control\nover interpretability of decision-making becomes a critical issue for many\ndata-driven automated applications. But how may it affect the system's security\nand trustworthiness? This chapter conducts a comprehensive study of machine\nlearning applications in cybersecurity to indicate the need for explainability\nto address this question. While doing that, this chapter first discusses the\nblack-box problems of AI technologies for Cybersecurity applications in smart\ncity-based solutions. Later, considering the new technological paradigm,\nExplainable Artificial Intelligence (XAI), this chapter discusses the\ntransition from black-box to white-box. This chapter also discusses the\ntransition requirements concerning the interpretability, transparency,\nunderstandability, and Explainability of AI-based technologies in applying\ndifferent autonomous systems in smart cities. Finally, it has presented some\ncommercial XAI platforms that offer explainability over traditional AI\ntechnologies before presenting future challenges and opportunities.",
    "published_date": "2021-10-31T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.00601v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.00557v2",
    "title": "On The Absolute Constant in Hanson-Wright Inequality",
    "authors": [
      "Kamyar Moshksar"
    ],
    "author_ids": [],
    "abstract": "We revisit and slightly modify the proof of the Gaussian Hanson-Wright\ninequality where we keep track of the absolute constant in its formulation.",
    "published_date": "2021-10-31T00:00:00",
    "year": 2021,
    "categories": [
      "math.PR",
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.00557v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2111.00528v2",
    "title": "Calibrating the Dice loss to handle neural network overconfidence for biomedical image segmentation",
    "authors": [
      "Michael Yeung",
      "Leonardo Rundo",
      "Yang Nan",
      "Evis Sala",
      "Carola-Bibiane Schönlieb",
      "Guang Yang"
    ],
    "author_ids": [],
    "abstract": "The Dice similarity coefficient (DSC) is both a widely used metric and loss\nfunction for biomedical image segmentation due to its robustness to class\nimbalance. However, it is well known that the DSC loss is poorly calibrated,\nresulting in overconfident predictions that cannot be usefully interpreted in\nbiomedical and clinical practice. Performance is often the only metric used to\nevaluate segmentations produced by deep neural networks, and calibration is\noften neglected. However, calibration is important for translation into\nbiomedical and clinical practice, providing crucial contextual information to\nmodel predictions for interpretation by scientists and clinicians. In this\nstudy, we provide a simple yet effective extension of the DSC loss, named the\nDSC++ loss, that selectively modulates the penalty associated with\noverconfident, incorrect predictions. As a standalone loss function, the DSC++\nloss achieves significantly improved calibration over the conventional DSC loss\nacross six well-validated open-source biomedical imaging datasets, including\nboth 2D binary and 3D multi-class segmentation tasks. Similarly, we observe\nsignificantly improved calibration when integrating the DSC++ loss into four\nDSC-based loss functions. Finally, we use softmax thresholding to illustrate\nthat well calibrated outputs enable tailoring of recall-precision bias, which\nis an important post-processing technique to adapt the model predictions to\nsuit the biomedical or clinical task. The DSC++ loss overcomes the major\nlimitation of the DSC loss, providing a suitable loss function for training\ndeep learning segmentation models for use in biomedical and clinical practice.\nSource code is available at: https://github.com/mlyg/DicePlusPlus.",
    "published_date": "2021-10-31T00:00:00",
    "year": 2021,
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.00528v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.00340v1",
    "title": "Identifying and mitigating bias in algorithms used to manage patients in a pandemic",
    "authors": [
      "Yifan Li",
      "Garrett Yoon",
      "Mustafa Nasir-Moin",
      "David Rosenberg",
      "Sean Neifert",
      "Douglas Kondziolka",
      "Eric Karl Oermann"
    ],
    "author_ids": [],
    "abstract": "Numerous COVID-19 clinical decision support systems have been developed.\nHowever many of these systems do not have the merit for validity due to\nmethodological shortcomings including algorithmic bias. Methods Logistic\nregression models were created to predict COVID-19 mortality, ventilator status\nand inpatient status using a real-world dataset consisting of four hospitals in\nNew York City and analyzed for biases against race, gender and age. Simple\nthresholding adjustments were applied in the training process to establish more\nequitable models. Results Compared to the naively trained models, the\ncalibrated models showed a 57% decrease in the number of biased trials, while\npredictive performance, measured by area under the receiver/operating curve\n(AUC), remained unchanged. After calibration, the average sensitivity of the\npredictive models increased from 0.527 to 0.955. Conclusion We demonstrate that\nnaively training and deploying machine learning models on real world data for\npredictive analytics of COVID-19 has a high risk of bias. Simple implemented\nadjustments or calibrations during model training can lead to substantial and\nsustained gains in fairness on subsequent deployment.",
    "published_date": "2021-10-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.00340v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.00107v4",
    "title": "The Golden Rule as a Heuristic to Measure the Fairness of Texts Using Machine Learning",
    "authors": [
      "Ahmed Izzidien",
      "David Stillwell"
    ],
    "author_ids": [],
    "abstract": "In this paper we present a natural language programming framework to consider\nhow the fairness of acts can be measured. For the purposes of the paper, a fair\nact is defined as one that one would be accepting of if it were done to\noneself. The approach is based on an implementation of the golden rule (GR) in\nthe digital domain. Despite the GRs prevalence as an axiom throughout history,\nno transfer of this moral philosophy into computational systems exists. In this\npaper we consider how to algorithmically operationalise this rule so that it\nmay be used to measure sentences such as: the boy harmed the girl, and\ncategorise them as fair or unfair. A review and reply to criticisms of the GR\nis made. A suggestion of how the technology may be implemented to avoid unfair\nbiases in word embeddings is made - given that individuals would typically not\nwish to be on the receiving end of an unfair act, such as racism, irrespective\nof whether the corpus being used deems such discrimination as praiseworthy.",
    "published_date": "2021-10-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.00107v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.00086v4",
    "title": "Measuring a Texts Fairness Dimensions Using Machine Learning Based on Social Psychological Factors",
    "authors": [
      "Ahmed Izzidien",
      "David Stillwell"
    ],
    "author_ids": [],
    "abstract": "Fairness is a principal social value that can be observed in civilisations\naround the world. A manifestation of this is in social agreements, often\ndescribed in texts, such as contracts. Yet, despite the prevalence of such, a\nfairness metric for texts describing a social act remains wanting. To address\nthis, we take a step back to consider the problem based on first principals.\nInstead of using rules or templates, we utilise social psychology literature to\ndetermine the principal factors that humans use when making a fairness\nassessment. We then attempt to digitise these using word embeddings into a\nmulti-dimensioned sentence level fairness perceptions vector to serve as an\napproximation for these fairness perceptions. The method leverages a pro-social\nbias within word embeddings, for which we obtain an F1= 81.0. A second\napproach, using PCA and ML based on the said fairness approximation vector\nproduces an F1 score of 86.2. We detail improvements that can be made in the\nmethodology to incorporate the projection of sentence embedding on to a\nsubspace representation of fairness.",
    "published_date": "2021-10-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "I.2.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.00086v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.00052v3",
    "title": "Diagnosing Data from ICTs to Provide Focused Assistance in Agricultural Adoptions",
    "authors": [
      "Ashwin Singh",
      "Mallika Subramanian",
      "Anmol Agarwal",
      "Pratyush Priyadarshi",
      "Shrey Gupta",
      "Kiran Garimella",
      "Sanjeev Kumar",
      "Ritesh Kumar",
      "Lokesh Garg",
      "Erica Arya",
      "Ponnurangam Kumaraguru"
    ],
    "author_ids": [],
    "abstract": "In the last two decades, ICTs have played a pivotal role in empowering rural\npopulations in India by making knowledge more accessible. Digital Green (DG) is\none such ICT that employs a participatory approach with smallholder farmers to\nproduce instructional videos that encompass content specific to them. With help\nof human mediators, they disseminate these videos using projectors to improve\nthe adoption of agricultural practices. DG's web-based data tracker stores\nattendance and adoption logs of millions of farmers, videos screened and their\ndemographic information. We leverage this data for a period of ten years\nbetween 2010-2020 across five states in India and use it to conduct a holistic\nevaluation of the ICT. First, we find disparities in adoption rates of farmers,\nfollowing which we use statistical tests to identify different factors that\nlead to these disparities and gender-based inequalities. Second, to provide\nassistance to farmers facing challenges, we model the adoption of practices\nfrom a video as a prediction problem and experiment with different model\narchitectures. Our classifier achieves accuracies ranging from 79% to 90%\nacross the five states, demonstrating its potential for assisting future\nethnographic investigations. Third, we use SHAP values in conjunction with our\nmodel for explaining the impact of various network, content and demographic\nfeatures on adoption. Our research finds that farmers greatly benefit from past\nadopters of a video from their group and village. We also discover that videos\nwith a low content-specificity benefit some farmers more than others. Next, we\nhighlight the implications of our findings by translating them into\nrecommendations for community building, revisiting participatory approach and\nmitigating inequalities. We conclude with a discussion on how our work can\nassist future investigations into the lived experiences of farmers.",
    "published_date": "2021-10-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.00052v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.15926v1",
    "title": "Delayed Propagation Transformer: A Universal Computation Engine towards Practical Control in Cyber-Physical Systems",
    "authors": [
      "Wenqing Zheng",
      "Qiangqiang Guo",
      "Hao Yang",
      "Peihao Wang",
      "Zhangyang Wang"
    ],
    "author_ids": [],
    "abstract": "Multi-agent control is a central theme in the Cyber-Physical Systems (CPS).\nHowever, current control methods either receive non-Markovian states due to\ninsufficient sensing and decentralized design, or suffer from poor convergence.\nThis paper presents the Delayed Propagation Transformer (DePT), a new\ntransformer-based model that specializes in the global modeling of CPS while\ntaking into account the immutable constraints from the physical world. DePT\ninduces a cone-shaped spatial-temporal attention prior, which injects the\ninformation propagation and aggregation principles and enables a global view.\nWith physical constraint inductive bias baked into its design, our DePT is\nready to plug and play for a broad class of multi-agent systems. The\nexperimental results on one of the most challenging CPS -- network-scale\ntraffic signal control system in the open world -- show that our model\noutperformed the state-of-the-art expert methods on synthetic and real-world\ndatasets. Our codes are released at: https://github.com/VITA-Group/DePT.",
    "published_date": "2021-10-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.15926v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.15683v1",
    "title": "Incentives for Item Duplication under Fair Ranking Policies",
    "authors": [
      "Giorgio Maria Di Nunzio",
      "Alessandro Fabris",
      "Gianmaria Silvello",
      "Gian Antonio Susto"
    ],
    "author_ids": [],
    "abstract": "Ranking is a fundamental operation in information access systems, to filter\ninformation and direct user attention towards items deemed most relevant to\nthem. Due to position bias, items of similar relevance may receive\nsignificantly different exposure, raising fairness concerns for item providers\nand motivating recent research into fair ranking. While the area has progressed\ndramatically over recent years, no study to date has investigated the potential\nproblem posed by duplicated items. Duplicates and near-duplicates are common in\nseveral domains, including marketplaces and document collections available to\nsearch engines. In this work, we study the behaviour of different fair ranking\npolicies in the presence of duplicates, quantifying the extra-exposure gained\nby redundant items. We find that fairness-aware ranking policies may conflict\nwith diversity, due to their potential to incentivize duplication more than\npolicies solely focused on relevance. This fact poses a problem for system\nowners who, as a result of this incentive, may have to deal with increased\nredundancy, which is at odds with user satisfaction. Finally, we argue that\nthis aspect represents a blind spot in the normative reasoning underlying\ncommon fair ranking metrics, as rewarding providers who duplicate their items\nwith increased exposure seems unfair for the remaining providers.",
    "published_date": "2021-10-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.15683v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2110.15664v2",
    "title": "3D-OOCS: Learning Prostate Segmentation with Inductive Bias",
    "authors": [
      "Shrajan Bhandary",
      "Zahra Babaiee",
      "Dejan Kostyszyn",
      "Tobias Fechter",
      "Constantinos Zamboglou",
      "Anca-Ligia Grosu",
      "Radu Grosu"
    ],
    "author_ids": [],
    "abstract": "Despite the great success of convolutional neural networks (CNN) in 3D\nmedical image segmentation tasks, the methods currently in use are still not\nrobust enough to the different protocols utilized by different scanners, and to\nthe variety of image properties or artefacts they produce. To this end, we\nintroduce OOCS-enhanced networks, a novel architecture inspired by the innate\nnature of visual processing in the vertebrates. With different 3D U-Net\nvariants as the base, we add two 3D residual components to the second encoder\nblocks: on and off center-surround (OOCS). They generalise the ganglion\npathways in the retina to a 3D setting. The use of 2D-OOCS in any standard CNN\nnetwork complements the feedforward framework with sharp edge-detection\ninductive biases. The use of 3D-OOCS also helps 3D U-Nets to scrutinise and\ndelineate anatomical structures present in 3D images with increased accuracy.We\ncompared the state-of-the-art 3D U-Nets with their 3D-OOCS extensions and\nshowed the superior accuracy and robustness of the latter in automatic prostate\nsegmentation from 3D Magnetic Resonance Images (MRIs). For a fair comparison,\nwe trained and tested all the investigated 3D U-Nets with the same pipeline,\nincluding automatic hyperparameter optimisation and data augmentation.",
    "published_date": "2021-10-29T00:00:00",
    "year": 2021,
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.15664v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.00008v1",
    "title": "Reinforced Workload Distribution Fairness",
    "authors": [
      "Zhiyuan Yao",
      "Zihan Ding",
      "Thomas Heide Clausen"
    ],
    "author_ids": [],
    "abstract": "Network load balancers are central components in data centers, that\ndistributes workloads across multiple servers and thereby contribute to\noffering scalable services. However, when load balancers operate in dynamic\nenvironments with limited monitoring of application server loads, they rely on\nheuristic algorithms that require manual configurations for fairness and\nperformance. To alleviate that, this paper proposes a distributed asynchronous\nreinforcement learning mechanism to-with no active load balancer state\nmonitoring and limited network observations-improve the fairness of the\nworkload distribution achieved by a load balancer. The performance of proposed\nmechanism is evaluated and compared with stateof-the-art load balancing\nalgorithms in a simulator, under configurations with progressively increasing\ncomplexities. Preliminary results show promise in RLbased load balancing\nalgorithms, and identify additional challenges and future research directions,\nincluding reward function design and model scalability.",
    "published_date": "2021-10-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.00008v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.15545v3",
    "title": "Improving Fairness via Federated Learning",
    "authors": [
      "Yuchen Zeng",
      "Hongxu Chen",
      "Kangwook Lee"
    ],
    "author_ids": [],
    "abstract": "Recently, lots of algorithms have been proposed for learning a fair\nclassifier from decentralized data. However, many theoretical and algorithmic\nquestions remain open. First, is federated learning necessary, i.e., can we\nsimply train locally fair classifiers and aggregate them? In this work, we\nfirst propose a new theoretical framework, with which we demonstrate that\nfederated learning can strictly boost model fairness compared with such\nnon-federated algorithms. We then theoretically and empirically show that the\nperformance tradeoff of FedAvg-based fair learning algorithms is strictly worse\nthan that of a fair classifier trained on centralized data. To bridge this gap,\nwe propose FedFB, a private fair learning algorithm on decentralized data. The\nkey idea is to modify the FedAvg protocol so that it can effectively mimic the\ncentralized fair learning. Our experimental results show that FedFB\nsignificantly outperforms existing approaches, sometimes matching the\nperformance of the centrally trained model.",
    "published_date": "2021-10-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.15545v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.15503v2",
    "title": "A Pre-processing Method for Fairness in Ranking",
    "authors": [
      "Ryosuke Sonoda"
    ],
    "author_ids": [],
    "abstract": "Fair ranking problems arise in many decision-making processes that often\nnecessitate a trade-off between accuracy and fairness. Many existing studies\nhave proposed correction methods such as adding fairness constraints to a\nranking model's loss. However, the challenge of correcting the data bias for\nfair ranking remains, and the trade-off of the ranking models leaves room for\nimprovement. In this paper, we propose a fair ranking framework that evaluates\nthe order of training data in a pairwise manner as well as various fairness\nmeasurements in ranking. This study is the first proposal of a pre-processing\nmethod that solves fair ranking problems using the pairwise ordering method\nwith our best knowledge. The fair pairwise ordering method is prominent in\ntraining the fair ranking models because it ensures that the resulting ranking\nlikely becomes parity across groups. As far as the fairness measurements in\nranking are represented as a linear constraint of the ranking models, we proved\nthat the minimization of loss function subject to the constraints is reduced to\nthe closed solution of the minimization problem augmented by weights to\ntraining data. This closed solution inspires us to present a practical and\nstable algorithm that iterates the optimization of weights and model\nparameters. The empirical results over real-world datasets demonstrated that\nour method outperforms the existing methods in the trade-off between accuracy\nand fairness over real-world datasets and various fairness measurements.",
    "published_date": "2021-10-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.15503v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.15499v1",
    "title": "UDIS: Unsupervised Discovery of Bias in Deep Visual Recognition Models",
    "authors": [
      "Arvindkumar Krishnakumar",
      "Viraj Prabhu",
      "Sruthi Sudhakar",
      "Judy Hoffman"
    ],
    "author_ids": [],
    "abstract": "Deep learning models have been shown to learn spurious correlations from data\nthat sometimes lead to systematic failures for certain subpopulations. Prior\nwork has typically diagnosed this by crowdsourcing annotations for various\nprotected attributes and measuring performance, which is both expensive to\nacquire and difficult to scale. In this work, we propose UDIS, an unsupervised\nalgorithm for surfacing and analyzing such failure modes. UDIS identifies\nsubpopulations via hierarchical clustering of dataset embeddings and surfaces\nsystematic failure modes by visualizing low performing clusters along with\ntheir gradient-weighted class-activation maps. We show the effectiveness of\nUDIS in identifying failure modes in models trained for image classification on\nthe CelebA and MSCOCO datasets.",
    "published_date": "2021-10-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.15499v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.15403v3",
    "title": "Selective Regression Under Fairness Criteria",
    "authors": [
      "Abhin Shah",
      "Yuheng Bu",
      "Joshua Ka-Wing Lee",
      "Subhro Das",
      "Rameswar Panda",
      "Prasanna Sattigeri",
      "Gregory W. Wornell"
    ],
    "author_ids": [],
    "abstract": "Selective regression allows abstention from prediction if the confidence to\nmake an accurate prediction is not sufficient. In general, by allowing a reject\noption, one expects the performance of a regression model to increase at the\ncost of reducing coverage (i.e., by predicting on fewer samples). However, as\nwe show, in some cases, the performance of a minority subgroup can decrease\nwhile we reduce the coverage, and thus selective regression can magnify\ndisparities between different sensitive subgroups. Motivated by these\ndisparities, we propose new fairness criteria for selective regression\nrequiring the performance of every subgroup to improve with a decrease in\ncoverage. We prove that if a feature representation satisfies the sufficiency\ncriterion or is calibrated for mean and variance, than the proposed fairness\ncriteria is met. Further, we introduce two approaches to mitigate the\nperformance disparity across subgroups: (a) by regularizing an upper bound of\nconditional mutual information under a Gaussian assumption and (b) by\nregularizing a contrastive loss for conditional mean and conditional variance\nprediction. The effectiveness of these approaches is demonstrated on synthetic\nand real-world datasets.",
    "published_date": "2021-10-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.15403v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.15350v1",
    "title": "XDEEP-MSI: Explainable Bias-Rejecting Microsatellite Instability Deep Learning System In Colorectal Cancer",
    "authors": [
      "Aurelia Bustos",
      "Artemio Payá",
      "Andres Torrubia",
      "Rodrigo Jover",
      "Xavier Llor",
      "Xavier Bessa",
      "Antoni Castells",
      "Cristina Alenda"
    ],
    "author_ids": [],
    "abstract": "We present a system for the prediction of microsatellite instability (MSI)\nfrom H&E images of colorectal cancer using deep learning (DL) techniques\ncustomized for tissue microarrays (TMAs). The system incorporates an end-to-end\nimage preprocessing module that produces tiles at multiple magnifications in\nthe regions of interest as guided by a tissue classifier module, and a\nmultiple-bias rejecting module. The training and validation TMA samples were\nobtained from the EPICOLON project and further enriched with samples from a\nsingle institution. A systematic study of biases at tile level identified three\nprotected (bias) variables associated with the learned representations of a\nbaseline model: the project of origin of samples, the patient spot and the TMA\nglass where each spot was placed. A multiple bias rejecting technique based on\nadversarial training is implemented at the DL architecture so to directly avoid\nlearning the batch effects of those variables. The learned features from the\nbias-ablated model have maximum discriminative power with respect to the task\nand minimal statistical mean dependence with the biases. The impact of\ndifferent magnifications, types of tissues and the model performance at tile vs\npatient level is analyzed. The AUC at tile level, and including all three\nselected tissues (tumor epithelium, mucine and lymphocytic regions) and 4\nmagnifications, was 0.87 +/- 0.03 and increased to 0.9 +/- 0.03 at patient\nlevel. To the best of our knowledge, this is the first work that incorporates a\nmultiple bias ablation technique at the DL architecture in digital pathology,\nand the first using TMAs for the MSI prediction task.",
    "published_date": "2021-10-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.15350v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.15781v1",
    "title": "Two-sided fairness in rankings via Lorenz dominance",
    "authors": [
      "Virginie Do",
      "Sam Corbett-Davies",
      "Jamal Atif",
      "Nicolas Usunier"
    ],
    "author_ids": [],
    "abstract": "We consider the problem of generating rankings that are fair towards both\nusers and item producers in recommender systems. We address both usual\nrecommendation (e.g., of music or movies) and reciprocal recommendation (e.g.,\ndating). Following concepts of distributive justice in welfare economics, our\nnotion of fairness aims at increasing the utility of the worse-off individuals,\nwhich we formalize using the criterion of Lorenz efficiency. It guarantees that\nrankings are Pareto efficient, and that they maximally redistribute utility\nfrom better-off to worse-off, at a given level of overall utility. We propose\nto generate rankings by maximizing concave welfare functions, and develop an\nefficient inference procedure based on the Frank-Wolfe algorithm. We prove that\nunlike existing approaches based on fairness constraints, our approach always\nproduces fair rankings. Our experiments also show that it increases the utility\nof the worse-off at lower costs in terms of overall utility.",
    "published_date": "2021-10-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.15781v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.15310v2",
    "title": "On the Fairness of Machine-Assisted Human Decisions",
    "authors": [
      "Talia Gillis",
      "Bryce McLaughlin",
      "Jann Spiess"
    ],
    "author_ids": [],
    "abstract": "When machine-learning algorithms are used in high-stakes decisions, we want\nto ensure that their deployment leads to fair and equitable outcomes. This\nconcern has motivated a fast-growing literature that focuses on diagnosing and\naddressing disparities in machine predictions. However, many machine\npredictions are deployed to assist in decisions where a human decision-maker\nretains the ultimate decision authority. In this article, we therefore consider\nin a formal model and in a lab experiment how properties of machine predictions\naffect the resulting human decisions. In our formal model of statistical\ndecision-making, we show that the inclusion of a biased human decision-maker\ncan revert common relationships between the structure of the algorithm and the\nqualities of resulting decisions. Specifically, we document that excluding\ninformation about protected groups from the prediction may fail to reduce, and\nmay even increase, ultimate disparities. In the lab experiment, we demonstrate\nhow predictions informed by gender-specific information can reduce average\ngender disparities in decisions. While our concrete theoretical results rely on\nspecific assumptions about the data, algorithm, and decision-maker, and the\nexperiment focuses on a particular prediction task, our findings show more\nbroadly that any study of critical properties of complex decision systems, such\nas the fairness of machine-assisted human decisions, should go beyond focusing\non the underlying algorithmic predictions in isolation.",
    "published_date": "2021-10-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.HC",
      "cs.LG",
      "econ.GN",
      "q-fin.EC",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.15310v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.04454v1",
    "title": "AI Federalism: Shaping AI Policy within States in Germany",
    "authors": [
      "Anna Jobin",
      "Licinia Guettel",
      "Laura Liebig",
      "Christian Katzenbach"
    ],
    "author_ids": [],
    "abstract": "Recent AI governance research has focused heavily on the analysis of strategy\npapers and ethics guidelines for AI published by national governments and\ninternational bodies. Meanwhile, subnational institutions have also published\ndocuments on Artificial Intelligence, yet these have been largely absent from\npolicy analyses. This is surprising because AI is connected to many policy\nareas, such as economic or research policy, where the competences are already\ndistributed between the national and subnational level. To better understand\nthe current dynamics of AI governance, it is essential to consider the context\nof policy making beyond the federal government. Although AI may be considered a\nnew policy field, it is created, contested and ultimately shaped within\nexisting political structures and dynamics. We therefore argue that more\nattention should be dedicated to subnational efforts to shape AI and present\ninitial findings from our case study of Germany. Analyzing AI as a policy field\non different levels of government will contribute to a better understanding of\nthe developments and implementations of AI strategies in different national\ncontexts.",
    "published_date": "2021-10-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "K.4.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.04454v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.15074v1",
    "title": "Meta Guided Metric Learner for Overcoming Class Confusion in Few-Shot Road Object Detection",
    "authors": [
      "Anay Majee",
      "Anbumani Subramanian",
      "Kshitij Agrawal"
    ],
    "author_ids": [],
    "abstract": "Localization and recognition of less-occurring road objects have been a\nchallenge in autonomous driving applications due to the scarcity of data\nsamples. Few-Shot Object Detection techniques extend the knowledge from\nexisting base object classes to learn novel road objects given few training\nexamples. Popular techniques in FSOD adopt either meta or metric learning\ntechniques which are prone to class confusion and base class forgetting. In\nthis work, we introduce a novel Meta Guided Metric Learner (MGML) to overcome\nclass confusion in FSOD. We re-weight the features of the novel classes higher\nthan the base classes through a novel Squeeze and Excite module and encourage\nthe learning of truly discriminative class-specific features by applying an\nOrthogonality Constraint to the meta learner. Our method outperforms\nState-of-the-Art (SoTA) approaches in FSOD on the India Driving Dataset (IDD)\nby upto 11 mAP points while suffering from the least class confusion of 20%\ngiven only 10 examples of each novel road object. We further show similar\nimprovements on the few-shot splits of PASCAL VOC dataset where we outperform\nSoTA approaches by upto 5.8 mAP accross all splits.",
    "published_date": "2021-10-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.15074v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.00002v3",
    "title": "Fair Incentives for Repeated Engagement",
    "authors": [
      "Daniel Freund",
      "Chamsi Hssaine"
    ],
    "author_ids": [],
    "abstract": "We study a decision-maker's problem of finding optimal monetary incentive\nschemes for retention when faced with agents whose participation decisions\n(stochastically) depend on the incentive they receive. Our focus is on policies\nconstrained to fulfill two fairness properties that preclude outcomes wherein\ndifferent groups of agents experience different treatment on average. We\nformulate the problem as a high-dimensional stochastic optimization problem,\nand study it through the use of a closely related deterministic variant. We\nshow that the optimal static solution to this deterministic variant is\nasymptotically optimal for the dynamic problem under fairness constraints.\nThough solving for the optimal static solution gives rise to a non-convex\noptimization problem, we uncover a structural property that allows us to design\na tractable, fast-converging heuristic policy. Traditional schemes for\nretention ignore fairness constraints; indeed, the goal in these is to use\ndifferentiation to incentivize repeated engagement with the system. Our work\n(i) shows that even in the absence of explicit discrimination, dynamic policies\nmay unintentionally discriminate between agents of different types by varying\nthe type composition of the system, and (ii) presents an asymptotically optimal\npolicy to avoid such discriminatory outcomes.",
    "published_date": "2021-10-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT",
      "cs.LG",
      "cs.MA",
      "math.OC",
      "math.PR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.00002v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.14842v2",
    "title": "Towards the ultimate limits of quantum channel discrimination",
    "authors": [
      "Kun Fang",
      "Gilad Gour",
      "Xin Wang"
    ],
    "author_ids": [],
    "abstract": "This note studies the difficulty of discriminating quantum channels under\noperational regimes. First, we make a conjecture on the exponentially strong\nconverse of quantum channel hypothesis testing under coherent strategies,\nmeaning that any strategy to make the Type II error decays with an exponent\nlarger than the regularized channel relative entropy will unavoidably result in\nthe Type I error converging to one exponentially fast in the asymptotic limit.\nThis conjecture will imply the desirable quantum channel Stein's Lemma and the\ncontinuity of the regularized (amortized) Sandwiched R\\'{e}nyi channel\ndivergence at $\\alpha=1$. We also remark that there was a gap in the proof of\nthe above conjecture in our previous arXiv version. Such gap exists since a\nlemma basically comes from [Brandao and Plenio, 2010] was found to be false.\nSecond, we develop a framework to show the interplay between the strategies of\nchannel discrimination, the operational regimes, and variants of channel\ndivergences. This framework systematically underlies the operational meaning of\nquantum channel divergences in quantum channel discrimination. Our work makes\nan attempt towards understanding the ultimate limit of quantum channel\ndiscrimination, as well as its connection to quantum channel divergences in the\nasymptotic regime.",
    "published_date": "2021-10-28T00:00:00",
    "year": 2021,
    "categories": [
      "quant-ph",
      "cs.IT",
      "math-ph",
      "math.IT",
      "math.MP",
      "math.ST",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.14842v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2110.14839v1",
    "title": "Hate Speech Classifiers Learn Human-Like Social Stereotypes",
    "authors": [
      "Aida Mostafazadeh Davani",
      "Mohammad Atari",
      "Brendan Kennedy",
      "Morteza Dehghani"
    ],
    "author_ids": [],
    "abstract": "Social stereotypes negatively impact individuals' judgements about different\ngroups and may have a critical role in how people understand language directed\ntoward minority social groups. Here, we assess the role of social stereotypes\nin the automated detection of hateful language by examining the relation\nbetween individual annotator biases and erroneous classification of texts by\nhate speech classifiers. Specifically, in Study 1 we investigate the impact of\nnovice annotators' stereotypes on their hate-speech-annotation behavior. In\nStudy 2 we examine the effect of language-embedded stereotypes on expert\nannotators' aggregated judgements in a large annotated corpus. Finally, in\nStudy 3 we demonstrate how language-embedded stereotypes are associated with\nsystematic prediction errors in a neural-network hate speech classifier. Our\nresults demonstrate that hate speech classifiers learn human-like biases which\ncan further perpetuate social inequalities when propagated at scale. This\nframework, combining social psychological and computational linguistic methods,\nprovides insights into additional sources of bias in hate speech moderation,\ninforming ongoing debates regarding fairness in machine learning.",
    "published_date": "2021-10-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.14839v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.14739v2",
    "title": "Generalized Shape Metrics on Neural Representations",
    "authors": [
      "Alex H. Williams",
      "Erin Kunz",
      "Simon Kornblith",
      "Scott W. Linderman"
    ],
    "author_ids": [],
    "abstract": "Understanding the operation of biological and artificial networks remains a\ndifficult and important challenge. To identify general principles, researchers\nare increasingly interested in surveying large collections of networks that are\ntrained on, or biologically adapted to, similar tasks. A standardized set of\nanalysis tools is now needed to identify how network-level covariates -- such\nas architecture, anatomical brain region, and model organism -- impact neural\nrepresentations (hidden layer activations). Here, we provide a rigorous\nfoundation for these analyses by defining a broad family of metric spaces that\nquantify representational dissimilarity. Using this framework we modify\nexisting representational similarity measures based on canonical correlation\nanalysis to satisfy the triangle inequality, formulate a novel metric that\nrespects the inductive biases in convolutional layers, and identify approximate\nEuclidean embeddings that enable network representations to be incorporated\ninto essentially any off-the-shelf machine learning method. We demonstrate\nthese methods on large-scale datasets from biology (Allen Institute Brain\nObservatory) and deep learning (NAS-Bench-101). In doing so, we identify\nrelationships between neural representations that are interpretable in terms of\nanatomical features and model performance.",
    "published_date": "2021-10-27T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.14739v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.14690v1",
    "title": "VACA: Design of Variational Graph Autoencoders for Interventional and Counterfactual Queries",
    "authors": [
      "Pablo Sanchez-Martin",
      "Miriam Rateike",
      "Isabel Valera"
    ],
    "author_ids": [],
    "abstract": "In this paper, we introduce VACA, a novel class of variational graph\nautoencoders for causal inference in the absence of hidden confounders, when\nonly observational data and the causal graph are available. Without making any\nparametric assumptions, VACA mimics the necessary properties of a Structural\nCausal Model (SCM) to provide a flexible and practical framework for\napproximating interventions (do-operator) and abduction-action-prediction\nsteps. As a result, and as shown by our empirical results, VACA accurately\napproximates the interventional and counterfactual distributions on diverse\nSCMs. Finally, we apply VACA to evaluate counterfactual fairness in fair\nclassification problems, as well as to learn fair classifiers without\ncompromising performance.",
    "published_date": "2021-10-27T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.14690v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.14419v3",
    "title": "Toward a Theory of Justice for Artificial Intelligence",
    "authors": [
      "Iason Gabriel"
    ],
    "author_ids": [],
    "abstract": "This paper explores the relationship between artificial intelligence and\nprinciples of distributive justice. Drawing upon the political philosophy of\nJohn Rawls, it holds that the basic structure of society should be understood\nas a composite of socio-technical systems, and that the operation of these\nsystems is increasingly shaped and influenced by AI. As a consequence,\negalitarian norms of justice apply to the technology when it is deployed in\nthese contexts. These norms entail that the relevant AI systems must meet a\ncertain standard of public justification, support citizens rights, and promote\nsubstantively fair outcomes -- something that requires specific attention be\npaid to the impact they have on the worst-off members of society.",
    "published_date": "2021-10-27T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "K.4.1; K.4.2; K.5.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.14419v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.14336v1",
    "title": "Feature and Label Embedding Spaces Matter in Addressing Image Classifier Bias",
    "authors": [
      "William Thong",
      "Cees G. M. Snoek"
    ],
    "author_ids": [],
    "abstract": "This paper strives to address image classifier bias, with a focus on both\nfeature and label embedding spaces. Previous works have shown that spurious\ncorrelations from protected attributes, such as age, gender, or skin tone, can\ncause adverse decisions. To balance potential harms, there is a growing need to\nidentify and mitigate image classifier bias. First, we identify in the feature\nspace a bias direction. We compute class prototypes of each protected attribute\nvalue for every class, and reveal an existing subspace that captures the\nmaximum variance of the bias. Second, we mitigate biases by mapping image\ninputs to label embedding spaces. Each value of the protected attribute has its\nprojection head where classes are embedded through a latent vector\nrepresentation rather than a common one-hot encoding. Once trained, we further\nreduce in the feature space the bias effect by removing its direction.\nEvaluation on biased image datasets, for multi-class, multi-label and binary\nclassifications, shows the effectiveness of tackling both feature and label\nembedding spaces in improving the fairness of the classifier predictions, while\npreserving classification performance.",
    "published_date": "2021-10-27T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.14336v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.14309v1",
    "title": "Inferring the Class Conditional Response Map for Weakly Supervised Semantic Segmentation",
    "authors": [
      "Weixuan Sun",
      "Jing Zhang",
      "Nick Barnes"
    ],
    "author_ids": [],
    "abstract": "Image-level weakly supervised semantic segmentation (WSSS) relies on class\nactivation maps (CAMs) for pseudo labels generation. As CAMs only highlight the\nmost discriminative regions of objects, the generated pseudo labels are usually\nunsatisfactory to serve directly as supervision. To solve this, most existing\napproaches follow a multi-training pipeline to refine CAMs for better\npseudo-labels, which includes: 1) re-training the classification model to\ngenerate CAMs; 2) post-processing CAMs to obtain pseudo labels; and 3) training\na semantic segmentation model with the obtained pseudo labels. However, this\nmulti-training pipeline requires complicated adjustment and additional time. To\naddress this, we propose a class-conditional inference strategy and an\nactivation aware mask refinement loss function to generate better pseudo labels\nwithout re-training the classifier. The class conditional inference-time\napproach is presented to separately and iteratively reveal the classification\nnetwork's hidden object activation to generate more complete response maps.\nFurther, our activation aware mask refinement loss function introduces a novel\nway to exploit saliency maps during segmentation training and refine the\nforeground object masks without suppressing background objects. Our method\nachieves superior WSSS results without requiring re-training of the classifier.",
    "published_date": "2021-10-27T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.14309v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.13986v1",
    "title": "Fair Sequential Selection Using Supervised Learning Models",
    "authors": [
      "Mohammad Mahdi Khalili",
      "Xueru Zhang",
      "Mahed Abroshan"
    ],
    "author_ids": [],
    "abstract": "We consider a selection problem where sequentially arrived applicants apply\nfor a limited number of positions/jobs. At each time step, a decision maker\naccepts or rejects the given applicant using a pre-trained supervised learning\nmodel until all the vacant positions are filled. In this paper, we discuss\nwhether the fairness notions (e.g., equal opportunity, statistical parity,\netc.) that are commonly used in classification problems are suitable for the\nsequential selection problems. In particular, we show that even with a\npre-trained model that satisfies the common fairness notions, the selection\noutcomes may still be biased against certain demographic groups. This\nobservation implies that the fairness notions used in classification problems\nare not suitable for a selection problem where the applicants compete for a\nlimited number of positions. We introduce a new fairness notion, ``Equal\nSelection (ES),'' suitable for sequential selection problems and propose a\npost-processing approach to satisfy the ES fairness notion. We also consider a\nsetting where the applicants have privacy concerns, and the decision maker only\nhas access to the noisy version of sensitive attributes. In this setting, we\ncan show that the perfect ES fairness can still be attained under certain\nconditions.",
    "published_date": "2021-10-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.13986v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.13968v2",
    "title": "On the Effects of Artificial Data Modification",
    "authors": [
      "Antonia Marcu",
      "Adam Prügel-Bennett"
    ],
    "author_ids": [],
    "abstract": "Data distortion is commonly applied in vision models during both training\n(e.g methods like MixUp and CutMix) and evaluation (e.g. shape-texture bias and\nrobustness). This data modification can introduce artificial information. It is\noften assumed that the resulting artefacts are detrimental to training, whilst\nbeing negligible when analysing models. We investigate these assumptions and\nconclude that in some cases they are unfounded and lead to incorrect results.\nSpecifically, we show current shape bias identification methods and occlusion\nrobustness measures are biased and propose a fairer alternative for the latter.\nSubsequently, through a series of experiments we seek to correct and strengthen\nthe community's perception of how augmenting affects learning of vision models.\nBased on our empirical results we argue that the impact of the artefacts must\nbe understood and exploited rather than eliminated.",
    "published_date": "2021-10-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.13968v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.13957v4",
    "title": "Unbiased Graph Embedding with Biased Graph Observations",
    "authors": [
      "Nan Wang",
      "Lu Lin",
      "Jundong Li",
      "Hongning Wang"
    ],
    "author_ids": [],
    "abstract": "Graph embedding techniques are pivotal in real-world machine learning tasks\nthat operate on graph-structured data, such as social recommendation and\nprotein structure modeling. Embeddings are mostly performed on the node level\nfor learning representations of each node. Since the formation of a graph is\ninevitably affected by certain sensitive node attributes, the node embeddings\ncan inherit such sensitive information and introduce undesirable biases in\ndownstream tasks. Most existing works impose ad-hoc constraints on the node\nembeddings to restrict their distributions for unbiasedness/fairness, which\nhowever compromise the utility of the resulting embeddings. In this paper, we\npropose a principled new way for unbiased graph embedding by learning node\nembeddings from an underlying bias-free graph, which is not influenced by\nsensitive node attributes. Motivated by this new perspective, we propose two\ncomplementary methods for uncovering such an underlying graph, with the goal of\nintroducing minimum impact on the utility of the embeddings. Both our\ntheoretical justification and extensive experimental comparisons against\nstate-of-the-art solutions demonstrate the effectiveness of our proposed\nmethods.",
    "published_date": "2021-10-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.13957v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.13948v2",
    "title": "Boosted CVaR Classification",
    "authors": [
      "Runtian Zhai",
      "Chen Dan",
      "Arun Sai Suggala",
      "Zico Kolter",
      "Pradeep Ravikumar"
    ],
    "author_ids": [],
    "abstract": "Many modern machine learning tasks require models with high tail performance,\ni.e. high performance over the worst-off samples in the dataset. This problem\nhas been widely studied in fields such as algorithmic fairness, class\nimbalance, and risk-sensitive decision making. A popular approach to maximize\nthe model's tail performance is to minimize the CVaR (Conditional Value at\nRisk) loss, which computes the average risk over the tails of the loss.\nHowever, for classification tasks where models are evaluated by the zero-one\nloss, we show that if the classifiers are deterministic, then the minimizer of\nthe average zero-one loss also minimizes the CVaR zero-one loss, suggesting\nthat CVaR loss minimization is not helpful without additional assumptions. We\ncircumvent this negative result by minimizing the CVaR loss over randomized\nclassifiers, for which the minimizers of the average zero-one loss and the CVaR\nzero-one loss are no longer the same, so minimizing the latter can lead to\nbetter tail performance. To learn such randomized classifiers, we propose the\nBoosted CVaR Classification framework which is motivated by a direct\nrelationship between CVaR and a classical boosting algorithm called LPBoost.\nBased on this framework, we design an algorithm called $\\alpha$-AdaLPBoost. We\nempirically evaluate our proposed algorithm on four benchmark datasets and show\nthat it achieves higher tail performance than deterministic model training\nmethods.",
    "published_date": "2021-10-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.13948v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.13905v2",
    "title": "Gradient Descent on Two-layer Nets: Margin Maximization and Simplicity Bias",
    "authors": [
      "Kaifeng Lyu",
      "Zhiyuan Li",
      "Runzhe Wang",
      "Sanjeev Arora"
    ],
    "author_ids": [],
    "abstract": "The generalization mystery of overparametrized deep nets has motivated\nefforts to understand how gradient descent (GD) converges to low-loss solutions\nthat generalize well. Real-life neural networks are initialized from small\nrandom values and trained with cross-entropy loss for classification (unlike\nthe \"lazy\" or \"NTK\" regime of training where analysis was more successful), and\na recent sequence of results (Lyu and Li, 2020; Chizat and Bach, 2020; Ji and\nTelgarsky, 2020) provide theoretical evidence that GD may converge to the\n\"max-margin\" solution with zero loss, which presumably generalizes well.\nHowever, the global optimality of margin is proved only in some settings where\nneural nets are infinitely or exponentially wide. The current paper is able to\nestablish this global optimality for two-layer Leaky ReLU nets trained with\ngradient flow on linearly separable and symmetric data, regardless of the\nwidth. The analysis also gives some theoretical justification for recent\nempirical findings (Kalimeris et al., 2019) on the so-called simplicity bias of\nGD towards linear or other \"simple\" classes of solutions, especially early in\ntraining. On the pessimistic side, the paper suggests that such results are\nfragile. A simple data manipulation can make gradient flow converge to a linear\nclassifier with suboptimal margin.",
    "published_date": "2021-10-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.13905v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.13796v1",
    "title": "Post-processing for Individual Fairness",
    "authors": [
      "Felix Petersen",
      "Debarghya Mukherjee",
      "Yuekai Sun",
      "Mikhail Yurochkin"
    ],
    "author_ids": [],
    "abstract": "Post-processing in algorithmic fairness is a versatile approach for\ncorrecting bias in ML systems that are already used in production. The main\nappeal of post-processing is that it avoids expensive retraining. In this work,\nwe propose general post-processing algorithms for individual fairness (IF). We\nconsider a setting where the learner only has access to the predictions of the\noriginal model and a similarity graph between individuals, guiding the desired\nfairness constraints. We cast the IF post-processing problem as a graph\nsmoothing problem corresponding to graph Laplacian regularization that\npreserves the desired \"treat similar individuals similarly\" interpretation. Our\ntheoretical results demonstrate the connection of the new objective function to\na local relaxation of the original individual fairness. Empirically, our\npost-processing algorithms correct individual biases in large-scale NLP models\nsuch as BERT, while preserving accuracy.",
    "published_date": "2021-10-26T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.13796v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.13567v2",
    "title": "Pairwise Half-graph Discrimination: A Simple Graph-level Self-supervised Strategy for Pre-training Graph Neural Networks",
    "authors": [
      "Pengyong Li",
      "Jun Wang",
      "Ziliang Li",
      "Yixuan Qiao",
      "Xianggen Liu",
      "Fei Ma",
      "Peng Gao",
      "Seng Song",
      "Guotong Xie"
    ],
    "author_ids": [],
    "abstract": "Self-supervised learning has gradually emerged as a powerful technique for\ngraph representation learning. However, transferable, generalizable, and robust\nrepresentation learning on graph data still remains a challenge for\npre-training graph neural networks. In this paper, we propose a simple and\neffective self-supervised pre-training strategy, named Pairwise Half-graph\nDiscrimination (PHD), that explicitly pre-trains a graph neural network at\ngraph-level. PHD is designed as a simple binary classification task to\ndiscriminate whether two half-graphs come from the same source. Experiments\ndemonstrate that the PHD is an effective pre-training strategy that offers\ncomparable or superior performance on 13 graph classification tasks compared\nwith state-of-the-art strategies, and achieves notable improvements when\ncombined with node-level strategies. Moreover, the visualization of learned\nrepresentation revealed that PHD strategy indeed empowers the model to learn\ngraph-level knowledge like the molecular scaffold. These results have\nestablished PHD as a powerful and effective self-supervised learning strategy\nin graph-level representation learning.",
    "published_date": "2021-10-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.13567v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.13556v1",
    "title": "Learning Explicit and Implicit Latent Common Spaces for Audio-Visual Cross-Modal Retrieval",
    "authors": [
      "Donghuo Zeng",
      "Jianming Wu",
      "Gen Hattori",
      "Yi Yu",
      "Rong Xu"
    ],
    "author_ids": [],
    "abstract": "Learning common subspace is prevalent way in cross-modal retrieval to solve\nthe problem of data from different modalities having inconsistent distributions\nand representations that cannot be directly compared. Previous cross-modal\nretrieval methods focus on projecting the cross-modal data into a common space\nby learning the correlation between them to bridge the modality gap. However,\nthe rich semantic information in the video and the heterogeneous nature of\naudio-visual data leads to more serious heterogeneous gaps intuitively, which\nmay lead to the loss of key semantic content of video with single clue by the\nprevious methods when eliminating the modality gap, while the semantics of the\ncategories may undermine the properties of the original features. In this work,\nwe aim to learn effective audio-visual representations to support audio-visual\ncross-modal retrieval (AVCMR). We propose a novel model that maps audio-visual\nmodalities into two distinct shared latent subspaces: explicit and implicit\nshared spaces. In particular, the explicit shared space is used to optimize\npairwise correlations, where learned representations across modalities capture\nthe commonalities of audio-visual pairs and reduce the modality gap. The\nimplicit shared space is used to preserve the distinctive features between\nmodalities by maintaining the discrimination of audio/video patterns from\ndifferent semantic categories. Finally, the fusion of the features learned from\nthe two latent subspaces is used for the similarity computation of the AVCMR\ntask. The comprehensive experimental results on two audio-visual datasets\ndemonstrate that our proposed model for using two different latent subspaces\nfor audio-visual cross-modal learning is effective and significantly\noutperforms the state-of-the-art cross-modal models that learn features from a\nsingle subspace.",
    "published_date": "2021-10-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.MM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.13556v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2110.13523v2",
    "title": "Automating Control of Overestimation Bias for Reinforcement Learning",
    "authors": [
      "Arsenii Kuznetsov",
      "Alexander Grishin",
      "Artem Tsypin",
      "Arsenii Ashukha",
      "Artur Kadurin",
      "Dmitry Vetrov"
    ],
    "author_ids": [],
    "abstract": "Overestimation bias control techniques are used by the majority of\nhigh-performing off-policy reinforcement learning algorithms. However, most of\nthese techniques rely on pre-defined bias correction policies that are either\nnot flexible enough or require environment-specific tuning of hyperparameters.\nIn this work, we present a general data-driven approach for the automatic\nselection of bias control hyperparameters. We demonstrate its effectiveness on\nthree algorithms: Truncated Quantile Critics, Weighted Delayed DDPG, and Maxmin\nQ-learning. The proposed technique eliminates the need for an extensive\nhyperparameter search. We show that it leads to a significant reduction of the\nactual number of interactions while preserving the performance.",
    "published_date": "2021-10-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.13523v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.13504v1",
    "title": "Managing Bias in Human-Annotated Data: Moving Beyond Bias Removal",
    "authors": [
      "Gianluca Demartini",
      "Kevin Roitero",
      "Stefano Mizzaro"
    ],
    "author_ids": [],
    "abstract": "Due to the widespread use of data-powered systems in our everyday lives, the\nnotions of bias and fairness gained significant attention among researchers and\npractitioners, in both industry and academia. Such issues typically emerge from\nthe data, which comes with varying levels of quality, used to train systems.\nWith the commercialization and employment of such systems that are sometimes\ndelegated to make life-changing decisions, a significant effort is being made\ntowards the identification and removal of possible sources of bias that may\nsurface to the final end-user. In this position paper, we instead argue that\nbias is not something that should necessarily be removed in all cases, and the\nattention and effort should shift from bias removal to the identification,\nmeasurement, indexing, surfacing, and adjustment of bias, which we name bias\nmanagement. We argue that if correctly managed, bias can be a resource that can\nbe made transparent to the the users and empower them to make informed choices\nabout their experience with the system.",
    "published_date": "2021-10-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.13504v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2110.13409v3",
    "title": "Task-Aware Meta Learning-based Siamese Neural Network for Classifying Obfuscated Malware",
    "authors": [
      "Jinting Zhu",
      "Julian Jang-Jaccard",
      "Amardeep Singh",
      "Paul A. Watters",
      "Seyit Camtepe"
    ],
    "author_ids": [],
    "abstract": "Malware authors apply different techniques of control flow obfuscation, in\norder to create new malware variants to avoid detection. Existing Siamese\nneural network (SNN)-based malware detection methods fail to correctly classify\ndifferent malware families when such obfuscated malware samples are present in\nthe training dataset, resulting in high false-positive rates. To address this\nissue, we propose a novel task-aware few-shot-learning-based Siamese Neural\nNetwork that is resilient against the presence of malware variants affected by\nsuch control flow obfuscation techniques. Using the average entropy features of\neach malware family as inputs, in addition to the image features, our model\ngenerates the parameters for the feature layers, to more accurately adjust the\nfeature embedding for different malware families, each of which has obfuscated\nmalware variants. In addition, our proposed method can classify malware\nclasses, even if there are only one or a few training samples available. Our\nmodel utilizes few-shot learning with the extracted features of a pre-trained\nnetwork (e.g., VGG-16), to avoid the bias typically associated with a model\ntrained with a limited number of training samples. Our proposed approach is\nhighly effective in recognizing unique malware signatures, thus correctly\nclassifying malware samples that belong to the same malware family, even in the\npresence of obfuscated malware variants. Our experimental results, validated by\nN-way on N-shot learning, show that our model is highly effective in\nclassification accuracy, exceeding a rate \\textgreater 91\\%, compared to other\nsimilar methods.",
    "published_date": "2021-10-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.13409v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.13402v3",
    "title": "Revisiting randomized choices in isolation forests",
    "authors": [
      "David Cortes"
    ],
    "author_ids": [],
    "abstract": "Isolation forest or \"iForest\" is an intuitive and widely used algorithm for\nanomaly detection that follows a simple yet effective idea: in a given data\ndistribution, if a threshold (split point) is selected uniformly at random\nwithin the range of some variable and data points are divided according to\nwhether they are greater or smaller than this threshold, outlier points are\nmore likely to end up alone or in the smaller partition. The original procedure\nsuggested the choice of variable to split and split point within a variable to\nbe done uniformly at random at each step, but this paper shows that \"clustered\"\ndiverse outliers - oftentimes a more interesting class of outliers than others\n- can be more easily identified by applying a non-uniformly-random choice of\nvariables and/or thresholds. Different split guiding criteria are compared and\nsome are found to result in significantly better outlier discrimination for\ncertain classes of outliers.",
    "published_date": "2021-10-26T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.13402v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.13244v1",
    "title": "DeepHelp: Deep Learning for Shout Crisis Text Conversations",
    "authors": [
      "Daniel Cahn"
    ],
    "author_ids": [],
    "abstract": "The Shout Crisis Text Line provides individuals undergoing mental health\ncrises an opportunity to have an anonymous text message conversation with a\ntrained Crisis Volunteer (CV). This project partners with Shout and its parent\norganisation, Mental Health Innovations, to explore the applications of Machine\nLearning in understanding Shout's conversations and improving its service. The\noverarching aim of this project is to develop a proof-of-concept model to\ndemonstrate the potential of applying deep learning to crisis text messages.\n  Specifically, this project aims to use deep learning to (1) predict an\nindividual's risk of suicide or self-harm, (2) assess conversation success and\nCV skill using robust metrics, and (3) extrapolate demographic information from\na texter survey to conversations where the texter did not complete the survey.\nTo these ends, contributions to deep learning include a modified\nTransformer-over-BERT model; a framework for multitask learning to improve\ngeneralisation in the presence of sparse labels; and a mathematical model for\nusing imperfect machine learning models to estimate population parameters from\na biased training set.\n  Key results include a deep learning model with likely better performance at\npredicting suicide risk than trained CVs and the ability to predict whether a\ntexter is 21 or under with 88.4% accuracy. We produce three metrics for\nconversation success and evaluate the validity and usefulness for each.\nFinally, reversal of participation bias provides evidence that women, who make\nup 80.3% of conversations with an associated texter survey, make up closer to\n73.5%- 74.8% of all conversations; and that if, after every conversation, the\ntexter had shared whether they found their conversation helpful, affirmative\nanswers would fall from 85.1% to 45.45% - 46.51%.",
    "published_date": "2021-10-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.13244v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.13054v2",
    "title": "Adaptive Data Debiasing through Bounded Exploration",
    "authors": [
      "Yifan Yang",
      "Yang Liu",
      "Parinaz Naghizadeh"
    ],
    "author_ids": [],
    "abstract": "Biases in existing datasets used to train algorithmic decision rules can\nraise ethical and economic concerns due to the resulting disparate treatment of\ndifferent groups. We propose an algorithm for sequentially debiasing such\ndatasets through adaptive and bounded exploration in a classification problem\nwith costly and censored feedback. Exploration in this context means that at\ntimes, and to a judiciously-chosen extent, the decision maker deviates from its\n(current) loss-minimizing rule, and instead accepts some individuals that would\notherwise be rejected, so as to reduce statistical data biases. Our proposed\nalgorithm includes parameters that can be used to balance between the ultimate\ngoal of removing data biases -- which will in turn lead to more accurate and\nfair decisions, and the exploration risks incurred to achieve this goal. We\nanalytically show that such exploration can help debias data in certain\ndistributions. We further investigate how fairness criteria can work in\nconjunction with our data debiasing algorithm. We illustrate the performance of\nour algorithm using experiments on synthetic and real-world datasets.",
    "published_date": "2021-10-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.13054v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.13029v2",
    "title": "Fair Enough: Searching for Sufficient Measures of Fairness",
    "authors": [
      "Suvodeep Majumder",
      "Joymallya Chakraborty",
      "Gina R. Bai",
      "Kathryn T. Stolee",
      "Tim Menzies"
    ],
    "author_ids": [],
    "abstract": "Testing machine learning software for ethical bias has become a pressing\ncurrent concern. In response, recent research has proposed a plethora of new\nfairness metrics, for example, the dozens of fairness metrics in the IBM AIF360\ntoolkit. This raises the question: How can any fairness tool satisfy such a\ndiverse range of goals? While we cannot completely simplify the task of\nfairness testing, we can certainly reduce the problem. This paper shows that\nmany of those fairness metrics effectively measure the same thing. Based on\nexperiments using seven real-world datasets, we find that (a) 26 classification\nmetrics can be clustered into seven groups, and (b) four dataset metrics can be\nclustered into three groups. Further, each reduced set may actually predict\ndifferent things. Hence, it is no longer necessary (or even possible) to\nsatisfy all fairness metrics. In summary, to simplify the fairness testing\nproblem, we recommend the following steps: (1)~determine what type of fairness\nis desirable (and we offer a handful of such types); then (2) lookup those\ntypes in our clusters; then (3) just test for one item per cluster.",
    "published_date": "2021-10-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.13029v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.12925v2",
    "title": "CoProtector: Protect Open-Source Code against Unauthorized Training Usage with Data Poisoning",
    "authors": [
      "Zhensu Sun",
      "Xiaoning Du",
      "Fu Song",
      "Mingze Ni",
      "Li Li"
    ],
    "author_ids": [],
    "abstract": "Github Copilot, trained on billions of lines of public code, has recently\nbecome the buzzword in the computer science research and practice community.\nAlthough it is designed to help developers implement safe and effective code\nwith powerful intelligence, practitioners and researchers raise concerns about\nits ethical and security problems, e.g., should the copyleft licensed code be\nfreely leveraged or insecure code be considered for training in the first\nplace? These problems pose a significant impact on Copilot and other similar\nproducts that aim to learn knowledge from large-scale open-source code through\ndeep learning models, which are inevitably on the rise with the fast\ndevelopment of artificial intelligence. To mitigate such impacts, we argue that\nthere is a need to invent effective mechanisms for protecting open-source code\nfrom being exploited by deep learning models. Here, we design and implement a\nprototype, CoProtector, which utilizes data poisoning techniques to arm source\ncode repositories for defending against such exploits. Our large-scale\nexperiments empirically show that CoProtector is effective in achieving its\npurpose, significantly reducing the performance of Copilot-like deep learning\nmodels while being able to stably reveal the secretly embedded watermark\nbackdoors.",
    "published_date": "2021-10-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.12925v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.12884v2",
    "title": "DECAF: Generating Fair Synthetic Data Using Causally-Aware Generative Networks",
    "authors": [
      "Boris van Breugel",
      "Trent Kyono",
      "Jeroen Berrevoets",
      "Mihaela van der Schaar"
    ],
    "author_ids": [],
    "abstract": "Machine learning models have been criticized for reflecting unfair biases in\nthe training data. Instead of solving for this by introducing fair learning\nalgorithms directly, we focus on generating fair synthetic data, such that any\ndownstream learner is fair. Generating fair synthetic data from unfair data -\nwhile remaining truthful to the underlying data-generating process (DGP) - is\nnon-trivial. In this paper, we introduce DECAF: a GAN-based fair synthetic data\ngenerator for tabular data. With DECAF we embed the DGP explicitly as a\nstructural causal model in the input layers of the generator, allowing each\nvariable to be reconstructed conditioned on its causal parents. This procedure\nenables inference time debiasing, where biased edges can be strategically\nremoved for satisfying user-defined fairness requirements. The DECAF framework\nis versatile and compatible with several popular definitions of fairness. In\nour experiments, we show that DECAF successfully removes undesired bias and -\nin contrast to existing methods - is capable of generating high-quality\nsynthetic data. Furthermore, we provide theoretical guarantees on the\ngenerator's convergence and the fairness of downstream models.",
    "published_date": "2021-10-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.12884v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.12838v1",
    "title": "Debiasing Credit Scoring using Evolutionary Algorithms",
    "authors": [
      "Nigel Kingsman"
    ],
    "author_ids": [],
    "abstract": "This paper investigates the application of machine learning when training a\ncredit decision model over real, publicly available data whilst accounting for\n\"bias objectives\". We use the term \"bias objective\" to describe the requirement\nthat a trained model displays discriminatory bias against a given groups of\nindividuals that doesn't exceed a prescribed level, where such level can be\nzero. This research presents an empirical study examining the tension between\ncompeting model training objectives which in all cases include one or more bias\nobjectives.\n  This work is motivated by the observation that the parties associated with\ncreditworthiness models have requirements that can not certainly be fully met\nsimultaneously. The research herein seeks to highlight the impracticality of\nsatisfying all parties' objectives, demonstrating the need for \"trade-offs\" to\nbe made. The results and conclusions presented by this paper are of particular\nimportance for all stakeholders within the credit scoring industry that rely\nupon artificial intelligence (AI) models as part of the decision-making process\nwhen determining the creditworthiness of individuals. This paper provides an\nexposition of the difficulty of training AI models that are able to\nsimultaneously satisfy multiple bias objectives whilst maintaining acceptable\nlevels of accuracy. Stakeholders should be aware of this difficulty and should\nacknowledge that some degree of discriminatory bias, across a number of\nprotected characteristics and formulations of bias, cannot be avoided.",
    "published_date": "2021-10-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.NE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.12838v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.12793v3",
    "title": "Quantum Boosting using Domain-Partitioning Hypotheses",
    "authors": [
      "Debajyoti Bera",
      "Rohan Bhatia",
      "Parmeet Singh Chani",
      "Sagnik Chatterjee"
    ],
    "author_ids": [],
    "abstract": "Boosting is an ensemble learning method that converts a weak learner into a\nstrong learner in the PAC learning framework. Freund and Schapire designed the\nGodel prize-winning algorithm named AdaBoost that can boost learners, which\noutput binary hypotheses. Recently, Arunachalam and Maity presented the first\nquantum boosting algorithm with similar theoretical guarantees. Their\nalgorithm, which we refer to as QAdaBoost henceforth, is a quantum adaptation\nof AdaBoost and only works for the binary hypothesis case. QAdaBoost is\nquadratically faster than AdaBoost in terms of the VC-dimension of the\nhypothesis class of the weak learner but polynomially worse in the bias of the\nweak learner.\n  Izdebski et al. posed an open question on whether we can boost quantum weak\nlearners that output non-binary hypothesis. In this work, we address this open\nquestion by developing the QRealBoost algorithm which was motivated by the\nclassical RealBoost algorithm. The main technical challenge was to provide\nprovable guarantees for convergence, generalization bounds, and quantum\nspeedup, given that quantum subroutines are noisy and probabilistic. We prove\nthat QRealBoost retains the quadratic speedup of QAdaBoost over AdaBoost and\nfurther achieves a polynomial speedup over QAdaBoost in terms of both the bias\nof the learner and the time taken by the learner to learn the target concept\nclass.\n  Finally, we perform empirical evaluations on QRealBoost and report\nencouraging observations on quantum simulators by benchmarking the convergence\nperformance of QRealBoost against QAdaBoost, AdaBoost, and RealBoost on a\nsubset of the MNIST dataset and Breast Cancer Wisconsin dataset.",
    "published_date": "2021-10-25T00:00:00",
    "year": 2021,
    "categories": [
      "quant-ph",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.12793v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.12734v3",
    "title": "Fast Gradient Non-sign Methods",
    "authors": [
      "Yaya Cheng",
      "Jingkuan Song",
      "Xiaosu Zhu",
      "Qilong Zhang",
      "Lianli Gao",
      "Heng Tao Shen"
    ],
    "author_ids": [],
    "abstract": "Adversarial attacks make their success in DNNs, and among them,\ngradient-based algorithms become one of the mainstreams. Based on the linearity\nhypothesis, under $\\ell_\\infty$ constraint, $sign$ operation applied to the\ngradients is a good choice for generating perturbations. However, side-effects\nfrom such operation exist since it leads to the bias of direction between real\ngradients and perturbations. In other words, current methods contain a gap\nbetween real gradients and actual noises, which leads to biased and inefficient\nattacks. Therefore in this paper, based on the Taylor expansion, the bias is\nanalyzed theoretically, and the correction of $sign$, i.e., Fast Gradient\nNon-sign Method (FGNM), is further proposed. Notably, FGNM is a general routine\nthat seamlessly replaces the conventional $sign$ operation in gradient-based\nattacks with negligible extra computational cost. Extensive experiments\ndemonstrate the effectiveness of our methods. Specifically, for untargeted\nblack-box attacks, ours outperform them by 27.5% at most and 9.5% on average.\nFor targeted attacks against defense models, it is 15.1% and 12.7%. Our\nanonymous code is publicly available at https://github.com/yaya-cheng/FGNM",
    "published_date": "2021-10-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.CR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.12734v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.12674v2",
    "title": "mlr3spatiotempcv: Spatiotemporal resampling methods for machine learning in R",
    "authors": [
      "Patrick Schratz",
      "Marc Becker",
      "Michel Lang",
      "Alexander Brenning"
    ],
    "author_ids": [],
    "abstract": "Spatial and spatiotemporal machine-learning models require a suitable\nframework for their model assessment, model selection, and hyperparameter\ntuning, in order to avoid error estimation bias and over-fitting. This\ncontribution reviews the state-of-the-art in spatial and spatiotemporal\ncross-validation, and introduces the {R} package {mlr3spatiotempcv} as an\nextension package of the machine-learning framework {mlr3}. Currently various\n{R} packages implementing different spatiotemporal partitioning strategies\nexist: {blockCV}, {CAST}, {skmeans} and {sperrorest}. The goal of\n{mlr3spatiotempcv} is to gather the available spatiotemporal resampling methods\nin {R} and make them available to users through a simple and common interface.\nThis is made possible by integrating the package directly into the {mlr3}\nmachine-learning framework, which already has support for generic\nnon-spatiotemporal resampling methods such as random partitioning. One\nadvantage is the use of a consistent nomenclature in an overarching\nmachine-learning toolkit instead of a varying package-specific syntax, making\nit easier for users to choose from a variety of spatiotemporal resampling\nmethods. This package avoids giving recommendations which method to use in\npractice as this decision depends on the predictive task at hand, the\nautocorrelation within the data, and the spatial structure of the sampling\ndesign or geographic objects being studied.",
    "published_date": "2021-10-25T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.ME"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.12674v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.12508v1",
    "title": "A Deep Learning Approach to Predicting Collateral Flow in Stroke Patients Using Radiomic Features from Perfusion Images",
    "authors": [
      "Giles Tetteh",
      "Fernando Navarro",
      "Johannes Paetzold",
      "Jan Kirschke",
      "Claus Zimmer",
      "Bjoern H. Menze"
    ],
    "author_ids": [],
    "abstract": "Collateral circulation results from specialized anastomotic channels which\nare capable of providing oxygenated blood to regions with compromised blood\nflow caused by ischemic injuries. The quality of collateral circulation has\nbeen established as a key factor in determining the likelihood of a favorable\nclinical outcome and goes a long way to determine the choice of stroke care\nmodel - that is the decision to transport or treat eligible patients\nimmediately.\n  Though there exist several imaging methods and grading criteria for\nquantifying collateral blood flow, the actual grading is mostly done through\nmanual inspection of the acquired images. This approach is associated with a\nnumber of challenges. First, it is time-consuming - the clinician needs to scan\nthrough several slices of images to ascertain the region of interest before\ndeciding on what severity grade to assign to a patient. Second, there is a high\ntendency for bias and inconsistency in the final grade assigned to a patient\ndepending on the experience level of the clinician.\n  We present a deep learning approach to predicting collateral flow grading in\nstroke patients based on radiomic features extracted from MR perfusion data.\nFirst, we formulate a region of interest detection task as a reinforcement\nlearning problem and train a deep learning network to automatically detect the\noccluded region within the 3D MR perfusion volumes. Second, we extract radiomic\nfeatures from the obtained region of interest through local image descriptors\nand denoising auto-encoders. Finally, we apply a convolutional neural network\nand other machine learning classifiers to the extracted radiomic features to\nautomatically predict the collateral flow grading of the given patient volume\nas one of three severity classes - no flow (0), moderate flow (1), and good\nflow (2)...",
    "published_date": "2021-10-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.12508v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.12478v2",
    "title": "Deep Asymmetric Hashing with Dual Semantic Regression and Class Structure Quantization",
    "authors": [
      "Jianglin Lu",
      "Hailing Wang",
      "Jie Zhou",
      "Mengfan Yan",
      "Jiajun Wen"
    ],
    "author_ids": [],
    "abstract": "Recently, deep hashing methods have been widely used in image retrieval task.\nMost existing deep hashing approaches adopt one-to-one quantization to reduce\ninformation loss. However, such class-unrelated quantization cannot give\ndiscriminative feedback for network training. In addition, these methods only\nutilize single label to integrate supervision information of data for hashing\nfunction learning, which may result in inferior network generalization\nperformance and relatively low-quality hash codes since the inter-class\ninformation of data is totally ignored. In this paper, we propose a dual\nsemantic asymmetric hashing (DSAH) method, which generates discriminative hash\ncodes under three-fold constraints. Firstly, DSAH utilizes class prior to\nconduct class structure quantization so as to transmit class information during\nthe quantization process. Secondly, a simple yet effective label mechanism is\ndesigned to characterize both the intra-class compactness and inter-class\nseparability of data, thereby achieving semantic-sensitive binary code\nlearning. Finally, a meaningful pairwise similarity preserving loss is devised\nto minimize the distances between class-related network outputs based on an\naffinity graph. With these three main components, high-quality hash codes can\nbe generated through network. Extensive experiments conducted on various\ndatasets demonstrate the superiority of DSAH in comparison with\nstate-of-the-art deep hashing methods.",
    "published_date": "2021-10-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.12478v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.12403v3",
    "title": "Learning to Estimate Without Bias",
    "authors": [
      "Tzvi Diskin",
      "Yonina C. Eldar",
      "Ami Wiesel"
    ],
    "author_ids": [],
    "abstract": "The Gauss Markov theorem states that the weighted least squares estimator is\na linear minimum variance unbiased estimation (MVUE) in linear models. In this\npaper, we take a first step towards extending this result to non linear\nsettings via deep learning with bias constraints. The classical approach to\ndesigning non-linear MVUEs is through maximum likelihood estimation (MLE) which\noften involves computationally challenging optimizations. On the other hand,\ndeep learning methods allow for non-linear estimators with fixed computational\ncomplexity. Learning based estimators perform optimally on average with respect\nto their training set but may suffer from significant bias in other parameters.\nTo avoid this, we propose to add a simple bias constraint to the loss function,\nresulting in an estimator we refer to as Bias Constrained Estimator (BCE). We\nprove that this yields asymptotic MVUEs that behave similarly to the classical\nMLEs and asymptotically attain the Cramer Rao bound. We demonstrate the\nadvantages of our approach in the context of signal to noise ratio estimation\nas well as covariance estimation. A second motivation to BCE is in applications\nwhere multiple estimates of the same unknown are averaged for improved\nperformance. Examples include distributed sensor networks and data augmentation\nin test-time. In such applications, we show that BCE leads to asymptotically\nconsistent estimators.",
    "published_date": "2021-10-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.12403v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.12285v1",
    "title": "Generalized Resubstitution for Classification Error Estimation",
    "authors": [
      "Parisa Ghane",
      "Ulisses Braga-Neto"
    ],
    "author_ids": [],
    "abstract": "We propose the family of generalized resubstitution classifier error\nestimators based on empirical measures. These error estimators are\ncomputationally efficient and do not require re-training of classifiers. The\nplain resubstitution error estimator corresponds to choosing the standard\nempirical measure. Other choices of empirical measure lead to bolstered,\nposterior-probability, Gaussian-process, and Bayesian error estimators; in\naddition, we propose bolstered posterior-probability error estimators as a new\nfamily of generalized resubstitution estimators. In the two-class case, we show\nthat a generalized resubstitution estimator is consistent and asymptotically\nunbiased, regardless of the distribution of the features and label, if the\ncorresponding generalized empirical measure converges uniformly to the standard\nempirical measure and the classification rule has a finite VC dimension. A\ngeneralized resubstitution estimator typically has hyperparameters that can be\ntuned to control its bias and variance, which adds flexibility. Numerical\nexperiments with various classification rules trained on synthetic data assess\nthe thefinite-sample performance of several representative generalized\nresubstitution error estimators. In addition, results of an image\nclassification experiment using the LeNet-5 convolutional neural network and\nthe MNIST data set demonstrate the potential of this class of error estimators\nin deep learning for computer vision.",
    "published_date": "2021-10-23T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.12285v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.12185v1",
    "title": "Group-disentangled Representation Learning with Weakly-Supervised Regularization",
    "authors": [
      "Linh Tran",
      "Amir Hosein Khasahmadi",
      "Aditya Sanghi",
      "Saeid Asgari"
    ],
    "author_ids": [],
    "abstract": "Learning interpretable and human-controllable representations that uncover\nfactors of variation in data remains an ongoing key challenge in representation\nlearning. We investigate learning group-disentangled representations for groups\nof factors with weak supervision. Existing techniques to address this challenge\nmerely constrain the approximate posterior by averaging over observations of a\nshared group. As a result, observations with a common set of variations are\nencoded to distinct latent representations, reducing their capacity to\ndisentangle and generalize to downstream tasks. In contrast to previous works,\nwe propose GroupVAE, a simple yet effective Kullback-Leibler (KL)\ndivergence-based regularization across shared latent representations to enforce\nconsistent and disentangled representations. We conduct a thorough evaluation\nand demonstrate that our GroupVAE significantly improves group disentanglement.\nFurther, we demonstrate that learning group-disentangled representations\nimprove upon downstream tasks, including fair classification and 3D\nshape-related tasks such as reconstruction, classification, and transfer\nlearning, and is competitive to supervised methods.",
    "published_date": "2021-10-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.12185v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.12149v2",
    "title": "On Parameter Estimation in Unobserved Components Models subject to Linear Inequality Constraints",
    "authors": [
      "Abhishek K. Umrawal",
      "Joshua C. C. Chan"
    ],
    "author_ids": [],
    "abstract": "We propose a new \\textit{quadratic programming-based} method of approximating\na nonstandard density using a multivariate Gaussian density. Such nonstandard\ndensities usually arise while developing posterior samplers for unobserved\ncomponents models involving inequality constraints on the parameters. For\ninstance, Chan et al. (2016) provided a new model of trend inflation with\nlinear inequality constraints on the stochastic trend. We implemented the\nproposed quadratic programming-based method for this model and compared it to\nthe existing approximation. We observed that the proposed method works as well\nas the existing approximation in terms of the final trend estimates while\nachieving gains in terms of sample efficiency.",
    "published_date": "2021-10-23T00:00:00",
    "year": 2021,
    "categories": [
      "econ.EM",
      "cs.CE",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.12149v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.12132v2",
    "title": "Towards the D-Optimal Online Experiment Design for Recommender Selection",
    "authors": [
      "Da Xu",
      "Chuanwei Ruan",
      "Evren Korpeoglu",
      "Sushant Kumar",
      "Kannan Achan"
    ],
    "author_ids": [],
    "abstract": "Selecting the optimal recommender via online exploration-exploitation is\ncatching increasing attention where the traditional A/B testing can be slow and\ncostly, and offline evaluations are prone to the bias of history data. Finding\nthe optimal online experiment is nontrivial since both the users and displayed\nrecommendations carry contextual features that are informative to the reward.\nWhile the problem can be formalized via the lens of multi-armed bandits, the\nexisting solutions are found less satisfactorily because the general\nmethodologies do not account for the case-specific structures, particularly for\nthe e-commerce recommendation we study. To fill in the gap, we leverage the\n\\emph{D-optimal design} from the classical statistics literature to achieve the\nmaximum information gain during exploration, and reveal how it fits seamlessly\nwith the modern infrastructure of online inference. To demonstrate the\neffectiveness of the optimal designs, we provide semi-synthetic simulation\nstudies with published code and data for reproducibility purposes. We then use\nour deployment example on Walmart.com to fully illustrate the practical\ninsights and effectiveness of the proposed methods.",
    "published_date": "2021-10-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.12132v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.12038v1",
    "title": "Characterizing Performance Inequity Across U.S. Ookla Speedtest Users",
    "authors": [
      "Udit Paul",
      "Jiamo Liu",
      "Vivek Adarsh",
      "Mengyang Gu",
      "Arpit Gupta",
      "Elizabeth Belding"
    ],
    "author_ids": [],
    "abstract": "The Internet has become indispensable to daily activities, such as work,\neducation and health care. Many of these activities require Internet access\ndata rates that support real-time video conferencing. However, digital\ninequality persists across the United States, not only in who has access but in\nthe quality of that access. Speedtest by Ookla allows users to run network\ndiagnostic tests to better understand the current performance of their network.\nIn this work, we leverage an Internet performance dataset from Ookla, together\nwith an ESRI demographic dataset, to conduct a comprehensive analysis that\ncharacterizes performance differences between Speedtest users across the U.S.\nOur analysis shows that median download speeds for Speedtest users can differ\nby over 150Mbps between states. Further, there are important distinctions\nbetween user categories. For instance, all but one state showed statistically\nsignificant differences in performance between Speedtest users in urban and\nrural areas. The difference also exists in urban areas between high and low\nincome users in 27 states. Our analysis reveals that states that demonstrate\nthis disparity in Speedtest results are geographically bigger, more populous\nand have a wider dispersion of median household income. We conclude by\nhighlighting several challenges to the complex problem space of digital\ninequality characterization and provide recommendations for furthering research\non this topic.",
    "published_date": "2021-10-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.12038v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2110.12020v1",
    "title": "Fairness Degrading Adversarial Attacks Against Clustering Algorithms",
    "authors": [
      "Anshuman Chhabra",
      "Adish Singla",
      "Prasant Mohapatra"
    ],
    "author_ids": [],
    "abstract": "Clustering algorithms are ubiquitous in modern data science pipelines, and\nare utilized in numerous fields ranging from biology to facility location. Due\nto their widespread use, especially in societal resource allocation problems,\nrecent research has aimed at making clustering algorithms fair, with great\nsuccess. Furthermore, it has also been shown that clustering algorithms, much\nlike other machine learning algorithms, are susceptible to adversarial attacks\nwhere a malicious entity seeks to subvert the performance of the learning\nalgorithm. However, despite these known vulnerabilities, there has been no\nresearch undertaken that investigates fairness degrading adversarial attacks\nfor clustering. We seek to bridge this gap by formulating a generalized attack\noptimization problem aimed at worsening the group-level fairness of\ncentroid-based clustering algorithms. As a first step, we propose a fairness\ndegrading attack algorithm for k-median clustering that operates under a\nwhitebox threat model -- where the clustering algorithm, fairness notion, and\nthe input dataset are known to the adversary. We provide empirical results as\nwell as theoretical analysis for our simple attack algorithm, and find that the\naddition of the generated adversarial samples can lead to significantly lower\nfairness values. In this manner, we aim to motivate fairness degrading\nadversarial attacks as a direction for future research in fair clustering.",
    "published_date": "2021-10-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.12020v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.12002v1",
    "title": "Fairness in Missing Data Imputation",
    "authors": [
      "Yiliang Zhang",
      "Qi Long"
    ],
    "author_ids": [],
    "abstract": "Missing data are ubiquitous in the era of big data and, if inadequately\nhandled, are known to lead to biased findings and have deleterious impact on\ndata-driven decision makings. To mitigate its impact, many missing value\nimputation methods have been developed. However, the fairness of these\nimputation methods across sensitive groups has not been studied. In this paper,\nwe conduct the first known research on fairness of missing data imputation. By\nstudying the performance of imputation methods in three commonly used datasets,\nwe demonstrate that unfairness of missing value imputation widely exists and\nmay be associated with multiple factors. Our results suggest that, in practice,\na careful investigation of related factors can provide valuable insights on\nmitigating unfairness associated with missing data imputation.",
    "published_date": "2021-10-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ME"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.12002v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.11794v3",
    "title": "Federated Unlearning via Class-Discriminative Pruning",
    "authors": [
      "Junxiao Wang",
      "Song Guo",
      "Xin Xie",
      "Heng Qi"
    ],
    "author_ids": [],
    "abstract": "We explore the problem of selectively forgetting categories from trained CNN\nclassification models in the federated learning (FL). Given that the data used\nfor training cannot be accessed globally in FL, our insights probe deep into\nthe internal influence of each channel. Through the visualization of feature\nmaps activated by different channels, we observe that different channels have a\nvarying contribution to different categories in image classification. Inspired\nby this, we propose a method for scrubbing the model clean of information about\nparticular categories. The method does not require retraining from scratch, nor\nglobal access to the data used for training. Instead, we introduce the concept\nof Term Frequency Inverse Document Frequency (TF-IDF) to quantize the class\ndiscrimination of channels. Channels with high TF-IDF scores have more\ndiscrimination on the target categories and thus need to be pruned to unlearn.\nThe channel pruning is followed by a fine-tuning process to recover the\nperformance of the pruned model. Evaluated on CIFAR10 dataset, our method\naccelerates the speed of unlearning by 8.9x for the ResNet model, and 7.9x for\nthe VGG model under no degradation in accuracy, compared to retraining from\nscratch. For CIFAR100 dataset, the speedups are 9.9x and 8.4x, respectively. We\nenvision this work as a complementary block for FL towards compliance with\nlegal and ethical criteria.",
    "published_date": "2021-10-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.CR",
      "cs.DC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.11794v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.11784v3",
    "title": "Safe rules for the identification of zeros in the solutions of the SLOPE problem",
    "authors": [
      "Clément Elvira",
      "Cédric Herzet"
    ],
    "author_ids": [],
    "abstract": "In this paper we propose a methodology to accelerate the resolution of the\nso-called \"Sorted L-One Penalized Estimation\" (SLOPE) problem. Our method\nleverages the concept of \"safe screening\", well-studied in the literature for\n\\textit{group-separable} sparsity-inducing norms, and aims at identifying the\nzeros in the solution of SLOPE. More specifically, we derive a set of\n\\(\\tfrac{n(n+1)}{2}\\) inequalities for each element of the \\(n\\)-dimensional\nprimal vector and prove that the latter can be safely screened if some subsets\nof these inequalities are verified. We propose moreover an efficient algorithm\nto jointly apply the proposed procedure to all the primal variables. Our\nprocedure has a complexity \\(\\mathcal{O}(n\\log n + LT)\\) where \\(T\\leq n\\) is a\nproblem-dependent constant and \\(L\\) is the number of zeros identified by the\ntests. Numerical experiments confirm that, for a prescribed computational\nbudget, the proposed methodology leads to significant improvements of the\nsolving precision.",
    "published_date": "2021-10-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.11784v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.12906v3",
    "title": "Tackling the Local Bias in Federated Graph Learning",
    "authors": [
      "Binchi Zhang",
      "Minnan Luo",
      "Shangbin Feng",
      "Ziqi Liu",
      "Jun Zhou",
      "Qinghua Zheng"
    ],
    "author_ids": [],
    "abstract": "Federated graph learning (FGL) has become an important research topic in\nresponse to the increasing scale and the distributed nature of graph-structured\ndata in the real world. In FGL, a global graph is distributed across different\nclients, where each client holds a subgraph. Existing FGL methods often fail to\neffectively utilize cross-client edges, losing structural information during\nthe training; additionally, local graphs often exhibit significant distribution\ndivergence. These two issues make local models in FGL less desirable than in\ncentralized graph learning, namely the local bias problem in this paper. To\nsolve this problem, we propose a novel FGL framework to make the local models\nsimilar to the model trained in a centralized setting. Specifically, we design\na distributed learning scheme, fully leveraging cross-client edges to aggregate\ninformation from other clients. In addition, we propose a label-guided sampling\napproach to alleviate the imbalanced local data and meanwhile, distinctly\nreduce the training overhead. Extensive experiments demonstrate that local bias\ncan compromise the model performance and slow down the convergence during\ntraining. Experimental results also verify that our framework successfully\nmitigates local bias, achieving better performance than other baselines with\nlower time and memory overhead.",
    "published_date": "2021-10-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.12906v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.11611v4",
    "title": "Error-correcting neural networks for semi-Lagrangian advection in the level-set method",
    "authors": [
      "Luis Ángel Larios-Cárdenas",
      "Frédéric Gibou"
    ],
    "author_ids": [],
    "abstract": "We present a machine learning framework that blends image super-resolution\ntechnologies with passive, scalar transport in the level-set method. Here, we\ninvestigate whether we can compute on-the-fly, data-driven corrections to\nminimize numerical viscosity in the coarse-mesh evolution of an interface. The\nproposed system's starting point is the semi-Lagrangian formulation. And, to\nreduce numerical dissipation, we introduce an error-quantifying multilayer\nperceptron. The role of this neural network is to improve the numerically\nestimated surface trajectory. To do so, it processes localized level-set,\nvelocity, and positional data in a single time frame for select vertices near\nthe moving front. Our main contribution is thus a novel\nmachine-learning-augmented transport algorithm that operates alongside\nselective redistancing and alternates with conventional advection to keep the\nadjusted interface trajectory smooth. Consequently, our procedure is more\nefficient than full-scan convolutional-based applications because it\nconcentrates computational effort only around the free boundary. Also, we show\nthrough various tests that our strategy is effective at counteracting both\nnumerical diffusion and mass loss. In simple advection problems, for example,\nour method can achieve the same precision as the baseline scheme at twice the\nresolution but at a fraction of the cost. Similarly, our hybrid technique can\nproduce feasible solidification fronts for crystallization processes. On the\nother hand, tangential shear flows and highly deforming simulations can\nprecipitate bias artifacts and inference deterioration. Likewise, stringent\ndesign velocity constraints can limit our solver's application to problems\ninvolving rapid interface changes. In the latter cases, we have identified\nseveral opportunities to enhance robustness without forgoing our approach's\nbasic concept.",
    "published_date": "2021-10-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.NA",
      "math.NA",
      "65D15, 65D18, 65N06, 65N50, 65Z05, 68T20",
      "I.2.6; G.1.8"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.11611v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.11560v1",
    "title": "Adaptive Bridge between Training and Inference for Dialogue",
    "authors": [
      "Haoran Xu",
      "Hainan Zhang",
      "Yanyan Zou",
      "Hongshen Chen",
      "Zhuoye Ding",
      "Yanyan Lan"
    ],
    "author_ids": [],
    "abstract": "Although exposure bias has been widely studied in some NLP tasks, it faces\nits unique challenges in dialogue response generation, the representative\none-to-various generation scenario. In real human dialogue, there are many\nappropriate responses for the same context, not only with different\nexpressions, but also with different topics. Therefore, due to the much bigger\ngap between various ground-truth responses and the generated synthetic\nresponse, exposure bias is more challenging in dialogue generation task. What's\nmore, as MLE encourages the model to only learn the common words among\ndifferent ground-truth responses, but ignores the interesting and specific\nparts, exposure bias may further lead to the common response generation\nproblem, such as \"I don't know\" and \"HaHa?\" In this paper, we propose a novel\nadaptive switching mechanism, which learns to automatically transit between\nground-truth learning and generated learning regarding the word-level matching\nscore, such as the cosine similarity. Experimental results on both Chinese STC\ndataset and English Reddit dataset, show that our adaptive method achieves a\nsignificant improvement in terms of metric-based evaluation and human\nevaluation, as compared with the state-of-the-art exposure bias approaches.\nFurther analysis on NMT task also shows that our model can achieve a\nsignificant improvement.",
    "published_date": "2021-10-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.11560v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.11479v1",
    "title": "Synt++: Utilizing Imperfect Synthetic Data to Improve Speech Recognition",
    "authors": [
      "Ting-Yao Hu",
      "Mohammadreza Armandpour",
      "Ashish Shrivastava",
      "Jen-Hao Rick Chang",
      "Hema Koppula",
      "Oncel Tuzel"
    ],
    "author_ids": [],
    "abstract": "With recent advances in speech synthesis, synthetic data is becoming a viable\nalternative to real data for training speech recognition models. However,\nmachine learning with synthetic data is not trivial due to the gap between the\nsynthetic and the real data distributions. Synthetic datasets may contain\nartifacts that do not exist in real data such as structured noise, content\nerrors, or unrealistic speaking styles. Moreover, the synthesis process may\nintroduce a bias due to uneven sampling of the data manifold. We propose two\nnovel techniques during training to mitigate the problems due to the\ndistribution gap: (i) a rejection sampling algorithm and (ii) using separate\nbatch normalization statistics for the real and the synthetic samples. We show\nthat these methods significantly improve the training of speech recognition\nmodels using synthetic data. We evaluate the proposed approach on keyword\ndetection and Automatic Speech Recognition (ASR) tasks, and observe up to 18%\nand 13% relative error reduction, respectively, compared to naively using the\nsynthetic data.",
    "published_date": "2021-10-21T00:00:00",
    "year": 2021,
    "categories": [
      "eess.AS",
      "cs.LG",
      "cs.SD"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.11479v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.11450v1",
    "title": "Online Meta-Learning for Scene-Diverse Waveform-Agile Radar Target Tracking",
    "authors": [
      "Charles E. Thornton",
      "R. Michael Buehrer",
      "Anthony F. Martone"
    ],
    "author_ids": [],
    "abstract": "A fundamental problem for waveform-agile radar systems is that the true\nenvironment is unknown, and transmission policies which perform well for a\nparticular tracking instance may be sub-optimal for another. Additionally,\nthere is a limited time window for each target track, and the radar must learn\nan effective strategy from a sequence of measurements in a timely manner. This\npaper studies a Bayesian meta-learning model for radar waveform selection which\nseeks to learn an inductive bias to quickly optimize tracking performance\nacross a class of radar scenes. We cast the waveform selection problem in the\nframework of sequential Bayesian inference, and introduce a contextual bandit\nvariant of the recently proposed meta-Thompson Sampling algorithm, which learns\nan inductive bias in the form of a prior distribution. Each track is treated as\nan instance of a contextual bandit learning problem, coming from a task\ndistribution. We show that the meta-learning process results in an appreciably\nfaster learning, resulting in significantly fewer lost tracks than a\nconventional learning approach equipped with an uninformative prior.",
    "published_date": "2021-10-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IT",
      "eess.SP",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.11450v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2110.11404v1",
    "title": "Statistical discrimination in learning agents",
    "authors": [
      "Edgar A. Duéñez-Guzmán",
      "Kevin R. McKee",
      "Yiran Mao",
      "Ben Coppin",
      "Silvia Chiappa",
      "Alexander Sasha Vezhnevets",
      "Michiel A. Bakker",
      "Yoram Bachrach",
      "Suzanne Sadedin",
      "William Isaac",
      "Karl Tuyls",
      "Joel Z. Leibo"
    ],
    "author_ids": [],
    "abstract": "Undesired bias afflicts both human and algorithmic decision making, and may\nbe especially prevalent when information processing trade-offs incentivize the\nuse of heuristics. One primary example is \\textit{statistical discrimination}\n-- selecting social partners based not on their underlying attributes, but on\nreadily perceptible characteristics that covary with their suitability for the\ntask at hand. We present a theoretical model to examine how information\nprocessing influences statistical discrimination and test its predictions using\nmulti-agent reinforcement learning with various agent architectures in a\npartner choice-based social dilemma. As predicted, statistical discrimination\nemerges in agent policies as a function of both the bias in the training\npopulation and of agent architecture. All agents showed substantial statistical\ndiscrimination, defaulting to using the readily available correlates instead of\nthe outcome relevant features. We show that less discrimination emerges with\nagents that use recurrent neural networks, and when their training environment\nhas less bias. However, all agent algorithms we tried still exhibited\nsubstantial bias after learning in biased training populations.",
    "published_date": "2021-10-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GT",
      "cs.MA",
      "68T07 (Primary) 91A26, 91-10, 93A16 (Secondary)",
      "I.2.11; I.2.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.11404v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.11150v2",
    "title": "Lottery Tickets with Nonzero Biases",
    "authors": [
      "Jonas Fischer",
      "Advait Gadhikar",
      "Rebekka Burkholz"
    ],
    "author_ids": [],
    "abstract": "The strong lottery ticket hypothesis holds the promise that pruning randomly\ninitialized deep neural networks could offer a computationally efficient\nalternative to deep learning with stochastic gradient descent. Common parameter\ninitialization schemes and existence proofs, however, are focused on networks\nwith zero biases, thus foregoing the potential universal approximation property\nof pruning. To fill this gap, we extend multiple initialization schemes and\nexistence proofs to nonzero biases, including explicit 'looks-linear'\napproaches for ReLU activation functions. These do not only enable truly\northogonal parameter initialization but also reduce potential pruning errors.\nIn experiments on standard benchmark data, we further highlight the practical\nbenefits of nonzero bias initialization schemes, and present theoretically\ninspired extensions for state-of-the-art strong lottery ticket pruning.",
    "published_date": "2021-10-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.11150v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.11039v1",
    "title": "Automated Climate Analyses Using Knowledge Graph",
    "authors": [
      "Jiantao Wu",
      "Huan Chen",
      "Fabrizio Orlandi",
      "Yee Hui Lee",
      "Declan O'Sullivan",
      "Soumyabrata Dev"
    ],
    "author_ids": [],
    "abstract": "The FAIR (Findable, Accessible, Interoperable, Reusable) data principles are\nfundamental for climate researchers and all stakeholders in the current digital\necosystem. In this paper, we demonstrate how relational climate data can be\n\"FAIR\" and modeled using RDF, in line with Semantic Web technologies and our\nClimate Analysis ontology. Thus, heterogeneous climate data can be stored in\ngraph databases and offered as Linked Data on the Web. As a result, climate\nresearchers will be able to use the standard SPARQL query language to query\nthese sources directly on the Web. In this paper, we demonstrate the usefulness\nof our SPARQL endpoint for automated climate analytics. We illustrate two\nsample use cases that establish the advantage of representing climate data as\nknowledge graphs.",
    "published_date": "2021-10-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DB"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.11039v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2110.10980v1",
    "title": "Ethics-Based Auditing of Automated Decision-Making Systems: Nature, Scope, and Limitations",
    "authors": [
      "Jakob Mokander",
      "Jessica Morley",
      "Mariarosaria Taddeo",
      "Luciano Floridi"
    ],
    "author_ids": [],
    "abstract": "Important decisions that impact human lives, livelihoods, and the natural\nenvironment are increasingly being automated. Delegating tasks to so-called\nautomated decision-making systems (ADMS) can improve efficiency and enable new\nsolutions. However, these benefits are coupled with ethical challenges. For\nexample, ADMS may produce discriminatory outcomes, violate individual privacy,\nand undermine human self-determination. New governance mechanisms are thus\nneeded that help organisations design and deploy ADMS in ways that are ethical,\nwhile enabling society to reap the full economic and social benefits of\nautomation. In this article, we consider the feasibility and efficacy of\nethics-based auditing (EBA) as a governance mechanism that allows organisations\nto validate claims made about their ADMS. Building on previous work, we define\nEBA as a structured process whereby an entity's present or past behaviour is\nassessed for consistency with relevant principles or norms. We then offer three\ncontributions to the existing literature. First, we provide a theoretical\nexplanation of how EBA can contribute to good governance by promoting\nprocedural regularity and transparency. Second, we propose seven criteria for\nhow to design and implement EBA procedures successfully. Third, we identify and\ndiscuss the conceptual, technical, social, economic, organisational, and\ninstitutional constraints associated with EBA. We conclude that EBA should be\nconsidered an integral component of multifaced approaches to managing the\nethical risks posed by ADMS.",
    "published_date": "2021-10-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.10980v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2110.10974v1",
    "title": "A Decentralized Framework for Serverless Edge Computing in the Internet of Things",
    "authors": [
      "Claudio Cicconetti",
      "Marco Conti",
      "Andrea Passarella"
    ],
    "author_ids": [],
    "abstract": "Serverless computing is becoming widely adopted among cloud providers, thus\nmaking increasingly popular the Function-as-a-Service (FaaS) programming model,\nwhere the developers realize services by packaging sequences of stateless\nfunction calls.\n  The current technologies are very well suited to data centers, but cannot\nprovide equally good performance in decentralized environments, such as edge\ncomputing systems, which are expected to be typical for Internet of Things\n(IoT) applications.\n  In this paper, we fill this gap by proposing a framework for efficient\ndispatching of stateless tasks to in-network executors so as to minimize the\nresponse times while exhibiting short- and long-term fairness, also leveraging\ninformation from a virtualized network infrastructure when available.\n  Our solution is shown to be simple enough to be installed on devices with\nlimited computational capabilities, such as IoT gateways, especially when using\na hierarchical forwarding extension.\n  We evaluate the proposed platform by means of extensive emulation experiments\nwith a prototype implementation in realistic conditions.\n  The results show that it is able to smoothly adapt to the mobility of clients\nand to the variations of their service request patterns, while coping promptly\nwith network congestion.",
    "published_date": "2021-10-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.NI",
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.10974v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2110.10926v1",
    "title": "PipAttack: Poisoning Federated Recommender Systems forManipulating Item Promotion",
    "authors": [
      "Shijie Zhang",
      "Hongzhi Yin",
      "Tong Chen",
      "Zi Huang",
      "Quoc Viet Hung Nguyen",
      "Lizhen Cui"
    ],
    "author_ids": [],
    "abstract": "Due to the growing privacy concerns, decentralization emerges rapidly in\npersonalized services, especially recommendation. Also, recent studies have\nshown that centralized models are vulnerable to poisoning attacks, compromising\ntheir integrity. In the context of recommender systems, a typical goal of such\npoisoning attacks is to promote the adversary's target items by interfering\nwith the training dataset and/or process. Hence, a common practice is to\nsubsume recommender systems under the decentralized federated learning\nparadigm, which enables all user devices to collaboratively learn a global\nrecommender while retaining all the sensitive data locally. Without exposing\nthe full knowledge of the recommender and entire dataset to end-users, such\nfederated recommendation is widely regarded `safe' towards poisoning attacks.\nIn this paper, we present a systematic approach to backdooring federated\nrecommender systems for targeted item promotion. The core tactic is to take\nadvantage of the inherent popularity bias that commonly exists in data-driven\nrecommenders. As popular items are more likely to appear in the recommendation\nlist, our innovatively designed attack model enables the target item to have\nthe characteristics of popular items in the embedding space. Then, by uploading\ncarefully crafted gradients via a small number of malicious users during the\nmodel update, we can effectively increase the exposure rate of a target\n(unpopular) item in the resulted federated recommender. Evaluations on two\nreal-world datasets show that 1) our attack model significantly boosts the\nexposure rate of the target item in a stealthy way, without harming the\naccuracy of the poisoned recommender; and 2) existing defenses are not\neffective enough, highlighting the need for new defenses against our local\nmodel poisoning attacks to federated recommender systems.",
    "published_date": "2021-10-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.10926v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.10921v2",
    "title": "CATRO: Channel Pruning via Class-Aware Trace Ratio Optimization",
    "authors": [
      "Wenzheng Hu",
      "Zhengping Che",
      "Ning Liu",
      "Mingyang Li",
      "Jian Tang",
      "Changshui Zhang",
      "Jianqiang Wang"
    ],
    "author_ids": [],
    "abstract": "Deep convolutional neural networks are shown to be overkill with high\nparametric and computational redundancy in many application scenarios, and an\nincreasing number of works have explored model pruning to obtain lightweight\nand efficient networks. However, most existing pruning approaches are driven by\nempirical heuristic and rarely consider the joint impact of channels, leading\nto unguaranteed and suboptimal performance. In this paper, we propose a novel\nchannel pruning method via Class-Aware Trace Ratio Optimization (CATRO) to\nreduce the computational burden and accelerate the model inference. Utilizing\nclass information from a few samples, CATRO measures the joint impact of\nmultiple channels by feature space discriminations and consolidates the\nlayer-wise impact of preserved channels. By formulating channel pruning as a\nsubmodular set function maximization problem, CATRO solves it efficiently via a\ntwo-stage greedy iterative optimization procedure. More importantly, we present\ntheoretical justifications on convergence of CATRO and performance of pruned\nnetworks. Experimental results demonstrate that CATRO achieves higher accuracy\nwith similar computation cost or lower computation cost with similar accuracy\nthan other state-of-the-art channel pruning algorithms. In addition, because of\nits class-aware property, CATRO is suitable to prune efficient networks\nadaptively for various classification subtasks, enhancing handy deployment and\nusage of deep networks in real-world applications.",
    "published_date": "2021-10-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.10921v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.10864v1",
    "title": "Class-Discriminative CNN Compression",
    "authors": [
      "Yuchen Liu",
      "David Wentzlaff",
      "S. Y. Kung"
    ],
    "author_ids": [],
    "abstract": "Compressing convolutional neural networks (CNNs) by pruning and distillation\nhas received ever-increasing focus in the community. In particular, designing a\nclass-discrimination based approach would be desired as it fits seamlessly with\nthe CNNs training objective. In this paper, we propose class-discriminative\ncompression (CDC), which injects class discrimination in both pruning and\ndistillation to facilitate the CNNs training goal. We first study the\neffectiveness of a group of discriminant functions for channel pruning, where\nwe include well-known single-variate binary-class statistics like Student's\nT-Test in our study via an intuitive generalization. We then propose a novel\nlayer-adaptive hierarchical pruning approach, where we use a coarse class\ndiscrimination scheme for early layers and a fine one for later layers. This\nmethod naturally accords with the fact that CNNs process coarse semantics in\nthe early layers and extract fine concepts at the later. Moreover, we leverage\ndiscriminant component analysis (DCA) to distill knowledge of intermediate\nrepresentations in a subspace with rich discriminative information, which\nenhances hidden layers' linear separability and classification accuracy of the\nstudent. Combining pruning and distillation, CDC is evaluated on CIFAR and\nILSVRC 2012, where we consistently outperform the state-of-the-art results.",
    "published_date": "2021-10-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.10864v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.10828v2",
    "title": "AdamD: Improved bias-correction in Adam",
    "authors": [
      "John St John"
    ],
    "author_ids": [],
    "abstract": "Here I present a small update to the bias-correction term in the Adam\noptimizer that has the advantage of making smaller gradient updates in the\nfirst several steps of training. With the default bias-correction, Adam may\nactually make larger than requested gradient updates early in training. By only\nincluding the well-justified bias-correction of the second moment gradient\nestimate, $v_t$, and excluding the bias-correction on the first-order estimate,\n$m_t$, we attain these more desirable gradient update properties in the first\nseries of steps. The default implementation of Adam may be as sensitive as it\nis to the hyperparameters $\\beta_1, \\beta_2$ partially due to the originally\nproposed bias correction procedure, and its behavior in early steps.",
    "published_date": "2021-10-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.10828v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.10740v3",
    "title": "Log-concave poset inequalities",
    "authors": [
      "Swee Hong Chan",
      "Igor Pak"
    ],
    "author_ids": [],
    "abstract": "We study combinatorial inequalities for various classes of set systems:\nmatroids, polymatroids, poset antimatroids, and interval greedoids. We prove\nlog-concavity inequalities for counting certain weighted feasible words, which\ngeneralize and extend several previous results establishing Mason conjectures\nfor the numbers of independent sets of matroids. Notably, we prove matching\nequality conditions for both earlier inequalities and our extensions.\n  In contrast with much of the previous work, our proofs are combinatorial and\nemploy nothing but linear algebra. We use the language formulation of greedoids\nwhich allows a linear algebraic setup, which in turn can be analyzed\nrecursively. The underlying non-commutative nature of matrices associated with\ngreedoids allows us to proceed beyond polymatroids and prove the equality\nconditions. As further application of our tools, we rederive both Stanley's\ninequality on the number of certain linear extensions, and its equality\nconditions, which we then also extend to the weighted case.",
    "published_date": "2021-10-20T00:00:00",
    "year": 2021,
    "categories": [
      "math.CO",
      "cs.DM",
      "05A20 (Primary) 05B35, 06A11 (Secondary)"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.10740v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2110.10534v1",
    "title": "FairNet: A Measurement Framework for Traffic Discrimination Detection on the Internet",
    "authors": [
      "Vinod S. Khandkar",
      "Manjesh K. Hanawal"
    ],
    "author_ids": [],
    "abstract": "Network neutrality is related to the non-discriminatory treatment of packets\non the Internet. Any deliberate discrimination of traffic of one application\nwhile favoring others violates the principle of neutrality. Many countries have\nenforced laws against such discrimination. To enforce such laws, one requires\ntools to detect any net neutrality violations. However, detecting such\nviolations is challenging as it is hard to separate any degradation in quality\ndue to natural network effects and selective degradation. Also, legitimate\ntraffic management and deliberate discrimination methods can be technically the\nsame, making it further challenging to distinguish them.\n  We developed an end-to-end measurement framework named FairNet to detect\ndiscrimination of traffic. It compares the performance of similar services. Our\nfocus is on HTTPS streaming services which constitute a predominant portion of\nthe Internet traffic. The effect of confounding factors (congestion, traffic\nmanagement policy, dynamic rate adaptation) is made `similar' on the test\nservices to ensure a fair comparison. FairNet framework uses a ``replay\nserver'' and user-client that exchanges correctly identifiable traffic streams\nover the Internet. The Server Name Indication (SNI) field in the TLS handshake,\nwhich goes in plaintext, ensures that the traffic from the replay server\nappears to network middle-boxes as that coming from its actual server. We\nvalidated that appropriate SNIs results in the correct classification of\nservices using a commercial traffic shaper. FairNet uses two novel algorithms\nbased on application-level throughput and connection status to detect traffic\ndiscrimination. We also validated the methodology's effectiveness by collecting\nnetwork logs through mobile apps over the live Internet and analyzing them.",
    "published_date": "2021-10-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.NI",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.10534v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2110.10389v1",
    "title": "Does Data Repair Lead to Fair Models? Curating Contextually Fair Data To Reduce Model Bias",
    "authors": [
      "Sharat Agarwal",
      "Sumanyu Muku",
      "Saket Anand",
      "Chetan Arora"
    ],
    "author_ids": [],
    "abstract": "Contextual information is a valuable cue for Deep Neural Networks (DNNs) to\nlearn better representations and improve accuracy. However, co-occurrence bias\nin the training dataset may hamper a DNN model's generalizability to unseen\nscenarios in the real world. For example, in COCO, many object categories have\na much higher co-occurrence with men compared to women, which can bias a DNN's\nprediction in favor of men. Recent works have focused on task-specific training\nstrategies to handle bias in such scenarios, but fixing the available data is\noften ignored. In this paper, we propose a novel and more generic solution to\naddress the contextual bias in the datasets by selecting a subset of the\nsamples, which is fair in terms of the co-occurrence with various classes for a\nprotected attribute. We introduce a data repair algorithm using the coefficient\nof variation, which can curate fair and contextually balanced data for a\nprotected class(es). This helps in training a fair model irrespective of the\ntask, architecture or training methodology. Our proposed solution is simple,\neffective, and can even be used in an active learning setting where the data\nlabels are not present or being generated incrementally. We demonstrate the\neffectiveness of our algorithm for the task of object detection and multi-label\nimage classification across different datasets. Through a series of\nexperiments, we validate that curating contextually fair data helps make model\npredictions fair by balancing the true positive rate for the protected class\nacross groups without compromising on the model's overall performance.",
    "published_date": "2021-10-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.10389v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.10200v1",
    "title": "fairadapt: Causal Reasoning for Fair Data Pre-processing",
    "authors": [
      "Drago Plečko",
      "Nicolas Bennett",
      "Nicolai Meinshausen"
    ],
    "author_ids": [],
    "abstract": "Machine learning algorithms are useful for various predictions tasks, but\nthey can also learn how to discriminate, based on gender, race or other\nsensitive attributes. This realization gave rise to the field of fair machine\nlearning, which aims to measure and mitigate such algorithmic bias. This\nmanuscript describes the R-package fairadapt, which implements a causal\ninference pre-processing method. By making use of a causal graphical model and\nthe observed data, the method can be used to address hypothetical questions of\nthe form \"What would my salary have been, had I been of a different\ngender/race?\". Such individual level counterfactual reasoning can help\neliminate discrimination and help justify fair decisions. We also discuss\nappropriate relaxations which assume certain causal pathways from the sensitive\nattribute to the outcome are not discriminatory.",
    "published_date": "2021-10-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML",
      "I.2.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.10200v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.10122v1",
    "title": "Electricity Tariff Design via Lens of Energy Justice",
    "authors": [
      "Hafiz Anwar Ullah Khan",
      "Burcin Unel",
      "Yury Dvorkin"
    ],
    "author_ids": [],
    "abstract": "Distributed Energy Resources (DERs) can significantly affect the net social\nbenefit in power systems, raising concerns pertaining to distributive justice,\nequity, and fairness. Electricity tariff and DERs share a symbiotic\nrelationship whereby the design of the former directly impacts the economic\nefficiency and equity in the system. Current tariff design approaches suffer\nfrom opaque efficiency-equity trade-offs and are also agnostic of the\nexternalities that affect both economic efficiency and equity. Therefore, this\npaper develops a justice-cognizant tariff design framework that improves the\neconomic efficiency of tariff without sacrificing its distributional equity,\nand encompasses economic welfare, social costs of environmental and public\nhealth impacts, and socio-economic and demographic characteristics of\nelectricity consumers. The proposed framework is based on a Single Leader\nSingle Follower (SLSF) game incorporating a multi-objective optimization\nproblem, and is evaluated on four different tariff structures. The SLSF game is\nreformulated as a Multi-Objective Problem with Equilibrium Constraints (MOPEC)\nand is solved by integrating the objective sum method for multi-objective\noptimization and Scholtes's relaxation technique for equilibrium constraints.\nWe compare the economic efficiency and equity of the proposed framework using\nthe 11-zone New York ISO and 7-bus Manhattan power networks. The results\ndemonstrate that spatially- and temporally-granular tariffs ensure equity and\neconomic efficiency at a lower energy burden to consumers.",
    "published_date": "2021-10-19T00:00:00",
    "year": 2021,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.10122v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2110.09843v2",
    "title": "AequeVox: Automated Fairness Testing of Speech Recognition Systems",
    "authors": [
      "Sai Sathiesh Rajan",
      "Sakshi Udeshi",
      "Sudipta Chattopadhyay"
    ],
    "author_ids": [],
    "abstract": "Automatic Speech Recognition (ASR) systems have become ubiquitous. They can\nbe found in a variety of form factors and are increasingly important in our\ndaily lives. As such, ensuring that these systems are equitable to different\nsubgroups of the population is crucial. In this paper, we introduce, AequeVox,\nan automated testing framework for evaluating the fairness of ASR systems.\nAequeVox simulates different environments to assess the effectiveness of ASR\nsystems for different populations. In addition, we investigate whether the\nchosen simulations are comprehensible to humans. We further propose a fault\nlocalization technique capable of identifying words that are not robust to\nthese varying environments. Both components of AequeVox are able to operate in\nthe absence of ground truth data.\n  We evaluated AequeVox on speech from four different datasets using three\ndifferent commercial ASRs. Our experiments reveal that non-native English,\nfemale and Nigerian English speakers generate 109%, 528.5% and 156.9% more\nerrors, on average than native English, male and UK Midlands speakers,\nrespectively. Our user study also reveals that 82.9% of the simulations\n(employed through speech transformations) had a comprehensibility rating above\nseven (out of ten), with the lowest rating being 6.78. This further validates\nthe fairness violations discovered by AequeVox. Finally, we show that the\nnon-robust words, as predicted by the fault localization technique embodied in\nAequeVox, show 223.8% more errors than the predicted robust words across all\nASRs.",
    "published_date": "2021-10-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.09843v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.09839v1",
    "title": "Measuring Hidden Bias within Face Recognition via Racial Phenotypes",
    "authors": [
      "Seyma Yucer",
      "Furkan Tektas",
      "Noura Al Moubayed",
      "Toby P. Breckon"
    ],
    "author_ids": [],
    "abstract": "Recent work reports disparate performance for intersectional racial groups\nacross face recognition tasks: face verification and identification. However,\nthe definition of those racial groups has a significant impact on the\nunderlying findings of such racial bias analysis. Previous studies define these\ngroups based on either demographic information (e.g. African, Asian etc.) or\nskin tone (e.g. lighter or darker skins). The use of such sensitive or broad\ngroup definitions has disadvantages for bias investigation and subsequent\ncounter-bias solutions design. By contrast, this study introduces an\nalternative racial bias analysis methodology via facial phenotype attributes\nfor face recognition. We use the set of observable characteristics of an\nindividual face where a race-related facial phenotype is hence specific to the\nhuman face and correlated to the racial profile of the subject. We propose\ncategorical test cases to investigate the individual influence of those\nattributes on bias within face recognition tasks. We compare our\nphenotype-based grouping methodology with previous grouping strategies and show\nthat phenotype-based groupings uncover hidden bias without reliance upon any\npotentially protected attributes or ill-defined grouping strategies.\nFurthermore, we contribute corresponding phenotype attribute category labels\nfor two face recognition tasks: RFW for face verification and VGGFace2 (test\nset) for face identification.",
    "published_date": "2021-10-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.09839v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.09795v1",
    "title": "Geo-DefakeHop: High-Performance Geographic Fake Image Detection",
    "authors": [
      "Hong-Shuo Chen",
      "Kaitai Zhang",
      "Shuowen Hu",
      "Suya You",
      "C. -C. Jay Kuo"
    ],
    "author_ids": [],
    "abstract": "A robust fake satellite image detection method, called Geo-DefakeHop, is\nproposed in this work. Geo-DefakeHop is developed based on the parallel\nsubspace learning (PSL) methodology. PSL maps the input image space into\nseveral feature subspaces using multiple filter banks. By exploring response\ndifferences of different channels between real and fake images for a filter\nbank, Geo-DefakeHop learns the most discriminant channels and uses their soft\ndecision scores as features. Then, Geo-DefakeHop selects a few discriminant\nfeatures from each filter bank and ensemble them to make a final binary\ndecision. Geo-DefakeHop offers a light-weight high-performance solution to fake\nsatellite images detection. Its model size is analyzed, which ranges from 0.8\nto 62K parameters. Furthermore, it is shown by experimental results that it\nachieves an F1-score higher than 95\\% under various common image manipulations\nsuch as resizing, compression and noise corruption.",
    "published_date": "2021-10-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.09795v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.04424v2",
    "title": "A Framework for Deprecating Datasets: Standardizing Documentation, Identification, and Communication",
    "authors": [
      "Alexandra Sasha Luccioni",
      "Frances Corry",
      "Hamsini Sridharan",
      "Mike Ananny",
      "Jason Schultz",
      "Kate Crawford"
    ],
    "author_ids": [],
    "abstract": "Datasets are central to training machine learning (ML) models. The ML\ncommunity has recently made significant improvements to data stewardship and\ndocumentation practices across the model development life cycle. However, the\nact of deprecating, or deleting, datasets has been largely overlooked, and\nthere are currently no standardized approaches for structuring this stage of\nthe dataset life cycle. In this paper, we study the practice of dataset\ndeprecation in ML, identify several cases of datasets that continued to\ncirculate despite having been deprecated, and describe the different technical,\nlegal, ethical, and organizational issues raised by such continuations. We then\npropose a Dataset Deprecation Framework that includes considerations of risk,\nmitigation of impact, appeal mechanisms, timeline, post-deprecation protocols,\nand publication checks that can be adapted and implemented by the ML community.\nFinally, we propose creating a centralized, sustainable repository system for\narchiving datasets, tracking dataset modifications or deprecations, and\nfacilitating practices of care and stewardship that can be integrated into\nresearch and publication processes.",
    "published_date": "2021-10-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.04424v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.09601v1",
    "title": "Fair and Efficient Allocations of Chores under Bivalued Preferences",
    "authors": [
      "Jugal Garg",
      "Aniket Murhekar",
      "John Qin"
    ],
    "author_ids": [],
    "abstract": "We study the problem of fair and efficient allocation of a set of indivisible\nchores to agents with additive cost functions. We consider the popular fairness\nnotion of envy-freeness up to one good (EF1) with the efficiency notion of\nPareto-optimality (PO). While it is known that an EF1+PO allocation exists and\ncan be computed in pseudo-polynomial time in the case of goods, the same\nproblem is open for chores.\n  Our first result is a strongly polynomial-time algorithm for computing an\nEF1+PO allocation for bivalued instances, where agents have (at most) two\ndisutility values for the chores. To the best of our knowledge, this is the\nfirst non-trivial class of indivisible chores to admit an EF1+PO allocation and\nan efficient algorithm for its computation.\n  We also study the problem of computing an envy-free (EF) and PO allocation\nfor the case of divisible chores. While the existence of an EF+PO allocation is\nknown via competitive equilibrium with equal incomes, its efficient computation\nis open. Our second result shows that for bivalued instances, an EF+PO\nallocation can be computed in strongly polynomial-time.",
    "published_date": "2021-10-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.09601v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2110.09424v1",
    "title": "Don't Judge Me by My Face : An Indirect Adversarial Approach to Remove Sensitive Information From Multimodal Neural Representation in Asynchronous Job Video Interviews",
    "authors": [
      "Léo Hemamou",
      "Arthur Guillon",
      "Jean-Claude Martin",
      "Chloé Clavel"
    ],
    "author_ids": [],
    "abstract": "se of machine learning for automatic analysis of job interview videos has\nrecently seen increased interest. Despite claims of fair output regarding\nsensitive information such as gender or ethnicity of the candidates, the\ncurrent approaches rarely provide proof of unbiased decision-making, or that\nsensitive information is not used. Recently, adversarial methods have been\nproved to effectively remove sensitive information from the latent\nrepresentation of neural networks. However, these methods rely on the use of\nexplicitly labeled protected variables (e.g. gender), which cannot be collected\nin the context of recruiting in some countries (e.g. France). In this article,\nwe propose a new adversarial approach to remove sensitive information from the\nlatent representation of neural networks without the need to collect any\nsensitive variable. Using only a few frames of the interview, we train our\nmodel to not be able to find the face of the candidate related to the job\ninterview in the inner layers of the model. This, in turn, allows us to remove\nrelevant private information from these layers. Comparing our approach to a\nstandard baseline on a public dataset with gender and ethnicity annotations, we\nshow that it effectively removes sensitive information from the main network.\nMoreover, to the best of our knowledge, this is the first application of\nadversarial techniques for obtaining a multimodal fair representation in the\ncontext of video job interviews. In summary, our contributions aim at improving\nfairness of the upcoming automatic systems processing videos of job interviews\nfor equality in job selection.",
    "published_date": "2021-10-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.HC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.09424v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.09421v1",
    "title": "Measuring Cognitive Status from Speech in a Smart Home Environment",
    "authors": [
      "Kathleen C. Fraser",
      "Majid Komeili"
    ],
    "author_ids": [],
    "abstract": "The population is aging, and becoming more tech-savvy. The United Nations\npredicts that by 2050, one in six people in the world will be over age 65 (up\nfrom one in 11 in 2019), and this increases to one in four in Europe and\nNorthern America. Meanwhile, the proportion of American adults over 65 who own\na smartphone has risen 24 percentage points from 2013-2017, and the majority\nhave Internet in their homes. Smart devices and smart home technology have\nprofound potential to transform how people age, their ability to live\nindependently in later years, and their interactions with their circle of care.\nCognitive health is a key component to independence and well-being in old age,\nand smart homes present many opportunities to measure cognitive status in a\ncontinuous, unobtrusive manner. In this article, we focus on speech as a\nmeasurement instrument for cognitive health. Existing methods of cognitive\nassessment suffer from a number of limitations that could be addressed through\nsmart home speech sensing technologies. We begin with a brief tutorial on\nmeasuring cognitive status from speech, including some pointers to useful\nopen-source software toolboxes for the interested reader. We then present an\noverview of the preliminary results from pilot studies on active and passive\nsmart home speech sensing for the measurement of cognitive health, and conclude\nwith some recommendations and challenge statements for the next wave of work in\nthis area, to help overcome both technical and ethical barriers to success.",
    "published_date": "2021-10-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.09421v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.09295v3",
    "title": "Fair Tree Classifier using Strong Demographic Parity",
    "authors": [
      "António Pereira Barata",
      "Frank W. Takes",
      "H. Jaap van den Herik",
      "Cor J. Veenman"
    ],
    "author_ids": [],
    "abstract": "When dealing with sensitive data in automated data-driven decision-making, an\nimportant concern is to learn predictors with high performance towards a class\nlabel, whilst minimising for the discrimination towards any sensitive\nattribute, like gender or race, induced from biased data. A few hybrid tree\noptimisation criteria exist that combine classification performance and\nfairness. Although the threshold-free ROC-AUC is the standard for measuring\ntraditional classification model performance, current fair tree classification\nmethods mainly optimise for a fixed threshold on both the classification task\nas well as the fairness metric. In this paper, we propose a compound splitting\ncriterion which combines threshold-free (i.e., strong) demographic parity with\nROC-AUC termed SCAFF -- Splitting Criterion AUC for Fairness -- and easily\nextends to bagged and boosted tree frameworks. Our method simultaneously\nleverages multiple sensitive attributes of which the values may be\nmulticategorical or intersectional, and is tunable with respect to the\nunavoidable performance-fairness trade-off. In our experiments, we demonstrate\nhow SCAFF generates models with performance and fairness with respect to\nbinary, multicategorical, and multiple sensitive attributes.",
    "published_date": "2021-10-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.09295v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.09216v1",
    "title": "The Arabic Parallel Gender Corpus 2.0: Extensions and Analyses",
    "authors": [
      "Bashar Alhafni",
      "Nizar Habash",
      "Houda Bouamor"
    ],
    "author_ids": [],
    "abstract": "Gender bias in natural language processing (NLP) applications, particularly\nmachine translation, has been receiving increasing attention. Much of the\nresearch on this issue has focused on mitigating gender bias in English NLP\nmodels and systems. Addressing the problem in poorly resourced, and/or\nmorphologically rich languages has lagged behind, largely due to the lack of\ndatasets and resources. In this paper, we introduce a new corpus for gender\nidentification and rewriting in contexts involving one or two target users (I\nand/or You) -- first and second grammatical persons with independent\ngrammatical gender preferences. We focus on Arabic, a gender-marking\nmorphologically rich language. The corpus has multiple parallel components:\nfour combinations of 1st and 2nd person in feminine and masculine grammatical\ngenders, as well as English, and English to Arabic machine translation output.\nThis corpus expands on Habash et al. (2019)'s Arabic Parallel Gender Corpus\n(APGC v1.0) by adding second person targets as well as increasing the total\nnumber of sentences over 6.5 times, reaching over 590K words. Our new dataset\nwill aid the research and development of gender identification, controlled text\ngeneration, and post-editing rewrite systems that could be used to personalize\nNLP applications and provide users with the correct outputs based on their\ngrammatical gender preferences. We make the Arabic Parallel Gender Corpus (APGC\nv2.0) publicly available.",
    "published_date": "2021-10-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.09216v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.09158v1",
    "title": "Newsalyze: Effective Communication of Person-Targeting Biases in News Articles",
    "authors": [
      "Felix Hamborg",
      "Kim Heinser",
      "Anastasia Zhukova",
      "Karsten Donnay",
      "Bela Gipp"
    ],
    "author_ids": [],
    "abstract": "Media bias and its extreme form, fake news, can decisively affect public\nopinion. Especially when reporting on policy issues, slanted news coverage may\nstrongly influence societal decisions, e.g., in democratic elections. Our paper\nmakes three contributions to address this issue. First, we present a system for\nbias identification, which combines state-of-the-art methods from natural\nlanguage understanding. Second, we devise bias-sensitive visualizations to\ncommunicate bias in news articles to non-expert news consumers. Third, our main\ncontribution is a large-scale user study that measures bias-awareness in a\nsetting that approximates daily news consumption, e.g., we present respondents\nwith a news overview and individual articles. We not only measure the\nvisualizations' effect on respondents' bias-awareness, but we can also pinpoint\nthe effects on individual components of the visualizations by employing a\nconjoint design. Our bias-sensitive overviews strongly and significantly\nincrease bias-awareness in respondents. Our study further suggests that our\ncontent-driven identification method detects groups of similarly slanted news\narticles due to substantial biases present in individual news articles. In\ncontrast, the reviewed prior work rather only facilitates the visibility of\nbiases, e.g., by distinguishing left- and right-wing outlets.",
    "published_date": "2021-10-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.09158v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.09151v1",
    "title": "How to Effectively Identify and Communicate Person-Targeting Media Bias in Daily News Consumption?",
    "authors": [
      "Felix Hamborg",
      "Timo Spinde",
      "Kim Heinser",
      "Karsten Donnay",
      "Bela Gipp"
    ],
    "author_ids": [],
    "abstract": "Slanted news coverage strongly affects public opinion. This is especially\ntrue for coverage on politics and related issues, where studies have shown that\nbias in the news may influence elections and other collective decisions. Due to\nits viable importance, news coverage has long been studied in the social\nsciences, resulting in comprehensive models to describe it and effective yet\ncostly methods to analyze it, such as content analysis. We present an\nin-progress system for news recommendation that is the first to automate the\nmanual procedure of content analysis to reveal person-targeting biases in news\narticles reporting on policy issues. In a large-scale user study, we find very\npromising results regarding this interdisciplinary research direction. Our\nrecommender detects and reveals substantial frames that are actually present in\nindividual news articles. In contrast, prior work rather only facilitates the\nvisibility of biases, e.g., by distinguishing left- and right-wing outlets.\nFurther, our study shows that recommending news articles that differently frame\nan event significantly improves respondents' awareness of bias.",
    "published_date": "2021-10-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.09151v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.09066v2",
    "title": "Fairness Concepts for Indivisible Items with Externalities",
    "authors": [
      "Haris Aziz",
      "Warut Suksompong",
      "Zhaohong Sun",
      "Toby Walsh"
    ],
    "author_ids": [],
    "abstract": "We study a fair allocation problem of indivisible items under additive\nexternalities in which each agent also receives values from items that are\nassigned to other agents. We propose several new fairness concepts. We extend\nthe well-studied envy-freeness up to one item (EF1) and envy-freeness up to any\nitem (EFX) to this setting, and we propose a new fairness concept called\ngeneral fair share (GFS). We undertake a detailed study and present algorithms\nfor finding fair allocations.",
    "published_date": "2021-10-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.09066v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2110.08994v1",
    "title": "CMTR: Cross-modality Transformer for Visible-infrared Person Re-identification",
    "authors": [
      "Tengfei Liang",
      "Yi Jin",
      "Yajun Gao",
      "Wu Liu",
      "Songhe Feng",
      "Tao Wang",
      "Yidong Li"
    ],
    "author_ids": [],
    "abstract": "Visible-infrared cross-modality person re-identification is a challenging\nReID task, which aims to retrieve and match the same identity's images between\nthe heterogeneous visible and infrared modalities. Thus, the core of this task\nis to bridge the huge gap between these two modalities. The existing\nconvolutional neural network-based methods mainly face the problem of\ninsufficient perception of modalities' information, and can not learn good\ndiscriminative modality-invariant embeddings for identities, which limits their\nperformance. To solve these problems, we propose a cross-modality\ntransformer-based method (CMTR) for the visible-infrared person\nre-identification task, which can explicitly mine the information of each\nmodality and generate better discriminative features based on it. Specifically,\nto capture modalities' characteristics, we design the novel modality\nembeddings, which are fused with token embeddings to encode modalities'\ninformation. Furthermore, to enhance representation of modality embeddings and\nadjust matching embeddings' distribution, we propose a modality-aware\nenhancement loss based on the learned modalities' information, reducing\nintra-class distance and enlarging inter-class distance. To our knowledge, this\nis the first work of applying transformer network to the cross-modality\nre-identification task. We implement extensive experiments on the public\nSYSU-MM01 and RegDB datasets, and our proposed CMTR model's performance\nsignificantly surpasses existing outstanding CNN-based methods.",
    "published_date": "2021-10-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.08994v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.08944v3",
    "title": "Developing a novel fair-loan-predictor through a multi-sensitive debiasing pipeline: DualFair",
    "authors": [
      "Jashandeep Singh",
      "Arashdeep Singh",
      "Ariba Khan",
      "Amar Gupta"
    ],
    "author_ids": [],
    "abstract": "Machine learning (ML) models are increasingly used for high-stake\napplications that can greatly impact people's lives. Despite their use, these\nmodels have the potential to be biased towards certain social groups on the\nbasis of race, gender, or ethnicity. Many prior works have attempted to\nmitigate this \"model discrimination\" by updating the training data\n(pre-processing), altering the model learning process (in-processing), or\nmanipulating model output (post-processing). However, these works have not yet\nbeen extended to the realm of multi-sensitive parameters and sensitive options\n(MSPSO), where sensitive parameters are attributes that can be discriminated\nagainst (e.g race) and sensitive options are options within sensitive\nparameters (e.g black or white), thus giving them limited real-world usability.\nPrior work in fairness has also suffered from an accuracy-fairness tradeoff\nthat prevents both the accuracy and fairness from being high. Moreover,\nprevious literature has failed to provide holistic fairness metrics that work\nwith MSPSO. In this paper, we solve all three of these problems by (a) creating\na novel bias mitigation technique called DualFair and (b) developing a new\nfairness metric (i.e. AWI) that can handle MSPSO. Lastly, we test our novel\nmitigation method using a comprehensive U.S mortgage lending dataset and show\nthat our classifier, or fair loan predictor, obtains better fairness and\naccuracy metrics than current state-of-the-art models.",
    "published_date": "2021-10-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.08944v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.08932v1",
    "title": "Poisoning Attacks on Fair Machine Learning",
    "authors": [
      "Minh-Hao Van",
      "Wei Du",
      "Xintao Wu",
      "Aidong Lu"
    ],
    "author_ids": [],
    "abstract": "Both fair machine learning and adversarial learning have been extensively\nstudied. However, attacking fair machine learning models has received less\nattention. In this paper, we present a framework that seeks to effectively\ngenerate poisoning samples to attack both model accuracy and algorithmic\nfairness. Our attacking framework can target fair machine learning models\ntrained with a variety of group based fairness notions such as demographic\nparity and equalized odds. We develop three online attacks, adversarial\nsampling , adversarial labeling, and adversarial feature modification. All\nthree attacks effectively and efficiently produce poisoning samples via\nsampling, labeling, or modifying a fraction of training data in order to reduce\nthe test accuracy. Our framework enables attackers to flexibly adjust the\nattack's focus on prediction accuracy or fairness and accurately quantify the\nimpact of each candidate point to both accuracy loss and fairness violation,\nthus producing effective poisoning samples. Experiments on two real datasets\ndemonstrate the effectiveness and efficiency of our framework.",
    "published_date": "2021-10-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.08932v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2111.04417v1",
    "title": "Providing a Philosophical Critique and Guidance of Fairness Metrics",
    "authors": [
      "Henry Cerbone"
    ],
    "author_ids": [],
    "abstract": "In this project, I seek to present a summarization and unpacking of themes of\nfairness both in the field of computer science and philosophy. This is\nmotivated by an increased dependence on notions of fairness in computer science\nand the millennia of thought on the subject in the field of philosophy. It is\nmy hope that this acts as a crash course in $\\textit{fairness philosophy}$ for\nthe everyday computer scientist and specifically roboticist. This paper will\nconsider current state-of-the-art ideas in computer science, specifically\nalgorithmic fairness, as well as attempt to lay out a rough set of guidelines\nfor metric fairness. Throughout the discussion of philosophy, we will return to\na thought experiment posed by Cynthia Dwork on the question of randomness.",
    "published_date": "2021-10-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.04417v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2110.08835v1",
    "title": "Towards More Accountable Search Engines: Online Evaluation of Representation Bias",
    "authors": [
      "Aldo Lipani",
      "Florina Piroi",
      "Emine Yilmaz"
    ],
    "author_ids": [],
    "abstract": "Information availability affects people's behavior and perception of the\nworld. Notably, people rely on search engines to satisfy their need for\ninformation. Search engines deliver results relevant to user requests usually\nwithout being or making themselves accountable for the information they\ndeliver, which may harm people's lives and, in turn, society. This potential\nrisk urges the development of evaluation mechanisms of bias in order to empower\nthe user in judging the results of search engines. In this paper, we give a\npossible solution to measuring representation bias with respect to societal\nfeatures for search engines and apply it to evaluating the gender\nrepresentation bias for Google's Knowledge Graph Carousel for listing\noccupations.",
    "published_date": "2021-10-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.08835v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2110.08667v1",
    "title": "Face Verification with Challenging Imposters and Diversified Demographics",
    "authors": [
      "Adrian Popescu",
      "Liviu-Daniel Ştefan",
      "Jérôme Deshayes-Chossart",
      "Bogdan Ionescu"
    ],
    "author_ids": [],
    "abstract": "Face verification aims to distinguish between genuine and imposter pairs of\nfaces, which include the same or different identities, respectively. The\nperformance reported in recent years gives the impression that the task is\npractically solved. Here, we revisit the problem and argue that existing\nevaluation datasets were built using two oversimplifying design choices. First,\nthe usual identity selection to form imposter pairs is not challenging enough\nbecause, in practice, verification is needed to detect challenging imposters.\nSecond, the underlying demographics of existing datasets are often insufficient\nto account for the wide diversity of facial characteristics of people from\nacross the world. To mitigate these limitations, we introduce the $FaVCI2D$\ndataset. Imposter pairs are challenging because they include visually similar\nfaces selected from a large pool of demographically diversified identities. The\ndataset also includes metadata related to gender, country and age to facilitate\nfine-grained analysis of results. $FaVCI2D$ is generated from freely\ndistributable resources. Experiments with state-of-the-art deep models that\nprovide nearly 100\\% performance on existing datasets show a significant\nperformance drop for $FaVCI2D$, confirming our starting hypothesis. Equally\nimportant, we analyze legal and ethical challenges which appeared in recent\nyears and hindered the development of face analysis research. We introduce a\nseries of design choices which address these challenges and make the dataset\nconstitution and usage more sustainable and fairer. $FaVCI2D$ is available\nat~\\url{https://github.com/AIMultimediaLab/FaVCI2D-Face-Verification-with-Challenging-Imposters-and-Diversified-Demographics}.",
    "published_date": "2021-10-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.08667v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.08634v2",
    "title": "Towards Robust Waveform-Based Acoustic Models",
    "authors": [
      "Dino Oglic",
      "Zoran Cvetkovic",
      "Peter Sollich",
      "Steve Renals",
      "Bin Yu"
    ],
    "author_ids": [],
    "abstract": "We study the problem of learning robust acoustic models in adverse\nenvironments, characterized by a significant mismatch between training and test\nconditions. This problem is of paramount importance for the deployment of\nspeech recognition systems that need to perform well in unseen environments.\nFirst, we characterize data augmentation theoretically as an instance of\nvicinal risk minimization, which aims at improving risk estimates during\ntraining by replacing the delta functions that define the empirical density\nover the input space with an approximation of the marginal population density\nin the vicinity of the training samples. More specifically, we assume that\nlocal neighborhoods centered at training samples can be approximated using a\nmixture of Gaussians, and demonstrate theoretically that this can incorporate\nrobust inductive bias into the learning process. We then specify the individual\nmixture components implicitly via data augmentation schemes, designed to\naddress common sources of spurious correlations in acoustic models. To avoid\npotential confounding effects on robustness due to information loss, which has\nbeen associated with standard feature extraction techniques (e.g., FBANK and\nMFCC features), we focus on the waveform-based setting. Our empirical results\nshow that the approach can generalize to unseen noise conditions, with 150%\nrelative improvement in out-of-distribution generalization compared to training\nusing the standard risk minimization principle. Moreover, the results\ndemonstrate competitive performance relative to models learned using a training\nsample designed to match the acoustic conditions characteristic of test\nutterances.",
    "published_date": "2021-10-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SD",
      "cs.LG",
      "eess.AS",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.08634v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.08583v1",
    "title": "ASR4REAL: An extended benchmark for speech models",
    "authors": [
      "Morgane Riviere",
      "Jade Copet",
      "Gabriel Synnaeve"
    ],
    "author_ids": [],
    "abstract": "Popular ASR benchmarks such as Librispeech and Switchboard are limited in the\ndiversity of settings and speakers they represent. We introduce a set of\nbenchmarks matching real-life conditions, aimed at spotting possible biases and\nweaknesses in models. We have found out that even though recent models do not\nseem to exhibit a gender bias, they usually show important performance\ndiscrepancies by accent, and even more important ones depending on the\nsocio-economic status of the speakers. Finally, all tested models show a strong\nperformance drop when tested on conversational speech, and in this precise\ncontext even a language model trained on a dataset as big as Common Crawl does\nnot seem to have significant positive effect which reiterates the importance of\ndeveloping conversational language models",
    "published_date": "2021-10-16T00:00:00",
    "year": 2021,
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.SD"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.08583v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.08578v1",
    "title": "Visual-aware Attention Dual-stream Decoder for Video Captioning",
    "authors": [
      "Zhixin Sun",
      "Xian Zhong",
      "Shuqin Chen",
      "Lin Li",
      "Luo Zhong"
    ],
    "author_ids": [],
    "abstract": "Video captioning is a challenging task that captures different visual parts\nand describes them in sentences, for it requires visual and linguistic\ncoherence. The attention mechanism in the current video captioning method\nlearns to assign weight to each frame, promoting the decoder dynamically. This\nmay not explicitly model the correlation and the temporal coherence of the\nvisual features extracted in the sequence frames.To generate semantically\ncoherent sentences, we propose a new Visual-aware Attention (VA) model, which\nconcatenates dynamic changes of temporal sequence frames with the words at the\nprevious moment, as the input of attention mechanism to extract sequence\nfeatures.In addition, the prevalent approaches widely use the teacher-forcing\n(TF) learning during training, where the next token is generated conditioned on\nthe previous ground-truth tokens. The semantic information in the previously\ngenerated tokens is lost. Therefore, we design a self-forcing (SF) stream that\ntakes the semantic information in the probability distribution of the previous\ntoken as input to enhance the current token.The Dual-stream Decoder (DD)\narchitecture unifies the TF and SF streams, generating sentences to promote the\nannotated captioning for both streams.Meanwhile, with the Dual-stream Decoder\nutilized, the exposure bias problem is alleviated, caused by the discrepancy\nbetween the training and testing in the TF learning.The effectiveness of the\nproposed Visual-aware Attention Dual-stream Decoder (VADD) is demonstrated\nthrough the result of experimental studies on Microsoft video description\n(MSVD) corpus and MSR-Video to text (MSR-VTT) datasets.",
    "published_date": "2021-10-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.08578v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.08527v3",
    "title": "An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models",
    "authors": [
      "Nicholas Meade",
      "Elinor Poole-Dayan",
      "Siva Reddy"
    ],
    "author_ids": [],
    "abstract": "Recent work has shown pre-trained language models capture social biases from\nthe large amounts of text they are trained on. This has attracted attention to\ndeveloping techniques that mitigate such biases. In this work, we perform an\nempirical survey of five recently proposed bias mitigation techniques:\nCounterfactual Data Augmentation (CDA), Dropout, Iterative Nullspace\nProjection, Self-Debias, and SentenceDebias. We quantify the effectiveness of\neach technique using three intrinsic bias benchmarks while also measuring the\nimpact of these techniques on a model's language modeling ability, as well as\nits performance on downstream NLU tasks. We experimentally find that: (1)\nSelf-Debias is the strongest debiasing technique, obtaining improved scores on\nall bias benchmarks; (2) Current debiasing techniques perform less consistently\nwhen mitigating non-gender biases; And (3) improvements on bias benchmarks such\nas StereoSet and CrowS-Pairs by using debiasing strategies are often\naccompanied by a decrease in language modeling ability, making it difficult to\ndetermine whether the bias mitigation was effective.",
    "published_date": "2021-10-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.08527v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.08427v2",
    "title": "COVID-19 Detection in Chest X-ray Images Using Swin-Transformer and Transformer in Transformer",
    "authors": [
      "Juntao Jiang",
      "Shuyi Lin"
    ],
    "author_ids": [],
    "abstract": "The Coronavirus Disease 2019 (COVID-19) has spread globally and caused\nserious damage. Chest X-ray images are widely used for COVID-19 diagnosis and\nthe Artificial Intelligence method can increase efficiency and accuracy. In the\nChallenge of Chest XR COVID-19 detection in Ethics and Explainability for\nResponsible Data Science (EE-RDS) conference 2021, we proposed a method that\ncombined Swin Transformer and Transformer in Transformer to classify chest\nX-ray images as three classes: COVID-19, Pneumonia, and Normal (healthy) and\nachieved 0.9475 accuracies on the test set.",
    "published_date": "2021-10-16T00:00:00",
    "year": 2021,
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.08427v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.08421v1",
    "title": "Dataset Knowledge Transfer for Class-Incremental Learning without Memory",
    "authors": [
      "Habib Slim",
      "Eden Belouadah",
      "Adrian Popescu",
      "Darian Onchis"
    ],
    "author_ids": [],
    "abstract": "Incremental learning enables artificial agents to learn from sequential data.\nWhile important progress was made by exploiting deep neural networks,\nincremental learning remains very challenging. This is particularly the case\nwhen no memory of past data is allowed and catastrophic forgetting has a strong\nnegative effect. We tackle class-incremental learning without memory by\nadapting prediction bias correction, a method which makes predictions of past\nand new classes more comparable. It was proposed when a memory is allowed and\ncannot be directly used without memory, since samples of past classes are\nrequired. We introduce a two-step learning process which allows the transfer of\nbias correction parameters between reference and target datasets. Bias\ncorrection is first optimized offline on reference datasets which have an\nassociated validation memory. The obtained correction parameters are then\ntransferred to target datasets, for which no memory is available. The second\ncontribution is to introduce a finer modeling of bias correction by learning\nits parameters per incremental state instead of the usual past vs. new class\nmodeling. The proposed dataset knowledge transfer is applicable to any\nincremental method which works without memory. We test its effectiveness by\napplying it to four existing methods. Evaluation with four target datasets and\ndifferent configurations shows consistent improvement, with practically no\ncomputational and memory overhead.",
    "published_date": "2021-10-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.08421v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.08396v2",
    "title": "Comparing Human and Machine Bias in Face Recognition",
    "authors": [
      "Samuel Dooley",
      "Ryan Downing",
      "George Wei",
      "Nathan Shankar",
      "Bradon Thymes",
      "Gudrun Thorkelsdottir",
      "Tiye Kurtz-Miott",
      "Rachel Mattson",
      "Olufemi Obiwumi",
      "Valeriia Cherepanova",
      "Micah Goldblum",
      "John P Dickerson",
      "Tom Goldstein"
    ],
    "author_ids": [],
    "abstract": "Much recent research has uncovered and discussed serious concerns of bias in\nfacial analysis technologies, finding performance disparities between groups of\npeople based on perceived gender, skin type, lighting condition, etc. These\naudits are immensely important and successful at measuring algorithmic bias but\nhave two major challenges: the audits (1) use facial recognition datasets which\nlack quality metadata, like LFW and CelebA, and (2) do not compare their\nobserved algorithmic bias to the biases of their human alternatives. In this\npaper, we release improvements to the LFW and CelebA datasets which will enable\nfuture researchers to obtain measurements of algorithmic bias that are not\ntainted by major flaws in the dataset (e.g. identical images appearing in both\nthe gallery and test set). We also use these new data to develop a series of\nchallenging facial identification and verification questions that we\nadministered to various algorithms and a large, balanced sample of human\nreviewers. We find that both computer models and human survey participants\nperform significantly better at the verification task, generally obtain lower\naccuracy rates on dark-skinned or female subjects for both tasks, and obtain\nhigher accuracy rates when their demographics match that of the question.\nComputer models are observed to achieve a higher level of accuracy than the\nsurvey participants on both tasks and exhibit bias to similar degrees as the\nhuman survey participants.",
    "published_date": "2021-10-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.08396v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.08388v2",
    "title": "Probing as Quantifying Inductive Bias",
    "authors": [
      "Alexander Immer",
      "Lucas Torroba Hennigen",
      "Vincent Fortuin",
      "Ryan Cotterell"
    ],
    "author_ids": [],
    "abstract": "Pre-trained contextual representations have led to dramatic performance\nimprovements on a range of downstream tasks. Such performance improvements have\nmotivated researchers to quantify and understand the linguistic information\nencoded in these representations. In general, researchers quantify the amount\nof linguistic information through probing, an endeavor which consists of\ntraining a supervised model to predict a linguistic property directly from the\ncontextual representations. Unfortunately, this definition of probing has been\nsubject to extensive criticism in the literature, and has been observed to lead\nto paradoxical and counter-intuitive results. In the theoretical portion of\nthis paper, we take the position that the goal of probing ought to be measuring\nthe amount of inductive bias that the representations encode on a specific\ntask. We further describe a Bayesian framework that operationalizes this goal\nand allows us to quantify the representations' inductive bias. In the empirical\nportion of the paper, we apply our framework to a variety of NLP tasks. Our\nresults suggest that our proposed framework alleviates many previous problems\nfound in probing. Moreover, we are able to offer concrete evidence that -- for\nsome tasks -- fastText can offer a better inductive bias than BERT.",
    "published_date": "2021-10-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.08388v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.15733v1",
    "title": "Detecting Gender Bias in Transformer-based Models: A Case Study on BERT",
    "authors": [
      "Bingbing Li",
      "Hongwu Peng",
      "Rajat Sainju",
      "Junhuan Yang",
      "Lei Yang",
      "Yueying Liang",
      "Weiwen Jiang",
      "Binghui Wang",
      "Hang Liu",
      "Caiwen Ding"
    ],
    "author_ids": [],
    "abstract": "In this paper, we propose a novel gender bias detection method by utilizing\nattention map for transformer-based models. We 1) give an intuitive gender bias\njudgement method by comparing the different relation degree between the genders\nand the occupation according to the attention scores, 2) design a gender bias\ndetector by modifying the attention module, 3) insert the gender bias detector\ninto different positions of the model to present the internal gender bias flow,\nand 4) draw the consistent gender bias conclusion by scanning the entire\nWikipedia, a BERT pretraining dataset. We observe that 1) the attention\nmatrices, Wq and Wk introduce much more gender bias than other modules\n(including the embedding layer) and 2) the bias degree changes periodically\ninside of the model (attention matrix Q, K, V, and the remaining part of the\nattention layer (including the fully-connected layer, the residual connection,\nand the layer normalization module) enhance the gender bias while the averaged\nattentions reduces the bias).",
    "published_date": "2021-10-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.LG",
      "I.2; I.7; H.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.15733v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.08353v1",
    "title": "Revisiting Popularity and Demographic Biases in Recommender Evaluation and Effectiveness",
    "authors": [
      "Nicola Neophytou",
      "Bhaskar Mitra",
      "Catherine Stinson"
    ],
    "author_ids": [],
    "abstract": "Recommendation algorithms are susceptible to popularity bias: a tendency to\nrecommend popular items even when they fail to meet user needs. A related issue\nis that the recommendation quality can vary by demographic groups. Marginalized\ngroups or groups that are under-represented in the training data may receive\nless relevant recommendations from these algorithms compared to others. In a\nrecent study, Ekstrand et al. investigate how recommender performance varies\naccording to popularity and demographics, and find statistically significant\ndifferences in recommendation utility between binary genders on two datasets,\nand significant effects based on age on one dataset. Here we reproduce those\nresults and extend them with additional analyses. We find statistically\nsignificant differences in recommender performance by both age and gender. We\nobserve that recommendation utility steadily degrades for older users, and is\nlower for women than men. We also find that the utility is higher for users\nfrom countries with more representation in the dataset. In addition, we find\nthat total usage and the popularity of consumed content are strong predictors\nof recommender performance and also vary significantly across demographic\ngroups.",
    "published_date": "2021-10-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.08353v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.08330v1",
    "title": "Nothing Wasted: Full Contribution Enforcement in Federated Edge Learning",
    "authors": [
      "Qin Hu",
      "Shengling Wang",
      "Zeihui Xiong",
      "Xiuzhen Cheng"
    ],
    "author_ids": [],
    "abstract": "The explosive amount of data generated at the network edge makes mobile edge\ncomputing an essential technology to support real-time applications, calling\nfor powerful data processing and analysis provided by machine learning (ML)\ntechniques. In particular, federated edge learning (FEL) becomes prominent in\nsecuring the privacy of data owners by keeping the data locally used to train\nML models. Existing studies on FEL either utilize in-process optimization or\nremove unqualified participants in advance. In this paper, we enhance the\ncollaboration from all edge devices in FEL to guarantee that the ML model is\ntrained using all available local data to accelerate the learning process. To\nthat aim, we propose a collective extortion (CE) strategy under the\nimperfect-information multi-player FEL game, which is proved to be effective in\nhelping the server efficiently elicit the full contribution of all devices\nwithout worrying about suffering from any economic loss. Technically, our\nproposed CE strategy extends the classical extortion strategy in controlling\nthe proportionate share of expected utilities for a single opponent to the\nswiftly homogeneous control over a group of players, which further presents an\nattractive trait of being impartial for all participants. Moreover, the CE\nstrategy enriches the game theory hierarchy, facilitating a wider application\nscope of the extortion strategy. Both theoretical analysis and experimental\nevaluations validate the effectiveness and fairness of our proposed scheme.",
    "published_date": "2021-10-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.08330v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.08308v1",
    "title": "Adaptive and Fair Transformation for Recoverable Mutual Exclusion",
    "authors": [
      "Sahil Dhoked",
      "Neeraj Mittal"
    ],
    "author_ids": [],
    "abstract": "Mutual exclusion is one of the most commonly used techniques to handle\ncontention in concurrent systems. Traditionally, mutual exclusion algorithms\nhave been designed under the assumption that a process does not fail while\nacquiring/releasing a lock or while executing its critical section. However,\nfailures do occur in real life, potentially leaving the lock in an inconsistent\nstate. This gives rise to the problem of recoverable mutual exclusion (RME)\nthat involves designing a mutual exclusion (ME) algorithm that can tolerate\nfailures, while maintaining safety and liveness properties.\n  In this work, we present a framework that transforms any algorithm that\nsolves the RME problem into an algorithm that can also simultaneously adapt to\n(1) the number of processes competing for the lock, as well as (2) the number\nof failures that have occurred in the recent past, while maintaining the\ncorrectness and performance properties of the underlying RME algorithm.\nAdditionally, the algorithm constructed as a result of this transformation adds\ncertain desirable properties like fairness (a variation of FCFS) and bounded\nrecovery. Assume that the worst-case RMR complexity of a critical section\nrequest in the underlying RME algorithm is $R(n)$. Then, our framework yields\nan RME algorithm for which the worst-case RMR complexity of a critical section\nrequest is given by $\\mathcal{O}(\\min \\{\\ddot{c}, \\sqrt{F+1}, R(n)\\})$, where\n$\\ddot{c}$ denotes the point contention of the request and $F$ denotes the\nnumber of failures in the recent past of the request.\n  We further extend our framework by presenting a novel memory reclamation\nalgorithm to bound the worst-case space complexity of the RME algorithm. The\nmemory reclamation techniques maintain the fairness, performance and\ncorrectness properties of our transformation and is general enough to be\nemployed to bound the space of other RME algorithms.",
    "published_date": "2021-10-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.08308v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2110.08307v1",
    "title": "GrowSpace: Learning How to Shape Plants",
    "authors": [
      "Yasmeen Hitti",
      "Ionelia Buzatu",
      "Manuel Del Verme",
      "Mark Lefsrud",
      "Florian Golemo",
      "Audrey Durand"
    ],
    "author_ids": [],
    "abstract": "Plants are dynamic systems that are integral to our existence and survival.\nPlants face environment changes and adapt over time to their surrounding\nconditions. We argue that plant responses to an environmental stimulus are a\ngood example of a real-world problem that can be approached within a\nreinforcement learning (RL)framework. With the objective of controlling a plant\nby moving the light source, we propose GrowSpace, as a new RL benchmark. The\nback-end of the simulator is implemented using the Space Colonisation\nAlgorithm, a plant growing model based on competition for space. Compared to\nvideo game RL environments, this simulator addresses a real-world problem and\nserves as a test bed to visualize plant growth and movement in a faster way\nthan physical experiments. GrowSpace is composed of a suite of challenges that\ntackle several problems such as control, multi-stage learning,fairness and\nmulti-objective learning. We provide agent baselines alongside case studies to\ndemonstrate the difficulty of the proposed benchmark.",
    "published_date": "2021-10-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.08307v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.08193v2",
    "title": "BBQ: A Hand-Built Bias Benchmark for Question Answering",
    "authors": [
      "Alicia Parrish",
      "Angelica Chen",
      "Nikita Nangia",
      "Vishakh Padmakumar",
      "Jason Phang",
      "Jana Thompson",
      "Phu Mon Htut",
      "Samuel R. Bowman"
    ],
    "author_ids": [],
    "abstract": "It is well documented that NLP models learn social biases, but little work\nhas been done on how these biases manifest in model outputs for applied tasks\nlike question answering (QA). We introduce the Bias Benchmark for QA (BBQ), a\ndataset of question sets constructed by the authors that highlight attested\nsocial biases against people belonging to protected classes along nine social\ndimensions relevant for U.S. English-speaking contexts. Our task evaluates\nmodel responses at two levels: (i) given an under-informative context, we test\nhow strongly responses reflect social biases, and (ii) given an adequately\ninformative context, we test whether the model's biases override a correct\nanswer choice. We find that models often rely on stereotypes when the context\nis under-informative, meaning the model's outputs consistently reproduce\nharmful biases in this setting. Though models are more accurate when the\ncontext provides an informative answer, they still rely on stereotypes and\naverage up to 3.4 percentage points higher accuracy when the correct answer\naligns with a social bias than when it conflicts, with this difference widening\nto over 5 points on examples targeting gender for most models tested.",
    "published_date": "2021-10-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.08193v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.08182v1",
    "title": "The World of an Octopus: How Reporting Bias Influences a Language Model's Perception of Color",
    "authors": [
      "Cory Paik",
      "Stéphane Aroca-Ouellette",
      "Alessandro Roncone",
      "Katharina Kann"
    ],
    "author_ids": [],
    "abstract": "Recent work has raised concerns about the inherent limitations of text-only\npretraining. In this paper, we first demonstrate that reporting bias, the\ntendency of people to not state the obvious, is one of the causes of this\nlimitation, and then investigate to what extent multimodal training can\nmitigate this issue. To accomplish this, we 1) generate the Color Dataset\n(CoDa), a dataset of human-perceived color distributions for 521 common\nobjects; 2) use CoDa to analyze and compare the color distribution found in\ntext, the distribution captured by language models, and a human's perception of\ncolor; and 3) investigate the performance differences between text-only and\nmultimodal models on CoDa. Our results show that the distribution of colors\nthat a language model recovers correlates more strongly with the inaccurate\ndistribution found in text than with the ground-truth, supporting the claim\nthat reporting bias negatively impacts and inherently limits text-only\ntraining. We then demonstrate that multimodal models can leverage their visual\ntraining to mitigate these effects, providing a promising avenue for future\nresearch.",
    "published_date": "2021-10-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.08182v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.08009v3",
    "title": "MaGNET: Uniform Sampling from Deep Generative Network Manifolds Without Retraining",
    "authors": [
      "Ahmed Imtiaz Humayun",
      "Randall Balestriero",
      "Richard Baraniuk"
    ],
    "author_ids": [],
    "abstract": "Deep Generative Networks (DGNs) are extensively employed in Generative\nAdversarial Networks (GANs), Variational Autoencoders (VAEs), and their\nvariants to approximate the data manifold and distribution. However, training\nsamples are often distributed in a non-uniform fashion on the manifold, due to\ncosts or convenience of collection. For example, the CelebA dataset contains a\nlarge fraction of smiling faces. These inconsistencies will be reproduced when\nsampling from the trained DGN, which is not always preferred, e.g., for\nfairness or data augmentation. In response, we develop MaGNET, a novel and\ntheoretically motivated latent space sampler for any pre-trained DGN, that\nproduces samples uniformly distributed on the learned manifold. We perform a\nrange of experiments on various datasets and DGNs, e.g., for the\nstate-of-the-art StyleGAN2 trained on FFHQ dataset, uniform sampling via MaGNET\nincreases distribution precision and recall by 4.1\\% \\& 3.0\\% and decreases\ngender bias by 41.2\\%, without requiring labels or retraining. As uniform\ndistribution does not imply uniform semantic distribution, we also explore\nseparately how semantic attributes of generated samples vary under MaGNET\nsampling.",
    "published_date": "2021-10-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.08009v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.07957v3",
    "title": "Don't speak too fast: The impact of data bias on self-supervised speech models",
    "authors": [
      "Yen Meng",
      "Yi-Hui Chou",
      "Andy T. Liu",
      "Hung-yi Lee"
    ],
    "author_ids": [],
    "abstract": "Self-supervised Speech Models (S3Ms) have been proven successful in many\nspeech downstream tasks, like ASR. However, how pre-training data affects S3Ms'\ndownstream behavior remains an unexplored issue. In this paper, we study how\npre-training data affects S3Ms by pre-training models on biased datasets\ntargeting different factors of speech, including gender, content, and prosody,\nand evaluate these pre-trained S3Ms on selected downstream tasks in SUPERB\nBenchmark. Our experiments show that S3Ms have tolerance toward gender bias.\nMoreover, we find that the content of speech has little impact on the\nperformance of S3Ms across downstream tasks, but S3Ms do show a preference\ntoward a slower speech rate.",
    "published_date": "2021-10-15T00:00:00",
    "year": 2021,
    "categories": [
      "eess.AS",
      "cs.CL",
      "cs.SD"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.07957v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.07871v2",
    "title": "Socially Aware Bias Measurements for Hindi Language Representations",
    "authors": [
      "Vijit Malik",
      "Sunipa Dev",
      "Akihiro Nishi",
      "Nanyun Peng",
      "Kai-Wei Chang"
    ],
    "author_ids": [],
    "abstract": "Language representations are efficient tools used across NLP applications,\nbut they are strife with encoded societal biases. These biases are studied\nextensively, but with a primary focus on English language representations and\nbiases common in the context of Western society. In this work, we investigate\nbiases present in Hindi language representations with focuses on caste and\nreligion-associated biases. We demonstrate how biases are unique to specific\nlanguage representations based on the history and culture of the region they\nare widely spoken in, and how the same societal bias (such as binary\ngender-associated biases) is encoded by different words and text spans across\nlanguages. The discoveries of our work highlight the necessity of culture\nawareness and linguistic artifacts when modeling language representations, in\norder to better understand the encoded biases.",
    "published_date": "2021-10-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.07871v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.07701v3",
    "title": "Exposing Query Identification for Search Transparency",
    "authors": [
      "Ruohan Li",
      "Jianxiang Li",
      "Bhaskar Mitra",
      "Fernando Diaz",
      "Asia J. Biega"
    ],
    "author_ids": [],
    "abstract": "Search systems control the exposure of ranked content to searchers. In many\ncases, creators value not only the exposure of their content but, moreover, an\nunderstanding of the specific searches where the content is surfaced. The\nproblem of identifying which queries expose a given piece of content in the\nranking results is an important and relatively under-explored search\ntransparency challenge. Exposing queries are useful for quantifying various\nissues of search bias, privacy, data protection, security, and search engine\noptimization.\n  Exact identification of exposing queries in a given system is computationally\nexpensive, especially in dynamic contexts such as web search. We explore the\nfeasibility of approximate exposing query identification (EQI) as a retrieval\ntask by reversing the role of queries and documents in two classes of search\nsystems: dense dual-encoder models and traditional BM25 models. We then propose\nhow this approach can be improved through metric learning over the retrieval\nembedding space. We further derive an evaluation metric to measure the quality\nof a ranking of exposing queries, as well as conducting an empirical analysis\nfocusing on various practical aspects of approximate EQI. Overall, our work\ncontributes a novel conception of transparency in search systems and\ncomputational means of achieving it.",
    "published_date": "2021-10-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.07701v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.07592v3",
    "title": "DeToxy: A Large-Scale Multimodal Dataset for Toxicity Classification in Spoken Utterances",
    "authors": [
      "Sreyan Ghosh",
      "Samden Lepcha",
      "S Sakshi",
      "Rajiv Ratn Shah",
      "S. Umesh"
    ],
    "author_ids": [],
    "abstract": "Toxic speech, also known as hate speech, is regarded as one of the crucial\nissues plaguing online social media today. Most recent work on toxic speech\ndetection is constrained to the modality of text and written conversations with\nvery limited work on toxicity detection from spoken utterances or using the\nmodality of speech. In this paper, we introduce a new dataset DeToxy, the first\npublicly available toxicity annotated dataset for the English language. DeToxy\nis sourced from various openly available speech databases and consists of over\n2 million utterances. We believe that our dataset would act as a benchmark for\nthe relatively new and un-explored Spoken Language Processing task of detecting\ntoxicity from spoken utterances and boost further research in this space.\nFinally, we also provide strong unimodal baselines for our dataset and compare\ntraditional two-step and E2E approaches. Our experiments show that in the case\nof spoken utterances, text-based approaches are largely dependent on gold\nhuman-annotated transcripts for their performance and also suffer from the\nproblem of keyword bias. However, the presence of speech files in DeToxy helps\nfacilitates the development of E2E speech models which alleviate both the\nabove-stated problems by better capturing speech clues.",
    "published_date": "2021-10-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.07592v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.07575v1",
    "title": "Spoken ObjectNet: A Bias-Controlled Spoken Caption Dataset",
    "authors": [
      "Ian Palmer",
      "Andrew Rouditchenko",
      "Andrei Barbu",
      "Boris Katz",
      "James Glass"
    ],
    "author_ids": [],
    "abstract": "Visually-grounded spoken language datasets can enable models to learn\ncross-modal correspondences with very weak supervision. However, modern\naudio-visual datasets contain biases that undermine the real-world performance\nof models trained on that data. We introduce Spoken ObjectNet, which is\ndesigned to remove some of these biases and provide a way to better evaluate\nhow effectively models will perform in real-world scenarios. This dataset\nexpands upon ObjectNet, which is a bias-controlled image dataset that features\nsimilar image classes to those present in ImageNet. We detail our data\ncollection pipeline, which features several methods to improve caption quality,\nincluding automated language model checks. Lastly, we show baseline results on\nimage retrieval and audio retrieval tasks. These results show that models\ntrained on other datasets and then evaluated on Spoken ObjectNet tend to\nperform poorly due to biases in other datasets that the models have learned. We\nalso show evidence that the performance decrease is due to the dataset\ncontrols, and not the transfer setting.",
    "published_date": "2021-10-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.CV",
      "cs.MM",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.07575v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.07574v2",
    "title": "Can Machines Learn Morality? The Delphi Experiment",
    "authors": [
      "Liwei Jiang",
      "Jena D. Hwang",
      "Chandra Bhagavatula",
      "Ronan Le Bras",
      "Jenny Liang",
      "Jesse Dodge",
      "Keisuke Sakaguchi",
      "Maxwell Forbes",
      "Jon Borchardt",
      "Saadia Gabriel",
      "Yulia Tsvetkov",
      "Oren Etzioni",
      "Maarten Sap",
      "Regina Rini",
      "Yejin Choi"
    ],
    "author_ids": [],
    "abstract": "As AI systems become increasingly powerful and pervasive, there are growing\nconcerns about machines' morality or a lack thereof. Yet, teaching morality to\nmachines is a formidable task, as morality remains among the most intensely\ndebated questions in humanity, let alone for AI. Existing AI systems deployed\nto millions of users, however, are already making decisions loaded with moral\nimplications, which poses a seemingly impossible challenge: teaching machines\nmoral sense, while humanity continues to grapple with it.\n  To explore this challenge, we introduce Delphi, an experimental framework\nbased on deep neural networks trained directly to reason about descriptive\nethical judgments, e.g., \"helping a friend\" is generally good, while \"helping a\nfriend spread fake news\" is not. Empirical results shed novel insights on the\npromises and limits of machine ethics; Delphi demonstrates strong\ngeneralization capabilities in the face of novel ethical situations, while\noff-the-shelf neural network models exhibit markedly poor judgment including\nunjust biases, confirming the need for explicitly teaching machines moral\nsense.\n  Yet, Delphi is not perfect, exhibiting susceptibility to pervasive biases and\ninconsistencies. Despite that, we demonstrate positive use cases of imperfect\nDelphi, including using it as a component model within other imperfect AI\nsystems. Importantly, we interpret the operationalization of Delphi in light of\nprominent ethical theories, which leads us to important future research\nquestions.",
    "published_date": "2021-10-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.07574v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.07460v1",
    "title": "IB-GAN: A Unified Approach for Multivariate Time Series Classification under Class Imbalance",
    "authors": [
      "Grace Deng",
      "Cuize Han",
      "Tommaso Dreossi",
      "Clarence Lee",
      "David S. Matteson"
    ],
    "author_ids": [],
    "abstract": "Classification of large multivariate time series with strong class imbalance\nis an important task in real-world applications. Standard methods of class\nweights, oversampling, or parametric data augmentation do not always yield\nsignificant improvements for predicting minority classes of interest.\nNon-parametric data augmentation with Generative Adversarial Networks (GANs)\noffers a promising solution. We propose Imputation Balanced GAN (IB-GAN), a\nnovel method that joins data augmentation and classification in a one-step\nprocess via an imputation-balancing approach. IB-GAN uses imputation and\nresampling techniques to generate higher quality samples from randomly masked\nvectors than from white noise, and augments classification through a\nclass-balanced set of real and synthetic samples. Imputation hyperparameter\n$p_{miss}$ allows for regularization of classifier variability by tuning\ninnovations introduced via generator imputation. IB-GAN is simple to train and\nmodel-agnostic, pairing any deep learning classifier with a\ngenerator-discriminator duo and resulting in higher accuracy for under-observed\nclasses. Empirical experiments on open-source UCR data and proprietary 90K\nproduct dataset show significant performance gains against state-of-the-art\nparametric and GAN baselines.",
    "published_date": "2021-10-14T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.07460v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.07444v1",
    "title": "Designing Language Technologies for Social Good: The Road not Taken",
    "authors": [
      "Namrata Mukhija",
      "Monojit Choudhury",
      "Kalika Bali"
    ],
    "author_ids": [],
    "abstract": "Development of speech and language technology for social good (LT4SG),\nespecially those targeted at the welfare of marginalized communities and\nspeakers of low-resource and under-served languages, has been a prominent theme\nof research within NLP, Speech, and the AI communities. Researchers have mostly\nrelied on their individual expertise, experiences or ad hoc surveys for\nprioritization of language technologies that provide social good to the\nend-users. This has been criticized by several scholars who argue that work on\nLT4SG must include the target linguistic communities during the design and\ndevelopment process. However, none of the LT4SG work and their critiques\nsuggest principled techniques for prioritization of the technologies and\nmethods for inclusion of the end-user during the development cycle. Drawing\ninspiration from the fields of Economics, Ethics, Psychology, and Participatory\nDesign, here we chart out a set of methodologies for prioritizing LT4SG that\nare aligned with the end-user preferences. We then analyze several LT4SG\nefforts in light of the proposed methodologies and bring out their hidden\nassumptions and potential pitfalls. While the current study is limited to\nlanguage technologies, we believe that the principles and prioritization\ntechniques highlighted here are applicable more broadly to AI for Social Good.",
    "published_date": "2021-10-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.07444v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.07370v1",
    "title": "Ethics lines and Machine learning: a design and simulation of an Association Rules Algorithm for exploiting the data",
    "authors": [
      "Patrici Calvo",
      "Rebeca Egea-Moreno"
    ],
    "author_ids": [],
    "abstract": "Data mining techniques offer great opportunities for developing ethics lines,\ntools for communication, participation and innovation whose main aim is to\nensure improvements and compliance with the values, conduct and commitments\nmaking up the code of ethics. The aim of this study is to suggest a process for\nexploiting the data generated by the data generated and collected from an\nethics line by extracting rules of association and applying the Apriori\nalgorithm. This makes it possible to identify anomalies and behaviour patterns\nrequiring action to review, correct, promote or expand them, as appropriate.\nFinally, I offer a simulated application of the Apriori algorithm, supplying it\nwith synthetic data to find out its potential, strengths and limitations.",
    "published_date": "2021-10-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.07370v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.07320v2",
    "title": "Quantum Rényi divergences and the strong converse exponent of state discrimination in operator algebras",
    "authors": [
      "Fumio Hiai",
      "Milán Mosonyi"
    ],
    "author_ids": [],
    "abstract": "The sandwiched R\\'enyi $\\alpha$-divergences of two finite-dimensional quantum\nstates play a distinguished role among the many quantum versions of R\\'enyi\ndivergences as the tight quantifiers of the trade-off between the two error\nprobabilities in the strong converse domain of state discrimination. In this\npaper we show the same for the sandwiched R\\'enyi divergences of two normal\nstates on an injective von Neumann algebra, thereby establishing the\noperational significance of these quantities. Moreover, we show that in this\nsetting, again similarly to the finite-dimensional case, the sandwiched R\\'enyi\ndivergences coincide with the regularized measured R\\'enyi divergences, another\ndistinctive feature of the former quantities. Our main tool is an approximation\ntheorem (martingale convergence) for the sandwiched R\\'enyi divergences, which\nmay be used for the extension of various further results from the\nfinite-dimensional to the von Neumann algebra setting.\n  We also initiate the study of the sandwiched R\\'enyi divergences of pairs of\nstates on a $C^*$-algebra, and show that the above operational interpretation,\nas well as the equality to the regularized measured R\\'enyi divergence, holds\nmore generally for pairs of states on a nuclear $C^*$-algebra.",
    "published_date": "2021-10-14T00:00:00",
    "year": 2021,
    "categories": [
      "quant-ph",
      "cs.IT",
      "math-ph",
      "math.IT",
      "math.MP",
      "math.OA",
      "81P45, 81P18, 94A17, 46L52, 46L53, 81R15, 62H15"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.07320v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2110.07307v3",
    "title": "FocusNet: Classifying Better by Focusing on Confusing Classes",
    "authors": [
      "Xue Zhang",
      "Zehua Sheng",
      "Hui-Liang Shen"
    ],
    "author_ids": [],
    "abstract": "Nowadays, most classification networks use one-hot encoding to represent\ncategorical data because of its simplicity. However, one-hot encoding may\naffect the generalization ability as it neglects inter-class correlations. We\nobserve that, even when a neural network trained with one-hot labels produces\nincorrect predictions, it still pays attention to the target image region and\nreveals which classes confuse the network. Inspired by this observation, we\npropose a confusion-focusing mechanism to address the class-confusion issue.\nOur confusion-focusing mechanism is implemented by a two-branch network\narchitecture. Its baseline branch generates confusing classes, and its FocusNet\nbranch, whose architecture is flexible, discriminates correct labels from these\nconfusing classes. We also introduce a novel focus-picking loss function to\nimprove classification accuracy by encouraging FocusNet to focus on the most\nconfusing classes. The experimental results validate that our FocusNet is\neffective for image classification on common datasets, and that our\nfocus-picking loss function can also benefit the current neural networks in\nimproving their classification accuracy.",
    "published_date": "2021-10-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.07307v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.07244v2",
    "title": "Building Chinese Biomedical Language Models via Multi-Level Text Discrimination",
    "authors": [
      "Quan Wang",
      "Songtai Dai",
      "Benfeng Xu",
      "Yajuan Lyu",
      "Yong Zhu",
      "Hua Wu",
      "Haifeng Wang"
    ],
    "author_ids": [],
    "abstract": "Pre-trained language models (PLMs), such as BERT and GPT, have revolutionized\nthe field of NLP, not only in the general domain but also in the biomedical\ndomain. Most prior efforts in building biomedical PLMs have resorted simply to\ndomain adaptation and focused mainly on English. In this work we introduce\neHealth, a Chinese biomedical PLM built from scratch with a new pre-training\nframework. This new framework pre-trains eHealth as a discriminator through\nboth token- and sequence-level discrimination. The former is to detect input\ntokens corrupted by a generator and recover their original identities from\nplausible candidates, while the latter is to further distinguish corruptions of\na same original sequence from those of others. As such, eHealth can learn\nlanguage semantics at both token and sequence levels. Extensive experiments on\n11 Chinese biomedical language understanding tasks of various forms verify the\neffectiveness and superiority of our approach. We release the pre-trained model\nat \\url{https://github.com/PaddlePaddle/Research/tree/master/KG/eHealth} and\nwill also release the code later.",
    "published_date": "2021-10-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.07244v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.07059v2",
    "title": "Subspace Regularizers for Few-Shot Class Incremental Learning",
    "authors": [
      "Afra Feyza Akyürek",
      "Ekin Akyürek",
      "Derry Tanti Wijaya",
      "Jacob Andreas"
    ],
    "author_ids": [],
    "abstract": "Few-shot class incremental learning -- the problem of updating a trained\nclassifier to discriminate among an expanded set of classes with limited\nlabeled data -- is a key challenge for machine learning systems deployed in\nnon-stationary environments. Existing approaches to the problem rely on complex\nmodel architectures and training procedures that are difficult to tune and\nre-use. In this paper, we present an extremely simple approach that enables the\nuse of ordinary logistic regression classifiers for few-shot incremental\nlearning. The key to this approach is a new family of subspace regularization\nschemes that encourage weight vectors for new classes to lie close to the\nsubspace spanned by the weights of existing classes. When combined with\npretrained convolutional feature extractors, logistic regression models trained\nwith subspace regularization outperform specialized, state-of-the-art\napproaches to few-shot incremental image classification by up to 22% on the\nminiImageNet dataset. Because of its simplicity, subspace regularization can be\nstraightforwardly extended to incorporate additional background information\nabout the new classes (including class names and descriptions specified in\nnatural language); these further improve accuracy by up to 2%. Our results show\nthat simple geometric regularization of class representations offers an\neffective tool for continual learning.",
    "published_date": "2021-10-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.07059v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.07058v3",
    "title": "Ego4D: Around the World in 3,000 Hours of Egocentric Video",
    "authors": [
      "Kristen Grauman",
      "Andrew Westbury",
      "Eugene Byrne",
      "Zachary Chavis",
      "Antonino Furnari",
      "Rohit Girdhar",
      "Jackson Hamburger",
      "Hao Jiang",
      "Miao Liu",
      "Xingyu Liu",
      "Miguel Martin",
      "Tushar Nagarajan",
      "Ilija Radosavovic",
      "Santhosh Kumar Ramakrishnan",
      "Fiona Ryan",
      "Jayant Sharma",
      "Michael Wray",
      "Mengmeng Xu",
      "Eric Zhongcong Xu",
      "Chen Zhao",
      "Siddhant Bansal",
      "Dhruv Batra",
      "Vincent Cartillier",
      "Sean Crane",
      "Tien Do",
      "Morrie Doulaty",
      "Akshay Erapalli",
      "Christoph Feichtenhofer",
      "Adriano Fragomeni",
      "Qichen Fu",
      "Abrham Gebreselasie",
      "Cristina Gonzalez",
      "James Hillis",
      "Xuhua Huang",
      "Yifei Huang",
      "Wenqi Jia",
      "Weslie Khoo",
      "Jachym Kolar",
      "Satwik Kottur",
      "Anurag Kumar",
      "Federico Landini",
      "Chao Li",
      "Yanghao Li",
      "Zhenqiang Li",
      "Karttikeya Mangalam",
      "Raghava Modhugu",
      "Jonathan Munro",
      "Tullie Murrell",
      "Takumi Nishiyasu",
      "Will Price",
      "Paola Ruiz Puentes",
      "Merey Ramazanova",
      "Leda Sari",
      "Kiran Somasundaram",
      "Audrey Southerland",
      "Yusuke Sugano",
      "Ruijie Tao",
      "Minh Vo",
      "Yuchen Wang",
      "Xindi Wu",
      "Takuma Yagi",
      "Ziwei Zhao",
      "Yunyi Zhu",
      "Pablo Arbelaez",
      "David Crandall",
      "Dima Damen",
      "Giovanni Maria Farinella",
      "Christian Fuegen",
      "Bernard Ghanem",
      "Vamsi Krishna Ithapu",
      "C. V. Jawahar",
      "Hanbyul Joo",
      "Kris Kitani",
      "Haizhou Li",
      "Richard Newcombe",
      "Aude Oliva",
      "Hyun Soo Park",
      "James M. Rehg",
      "Yoichi Sato",
      "Jianbo Shi",
      "Mike Zheng Shou",
      "Antonio Torralba",
      "Lorenzo Torresani",
      "Mingfei Yan",
      "Jitendra Malik"
    ],
    "author_ids": [],
    "abstract": "We introduce Ego4D, a massive-scale egocentric video dataset and benchmark\nsuite. It offers 3,670 hours of daily-life activity video spanning hundreds of\nscenarios (household, outdoor, workplace, leisure, etc.) captured by 931 unique\ncamera wearers from 74 worldwide locations and 9 different countries. The\napproach to collection is designed to uphold rigorous privacy and ethics\nstandards with consenting participants and robust de-identification procedures\nwhere relevant. Ego4D dramatically expands the volume of diverse egocentric\nvideo footage publicly available to the research community. Portions of the\nvideo are accompanied by audio, 3D meshes of the environment, eye gaze, stereo,\nand/or synchronized videos from multiple egocentric cameras at the same event.\nFurthermore, we present a host of new benchmark challenges centered around\nunderstanding the first-person visual experience in the past (querying an\nepisodic memory), present (analyzing hand-object manipulation, audio-visual\nconversation, and social interactions), and future (forecasting activities). By\npublicly sharing this massive annotated dataset and benchmark suite, we aim to\npush the frontier of first-person perception. Project page:\nhttps://ego4d-data.org/",
    "published_date": "2021-10-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.07058v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.06847v2",
    "title": "Ousiometrics and Telegnomics: The essence of meaning conforms to a two-dimensional powerful-weak and dangerous-safe framework with diverse corpora presenting a safety bias",
    "authors": [
      "P. S. Dodds",
      "T. Alshaabi",
      "M. I. Fudolig",
      "J. W. Zimmerman",
      "J. Lovato",
      "S. Beaulieu",
      "J. R. Minot",
      "M. V. Arnold",
      "A. J. Reagan",
      "C. M. Danforth"
    ],
    "author_ids": [],
    "abstract": "We define `ousiometrics' to be the study of essential meaning in whatever\ncontext that meaningful signals are communicated, and `telegnomics' as the\nstudy of remotely sensed knowledge. From work emerging through the middle of\nthe 20th century, the essence of meaning has become generally accepted as being\nwell captured by the three orthogonal dimensions of evaluation, potency, and\nactivation (EPA). By re-examining first types and then tokens for the English\nlanguage, and through the use of automatically annotated histograms --\n`ousiograms' -- we find here that: 1. The essence of meaning conveyed by words\nis instead best described by a compass-like power-danger (PD) framework, and 2.\nAnalysis of a disparate collection of large-scale English language corpora --\nliterature, news, Wikipedia, talk radio, and social media -- shows that natural\nlanguage exhibits a systematic bias toward safe, low danger words -- a\nreinterpretation of the Pollyanna principle's positivity bias for written\nexpression. To help justify our choice of dimension names and to help address\nthe problems with representing observed ousiometric dimensions by bipolar\nadjective pairs, we introduce and explore `synousionyms' and `antousionyms' --\nousiometric counterparts of synonyms and antonyms. We further show that the PD\nframework revises the circumplex model of affect as a more general model of\nstate of mind. Finally, we use our findings to construct and test a prototype\n`ousiometer', a telegnomic instrument that measures ousiometric time series for\ntemporal corpora. We contend that our power-danger ousiometric framework\nprovides a complement for entropy-based measurements, and may be of value for\nthe study of a wide variety of communication across biological and artificial\nlife.",
    "published_date": "2021-10-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.SI",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.06847v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.06803v3",
    "title": "Learn to Ignore: Domain Adaptation for Multi-Site MRI Analysis",
    "authors": [
      "Julia Wolleb",
      "Robin Sandkühler",
      "Florentin Bieder",
      "Muhamed Barakovic",
      "Nouchine Hadjikhani",
      "Athina Papadopoulou",
      "Özgür Yaldizli",
      "Jens Kuhle",
      "Cristina Granziera",
      "Philippe C. Cattin"
    ],
    "author_ids": [],
    "abstract": "The limited availability of large image datasets, mainly due to data privacy\nand differences in acquisition protocols or hardware, is a significant issue in\nthe development of accurate and generalizable machine learning methods in\nmedicine. This is especially the case for Magnetic Resonance (MR) images, where\ndifferent MR scanners introduce a bias that limits the performance of a machine\nlearning model. We present a novel method that learns to ignore the\nscanner-related features present in MR images, by introducing specific\nadditional constraints on the latent space. We focus on a real-world\nclassification scenario, where only a small dataset provides images of all\nclasses. Our method \\textit{Learn to Ignore (L2I)} outperforms state-of-the-art\ndomain adaptation methods on a multi-site MR dataset for a classification task\nbetween multiple sclerosis patients and healthy controls.",
    "published_date": "2021-10-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.06803v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.06733v1",
    "title": "Systematic Inequalities in Language Technology Performance across the World's Languages",
    "authors": [
      "Damián Blasi",
      "Antonios Anastasopoulos",
      "Graham Neubig"
    ],
    "author_ids": [],
    "abstract": "Natural language processing (NLP) systems have become a central technology in\ncommunication, education, medicine, artificial intelligence, and many other\ndomains of research and development. While the performance of NLP methods has\ngrown enormously over the last decade, this progress has been restricted to a\nminuscule subset of the world's 6,500 languages. We introduce a framework for\nestimating the global utility of language technologies as revealed in a\ncomprehensive snapshot of recent publications in NLP. Our analyses involve the\nfield at large, but also more in-depth studies on both user-facing technologies\n(machine translation, language understanding, question answering,\ntext-to-speech synthesis) as well as more linguistic NLP tasks (dependency\nparsing, morphological inflection). In the process, we (1) quantify disparities\nin the current state of NLP research, (2) explore some of its associated\nsocietal and academic factors, and (3) produce tailored recommendations for\nevidence-based policy making aimed at promoting more global and equitable\nlanguage technologies.",
    "published_date": "2021-10-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.06733v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.06488v1",
    "title": "The Convex Geometry of Backpropagation: Neural Network Gradient Flows Converge to Extreme Points of the Dual Convex Program",
    "authors": [
      "Yifei Wang",
      "Mert Pilanci"
    ],
    "author_ids": [],
    "abstract": "We study non-convex subgradient flows for training two-layer ReLU neural\nnetworks from a convex geometry and duality perspective. We characterize the\nimplicit bias of unregularized non-convex gradient flow as convex\nregularization of an equivalent convex model. We then show that the limit\npoints of non-convex subgradient flows can be identified via primal-dual\ncorrespondence in this convex optimization problem. Moreover, we derive a\nsufficient condition on the dual variables which ensures that the stationary\npoints of the non-convex objective are the KKT points of the convex objective,\nthus proving convergence of non-convex gradient flows to the global optimum.\nFor a class of regular training data distributions such as orthogonal separable\ndata, we show that this sufficient condition holds. Therefore, non-convex\ngradient flows in fact converge to optimal solutions of a convex optimization\nproblem. We present numerical results verifying the predictions of our theory\nfor non-convex subgradient descent.",
    "published_date": "2021-10-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.06488v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.06475v2",
    "title": "SAR-Net: A Scenario-Aware Ranking Network for Personalized Fair Recommendation in Hundreds of Travel Scenarios",
    "authors": [
      "Qijie Shen",
      "Wanjie Tao",
      "Jing Zhang",
      "Hong Wen",
      "Zulong Chen",
      "Quan Lu"
    ],
    "author_ids": [],
    "abstract": "The travel marketing platform of Alibaba serves an indispensable role for\nhundreds of different travel scenarios from Fliggy, Taobao, Alipay apps, etc.\nTo provide personalized recommendation service for users visiting different\nscenarios, there are two critical issues to be carefully addressed. First,\nsince the traffic characteristics of different scenarios, it is very\nchallenging to train a unified model to serve all. Second, during the promotion\nperiod, the exposure of some specific items will be re-weighted due to manual\nintervention, resulting in biased logs, which will degrade the ranking model\ntrained using these biased data. In this paper, we propose a novel\nScenario-Aware Ranking Network (SAR-Net) to address these issues. SAR-Net\nharvests the abundant data from different scenarios by learning users'\ncross-scenario interests via two specific attention modules, which leverage the\nscenario features and item features to modulate the user behavior features,\nrespectively. Then, taking the encoded features of previous module as input, a\nscenario-specific linear transformation layer is adopted to further extract\nscenario-specific features, followed by two groups of debias expert networks,\ni.e., scenario-specific experts and scenario-shared experts. They output\nintermediate results independently, which are further fused into the final\nresult by a multi-scenario gating module. In addition, to mitigate the data\nfairness issue caused by manual intervention, we propose the concept of\nFairness Coefficient (FC) to measures the importance of individual sample and\nuse it to reweigh the prediction in the debias expert networks. Experiments on\nan offline dataset covering over 80 million users and 1.55 million travel items\nand an online A/B test demonstrate the effectiveness of our SAR-Net and its\nsuperiority over state-of-the-art methods.",
    "published_date": "2021-10-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR",
      "H.3.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.06475v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.06306v2",
    "title": "Fine-grained style control in Transformer-based Text-to-speech Synthesis",
    "authors": [
      "Li-Wei Chen",
      "Alexander Rudnicky"
    ],
    "author_ids": [],
    "abstract": "In this paper, we present a novel architecture to realize fine-grained style\ncontrol on the transformer-based text-to-speech synthesis (TransformerTTS).\nSpecifically, we model the speaking style by extracting a time sequence of\nlocal style tokens (LST) from the reference speech. The existing content\nencoder in TransformerTTS is then replaced by our designed cross-attention\nblocks for fusion and alignment between content and style. As the fusion is\nperformed along with the skip connection, our cross-attention block provides a\ngood inductive bias to gradually infuse the phoneme representation with a given\nstyle. Additionally, we prevent the style embedding from encoding linguistic\ncontent by randomly truncating LST during training and using wav2vec 2.0\nfeatures. Experiments show that with fine-grained style control, our system\nperforms better in terms of naturalness, intelligibility, and style\ntransferability. Our code and samples are publicly available.",
    "published_date": "2021-10-12T00:00:00",
    "year": 2021,
    "categories": [
      "eess.AS",
      "cs.CL",
      "cs.LG",
      "cs.SD"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.06306v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.06282v4",
    "title": "The Rich Get Richer: Disparate Impact of Semi-Supervised Learning",
    "authors": [
      "Zhaowei Zhu",
      "Tianyi Luo",
      "Yang Liu"
    ],
    "author_ids": [],
    "abstract": "Semi-supervised learning (SSL) has demonstrated its potential to improve the\nmodel accuracy for a variety of learning tasks when the high-quality supervised\ndata is severely limited. Although it is often established that the average\naccuracy for the entire population of data is improved, it is unclear how SSL\nfares with different sub-populations. Understanding the above question has\nsubstantial fairness implications when different sub-populations are defined by\nthe demographic groups that we aim to treat fairly. In this paper, we reveal\nthe disparate impacts of deploying SSL: the sub-population who has a higher\nbaseline accuracy without using SSL (the \"rich\" one) tends to benefit more from\nSSL; while the sub-population who suffers from a low baseline accuracy (the\n\"poor\" one) might even observe a performance drop after adding the SSL module.\nWe theoretically and empirically establish the above observation for a broad\nfamily of SSL algorithms, which either explicitly or implicitly use an\nauxiliary \"pseudo-label\". Experiments on a set of image and text classification\ntasks confirm our claims. We introduce a new metric, Benefit Ratio, and promote\nthe evaluation of the fairness of SSL (Equalized Benefit Ratio). We further\ndiscuss how the disparate impact can be mitigated. We hope our paper will alarm\nthe potential pitfall of using SSL and encourage a multifaceted evaluation of\nfuture SSL algorithms.",
    "published_date": "2021-10-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.06282v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.06197v3",
    "title": "Crystal Diffusion Variational Autoencoder for Periodic Material Generation",
    "authors": [
      "Tian Xie",
      "Xiang Fu",
      "Octavian-Eugen Ganea",
      "Regina Barzilay",
      "Tommi Jaakkola"
    ],
    "author_ids": [],
    "abstract": "Generating the periodic structure of stable materials is a long-standing\nchallenge for the material design community. This task is difficult because\nstable materials only exist in a low-dimensional subspace of all possible\nperiodic arrangements of atoms: 1) the coordinates must lie in the local energy\nminimum defined by quantum mechanics, and 2) global stability also requires the\nstructure to follow the complex, yet specific bonding preferences between\ndifferent atom types. Existing methods fail to incorporate these factors and\noften lack proper invariances. We propose a Crystal Diffusion Variational\nAutoencoder (CDVAE) that captures the physical inductive bias of material\nstability. By learning from the data distribution of stable materials, the\ndecoder generates materials in a diffusion process that moves atomic\ncoordinates towards a lower energy state and updates atom types to satisfy\nbonding preferences between neighbors. Our model also explicitly encodes\ninteractions across periodic boundaries and respects permutation, translation,\nrotation, and periodic invariances. We significantly outperform past methods in\nthree tasks: 1) reconstructing the input structure, 2) generating valid,\ndiverse, and realistic materials, and 3) generating materials that optimize a\nspecific property. We also provide several standard datasets and evaluation\nmetrics for the broader machine learning community.",
    "published_date": "2021-10-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cond-mat.mtrl-sci",
      "physics.comp-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.06197v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.15728v1",
    "title": "Deep Learning for Bias Detection: From Inception to Deployment",
    "authors": [
      "Md Abul Bashar",
      "Richi Nayak",
      "Anjor Kothare",
      "Vishal Sharma",
      "Kesavan Kandadai"
    ],
    "author_ids": [],
    "abstract": "To create a more inclusive workplace, enterprises are actively investing in\nidentifying and eliminating unconscious bias (e.g., gender, race, age,\ndisability, elitism and religion) across their various functions. We propose a\ndeep learning model with a transfer learning based language model to learn from\nmanually tagged documents for automatically identifying bias in enterprise\ncontent. We first pretrain a deep learning-based language-model using\nWikipedia, then fine tune the model with a large unlabelled data set related\nwith various types of enterprise content. Finally, a linear layer followed by\nsoftmax layer is added at the end of the language model and the model is\ntrained on a labelled bias dataset consisting of enterprise content. The\ntrained model is thoroughly evaluated on independent datasets to ensure a\ngeneral application. We present the proposed method and its deployment detail\nin a real-world application.",
    "published_date": "2021-10-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.NE",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.15728v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.05933v1",
    "title": "A Deployment Model to Extend Ethically Aligned AI Implementation Method ECCOLA",
    "authors": [
      "Jani Antikainen",
      "Mamia Agbese",
      "Hanna-Kaisa Alanen",
      "Erika Halme",
      "Hannakaisa Isomäki",
      "Marianna Jantunen",
      "Kai-Kristian Kemell",
      "Rebekah Rousi",
      "Heidi Vainio-Pekka",
      "Ville Vakkuri"
    ],
    "author_ids": [],
    "abstract": "There is a struggle in Artificial intelligence (AI) ethics to gain ground in\nactionable methods and models to be utilized by practitioners while developing\nand implementing ethically sound AI systems. AI ethics is a vague concept\nwithout a consensus of definition or theoretical grounding and bearing little\nconnection to practice. Practice involving primarily technical tasks like\nsoftware development is not aptly equipped to process and decide upon ethical\nconsiderations. Efforts to create tools and guidelines to help people working\nwith AI development have been concentrating almost solely on the technical\naspects of AI. A few exceptions do apply, such as the ECCOLA method for\ncreating ethically aligned AI -systems. ECCOLA has proven results in terms of\nincreased ethical considerations in AI systems development. Yet, it is a novel\ninnovation, and room for development still exists. This study aims to extend\nECCOLA with a deployment model to drive the adoption of ECCOLA, as any method,\nno matter how good, is of no value without adoption and use. The model includes\nsimple metrics to facilitate the communication of ethical gaps or outcomes of\nethical AI development. It offers the opportunity to assess any AI system at\nany given lifecycle phase, e.g., opening possibilities like analyzing the\nethicality of an AI system under acquisition.",
    "published_date": "2021-10-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.05933v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.05906v1",
    "title": "Energy-cost aware off-grid base stations with IoT devices for developing a green heterogeneous network",
    "authors": [
      "Khondoker Ziaul Islam",
      "MD. Sanwar Hossain",
      "B. M. Ruhul Amin",
      "Ferdous Sohel"
    ],
    "author_ids": [],
    "abstract": "Heterogeneous network (HetNet) is a specified cellular platform to tackle the\nrapidly growing anticipated data traffic. From communications perspective, data\nloads can be mapped to energy loads that are generally placed on the operator\nnetworks. Meanwhile, renewable energy aided networks offer to curtail fossil\nfuel consumption, so to reduce environmental pollution. This paper proposes a\nrenewable energy based power supply architecture for off-grid HetNet using a\nnovel energy sharing model. Solar photovoltaic (PV) along with sufficient\nenergy storage devices are used for each macro, micro, pico, or femto base\nstation (BS). Additionally, biomass generator (BG) is used for macro and micro\nBSs. The collocated macro and micro BSs are connected through end-to-end\nresistive lines. A novel weighted proportional-fair resource-scheduling\nalgorithm with sleep mechanisms is proposed for non-real time (NRT)\napplications by trading-off the power consumption and communication delays.\nFurthermore, the proposed algorithm with extended discontinuous reception\n(eDRX) and power saving mode (PSM) for narrowband internet of things (IoT)\napplications extends battery lifetime for IoT devices. HOMER optimization\nsoftware is used to perform optimal system architecture, economic, and carbon\nfootprint analyses while Monte-Carlo simulation tool is used for evaluating the\nthroughput and energy efficiency performances. The proposed algorithms are\nvalid for the practical data of the rural areas. We demonstrate the proposed\npower supply architecture is energy-efficient, cost-effective, reliable, and\neco-friendly.",
    "published_date": "2021-10-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.NI",
      "eess.SP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.05906v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2110.05797v1",
    "title": "Zero-bias Deep Neural Network for Quickest RF Signal Surveillance",
    "authors": [
      "Yongxin Liu",
      "Yingjie Chen",
      "Jian Wang",
      "Shuteng Niu",
      "Dahai Liu",
      "Houbing Song"
    ],
    "author_ids": [],
    "abstract": "The Internet of Things (IoT) is reshaping modern society by allowing a decent\nnumber of RF devices to connect and share information through RF channels.\nHowever, such an open nature also brings obstacles to surveillance. For\nalleviation, a surveillance oracle, or a cognitive communication entity needs\nto identify and confirm the appearance of known or unknown signal sources in\nreal-time. In this paper, we provide a deep learning framework for RF signal\nsurveillance. Specifically, we jointly integrate the Deep Neural Networks\n(DNNs) and Quickest Detection (QD) to form a sequential signal surveillance\nscheme. We first analyze the latent space characteristic of neural network\nclassification models, and then we leverage the response characteristics of DNN\nclassifiers and propose a novel method to transform existing DNN classifiers\ninto performance-assured binary abnormality detectors. In this way, we\nseamlessly integrate the DNNs with the parametric quickest detection. Finally,\nwe propose an enhanced Elastic Weight Consolidation (EWC) algorithm with better\nnumerical stability for DNNs in signal surveillance systems to evolve\nincrementally, we demonstrate that the zero-bias DNN is superior to regular DNN\nmodels considering incremental learning and decision fairness. We evaluated the\nproposed framework using real signal datasets and we believe this framework is\nhelpful in developing a trustworthy IoT ecosystem.",
    "published_date": "2021-10-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "eess.SP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.05797v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.05367v3",
    "title": "Improving Gender Fairness of Pre-Trained Language Models without Catastrophic Forgetting",
    "authors": [
      "Zahra Fatemi",
      "Chen Xing",
      "Wenhao Liu",
      "Caiming Xiong"
    ],
    "author_ids": [],
    "abstract": "Existing studies addressing gender bias of pre-trained language models,\nusually build a small gender-neutral data set and conduct a second phase\npre-training on the model with such data. However, given the limited size and\nconcentrated focus of the gender-neutral data, catastrophic forgetting would\noccur during second-phase pre-training. Forgetting information in the original\ntraining data may damage the model's downstream performance by a large margin.\nIn this work, we empirically show that catastrophic forgetting occurs in such\nmethods by evaluating them with general NLP tasks in GLUE. Then, we propose a\nnew method, GEnder Equality Prompt (GEEP), to improve gender fairness of\npre-trained models with less forgetting. GEEP freezes the pre-trained model and\nlearns gender-related prompts with gender-neutral data. Empirical results show\nthat GEEP not only achieves SOTA performances on gender fairness tasks, but\nalso forgets less and performs better on GLUE by a large margin.",
    "published_date": "2021-10-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.05367v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.05343v1",
    "title": "Leveraging Transformers for StarCraft Macromanagement Prediction",
    "authors": [
      "Muhammad Junaid Khan",
      "Shah Hassan",
      "Gita Sukthankar"
    ],
    "author_ids": [],
    "abstract": "Inspired by the recent success of transformers in natural language processing\nand computer vision applications, we introduce a transformer-based neural\narchitecture for two key StarCraft II (SC2) macromanagement tasks: global state\nand build order prediction. Unlike recurrent neural networks which suffer from\na recency bias, transformers are able to capture patterns across very long time\nhorizons, making them well suited for full game analysis. Our model utilizes\nthe MSC (Macromanagement in StarCraft II) dataset and improves on the top\nperforming gated recurrent unit (GRU) architecture in predicting global state\nand build order as measured by mean accuracy over multiple time horizons. We\npresent ablation studies on our proposed architecture that support our design\ndecisions. One key advantage of transformers is their ability to generalize\nwell, and we demonstrate that our model achieves an even better accuracy when\nused in a transfer learning setting in which models trained on games with one\nracial matchup (e.g., Terran vs. Protoss) are transferred to a different one.\nWe believe that transformers' ability to model long games, potential for\nparallelization, and generalization performance make them an excellent choice\nfor StarCraft agents.",
    "published_date": "2021-10-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.05343v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.05301v1",
    "title": "On a Benefit of Mask Language Modeling: Robustness to Simplicity Bias",
    "authors": [
      "Ting-Rui Chiang"
    ],
    "author_ids": [],
    "abstract": "Despite the success of pretrained masked language models (MLM), why MLM\npretraining is useful is still a qeustion not fully answered. In this work we\ntheoretically and empirically show that MLM pretraining makes models robust to\nlexicon-level spurious features, partly answer the question. We theoretically\nshow that, when we can model the distribution of a spurious feature $\\Pi$\nconditioned on the context, then (1) $\\Pi$ is at least as informative as the\nspurious feature, and (2) learning from $\\Pi$ is at least as simple as learning\nfrom the spurious feature. Therefore, MLM pretraining rescues the model from\nthe simplicity bias caused by the spurious feature. We also explore the\nefficacy of MLM pretraing in causal settings. Finally we close the gap between\nour theories and the real world practices by conducting experiments on the hate\nspeech detection and the name entity recognition tasks.",
    "published_date": "2021-10-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.05301v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.05283v2",
    "title": "Phase Collapse in Neural Networks",
    "authors": [
      "Florentin Guth",
      "John Zarka",
      "Stéphane Mallat"
    ],
    "author_ids": [],
    "abstract": "Deep convolutional classifiers linearly separate image classes and improve\naccuracy as depth increases. They progressively reduce the spatial dimension\nwhereas the number of channels grows with depth. Spatial variability is\ntherefore transformed into variability along channels. A fundamental challenge\nis to understand the role of non-linearities together with convolutional\nfilters in this transformation. ReLUs with biases are often interpreted as\nthresholding operators that improve discrimination through sparsity. This paper\ndemonstrates that it is a different mechanism called phase collapse which\neliminates spatial variability while linearly separating classes. We show that\ncollapsing the phases of complex wavelet coefficients is sufficient to reach\nthe classification accuracy of ResNets of similar depths. However, replacing\nthe phase collapses with thresholding operators that enforce sparsity\nconsiderably degrades the performance. We explain these numerical results by\nshowing that the iteration of phase collapses progressively improves separation\nof classes, as opposed to thresholding non-linearities.",
    "published_date": "2021-10-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "eess.SP",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.05283v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.05164v1",
    "title": "Ethical Assurance: A practical approach to the responsible design, development, and deployment of data-driven technologies",
    "authors": [
      "Christopher Burr",
      "David Leslie"
    ],
    "author_ids": [],
    "abstract": "This article offers several contributions to the interdisciplinary project of\nresponsible research and innovation in data science and AI. First, it provides\na critical analysis of current efforts to establish practical mechanisms for\nalgorithmic assessment, which are used to operationalise normative principles,\nsuch as sustainability, accountability, transparency, fairness, and\nexplainability, in order to identify limitations and gaps with the current\napproaches. Second, it provides an accessible introduction to the methodology\nof argument-based assurance, and explores how it is currently being applied in\nthe development of safety cases for autonomous and intelligent systems. Third,\nit generalises this method to incorporate wider ethical, social, and legal\nconsiderations, in turn establishing a novel version of argument-based\nassurance that we call 'ethical assurance'. Ethical assurance is presented as a\nstructured means for unifying the myriad practical mechanisms that have been\nproposed, as it is built upon a process-based form of project governance that\nsupports inclusive and participatory ethical deliberation while also remaining\ngrounded in social and technical realities. Finally, it sets an agenda for\nethical assurance, by detailing current challenges, open questions, and next\nsteps, which serve as a springboard to build an active (and interdisciplinary)\nresearch programme as well as contribute to ongoing discussions in policy and\ngovernance.",
    "published_date": "2021-10-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.05164v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.04926v4",
    "title": "Convergence of Random Reshuffling Under The Kurdyka-Łojasiewicz Inequality",
    "authors": [
      "Xiao Li",
      "Andre Milzarek",
      "Junwen Qiu"
    ],
    "author_ids": [],
    "abstract": "We study the random reshuffling (RR) method for smooth nonconvex optimization\nproblems with a finite-sum structure. Though this method is widely utilized in\npractice such as the training of neural networks, its convergence behavior is\nonly understood in several limited settings. In this paper, under the\nwell-known Kurdyka-Lojasiewicz (KL) inequality, we establish strong limit-point\nconvergence results for RR with appropriate diminishing step sizes, namely, the\nwhole sequence of iterates generated by RR is convergent and converges to a\nsingle stationary point in an almost sure sense. In addition, we derive the\ncorresponding rate of convergence, depending on the KL exponent and the\nsuitably selected diminishing step sizes. When the KL exponent lies in\n$[0,\\frac12]$, the convergence is at a rate of $\\mathcal{O}(t^{-1})$ with $t$\ncounting the iteration number. When the KL exponent belongs to $(\\frac12,1)$,\nour derived convergence rate is of the form $\\mathcal{O}(t^{-q})$ with $q\\in\n(0,1)$ depending on the KL exponent. The standard KL inequality-based\nconvergence analysis framework only applies to algorithms with a certain\ndescent property. We conduct a novel convergence analysis for the non-descent\nRR method with diminishing step sizes based on the KL inequality, which\ngeneralizes the standard KL framework. We summarize our main steps and core\nideas in an informal analysis framework, which is of independent interest. As a\ndirect application of this framework, we also establish similar strong\nlimit-point convergence results for the reshuffled proximal point method.",
    "published_date": "2021-10-10T00:00:00",
    "year": 2021,
    "categories": [
      "math.OC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.04926v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.04880v1",
    "title": "Human Factors Considerations in Satellite Operation's Human-Computer Interaction Technologies: A Review of Current Applications and Theory",
    "authors": [
      "David G. I. Heinrich",
      "Ian McAndrew",
      "Jeremy Pretty"
    ],
    "author_ids": [],
    "abstract": "Satellite operations are a subset of remote operations that draw similarities\nwith remotely piloted aircraft (RPA) and uncrewed aerial vehicle (UAV)\noperations. Increased research into boredom, complacency, habituation, and\nvigilance as they relate to satellite operations is required due to a lack of\nprevalence in the literature. Circadian rhythms, crew resource management, and\nshift work dynamics may exacerbate complacency-driven automation bias and\nsocial loafing errors in satellite operations. This overview of theory and\napplications aims to specifically focus on satellite operations literature\nwithin human factors research to identify areas requiring an expansion of\nknowledge. The human-in-the-loop commonality enables human factors lessons to\nbe passed to satellite operations from unrelated sectors to mitigate\ncatastrophic human error potentially. As such, this literature review details\nthe need for increased research in satellite operations human factors.",
    "published_date": "2021-10-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.04880v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.04837v1",
    "title": "Using Edge Cases to Disentangle Fairness and Solidarity in AI Ethics",
    "authors": [
      "James Brusseau"
    ],
    "author_ids": [],
    "abstract": "Principles of fairness and solidarity in AI ethics regularly overlap,\ncreating obscurity in practice: acting in accordance with one can appear\nindistinguishable from deciding according to the rules of the other. However,\nthere exist irregular cases where the two concepts split, and so reveal their\ndisparate meanings and uses. This paper explores two cases in AI medical\nethics, one that is irregular and the other more conventional, to fully\ndistinguish fairness and solidarity. Then the distinction is applied to the\nfrequently cited COMPAS versus ProPublica dispute in judicial ethics. The\napplication provides a broader model for settling contemporary and topical\ndebates about fairness and solidarity. It also implies a deeper and\ndisorienting truth about AI ethics principles and their justification.",
    "published_date": "2021-10-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.04837v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.04770v1",
    "title": "Weakly Supervised Contrastive Learning",
    "authors": [
      "Mingkai Zheng",
      "Fei Wang",
      "Shan You",
      "Chen Qian",
      "Changshui Zhang",
      "Xiaogang Wang",
      "Chang Xu"
    ],
    "author_ids": [],
    "abstract": "Unsupervised visual representation learning has gained much attention from\nthe computer vision community because of the recent achievement of contrastive\nlearning. Most of the existing contrastive learning frameworks adopt the\ninstance discrimination as the pretext task, which treating every single\ninstance as a different class. However, such method will inevitably cause class\ncollision problems, which hurts the quality of the learned representation.\nMotivated by this observation, we introduced a weakly supervised contrastive\nlearning framework (WCL) to tackle this issue. Specifically, our proposed\nframework is based on two projection heads, one of which will perform the\nregular instance discrimination task. The other head will use a graph-based\nmethod to explore similar samples and generate a weak label, then perform a\nsupervised contrastive learning task based on the weak label to pull the\nsimilar images closer. We further introduced a K-Nearest Neighbor based\nmulti-crop strategy to expand the number of positive samples. Extensive\nexperimental results demonstrate WCL improves the quality of self-supervised\nrepresentations across different datasets. Notably, we get a new\nstate-of-the-art result for semi-supervised learning. With only 1\\% and 10\\%\nlabeled examples, WCL achieves 65\\% and 72\\% ImageNet Top-1 Accuracy using\nResNet50, which is even higher than SimCLRv2 with ResNet101.",
    "published_date": "2021-10-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.04770v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.04729v2",
    "title": "Humans' Assessment of Robots as Moral Regulators: Importance of Perceived Fairness and Legitimacy",
    "authors": [
      "Boyoung Kim",
      "Elizabeth Phillips"
    ],
    "author_ids": [],
    "abstract": "Previous research has shown that the fairness and the legitimacy of a moral\ndecision-maker are important for people's acceptance of and compliance with the\ndecision-maker. As technology rapidly advances, there have been increasing\nhopes and concerns about building artificially intelligent entities that are\ndesigned to intervene against norm violations. However, it is unclear how\npeople would perceive artificial moral regulators that impose punishment on\nhuman wrongdoers. Grounded in theories of psychology and law, we predict that\nthe perceived fairness of punishment imposed by a robot would increase the\nlegitimacy of the robot functioning as a moral regulator, which would in turn,\nincrease people's willingness to accept and comply with the robot's decisions.\nWe close with a conceptual framework for building a robot moral regulator that\nsuccessfully can regulate norm violations.",
    "published_date": "2021-10-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.RO",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.04729v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2110.04541v3",
    "title": "The Inductive Bias of In-Context Learning: Rethinking Pretraining Example Design",
    "authors": [
      "Yoav Levine",
      "Noam Wies",
      "Daniel Jannai",
      "Dan Navon",
      "Yedid Hoshen",
      "Amnon Shashua"
    ],
    "author_ids": [],
    "abstract": "Pretraining Neural Language Models (NLMs) over a large corpus involves\nchunking the text into training examples, which are contiguous text segments of\nsizes processable by the neural architecture. We highlight a bias introduced by\nthis common practice: we prove that the pretrained NLM can model much stronger\ndependencies between text segments that appeared in the same training example,\nthan it can between text segments that appeared in different training examples.\nThis intuitive result has a twofold role. First, it formalizes the motivation\nbehind a broad line of recent successful NLM training heuristics, proposed for\nthe pretraining and fine-tuning stages, which do not necessarily appear related\nat first glance. Second, our result clearly indicates further improvements to\nbe made in NLM pretraining for the benefit of Natural Language Understanding\ntasks. As an example, we propose \"kNN-Pretraining\": we show that including\nsemantically related non-neighboring sentences in the same pretraining example\nyields improved sentence representations and open domain question answering\nabilities. This theoretically motivated degree of freedom for pretraining\nexample design indicates new training schemes for self-improving\nrepresentations.",
    "published_date": "2021-10-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.04541v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.04487v1",
    "title": "Colour augmentation for improved semi-supervised semantic segmentation",
    "authors": [
      "Geoff French",
      "Michal Mackiewicz"
    ],
    "author_ids": [],
    "abstract": "Consistency regularization describes a class of approaches that have yielded\nstate-of-the-art results for semi-supervised classification. While\nsemi-supervised semantic segmentation proved to be more challenging, a number\nof successful approaches have been recently proposed. Recent work explored the\nchallenges involved in using consistency regularization for segmentation\nproblems. In their self-supervised work Chen et al. found that colour\naugmentation prevents a classification network from using image colour\nstatistics as a short-cut for self-supervised learning via instance\ndiscrimination. Drawing inspiration from this we find that a similar problem\nimpedes semi-supervised semantic segmentation and offer colour augmentation as\na solution, improving semi-supervised semantic segmentation performance on\nchallenging photographic imagery.",
    "published_date": "2021-10-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.04487v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.04452v1",
    "title": "Towards AI Logic for Social Reasoning",
    "authors": [
      "Huimin Dong",
      "Réka Markovich",
      "Leendert van der Torre"
    ],
    "author_ids": [],
    "abstract": "Artificial Intelligence (AI) logic formalizes the reasoning of intelligent\nagents. In this paper, we discuss how an argumentation-based AI logic could be\nused also to formalize important aspects of social reasoning. Besides reasoning\nabout the knowledge and actions of individual agents, social AI logic can\nreason also about social dependencies among agents using the rights,\nobligations and permissions of the agents. We discuss four aspects of social AI\nlogic. First, we discuss how rights represent relations between the obligations\nand permissions of intelligent agents. Second, we discuss how to argue about\nthe right-to-know, a central issue in the recent discussion of privacy and\nethics. Third, we discuss how a wide variety of conflicts among intelligent\nagents can be identified and (sometimes) resolved by comparing formal\narguments. Importantly, to cover a wide range of arguments occurring in daily\nlife, also fallacious arguments can be represented and reasoned about. Fourth,\nwe discuss how to argue about the freedom to act for intelligent agents.\nExamples from social, legal and ethical reasoning highlight the challenges in\ndeveloping social AI logic. The discussion of the four challenges leads to a\nresearch program for argumentation-based social AI logic, contributing towards\nthe future development of AI logic.",
    "published_date": "2021-10-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.04452v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.04397v1",
    "title": "Measure Twice, Cut Once: Quantifying Bias and Fairness in Deep Neural Networks",
    "authors": [
      "Cody Blakeney",
      "Gentry Atkinson",
      "Nathaniel Huish",
      "Yan Yan",
      "Vangelis Metris",
      "Ziliang Zong"
    ],
    "author_ids": [],
    "abstract": "Algorithmic bias is of increasing concern, both to the research community,\nand society at large. Bias in AI is more abstract and unintuitive than\ntraditional forms of discrimination and can be more difficult to detect and\nmitigate. A clear gap exists in the current literature on evaluating the\nrelative bias in the performance of multi-class classifiers. In this work, we\npropose two simple yet effective metrics, Combined Error Variance (CEV) and\nSymmetric Distance Error (SDE), to quantitatively evaluate the class-wise bias\nof two models in comparison to one another. By evaluating the performance of\nthese new metrics and by demonstrating their practical application, we show\nthat they can be used to measure fairness as well as bias. These demonstrations\nshow that our metrics can address specific needs for measuring bias in\nmulti-class classification.",
    "published_date": "2021-10-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.04397v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.04384v1",
    "title": "Evaluation of Summarization Systems across Gender, Age, and Race",
    "authors": [
      "Anna Jørgensen",
      "Anders Søgaard"
    ],
    "author_ids": [],
    "abstract": "Summarization systems are ultimately evaluated by human annotators and\nraters. Usually, annotators and raters do not reflect the demographics of end\nusers, but are recruited through student populations or crowdsourcing platforms\nwith skewed demographics. For two different evaluation scenarios -- evaluation\nagainst gold summaries and system output ratings -- we show that summary\nevaluation is sensitive to protected attributes. This can severely bias system\ndevelopment and evaluation, leading us to build models that cater for some\ngroups rather than others.",
    "published_date": "2021-10-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.04384v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.04372v1",
    "title": "Fair Regression under Sample Selection Bias",
    "authors": [
      "Wei Du",
      "Xintao Wu",
      "Hanghang Tong"
    ],
    "author_ids": [],
    "abstract": "Recent research on fair regression focused on developing new fairness notions\nand approximation methods as target variables and even the sensitive attribute\nare continuous in the regression setting. However, all previous fair regression\nresearch assumed the training data and testing data are drawn from the same\ndistributions. This assumption is often violated in real world due to the\nsample selection bias between the training and testing data. In this paper, we\ndevelop a framework for fair regression under sample selection bias when\ndependent variable values of a set of samples from the training data are\nmissing as a result of another hidden process. Our framework adopts the classic\nHeckman model for bias correction and the Lagrange duality to achieve fairness\nin regression based on a variety of fairness notions. Heckman model describes\nthe sample selection process and uses a derived variable called the Inverse\nMills Ratio (IMR) to correct sample selection bias. We use fairness inequality\nand equality constraints to describe a variety of fairness notions and apply\nthe Lagrange duality theory to transform the primal problem into the dual\nconvex optimization. For the two popular fairness notions, mean difference and\nmean squared error difference, we derive explicit formulas without iterative\noptimization, and for Pearson correlation, we derive its conditions of\nachieving strong duality. We conduct experiments on three real-world datasets\nand the experimental results demonstrate the approach's effectiveness in terms\nof both utility and fairness metrics.",
    "published_date": "2021-10-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.04372v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.04363v1",
    "title": "Certifying Robustness to Programmable Data Bias in Decision Trees",
    "authors": [
      "Anna P. Meyer",
      "Aws Albarghouthi",
      "Loris D'Antoni"
    ],
    "author_ids": [],
    "abstract": "Datasets can be biased due to societal inequities, human biases,\nunder-representation of minorities, etc. Our goal is to certify that models\nproduced by a learning algorithm are pointwise-robust to potential dataset\nbiases. This is a challenging problem: it entails learning models for a large,\nor even infinite, number of datasets, ensuring that they all produce the same\nprediction. We focus on decision-tree learning due to the interpretable nature\nof the models. Our approach allows programmatically specifying bias models\nacross a variety of dimensions (e.g., missing data for minorities), composing\ntypes of bias, and targeting bias towards a specific group. To certify\nrobustness, we use a novel symbolic technique to evaluate a decision-tree\nlearner on a large, or infinite, number of datasets, certifying that each and\nevery dataset produces the same prediction for a specific test point. We\nevaluate our approach on datasets that are commonly used in the fairness\nliterature, and demonstrate our approach's viability on a range of bias models.",
    "published_date": "2021-10-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY",
      "I.2.2; I.5.0; K.4.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.04363v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.04328v2",
    "title": "Distinguishing rule- and exemplar-based generalization in learning systems",
    "authors": [
      "Ishita Dasgupta",
      "Erin Grant",
      "Thomas L. Griffiths"
    ],
    "author_ids": [],
    "abstract": "Machine learning systems often do not share the same inductive biases as\nhumans and, as a result, extrapolate or generalize in ways that are\ninconsistent with our expectations. The trade-off between exemplar- and\nrule-based generalization has been studied extensively in cognitive psychology;\nin this work, we present a protocol inspired by these experimental approaches\nto probe the inductive biases that control this tradeoff in category-learning\nsystems. We isolate two such inductive biases: feature-level bias (differences\nin which features are more readily learned) and exemplar or rule bias\n(differences in how these learned features are used for generalization). We\nfind that standard neural network models are feature-biased and exemplar-based,\nand discuss the implications of these findings for machine learning research on\nsystematic generalization, fairness, and data augmentation.",
    "published_date": "2021-10-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.04328v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.09231v2",
    "title": "Machine Learning Featurizations for AI Hacking of Political Systems",
    "authors": [
      "Nathan E Sanders",
      "Bruce Schneier"
    ],
    "author_ids": [],
    "abstract": "What would the inputs be to a machine whose output is the destabilization of\na robust democracy, or whose emanations could disrupt the political power of\nnations? In the recent essay \"The Coming AI Hackers,\" Schneier (2021) proposed\na future application of artificial intelligences to discover, manipulate, and\nexploit vulnerabilities of social, economic, and political systems at speeds\nfar greater than humans' ability to recognize and respond to such threats. This\nwork advances the concept by applying to it theory from machine learning,\nhypothesizing some possible \"featurization\" (input specification and\ntransformation) frameworks for AI hacking. Focusing on the political domain, we\ndevelop graph and sequence data representations that would enable the\napplication of a range of deep learning models to predict attributes and\noutcomes of political, particularly legislative, systems. We explore possible\ndata models, datasets, predictive tasks, and actionable applications associated\nwith each framework. We speculate about the likely practical impact and\nfeasibility of such models, and conclude by discussing their ethical\nimplications.",
    "published_date": "2021-10-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.09231v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.08038v1",
    "title": "Toward Annotator Group Bias in Crowdsourcing",
    "authors": [
      "Haochen Liu",
      "Joseph Thekinen",
      "Sinem Mollaoglu",
      "Da Tang",
      "Ji Yang",
      "Youlong Cheng",
      "Hui Liu",
      "Jiliang Tang"
    ],
    "author_ids": [],
    "abstract": "Crowdsourcing has emerged as a popular approach for collecting annotated data\nto train supervised machine learning models. However, annotator bias can lead\nto defective annotations. Though there are a few works investigating individual\nannotator bias, the group effects in annotators are largely overlooked. In this\nwork, we reveal that annotators within the same demographic group tend to show\nconsistent group bias in annotation tasks and thus we conduct an initial study\non annotator group bias. We first empirically verify the existence of annotator\ngroup bias in various real-world crowdsourcing datasets. Then, we develop a\nnovel probabilistic graphical framework GroupAnno to capture annotator group\nbias with a new extended Expectation Maximization (EM) training algorithm. We\nconduct experiments on both synthetic and real-world datasets. Experimental\nresults demonstrate the effectiveness of our model in modeling annotator group\nbias in label aggregation and model learning over competitive baselines.",
    "published_date": "2021-10-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.08038v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.03873v2",
    "title": "Representation of professions in entertainment media: Insights into frequency and sentiment trends through computational text analysis",
    "authors": [
      "Sabyasachee Baruah",
      "Krishna Somandepalli",
      "Shrikanth Narayanan"
    ],
    "author_ids": [],
    "abstract": "Societal ideas and trends dictate media narratives and cinematic depictions\nwhich in turn influences people's beliefs and perceptions of the real world.\nMedia portrayal of culture, education, government, religion, and family affect\ntheir function and evolution over time as people interpret and perceive these\nrepresentations and incorporate them into their beliefs and actions. It is\nimportant to study media depictions of these social structures so that they do\nnot propagate or reinforce negative stereotypes, or discriminate against any\ndemographic section. In this work, we examine media representation of\nprofessions and provide computational insights into their incidence, and\nsentiment expressed, in entertainment media content. We create a searchable\ntaxonomy of professional groups and titles to facilitate their retrieval from\nspeaker-agnostic text passages like movie and television (TV) show subtitles.\nWe leverage this taxonomy and relevant natural language processing (NLP) models\nto create a corpus of professional mentions in media content, spanning more\nthan 136,000 IMDb titles over seven decades (1950-2017). We analyze the\nfrequency and sentiment trends of different occupations, study the effect of\nmedia attributes like genre, country of production, and title type on these\ntrends, and investigate if the incidence of professions in media subtitles\ncorrelate with their real-world employment statistics. We observe increased\nmedia mentions of STEM, arts, sports, and entertainment occupations in the\nanalyzed subtitles, and a decreased frequency of manual labor jobs and military\noccupations. The sentiment expressed toward lawyers, police, and doctors is\nbecoming negative over time, whereas astronauts, musicians, singers, and\nengineers are mentioned favorably. Professions that employ more people have\nincreased media frequency, supporting our hypothesis that media acts as a\nmirror to society.",
    "published_date": "2021-10-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.03873v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.03588v6",
    "title": "A transformer-based deep learning approach for classifying brain metastases into primary organ sites using clinical whole brain MRI",
    "authors": [
      "Qing Lyu",
      "Sanjeev V. Namjoshi",
      "Emory McTyre",
      "Umit Topaloglu",
      "Richard Barcus",
      "Michael D. Chan",
      "Christina K. Cramer",
      "Waldemar Debinski",
      "Metin N. Gurcan",
      "Glenn J. Lesser",
      "Hui-Kuan Lin",
      "Reginald F. Munden",
      "Boris C. Pasche",
      "Kiran Kumar Solingapuram Sai",
      "Roy E. Strowd",
      "Stephen B. Tatter",
      "Kounosuke Watabe",
      "Wei Zhang",
      "Ge Wang",
      "Christopher T. Whitlow"
    ],
    "author_ids": [],
    "abstract": "Treatment decisions for brain metastatic disease rely on knowledge of the\nprimary organ site, and currently made with biopsy and histology. Here we\ndevelop a novel deep learning approach for accurate non-invasive digital\nhistology with whole-brain MRI data. Our IRB-approved single-site retrospective\nstudy was comprised of patients (n=1,399) referred for MRI treatment-planning\nand gamma knife radiosurgery over 21 years. Contrast-enhanced T1-weighted and\nT2-weighted Fluid-Attenuated Inversion Recovery brain MRI exams (n=1,582) were\npreprocessed and input to the proposed deep learning workflow for tumor\nsegmentation, modality transfer, and primary site classification into one of\nfive classes. Ten-fold cross-validation generated overall AUC of 0.878\n(95%CI:0.873,0.883), lung class AUC of 0.889 (95%CI:0.883,0.895), breast class\nAUC of 0.873 (95%CI:0.860,0.886), melanoma class AUC of 0.852\n(95%CI:0.842,0.862), renal class AUC of 0.830 (95%CI:0.809,0.851), and other\nclass AUC of 0.822 (95%CI:0.805,0.839). These data establish that whole-brain\nimaging features are discriminative to allow accurate diagnosis of the primary\norgan site of malignancy. Our end-to-end deep radiomic approach has great\npotential for classifying metastatic tumor types from whole-brain MRI images.\nFurther refinement may offer an invaluable clinical tool to expedite primary\ncancer site identification for precision treatment and improved outcomes.",
    "published_date": "2021-10-07T00:00:00",
    "year": 2021,
    "categories": [
      "eess.IV",
      "cs.CV",
      "physics.med-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.03588v6",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.03549v2",
    "title": "Bias-Variance Tradeoffs in Single-Sample Binary Gradient Estimators",
    "authors": [
      "Alexander Shekhovtsov"
    ],
    "author_ids": [],
    "abstract": "Discrete and especially binary random variables occur in many machine\nlearning models, notably in variational autoencoders with binary latent states\nand in stochastic binary networks. When learning such models, a key tool is an\nestimator of the gradient of the expected loss with respect to the\nprobabilities of binary variables. The straight-through (ST) estimator gained\npopularity due to its simplicity and efficiency, in particular in deep networks\nwhere unbiased estimators are impractical. Several techniques were proposed to\nimprove over ST while keeping the same low computational complexity:\nGumbel-Softmax, ST-Gumbel-Softmax, BayesBiNN, FouST. We conduct a theoretical\nanalysis of bias and variance of these methods in order to understand tradeoffs\nand verify the originally claimed properties. The presented theoretical results\nallow for better understanding of these methods and in some cases reveal\nserious issues.",
    "published_date": "2021-10-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.NE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.03549v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.03524v1",
    "title": "Data-Driven Methods for Balancing Fairness and Efficiency in Ride-Pooling",
    "authors": [
      "Naveen Raman",
      "Sanket Shah",
      "John Dickerson"
    ],
    "author_ids": [],
    "abstract": "Rideshare and ride-pooling platforms use artificial intelligence-based\nmatching algorithms to pair riders and drivers. However, these platforms can\ninduce inequality either through an unequal income distribution or disparate\ntreatment of riders. We investigate two methods to reduce forms of inequality\nin ride-pooling platforms: (1) incorporating fairness constraints into the\nobjective function and (2) redistributing income to drivers to reduce income\nfluctuation and inequality. To evaluate our solutions, we use the New York City\ntaxi data set. For the first method, we find that optimizing for driver-side\nfairness outperforms state-of-the-art models on the number of riders serviced,\nboth in the worst-off neighborhood and overall, showing that optimizing for\nfairness can assist profitability in certain circumstances. For the second\nmethod, we explore income redistribution as a way to combat income inequality\nby having drivers keep an $r$ fraction of their income, and contributing the\nrest to a redistribution pool. For certain values of $r$, most drivers earn\nnear their Shapley value, while still incentivizing drivers to maximize value,\nthereby avoiding the free-rider problem and reducing income variability. The\nfirst method can be extended to many definitions of fairness and the second\nmethod provably improves fairness without affecting profitability.",
    "published_date": "2021-10-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.03524v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.03382v1",
    "title": "Analysis of the influence of political polarization in the vaccination stance: the Brazilian COVID-19 scenario",
    "authors": [
      "Régis Ebeling",
      "Carlos Abel Córdova Sáenz",
      "Jeferson Nobre",
      "Karin Becker"
    ],
    "author_ids": [],
    "abstract": "The outbreak of COVID-19 had a huge global impact, and non-scientific beliefs\nand political polarization have significantly influenced the population's\nbehavior. In this context, COVID vaccines were made available in an\nunprecedented time, but a high level of hesitance has been observed that can\nundermine community immunization. Traditionally, anti-vaccination attitudes are\nmore related to conspiratorial thinking rather than political bias. In Brazil,\na country with an exemplar tradition in large-scale vaccination programs, all\nCOVID-related topics have also been discussed under a strong political bias. In\nthis paper, we use a multidimensional analysis framework to understand if\nanti/pro-vaccination stances expressed by Brazilians in social media are\ninfluenced by political polarization. The analysis framework incorporates\ntechniques to automatically infer from users their political orientation, topic\nmodeling to discover their concerns, network analysis to characterize their\nsocial behavior, and the characterization of information sources and external\ninfluence. Our main findings confirm that anti/pro stances are biased by\npolitical polarization, right and left, respectively. While a significant\nproportion of pro-vaxxers display haste for an immunization program and\ncriticize the government's actions, the anti-vaxxers distrust a vaccine\ndeveloped in a record time. Anti-vaccination stance is also related to\nprejudice against China (anti-sinovaxxers), revealing conspiratorial theories\nrelated to communism. All groups display an \"echo chamber behavior, revealing\nthey are not open to distinct views.",
    "published_date": "2021-10-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.03382v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2110.03313v3",
    "title": "Distributed Methods with Compressed Communication for Solving Variational Inequalities, with Theoretical Guarantees",
    "authors": [
      "Aleksandr Beznosikov",
      "Peter Richtárik",
      "Michael Diskin",
      "Max Ryabinin",
      "Alexander Gasnikov"
    ],
    "author_ids": [],
    "abstract": "Variational inequalities in general and saddle point problems in particular\nare increasingly relevant in machine learning applications, including\nadversarial learning, GANs, transport and robust optimization. With increasing\ndata and problem sizes necessary to train high performing models across various\napplications, we need to rely on parallel and distributed computing. However,\nin distributed training, communication among the compute nodes is a key\nbottleneck during training, and this problem is exacerbated for high\ndimensional and over-parameterized models. Due to these considerations, it is\nimportant to equip existing methods with strategies that would allow to reduce\nthe volume of transmitted information during training while obtaining a model\nof comparable quality. In this paper, we present the first theoretically\ngrounded distributed methods for solving variational inequalities and saddle\npoint problems using compressed communication: MASHA1 and MASHA2. Our theory\nand methods allow for the use of both unbiased (such as Rand$k$; MASHA1) and\ncontractive (such as Top$k$; MASHA2) compressors. New algorithms support\nbidirectional compressions, and also can be modified for stochastic setting\nwith batches and for federated learning with partial participation of clients.\nWe empirically validated our conclusions using two experimental setups: a\nstandard bilinear min-max problem, and large-scale distributed adversarial\ntraining of transformers.",
    "published_date": "2021-10-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.03313v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.03275v1",
    "title": "Doing Data Right: How Lessons Learned Working with Conventional Data should Inform the Future of Synthetic Data for Recommender Systems",
    "authors": [
      "Manel Slokom",
      "Martha Larson"
    ],
    "author_ids": [],
    "abstract": "We present a case that the newly emerging field of synthetic data in the area\nof recommender systems should prioritize `doing data right'. We consider this\ncatchphrase to have two aspects: First, we should not repeat the mistakes of\nthe past, and, second, we should explore the full scope of opportunities\npresented by synthetic data as we move into the future. We argue that explicit\nattention to dataset design and description will help to avoid past mistakes\nwith dataset bias and evaluation. In order to fully exploit the opportunities\nof synthetic data, we point out that researchers can investigate new areas such\nas using data synthesize to support reproducibility by making data open, as\nwell as FAIR, and to push forward our understanding of data minimization.",
    "published_date": "2021-10-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.03275v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2110.03266v2",
    "title": "Lagrangian Neural Network with Differentiable Symmetries and Relational Inductive Bias",
    "authors": [
      "Ravinder Bhattoo",
      "Sayan Ranu",
      "N. M. Anoop Krishnan"
    ],
    "author_ids": [],
    "abstract": "Realistic models of physical world rely on differentiable symmetries that, in\nturn, correspond to conservation laws. Recent works on Lagrangian and\nHamiltonian neural networks show that the underlying symmetries of a system can\nbe easily learned by a neural network when provided with an appropriate\ninductive bias. However, these models still suffer from issues such as\ninability to generalize to arbitrary system sizes, poor interpretability, and\nmost importantly, inability to learn translational and rotational symmetries,\nwhich lead to the conservation laws of linear and angular momentum,\nrespectively. Here, we present a momentum conserving Lagrangian neural network\n(MCLNN) that learns the Lagrangian of a system, while also preserving the\ntranslational and rotational symmetries. We test our approach on linear and\nnon-linear spring systems, and a gravitational system, demonstrating the energy\nand momentum conservation. We also show that the model developed can generalize\nto systems of any arbitrary size. Finally, we discuss the interpretability of\nthe MCLNN, which directly provides physical insights into the interactions of\nmulti-particle systems.",
    "published_date": "2021-10-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cond-mat.mtrl-sci",
      "cs.AI",
      "math.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.03266v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.03095v2",
    "title": "Which Shortcut Cues Will DNNs Choose? A Study from the Parameter-Space Perspective",
    "authors": [
      "Luca Scimeca",
      "Seong Joon Oh",
      "Sanghyuk Chun",
      "Michael Poli",
      "Sangdoo Yun"
    ],
    "author_ids": [],
    "abstract": "Deep neural networks (DNNs) often rely on easy-to-learn discriminatory\nfeatures, or cues, that are not necessarily essential to the problem at hand.\nFor example, ducks in an image may be recognized based on their typical\nbackground scenery, such as lakes or streams. This phenomenon, also known as\nshortcut learning, is emerging as a key limitation of the current generation of\nmachine learning models. In this work, we introduce a set of experiments to\ndeepen our understanding of shortcut learning and its implications. We design a\ntraining setup with several shortcut cues, named WCST-ML, where each cue is\nequally conducive to the visual recognition problem at hand. Even under equal\nopportunities, we observe that (1) certain cues are preferred to others, (2)\nsolutions biased to the easy-to-learn cues tend to converge to relatively flat\nminima on the loss surface, and (3) the solutions focusing on those preferred\ncues are far more abundant in the parameter space. We explain the abundance of\ncertain cues via their Kolmogorov (descriptional) complexity: solutions\ncorresponding to Kolmogorov-simple cues are abundant in the parameter space and\nare thus preferred by DNNs. Our studies are based on the synthetic dataset\nDSprites and the face dataset UTKFace. In our WCST-ML, we observe that the\ninborn bias of models leans toward simple cues, such as color and ethnicity.\nOur findings emphasize the importance of active human intervention to remove\nthe inborn model biases that may cause negative societal impacts.",
    "published_date": "2021-10-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.03095v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.03091v2",
    "title": "Improving Fractal Pre-training",
    "authors": [
      "Connor Anderson",
      "Ryan Farrell"
    ],
    "author_ids": [],
    "abstract": "The deep neural networks used in modern computer vision systems require\nenormous image datasets to train them. These carefully-curated datasets\ntypically have a million or more images, across a thousand or more distinct\ncategories. The process of creating and curating such a dataset is a monumental\nundertaking, demanding extensive effort and labelling expense and necessitating\ncareful navigation of technical and social issues such as label accuracy,\ncopyright ownership, and content bias.\n  What if we had a way to harness the power of large image datasets but with\nfew or none of the major issues and concerns currently faced? This paper\nextends the recent work of Kataoka et. al. (2020), proposing an improved\npre-training dataset based on dynamically-generated fractal images. Challenging\nissues with large-scale image datasets become points of elegance for fractal\npre-training: perfect label accuracy at zero cost; no need to store/transmit\nlarge image archives; no privacy/demographic bias/concerns of inappropriate\ncontent, as no humans are pictured; limitless supply and diversity of images;\nand the images are free/open-source. Perhaps surprisingly, avoiding these\ndifficulties imposes only a small penalty in performance. Leveraging a\nnewly-proposed pre-training task -- multi-instance prediction -- our\nexperiments demonstrate that fine-tuning a network pre-trained using fractals\nattains 92.7-98.1% of the accuracy of an ImageNet pre-trained network.",
    "published_date": "2021-10-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.03091v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.03026v1",
    "title": "Human Capabilities as Guiding Lights for the Field of AI-HRI: Insights from Engineering Education",
    "authors": [
      "Tom Williams",
      "Ruchen Wen"
    ],
    "author_ids": [],
    "abstract": "Social Justice oriented Engineering Education frameworks have been developed\nto help guide engineering students' decisions about which projects will\ngenuinely address human needs to create a better and more equitable society. In\nthis paper, we explore the role such theories might play in the field of\nAI-HRI, consider the extent to which our community is (or is not) aligned with\nthese recommendations, and envision a future in which our research community\ntakes guidance from these theories. In particular, we analyze recent AI-HRI\n(through analysis of 2020 AI-HRI papers) and consider possible futures of\nAI-HRI (through a speculative ethics exercise). Both activities are guided\nthrough the lens of the Engineering for Social Justice (E4SJ) framework, which\ncenters contextual listening and enhancement of human capabilities. Our\nanalysis suggests that current AI-HRI research is not well aligned with the\nguiding principles of Engineering for Social Justice, and as such, does not\nobviously meet the needs of the communities we could be helping most. As such,\nwe suggest that motivating future work through the E4SJ framework could help to\nensure that we as researchers are developing technologies that will actually\nlead to a more equitable world.",
    "published_date": "2021-10-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.RO",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.03026v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.03001v1",
    "title": "Predictability and Fairness in Load Aggregation and Operations of Virtual Power Plants",
    "authors": [
      "Jakub Marecek",
      "Michal Roubalik",
      "Ramen Ghosh",
      "Robert N. Shorten",
      "Fabian R. Wirth"
    ],
    "author_ids": [],
    "abstract": "In power systems, one wishes to regulate the aggregate demand of an ensemble\nof distributed energy resources (DERs), such as controllable loads and battery\nenergy storage systems. We suggest a notion of predictability and fairness,\nwhich suggests that the long-term averages of prices or incentives offered\nshould be independent of the initial states of the operators of the DER, the\naggregator, and the power grid. We show that this notion cannot be guaranteed\nwith many traditional controllers used by the load aggregator, including the\nusual proportional-integral (PI) controller. We show that even considering the\nnon-linearity of the alternating-current model, this notion of predictability\nand fairness can be guaranteed for incrementally input-to-state stable (iISS)\ncontrollers, under mild assumptions.",
    "published_date": "2021-10-06T00:00:00",
    "year": 2021,
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.03001v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.02932v1",
    "title": "Machine Learning Practices Outside Big Tech: How Resource Constraints Challenge Responsible Development",
    "authors": [
      "Aspen Hopkins",
      "Serena Booth"
    ],
    "author_ids": [],
    "abstract": "Practitioners from diverse occupations and backgrounds are increasingly using\nmachine learning (ML) methods. Nonetheless, studies on ML Practitioners\ntypically draw populations from Big Tech and academia, as researchers have\neasier access to these communities. Through this selection bias, past research\noften excludes the broader, lesser-resourced ML community -- for example,\npractitioners working at startups, at non-tech companies, and in the public\nsector. These practitioners share many of the same ML development difficulties\nand ethical conundrums as their Big Tech counterparts; however, their\nexperiences are subject to additional under-studied challenges stemming from\ndeploying ML with limited resources, increased existential risk, and absent\naccess to in-house research teams. We contribute a qualitative analysis of 17\ninterviews with stakeholders from organizations which are less represented in\nprior studies. We uncover a number of tensions which are introduced or\nexacerbated by these organizations' resource constraints -- tensions between\nprivacy and ubiquity, resource management and performance optimization, and\naccess and monopolization. Increased academic focus on these practitioners can\nfacilitate a more holistic understanding of ML limitations, and so is useful\nfor prescribing a research agenda to facilitate responsible ML development for\nall.",
    "published_date": "2021-10-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.02932v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.02817v1",
    "title": "Combined Regularization and Discretization of Equilibrium Problems and Primal-Dual Gap Estimators",
    "authors": [
      "Steven-Marian Stengl"
    ],
    "author_ids": [],
    "abstract": "The present work aims at the application of finite element discretizations to\na class of equilibrium problems involving moving constraints. Therefore, a\nMoreau--Yosida based regularization technique, controlled by a parameter, is\ndiscussed and, using a generalized $\\Gamma$-convergence concept, a priori\nconvergence results are derived. The latter technique is applied to the\ndiscretization of the regularized problems and is used to prove the convergence\nto the orginal equilibrium problem, when both -- regularization and\ndiscretization -- are imposed simultaneously. In addition, a primal-dual gap\ntechnique is used for the derivation of error estimators suitable for adaptive\nmesh refinement. A strategy for balancing between a refinement of the mesh and\nan update of the regularization parameter is established, too. The theoretical\nfindings are illustrated for the obstacle problem as well as numerical\nexperiments are performed for two quasi-variational inequalities with\napplication to thermoforming and biomedicine, respectively.",
    "published_date": "2021-10-06T00:00:00",
    "year": 2021,
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.02817v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2110.03453v1",
    "title": "Recurrent Multigraph Integrator Network for Predicting the Evolution of Population-Driven Brain Connectivity Templates",
    "authors": [
      "Oytun Demirbilek",
      "Islem Rekik"
    ],
    "author_ids": [],
    "abstract": "Learning how to estimate a connectional brain template(CBT) from a population\nof brain multigraphs, where each graph (e.g., functional) quantifies a\nparticular relationship between pairs of brain regions of interest (ROIs),\nallows to pin down the unique connectivity patterns shared across individuals.\nSpecifically, a CBT is viewed as an integral representation of a set of highly\nheterogeneous graphs and ideally meeting the centeredness (i.e., minimum\ndistance to all graphs in the population) and discriminativeness (i.e.,\ndistinguishes the healthy from the disordered population) criteria. So far,\nexisting works have been limited to only integrating and fusing a population of\nbrain multigraphs acquired at a single timepoint. In this paper, we\nunprecedentedly tackle the question: Given a baseline multigraph population,\ncan we learn how to integrate and forecast its CBT representations at follow-up\ntimepoints? Addressing such question is of paramount in predicting common\nalternations across healthy and disordered populations. To fill this gap, we\npropose Recurrent Multigraph Integrator Network (ReMI-Net), the first graph\nrecurrent neural network which infers the baseline CBT of an input population\nt1 and predicts its longitudinal evolution over time (ti > t1). Our ReMI-Net is\ncomposed of recurrent neural blocks with graph convolutional layers using a\ncross-node message passing to first learn hidden-states embeddings of each CBT\nnode (i.e., brain region of interest) and then predict its evolution at the\nconsecutive timepoint. Moreover, we design a novel time-dependent loss to\nregularize the CBT evolution trajectory over time and further introduce a\ncyclic recursion and learnable normalization layer to generate well-centered\nCBTs from time-dependent hidden-state embeddings. Finally, we derive the CBT\nadjacency matrix from the learned hidden state graph representation.",
    "published_date": "2021-10-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CV",
      "q-bio.NC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.03453v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.11238v1",
    "title": "One Representative-Shot Learning Using a Population-Driven Template with Application to Brain Connectivity Classification and Evolution Prediction",
    "authors": [
      "Umut Guvercin",
      "Mohammed Amine Gharsallaoui",
      "Islem Rekik"
    ],
    "author_ids": [],
    "abstract": "Few-shot learning presents a challenging paradigm for training discriminative\nmodels on a few training samples representing the target classes to\ndiscriminate. However, classification methods based on deep learning are\nill-suited for such learning as they need large amounts of training data --let\nalone one-shot learning. Recently, graph neural networks (GNNs) have been\nintroduced to the field of network neuroscience, where the brain connectivity\nis encoded in a graph. However, with scarce neuroimaging datasets particularly\nfor rare diseases and low-resource clinical facilities, such data-devouring\narchitectures might fail in learning the target task. In this paper, we take a\nvery different approach in training GNNs, where we aim to learn with one sample\nand achieve the best performance --a formidable challenge to tackle.\nSpecifically, we present the first one-shot paradigm where a GNN is trained on\na single population-driven template --namely a connectional brain template\n(CBT). A CBT is a compact representation of a population of brain graphs\ncapturing the unique connectivity patterns shared across individuals. It is\nanalogous to brain image atlases for neuroimaging datasets. Using a\none-representative CBT as a training sample, we alleviate the training load of\nGNN models while boosting their performance across a variety of classification\nand regression tasks. We demonstrate that our method significantly outperformed\nbenchmark one-shot learning methods with downstream classification and\ntime-dependent brain graph data forecasting tasks while competing with the\ntrain-on-all conventional training strategy. Our source code can be found at\nhttps://github.com/basiralab/one-representative-shot-learning.",
    "published_date": "2021-10-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.NE",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.11238v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.02529v2",
    "title": "On the Importance of Firth Bias Reduction in Few-Shot Classification",
    "authors": [
      "Saba Ghaffari",
      "Ehsan Saleh",
      "David Forsyth",
      "Yu-xiong Wang"
    ],
    "author_ids": [],
    "abstract": "Learning accurate classifiers for novel categories from very few examples,\nknown as few-shot image classification, is a challenging task in statistical\nmachine learning and computer vision. The performance in few-shot\nclassification suffers from the bias in the estimation of classifier\nparameters; however, an effective underlying bias reduction technique that\ncould alleviate this issue in training few-shot classifiers has been\noverlooked. In this work, we demonstrate the effectiveness of Firth bias\nreduction in few-shot classification. Theoretically, Firth bias reduction\nremoves the $O(N^{-1})$ first order term from the small-sample bias of the\nMaximum Likelihood Estimator. Here we show that the general Firth bias\nreduction technique simplifies to encouraging uniform class assignment\nprobabilities for multinomial logistic classification, and almost has the same\neffect in cosine classifiers. We derive an easy-to-implement optimization\nobjective for Firth penalized multinomial logistic and cosine classifiers,\nwhich is equivalent to penalizing the cross-entropy loss with a KL-divergence\nbetween the uniform label distribution and the predictions. Then, we\nempirically evaluate that it is consistently effective across the board for\nfew-shot image classification, regardless of (1) the feature representations\nfrom different backbones, (2) the number of samples per class, and (3) the\nnumber of classes. Finally, we show the robustness of Firth bias reduction, in\nthe case of imbalanced data distribution. Our implementation is available at\nhttps://github.com/ehsansaleh/firth_bias_reduction",
    "published_date": "2021-10-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.02529v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.02424v4",
    "title": "Spectral Bias in Practice: The Role of Function Frequency in Generalization",
    "authors": [
      "Sara Fridovich-Keil",
      "Raphael Gontijo-Lopes",
      "Rebecca Roelofs"
    ],
    "author_ids": [],
    "abstract": "Despite their ability to represent highly expressive functions, deep learning\nmodels seem to find simple solutions that generalize surprisingly well.\nSpectral bias -- the tendency of neural networks to prioritize learning low\nfrequency functions -- is one possible explanation for this phenomenon, but so\nfar spectral bias has primarily been observed in theoretical models and\nsimplified experiments. In this work, we propose methodologies for measuring\nspectral bias in modern image classification networks on CIFAR-10 and ImageNet.\nWe find that these networks indeed exhibit spectral bias, and that\ninterventions that improve test accuracy on CIFAR-10 tend to produce learned\nfunctions that have higher frequencies overall but lower frequencies in the\nvicinity of examples from each class. This trend holds across variation in\ntraining time, model architecture, number of training examples, data\naugmentation, and self-distillation. We also explore the connections between\nfunction frequency and image frequency and find that spectral bias is sensitive\nto the low frequencies prevalent in natural images. On ImageNet, we find that\nlearned function frequency also varies with internal class diversity, with\nhigher frequencies on more diverse classes. Our work enables measuring and\nultimately influencing the spectral behavior of neural networks used for image\nclassification, and is a step towards understanding why deep models generalize\nwell.",
    "published_date": "2021-10-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.02424v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.02421v1",
    "title": "Explaining Off-Policy Actor-Critic From A Bias-Variance Perspective",
    "authors": [
      "Ting-Han Fan",
      "Peter J. Ramadge"
    ],
    "author_ids": [],
    "abstract": "Off-policy Actor-Critic algorithms have demonstrated phenomenal experimental\nperformance but still require better explanations. To this end, we show its\npolicy evaluation error on the distribution of transitions decomposes into: a\nBellman error, a bias from policy mismatch, and a variance term from sampling.\nBy comparing the magnitude of bias and variance, we explain the success of the\nEmphasizing Recent Experience sampling and 1/age weighted sampling. Both\nsampling strategies yield smaller bias and variance and are hence preferable to\nuniform sampling.",
    "published_date": "2021-10-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.02421v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.02370v1",
    "title": "Leveraging the Inductive Bias of Large Language Models for Abstract Textual Reasoning",
    "authors": [
      "Christopher Michael Rytting",
      "David Wingate"
    ],
    "author_ids": [],
    "abstract": "Large natural language models (such as GPT-3 or T5) demonstrate impressive\nabilities across a range of general NLP tasks. Here, we show that the knowledge\nembedded in such models provides a useful inductive bias, not just on\ntraditional NLP tasks, but also in the nontraditional task of training a\nsymbolic reasoning engine. We observe that these engines learn quickly and\ngeneralize in a natural way that reflects human intuition. For example,\ntraining such a system to model block-stacking might naturally generalize to\nstacking other types of objects because of structure in the real world that has\nbeen partially captured by the language describing it. We study several\nabstract textual reasoning tasks, such as object manipulation and navigation,\nand demonstrate multiple types of generalization to novel scenarios and the\nsymbols that comprise them. We also demonstrate the surprising utility of\n\\textit{compositional learning}, where a learner dedicated to mastering a\ncomplicated task gains an advantage by training on relevant simpler tasks\ninstead of jumping straight to the complicated task.",
    "published_date": "2021-10-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.02370v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.02168v2",
    "title": "A Community Roadmap for Scientific Workflows Research and Development",
    "authors": [
      "Rafael Ferreira da Silva",
      "Henri Casanova",
      "Kyle Chard",
      "Ilkay Altintas",
      "Rosa M Badia",
      "Bartosz Balis",
      "Tainã Coleman",
      "Frederik Coppens",
      "Frank Di Natale",
      "Bjoern Enders",
      "Thomas Fahringer",
      "Rosa Filgueira",
      "Grigori Fursin",
      "Daniel Garijo",
      "Carole Goble",
      "Dorran Howell",
      "Shantenu Jha",
      "Daniel S. Katz",
      "Daniel Laney",
      "Ulf Leser",
      "Maciej Malawski",
      "Kshitij Mehta",
      "Loïc Pottier",
      "Jonathan Ozik",
      "J. Luc Peterson",
      "Lavanya Ramakrishnan",
      "Stian Soiland-Reyes",
      "Douglas Thain",
      "Matthew Wolf"
    ],
    "author_ids": [],
    "abstract": "The landscape of workflow systems for scientific applications is notoriously\nconvoluted with hundreds of seemingly equivalent workflow systems, many\nisolated research claims, and a steep learning curve. To address some of these\nchallenges and lay the groundwork for transforming workflows research and\ndevelopment, the WorkflowsRI and ExaWorks projects partnered to bring the\ninternational workflows community together. This paper reports on discussions\nand findings from two virtual \"Workflows Community Summits\" (January and April,\n2021). The overarching goals of these workshops were to develop a view of the\nstate of the art, identify crucial research challenges in the workflows\ncommunity, articulate a vision for potential community efforts, and discuss\ntechnical approaches for realizing this vision. To this end, participants\nidentified six broad themes: FAIR computational workflows; AI workflows;\nexascale challenges; APIs, interoperability, reuse, and standards; training and\neducation; and building a workflows community. We summarize discussions and\nrecommendations for each of these themes.",
    "published_date": "2021-10-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.02168v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.02102v2",
    "title": "CARL: A Benchmark for Contextual and Adaptive Reinforcement Learning",
    "authors": [
      "Carolin Benjamins",
      "Theresa Eimer",
      "Frederik Schubert",
      "André Biedenkapp",
      "Bodo Rosenhahn",
      "Frank Hutter",
      "Marius Lindauer"
    ],
    "author_ids": [],
    "abstract": "While Reinforcement Learning has made great strides towards solving ever more\ncomplicated tasks, many algorithms are still brittle to even slight changes in\ntheir environment. This is a limiting factor for real-world applications of RL.\nAlthough the research community continuously aims at improving both robustness\nand generalization of RL algorithms, unfortunately it still lacks an\nopen-source set of well-defined benchmark problems based on a consistent\ntheoretical framework, which allows comparing different approaches in a fair,\nreliable and reproducibleway. To fill this gap, we propose CARL, a collection\nof well-known RL environments extended to contextual RL problems to study\ngeneralization. We show the urgent need of such benchmarks by demonstrating\nthat even simple toy environments become challenging for commonly used\napproaches if different contextual instances of this task have to be\nconsidered. Furthermore, CARL allows us to provide first evidence that\ndisentangling representation learning of the states from the policy learning\nwith the context facilitates better generalization. By providing variations of\ndiverse benchmarks from classic control, physical simulations, games and a\nreal-world application of RNA design, CARL will allow the community to derive\nmany more such insights on a solid empirical foundation.",
    "published_date": "2021-10-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.02102v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.01999v2",
    "title": "Federating for Learning Group Fair Models",
    "authors": [
      "Afroditi Papadaki",
      "Natalia Martinez",
      "Martin Bertran",
      "Guillermo Sapiro",
      "Miguel Rodrigues"
    ],
    "author_ids": [],
    "abstract": "Federated learning is an increasingly popular paradigm that enables a large\nnumber of entities to collaboratively learn better models. In this work, we\nstudy minmax group fairness in paradigms where different participating entities\nmay only have access to a subset of the population groups during the training\nphase. We formally analyze how this fairness objective differs from existing\nfederated learning fairness criteria that impose similar performance across\nparticipants instead of demographic groups. We provide an optimization\nalgorithm -- FedMinMax -- for solving the proposed problem that provably enjoys\nthe performance guarantees of centralized learning algorithms. We\nexperimentally compare the proposed approach against other methods in terms of\ngroup fairness in various federated learning setups.",
    "published_date": "2021-10-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.01999v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.01962v1",
    "title": "Gender Bias in Remote Pair Programming among Software Engineering Students: The twincode Exploratory Study",
    "authors": [
      "Amador Durán",
      "Pablo Fernández",
      "Beatriz Bernárdez",
      "Nathaniel Weinman",
      "Aslı Akalın",
      "Armando Fox"
    ],
    "author_ids": [],
    "abstract": "Context. Pair programming (PP) has been found to increase student interest in\nComputer Science, particularly so for women, and would therefore appear to be a\nway to help remedy their under-representation, which could be partially\nmotivated by gender stereotypes applied to software engineers, assuming that\nmen perform better than their women peers. If this same bias is present in pair\nprogramming, it could work against the goal of improving gender balance.\n  Objective. In a remote setting in which students cannot directly observe\ntheir peers, we aim to explore whether they behave differently when the\nperceived gender of their remote PP partners changes, searching for differences\nin (i) the perceived productivity compared to solo programming; (ii) the\npartner's perceived technical competency compared to their own; (iii) the\npartner's perceived skill level; (iv) the interaction behavior, such as the\nfrequency of source code additions, deletions, etc.; and (v) the type and\nrelative frequencies of dialog messages in a chat window.\n  Method. Using the twincode platform, several behaviors are automatically\nmeasured during the remote PP process, together with two questionnaires and a\nsemantic tagging of the pairs' chats. A series of experiments to identify the\neffect, if any, of possible gender bias shall be performed. The control group\nwill have no information about their partner's gender, whereas the treatment\ngroup will receive such information but will be selectively deceived about\ntheir partner's gender. For each response variable we will (i) compare control\nand experimental groups for the score distance between two in-pair tasks; then,\nusing the data from the experimental group only, we will (ii) compare scores\nusing the partner's perceived gender as a within-subjects variable; and (iii)\nanalyze the interaction between the partner's perceived gender and the\nsubject's gender.",
    "published_date": "2021-10-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.01962v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2110.01951v1",
    "title": "Multi-Objective Few-shot Learning for Fair Classification",
    "authors": [
      "Ishani Mondal",
      "Procheta Sen",
      "Debasis Ganguly"
    ],
    "author_ids": [],
    "abstract": "In this paper, we propose a general framework for mitigating the disparities\nof the predicted classes with respect to secondary attributes within the data\n(e.g., race, gender etc.). Our proposed method involves learning a\nmulti-objective function that in addition to learning the primary objective of\npredicting the primary class labels from the data, also employs a\nclustering-based heuristic to minimize the disparities of the class label\ndistribution with respect to the cluster memberships, with the assumption that\neach cluster should ideally map to a distinct combination of attribute values.\nExperiments demonstrate effective mitigation of cognitive biases on a benchmark\ndataset without the use of annotations of secondary attribute values (the\nzero-shot case) or with the use of a small number of attribute value\nannotations (the few-shot case).",
    "published_date": "2021-10-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.01951v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.01799v1",
    "title": "ContractNLI: A Dataset for Document-level Natural Language Inference for Contracts",
    "authors": [
      "Yuta Koreeda",
      "Christopher D. Manning"
    ],
    "author_ids": [],
    "abstract": "Reviewing contracts is a time-consuming procedure that incurs large expenses\nto companies and social inequality to those who cannot afford it. In this work,\nwe propose \"document-level natural language inference (NLI) for contracts\", a\nnovel, real-world application of NLI that addresses such problems. In this\ntask, a system is given a set of hypotheses (such as \"Some obligations of\nAgreement may survive termination.\") and a contract, and it is asked to\nclassify whether each hypothesis is \"entailed by\", \"contradicting to\" or \"not\nmentioned by\" (neutral to) the contract as well as identifying \"evidence\" for\nthe decision as spans in the contract. We annotated and release the largest\ncorpus to date consisting of 607 annotated contracts. We then show that\nexisting models fail badly on our task and introduce a strong baseline, which\n(1) models evidence identification as multi-label classification over spans\ninstead of trying to predict start and end tokens, and (2) employs more\nsophisticated context segmentation for dealing with long documents. We also\nshow that linguistic characteristics of contracts, such as negations by\nexceptions, are contributing to the difficulty of this task and that there is\nmuch room for improvement.",
    "published_date": "2021-10-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.01799v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.01689v1",
    "title": "Motion Control of Redundant Robots with Generalised Inequality Constraints",
    "authors": [
      "Amirhossein Kazemipour",
      "Maram Khatib",
      "Khaled Al Khudir",
      "Alessandro De Luca"
    ],
    "author_ids": [],
    "abstract": "We present an improved version of the Saturation in the Null Space (SNS)\nalgorithm for redundancy resolution at the velocity level. In addition to hard\nbounds on joint space motion, we consider also Cartesian box constraints that\ncannot be violated at any time. The modified algorithm combines all bounds into\na single augmented generalised vector and gives equal, highest priority to all\ninequality constraints. When needed, feasibility of the original task is\nenforced by the SNS task scaling procedure. Simulation results are reported for\na 6R planar robot.",
    "published_date": "2021-10-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.01689v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2110.01641v1",
    "title": "Investigating Fairness of Ocular Biometrics Among Young, Middle-Aged, and Older Adults",
    "authors": [
      "Anoop Krishnan",
      "Ali Almadan",
      "Ajita Rattani"
    ],
    "author_ids": [],
    "abstract": "A number of studies suggest bias of the face biometrics, i.e., face\nrecognition and soft-biometric estimation methods, across gender, race, and age\ngroups. There is a recent urge to investigate the bias of different biometric\nmodalities toward the deployment of fair and trustworthy biometric solutions.\nOcular biometrics has obtained increased attention from academia and industry\ndue to its high accuracy, security, privacy, and ease of use in mobile devices.\nA recent study in $2020$ also suggested the fairness of ocular-based user\nrecognition across males and females. This paper aims to evaluate the fairness\nof ocular biometrics in the visible spectrum among age groups; young, middle,\nand older adults. Thanks to the availability of the latest large-scale 2020\nUFPR ocular biometric dataset, with subjects acquired in the age range 18 - 79\nyears, to facilitate this study. Experimental results suggest the overall\nequivalent performance of ocular biometrics across gender and age groups in\nuser verification and gender classification. Performance difference for older\nadults at lower false match rate and young adults was noted at user\nverification and age classification, respectively. This could be attributed to\ninherent characteristics of the biometric data from these age groups impacting\nspecific applications, which suggest a need for advancement in sensor\ntechnology and software solutions.",
    "published_date": "2021-10-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.01641v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.01602v2",
    "title": "Clustering a Mixture of Gaussians with Unknown Covariance",
    "authors": [
      "Damek Davis",
      "Mateo Díaz",
      "Kaizheng Wang"
    ],
    "author_ids": [],
    "abstract": "We investigate a clustering problem with data from a mixture of Gaussians\nthat share a common but unknown, and potentially ill-conditioned, covariance\nmatrix. We start by considering Gaussian mixtures with two equally-sized\ncomponents and derive a Max-Cut integer program based on maximum likelihood\nestimation. We prove its solutions achieve the optimal misclassification rate\nwhen the number of samples grows linearly in the dimension, up to a logarithmic\nfactor. However, solving the Max-cut problem appears to be computationally\nintractable. To overcome this, we develop an efficient spectral algorithm that\nattains the optimal rate but requires a quadratic sample size. Although this\nsample complexity is worse than that of the Max-cut problem, we conjecture that\nno polynomial-time method can perform better. Furthermore, we gather numerical\nand theoretical evidence that supports the existence of a\nstatistical-computational gap. Finally, we generalize the Max-Cut program to a\n$k$-means program that handles multi-component mixtures with possibly unequal\nweights. It enjoys similar optimality guarantees for mixtures of distributions\nthat satisfy a transportation-cost inequality, encompassing Gaussian and\nstrongly log-concave distributions.",
    "published_date": "2021-10-04T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.IT",
      "cs.LG",
      "math.IT",
      "math.OC",
      "math.ST",
      "stat.TH",
      "62H30, 62H12, 62H05"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.01602v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.01577v1",
    "title": "An Empirical Investigation of Learning from Biased Toxicity Labels",
    "authors": [
      "Neel Nanda",
      "Jonathan Uesato",
      "Sven Gowal"
    ],
    "author_ids": [],
    "abstract": "Collecting annotations from human raters often results in a trade-off between\nthe quantity of labels one wishes to gather and the quality of these labels. As\nsuch, it is often only possible to gather a small amount of high-quality\nlabels. In this paper, we study how different training strategies can leverage\na small dataset of human-annotated labels and a large but noisy dataset of\nsynthetically generated labels (which exhibit bias against identity groups) for\npredicting toxicity of online comments. We evaluate the accuracy and fairness\nproperties of these approaches, and trade-offs between the two. While we find\nthat initial training on all of the data and fine-tuning on clean data produces\nmodels with the highest AUC, we find that no single strategy performs best\nacross all fairness metrics.",
    "published_date": "2021-10-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.01577v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.01506v1",
    "title": "Fairness and underspecification in acoustic scene classification: The case for disaggregated evaluations",
    "authors": [
      "Andreas Triantafyllopoulos",
      "Manuel Milling",
      "Konstantinos Drossos",
      "Björn W. Schuller"
    ],
    "author_ids": [],
    "abstract": "Underspecification and fairness in machine learning (ML) applications have\nrecently become two prominent issues in the ML community. Acoustic scene\nclassification (ASC) applications have so far remained unaffected by this\ndiscussion, but are now becoming increasingly used in real-world systems where\nfairness and reliability are critical aspects. In this work, we argue for the\nneed of a more holistic evaluation process for ASC models through disaggregated\nevaluations. This entails taking into account performance differences across\nseveral factors, such as city, location, and recording device. Although these\nfactors play a well-understood role in the performance of ASC models, most\nworks report single evaluation metrics taking into account all different strata\nof a particular dataset. We argue that metrics computed on specific\nsub-populations of the underlying data contain valuable information about the\nexpected real-world behaviour of proposed systems, and their reporting could\nimprove the transparency and trustability of such systems. We demonstrate the\neffectiveness of the proposed evaluation process in uncovering\nunderspecification and fairness problems exhibited by several standard ML\narchitectures when trained on two widely-used ASC datasets. Our evaluation\nshows that all examined architectures exhibit large biases across all factors\ntaken into consideration, and in particular with respect to the recording\nlocation. Additionally, different architectures exhibit different biases even\nthough they are trained with the same experimental configurations.",
    "published_date": "2021-10-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.01506v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.01438v2",
    "title": "Instrumental Variable-Driven Domain Generalization with Unobserved Confounders",
    "authors": [
      "Junkun Yuan",
      "Xu Ma",
      "Ruoxuan Xiong",
      "Mingming Gong",
      "Xiangyu Liu",
      "Fei Wu",
      "Lanfen Lin",
      "Kun Kuang"
    ],
    "author_ids": [],
    "abstract": "Domain generalization (DG) aims to learn from multiple source domains a model\nthat can generalize well on unseen target domains. Existing DG methods mainly\nlearn the representations with invariant marginal distribution of the input\nfeatures, however, the invariance of the conditional distribution of the labels\ngiven the input features is more essential for unknown domain prediction.\nMeanwhile, the existing of unobserved confounders which affect the input\nfeatures and labels simultaneously cause spurious correlation and hinder the\nlearning of the invariant relationship contained in the conditional\ndistribution. Interestingly, with a causal view on the data generating process,\nwe find that the input features of one domain are valid instrumental variables\nfor other domains. Inspired by this finding, we propose an instrumental\nvariable-driven DG method (IV-DG) by removing the bias of the unobserved\nconfounders with two-stage learning. In the first stage, it learns the\nconditional distribution of the input features of one domain given input\nfeatures of another domain. In the second stage, it estimates the relationship\nby predicting labels with the learned conditional distribution. Theoretical\nanalyses and simulation experiments show that it accurately captures the\ninvariant relationship. Extensive experiments on real-world datasets\ndemonstrate that IV-DG method yields state-of-the-art results.",
    "published_date": "2021-10-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.01438v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.01330v2",
    "title": "InfiniteForm: A synthetic, minimal bias dataset for fitness applications",
    "authors": [
      "Andrew Weitz",
      "Lina Colucci",
      "Sidney Primas",
      "Brinnae Bent"
    ],
    "author_ids": [],
    "abstract": "The growing popularity of remote fitness has increased the demand for highly\naccurate computer vision models that track human poses. However, the best\nmethods still fail in many real-world fitness scenarios, suggesting that there\nis a domain gap between current datasets and real-world fitness data. To enable\nthe field to address fitness-specific vision problems, we created InfiniteForm,\nan open-source synthetic dataset of 60k images with diverse fitness poses (15\ncategories), both single- and multi-person scenes, and realistic variation in\nlighting, camera angles, and occlusions. As a synthetic dataset, InfiniteForm\noffers minimal bias in body shape and skin tone, and provides pixel-perfect\nlabels for standard annotations like 2D keypoints, as well as those that are\ndifficult or impossible for humans to produce like depth and occlusion. In\naddition, we introduce a novel generative procedure for creating diverse\nsynthetic poses from predefined exercise categories. This generative process\ncan be extended to any application where pose diversity is needed to train\nrobust computer vision models.",
    "published_date": "2021-10-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.01330v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.01307v1",
    "title": "Collective eXplainable AI: Explaining Cooperative Strategies and Agent Contribution in Multiagent Reinforcement Learning with Shapley Values",
    "authors": [
      "Alexandre Heuillet",
      "Fabien Couthouis",
      "Natalia Díaz-Rodríguez"
    ],
    "author_ids": [],
    "abstract": "While Explainable Artificial Intelligence (XAI) is increasingly expanding\nmore areas of application, little has been applied to make deep Reinforcement\nLearning (RL) more comprehensible. As RL becomes ubiquitous and used in\ncritical and general public applications, it is essential to develop methods\nthat make it better understood and more interpretable. This study proposes a\nnovel approach to explain cooperative strategies in multiagent RL using Shapley\nvalues, a game theory concept used in XAI that successfully explains the\nrationale behind decisions taken by Machine Learning algorithms. Through\ntesting common assumptions of this technique in two cooperation-centered\nsocially challenging multi-agent environments environments, this article argues\nthat Shapley values are a pertinent way to evaluate the contribution of players\nin a cooperative multi-agent RL context. To palliate the high overhead of this\nmethod, Shapley values are approximated using Monte Carlo sampling.\nExperimental results on Multiagent Particle and Sequential Social Dilemmas show\nthat Shapley values succeed at estimating the contribution of each agent. These\nresults could have implications that go beyond games in economics, (e.g., for\nnon-discriminatory decision making, ethical and responsible AI-derived\ndecisions or policy making under fairness constraints). They also expose how\nShapley values only give general explanations about a model and cannot explain\na single run, episode nor justify precise actions taken by agents. Future work\nshould focus on addressing these critical aspects.",
    "published_date": "2021-10-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.GT",
      "cs.LG",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.01307v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.01167v2",
    "title": "Trustworthy AI: From Principles to Practices",
    "authors": [
      "Bo Li",
      "Peng Qi",
      "Bo Liu",
      "Shuai Di",
      "Jingen Liu",
      "Jiquan Pei",
      "Jinfeng Yi",
      "Bowen Zhou"
    ],
    "author_ids": [],
    "abstract": "The rapid development of Artificial Intelligence (AI) technology has enabled\nthe deployment of various systems based on it. However, many current AI systems\nare found vulnerable to imperceptible attacks, biased against underrepresented\ngroups, lacking in user privacy protection. These shortcomings degrade user\nexperience and erode people's trust in all AI systems. In this review, we\nprovide AI practitioners with a comprehensive guide for building trustworthy AI\nsystems. We first introduce the theoretical framework of important aspects of\nAI trustworthiness, including robustness, generalization, explainability,\ntransparency, reproducibility, fairness, privacy preservation, and\naccountability. To unify currently available but fragmented approaches toward\ntrustworthy AI, we organize them in a systematic approach that considers the\nentire lifecycle of AI systems, ranging from data acquisition to model\ndevelopment, to system development and deployment, finally to continuous\nmonitoring and governance. In this framework, we offer concrete action items\nfor practitioners and societal stakeholders (e.g., researchers, engineers, and\nregulators) to improve AI trustworthiness. Finally, we identify key\nopportunities and challenges for the future development of trustworthy AI\nsystems, where we identify the need for a paradigm shift toward comprehensively\ntrustworthy AI systems.",
    "published_date": "2021-10-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.01167v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.01152v2",
    "title": "Efficiency, Fairness, and Stability in Non-Commercial Peer-to-Peer Ridesharing",
    "authors": [
      "Hoon Oh",
      "Yanhan Tang",
      "Zong Zhang",
      "Alexandre Jacquillat",
      "Fei Fang"
    ],
    "author_ids": [],
    "abstract": "Unlike commercial ridesharing, non-commercial peer-to-peer (P2P) ridesharing\nhas been subject to limited research -- although it can promote viable\nsolutions in non-urban communities. This paper focuses on the core problem in\nP2P ridesharing: the matching of riders and drivers. We elevate users'\npreferences as a first-order concern and introduce novel notions of fairness\nand stability in P2P ridesharing. We propose algorithms for efficient matching\nwhile considering user-centric factors, including users' preferred departure\ntime, fairness, and stability. Results suggest that fair and stable solutions\ncan be obtained in reasonable computational times and can improve baseline\noutcomes based on system-wide efficiency exclusively.",
    "published_date": "2021-10-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "econ.GN",
      "q-fin.EC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.01152v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.01109v3",
    "title": "FairMask: Better Fairness via Model-based Rebalancing of Protected Attributes",
    "authors": [
      "Kewen Peng",
      "Joymallya Chakraborty",
      "Tim Menzies"
    ],
    "author_ids": [],
    "abstract": "Context: Machine learning software can generate models that inappropriately\ndiscriminate against specific protected social groups (e.g., groups based on\ngender, ethnicity, etc). Motivated by those results, software engineering\nresearchers have proposed many methods for mitigating those discriminatory\neffects. While those methods are effective in mitigating bias, few of them can\nprovide explanations on what is the root cause of bias.\n  Objective: We aim at better detection and mitigation of algorithmic\ndiscrimination in machine learning software problems.\n  Method: Here we propose xFAIR, a model-based extrapolation method, that is\ncapable of both mitigating bias and explaining the cause. In our xFAIR\napproach, protected attributes are represented by models learned from the other\nindependent variables (and these models offer extrapolations over the space\nbetween existing examples). We then use the extrapolation models to relabel\nprotected attributes later seen in testing data or deployment time. Our\napproach aims to offset the biased predictions of the classification model via\nrebalancing the distribution of protected attributes.\n  Results: The experiments of this paper show that, without compromising\n(original) model performance, xFAIR can achieve significantly better group and\nindividual fairness (as measured in different metrics) than benchmark methods.\nMoreover, when compared to another instance-based rebalancing method, our\nmodel-based approach shows faster runtime and thus better scalability.\n  Conclusion: Algorithmic decision bias can be removed via extrapolation that\nsmooths away outlier points. As evidence for this, our proposed xFAIR is not\nonly performance-wise better (measured by fairness and performance metrics)\nthan two state-of-the-art fairness algorithms.",
    "published_date": "2021-10-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.SE",
      "D.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.01109v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.09237v1",
    "title": "Designing Anonymity",
    "authors": [
      "Paula Helm"
    ],
    "author_ids": [],
    "abstract": "Creating anonymity means cutting connections. A common goal in this context\nis to prevent accountability. This prevention of accountability can be\nproblematic, for example, if it leads to delinquents remaining undetected.\nHowever, imputability can also provide protection against discrimination. In\nmedical, religious or legal matters, this is of fundamental importance. Thus,\nwhen individuals actively establish anonymity, they do so mostly because they\nwant to prevent certain information about them, that is, sensitive and/or\ncompromising information, from being associated with their identities. By\nremaining inaccessible as individuals with respect to certain information about\nthem, they can engage in forms of exchange that would otherwise be impossible\nfor them. Examples include practices of exchange in (self-organized) therapy\ngroups, acting out stigmatized sexual preferences, the role of anonymity in the\nperforming arts, or political resistance movements. Given the variety of\nexamples in which personal anonymity is important, it is not surprising that it\nis primarily these personal dimensions that are the focus of current debates\nabout the increasing precariousness of anonymity in the face of new technical\npossibilities of data mining and processing. In this paper, we nevertheless -\nor precisely because of this - want to focus on another aspect of anonymity\nthat has received much less attention so far. Namely, we assume that\nresearching and working with and about anonymity can open up new perspectives\non and for contemporary forms of knowledge production.",
    "published_date": "2021-10-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.09237v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2110.01094v1",
    "title": "Adversarial Examples Generation for Reducing Implicit Gender Bias in Pre-trained Models",
    "authors": [
      "Wenqian Ye",
      "Fei Xu",
      "Yaojia Huang",
      "Cassie Huang",
      "Ji A"
    ],
    "author_ids": [],
    "abstract": "Over the last few years, Contextualized Pre-trained Neural Language Models,\nsuch as BERT, GPT, have shown significant gains in various NLP tasks. To\nenhance the robustness of existing pre-trained models, one way is adversarial\nexamples generation and evaluation for conducting data augmentation or\nadversarial learning. In the meanwhile, gender bias embedded in the models\nseems to be a serious problem in practical applications. Many researches have\ncovered the gender bias produced by word-level information(e.g.\ngender-stereotypical occupations), while few researchers have investigated the\nsentence-level cases and implicit cases.\n  In this paper, we proposed a method to automatically generate implicit gender\nbias samples at sentence-level and a metric to measure gender bias. Samples\ngenerated by our method will be evaluated in terms of accuracy. The metric will\nbe used to guide the generation of examples from Pre-trained models. Therefore,\nthose examples could be used to impose attacks on Pre-trained Models. Finally,\nwe discussed the evaluation efficacy of our generated examples on reducing\ngender bias for future research.",
    "published_date": "2021-10-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.01094v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.01615v1",
    "title": "Geography of Science: Competitiveness and Inequality",
    "authors": [
      "Aurelio Patelli",
      "Lorenzo Napolitano",
      "Giulio Cimini",
      "Andrea Gabrielli"
    ],
    "author_ids": [],
    "abstract": "Using ideas and tools of complexity science we design a holistic measure of\n\\textit{Scientific Fitness}, encompassing the scientific knowledge,\ncapabilities and competitiveness of a research system. We characterize the\ntemporal dynamics of Scientific Fitness and R\\&D expenditures at the\ngeographical scale of nations, highlighting patterns of similar research\nsystems, and showing how developing nations (China in particular) are quickly\ncatching up the developed ones. Down-scaling the aggregation level of the\nanalysis, we find that even developed nations show a considerable level of\ninequality in the Scientific Fitness of their internal regions. Further, we\nassess comparatively how the competitiveness of each geographic region is\ndistributed over the spectrum of research sectors. Overall, the Scientific\nFitness represents the first high quality estimation of the scientific strength\nof nations and regions, opening new policy-making applications for better\nallocating resources, filling inequality gaps and ultimately promoting\ninnovation.",
    "published_date": "2021-10-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DL",
      "econ.GN",
      "physics.soc-ph",
      "q-fin.EC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.01615v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2110.00911v1",
    "title": "Enhancing Model Robustness and Fairness with Causality: A Regularization Approach",
    "authors": [
      "Zhao Wang",
      "Kai Shu",
      "Aron Culotta"
    ],
    "author_ids": [],
    "abstract": "Recent work has raised concerns on the risk of spurious correlations and\nunintended biases in statistical machine learning models that threaten model\nrobustness and fairness. In this paper, we propose a simple and intuitive\nregularization approach to integrate causal knowledge during model training and\nbuild a robust and fair model by emphasizing causal features and de-emphasizing\nspurious features. Specifically, we first manually identify causal and spurious\nfeatures with principles inspired from the counterfactual framework of causal\ninference. Then, we propose a regularization approach to penalize causal and\nspurious features separately. By adjusting the strength of the penalty for each\ntype of feature, we build a predictive model that relies more on causal\nfeatures and less on non-causal features. We conduct experiments to evaluate\nmodel robustness and fairness on three datasets with multiple metrics.\nEmpirical results show that the new models built with causal awareness\nsignificantly improve model robustness with respect to counterfactual texts and\nmodel fairness with respect to sensitive attributes.",
    "published_date": "2021-10-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.00911v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.00884v2",
    "title": "A Lagged Particle Filter for Stable Filtering of certain High-Dimensional State-Space Models",
    "authors": [
      "Hamza Ruzayqat",
      "Aimad Er-Raiy",
      "Alexandros Beskos",
      "Dan Crisan",
      "Ajay Jasra",
      "Nikolas Kantas"
    ],
    "author_ids": [],
    "abstract": "We consider the problem of high-dimensional filtering of state-space models\n(SSMs) at discrete times. This problem is particularly challenging as\nanalytical solutions are typically not available and many numerical\napproximation methods can have a cost that scales exponentially with the\ndimension of the hidden state. Inspired by lag-approximation methods for the\nsmoothing problem, we introduce a lagged approximation of the smoothing\ndistribution that is necessarily biased. For certain classes of SSMs,\nparticularly those that forget the initial condition exponentially fast in\ntime, the bias of our approximation is shown to be uniformly controlled in the\ndimension and exponentially small in time. We develop a sequential Monte Carlo\n(SMC) method to recursively estimate expectations with respect to our biased\nfiltering distributions. Moreover, we prove for a class of class of SSMs that\ncan contain dependencies amongst coordinates that as the dimension\n$d\\rightarrow\\infty$ the cost to achieve a stable mean square error in\nestimation, for classes of expectations, is of $\\mathcal{O}(Nd^2)$ per-unit\ntime, where $N$ is the number of simulated samples in the SMC algorithm. Our\nmethodology is implemented on several challenging high-dimensional examples\nincluding the conservative shallow-water model.",
    "published_date": "2021-10-02T00:00:00",
    "year": 2021,
    "categories": [
      "stat.CO",
      "cs.NA",
      "math.NA",
      "math.PR",
      "62M20, 60G35, 60J20, 60J10, 94A12, 93E11"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.00884v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2110.00860v1",
    "title": "Implicit and Explicit Attention for Zero-Shot Learning",
    "authors": [
      "Faisal Alamri",
      "Anjan Dutta"
    ],
    "author_ids": [],
    "abstract": "Most of the existing Zero-Shot Learning (ZSL) methods focus on learning a\ncompatibility function between the image representation and class attributes.\nFew others concentrate on learning image representation combining local and\nglobal features. However, the existing approaches still fail to address the\nbias issue towards the seen classes. In this paper, we propose implicit and\nexplicit attention mechanisms to address the existing bias problem in ZSL\nmodels. We formulate the implicit attention mechanism with a self-supervised\nimage angle rotation task, which focuses on specific image features aiding to\nsolve the task. The explicit attention mechanism is composed with the\nconsideration of a multi-headed self-attention mechanism via Vision Transformer\nmodel, which learns to map image features to semantic space during the training\nstage. We conduct comprehensive experiments on three popular benchmarks: AWA2,\nCUB and SUN. The performance of our proposed attention mechanisms has proved\nits effectiveness, and has achieved the state-of-the-art harmonic mean on all\nthe three datasets.",
    "published_date": "2021-10-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.00860v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.00857v3",
    "title": "FairFed: Enabling Group Fairness in Federated Learning",
    "authors": [
      "Yahya H. Ezzeldin",
      "Shen Yan",
      "Chaoyang He",
      "Emilio Ferrara",
      "Salman Avestimehr"
    ],
    "author_ids": [],
    "abstract": "Training ML models which are fair across different demographic groups is of\ncritical importance due to the increased integration of ML in crucial\ndecision-making scenarios such as healthcare and recruitment. Federated\nlearning has been viewed as a promising solution for collaboratively training\nmachine learning models among multiple parties while maintaining the privacy of\ntheir local data. However, federated learning also poses new challenges in\nmitigating the potential bias against certain populations (e.g., demographic\ngroups), as this typically requires centralized access to the sensitive\ninformation (e.g., race, gender) of each datapoint. Motivated by the importance\nand challenges of group fairness in federated learning, in this work, we\npropose FairFed, a novel algorithm for fairness-aware aggregation to enhance\ngroup fairness in federated learning. Our proposed approach is server-side and\nagnostic to the applied local debiasing thus allowing for flexible use of\ndifferent local debiasing methods across clients. We evaluate FairFed\nempirically versus common baselines for fair ML and federated learning, and\ndemonstrate that it provides fairer models particularly under highly\nheterogeneous data distributions across clients. We also demonstrate the\nbenefits of FairFed in scenarios involving naturally distributed real-life data\ncollected from different geographical locations or departments within an\norganization.",
    "published_date": "2021-10-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.00857v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.00828v1",
    "title": "Artificial intelligence for Sustainable Energy: A Contextual Topic Modeling and Content Analysis",
    "authors": [
      "Tahereh Saheb",
      "Mohammad Dehghani"
    ],
    "author_ids": [],
    "abstract": "Parallel to the rising debates over sustainable energy and artificial\nintelligence solutions, the world is currently discussing the ethics of\nartificial intelligence and its possible negative effects on society and the\nenvironment. In these arguments, sustainable AI is proposed, which aims at\nadvancing the pathway toward sustainability, such as sustainable energy. In\nthis paper, we offered a novel contextual topic modeling combining LDA, BERT,\nand Clustering. We then combined these computational analyses with content\nanalysis of related scientific publications to identify the main scholarly\ntopics, sub-themes, and cross-topic themes within scientific research on\nsustainable AI in energy. Our research identified eight dominant topics\nincluding sustainable buildings, AI-based DSSs for urban water management,\nclimate artificial intelligence, Agriculture 4, the convergence of AI with IoT,\nAI-based evaluation of renewable technologies, smart campus and engineering\neducation, and AI-based optimization. We then recommended 14 potential future\nresearch strands based on the observed theoretical gaps. Theoretically, this\nanalysis contributes to the existing literature on sustainable AI and\nsustainable energy, and practically, it intends to act as a general guide for\nenergy engineers and scientists, AI scientists, and social scientists to widen\ntheir knowledge of sustainability in AI and energy convergence research.",
    "published_date": "2021-10-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.00828v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.00813v1",
    "title": "Consider the Alternatives: Navigating Fairness-Accuracy Tradeoffs via Disqualification",
    "authors": [
      "Guy N. Rothblum",
      "Gal Yona"
    ],
    "author_ids": [],
    "abstract": "In many machine learning settings there is an inherent tension between\nfairness and accuracy desiderata. How should one proceed in light of such\ntrade-offs? In this work we introduce and study $\\gamma$-disqualification, a\nnew framework for reasoning about fairness-accuracy tradeoffs w.r.t a benchmark\nclass $H$ in the context of supervised learning. Our requirement stipulates\nthat a classifier should be disqualified if it is possible to improve its\nfairness by switching to another classifier from $H$ without paying \"too much\"\nin accuracy. The notion of \"too much\" is quantified via a parameter $\\gamma$\nthat serves as a vehicle for specifying acceptable tradeoffs between accuracy\nand fairness, in a way that is independent from the specific metrics used to\nquantify fairness and accuracy in a given task. Towards this objective, we\nestablish principled translations between units of accuracy and units of\n(un)fairness for different accuracy measures. We show $\\gamma$-disqualification\ncan be used to easily compare different learning strategies in terms of how\nthey trade-off fairness and accuracy, and we give an efficient reduction from\nthe problem of finding the optimal classifier that satisfies our requirement to\nthe problem of approximating the Pareto frontier of $H$.",
    "published_date": "2021-10-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.00813v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.00726v3",
    "title": "Domain-Specific Bias Filtering for Single Labeled Domain Generalization",
    "authors": [
      "Junkun Yuan",
      "Xu Ma",
      "Defang Chen",
      "Kun Kuang",
      "Fei Wu",
      "Lanfen Lin"
    ],
    "author_ids": [],
    "abstract": "Conventional Domain Generalization (CDG) utilizes multiple labeled source\ndatasets to train a generalizable model for unseen target domains. However, due\nto expensive annotation costs, the requirements of labeling all the source data\nare hard to be met in real-world applications. In this paper, we investigate a\nSingle Labeled Domain Generalization (SLDG) task with only one source domain\nbeing labeled, which is more practical and challenging than the CDG task. A\nmajor obstacle in the SLDG task is the discriminability-generalization bias:\nthe discriminative information in the labeled source dataset may contain\ndomain-specific bias, constraining the generalization of the trained model. To\ntackle this challenging task, we propose a novel framework called\nDomain-Specific Bias Filtering (DSBF), which initializes a discriminative model\nwith the labeled source data and then filters out its domain-specific bias with\nthe unlabeled source data for generalization improvement. We divide the\nfiltering process into (1) feature extractor debiasing via k-means\nclustering-based semantic feature re-extraction and (2) classifier\nrectification through attention-guided semantic feature projection. DSBF\nunifies the exploration of the labeled and the unlabeled source data to enhance\nthe discriminability and generalization of the trained model, resulting in a\nhighly generalizable model. We further provide theoretical analysis to verify\nthe proposed domain-specific bias filtering process. Extensive experiments on\nmultiple datasets show the superior performance of DSBF in tackling both the\nchallenging SLDG task and the CDG task.",
    "published_date": "2021-10-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.00726v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.00672v1",
    "title": "Low Frequency Names Exhibit Bias and Overfitting in Contextualizing Language Models",
    "authors": [
      "Robert Wolfe",
      "Aylin Caliskan"
    ],
    "author_ids": [],
    "abstract": "We use a dataset of U.S. first names with labels based on predominant gender\nand racial group to examine the effect of training corpus frequency on\ntokenization, contextualization, similarity to initial representation, and bias\nin BERT, GPT-2, T5, and XLNet. We show that predominantly female and non-white\nnames are less frequent in the training corpora of these four language models.\nWe find that infrequent names are more self-similar across contexts, with\nSpearman's r between frequency and self-similarity as low as -.763. Infrequent\nnames are also less similar to initial representation, with Spearman's r\nbetween frequency and linear centered kernel alignment (CKA) similarity to\ninitial representation as high as .702. Moreover, we find Spearman's r between\nracial bias and name frequency in BERT of .492, indicating that lower-frequency\nminority group names are more associated with unpleasantness. Representations\nof infrequent names undergo more processing, but are more self-similar,\nindicating that models rely on less context-informed representations of\nuncommon and minority names which are overfit to a lower number of observed\ncontexts.",
    "published_date": "2021-10-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.00672v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.00603v2",
    "title": "Algorithm Fairness in AI for Medicine and Healthcare",
    "authors": [
      "Richard J. Chen",
      "Tiffany Y. Chen",
      "Jana Lipkova",
      "Judy J. Wang",
      "Drew F. K. Williamson",
      "Ming Y. Lu",
      "Sharifa Sahai",
      "Faisal Mahmood"
    ],
    "author_ids": [],
    "abstract": "In the current development and deployment of many artificial intelligence\n(AI) systems in healthcare, algorithm fairness is a challenging problem in\ndelivering equitable care. Recent evaluation of AI models stratified across\nrace sub-populations have revealed inequalities in how patients are diagnosed,\ngiven treatments, and billed for healthcare costs. In this perspective article,\nwe summarize the intersectional field of fairness in machine learning through\nthe context of current issues in healthcare, outline how algorithmic biases\n(e.g. - image acquisition, genetic variation, intra-observer labeling\nvariability) arise in current clinical workflows and their resulting healthcare\ndisparities. Lastly, we also review emerging technology for mitigating bias via\nfederated learning, disentanglement, and model explainability, and their role\nin AI-SaMD development.",
    "published_date": "2021-10-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.00603v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.00577v4",
    "title": "Reconstruction for Powerful Graph Representations",
    "authors": [
      "Leonardo Cotta",
      "Christopher Morris",
      "Bruno Ribeiro"
    ],
    "author_ids": [],
    "abstract": "Graph neural networks (GNNs) have limited expressive power, failing to\nrepresent many graph classes correctly. While more expressive graph\nrepresentation learning (GRL) alternatives can distinguish some of these\nclasses, they are significantly harder to implement, may not scale well, and\nhave not been shown to outperform well-tuned GNNs in real-world tasks. Thus,\ndevising simple, scalable, and expressive GRL architectures that also achieve\nreal-world improvements remains an open challenge. In this work, we show the\nextent to which graph reconstruction -- reconstructing a graph from its\nsubgraphs -- can mitigate the theoretical and practical problems currently\nfaced by GRL architectures. First, we leverage graph reconstruction to build\ntwo new classes of expressive graph representations. Secondly, we show how\ngraph reconstruction boosts the expressive power of any GNN architecture while\nbeing a (provably) powerful inductive bias for invariances to vertex removals.\nEmpirically, we show how reconstruction can boost GNN's expressive power --\nwhile maintaining its invariance to permutations of the vertices -- by solving\nseven graph property tasks not solvable by the original GNN. Further, we\ndemonstrate how it boosts state-of-the-art GNN's performance across nine\nreal-world benchmark datasets.",
    "published_date": "2021-10-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.00577v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.00538v1",
    "title": "Evaluating the fairness of fine-tuning strategies in self-supervised learning",
    "authors": [
      "Jason Ramapuram",
      "Dan Busbridge",
      "Russ Webb"
    ],
    "author_ids": [],
    "abstract": "In this work we examine how fine-tuning impacts the fairness of contrastive\nSelf-Supervised Learning (SSL) models. Our findings indicate that Batch\nNormalization (BN) statistics play a crucial role, and that updating only the\nBN statistics of a pre-trained SSL backbone improves its downstream fairness\n(36% worst subgroup, 25% mean subgroup gap). This procedure is competitive with\nsupervised learning, while taking 4.4x less time to train and requiring only\n0.35% as many parameters to be updated. Finally, inspired by recent work in\nsupervised learning, we find that updating BN statistics and training residual\nskip connections (12.3% of the parameters) achieves parity with a fully\nfine-tuned model, while taking 1.33x less time to train.",
    "published_date": "2021-10-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.00538v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.00530v3",
    "title": "A survey on datasets for fairness-aware machine learning",
    "authors": [
      "Tai Le Quy",
      "Arjun Roy",
      "Vasileios Iosifidis",
      "Wenbin Zhang",
      "Eirini Ntoutsi"
    ],
    "author_ids": [],
    "abstract": "As decision-making increasingly relies on Machine Learning (ML) and (big)\ndata, the issue of fairness in data-driven Artificial Intelligence (AI) systems\nis receiving increasing attention from both research and industry. A large\nvariety of fairness-aware machine learning solutions have been proposed which\ninvolve fairness-related interventions in the data, learning algorithms and/or\nmodel outputs. However, a vital part of proposing new approaches is evaluating\nthem empirically on benchmark datasets that represent realistic and diverse\nsettings. Therefore, in this paper, we overview real-world datasets used for\nfairness-aware machine learning. We focus on tabular data as the most common\ndata representation for fairness-aware machine learning. We start our analysis\nby identifying relationships between the different attributes, particularly\nw.r.t. protected attributes and class attribute, using a Bayesian network. For\na deeper understanding of bias in the datasets, we investigate the interesting\nrelationships using exploratory analysis.",
    "published_date": "2021-10-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.00530v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.00521v1",
    "title": "Unpacking the Interdependent Systems of Discrimination: Ableist Bias in NLP Systems through an Intersectional Lens",
    "authors": [
      "Saad Hassan",
      "Matt Huenerfauth",
      "Cecilia Ovesdotter Alm"
    ],
    "author_ids": [],
    "abstract": "Much of the world's population experiences some form of disability during\ntheir lifetime. Caution must be exercised while designing natural language\nprocessing (NLP) systems to prevent systems from inadvertently perpetuating\nableist bias against people with disabilities, i.e., prejudice that favors\nthose with typical abilities. We report on various analyses based on word\npredictions of a large-scale BERT language model. Statistically significant\nresults demonstrate that people with disabilities can be disadvantaged.\nFindings also explore overlapping forms of discrimination related to\ninterconnected gender and race identities.",
    "published_date": "2021-10-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.00521v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.00497v1",
    "title": "DiVRsify: Break the Cycle and Develop VR for Everyone",
    "authors": [
      "Tabitha C. Peck",
      "Kyla McMullen",
      "John Quarles"
    ],
    "author_ids": [],
    "abstract": "Virtual reality technology is biased. It excludes approximately 95% the\nworld's population by being primarily designed for male, western, educated,\nindustrial, rich, and democratic populations. This bias may be due to the lack\nof diversity in virtual reality researchers, research participants, developers,\nand end users, fueling a noninclusive research, development, and usability\ncycle. The objective of this paper is to highlight the minimal virtual reality\nresearch involving understudied populations with respect to dimensions of\ndiversity, such as gender, race, culture, ethnicity, age, disability, and\nneurodivergence. Specifically, we highlight numerous differences in virtual\nreality usability between underrepresented groups compared to commonly studied\npopulations. These differences illustrate the lack of generalizability of prior\nvirtual reality research. Lastly, we present a call to action with the aim\nthat, over time, will break the cycle and enable virtual reality for everyone.",
    "published_date": "2021-10-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.00497v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2110.00394v1",
    "title": "Personalized Retrogress-Resilient Framework for Real-World Medical Federated Learning",
    "authors": [
      "Zhen Chen",
      "Meilu Zhu",
      "Chen Yang",
      "Yixuan Yuan"
    ],
    "author_ids": [],
    "abstract": "Nowadays, deep learning methods with large-scale datasets can produce\nclinically useful models for computer-aided diagnosis. However, the privacy and\nethical concerns are increasingly critical, which make it difficult to collect\nlarge quantities of data from multiple institutions. Federated Learning (FL)\nprovides a promising decentralized solution to train model collaboratively by\nexchanging client models instead of private data. However, the server\naggregation of existing FL methods is observed to degrade the model performance\nin real-world medical FL setting, which is termed as retrogress. To address\nthis problem, we propose a personalized retrogress-resilient framework to\nproduce a superior personalized model for each client. Specifically, we devise\na Progressive Fourier Aggregation (PFA) at the server to achieve more stable\nand effective global knowledge gathering by integrating client models from\nlow-frequency to high-frequency gradually. Moreover, with an introduced deputy\nmodel to receive the aggregated server model, we design a Deputy-Enhanced\nTransfer (DET) strategy at the client and conduct three steps of\nRecover-Exchange-Sublimate to ameliorate the personalized local model by\ntransferring the global knowledge smoothly. Extensive experiments on real-world\ndermoscopic FL dataset prove that our personalized retrogress-resilient\nframework outperforms state-of-the-art FL methods, as well as the\ngeneralization on an out-of-distribution cohort. The code and dataset are\navailable at https://github.com/CityU-AIM-Group/PRR-FL.",
    "published_date": "2021-10-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.DC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.00394v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.00380v1",
    "title": "GAN-based Reactive Motion Synthesis with Class-aware Discriminators for Human-human Interaction",
    "authors": [
      "Qianhui Men",
      "Hubert P. H. Shum",
      "Edmond S. L. Ho",
      "Howard Leung"
    ],
    "author_ids": [],
    "abstract": "Creating realistic characters that can react to the users' or another\ncharacter's movement can benefit computer graphics, games and virtual reality\nhugely. However, synthesizing such reactive motions in human-human interactions\nis a challenging task due to the many different ways two humans can interact.\nWhile there are a number of successful researches in adapting the generative\nadversarial network (GAN) in synthesizing single human actions, there are very\nfew on modelling human-human interactions. In this paper, we propose a\nsemi-supervised GAN system that synthesizes the reactive motion of a character\ngiven the active motion from another character. Our key insights are two-fold.\nFirst, to effectively encode the complicated spatial-temporal information of a\nhuman motion, we empower the generator with a part-based long short-term memory\n(LSTM) module, such that the temporal movement of different limbs can be\neffectively modelled. We further include an attention module such that the\ntemporal significance of the interaction can be learned, which enhances the\ntemporal alignment of the active-reactive motion pair. Second, as the reactive\nmotion of different types of interactions can be significantly different, we\nintroduce a discriminator that not only tells if the generated movement is\nrealistic or not, but also tells the class label of the interaction. This\nallows the use of such labels in supervising the training of the generator. We\nexperiment with the SBU and the HHOI datasets. The high quality of the\nsynthetic motion demonstrates the effective design of our generator, and the\ndiscriminability of the synthesis also demonstrates the strength of our\ndiscriminator.",
    "published_date": "2021-10-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.00380v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.00138v1",
    "title": "Deep Connection: Making Virtual Reality Artworks with Medical Scan Data",
    "authors": [
      "Marilene Oliver",
      "Gary James Joynes",
      "Kumar Punithakumar",
      "Peter Seres"
    ],
    "author_ids": [],
    "abstract": "Deep Connection is an installation and virtual reality artwork made using\nfull body 3D and 4D magnetic resonance scan datasets. When the user enters Deep\nConnection, they see a scanned body lying prone in mid-air. The user can walk\naround the body and inspect it. The user can dive inside and see its inner\nworkings, its lungs, spine, brain. The user can take hold of the figure's\noutstretched hand: holding the hand triggers the 4D dataset, making the heart\nbeat and the lungs breathe. When the user lets go of the hand, the heart stops\nbeating and the lungs stop breathing. Deep Connection creates a scenario where\nan embodied human becomes the companion for a virtual body. This paper maps the\nconceptual and theoretical framework for Deep Connection such as virtual\nintimacy and digital mediated companionship. It also reflects on working with\nscanned bodies more generally in virtual reality by discussing transparency,\nthe cyberbody versus the data body, as well as data privacy and data ethics.\nThe paper also explains the technical and procedural aspects of the Deep\nConnection project with respect to acquiring scan data for the creation of\nvirtual reality artworks.",
    "published_date": "2021-10-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.MM",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.00138v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2110.00118v1",
    "title": "Unbiased Experiments in Congested Networks",
    "authors": [
      "Bruce Spang",
      "Veronica Hannan",
      "Shravya Kunamalla",
      "Te-Yuan Huang",
      "Nick McKeown",
      "Ramesh Johari"
    ],
    "author_ids": [],
    "abstract": "When developing a new networking algorithm, it is established practice to run\na randomized experiment, or A/B test, to evaluate its performance. In an A/B\ntest, traffic is randomly allocated between a treatment group, which uses the\nnew algorithm, and a control group, which uses the existing algorithm. However,\nbecause networks are congested, both treatment and control traffic compete\nagainst each other for resources in a way that biases the outcome of these\ntests. This bias can have a surprisingly large effect; for example, in lab A/B\ntests with two widely used congestion control algorithms, the treatment\nappeared to deliver 150% higher throughput when used by a few flows, and 75%\nlower throughput when used by most flows-despite the fact that the two\nalgorithms have identical throughput when used by all traffic.\n  Beyond the lab, we show that A/B tests can also be biased at scale. In an\nexperiment run in cooperation with Netflix, estimates from A/B tests mistake\nthe direction of change of some metrics, miss changes in other metrics, and\noverestimate the size of effects. We propose alternative experiment designs,\npreviously used in online platforms, to more accurately evaluate new algorithms\nand allow experimenters to better understand the impact of congestion on their\ntests.",
    "published_date": "2021-09-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.NI",
      "cs.MM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.00118v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2110.00116v2",
    "title": "#ContextMatters: Advantages and Limitations of Using Machine Learning to Support Women in Politics",
    "authors": [
      "Jacqueline Comer",
      "Sam Work",
      "Kory W Mathewson",
      "Lana Cuthbertson",
      "Kasey Machin"
    ],
    "author_ids": [],
    "abstract": "The United Nations identified gender equality as a Sustainable Development\nGoal in 2015, recognizing the underrepresentation of women in politics as a\nspecific barrier to achieving gender equality. Political systems around the\nworld experience gender inequality across all levels of elected government as\nfewer women run for office than men. This is due in part to online abuse,\nparticularly on social media platforms like Twitter, where women seeking or in\npower tend to be targeted with more toxic maltreatment than their male\ncounterparts. In this paper, we present reflections on ParityBOT - the first\nnatural language processing-based intervention designed to affect online\ndiscourse for women in politics for the better, at scale. Deployed across\nelections in Canada, the United States and New Zealand, ParityBOT was used to\nanalyse and classify more than 12 million tweets directed at women candidates\nand counter toxic tweets with supportive ones. From these elections we present\nthree case studies highlighting the current limitations of, and future research\nand application opportunities for, using a natural language processing-based\nsystem to detect online toxicity, specifically with regards to contextually\nimportant microaggressions. We examine the rate of false negatives, where\nParityBOT failed to pick up on insults directed at specific high profile women,\nwhich would be obvious to human users. We examine the unaddressed harms of\nmicroaggressions and the potential of yet unseen damage they cause for women in\nthese communities, and for progress towards gender equality overall, in light\nof these technological blindspots. This work concludes with a discussion on the\nbenefits of partnerships between nonprofit social groups and technology experts\nto develop responsible, socially impactful approaches to addressing online\nhate.",
    "published_date": "2021-09-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.00116v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.00072v2",
    "title": "Inequality and Inequity in Network-based Ranking and Recommendation Algorithms",
    "authors": [
      "Lisette Espín-Noboa",
      "Claudia Wagner",
      "Markus Strohmaier",
      "Fariba Karimi"
    ],
    "author_ids": [],
    "abstract": "Though algorithms promise many benefits including efficiency, objectivity and\naccuracy, they may also introduce or amplify biases. Here we study two\nwell-known algorithms, namely PageRank and Who-to-Follow (WTF), and show to\nwhat extent their ranks produce inequality and inequity when applied to\ndirected social networks. To this end, we propose a directed network model with\npreferential attachment and homophily (DPAH) and demonstrate the influence of\nnetwork structure on the rank distributions of these algorithms. Our main\nfindings suggest that (i) inequality is positively correlated with inequity,\n(ii) inequality is driven by the interplay between preferential attachment,\nhomophily, node activity and edge density, and (iii) inequity is driven by the\ninterplay between homophily and minority size. In particular, these two\nalgorithms reduce, replicate and amplify the representation of minorities in\ntop ranks when majorities are homophilic, neutral and heterophilic,\nrespectively. Moreover, when this representation is reduced, minorities may\nimprove their visibility in the rank by connecting strategically in the\nnetwork. For instance, by increasing their out-degree or homophily when\nmajorities are also homophilic. These findings shed light on the social and\nalgorithmic mechanisms that hinder equality and equity in network-based ranking\nand recommendation algorithms.",
    "published_date": "2021-09-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI",
      "cs.DS",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.00072v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2109.15294v1",
    "title": "Targeted Ads and/as Racial Discrimination: Exploring Trends in New York City Ads for College Scholarships",
    "authors": [
      "Ho-Chun Herbert Chang",
      "Matt Bui",
      "Charlton McIlwain"
    ],
    "author_ids": [],
    "abstract": "This paper uses and recycles data from a third-party digital marketing firm,\nto explore how targeted ads contribute to larger systems of racial\ndiscrimination. Focusing on a case study of targeted ads for educational\nsearches in New York City, it discusses data visualizations and mappings of\ntrends in the advertisements' targeted populations alongside U.S census data\ncorresponding to these target zipcodes. We summarize and reflect on the results\nto consider how internet platforms systemically and differentially target\nadvertising messages to users based on race; the tangible harms and risks that\nresult from an internet traffic system designed to discriminate; and finally,\nnovel approaches and frameworks for further auditing systems amid opaque,\nblack-boxed processes forestalling transparency and accountability.",
    "published_date": "2021-09-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.15294v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2110.02802v4",
    "title": "Self-conditioning pre-trained language models",
    "authors": [
      "Xavier Suau",
      "Luca Zappella",
      "Nicholas Apostoloff"
    ],
    "author_ids": [],
    "abstract": "In this paper we aim to investigate the mechanisms that guide text generation\nwith pre-trained Transformer-based Language Models (TLMs). Grounded on the\nProduct of Experts formulation by Hinton (1999), we describe a generative\nmechanism that exploits expert units which naturally exist in TLMs. Such units\nare responsible for detecting concepts in the input and conditioning text\ngeneration on such concepts. We describe how to identify expert units and how\nto activate them during inference in order to induce any desired concept in the\ngenerated output. We find that the activation of a surprisingly small amount of\nunits is sufficient to steer text generation (as little as 3 units in a model\nwith 345M parameters). While the objective of this work is to learn more about\nhow TLMs work, we show that our method is effective for conditioning without\nfine-tuning or using extra parameters, even on fine-grained homograph concepts.\nAdditionally, we show that our method can be used to correct gender bias\npresent in the output of TLMs and achieves gender parity for all evaluated\ncontexts. We compare our method with FUDGE and PPLM-BoW, and show that our\napproach is able to achieve gender parity at a lower perplexity. The proposed\nmethod is accessible to a wide audience thanks to its simplicity and minimal\ncompute needs. The findings in this paper are a step forward in understanding\nthe generative mechanisms of TLMs.",
    "published_date": "2021-09-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.02802v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.14908v1",
    "title": "Proceedings 14th Interaction and Concurrency Experience",
    "authors": [
      "Julien Lange",
      "Anastasia Mavridou",
      "Larisa Safina",
      "Alceste Scalas"
    ],
    "author_ids": [],
    "abstract": "This volume contains the proceedings of ICE'21, the 14th Interaction and\nConcurrency Experience, which was held online on the 18th of June 2021, as a\nsatellite event of DisCoTec'21. The ICE workshop series features a\ndistinguishing review and selection procedure, allowing PC members to interact\nanonymously with authors. As in the past 13 editions, this interaction\nconsiderably improved the accuracy of the feedback from the reviewers and the\nquality of accepted papers, and offered the basis for lively discussion during\nthe workshop. The 2021 edition of ICE included double blind reviewing of\noriginal research papers, in order to increase fairness and avoid bias in\nreviewing. Each paper was reviewed by three or four PC members, and altogether\n5 papers were accepted for publication - plus 4 oral presentations which are\nnot part of this volume. We were proud to host 2 invited talks, by Laura Bocchi\nand Helene Coullon. The abstracts of these talks are included in this volume\ntogether with the regular papers. The final versions of the contributions,\ntaking into account the discussion at the workshop, are included.",
    "published_date": "2021-09-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.PL",
      "cs.LO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.14908v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2109.14772v1",
    "title": "An Automated Scanning Transmission Electron Microscope Guided by Sparse Data Analytics",
    "authors": [
      "Matthew Olszta",
      "Derek Hopkins",
      "Kevin R. Fiedler",
      "Marjolein Oostrom",
      "Sarah Akers",
      "Steven R. Spurgeon"
    ],
    "author_ids": [],
    "abstract": "Artificial intelligence (AI) promises to reshape scientific inquiry and\nenable breakthrough discoveries in areas such as energy storage, quantum\ncomputing, and biomedicine. Scanning transmission electron microscopy (STEM), a\ncornerstone of the study of chemical and materials systems, stands to benefit\ngreatly from AI-driven automation. However, present barriers to low-level\ninstrument control, as well as generalizable and interpretable feature\ndetection, make truly automated microscopy impractical. Here, we discuss the\ndesign of a closed-loop instrument control platform guided by emerging sparse\ndata analytics. We demonstrate how a centralized controller, informed by\nmachine learning combining limited $a$ $priori$ knowledge and task-based\ndiscrimination, can drive on-the-fly experimental decision-making. This\nplatform unlocks practical, automated analysis of a variety of material\nfeatures, enabling new high-throughput and statistical studies.",
    "published_date": "2021-09-30T00:00:00",
    "year": 2021,
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.14772v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.14715v1",
    "title": "USIS: Unsupervised Semantic Image Synthesis",
    "authors": [
      "George Eskandar",
      "Mohamed Abdelsamad",
      "Karim Armanious",
      "Bin Yang"
    ],
    "author_ids": [],
    "abstract": "Semantic Image Synthesis (SIS) is a subclass of image-to-image translation\nwhere a photorealistic image is synthesized from a segmentation mask. SIS has\nmostly been addressed as a supervised problem. However, state-of-the-art\nmethods depend on a huge amount of labeled data and cannot be applied in an\nunpaired setting. On the other hand, generic unpaired image-to-image\ntranslation frameworks underperform in comparison, because they color-code\nsemantic layouts and feed them to traditional convolutional networks, which\nthen learn correspondences in appearance instead of semantic content. In this\ninitial work, we propose a new Unsupervised paradigm for Semantic Image\nSynthesis (USIS) as a first step towards closing the performance gap between\npaired and unpaired settings. Notably, the framework deploys a SPADE generator\nthat learns to output images with visually separable semantic classes using a\nself-supervised segmentation loss. Furthermore, in order to match the color and\ntexture distribution of real images without losing high-frequency information,\nwe propose to use whole image wavelet-based discrimination. We test our\nmethodology on 3 challenging datasets and demonstrate its ability to generate\nmultimodal photorealistic images with an improved quality in the unpaired\nsetting.",
    "published_date": "2021-09-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.14715v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.14653v1",
    "title": "An Empirical Study of Accuracy, Fairness, Explainability, Distributional Robustness, and Adversarial Robustness",
    "authors": [
      "Moninder Singh",
      "Gevorg Ghalachyan",
      "Kush R. Varshney",
      "Reginald E. Bryant"
    ],
    "author_ids": [],
    "abstract": "To ensure trust in AI models, it is becoming increasingly apparent that\nevaluation of models must be extended beyond traditional performance metrics,\nlike accuracy, to other dimensions, such as fairness, explainability,\nadversarial robustness, and distribution shift. We describe an empirical study\nto evaluate multiple model types on various metrics along these dimensions on\nseveral datasets. Our results show that no particular model type performs well\non all dimensions, and demonstrate the kinds of trade-offs involved in\nselecting models evaluated along multiple dimensions.",
    "published_date": "2021-09-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.14653v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.14419v3",
    "title": "On the Estimation Bias in Double Q-Learning",
    "authors": [
      "Zhizhou Ren",
      "Guangxiang Zhu",
      "Hao Hu",
      "Beining Han",
      "Jianglun Chen",
      "Chongjie Zhang"
    ],
    "author_ids": [],
    "abstract": "Double Q-learning is a classical method for reducing overestimation bias,\nwhich is caused by taking maximum estimated values in the Bellman operation.\nIts variants in the deep Q-learning paradigm have shown great promise in\nproducing reliable value prediction and improving learning performance.\nHowever, as shown by prior work, double Q-learning is not fully unbiased and\nsuffers from underestimation bias. In this paper, we show that such\nunderestimation bias may lead to multiple non-optimal fixed points under an\napproximate Bellman operator. To address the concerns of converging to\nnon-optimal stationary solutions, we propose a simple but effective approach as\na partial fix for the underestimation bias in double Q-learning. This approach\nleverages an approximate dynamic programming to bound the target value. We\nextensively evaluate our proposed method in the Atari benchmark tasks and\ndemonstrate its significant improvement over baseline algorithms.",
    "published_date": "2021-09-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.14419v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.14376v1",
    "title": "Fairness-Driven Private Collaborative Machine Learning",
    "authors": [
      "Dana Pessach",
      "Tamir Tassa",
      "Erez Shmueli"
    ],
    "author_ids": [],
    "abstract": "The performance of machine learning algorithms can be considerably improved\nwhen trained over larger datasets. In many domains, such as medicine and\nfinance, larger datasets can be obtained if several parties, each having access\nto limited amounts of data, collaborate and share their data. However, such\ndata sharing introduces significant privacy challenges. While multiple recent\nstudies have investigated methods for private collaborative machine learning,\nthe fairness of such collaborative algorithms was overlooked. In this work we\nsuggest a feasible privacy-preserving pre-process mechanism for enhancing\nfairness of collaborative machine learning algorithms. Our experimentation with\nthe proposed method shows that it is able to enhance fairness considerably with\nonly a minor compromise in accuracy.",
    "published_date": "2021-09-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.14376v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.14345v1",
    "title": "Understanding Relations Between Perception of Fairness and Trust in Algorithmic Decision Making",
    "authors": [
      "Jianlong Zhou",
      "Sunny Verma",
      "Mudit Mittal",
      "Fang Chen"
    ],
    "author_ids": [],
    "abstract": "Algorithmic processes are increasingly employed to perform managerial\ndecision making, especially after the tremendous success in Artificial\nIntelligence (AI). This paradigm shift is occurring because these sophisticated\nAI techniques are guaranteeing the optimality of performance metrics. However,\nthis adoption is currently under scrutiny due to various concerns such as\nfairness, and how does the fairness of an AI algorithm affects user's trust is\nmuch legitimate to pursue. In this regard, we aim to understand the\nrelationship between induced algorithmic fairness and its perception in humans.\nIn particular, we are interested in whether these two are positively correlated\nand reflect substantive fairness. Furthermore, we also study how does induced\nalgorithmic fairness affects user trust in algorithmic decision making. To\nunderstand this, we perform a user study to simulate candidate shortlisting by\nintroduced (manipulating mathematical) fairness in a human resource recruitment\nsetting. Our experimental results demonstrate that different levels of\nintroduced fairness are positively related to human perception of fairness, and\nsimultaneously it is also positively related to user trust in algorithmic\ndecision making. Interestingly, we also found that users are more sensitive to\nthe higher levels of introduced fairness than the lower levels of introduced\nfairness. Besides, we summarize the theoretical and practical implications of\nthis research with a discussion on perception of fairness.",
    "published_date": "2021-09-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.14345v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.14218v1",
    "title": "Equivariant Neural Network for Factor Graphs",
    "authors": [
      "Fan-Yun Sun",
      "Jonathan Kuck",
      "Hao Tang",
      "Stefano Ermon"
    ],
    "author_ids": [],
    "abstract": "Several indices used in a factor graph data structure can be permuted without\nchanging the underlying probability distribution. An algorithm that performs\ninference on a factor graph should ideally be equivariant or invariant to\npermutations of global indices of nodes, variable orderings within a factor,\nand variable assignment orderings. However, existing neural network-based\ninference procedures fail to take advantage of this inductive bias. In this\npaper, we precisely characterize these isomorphic properties of factor graphs\nand propose two inference models: Factor-Equivariant Neural Belief Propagation\n(FE-NBP) and Factor-Equivariant Graph Neural Networks (FE-GNN). FE-NBP is a\nneural network that generalizes BP and respects each of the above properties of\nfactor graphs while FE-GNN is an expressive GNN model that relaxes an\nisomorphic property in favor of greater expressivity. Empirically, we\ndemonstrate on both real-world and synthetic datasets, for both marginal\ninference and MAP inference, that FE-NBP and FE-GNN together cover a range of\nsample complexity regimes: FE-NBP achieves state-of-the-art performance on\nsmall datasets while FE-GNN achieves state-of-the-art performance on large\ndatasets.",
    "published_date": "2021-09-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.14218v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.14047v1",
    "title": "Second Order WinoBias (SoWinoBias) Test Set for Latent Gender Bias Detection in Coreference Resolution",
    "authors": [
      "Hillary Dawkins"
    ],
    "author_ids": [],
    "abstract": "We observe an instance of gender-induced bias in a downstream application,\ndespite the absence of explicit gender words in the test cases. We provide a\ntest set, SoWinoBias, for the purpose of measuring such latent gender bias in\ncoreference resolution systems. We evaluate the performance of current\ndebiasing methods on the SoWinoBias test set, especially in reference to the\nmethod's design and altered embedding space properties. See\nhttps://github.com/hillarydawkins/SoWinoBias.",
    "published_date": "2021-09-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.14047v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.14039v1",
    "title": "Marked Attribute Bias in Natural Language Inference",
    "authors": [
      "Hillary Dawkins"
    ],
    "author_ids": [],
    "abstract": "Reporting and providing test sets for harmful bias in NLP applications is\nessential for building a robust understanding of the current problem. We\npresent a new observation of gender bias in a downstream NLP application:\nmarked attribute bias in natural language inference. Bias in downstream\napplications can stem from training data, word embeddings, or be amplified by\nthe model in use. However, focusing on biased word embeddings is potentially\nthe most impactful first step due to their universal nature. Here we seek to\nunderstand how the intrinsic properties of word embeddings contribute to this\nobserved marked attribute effect, and whether current post-processing methods\naddress the bias successfully. An investigation of the current debiasing\nlandscape reveals two open problems: none of the current debiased embeddings\nmitigate the marked attribute error, and none of the intrinsic bias measures\nare predictive of the marked attribute effect. By noticing that a new type of\nintrinsic bias measure correlates meaningfully with the marked attribute\neffect, we propose a new postprocessing debiasing scheme for static word\nembeddings. The proposed method applied to existing embeddings achieves new\nbest results on the marked attribute bias test set. See\nhttps://github.com/hillary-dawkins/MAB.",
    "published_date": "2021-09-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.14039v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.13845v1",
    "title": "Not Color Blind: AI Predicts Racial Identity from Black and White Retinal Vessel Segmentations",
    "authors": [
      "Aaron S. Coyner",
      "Praveer Singh",
      "James M. Brown",
      "Susan Ostmo",
      "R. V. Paul Chan",
      "Michael F. Chiang",
      "Jayashree Kalpathy-Cramer",
      "J. Peter Campbell"
    ],
    "author_ids": [],
    "abstract": "Background: Artificial intelligence (AI) may demonstrate racial bias when\nskin or choroidal pigmentation is present in medical images. Recent studies\nhave shown that convolutional neural networks (CNNs) can predict race from\nimages that were not previously thought to contain race-specific features. We\nevaluate whether grayscale retinal vessel maps (RVMs) of patients screened for\nretinopathy of prematurity (ROP) contain race-specific features.\n  Methods: 4095 retinal fundus images (RFIs) were collected from 245 Black and\nWhite infants. A U-Net generated RVMs from RFIs, which were subsequently\nthresholded, binarized, or skeletonized. To determine whether RVM differences\nbetween Black and White eyes were physiological, CNNs were trained to predict\nrace from color RFIs, raw RVMs, and thresholded, binarized, or skeletonized\nRVMs. Area under the precision-recall curve (AUC-PR) was evaluated.\n  Findings: CNNs predicted race from RFIs near perfectly (image-level AUC-PR:\n0.999, subject-level AUC-PR: 1.000). Raw RVMs were almost as informative as\ncolor RFIs (image-level AUC-PR: 0.938, subject-level AUC-PR: 0.995).\nUltimately, CNNs were able to detect whether RFIs or RVMs were from Black or\nWhite babies, regardless of whether images contained color, vessel segmentation\nbrightness differences were nullified, or vessel segmentation widths were\nnormalized.\n  Interpretation: AI can detect race from grayscale RVMs that were not thought\nto contain racial information. Two potential explanations for these findings\nare that: retinal vessels physiologically differ between Black and White babies\nor the U-Net segments the retinal vasculature differently for various fundus\npigmentations. Either way, the implications remain the same: AI algorithms have\npotential to demonstrate racial bias in practice, even when preliminary\nattempts to remove such information from the underlying images appear to be\nsuccessful.",
    "published_date": "2021-09-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.13845v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.13827v1",
    "title": "Intelligent Decision Assistance Versus Automated Decision-Making: Enhancing Knowledge Work Through Explainable Artificial Intelligence",
    "authors": [
      "Max Schemmer",
      "Niklas Kühl",
      "Gerhard Satzger"
    ],
    "author_ids": [],
    "abstract": "While recent advances in AI-based automated decision-making have shown many\nbenefits for businesses and society, they also come at a cost. It has for long\nbeen known that a high level of automation of decisions can lead to various\ndrawbacks, such as automation bias and deskilling. In particular, the\ndeskilling of knowledge workers is a major issue, as they are the same people\nwho should also train, challenge and evolve AI. To address this issue, we\nconceptualize a new class of DSS, namely Intelligent Decision Assistance (IDA)\nbased on a literature review of two different research streams -- DSS and\nautomation. IDA supports knowledge workers without influencing them through\nautomated decision-making. Specifically, we propose to use techniques of\nExplainable AI (XAI) while withholding concrete AI recommendations. To test\nthis conceptualization, we develop hypotheses on the impacts of IDA and provide\nfirst evidence for their validity based on empirical studies in the literature.",
    "published_date": "2021-09-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.13827v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.13767v1",
    "title": "Identifying and Mitigating Gender Bias in Hyperbolic Word Embeddings",
    "authors": [
      "Vaibhav Kumar",
      "Tenzin Singhay Bhotia",
      "Vaibhav Kumar",
      "Tanmoy Chakraborty"
    ],
    "author_ids": [],
    "abstract": "Euclidean word embedding models such as GloVe and Word2Vec have been shown to\nreflect human-like gender biases. In this paper, we extend the study of gender\nbias to the recently popularized hyperbolic word embeddings. We propose\ngyrocosine bias, a novel measure for quantifying gender bias in hyperbolic word\nrepresentations and observe a significant presence of gender bias. To address\nthis problem, we propose Poincar\\'e Gender Debias (PGD), a novel debiasing\nprocedure for hyperbolic word representations. Experiments on a suit of\nevaluation tests show that PGD effectively reduces bias while adding a minimal\nsemantic offset.",
    "published_date": "2021-09-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.13767v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.13589v1",
    "title": "Learning Ideological Embeddings from Information Cascades",
    "authors": [
      "Corrado Monti",
      "Giuseppe Manco",
      "Cigdem Aslay",
      "Francesco Bonchi"
    ],
    "author_ids": [],
    "abstract": "Modeling information cascades in a social network through the lenses of the\nideological leaning of its users can help understanding phenomena such as\nmisinformation propagation and confirmation bias, and devising techniques for\nmitigating their toxic effects.\n  In this paper we propose a stochastic model to learn the ideological leaning\nof each user in a multidimensional ideological space, by analyzing the way\npolitically salient content propagates. In particular, our model assumes that\ninformation propagates from one user to another if both users are interested in\nthe topic and ideologically aligned with each other. To infer the parameters of\nour model, we devise a gradient-based optimization procedure maximizing the\nlikelihood of an observed set of information cascades. Our experiments on\nreal-world political discussions on Twitter and Reddit confirm that our model\nis able to learn the political stance of the social media users in a\nmultidimensional ideological space.",
    "published_date": "2021-09-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI",
      "cs.CY",
      "cs.LG",
      "J.4; G.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.13589v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.15035v3",
    "title": "Focus! Rating XAI Methods and Finding Biases",
    "authors": [
      "Anna Arias-Duart",
      "Ferran Parés",
      "Dario Garcia-Gasulla",
      "Victor Gimenez-Abalos"
    ],
    "author_ids": [],
    "abstract": "AI explainability improves the transparency of models, making them more\ntrustworthy. Such goals are motivated by the emergence of deep learning models,\nwhich are obscure by nature; even in the domain of images, where deep learning\nhas succeeded the most, explainability is still poorly assessed. In the field\nof image recognition many feature attribution methods have been proposed with\nthe purpose of explaining a model's behavior using visual cues. However, no\nmetrics have been established so far to assess and select these methods\nobjectively. In this paper we propose a consistent evaluation score for feature\nattribution methods -- the Focus -- designed to quantify their coherency to the\ntask. While most previous work adds out-of-distribution noise to samples, we\nintroduce a methodology to add noise from within the distribution. This is done\nthrough mosaics of instances from different classes, and the explanations these\ngenerate. On those, we compute a visual pseudo-precision metric, Focus. First,\nwe show the robustness of the approach through a set of randomization\nexperiments. Then we use Focus to compare six popular explainability techniques\nacross several CNN architectures and classification datasets. Our results find\nsome methods to be consistently reliable (LRP, GradCAM), while others produce\nclass-agnostic explanations (SmoothGrad, IG). Finally we introduce another\napplication of Focus, using it for the identification and characterization of\nbiases found in models. This empowers bias-management tools, in another small\nstep towards trustworthy AI.",
    "published_date": "2021-09-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.15035v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.13493v2",
    "title": "Designed to Cooperate: A Kant-Inspired Ethic of Machine-to-Machine Cooperation",
    "authors": [
      "Seng W. Loke"
    ],
    "author_ids": [],
    "abstract": "This position paper highlights an ethic of machine-to-machine cooperation and\nmachine pro-sociality, and argues that machines capable of autonomous sensing,\ndecision-making and action, such as automated vehicles and urban robots, owned\nand used by different self-interested parties, and having their own agendas (or\ninterests of their owners) should be designed and built to be cooperative in\ntheir behaviours, especially if they share public spaces. That is, by design,\nthe machine should first cooperate, and then only consider alternatives if\nthere are problems. It is argued that being cooperative is not only important\nfor their improved functioning, especially, when they use shared resources\n(e.g., parking spaces, public roads, curbside space and walkways), but also as\na favourable requirement analogous to how humans cooperating with other humans\ncan be advantageous and often viewed favourably. The usefulness of such\nmachine-to-machine cooperation are illustrated via examples including\ncooperative crowdsourcing, cooperative traffic routing and parking as well as\nfuturistic scenarios involving urban robots for delivery and shopping. It is\nargued that just as privacy-by-design and security-by-design are important\nconsiderations, in order to yield systems that fulfil ethical requirements,\ncooperative-by-design should also be an imperative for autonomous systems that\nare separately owned but co-inhabit the same spaces and use common resources.\nIf a machine using shared public spaces is not cooperative, as one might\nexpect, then it is not only anti-social but not behaving ethically. It is also\nproposed that certification for urban robots that operate in public could be\nexplored.",
    "published_date": "2021-09-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.13493v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.13412v1",
    "title": "Discriminative Attribution from Counterfactuals",
    "authors": [
      "Nils Eckstein",
      "Alexander S. Bates",
      "Gregory S. X. E. Jefferis",
      "Jan Funke"
    ],
    "author_ids": [],
    "abstract": "We present a method for neural network interpretability by combining feature\nattribution with counterfactual explanations to generate attribution maps that\nhighlight the most discriminative features between pairs of classes. We show\nthat this method can be used to quantitatively evaluate the performance of\nfeature attribution methods in an objective manner, thus preventing potential\nobserver bias. We evaluate the proposed method on three diverse datasets,\nincluding a challenging artificial dataset and real-world biological data. We\nshow quantitatively and qualitatively that the highlighted features are\nsubstantially more discriminative than those extracted using conventional\nattribution methods and argue that this type of explanation is better suited\nfor understanding fine grained class differences as learned by a deep neural\nnetwork.",
    "published_date": "2021-09-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.13412v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.09253v1",
    "title": "A Sociotechnical View of Algorithmic Fairness",
    "authors": [
      "Mateusz Dolata",
      "Stefan Feuerriegel",
      "Gerhard Schwabe"
    ],
    "author_ids": [],
    "abstract": "Algorithmic fairness has been framed as a newly emerging technology that\nmitigates systemic discrimination in automated decision-making, providing\nopportunities to improve fairness in information systems (IS). However, based\non a state-of-the-art literature review, we argue that fairness is an\ninherently social concept and that technologies for algorithmic fairness should\ntherefore be approached through a sociotechnical lens. We advance the discourse\non algorithmic fairness as a sociotechnical phenomenon. Our research objective\nis to embed AF in the sociotechnical view of IS. Specifically, we elaborate on\nwhy outcomes of a system that uses algorithmic means to assure fairness depends\non mutual influences between technical and social structures. This perspective\ncan generate new insights that integrate knowledge from both technical fields\nand social studies. Further, it spurs new directions for IS debates. We\ncontribute as follows: First, we problematize fundamental assumptions in the\ncurrent discourse on algorithmic fairness based on a systematic analysis of 310\narticles. Second, we respond to these assumptions by theorizing algorithmic\nfairness as a sociotechnical construct. Third, we propose directions for IS\nresearchers to enhance their impacts by pursuing a unique understanding of\nsociotechnical algorithmic fairness. We call for and undertake a holistic\napproach to AF. A sociotechnical perspective on algorithmic fairness can yield\nholistic solutions to systemic biases and discrimination.",
    "published_date": "2021-09-27T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.09253v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.02737v1",
    "title": "Analysis of Trade-offs in RF Photonic Links based on Multi-Bias Tuning of Silicon Photonic Ring-Assisted Mach Zehnder Modulators",
    "authors": [
      "Md Jubayer Shawon",
      "Vishal Saxena"
    ],
    "author_ids": [],
    "abstract": "Recent progress in silicon-based photonic integrated circuits (PICs) have\nopened new avenues for analog circuit designers to explore hybrid integration\nof photonics with CMOS ICs. Traditionally, optoelectronic systems are designed\nusing discrete optics and electronics. Silicon photonic (SiP) platforms provide\nthe opportunity to realize these systems in a compact chip-scale form factor\nand alleviate long-standing challenges in optoelectronics. In this work, we\nanalyze multi-bias tuning in Ring-Assisted Mach Zehnder Modulator (RAMZM) and\nresulting trade-offs in analog RF photonic links realized using RAMZMs.\nMulti-bias tuning in the rings and the Mach-Zehnder arms allow informed\ntrade-offs between link noise figure and linearity. We derive performance\nmetrics including gain, noise figure, and linearity metrics associated with\ntuning of multiple bias settings in RAMZM based links and present resulting\ndesign optimization. Compared to MZM, an improvement of 18\ndB/Hz$^{\\frac{2}{3}}$ in SFDR is noted when RAMZM is linearized. We also\npropose a biasing scheme for RAMZM that provides 6x improvement in slope\nefficiency, or equivalently, 15.56dB in power Gain over MZMs (single drive)\nwhile still providing similar SFDR performance ($\\sim$ 109\ndB/Hz$^{\\frac{2}{3}}$) as MZMs. Moreover, a method to improve gain in\nphotodiode saturation limited links is presented and studied.",
    "published_date": "2021-09-27T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IT",
      "eess.SP",
      "math.IT",
      "physics.optics"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.02737v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2109.13291v1",
    "title": "Nonlinear modeling and feedback control of boom barrier automation",
    "authors": [
      "Daniel Cunico",
      "Angelo Cenedese",
      "Luca Zaccarian",
      "Mauro Borgo"
    ],
    "author_ids": [],
    "abstract": "We address modeling and control of a gate access automation system. A model\nof the mechatronic system is derived and identified. Then an approximate\nexplicit feedback linearization scheme is proposed, which ensures almost linear\nresponse between the external input and the delivered torque. A nonlinear\noptimization problem is solved offline to generate a feasible trajectory\nassociated with a feedforward action and a low level feedback controller is\ndesigned to track it. The feedback gains can be conveniently tuned by solving a\nset of convex linear matrix inequalities, performing a multi-objective\ntrade-off between disturbances attenuation and closed-loop performance.\nFinally, the proposed control strategy is tested on the real system and\nexperimental results show that it can effectively meet the requirements in\nterms of robustness, load disturbance rejection and tracking performance.",
    "published_date": "2021-09-27T00:00:00",
    "year": 2021,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.13291v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2109.13283v3",
    "title": "CS Education for the Socially-Just Worlds We Need: The Case for Justice-Centered Approaches to CS in Higher Education",
    "authors": [
      "Kevin Lin"
    ],
    "author_ids": [],
    "abstract": "Justice-centered approaches to equitable computer science (CS) education\nframe CS learning as a means for advancing peace, antiracism, and social\njustice rather than war, empire, and corporations. However, most research in\njustice-centered approaches in CS education focus on K-12 learning\nenvironments. In this position paper, we review justice-centered approaches to\nCS education, problematize the lack of justice-centered approaches to CS in\nhigher education in particular, and describe a justice-centered approach for\nundergraduate Data Structures and Algorithms. Our approach emphasizes three\ncomponents: (1) ethics: critiques the sociopolitical values of data structure\nand algorithm design as well as the underlying logics of dominant computing\nculture; (2) identity: draws on culturally responsive-sustaining pedagogies to\nemphasize student identity as rooted in resistance to the dominant computing\nculture; and (3) political vision: ensures the rightful presence of political\nstruggles by reauthoring rights to frame CS learning as a force for social\njustice. Through a case study of this Critical Comparative Data Structures and\nAlgorithms pedagogy, we argue that justice-centered approaches to higher CS\neducation can help all computing students not only learn about the ethical\nimplications of nominally technical concepts, but also develop greater respect\nfor diverse epistemologies, cultures, and experiences surrounding computing\nthat are essential to creating the socially-just worlds we need.",
    "published_date": "2021-09-27T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "K.3.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.13283v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2109.13228v1",
    "title": "PASS: An ImageNet replacement for self-supervised pretraining without humans",
    "authors": [
      "Yuki M. Asano",
      "Christian Rupprecht",
      "Andrew Zisserman",
      "Andrea Vedaldi"
    ],
    "author_ids": [],
    "abstract": "Computer vision has long relied on ImageNet and other large datasets of\nimages sampled from the Internet for pretraining models. However, these\ndatasets have ethical and technical shortcomings, such as containing personal\ninformation taken without consent, unclear license usage, biases, and, in some\ncases, even problematic image content. On the other hand, state-of-the-art\npretraining is nowadays obtained with unsupervised methods, meaning that\nlabelled datasets such as ImageNet may not be necessary, or perhaps not even\noptimal, for model pretraining. We thus propose an unlabelled dataset PASS:\nPictures without humAns for Self-Supervision. PASS only contains images with\nCC-BY license and complete attribution metadata, addressing the copyright\nissue. Most importantly, it contains no images of people at all, and also\navoids other types of images that are problematic for data protection or\nethics. We show that PASS can be used for pretraining with methods such as\nMoCo-v2, SwAV and DINO. In the transfer learning setting, it yields similar\ndownstream performances to ImageNet pretraining even on tasks that involve\nhumans, such as human pose estimation. PASS does not make existing datasets\nobsolete, as for instance it is insufficient for benchmarking. However, it\nshows that model pretraining is often possible while using safer data, and it\nalso provides the basis for a more robust evaluation of pretraining methods.",
    "published_date": "2021-09-27T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.13228v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.13193v1",
    "title": "On the Synthesis of Bellman Inequalities for Data-Driven Optimal Control",
    "authors": [
      "Andrea Martinelli",
      "Matilde Gargiani",
      "John Lygeros"
    ],
    "author_ids": [],
    "abstract": "In the context of the linear programming (LP) approach to data-driven\ncontrol, one assumes that the dynamical system is unknown but can be observed\nindirectly through data on its evolution. Both theoretical and empirical\nevidence suggest that a desired suboptimality gap is often only achieved with\nmassive exploration of the state-space. In case of linear systems, we discuss\nhow a relatively small but sufficiently rich dataset can be exploited to\ngenerate new constraints offline and without observing the corresponding\ntransitions. Moreover, we show how to reconstruct the associated unknown\nstage-costs and, when the system is stochastic, we offer insights on the\nrelated problem of estimating the expected value in the Bellman operator\nwithout re-initializing the dynamics in the same state-input pairs.",
    "published_date": "2021-09-27T00:00:00",
    "year": 2021,
    "categories": [
      "math.OC",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.13193v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2109.13137v1",
    "title": "Mitigating Racial Biases in Toxic Language Detection with an Equity-Based Ensemble Framework",
    "authors": [
      "Matan Halevy",
      "Camille Harris",
      "Amy Bruckman",
      "Diyi Yang",
      "Ayanna Howard"
    ],
    "author_ids": [],
    "abstract": "Recent research has demonstrated how racial biases against users who write\nAfrican American English exists in popular toxic language datasets. While\nprevious work has focused on a single fairness criteria, we propose to use\nadditional descriptive fairness metrics to better understand the source of\nthese biases. We demonstrate that different benchmark classifiers, as well as\ntwo in-process bias-remediation techniques, propagate racial biases even in a\nlarger corpus. We then propose a novel ensemble-framework that uses a\nspecialized classifier that is fine-tuned to the African American English\ndialect. We show that our proposed framework substantially reduces the racial\nbiases that the model learns from these datasets. We demonstrate how the\nensemble framework improves fairness metrics across all sample datasets with\nminimal impact on the classification performance, and provide empirical\nevidence in its ability to unlearn the annotation biases towards authors who\nuse African American English.\n  ** Please note that this work may contain examples of offensive words and\nphrases.",
    "published_date": "2021-09-27T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.13137v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.12932v3",
    "title": "Sparse Spatial Transformers for Few-Shot Learning",
    "authors": [
      "Haoxing Chen",
      "Huaxiong Li",
      "Yaohui Li",
      "Chunlin Chen"
    ],
    "author_ids": [],
    "abstract": "Learning from limited data is challenging because data scarcity leads to a\npoor generalization of the trained model. A classical global pooled\nrepresentation will probably lose useful local information. Many few-shot\nlearning methods have recently addressed this challenge using deep descriptors\nand learning a pixel-level metric. However, using deep descriptors as feature\nrepresentations may lose image contextual information. Moreover, most of these\nmethods independently address each class in the support set, which cannot\nsufficiently use discriminative information and task-specific embeddings. In\nthis paper, we propose a novel transformer-based neural network architecture\ncalled sparse spatial transformers (SSFormers), which finds task-relevant\nfeatures and suppresses task-irrelevant features. Particularly, we first divide\neach input image into several image patches of different sizes to obtain dense\nlocal features. These features retain contextual information while expressing\nlocal information. Then, a sparse spatial transformer layer is proposed to find\nspatial correspondence between the query image and the full support set to\nselect task-relevant image patches and suppress task-irrelevant image patches.\nFinally, we propose using an image patch-matching module to calculate the\ndistance between dense local representations, thus determining which category\nthe query image belongs to in the support set. Extensive experiments on popular\nfew-shot learning benchmarks demonstrate the superiority of our method over\nstate-of-the-art methods. Our source code is available at\n\\url{https://github.com/chenhaoxing/ssformers}.",
    "published_date": "2021-09-27T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.12932v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.12713v1",
    "title": "Provable Low Rank Plus Sparse Matrix Separation Via Nonconvex Regularizers",
    "authors": [
      "April Sagan",
      "John E. Mitchell"
    ],
    "author_ids": [],
    "abstract": "This paper considers a large class of problems where we seek to recover a low\nrank matrix and/or sparse vector from some set of measurements. While methods\nbased on convex relaxations suffer from a (possibly large) estimator bias, and\nother nonconvex methods require the rank or sparsity to be known a priori, we\nuse nonconvex regularizers to minimize the rank and $l_0$ norm without the\nestimator bias from the convex relaxation. We present a novel analysis of the\nalternating proximal gradient descent algorithm applied to such problems, and\nbound the error between the iterates and the ground truth sparse and low rank\nmatrices. The algorithm and error bound can be applied to sparse optimization,\nmatrix completion, and robust principal component analysis as special cases of\nour results.",
    "published_date": "2021-09-26T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.OC",
      "62J07, 15A83, 90C26"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.12713v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.12702v2",
    "title": "Extracting and Inferring Personal Attributes from Dialogue",
    "authors": [
      "Zhilin Wang",
      "Xuhui Zhou",
      "Rik Koncel-Kedziorski",
      "Alex Marin",
      "Fei Xia"
    ],
    "author_ids": [],
    "abstract": "Personal attributes represent structured information about a person, such as\ntheir hobbies, pets, family, likes and dislikes. We introduce the tasks of\nextracting and inferring personal attributes from human-human dialogue, and\nanalyze the linguistic demands of these tasks. To meet these challenges, we\nintroduce a simple and extensible model that combines an autoregressive\nlanguage model utilizing constrained attribute generation with a discriminative\nreranker. Our model outperforms strong baselines on extracting personal\nattributes as well as inferring personal attributes that are not contained\nverbatim in utterances and instead requires commonsense reasoning and lexical\ninferences, which occur frequently in everyday conversation. Finally, we\ndemonstrate the benefit of incorporating personal attributes in social\nchit-chat and task-oriented dialogue settings.",
    "published_date": "2021-09-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.12702v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.12563v1",
    "title": "Bayesian propensity score matching in automotive embedded software engineering",
    "authors": [
      "Yuchu Liu",
      "David Issa Mattos",
      "Jan Bosch",
      "Helena Holmström Olsson",
      "Jonn Lantz"
    ],
    "author_ids": [],
    "abstract": "Randomised field experiments, such as A/B testing, have long been the gold\nstandard for evaluating the value that new software brings to customers.\nHowever, running randomised field experiments is not always desired, possible\nor even ethical in the development of automotive embedded software. In the face\nof such restrictions, we propose the use of the Bayesian propensity score\nmatching technique for causal inference of observational studies in the\nautomotive domain. In this paper, we present a method based on the Bayesian\npropensity score matching framework, applied in the unique setting of\nautomotive software engineering. This method is used to generate balanced\ncontrol and treatment groups from an observational online evaluation and\nestimate causal treatment effects from the software changes, even with limited\nsamples in the treatment group. We exemplify the method with a proof-of-concept\nin the automotive domain. In the example, we have a larger control ($N_c=1100$)\nfleet of cars using the current software and a small treatment fleet\n($N_t=38$), in which we introduce a new software variant. We demonstrate a\nscenario that shipping of a new software to all users is restricted, as a\nresult, a fully randomised experiment could not be conducted. Therefore, we\nutilised the Bayesian propensity score matching method with 14 observed\ncovariates as inputs. The results show more balanced groups, suitable for\nestimating causal treatment effects from the collected observational data. We\ndescribe the method in detail and share our configuration. Furthermore, we\ndiscuss how can such a method be used for online evaluation of new software\nutilising small groups of samples.",
    "published_date": "2021-09-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.12563v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2109.12453v1",
    "title": "Classification of COVID-19 from CXR Images in a 15-class Scenario: an Attempt to Avoid Bias in the System",
    "authors": [
      "Chinmoy Bose",
      "Anirvan Basu"
    ],
    "author_ids": [],
    "abstract": "As of June 2021, the World Health Organization (WHO) has reported 171.7\nmillion confirmed cases including 3,698,621 deaths from COVID-19. Detecting\nCOVID-19 and other lung diseases from Chest X-Ray (CXR) images can be very\neffective for emergency diagnosis and treatment as CXR is fast and cheap. The\nobjective of this study is to develop a system capable of detecting COVID-19\nalong with 14 other lung diseases from CXRs in a fair and unbiased manner. The\nproposed system consists of a CXR image selection technique and a deep learning\nbased model to classify 15 diseases including COVID-19. The proposed CXR\nselection technique aims to retain the maximum variation uniformly and\neliminate poor quality CXRs with the goal of reducing the training dataset size\nwithout compromising classifier accuracy. More importantly, it reduces the\noften hidden bias and unfairness in decision making. The proposed solution\nexhibits a promising COVID-19 detection scheme in a more realistic situation\nthan most existing studies as it deals with 15 lung diseases together. We hope\nthe proposed method will have wider adoption in medical image classification\nand other related fields.",
    "published_date": "2021-09-25T00:00:00",
    "year": 2021,
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.12453v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.12422v1",
    "title": "Equality of opportunity in travel behavior prediction with deep neural networks and discrete choice models",
    "authors": [
      "Yunhan Zheng",
      "Shenhao Wang",
      "Jinhua Zhao"
    ],
    "author_ids": [],
    "abstract": "Although researchers increasingly adopt machine learning to model travel\nbehavior, they predominantly focus on prediction accuracy, ignoring the ethical\nchallenges embedded in machine learning algorithms. This study introduces an\nimportant missing dimension - computational fairness - to travel behavior\nanalysis. We first operationalize computational fairness by equality of\nopportunity, then differentiate between the bias inherent in data and the bias\nintroduced by modeling. We then demonstrate the prediction disparities in\ntravel behavior modeling using the 2017 National Household Travel Survey (NHTS)\nand the 2018-2019 My Daily Travel Survey in Chicago. Empirically, deep neural\nnetwork (DNN) and discrete choice models (DCM) reveal consistent prediction\ndisparities across multiple social groups: both over-predict the false negative\nrate of frequent driving for the ethnic minorities, the low-income and the\ndisabled populations, and falsely predict a higher travel burden of the\nsocially disadvantaged groups and the rural populations than reality. Comparing\nDNN with DCM, we find that DNN can outperform DCM in prediction disparities\nbecause of DNN's smaller misspecification error. To mitigate prediction\ndisparities, this study introduces an absolute correlation regularization\nmethod, which is evaluated with synthetic and real-world data. The results\ndemonstrate the prevalence of prediction disparities in travel behavior\nmodeling, and the disparities still persist regarding a variety of model\nspecifics such as the number of DNN layers, batch size and weight\ninitialization. Since these prediction disparities can exacerbate social\ninequity if prediction results without fairness adjustment are used for\ntransportation policy making, we advocate for careful consideration of the\nfairness problem in travel behavior modeling, and the use of bias mitigation\nalgorithms for fair transport decisions.",
    "published_date": "2021-09-25T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.12422v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.12378v2",
    "title": "Automaton-based Implicit Controlled Invariant Set Computation for Discrete-Time Linear Systems",
    "authors": [
      "Zexiang Liu",
      "Tzanis Anevlavis",
      "Necmiye Ozay",
      "Paulo Tabuada"
    ],
    "author_ids": [],
    "abstract": "In this paper, we derive closed-form expressions for implicit controlled\ninvariant sets for discrete-time controllable linear systems with measurable\ndisturbances. In particular, a disturbance-reactive (or disturbance feedback)\ncontroller in the form of a parameterized finite automaton is considered. We\nshow that, for a class of automata, the robust positively invariant sets of the\ncorresponding closed-loop systems can be expressed by a set of linear\ninequality constraints in the joint space of system states and controller\nparameters. This leads to an implicit representation of the invariant set in a\nlifted space. We further show how the same parameterization can be used to\ncompute invariant sets when the disturbance is not available for measurement.",
    "published_date": "2021-09-25T00:00:00",
    "year": 2021,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.12378v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2109.12176v1",
    "title": "On the Fairness of Swarm Learning in Skin Lesion Classification",
    "authors": [
      "Di Fan",
      "Yifan Wu",
      "Xiaoxiao Li"
    ],
    "author_ids": [],
    "abstract": "in healthcare. However, the existing AI model may be biased in its decision\nmarking. The bias induced by data itself, such as collecting data in subgroups\nonly, can be mitigated by including more diversified data. Distributed and\ncollaborative learning is an approach to involve training models in massive,\nheterogeneous, and distributed data sources, also known as nodes. In this work,\nwe target on examining the fairness issue in Swarm Learning (SL), a recent\nedge-computing based decentralized machine learning approach, which is designed\nfor heterogeneous illnesses detection in precision medicine. SL has achieved\nhigh performance in clinical applications, but no attempt has been made to\nevaluate if SL can improve fairness. To address the problem, we present an\nempirical study by comparing the fairness among single (node) training, SL,\ncentralized training. Specifically, we evaluate on large public available skin\nlesion dataset, which contains samples from various subgroups. The experiments\ndemonstrate that SL does not exacerbate the fairness problem compared to\ncentralized training and improves both performance and fairness compared to\nsingle training. However, there still exists biases in SL model and the\nimplementation of SL is more complex than the alternative two strategies.",
    "published_date": "2021-09-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DC",
      "cs.CY",
      "cs.LG",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.12176v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.12081v1",
    "title": "Deep Social Force",
    "authors": [
      "Sven Kreiss"
    ],
    "author_ids": [],
    "abstract": "The Social Force model introduced by Helbing and Molnar in 1995 is a\ncornerstone of pedestrian simulation. This paper introduces a differentiable\nsimulation of the Social Force model where the assumptions on the shapes of\ninteraction potentials are relaxed with the use of universal function\napproximators in the form of neural networks. Classical force-based pedestrian\nsimulations suffer from unnatural locking behavior on head-on collision paths.\nIn addition, they cannot model the bias of pedestrians to avoid each other on\nthe right or left depending on the geographic region. My experiments with more\ngeneral interaction potentials show that potentials with a sharp tip in the\nfront avoid locking. In addition, asymmetric interaction potentials lead to a\nleft or right bias when pedestrians avoid each other.",
    "published_date": "2021-09-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.12081v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.12077v2",
    "title": "The Mirror Langevin Algorithm Converges with Vanishing Bias",
    "authors": [
      "Ruilin Li",
      "Molei Tao",
      "Santosh S. Vempala",
      "Andre Wibisono"
    ],
    "author_ids": [],
    "abstract": "The technique of modifying the geometry of a problem from Euclidean to\nHessian metric has proved to be quite effective in optimization, and has been\nthe subject of study for sampling. The Mirror Langevin Diffusion (MLD) is a\nsampling analogue of mirror flow in continuous time, and it has nice\nconvergence properties under log-Sobolev or Poincare inequalities relative to\nthe Hessian metric, as shown by Chewi et al. (2020). In discrete time, a simple\ndiscretization of MLD is the Mirror Langevin Algorithm (MLA) studied by Zhang\net al. (2020), who showed a biased convergence bound with a non-vanishing bias\nterm (does not go to zero as step size goes to zero). This raised the question\nof whether we need a better analysis or a better discretization to achieve a\nvanishing bias. Here we study the basic Mirror Langevin Algorithm and show it\nindeed has a vanishing bias. We apply mean-square analysis based on Li et al.\n(2019) and Li et al. (2021) to show the mixing time bound for MLA under the\nmodified self-concordance condition introduced by Zhang et al. (2020).",
    "published_date": "2021-09-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DS",
      "cs.LG",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.12077v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.12072v1",
    "title": "SD-QA: Spoken Dialectal Question Answering for the Real World",
    "authors": [
      "Fahim Faisal",
      "Sharlina Keshava",
      "Md Mahfuz ibn Alam",
      "Antonios Anastasopoulos"
    ],
    "author_ids": [],
    "abstract": "Question answering (QA) systems are now available through numerous commercial\napplications for a wide variety of domains, serving millions of users that\ninteract with them via speech interfaces. However, current benchmarks in QA\nresearch do not account for the errors that speech recognition models might\nintroduce, nor do they consider the language variations (dialects) of the\nusers. To address this gap, we augment an existing QA dataset to construct a\nmulti-dialect, spoken QA benchmark on five languages (Arabic, Bengali, English,\nKiswahili, Korean) with more than 68k audio prompts in 24 dialects from 255\nspeakers. We provide baseline results showcasing the real-world performance of\nQA systems and analyze the effect of language variety and other sensitive\nspeaker attributes on downstream performance. Last, we study the fairness of\nthe ASR and QA models with respect to the underlying user populations. The\ndataset, model outputs, and code for reproducing all our experiments are\navailable: https://github.com/ffaisal93/SD-QA.",
    "published_date": "2021-09-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.12072v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.12062v3",
    "title": "SGDE: Secure Generative Data Exchange for Cross-Silo Federated Learning",
    "authors": [
      "Eugenio Lomurno",
      "Alberto Archetti",
      "Lorenzo Cazzella",
      "Stefano Samele",
      "Leonardo Di Perna",
      "Matteo Matteucci"
    ],
    "author_ids": [],
    "abstract": "Privacy regulation laws, such as GDPR, impose transparency and security as\ndesign pillars for data processing algorithms. In this context, federated\nlearning is one of the most influential frameworks for privacy-preserving\ndistributed machine learning, achieving astounding results in many natural\nlanguage processing and computer vision tasks. Several federated learning\nframeworks employ differential privacy to prevent private data leakage to\nunauthorized parties and malicious attackers. Many studies, however, highlight\nthe vulnerabilities of standard federated learning to poisoning and inference,\nthus raising concerns about potential risks for sensitive data. To address this\nissue, we present SGDE, a generative data exchange protocol that improves user\nsecurity and machine learning performance in a cross-silo federation. The core\nof SGDE is to share data generators with strong differential privacy guarantees\ntrained on private data instead of communicating explicit gradient information.\nThese generators synthesize an arbitrarily large amount of data that retain the\ndistinctive features of private samples but differ substantially. In this work,\nSGDE is tested in a cross-silo federated network on images and tabular\ndatasets, exploiting beta-variational autoencoders as data generators. From the\nresults, the inclusion of SGDE turns out to improve task accuracy and fairness,\nas well as resilience to the most influential attacks on federated learning.",
    "published_date": "2021-09-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.12062v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.11960v2",
    "title": "Towards a Governance Framework for Brain Data",
    "authors": [
      "Marcello Ienca",
      "Joseph J. Fins",
      "Ralf J. Jox",
      "Fabrice Jotterand",
      "Silja Voeneky",
      "Roberto Andorno",
      "Tonio Ball",
      "Claude Castelluccia",
      "Ricardo Chavarriaga",
      "Hervé Chneiweiss",
      "Agata Ferretti",
      "Orsolya Friedrich",
      "Samia Hurst",
      "Grischa Merkel",
      "Fruzsina Molnar-Gabor",
      "Jean-Marc Rickli",
      "James Scheibner",
      "Effy Vayena",
      "Rafael Yuste",
      "Philipp Kellmeyer"
    ],
    "author_ids": [],
    "abstract": "The increasing availability of brain data within and outside the biomedical\nfield, combined with the application of artificial intelligence (AI) to brain\ndata analysis, poses a challenge for ethics and governance. We identify\ndistinctive ethical implications of brain data acquisition and processing, and\noutline a multi-level governance framework. This framework is aimed at\nmaximizing the benefits of facilitated brain data collection and further\nprocessing for science and medicine whilst minimizing risks and preventing\nharmful use. The framework consists of four primary areas of regulatory\nintervention: binding regulation, ethics and soft law, responsible innovation,\nand human rights.",
    "published_date": "2021-09-24T00:00:00",
    "year": 2021,
    "categories": [
      "q-bio.NC",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.11960v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.11939v2",
    "title": "Discovering PDEs from Multiple Experiments",
    "authors": [
      "Georges Tod",
      "Gert-Jan Both",
      "Remy Kusters"
    ],
    "author_ids": [],
    "abstract": "Automated model discovery of partial differential equations (PDEs) usually\nconsiders a single experiment or dataset to infer the underlying governing\nequations. In practice, experiments have inherent natural variability in\nparameters, initial and boundary conditions that cannot be simply averaged out.\nWe introduce a randomised adaptive group Lasso sparsity estimator to promote\ngrouped sparsity and implement it in a deep learning based PDE discovery\nframework. It allows to create a learning bias that implies the a priori\nassumption that all experiments can be explained by the same underlying PDE\nterms with potentially different coefficients. Our experimental results show\nmore generalizable PDEs can be found from multiple highly noisy datasets, by\nthis grouped sparsity promotion rather than simply performing independent model\ndiscoveries.",
    "published_date": "2021-09-24T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG",
      "physics.comp-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.11939v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.11801v2",
    "title": "SIM2REALVIZ: Visualizing the Sim2Real Gap in Robot Ego-Pose Estimation",
    "authors": [
      "Theo Jaunet",
      "Guillaume Bono",
      "Romain Vuillemot",
      "Christian Wolf"
    ],
    "author_ids": [],
    "abstract": "The Robotics community has started to heavily rely on increasingly realistic\n3D simulators for large-scale training of robots on massive amounts of data.\nBut once robots are deployed in the real world, the simulation gap, as well as\nchanges in the real world (e.g. lights, objects displacements) lead to errors.\nIn this paper, we introduce Sim2RealViz, a visual analytics tool to assist\nexperts in understanding and reducing this gap for robot ego-pose estimation\ntasks, i.e. the estimation of a robot's position using trained models.\nSim2RealViz displays details of a given model and the performance of its\ninstances in both simulation and real-world. Experts can identify environment\ndifferences that impact model predictions at a given location and explore\nthrough direct interactions with the model hypothesis to fix it. We detail the\ndesign of the tool, and case studies related to the exploit of the regression\nto the mean bias and how it can be addressed, and how models are perturbed by\nthe vanish of landmarks such as bikes.",
    "published_date": "2021-09-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.HC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.11801v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.11788v3",
    "title": "Parameter-free Reduction of the Estimation Bias in Deep Reinforcement Learning for Deterministic Policy Gradients",
    "authors": [
      "Baturay Saglam",
      "Furkan Burak Mutlu",
      "Dogan Can Cicek",
      "Suleyman Serdar Kozat"
    ],
    "author_ids": [],
    "abstract": "Approximation of the value functions in value-based deep reinforcement\nlearning induces overestimation bias, resulting in suboptimal policies. We show\nthat when the reinforcement signals received by the agents have a high\nvariance, deep actor-critic approaches that overcome the overestimation bias\nlead to a substantial underestimation bias. We first address the detrimental\nissues in the existing approaches that aim to overcome such underestimation\nerror. Then, through extensive statistical analysis, we introduce a novel,\nparameter-free Deep Q-learning variant to reduce this underestimation bias in\ndeterministic policy gradients. By sampling the weights of a linear combination\nof two approximate critics from a highly shrunk estimation bias interval, our\nQ-value update rule is not affected by the variance of the rewards received by\nthe agents throughout learning. We test the performance of the introduced\nimprovement on a set of MuJoCo and Box2D continuous control tasks and\ndemonstrate that it considerably outperforms the existing approaches and\nimproves the state-of-the-art by a significant margin.",
    "published_date": "2021-09-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.11788v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.11725v5",
    "title": "Punctured Low-Bias Codes Behave Like Random Linear Codes",
    "authors": [
      "Venkatesan Guruswami",
      "Jonathan Mosheiff"
    ],
    "author_ids": [],
    "abstract": "Random linear codes are a workhorse in coding theory, and are used to show\nthe existence of codes with the best known or even near-optimal trade-offs in\nmany noise models. However, they have little structure besides linearity, and\nare not amenable to tractable error-correction algorithms.\n  In this work, we prove a general derandomization result applicable to random\nlinear codes. Namely, in settings where the coding-theoretic property of\ninterest is \"local\" (in the sense of forbidding certain bad configurations\ninvolving few vectors -- code distance and list-decodability being notable\nexamples), one can replace random linear codes (RLCs) with a significantly\nderandomized variant with essentially no loss in parameters. Specifically,\ninstead of randomly sampling coordinates of the (long) Hadamard code (which is\nan equivalent way to describe RLCs), one can randomly sample coordinates of any\ncode with low bias. Over large alphabets, the low bias requirement can be\nweakened to just large distance. Furthermore, large distance suffices even with\na small alphabet in order to match the current best known bounds for RLC\nlist-decodability.\n  In particular, by virtue of our result, all current (and future)\nachievability bounds for list-decodability of random linear codes extend\nautomatically to random puncturings of any low-bias (or large alphabet)\n\"mother\" code. We also show that our punctured codes emulate the behavior of\nRLCs on stochastic channels, thus giving a derandomization of RLCs in the\ncontext of achieving Shannon capacity as well. Thus, we have a\nrandomness-efficient way to sample codes achieving capacity in both worst-case\nand stochastic settings that can further inherit algebraic or other\nalgorithmically useful structural properties of the mother code.",
    "published_date": "2021-09-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CC",
      "cs.IT",
      "math.CO",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.11725v5",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2109.11502v3",
    "title": "Inequality Constrained Stochastic Nonlinear Optimization via Active-Set Sequential Quadratic Programming",
    "authors": [
      "Sen Na",
      "Mihai Anitescu",
      "Mladen Kolar"
    ],
    "author_ids": [],
    "abstract": "We study nonlinear optimization problems with a stochastic objective and\ndeterministic equality and inequality constraints, which emerge in numerous\napplications including finance, manufacturing, power systems and, recently,\ndeep neural networks. We propose an active-set stochastic sequential quadratic\nprogramming (StoSQP) algorithm that utilizes a differentiable exact augmented\nLagrangian as the merit function. The algorithm adaptively selects the penalty\nparameters of the augmented Lagrangian and performs a stochastic line search to\ndecide the stepsize. The global convergence is established: for any\ninitialization, the KKT residuals converge to zero almost surely. Our algorithm\nand analysis further develop the prior work of Na et al., (2022). Specifically,\nwe allow nonlinear inequality constraints without requiring the strict\ncomplementary condition; refine some of the designs in Na et al., (2022) such\nas the feasibility error condition and the monotonically increasing sample\nsize; strengthen the global convergence guarantee; and improve the sample\ncomplexity on the objective Hessian. We demonstrate the performance of the\ndesigned algorithm on a subset of nonlinear problems collected in CUTEst test\nset and on constrained logistic regression problems.",
    "published_date": "2021-09-23T00:00:00",
    "year": 2021,
    "categories": [
      "math.OC",
      "cs.LG",
      "cs.NA",
      "math.NA",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.11502v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.11484v1",
    "title": "Diversity by Design: Balancing Protection and Inclusion in Social Networks",
    "authors": [
      "Paula Helm",
      "Loizos Michael",
      "Laura Schelenz"
    ],
    "author_ids": [],
    "abstract": "The unreflected promotion of diversity as a value in social interactions --\nincluding technology-mediated ones -- risks emphasizing the benefits of\ninclusion at the cost of not recognizing the potential harm from failing to\nprotect stigmatized or marginalized individuals. Adopting the stance that\ntechnology is not value-neutral, we attempt to answer the question of how\ntechnology-mediated social platforms could accommodate \\emph{diversity by\ndesign}, by balancing the often competing values of protection and inclusion.\nThis short paper presents our research agenda as well as initial analysis and\noutcomes. Building on approaches from scenario planning and the methodology of\nValue Sensitive Design, we identify ethical principles and arguments on how to\ncurate diversity, which we seek to operationalize through formal argumentation.",
    "published_date": "2021-09-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.11484v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2109.11336v1",
    "title": "Towards Generalized and Incremental Few-Shot Object Detection",
    "authors": [
      "Yiting Li",
      "Haiyue Zhu",
      "Jun Ma",
      "Chek Sing Teo",
      "Cheng Xiang",
      "Prahlad Vadakkepat",
      "Tong Heng Lee"
    ],
    "author_ids": [],
    "abstract": "Real-world object detection is highly desired to be equipped with the\nlearning expandability that can enlarge its detection classes incrementally.\nMoreover, such learning from only few annotated training samples further adds\nthe flexibility for the object detector, which is highly expected in many\napplications such as autonomous driving, robotics, etc. However, such\nsequential learning scenario with few-shot training samples generally causes\ncatastrophic forgetting and dramatic overfitting. In this paper, to address the\nabove incremental few-shot learning issues, a novel Incremental Few-Shot Object\nDetection (iFSOD) method is proposed to enable the effective continual learning\nfrom few-shot samples. Specifically, a Double-Branch Framework (DBF) is\nproposed to decouple the feature representation of base and novel (few-shot)\nclass, which facilitates both the old-knowledge retention and new-class\nadaption simultaneously. Furthermore, a progressive model updating rule is\ncarried out to preserve the long-term memory on old classes effectively when\nadapt to sequential new classes. Moreover, an inter-task class separation loss\nis proposed to extend the decision region of new-coming classes for better\nfeature discrimination. We conduct experiments on both Pascal VOC and MS-COCO,\nwhich demonstrate that our method can effectively solve the problem of\nincremental few-shot detection and significantly improve the detection accuracy\non both base and novel classes.",
    "published_date": "2021-09-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.11336v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.11196v4",
    "title": "Fast and Efficient MMD-based Fair PCA via Optimization over Stiefel Manifold",
    "authors": [
      "Junghyun Lee",
      "Gwangsu Kim",
      "Matt Olfat",
      "Mark Hasegawa-Johnson",
      "Chang D. Yoo"
    ],
    "author_ids": [],
    "abstract": "This paper defines fair principal component analysis (PCA) as minimizing the\nmaximum mean discrepancy (MMD) between dimensionality-reduced conditional\ndistributions of different protected classes. The incorporation of MMD\nnaturally leads to an exact and tractable mathematical formulation of fairness\nwith good statistical properties. We formulate the problem of fair PCA subject\nto MMD constraints as a non-convex optimization over the Stiefel manifold and\nsolve it using the Riemannian Exact Penalty Method with Smoothing (REPMS; Liu\nand Boumal, 2019). Importantly, we provide local optimality guarantees and\nexplicitly show the theoretical effect of each hyperparameter in practical\nsettings, extending previous results. Experimental comparisons based on\nsynthetic and UCI datasets show that our approach outperforms prior work in\nexplained variance, fairness, and runtime.",
    "published_date": "2021-09-23T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.11196v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.11105v1",
    "title": "Distiller: A Systematic Study of Model Distillation Methods in Natural Language Processing",
    "authors": [
      "Haoyu He",
      "Xingjian Shi",
      "Jonas Mueller",
      "Zha Sheng",
      "Mu Li",
      "George Karypis"
    ],
    "author_ids": [],
    "abstract": "We aim to identify how different components in the KD pipeline affect the\nresulting performance and how much the optimal KD pipeline varies across\ndifferent datasets/tasks, such as the data augmentation policy, the loss\nfunction, and the intermediate representation for transferring the knowledge\nbetween teacher and student. To tease apart their effects, we propose\nDistiller, a meta KD framework that systematically combines a broad range of\ntechniques across different stages of the KD pipeline, which enables us to\nquantify each component's contribution. Within Distiller, we unify commonly\nused objectives for distillation of intermediate representations under a\nuniversal mutual information (MI) objective and propose a class of MI-$\\alpha$\nobjective functions with better bias/variance trade-off for estimating the MI\nbetween the teacher and the student. On a diverse set of NLP datasets, the best\nDistiller configurations are identified via large-scale hyperparameter\noptimization. Our experiments reveal the following: 1) the approach used to\ndistill the intermediate representations is the most important factor in KD\nperformance, 2) among different objectives for intermediate distillation,\nMI-$\\alpha$ performs the best, and 3) data augmentation provides a large boost\nfor small training datasets or small student networks. Moreover, we find that\ndifferent datasets/tasks prefer different KD algorithms, and thus propose a\nsimple AutoDistiller algorithm that can recommend a good KD pipeline for a new\ndataset.",
    "published_date": "2021-09-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.11105v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.11087v2",
    "title": "BiRdQA: A Bilingual Dataset for Question Answering on Tricky Riddles",
    "authors": [
      "Yunxiang Zhang",
      "Xiaojun Wan"
    ],
    "author_ids": [],
    "abstract": "A riddle is a question or statement with double or veiled meanings, followed\nby an unexpected answer. Solving riddle is a challenging task for both machine\nand human, testing the capability of understanding figurative, creative natural\nlanguage and reasoning with commonsense knowledge. We introduce BiRdQA, a\nbilingual multiple-choice question answering dataset with 6614 English riddles\nand 8751 Chinese riddles. For each riddle-answer pair, we provide four\ndistractors with additional information from Wikipedia. The distractors are\nautomatically generated at scale with minimal bias. Existing monolingual and\nmultilingual QA models fail to perform well on our dataset, indicating that\nthere is a long way to go before machine can beat human on solving tricky\nriddles. The dataset has been released to the community.",
    "published_date": "2021-09-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.11087v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.10949v2",
    "title": "Recursive Feasibility Guided Optimal Parameter Adaptation of Differential Convex Optimization Policies for Safety-Critical Systems",
    "authors": [
      "Hardik Parwana",
      "Dimitra Panagou"
    ],
    "author_ids": [],
    "abstract": "Quadratic Program(QP) based state-feedback controllers, whose inequality\nconstraints bound the rate of change of control barrier(CBFs) and lyapunov\nfunction with a class-$\\mathcal{K}$ function of their values, are sensitive to\nthe parameters of these class-$\\mathcal{K}$ functions. The construction of\nvalid CBFs, however, is not straightforward, and for arbitrarily chosen\nparameters of the QP, the system trajectories may enter states at which the QP\neither eventually becomes infeasible, or may not achieve desired performance.\nIn this work, we pose the control synthesis problem as a differential policy\nwhose parameters are optimized for performance over a time horizon at high\nlevel, thus resulting in a bi-level optimization routine. In the absence of\nknowledge of the set of feasible parameters, we develop a Recursive Feasibility\nGuided Gradient Descent approach for updating the parameters of QP so that the\nnew solution performs at least as well as previous solution. By considering the\ndynamical system as a directed graph over time, this work presents a novel way\nof optimizing performance of a QP controller over a time horizon for multiple\nCBFs by (1) using the gradient of its solution with respect to its parameters\nby employing sensitivity analysis, and (2) backpropagating these as well as\nsystem dynamics gradients to update parameters while maintaining feasibility of\nQPs.",
    "published_date": "2021-09-22T00:00:00",
    "year": 2021,
    "categories": [
      "math.OC",
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.10949v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2109.10836v2",
    "title": "AI-HRI 2021 Proceedings",
    "authors": [
      "Reuth Mirsky",
      "Megan Zimmerman",
      "Muneed Ahmad",
      "Shelly Bagchi",
      "Felix Gervits",
      "Zhao Han",
      "Justin Hart",
      "Daniel Hernández García",
      "Matteo Leonetti",
      "Ross Mead",
      "Emmanuel Senft",
      "Jivko Sinapov",
      "Jason Wilson"
    ],
    "author_ids": [],
    "abstract": "The Artificial Intelligence (AI) for Human-Robot Interaction (HRI) Symposium\nhas been a successful venue of discussion and collaboration since 2014. During\nthat time, these symposia provided a fertile ground for numerous collaborations\nand pioneered many discussions revolving trust in HRI, XAI for HRI, service\nrobots, interactive learning, and more.\n  This year, we aim to review the achievements of the AI-HRI community in the\nlast decade, identify the challenges facing ahead, and welcome new researchers\nwho wish to take part in this growing community. Taking this wide perspective,\nthis year there will be no single theme to lead the symposium and we encourage\nAI-HRI submissions from across disciplines and research interests. Moreover,\nwith the rising interest in AR and VR as part of an interaction and following\nthe difficulties in running physical experiments during the pandemic, this year\nwe specifically encourage researchers to submit works that do not include a\nphysical robot in their evaluation, but promote HRI research in general. In\naddition, acknowledging that ethics is an inherent part of the human-robot\ninteraction, we encourage submissions of works on ethics for HRI. Over the\ncourse of the two-day meeting, we will host a collaborative forum for\ndiscussion of current efforts in AI-HRI, with additional talks focused on the\ntopics of ethics in HRI and ubiquitous HRI.",
    "published_date": "2021-09-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.10836v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.10645v1",
    "title": "Contrastive Learning for Fair Representations",
    "authors": [
      "Aili Shen",
      "Xudong Han",
      "Trevor Cohn",
      "Timothy Baldwin",
      "Lea Frermann"
    ],
    "author_ids": [],
    "abstract": "Trained classification models can unintentionally lead to biased\nrepresentations and predictions, which can reinforce societal preconceptions\nand stereotypes. Existing debiasing methods for classification models, such as\nadversarial training, are often expensive to train and difficult to optimise.\nIn this paper, we propose a method for mitigating bias in classifier training\nby incorporating contrastive learning, in which instances sharing the same\nclass label are encouraged to have similar representations, while instances\nsharing a protected attribute are forced further apart. In such a way our\nmethod learns representations which capture the task label in focused regions,\nwhile ensuring the protected attribute has diverse spread, and thus has limited\nimpact on prediction and thereby results in fairer models. Extensive\nexperimental results across four tasks in NLP and computer vision show (a) that\nour proposed method can achieve fairer representations and realises bias\nreductions compared with competitive baselines; and (b) that it can do so\nwithout sacrificing main task performance; (c) that it sets a new\nstate-of-the-art performance in one task despite reducing the bias. Finally,\nour method is conceptually simple and agnostic to network architectures, and\nincurs minimal additional compute cost.",
    "published_date": "2021-09-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.10645v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.10569v5",
    "title": "The Curse Revisited: When are Distances Informative for the Ground Truth in Noisy High-Dimensional Data?",
    "authors": [
      "Robin Vandaele",
      "Bo Kang",
      "Tijl De Bie",
      "Yvan Saeys"
    ],
    "author_ids": [],
    "abstract": "Distances between data points are widely used in machine learning\napplications. Yet, when corrupted by noise, these distances -- and thus the\nmodels based upon them -- may lose their usefulness in high dimensions. Indeed,\nthe small marginal effects of the noise may then accumulate quickly, shifting\nempirical closest and furthest neighbors away from the ground truth. In this\npaper, we exactly characterize such effects in noisy high-dimensional data\nusing an asymptotic probabilistic expression. Previously, it has been argued\nthat neighborhood queries become meaningless and unstable when distance\nconcentration occurs, which means that there is a poor relative discrimination\nbetween the furthest and closest neighbors in the data. However, we conclude\nthat this is not necessarily the case when we decompose the data in a ground\ntruth -- which we aim to recover -- and noise component. More specifically, we\nderive that under particular conditions, empirical neighborhood relations\naffected by noise are still likely to be truthful even when distance\nconcentration occurs. We also include thorough empirical verification of our\nresults, as well as interesting experiments in which our derived 'phase shift'\nwhere neighbors become random or not turns out to be identical to the phase\nshift where common dimensionality reduction methods perform poorly or well for\nrecovering low-dimensional reconstructions of high-dimensional data with dense\nnoise.",
    "published_date": "2021-09-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.10569v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.10557v1",
    "title": "A Reinforcement Learning Benchmark for Autonomous Driving in Intersection Scenarios",
    "authors": [
      "Yuqi Liu",
      "Qichao Zhang",
      "Dongbin Zhao"
    ],
    "author_ids": [],
    "abstract": "In recent years, control under urban intersection scenarios becomes an\nemerging research topic. In such scenarios, the autonomous vehicle confronts\ncomplicated situations since it must deal with the interaction with social\nvehicles timely while obeying the traffic rules. Generally, the autonomous\nvehicle is supposed to avoid collisions while pursuing better efficiency. The\nexisting work fails to provide a framework that emphasizes the integrity of the\nscenarios while being able to deploy and test reinforcement learning(RL)\nmethods. Specifically, we propose a benchmark for training and testing RL-based\nautonomous driving agents in complex intersection scenarios, which is called\nRL-CIS. Then, a set of baselines are deployed consists of various algorithms.\nThe test benchmark and baselines are to provide a fair and comprehensive\ntraining and testing platform for the study of RL for autonomous driving in the\nintersection scenario, advancing the progress of RL-based methods for\nintersection autonomous driving control. The code of our proposed framework can\nbe found at https://github.com/liuyuqi123/ComplexUrbanScenarios.",
    "published_date": "2021-09-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.10557v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.10458v1",
    "title": "Achieving Counterfactual Fairness for Causal Bandit",
    "authors": [
      "Wen Huang",
      "Lu Zhang",
      "Xintao Wu"
    ],
    "author_ids": [],
    "abstract": "In online recommendation, customers arrive in a sequential and stochastic\nmanner from an underlying distribution and the online decision model recommends\na chosen item for each arriving individual based on some strategy. We study how\nto recommend an item at each step to maximize the expected reward while\nachieving user-side fairness for customers, i.e., customers who share similar\nprofiles will receive a similar reward regardless of their sensitive attributes\nand items being recommended. By incorporating causal inference into bandits and\nadopting soft intervention to model the arm selection strategy, we first\npropose the d-separation based UCB algorithm (D-UCB) to explore the utilization\nof the d-separation set in reducing the amount of exploration needed to achieve\nlow cumulative regret. Based on that, we then propose the fair causal bandit\n(F-UCB) for achieving the counterfactual individual fairness. Both theoretical\nanalysis and empirical evaluation demonstrate effectiveness of our algorithms.",
    "published_date": "2021-09-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.10458v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.10444v1",
    "title": "Fairness-aware Class Imbalanced Learning",
    "authors": [
      "Shivashankar Subramanian",
      "Afshin Rahimi",
      "Timothy Baldwin",
      "Trevor Cohn",
      "Lea Frermann"
    ],
    "author_ids": [],
    "abstract": "Class imbalance is a common challenge in many NLP tasks, and has clear\nconnections to bias, in that bias in training data often leads to higher\naccuracy for majority groups at the expense of minority groups. However there\nhas traditionally been a disconnect between research on class-imbalanced\nlearning and mitigating bias, and only recently have the two been looked at\nthrough a common lens. In this work we evaluate long-tail learning methods for\ntweet sentiment and occupation classification, and extend a margin-loss based\napproach with methods to enforce fairness. We empirically show through\ncontrolled experiments that the proposed approaches help mitigate both class\nimbalance and demographic biases.",
    "published_date": "2021-09-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.10444v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.10441v1",
    "title": "Evaluating Debiasing Techniques for Intersectional Biases",
    "authors": [
      "Shivashankar Subramanian",
      "Xudong Han",
      "Timothy Baldwin",
      "Trevor Cohn",
      "Lea Frermann"
    ],
    "author_ids": [],
    "abstract": "Bias is pervasive in NLP models, motivating the development of automatic\ndebiasing techniques. Evaluation of NLP debiasing methods has largely been\nlimited to binary attributes in isolation, e.g., debiasing with respect to\nbinary gender or race, however many corpora involve multiple such attributes,\npossibly with higher cardinality. In this paper we argue that a truly fair\nmodel must consider `gerrymandering' groups which comprise not only single\nattributes, but also intersectional groups. We evaluate a form of\nbias-constrained model which is new to NLP, as well an extension of the\niterative nullspace projection technique which can handle multiple protected\nattributes.",
    "published_date": "2021-09-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.10441v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.09271v3",
    "title": "Rebuilding Trust: Queer in AI Approach to Artificial Intelligence Risk Management",
    "authors": [
      "Ashwin",
      "William Agnew",
      "Umut Pajaro",
      "Hetvi Jethwani",
      "Arjun Subramonian"
    ],
    "author_ids": [],
    "abstract": "Trustworthy artificial intelligence (AI) has become an important topic\nbecause trust in AI systems and their creators has been lost. Researchers,\ncorporations, and governments have long and painful histories of excluding\nmarginalized groups from technology development, deployment, and oversight. As\na result, these technologies are less useful and even harmful to minoritized\ngroups. We argue that any AI development, deployment, and monitoring framework\nthat aspires to trust must incorporate both feminist, non-exploitative\nparticipatory design principles and strong, outside, and continual monitoring\nand testing. We additionally explain the importance of considering aspects of\ntrustworthiness beyond just transparency, fairness, and accountability,\nspecifically, to consider justice and shifting power to the disempowered as\ncore values to any trustworthy AI system. Creating trustworthy AI starts by\nfunding, supporting, and empowering grassroots organizations like Queer in AI\nso the field of AI has the diversity and inclusion to credibly and effectively\ndevelop trustworthy AI. We leverage the expert knowledge Queer in AI has\ndeveloped through its years of work and advocacy to discuss if and how gender,\nsexuality, and other aspects of queer identity should be used in datasets and\nAI systems and how harms along these lines should be mitigated. Based on this,\nwe share a gendered approach to AI and further propose a queer epistemology and\nanalyze the benefits it can bring to AI. We additionally discuss how to\nregulate AI with this queer epistemology in vision, proposing frameworks for\nmaking policies related to AI & gender diversity and privacy & queer data\nprotection.",
    "published_date": "2021-09-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.09271v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.10431v2",
    "title": "Fairness without Imputation: A Decision Tree Approach for Fair Prediction with Missing Values",
    "authors": [
      "Haewon Jeong",
      "Hao Wang",
      "Flavio P. Calmon"
    ],
    "author_ids": [],
    "abstract": "We investigate the fairness concerns of training a machine learning model\nusing data with missing values. Even though there are a number of fairness\nintervention methods in the literature, most of them require a complete\ntraining set as input. In practice, data can have missing values, and data\nmissing patterns can depend on group attributes (e.g. gender or race). Simply\napplying off-the-shelf fair learning algorithms to an imputed dataset may lead\nto an unfair model. In this paper, we first theoretically analyze different\nsources of discrimination risks when training with an imputed dataset. Then, we\npropose an integrated approach based on decision trees that does not require a\nseparate process of imputation and learning. Instead, we train a tree with\nmissing incorporated as attribute (MIA), which does not require explicit\nimputation, and we optimize a fairness-regularized objective function. We\ndemonstrate that our approach outperforms existing fairness intervention\nmethods applied to an imputed dataset, through several experiments on\nreal-world datasets.",
    "published_date": "2021-09-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.IT",
      "math.IT",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.10431v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.10053v4",
    "title": "Toward a Fairness-Aware Scoring System for Algorithmic Decision-Making",
    "authors": [
      "Yi Yang",
      "Ying Wu",
      "Mei Li",
      "Xiangyu Chang",
      "Yong Tan"
    ],
    "author_ids": [],
    "abstract": "Scoring systems, as a type of predictive model, have significant advantages\nin interpretability and transparency and facilitate quick decision-making. As\nsuch, scoring systems have been extensively used in a wide variety of\nindustries such as healthcare and criminal justice. However, the fairness\nissues in these models have long been criticized, and the use of big data and\nmachine learning algorithms in the construction of scoring systems heightens\nthis concern. In this paper, we propose a general framework to create\nfairness-aware, data-driven scoring systems. First, we develop a social welfare\nfunction that incorporates both efficiency and group fairness. Then, we\ntransform the social welfare maximization problem into the risk minimization\ntask in machine learning, and derive a fairness-aware scoring system with the\nhelp of mixed integer programming. Lastly, several theoretical bounds are\nderived for providing parameter selection suggestions. Our proposed framework\nprovides a suitable solution to address group fairness concerns in the\ndevelopment of scoring systems. It enables policymakers to set and customize\ntheir desired fairness requirements as well as other application-specific\nconstraints. We test the proposed algorithm with several empirical data sets.\nExperimental evidence supports the effectiveness of the proposed scoring system\nin achieving the optimal welfare of stakeholders and in balancing the needs for\ninterpretability, fairness, and efficiency.",
    "published_date": "2021-09-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ME"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.10053v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.09946v1",
    "title": "Identifying biases in legal data: An algorithmic fairness perspective",
    "authors": [
      "Jackson Sargent",
      "Melanie Weber"
    ],
    "author_ids": [],
    "abstract": "The need to address representation biases and sentencing disparities in legal\ncase data has long been recognized. Here, we study the problem of identifying\nand measuring biases in large-scale legal case data from an algorithmic\nfairness perspective. Our approach utilizes two regression models: A baseline\nthat represents the decisions of a \"typical\" judge as given by the data and a\n\"fair\" judge that applies one of three fairness concepts. Comparing the\ndecisions of the \"typical\" judge and the \"fair\" judge allows for quantifying\nbiases across demographic groups, as we demonstrate in four case studies on\ncriminal data from Cook County (Illinois).",
    "published_date": "2021-09-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "stat.ML",
      "K.4; K.5"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.09946v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2110.09272v1",
    "title": "Multi-Objective Allocation of COVID-19 Testing Centers: Improving Coverage and Equity in Access",
    "authors": [
      "Zhen Zhong",
      "Ribhu Sengupta",
      "Kamran Paynabar",
      "Lance A. Waller"
    ],
    "author_ids": [],
    "abstract": "At the time of this article, COVID-19 has been transmitted to more than 42\nmillion people and resulted in more than 673,000 deaths across the United\nStates. Throughout this pandemic, public health authorities have monitored the\nresults of diagnostic testing to identify hotspots of transmission. Such\ninformation can help reduce or block transmission paths of COVID-19 and help\ninfected patients receive early treatment. However, most current schemes of\ntest site allocation have been based on experience or convenience, often\nresulting in low efficiency and non-optimal allocation. In addition, the\nhistorical sociodemographic patterns of populations within cities can result in\nmeasurable inequities in access to testing between various racial and income\ngroups. To address these pressing issues, we propose a novel test site\nallocation scheme to (a) maximize population coverage, (b) minimize prediction\nuncertainties associated with projections of outbreak trajectories, and (c)\nreduce inequities in access. We illustrate our approach with case studies\ncomparing our allocation scheme with recorded allocation of testing sites in\nGeorgia, revealing increases in both population coverage and improvements in\nequity of access over current practice.",
    "published_date": "2021-09-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "math.OC",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.09272v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2109.09943v2",
    "title": "Identifiability of Chemical Reaction Networks with Intrinsic and Extrinsic Noise from Stationary Distributions",
    "authors": [
      "Theodore W. Grunberg",
      "Domitilla Del Vecchio"
    ],
    "author_ids": [],
    "abstract": "Many biological systems can be modeled as a chemical reaction network with\nunknown parameters. Data available to identify these parameters are often in\nthe form of a stationary distribution, such as that obtained from measurements\nof a cell population. In this work, we introduce a framework for analyzing the\nidentifiability of the reaction rate coefficients of chemical reaction networks\nfrom stationary distribution data. Working with the linear noise approximation,\nwhich is a diffusive approximation to the chemical master equation, we give a\ncomputational procedure to certify global identifiability based on Hilbert's\nNullstellensatz. We present a variety of examples that show the applicability\nof our method to chemical reaction networks of interest in systems and\nsynthetic biology, including discrimination between possible molecular\nmechanisms for the interaction between biochemical species.",
    "published_date": "2021-09-21T00:00:00",
    "year": 2021,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.09943v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2109.11344v1",
    "title": "Causal Inference in Network Economics",
    "authors": [
      "Sridhar Mahadevan"
    ],
    "author_ids": [],
    "abstract": "Network economics is the study of a rich class of equilibrium problems that\noccur in the real world, from traffic management to supply chains and two-sided\nonline marketplaces. In this paper we explore causal inference in network\neconomics, building on the mathematical framework of variational inequalities,\nwhich is a generalization of classical optimization. Our framework can be\nviewed as a synthesis of the well-known variational inequality formalism with\nthe broad principles of causal inference",
    "published_date": "2021-09-20T00:00:00",
    "year": 2021,
    "categories": [
      "math.OC",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.11344v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.09725v4",
    "title": "Model Bias in NLP -- Application to Hate Speech Classification using transfer learning techniques",
    "authors": [
      "Aygul Zagidullina",
      "Georgios Patoulidis",
      "Jonas Bokstaller"
    ],
    "author_ids": [],
    "abstract": "In this paper, a BERT based neural network model is applied to the JIGSAW\ndata set in order to create a model identifying hateful and toxic comments\n(strictly seperated from offensive language) in online social platforms\n(English language), in this case Twitter. Three other neural network\narchitectures and a GPT-2 model are also applied on the provided data set in\norder to compare these different models. The trained BERT model is then applied\non two different data sets to evaluate its generalisation power, namely on\nanother Twitter data set and the data set HASOC 2019 which includes Twitter and\nalso Facebook comments; we focus on the English HASOC 2019 data. In addition,\nit can be shown that by fine-tuning the trained BERT model on these two data\nsets by applying different transfer learning scenarios via retraining partial\nor all layers the predictive scores improve compared to simply applying the\nmodel pre-trained on the JIGSAW data set. With our results, we get precisions\nfrom 64% to around 90% while still achieving acceptable recall values of at\nleast lower 60s%, proving that BERT is suitable for real use cases in social\nplatforms.",
    "published_date": "2021-09-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.09725v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.09672v1",
    "title": "Actionable Approaches to Promote Ethical AI in Libraries",
    "authors": [
      "Helen Bubinger",
      "Jesse David Dinneen"
    ],
    "author_ids": [],
    "abstract": "The widespread use of artificial intelligence (AI) in many domains has\nrevealed numerous ethical issues from data and design to deployment. In\nresponse, countless broad principles and guidelines for ethical AI have been\npublished, and following those, specific approaches have been proposed for how\nto encourage ethical outcomes of AI. Meanwhile, library and information\nservices too are seeing an increase in the use of AI-powered and machine\nlearning-powered information systems, but no practical guidance currently\nexists for libraries to plan for, evaluate, or audit the ethics of intended or\ndeployed AI. We therefore report on several promising approaches for promoting\nethical AI that can be adapted from other contexts to AI-powered information\nservices and in different stages of the software lifecycle.",
    "published_date": "2021-09-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.DL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.09672v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.09658v6",
    "title": "FUTURE-AI: Guiding Principles and Consensus Recommendations for Trustworthy Artificial Intelligence in Medical Imaging",
    "authors": [
      "Karim Lekadir",
      "Richard Osuala",
      "Catherine Gallin",
      "Noussair Lazrak",
      "Kaisar Kushibar",
      "Gianna Tsakou",
      "Susanna Aussó",
      "Leonor Cerdá Alberich",
      "Kostas Marias",
      "Manolis Tsiknakis",
      "Sara Colantonio",
      "Nickolas Papanikolaou",
      "Zohaib Salahuddin",
      "Henry C Woodruff",
      "Philippe Lambin",
      "Luis Martí-Bonmatí"
    ],
    "author_ids": [],
    "abstract": "The recent advancements in artificial intelligence (AI) combined with the\nextensive amount of data generated by today's clinical systems, has led to the\ndevelopment of imaging AI solutions across the whole value chain of medical\nimaging, including image reconstruction, medical image segmentation,\nimage-based diagnosis and treatment planning. Notwithstanding the successes and\nfuture potential of AI in medical imaging, many stakeholders are concerned of\nthe potential risks and ethical implications of imaging AI solutions, which are\nperceived as complex, opaque, and difficult to comprehend, utilise, and trust\nin critical clinical applications. Addressing these concerns and risks, the\nFUTURE-AI framework has been proposed, which, sourced from a global\nmulti-domain expert consensus, comprises guiding principles for increased\ntrust, safety, and adoption for AI in healthcare. In this paper, we transform\nthe general FUTURE-AI healthcare principles to a concise and specific AI\nimplementation guide tailored to the needs of the medical imaging community. To\nthis end, we carefully assess each building block of the FUTURE-AI framework\nconsisting of (i) Fairness, (ii) Universality, (iii) Traceability, (iv)\nUsability, (v) Robustness and (vi) Explainability, and respectively define\nconcrete best practices based on accumulated AI implementation experiences from\nfive large European projects on AI in Health Imaging. We accompany our concrete\nstep-by-step medical imaging development guide with a practical AI solution\nmaturity checklist, thus enabling AI development teams to design, evaluate,\nmaintain, and deploy technically, clinically and ethically trustworthy imaging\nAI solutions into clinical practice.",
    "published_date": "2021-09-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.09658v6",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.09649v1",
    "title": "Data Augmentation Through Monte Carlo Arithmetic Leads to More Generalizable Classification in Connectomics",
    "authors": [
      "Gregory Kiar",
      "Yohan Chatelain",
      "Ali Salari",
      "Alan C. Evans",
      "Tristan Glatard"
    ],
    "author_ids": [],
    "abstract": "Machine learning models are commonly applied to human brain imaging datasets\nin an effort to associate function or structure with behaviour, health, or\nother individual phenotypes. Such models often rely on low-dimensional maps\ngenerated by complex processing pipelines. However, the numerical instabilities\ninherent to pipelines limit the fidelity of these maps and introduce\ncomputational bias. Monte Carlo Arithmetic, a technique for introducing\ncontrolled amounts of numerical noise, was used to perturb a structural\nconnectome estimation pipeline, ultimately producing a range of plausible\nnetworks for each sample. The variability in the perturbed networks was\ncaptured in an augmented dataset, which was then used for an age classification\ntask. We found that resampling brain networks across a series of such\nnumerically perturbed outcomes led to improved performance in all tested\nclassifiers, preprocessing strategies, and dimensionality reduction techniques.\nImportantly, we find that this benefit does not hinge on a large number of\nperturbations, suggesting that even minimally perturbing a dataset adds\nmeaningful variance which can be captured in the subsequently designed models.",
    "published_date": "2021-09-20T00:00:00",
    "year": 2021,
    "categories": [
      "q-bio.QM",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.09649v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.09586v1",
    "title": "Some Critical and Ethical Perspectives on the Empirical Turn of AI Interpretability",
    "authors": [
      "Jean-Marie John-Mathews"
    ],
    "author_ids": [],
    "abstract": "We consider two fundamental and related issues currently faced by Artificial\nIntelligence (AI) development: the lack of ethics and interpretability of AI\ndecisions. Can interpretable AI decisions help to address ethics in AI? Using a\nrandomized study, we experimentally show that the empirical and liberal turn of\nthe production of explanations tends to select AI explanations with a low\ndenunciatory power. Under certain conditions, interpretability tools are\ntherefore not means but, paradoxically, obstacles to the production of ethical\nAI since they can give the illusion of being sensitive to ethical incidents. We\nalso show that the denunciatory power of AI explanations is highly dependent on\nthe context in which the explanation takes place, such as the gender or\neducation level of the person to whom the explication is intended for. AI\nethics tools are therefore sometimes too flexible and self-regulation through\nthe liberal production of explanations do not seem to be enough to address\nethical issues. We then propose two scenarios for the future development of\nethical AI: more external regulation or more liberalization of AI explanations.\nThese two opposite paths will play a major role on the future development of\nethical AI.",
    "published_date": "2021-09-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.09586v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.09523v1",
    "title": "Solving systems of inequalities in two variables with floating point arithmetic",
    "authors": [
      "Walter F. Mascarenhas"
    ],
    "author_ids": [],
    "abstract": "From a theoretical point of view, finding the solution set of a system of\ninequalities in only two variables is easy. However, if we want to get rigorous\nbounds on this set with floating point arithmetic, in all possible cases, then\nthings are not so simple due to rounding errors. In this article we describe in\ndetail an efficient data structure to represent this solution set and an\nefficient and robust algorithm to build it using floating point arithmetic. The\ndata structure and the algorithm were developed as a building block for the\nrigorous solution of relevant practical problems. They were implemented in\n\\texttt{C++} and the code was carefully tested. This code is available as\nsupplementary material to the arxiv version of this article, and it is\ndistributed under the Mozilla Public License 2.0.",
    "published_date": "2021-09-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.09523v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2109.09447v2",
    "title": "Algorithmic Fairness Verification with Graphical Models",
    "authors": [
      "Bishwamittra Ghosh",
      "Debabrota Basu",
      "Kuldeep S. Meel"
    ],
    "author_ids": [],
    "abstract": "In recent years, machine learning (ML) algorithms have been deployed in\nsafety-critical and high-stake decision-making, where the fairness of\nalgorithms is of paramount importance. Fairness in ML centers on detecting bias\ntowards certain demographic populations induced by an ML classifier and\nproposes algorithmic solutions to mitigate the bias with respect to different\nfairness definitions. To this end, several fairness verifiers have been\nproposed that compute the bias in the prediction of an ML\nclassifier--essentially beyond a finite dataset--given the probability\ndistribution of input features. In the context of verifying linear classifiers,\nexisting fairness verifiers are limited by accuracy due to imprecise modeling\nof correlations among features and scalability due to restrictive formulations\nof the classifiers as SSAT/SMT formulas or by sampling.\n  In this paper, we propose an efficient fairness verifier, called FVGM, that\nencodes the correlations among features as a Bayesian network. In contrast to\nexisting verifiers, FVGM proposes a stochastic subset-sum based approach for\nverifying linear classifiers. Experimentally, we show that FVGM leads to an\naccurate and scalable assessment for more diverse families of\nfairness-enhancing algorithms, fairness attacks, and group/causal fairness\nmetrics than the state-of-the-art fairness verifiers. We also demonstrate that\nFVGM facilitates the computation of fairness influence functions as a stepping\nstone to detect the source of bias induced by subsets of features.",
    "published_date": "2021-09-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.09447v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.09417v1",
    "title": "Barely Biased Learning for Gaussian Process Regression",
    "authors": [
      "David R. Burt",
      "Artem Artemev",
      "Mark van der Wilk"
    ],
    "author_ids": [],
    "abstract": "Recent work in scalable approximate Gaussian process regression has discussed\na bias-variance-computation trade-off when estimating the log marginal\nlikelihood. We suggest a method that adaptively selects the amount of\ncomputation to use when estimating the log marginal likelihood so that the bias\nof the objective function is guaranteed to be small. While simple in principle,\nour current implementation of the method is not competitive computationally\nwith existing approximations.",
    "published_date": "2021-09-20T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.09417v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.09416v4",
    "title": "ElasticFace: Elastic Margin Loss for Deep Face Recognition",
    "authors": [
      "Fadi Boutros",
      "Naser Damer",
      "Florian Kirchbuchner",
      "Arjan Kuijper"
    ],
    "author_ids": [],
    "abstract": "Learning discriminative face features plays a major role in building\nhigh-performing face recognition models. The recent state-of-the-art face\nrecognition solutions proposed to incorporate a fixed penalty margin on\ncommonly used classification loss function, softmax loss, in the normalized\nhypersphere to increase the discriminative power of face recognition models, by\nminimizing the intra-class variation and maximizing the inter-class variation.\nMarginal penalty softmax losses, such as ArcFace and CosFace, assume that the\ngeodesic distance between and within the different identities can be equally\nlearned using a fixed penalty margin. However, such a learning objective is not\nrealistic for real data with inconsistent inter-and intra-class variation,\nwhich might limit the discriminative and generalizability of the face\nrecognition model. In this paper, we relax the fixed penalty margin constrain\nby proposing elastic penalty margin loss (ElasticFace) that allows flexibility\nin the push for class separability. The main idea is to utilize random margin\nvalues drawn from a normal distribution in each training iteration. This aims\nat giving the decision boundary chances to extract and retract to allow space\nfor flexible class separability learning. We demonstrate the superiority of our\nElasticFace loss over ArcFace and CosFace losses, using the same geometric\ntransformation, on a large set of mainstream benchmarks. From a wider\nperspective, our ElasticFace has advanced the state-of-the-art face recognition\nperformance on seven out of nine mainstream benchmarks.",
    "published_date": "2021-09-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.09416v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.09320v1",
    "title": "Robust Physical-World Attacks on Face Recognition",
    "authors": [
      "Xin Zheng",
      "Yanbo Fan",
      "Baoyuan Wu",
      "Yong Zhang",
      "Jue Wang",
      "Shirui Pan"
    ],
    "author_ids": [],
    "abstract": "Face recognition has been greatly facilitated by the development of deep\nneural networks (DNNs) and has been widely applied to many safety-critical\napplications. However, recent studies have shown that DNNs are very vulnerable\nto adversarial examples, raising serious concerns on the security of real-world\nface recognition. In this work, we study sticker-based physical attacks on face\nrecognition for better understanding its adversarial robustness. To this end,\nwe first analyze in-depth the complicated physical-world conditions confronted\nby attacking face recognition, including the different variations of stickers,\nfaces, and environmental conditions. Then, we propose a novel robust physical\nattack framework, dubbed PadvFace, to model these challenging variations\nspecifically. Furthermore, considering the difference in attack complexity, we\npropose an efficient Curriculum Adversarial Attack (CAA) algorithm that\ngradually adapts adversarial stickers to environmental variations from easy to\ncomplex. Finally, we construct a standardized testing protocol to facilitate\nthe fair evaluation of physical attacks on face recognition, and extensive\nexperiments on both dodging and impersonation attacks demonstrate the superior\nperformance of the proposed method.",
    "published_date": "2021-09-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.09320v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.09248v3",
    "title": "Wages and Utilities in a Closed Economy",
    "authors": [
      "Sanyukta Deshpande",
      "Milind A. Sohoni"
    ],
    "author_ids": [],
    "abstract": "The broad objective of this paper is to propose a mathematical model for the\nstudy of causes of wage inequality and relate it to choices of consumption, the\ntechnologies of production, and the composition of labor in an economy. The\npaper constructs a Simple Closed Model, or an SCM, for short, for closed\neconomies, in which the consumption and the production parts are clearly\nseparated and yet coupled. The model is established as a specialization of the\nArrow-Debreu model and its equilibria correspond directly with those of the\ngeneral Arrow-Debreu model. The formulation allows us to identify the\ncombinatorial data which link parameters of the economic system with its\nequilibria, in particular, the impact of consumer preferences on wages. The SCM\nmodel also allows the formulation and explicit construction of the consumer\nchoice game, where expressed utilities of various labor classes serve as\nstrategies with total or relative wages as the pay-offs. We illustrate, through\nexamples, the mathematical details of the consumer choice game. We show that\nconsumer preferences, expressed through modified utility functions, do indeed\npercolate through the economy, and influence not only prices but also\nproduction and wages. Thus, consumer choice may serve as an effective tool for\nwage redistribution.",
    "published_date": "2021-09-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.09248v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2109.09232v1",
    "title": "UPV at CheckThat! 2021: Mitigating Cultural Differences for Identifying Multilingual Check-worthy Claims",
    "authors": [
      "Ipek Baris Schlicht",
      "Angel Felipe Magnossão de Paula",
      "Paolo Rosso"
    ],
    "author_ids": [],
    "abstract": "Identifying check-worthy claims is often the first step of automated\nfact-checking systems. Tackling this task in a multilingual setting has been\nunderstudied. Encoding inputs with multilingual text representations could be\none approach to solve the multilingual check-worthiness detection. However,\nthis approach could suffer if cultural bias exists within the communities on\ndetermining what is check-worthy.In this paper, we propose a language\nidentification task as an auxiliary task to mitigate unintended bias.With this\npurpose, we experiment joint training by using the datasets from CLEF-2021\nCheckThat!, that contain tweets in English, Arabic, Bulgarian, Spanish and\nTurkish. Our results show that joint training of language identification and\ncheck-worthy claim detection tasks can provide performance gains for some of\nthe selected languages.",
    "published_date": "2021-09-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.LG",
      "I.7; J.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.09232v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.09088v3",
    "title": "Relaxed Conditions for Parameterized Linear Matrix Inequality in the Form of Double Sum",
    "authors": [
      "Do Wan Kim",
      "Dong Hwan Lee"
    ],
    "author_ids": [],
    "abstract": "The aim of this study is to investigate less conservative conditions for a\nparameterized linear matrix inequality (PLMI) expressed in the form of a double\nconvex sum. This type of PLMI frequently appears in T-S fuzzy control system\nanalysis and design problems. In this letter, we derive new, less conservative\nlinear matrix inequalities (LMIs) for the PLMI by employing the proposed sum\nrelaxation method based on Young's inequality. The derived LMIs are proven to\nbe less conservative than the existing conditions related to this topic in the\nliterature. The proposed technique is applicable to various stability analysis\nand control design problems for T-S fuzzy systems, which are formulated as\nsolving the PLMIs in the form of a double convex sum. Furthermore, examples is\nprovided to illustrate the reduced conservatism of the derived LMIs.",
    "published_date": "2021-09-19T00:00:00",
    "year": 2021,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.09088v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2109.09061v1",
    "title": "Model-Based Approach for Measuring the Fairness in ASR",
    "authors": [
      "Zhe Liu",
      "Irina-Elena Veliche",
      "Fuchun Peng"
    ],
    "author_ids": [],
    "abstract": "The issue of fairness arises when the automatic speech recognition (ASR)\nsystems do not perform equally well for all subgroups of the population. In any\nfairness measurement studies for ASR, the open questions of how to control the\nnuisance factors, how to handle unobserved heterogeneity across speakers, and\nhow to trace the source of any word error rate (WER) gap among different\nsubgroups are especially important - if not appropriately accounted for,\nincorrect conclusions will be drawn. In this paper, we introduce mixed-effects\nPoisson regression to better measure and interpret any WER difference among\nsubgroups of interest. Particularly, the presented method can effectively\naddress the three problems raised above and is very flexible to use in\npractical disparity analyses. We demonstrate the validity of proposed\nmodel-based approach on both synthetic and real-world speech data.",
    "published_date": "2021-09-19T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.09061v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.10697v1",
    "title": "Towards Automatic Bias Detection in Knowledge Graphs",
    "authors": [
      "Daphna Keidar",
      "Mian Zhong",
      "Ce Zhang",
      "Yash Raj Shrestha",
      "Bibek Paudel"
    ],
    "author_ids": [],
    "abstract": "With the recent surge in social applications relying on knowledge graphs, the\nneed for techniques to ensure fairness in KG based methods is becoming\nincreasingly evident. Previous works have demonstrated that KGs are prone to\nvarious social biases, and have proposed multiple methods for debiasing them.\nHowever, in such studies, the focus has been on debiasing techniques, while the\nrelations to be debiased are specified manually by the user. As manual\nspecification is itself susceptible to human cognitive bias, there is a need\nfor a system capable of quantifying and exposing biases, that can support more\ninformed decisions on what to debias. To address this gap in the literature, we\ndescribe a framework for identifying biases present in knowledge graph\nembeddings, based on numerical bias metrics. We illustrate the framework with\nthree different bias measures on the task of profession prediction, and it can\nbe flexibly extended to further bias definitions and applications. The\nrelations flagged as biased can then be handed to decision makers for judgement\nupon subsequent debiasing.",
    "published_date": "2021-09-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.10697v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.09046v3",
    "title": "Improving Fairness for Data Valuation in Horizontal Federated Learning",
    "authors": [
      "Zhenan Fan",
      "Huang Fang",
      "Zirui Zhou",
      "Jian Pei",
      "Michael P. Friedlander",
      "Changxin Liu",
      "Yong Zhang"
    ],
    "author_ids": [],
    "abstract": "Federated learning is an emerging decentralized machine learning scheme that\nallows multiple data owners to work collaboratively while ensuring data\nprivacy. The success of federated learning depends largely on the participation\nof data owners. To sustain and encourage data owners' participation, it is\ncrucial to fairly evaluate the quality of the data provided by the data owners\nand reward them correspondingly. Federated Shapley value, recently proposed by\nWang et al. [Federated Learning, 2020], is a measure for data value under the\nframework of federated learning that satisfies many desired properties for data\nvaluation. However, there are still factors of potential unfairness in the\ndesign of federated Shapley value because two data owners with the same local\ndata may not receive the same evaluation. We propose a new measure called\ncompleted federated Shapley value to improve the fairness of federated Shapley\nvalue. The design depends on completing a matrix consisting of all the possible\ncontributions by different subsets of the data owners. It is shown under mild\nconditions that this matrix is approximately low-rank by leveraging concepts\nand tools from optimization. Both theoretical analysis and empirical evaluation\nverify that the proposed measure does improve fairness in many circumstances.",
    "published_date": "2021-09-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.09046v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.09014v1",
    "title": "A Machine Learning Pipeline to Examine Political Bias with Congressional Speeches",
    "authors": [
      "Prasad hajare",
      "Sadia Kamal",
      "Siddharth Krishnan",
      "Arunkumar Bagavathi"
    ],
    "author_ids": [],
    "abstract": "Computational methods to model political bias in social media involve several\nchallenges due to heterogeneity, high-dimensional, multiple modalities, and the\nscale of the data. Political bias in social media has been studied in multiple\nviewpoints like media bias, political ideology, echo chambers, and\ncontroversies using machine learning pipelines. Most of the current methods\nrely heavily on the manually-labeled ground-truth data for the underlying\npolitical bias prediction tasks. Limitations of such methods include\nhuman-intensive labeling, labels related to only a specific problem, and the\ninability to determine the near future bias state of a social media\nconversation. In this work, we address such problems and give machine learning\napproaches to study political bias in two ideologically diverse social media\nforums: Gab and Twitter without the availability of human-annotated data. Our\nproposed methods exploit the use of transcripts collected from political\nspeeches in US congress to label the data and achieve the highest accuracy of\n70.5% and 65.1% in Twitter and Gab data respectively to predict political bias.\nWe also present a machine learning approach that combines features from\ncascades and text to forecast cascade's political bias with an accuracy of\nabout 85%.",
    "published_date": "2021-09-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.09014v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.08934v2",
    "title": "Fairness Maximization among Offline Agents in Online-Matching Markets",
    "authors": [
      "Will Ma",
      "Pan Xu",
      "Yifan Xu"
    ],
    "author_ids": [],
    "abstract": "Matching markets involve heterogeneous agents (typically from two parties)\nwho are paired for mutual benefit. During the last decade, matching markets\nhave emerged and grown rapidly through the medium of the Internet. They have\nevolved into a new format, called Online Matching Markets (OMMs), with examples\nranging from crowdsourcing to online recommendations to ridesharing. There are\ntwo features distinguishing OMMs from traditional matching markets. One is the\ndynamic arrival of one side of the market: we refer to these as online agents\nwhile the rest are offline agents. Examples of online and offline agents\ninclude keywords (online) and sponsors (offline) in Google Advertising; workers\n(online) and tasks (offline) in Amazon Mechanical Turk (AMT); riders (online)\nand drivers (offline when restricted to a short time window) in ridesharing.\nThe second distinguishing feature of OMMs is the real-time decision-making\nelement. However, studies have shown that the algorithms making decisions in\nthese OMMs leave disparities in the match rates of offline agents. For example,\ntasks in neighborhoods of low socioeconomic status rarely get matched to gig\nworkers, and drivers of certain races/genders get discriminated against in\nmatchmaking. In this paper, we propose online matching algorithms which\noptimize for either individual or group-level fairness among offline agents in\nOMMs. We present two linear-programming (LP) based sampling algorithms, which\nachieve online competitive ratios at least 0.725 for individual fairness\nmaximization (IFM) and 0.719 for group fairness maximization (GFM),\nrespectively. We conduct extensive numerical experiments and results show that\nour boosted version of sampling algorithms are not only conceptually easy to\nimplement but also highly effective in practical instances of\nfairness-maximization-related models.",
    "published_date": "2021-09-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.08934v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.08843v1",
    "title": "Memory Regulation and Alignment toward Generalizer RGB-Infrared Person",
    "authors": [
      "Feng Chen",
      "Fei Wu",
      "Qi Wu",
      "Zhiguo Wan"
    ],
    "author_ids": [],
    "abstract": "The domain shift, coming from unneglectable modality gap and non-overlapped\nidentity classes between training and test sets, is a major issue of\nRGB-Infrared person re-identification. A key to tackle the inherent issue --\ndomain shift -- is to enforce the data distributions of the two domains to be\nsimilar. However, RGB-IR ReID always demands discriminative features, leading\nto over-rely feature sensitivity of seen classes, \\textit{e.g.}, via\nattention-based feature alignment or metric learning. Therefore, predicting the\nunseen query category from predefined training classes may not be accurate and\nleads to a sub-optimal adversarial gradient. In this paper, we uncover it in a\nmore explainable way and propose a novel multi-granularity memory regulation\nand alignment module (MG-MRA) to solve this issue. By explicitly incorporating\na latent variable attribute, from fine-grained to coarse semantic granularity,\ninto intermediate features, our method could alleviate the over-confidence of\nthe model about discriminative features of seen classes. Moreover, instead of\nmatching discriminative features by traversing nearest neighbor, sparse\nattributes, \\textit{i.e.}, global structural pattern, are recollected with\nrespect to features and assigned to measure pair-wise image similarity in\nhashing. Extensive experiments on RegDB \\cite{RegDB} and SYSU-MM01 \\cite{SYSU}\nshow the superiority of the proposed method that outperforms existing\nstate-of-the-art methods. Our code is available in\nhttps://github.com/Chenfeng1271/MGMRA.",
    "published_date": "2021-09-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.08843v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.08792v4",
    "title": "Learning to be Fair: A Consequentialist Approach to Equitable Decision-Making",
    "authors": [
      "Alex Chohlas-Wood",
      "Madison Coots",
      "Henry Zhu",
      "Emma Brunskill",
      "Sharad Goel"
    ],
    "author_ids": [],
    "abstract": "In an attempt to make algorithms fair, the machine learning literature has\nlargely focused on equalizing decisions, outcomes, or error rates across race\nor gender groups. To illustrate, consider a hypothetical government rideshare\nprogram that provides transportation assistance to low-income people with\nupcoming court dates. Following this literature, one might allocate rides to\nthose with the highest estimated treatment effect per dollar, while\nconstraining spending to be equal across race groups. That approach, however,\nignores the downstream consequences of such constraints, and, as a result, can\ninduce unexpected harms. For instance, if one demographic group lives farther\nfrom court, enforcing equal spending would necessarily mean fewer total rides\nprovided, and potentially more people penalized for missing court. Here we\npresent an alternative framework for designing equitable algorithms that\nforegrounds the consequences of decisions. In our approach, one first elicits\nstakeholder preferences over the space of possible decisions and the resulting\noutcomes--such as preferences for balancing spending parity against court\nappearance rates. We then optimize over the space of decision policies, making\ntrade-offs in a way that maximizes the elicited utility. To do so, we develop\nan algorithm for efficiently learning these optimal policies from data for a\nlarge family of expressive utility functions. In particular, we use a\ncontextual bandit algorithm to explore the space of policies while solving a\nconvex optimization problem at each step to estimate the best policy based on\nthe available information. This consequentialist paradigm facilitates a more\nholistic approach to equitable decision-making.",
    "published_date": "2021-09-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.08792v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.08705v1",
    "title": "Relating Neural Text Degeneration to Exposure Bias",
    "authors": [
      "Ting-Rui Chiang",
      "Yun-Nung Chen"
    ],
    "author_ids": [],
    "abstract": "This work focuses on relating two mysteries in neural-based text generation:\nexposure bias, and text degeneration. Despite the long time since exposure bias\nwas mentioned and the numerous studies for its remedy, to our knowledge, its\nimpact on text generation has not yet been verified. Text degeneration is a\nproblem that the widely-used pre-trained language model GPT-2 was recently\nfound to suffer from (Holtzman et al., 2020). Motivated by the unknown\ncausation of the text degeneration, in this paper we attempt to relate these\ntwo mysteries. Specifically, we first qualitatively quantitatively identify\nmistakes made before text degeneration occurs. Then we investigate the\nsignificance of the mistakes by inspecting the hidden states in GPT-2. Our\nresults show that text degeneration is likely to be partly caused by exposure\nbias. We also study the self-reinforcing mechanism of text degeneration,\nexplaining why the mistakes amplify. In sum, our study provides a more concrete\nfoundation for further investigation on exposure bias and text degeneration\nproblems.",
    "published_date": "2021-09-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.08705v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.08654v1",
    "title": "Nonlinear Deterministic Filter for Inertial Navigation and Bias Estimation with Guaranteed Performance",
    "authors": [
      "Ajay Singh Ludher",
      "Marium Tawhid",
      "Hashim A. Hashim"
    ],
    "author_ids": [],
    "abstract": "Unmanned vehicle navigation concerns estimating attitude, position, and\nlinear velocity of the vehicle the six degrees of freedom (6 DoF). It has been\nknown that the true navigation dynamics are highly nonlinear modeled on the Lie\nGroup of $\\mathbb{SE}_{2}(3)$. In this paper, a nonlinear filter for inertial\nnavigation is proposed. The filter ensures systematic convergence of the error\ncomponents starting from almost any initial condition. Also, the errors\nconverge asymptotically to the origin. Experimental results validates the\nrobustness of the proposed filter.",
    "published_date": "2021-09-17T00:00:00",
    "year": 2021,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.08654v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2109.08644v2",
    "title": "Allocating Indivisible Goods to Strategic Agents: Pure Nash Equilibria and Fairness",
    "authors": [
      "Georgios Amanatidis",
      "Georgios Birmpas",
      "Federico Fusco",
      "Philip Lazos",
      "Stefano Leonardi",
      "Rebecca Reiffenhäuser"
    ],
    "author_ids": [],
    "abstract": "We consider the problem of fairly allocating a set of indivisible goods to a\nset of strategic agents with additive valuation functions. We assume no\nmonetary transfers and, therefore, a mechanism in our setting is an algorithm\nthat takes as input the reported -- rather than the true -- values of the\nagents. Our main goal is to explore whether there exist mechanisms that have\npure Nash equilibria for every instance and, at the same time, provide fairness\nguarantees for the allocations that correspond to these equilibria. We focus on\ntwo relaxations of envy-freeness, namely envy-freeness up to one good (EF1),\nand envy-freeness up to any good (EFX), and we positively answer the above\nquestion. In particular, we study two algorithms that are known to produce such\nallocations in the non-strategic setting: Round-Robin (EF1 allocations for any\nnumber of agents) and a cut-and-choose algorithm of Plaut and Roughgarden [SIAM\nJournal of Discrete Mathematics, 2020] (EFX allocations for two agents). For\nRound-Robin we show that all of its pure Nash equilibria induce allocations\nthat are EF1 with respect to the underlying true values, while for the\nalgorithm of Plaut and Roughgarden we show that the corresponding allocations\nnot only are EFX but also satisfy maximin share fairness, something that is not\ntrue for this algorithm in the non-strategic setting! Further, we show that a\nweaker version of the latter result holds for any mechanism for two agents that\nalways has pure Nash equilibria which all induce EFX allocations.",
    "published_date": "2021-09-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.DM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.08644v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.08630v1",
    "title": "A Fairness Analysis on Private Aggregation of Teacher Ensembles",
    "authors": [
      "Cuong Tran",
      "My H. Dinh",
      "Kyle Beiter",
      "Ferdinando Fioretto"
    ],
    "author_ids": [],
    "abstract": "The Private Aggregation of Teacher Ensembles (PATE) is an important private\nmachine learning framework. It combines multiple learning models used as\nteachers for a student model that learns to predict an output chosen by noisy\nvoting among the teachers. The resulting model satisfies differential privacy\nand has been shown effective in learning high-quality private models in\nsemisupervised settings or when one wishes to protect the data labels.\n  This paper asks whether this privacy-preserving framework introduces or\nexacerbates bias and unfairness and shows that PATE can introduce accuracy\ndisparity among individuals and groups of individuals. The paper analyzes which\nalgorithmic and data properties are responsible for the disproportionate\nimpacts, why these aspects are affecting different groups disproportionately,\nand proposes guidelines to mitigate these effects. The proposed approach is\nevaluated on several datasets and settings.",
    "published_date": "2021-09-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.08630v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.08613v1",
    "title": "Adversarial Scrubbing of Demographic Information for Text Classification",
    "authors": [
      "Somnath Basu Roy Chowdhury",
      "Sayan Ghosh",
      "Yiyuan Li",
      "Junier B. Oliva",
      "Shashank Srivastava",
      "Snigdha Chaturvedi"
    ],
    "author_ids": [],
    "abstract": "Contextual representations learned by language models can often encode\nundesirable attributes, like demographic associations of the users, while being\ntrained for an unrelated target task. We aim to scrub such undesirable\nattributes and learn fair representations while maintaining performance on the\ntarget task. In this paper, we present an adversarial learning framework\n\"Adversarial Scrubber\" (ADS), to debias contextual representations. We perform\ntheoretical analysis to show that our framework converges without leaking\ndemographic information under certain conditions. We extend previous evaluation\ntechniques by evaluating debiasing performance using Minimum Description Length\n(MDL) probing. Experimental evaluations on 8 datasets show that ADS generates\nrepresentations with minimal information about demographic attributes while\nbeing maximally informative about the target task.",
    "published_date": "2021-09-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.08613v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.08604v2",
    "title": "Enforcing fairness in private federated learning via the modified method of differential multipliers",
    "authors": [
      "Borja Rodríguez-Gálvez",
      "Filip Granqvist",
      "Rogier van Dalen",
      "Matt Seigel"
    ],
    "author_ids": [],
    "abstract": "Federated learning with differential privacy, or private federated learning,\nprovides a strategy to train machine learning models while respecting users'\nprivacy. However, differential privacy can disproportionately degrade the\nperformance of the models on under-represented groups, as these parts of the\ndistribution are difficult to learn in the presence of noise. Existing\napproaches for enforcing fairness in machine learning models have considered\nthe centralized setting, in which the algorithm has access to the users' data.\nThis paper introduces an algorithm to enforce group fairness in private\nfederated learning, where users' data does not leave their devices. First, the\npaper extends the modified method of differential multipliers to empirical risk\nminimization with fairness constraints, thus providing an algorithm to enforce\nfairness in the central setting. Then, this algorithm is extended to the\nprivate federated learning setting. The proposed algorithm, \\texttt{FPFL}, is\ntested on a federated version of the Adult dataset and an \"unfair\" version of\nthe FEMNIST dataset. The experiments on these datasets show how private\nfederated learning accentuates unfairness in the trained models, and how FPFL\nis able to mitigate such unfairness.",
    "published_date": "2021-09-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.08604v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.08549v5",
    "title": "Measuring Fairness Under Unawareness of Sensitive Attributes: A Quantification-Based Approach",
    "authors": [
      "Alessandro Fabris",
      "Andrea Esuli",
      "Alejandro Moreo",
      "Fabrizio Sebastiani"
    ],
    "author_ids": [],
    "abstract": "Algorithms and models are increasingly deployed to inform decisions about\npeople, inevitably affecting their lives. As a consequence, those in charge of\ndeveloping these models must carefully evaluate their impact on different\ngroups of people and favour group fairness, that is, ensure that groups\ndetermined by sensitive demographic attributes, such as race or sex, are not\ntreated unjustly. To achieve this goal, the availability (awareness) of these\ndemographic attributes to those evaluating the impact of these models is\nfundamental. Unfortunately, collecting and storing these attributes is often in\nconflict with industry practices and legislation on data minimisation and\nprivacy. For this reason, it can be hard to measure the group fairness of\ntrained models, even from within the companies developing them. In this work,\nwe tackle the problem of measuring group fairness under unawareness of\nsensitive attributes, by using techniques from quantification, a supervised\nlearning task concerned with directly providing group-level prevalence\nestimates (rather than individual-level class labels). We show that\nquantification approaches are particularly suited to tackle the\nfairness-under-unawareness problem, as they are robust to inevitable\ndistribution shifts while at the same time decoupling the (desirable) objective\nof measuring group fairness from the (undesirable) side effect of allowing the\ninference of sensitive attributes of individuals. More in detail, we show that\nfairness under unawareness can be cast as a quantification problem and solved\nwith proven methods from the quantification literature. We show that these\nmethods outperform previous approaches to measure demographic parity in five\nexperimental protocols, corresponding to important challenges that complicate\nthe estimation of classifier fairness under unawareness.",
    "published_date": "2021-09-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.08549v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.08344v3",
    "title": "Achieving Model Fairness in Vertical Federated Learning",
    "authors": [
      "Changxin Liu",
      "Zhenan Fan",
      "Zirui Zhou",
      "Yang Shi",
      "Jian Pei",
      "Lingyang Chu",
      "Yong Zhang"
    ],
    "author_ids": [],
    "abstract": "Vertical federated learning (VFL) has attracted greater and greater interest\nsince it enables multiple parties possessing non-overlapping features to\nstrengthen their machine learning models without disclosing their private data\nand model parameters. Similar to other machine learning algorithms, VFL faces\ndemands and challenges of fairness, i.e., the learned model may be unfairly\ndiscriminatory over some groups with sensitive attributes. To tackle this\nproblem, we propose a fair VFL framework in this work. First, we systematically\nformulate the problem of training fair models in VFL, where the learning task\nis modelled as a constrained optimization problem. To solve it in a federated\nand privacy-preserving manner, we consider the equivalent dual form of the\nproblem and develop an asynchronous gradient coordinate-descent ascent\nalgorithm, where some active data parties perform multiple parallelized local\nupdates per communication round to effectively reduce the number of\ncommunication rounds. The messages that the server sends to passive parties are\ndeliberately designed such that the information necessary for local updates is\nreleased without intruding on the privacy of data and sensitive attributes. We\nrigorously study the convergence of the algorithm when applied to general\nnonconvex-concave min-max problems. We prove that the algorithm finds a\n$\\delta$-stationary point of the dual objective in $\\mathcal{O}(\\delta^{-4})$\ncommunication rounds under mild conditions. Finally, the extensive experiments\non three benchmark datasets demonstrate the superior performance of our method\nin training fair models.",
    "published_date": "2021-09-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.08344v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.08294v1",
    "title": "A Logic-based Multi-agent System for Ethical Monitoring and Evaluation of Dialogues",
    "authors": [
      "Abeer Dyoub",
      "Stefania Costantini",
      "Ivan Letteri",
      "Francesca A. Lisi"
    ],
    "author_ids": [],
    "abstract": "Dialogue Systems are tools designed for various practical purposes concerning\nhuman-machine interaction. These systems should be built on ethical foundations\nbecause their behavior may heavily influence a user (think especially about\nchildren). The primary objective of this paper is to present the architecture\nand prototype implementation of a Multi Agent System (MAS) designed for ethical\nmonitoring and evaluation of a dialogue system. A prototype application, for\nmonitoring and evaluation of chatting agents' (human/artificial) ethical\nbehavior in an online customer service chat point w.r.t their\ninstitution/company's codes of ethics and conduct, is developed and presented.\nFuture work and open issues with this research are discussed.",
    "published_date": "2021-09-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.MA",
      "I.2;I.2.11;I.2.6;I.3.8"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.08294v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.08256v3",
    "title": "Ethics Sheet for Automatic Emotion Recognition and Sentiment Analysis",
    "authors": [
      "Saif M. Mohammad"
    ],
    "author_ids": [],
    "abstract": "The importance and pervasiveness of emotions in our lives makes affective\ncomputing a tremendously important and vibrant line of work. Systems for\nautomatic emotion recognition (AER) and sentiment analysis can be facilitators\nof enormous progress (e.g., in improving public health and commerce) but also\nenablers of great harm (e.g., for suppressing dissidents and manipulating\nvoters). Thus, it is imperative that the affective computing community actively\nengage with the ethical ramifications of their creations. In this paper, I have\nsynthesized and organized information from AI Ethics and Emotion Recognition\nliterature to present fifty ethical considerations relevant to AER. Notably,\nthe sheet fleshes out assumptions hidden in how AER is commonly framed, and in\nthe choices often made regarding the data, method, and evaluation. Special\nattention is paid to the implications of AER on privacy and social groups.\nAlong the way, key recommendations are made for responsible AER. The objective\nof the sheet is to facilitate and encourage more thoughtfulness on why to\nautomate, how to automate, and how to judge success well before the building of\nAER systems. Additionally, the sheet acts as a useful introductory document on\nemotion recognition (complementing survey articles).",
    "published_date": "2021-09-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.08256v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.08253v2",
    "title": "Balancing out Bias: Achieving Fairness Through Balanced Training",
    "authors": [
      "Xudong Han",
      "Timothy Baldwin",
      "Trevor Cohn"
    ],
    "author_ids": [],
    "abstract": "Group bias in natural language processing tasks manifests as disparities in\nsystem error rates across texts authorized by different demographic groups,\ntypically disadvantaging minority groups. Dataset balancing has been shown to\nbe effective at mitigating bias, however existing approaches do not directly\naccount for correlations between author demographics and linguistic variables,\nlimiting their effectiveness. To achieve Equal Opportunity fairness, such as\nequal job opportunity without regard to demographics, this paper introduces a\nsimple, but highly effective, objective for countering bias using balanced\ntraining. We extend the method in the form of a gated model, which incorporates\nprotected attributes as input, and show that it is effective at reducing bias\nin predictions through demographic input perturbation, outperforming all other\nbias mitigation techniques when combined with balanced training.",
    "published_date": "2021-09-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.08253v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.08245v3",
    "title": "The 2021 RecSys Challenge Dataset: Fairness is not optional",
    "authors": [
      "Luca Belli",
      "Alykhan Tejani",
      "Frank Portman",
      "Alexandre Lung-Yut-Fong",
      "Ben Chamberlain",
      "Yuanpu Xie",
      "Kristian Lum",
      "Jonathan Hunt",
      "Michael Bronstein",
      "Vito Walter Anelli",
      "Saikishore Kalloori",
      "Bruce Ferwerda",
      "Wenzhe Shi"
    ],
    "author_ids": [],
    "abstract": "After the success the RecSys 2020 Challenge, we are describing a novel and\nbigger dataset that was released in conjunction with the ACM RecSys Challenge\n2021. This year's dataset is not only bigger (~ 1B data points, a 5 fold\nincrease), but for the first time it take into consideration fairness aspects\nof the challenge. Unlike many static datsets, a lot of effort went into making\nsure that the dataset was synced with the Twitter platform: if a user deleted\ntheir content, the same content would be promptly removed from the dataset too.\nIn this paper, we introduce the dataset and challenge, highlighting some of the\nissues that arise when creating recommender systems at Twitter scale.",
    "published_date": "2021-09-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.08245v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2109.08240v2",
    "title": "Strategic Ranking",
    "authors": [
      "Lydia T. Liu",
      "Nikhil Garg",
      "Christian Borgs"
    ],
    "author_ids": [],
    "abstract": "Strategic classification studies the design of a classifier robust to the\nmanipulation of input by strategic individuals. However, the existing\nliterature does not consider the effect of competition among individuals as\ninduced by the algorithm design. Motivated by constrained allocation settings\nsuch as college admissions, we introduce strategic ranking, in which the\n(designed) individual reward depends on an applicant's post-effort rank in a\nmeasurement of interest. Our results illustrate how competition among\napplicants affects the resulting equilibria and model insights. We analyze how\nvarious ranking reward designs, belonging to a family of step functions, trade\noff applicant, school, and societal utility, as well as how ranking design\ncounters inequities arising from disparate access to resources. In particular,\nwe find that randomization in the reward design can mitigate two measures of\ndisparate impact, welfare gap and access.",
    "published_date": "2021-09-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.08240v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.08131v1",
    "title": "Studying Up Machine Learning Data: Why Talk About Bias When We Mean Power?",
    "authors": [
      "Milagros Miceli",
      "Julian Posada",
      "Tianling Yang"
    ],
    "author_ids": [],
    "abstract": "Research in machine learning (ML) has primarily argued that models trained on\nincomplete or biased datasets can lead to discriminatory outputs. In this\ncommentary, we propose moving the research focus beyond bias-oriented framings\nby adopting a power-aware perspective to \"study up\" ML datasets. This means\naccounting for historical inequities, labor conditions, and epistemological\nstandpoints inscribed in data. We draw on HCI and CSCW work to support our\nargument, critically analyze previous research, and point at two co-existing\nlines of work within our community -- one bias-oriented, the other power-aware.\nThis way, we highlight the need for dialogue and cooperation in three areas:\ndata quality, data work, and data documentation. In the first area, we argue\nthat reducing societal problems to \"bias\" misses the context-based nature of\ndata. In the second one, we highlight the corporate forces and market\nimperatives involved in the labor of data workers that subsequently shape ML\ndatasets. Finally, we propose expanding current transparency-oriented efforts\nin dataset documentation to reflect the social contexts of data design and\nproduction.",
    "published_date": "2021-09-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.08131v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.08107v6",
    "title": "Check-based generation of one-time tables using qutrits",
    "authors": [
      "Li Yu",
      "Xue-Tong Zhang",
      "Fuqun Wang",
      "Chui-Ping Yang"
    ],
    "author_ids": [],
    "abstract": "One-time tables are a class of two-party correlations that can help achieve\ninformation-theoretically secure two-party (interactive) classical or quantum\ncomputation. In this work we propose a bipartite quantum protocol for\ngenerating a simple type of one-time tables (the correlation in the\nPopescu-Rohrlich nonlocal box) with partial security. We then show that by\nrunning many instances of the first protocol and performing checks on some of\nthem, asymptotically information-theoretically secure generation of one-time\ntables can be achieved. The first protocol is adapted from a protocol for\nsemi-honest quantum oblivious transfer, with some changes so that no entangled\nstate needs to be prepared, and the communication involves only one qutrit in\neach direction. We show that some information tradeoffs in the first protocol\nare similar to that in the semi-honest oblivious transfer protocol. We also\nobtain two types of inequalities about guessing probabilities in some protocols\nfor generating one-time tables, from a single type of inequality about guessing\nprobabilities in semi-honest quantum oblivious transfer protocols.",
    "published_date": "2021-09-16T00:00:00",
    "year": 2021,
    "categories": [
      "quant-ph",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.08107v6",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2109.07946v1",
    "title": "Popularity Bias Is Not Always Evil: Disentangling Benign and Harmful Bias for Recommendation",
    "authors": [
      "Zihao Zhao",
      "Jiawei Chen",
      "Sheng Zhou",
      "Xiangnan He",
      "Xuezhi Cao",
      "Fuzheng Zhang",
      "Wei Wu"
    ],
    "author_ids": [],
    "abstract": "Recommender system usually suffers from severe popularity bias -- the\ncollected interaction data usually exhibits quite imbalanced or even\nlong-tailed distribution over items. Such skewed distribution may result from\nthe users' conformity to the group, which deviates from reflecting users' true\npreference. Existing efforts for tackling this issue mainly focus on completely\neliminating popularity bias. However, we argue that not all popularity bias is\nevil. Popularity bias not only results from conformity but also item quality,\nwhich is usually ignored by existing methods. Some items exhibit higher\npopularity as they have intrinsic better property. Blindly removing the\npopularity bias would lose such important signal, and further deteriorate model\nperformance. To sufficiently exploit such important information for\nrecommendation, it is essential to disentangle the benign popularity bias\ncaused by item quality from the harmful popularity bias caused by conformity.\nAlthough important, it is quite challenging as we lack an explicit signal to\ndifferentiate the two factors of popularity bias. In this paper, we propose to\nleverage temporal information as the two factors exhibit quite different\npatterns along the time: item quality revealing item inherent property is\nstable and static while conformity that depends on items' recent clicks is\nhighly time-sensitive. Correspondingly, we further propose a novel Time-aware\nDisEntangled framework (TIDE), where a click is generated from three components\nnamely the static item quality, the dynamic conformity effect, as well as the\nuser-item matching score returned by any recommendation model. Lastly, we\nconduct interventional inference such that the recommendation can benefit from\nthe benign popularity bias while circumvent the harmful one. Extensive\nexperiments on three real-world datasets demonstrated the effectiveness of\nTIDE.",
    "published_date": "2021-09-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.07946v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2109.07835v1",
    "title": "Incentives in Two-sided Matching Markets with Prediction-enhanced Preference-formation",
    "authors": [
      "Stefania Ionescu",
      "Yuhao Du",
      "Kenneth Joseph",
      "Anikó Hannák"
    ],
    "author_ids": [],
    "abstract": "Two-sided matching markets have long existed to pair agents in the absence of\nregulated exchanges. A common example is school choice, where a matching\nmechanism uses student and school preferences to assign students to schools. In\nsuch settings, forming preferences is both difficult and critical. Prior work\nhas suggested various prediction mechanisms that help agents make decisions\nabout their preferences. Although often deployed together, these matching and\nprediction mechanisms are almost always analyzed separately. The present work\nshows that at the intersection of the two lies a previously unexplored type of\nstrategic behavior: agents returning to the market (e.g., schools) can attack\nfuture predictions by interacting short-term non-optimally with their matches.\nHere, we first introduce this type of strategic behavior, which we call an\n`adversarial interaction attack'. Next, we construct a formal economic model\nthat captures the feedback loop between prediction mechanisms designed to\nassist agents and the matching mechanism used to pair them. This economic model\nallows us to analyze adversarial interaction attacks. Finally, using school\nchoice as an example, we build a simulation to show that, as the trust in and\naccuracy of predictions increases, schools gain progressively more by\ninitiating an adversarial interaction attack. We also show that this attack\nincreases inequality in the student population.",
    "published_date": "2021-09-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.07835v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.07785v4",
    "title": "MHFC: Multi-Head Feature Collaboration for Few-Shot Learning",
    "authors": [
      "Shuai Shao",
      "Lei Xing",
      "Yan Wang",
      "Rui Xu",
      "Chunyan Zhao",
      "Yan-Jiang Wang",
      "Bao-Di Liu"
    ],
    "author_ids": [],
    "abstract": "Few-shot learning (FSL) aims to address the data-scarce problem. A standard\nFSL framework is composed of two components: (1) Pre-train. Employ the base\ndata to generate a CNN-based feature extraction model (FEM). (2) Meta-test.\nApply the trained FEM to acquire the novel data's features and recognize them.\nFSL relies heavily on the design of the FEM. However, various FEMs have\ndistinct emphases. For example, several may focus more attention on the contour\ninformation, whereas others may lay particular emphasis on the texture\ninformation. The single-head feature is only a one-sided representation of the\nsample. Besides the negative influence of cross-domain (e.g., the trained FEM\ncan not adapt to the novel class flawlessly), the distribution of novel data\nmay have a certain degree of deviation compared with the ground truth\ndistribution, which is dubbed as distribution-shift-problem (DSP). To address\nthe DSP, we propose Multi-Head Feature Collaboration (MHFC) algorithm, which\nattempts to project the multi-head features (e.g., multiple features extracted\nfrom a variety of FEMs) to a unified space and fuse them to capture more\ndiscriminative information. Typically, first, we introduce a subspace learning\nmethod to transform the multi-head features to aligned low-dimensional\nrepresentations. It corrects the DSP via learning the feature with more\npowerful discrimination and overcomes the problem of inconsistent measurement\nscales from different head features. Then, we design an attention block to\nupdate combination weights for each head feature automatically. It\ncomprehensively considers the contribution of various perspectives and further\nimproves the discrimination of features. We evaluate the proposed method on\nfive benchmark datasets (including cross-domain experiments) and achieve\nsignificant improvements of 2.1%-7.8% compared with state-of-the-arts.",
    "published_date": "2021-09-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.07785v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.07756v1",
    "title": "Dense Semantic Contrast for Self-Supervised Visual Representation Learning",
    "authors": [
      "Xiaoni Li",
      "Yu Zhou",
      "Yifei Zhang",
      "Aoting Zhang",
      "Wei Wang",
      "Ning Jiang",
      "Haiying Wu",
      "Weiping Wang"
    ],
    "author_ids": [],
    "abstract": "Self-supervised representation learning for visual pre-training has achieved\nremarkable success with sample (instance or pixel) discrimination and semantics\ndiscovery of instance, whereas there still exists a non-negligible gap between\npre-trained model and downstream dense prediction tasks. Concretely, these\ndownstream tasks require more accurate representation, in other words, the\npixels from the same object must belong to a shared semantic category, which is\nlacking in the previous methods. In this work, we present Dense Semantic\nContrast (DSC) for modeling semantic category decision boundaries at a dense\nlevel to meet the requirement of these tasks. Furthermore, we propose a dense\ncross-image semantic contrastive learning framework for multi-granularity\nrepresentation learning. Specially, we explicitly explore the semantic\nstructure of the dataset by mining relations among pixels from different\nperspectives. For intra-image relation modeling, we discover pixel neighbors\nfrom multiple views. And for inter-image relations, we enforce pixel\nrepresentation from the same semantic class to be more similar than the\nrepresentation from different classes in one mini-batch. Experimental results\nshow that our DSC model outperforms state-of-the-art methods when transferring\nto downstream dense prediction tasks, including object detection, semantic\nsegmentation, and instance segmentation. Code will be made available.",
    "published_date": "2021-09-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.07756v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.07679v1",
    "title": "Benchmarking Commonsense Knowledge Base Population with an Effective Evaluation Dataset",
    "authors": [
      "Tianqing Fang",
      "Weiqi Wang",
      "Sehyun Choi",
      "Shibo Hao",
      "Hongming Zhang",
      "Yangqiu Song",
      "Bin He"
    ],
    "author_ids": [],
    "abstract": "Reasoning over commonsense knowledge bases (CSKB) whose elements are in the\nform of free-text is an important yet hard task in NLP. While CSKB completion\nonly fills the missing links within the domain of the CSKB, CSKB population is\nalternatively proposed with the goal of reasoning unseen assertions from\nexternal resources. In this task, CSKBs are grounded to a large-scale\neventuality (activity, state, and event) graph to discriminate whether novel\ntriples from the eventuality graph are plausible or not. However, existing\nevaluations on the population task are either not accurate (automatic\nevaluation with randomly sampled negative examples) or of small scale (human\nannotation). In this paper, we benchmark the CSKB population task with a new\nlarge-scale dataset by first aligning four popular CSKBs, and then presenting a\nhigh-quality human-annotated evaluation set to probe neural models' commonsense\nreasoning ability. We also propose a novel inductive commonsense reasoning\nmodel that reasons over graphs. Experimental results show that generalizing\ncommonsense reasoning on unseen assertions is inherently a hard task. Models\nachieving high accuracy during training perform poorly on the evaluation set,\nwith a large gap between human performance. We will make the data publicly\navailable for future contributions. Codes and data are available at\nhttps://github.com/HKUST-KnowComp/CSKB-Population.",
    "published_date": "2021-09-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.07679v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.07445v1",
    "title": "Challenges in Detoxifying Language Models",
    "authors": [
      "Johannes Welbl",
      "Amelia Glaese",
      "Jonathan Uesato",
      "Sumanth Dathathri",
      "John Mellor",
      "Lisa Anne Hendricks",
      "Kirsty Anderson",
      "Pushmeet Kohli",
      "Ben Coppin",
      "Po-Sen Huang"
    ],
    "author_ids": [],
    "abstract": "Large language models (LM) generate remarkably fluent text and can be\nefficiently adapted across NLP tasks. Measuring and guaranteeing the quality of\ngenerated text in terms of safety is imperative for deploying LMs in the real\nworld; to this end, prior work often relies on automatic evaluation of LM\ntoxicity. We critically discuss this approach, evaluate several toxicity\nmitigation strategies with respect to both automatic and human evaluation, and\nanalyze consequences of toxicity mitigation in terms of model bias and LM\nquality. We demonstrate that while basic intervention strategies can\neffectively optimize previously established automatic metrics on the\nRealToxicityPrompts dataset, this comes at the cost of reduced LM coverage for\nboth texts about, and dialects of, marginalized groups. Additionally, we find\nthat human raters often disagree with high automatic toxicity scores after\nstrong toxicity reduction interventions -- highlighting further the nuances\ninvolved in careful evaluation of LM toxicity.",
    "published_date": "2021-09-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "I.2.6; I.2.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.07445v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.07371v1",
    "title": "Self-learn to Explain Siamese Networks Robustly",
    "authors": [
      "Chao Chen",
      "Yifan Shen",
      "Guixiang Ma",
      "Xiangnan Kong",
      "Srinivas Rangarajan",
      "Xi Zhang",
      "Sihong Xie"
    ],
    "author_ids": [],
    "abstract": "Learning to compare two objects are essential in applications, such as\ndigital forensics, face recognition, and brain network analysis, especially\nwhen labeled data is scarce and imbalanced. As these applications make\nhigh-stake decisions and involve societal values like fairness and\ntransparency, it is critical to explain the learned models. We aim to study\npost-hoc explanations of Siamese networks (SN) widely used in learning to\ncompare. We characterize the instability of gradient-based explanations due to\nthe additional compared object in SN, in contrast to architectures with a\nsingle input instance. We propose an optimization framework that derives global\ninvariance from unlabeled data using self-learning to promote the stability of\nlocal explanations tailored for specific query-reference pairs. The\noptimization problems can be solved using gradient descent-ascent (GDA) for\nconstrained optimization, or SGD for KL-divergence regularized unconstrained\noptimization, with convergence proofs, especially when the objective functions\nare nonconvex due to the Siamese architecture. Quantitative results and case\nstudies on tabular and graph data from neuroscience and chemical engineering\nshow that the framework respects the self-learned invariance while robustly\noptimizing the faithfulness and simplicity of the explanation. We further\ndemonstrate the convergence of GDA experimentally.",
    "published_date": "2021-09-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.07371v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.07205v1",
    "title": "A Relation-Oriented Clustering Method for Open Relation Extraction",
    "authors": [
      "Jun Zhao",
      "Tao Gui",
      "Qi Zhang",
      "Yaqian Zhou"
    ],
    "author_ids": [],
    "abstract": "The clustering-based unsupervised relation discovery method has gradually\nbecome one of the important methods of open relation extraction (OpenRE).\nHowever, high-dimensional vectors can encode complex linguistic information\nwhich leads to the problem that the derived clusters cannot explicitly align\nwith the relational semantic classes. In this work, we propose a\nrelation-oriented clustering model and use it to identify the novel relations\nin the unlabeled data. Specifically, to enable the model to learn to cluster\nrelational data, our method leverages the readily available labeled data of\npre-defined relations to learn a relation-oriented representation. We minimize\ndistance between the instance with same relation by gathering the instances\ntowards their corresponding relation centroids to form a cluster structure, so\nthat the learned representation is cluster-friendly. To reduce the clustering\nbias on predefined classes, we optimize the model by minimizing a joint\nobjective on both labeled and unlabeled data. Experimental results show that\nour method reduces the error rate by 29.2% and 15.7%, on two datasets\nrespectively, compared with current SOTA methods.",
    "published_date": "2021-09-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.07205v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.07157v1",
    "title": "Learning to Match Job Candidates Using Multilingual Bi-Encoder BERT",
    "authors": [
      "Dor Lavi"
    ],
    "author_ids": [],
    "abstract": "In this talk, we will show how we used Randstad history of candidate\nplacements to generate labeled CV-vacancy pairs dataset. Afterwards we\nfine-tune a multilingual BERT with bi encoder structure over this dataset, by\nadding a cosine similarity log loss layer. We will explain how using the\nmentioned structure helps us overcome most of the challenges described above,\nand how it enables us to build a maintainable and scalable pipeline to match\nCVs and vacancies. In addition, we show how we gain a better semantic\nunderstanding, and learn to bridge the vocabulary gap. Finally, we highlight\nhow multilingual transformers help us handle cross language barrier and might\nreduce discrimination.",
    "published_date": "2021-09-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.07157v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.07140v2",
    "title": "On the Universality of Deep Contextual Language Models",
    "authors": [
      "Shaily Bhatt",
      "Poonam Goyal",
      "Sandipan Dandapat",
      "Monojit Choudhury",
      "Sunayana Sitaram"
    ],
    "author_ids": [],
    "abstract": "Deep Contextual Language Models (LMs) like ELMO, BERT, and their successors\ndominate the landscape of Natural Language Processing due to their ability to\nscale across multiple tasks rapidly by pre-training a single model, followed by\ntask-specific fine-tuning. Furthermore, multilingual versions of such models\nlike XLM-R and mBERT have given promising results in zero-shot cross-lingual\ntransfer, potentially enabling NLP applications in many under-served and\nunder-resourced languages. Due to this initial success, pre-trained models are\nbeing used as `Universal Language Models' as the starting point across diverse\ntasks, domains, and languages. This work explores the notion of `Universality'\nby identifying seven dimensions across which a universal model should be able\nto scale, that is, perform equally well or reasonably well, to be useful across\ndiverse settings. We outline the current theoretical and empirical results that\nsupport model performance across these dimensions, along with extensions that\nmay help address some of their current limitations. Through this survey, we lay\nthe foundation for understanding the capabilities and limitations of massive\ncontextual language models and help discern research gaps and directions for\nfuture work to make these LMs inclusive and fair to diverse applications,\nusers, and linguistic phenomena.",
    "published_date": "2021-09-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.07140v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.07131v2",
    "title": "Inequality Constrained Trajectory Optimization with A Hybrid Multiple-shooting iLQR",
    "authors": [
      "Yunxi Tang",
      "Xiangyu Chu",
      "Wanxin Jin",
      "K. W. Samuel Au"
    ],
    "author_ids": [],
    "abstract": "Trajectory optimization has been used extensively in robotic systems. In\nparticular, iterative Linear Quadratic Regulator (iLQR) has performed well as\nan off-line planner and online nonlinear model predictive control solver, with\na lower computational cost. However, standard iLQR cannot handle any\nconstraints or perform reasonable initialization of a state trajectory. In this\npaper, we propose a hybrid constrained iLQR variant with a multiple-shooting\nframework to incorporate general inequality constraints and infeasible states\ninitialization. The main technical contributions are twofold: 1) In addition to\ninheriting the simplicity of the initialization in multiple-shooting settings,\na two-stage framework is developed to deal with state and/or control\nconstraints robustly without loss of the linear feedback term of iLQR. Such a\nhybrid strategy offers fast convergence of constraint satisfaction. 2) An\nimproved globalization strategy is proposed to exploit the coupled effects\nbetween line-searching and regularization, which is able to enhance the\nnumerical robustness of the constrained iLQR approaches. Our approach is tested\non various constrained trajectory optimization problems and outperforms the\ncommonly-used collocation and shooting methods.",
    "published_date": "2021-09-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.07131v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2109.07102v3",
    "title": "Can Edge Probing Tasks Reveal Linguistic Knowledge in QA Models?",
    "authors": [
      "Sagnik Ray Choudhury",
      "Nikita Bhutani",
      "Isabelle Augenstein"
    ],
    "author_ids": [],
    "abstract": "There have been many efforts to try to understand what grammatical knowledge\n(e.g., ability to understand the part of speech of a token) is encoded in large\npre-trained language models (LM). This is done through `Edge Probing' (EP)\ntests: supervised classification tasks to predict the grammatical properties of\na span (whether it has a particular part of speech) using only the token\nrepresentations coming from the LM encoder. However, most NLP applications\nfine-tune these LM encoders for specific tasks. Here, we ask: if an LM is\nfine-tuned, does the encoding of linguistic information in it change, as\nmeasured by EP tests? Specifically, we focus on the task of Question Answering\n(QA) and conduct experiments on multiple datasets. We find that EP test results\ndo not change significantly when the fine-tuned model performs well or in\nadversarial situations where the model is forced to learn wrong correlations.\nFrom a similar finding, some recent papers conclude that fine-tuning does not\nchange linguistic knowledge in encoders but they do not provide an explanation.\nWe find that EP models themselves are susceptible to exploiting spurious\ncorrelations in the EP datasets. When this dataset bias is corrected, we do see\nan improvement in the EP test results as expected.",
    "published_date": "2021-09-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.07102v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.07061v1",
    "title": "Scalable Cell-Free Massive MIMO Systems with Finite Resolution ADCs/DACs over Spatially Correlated Rician Fading Channels",
    "authors": [
      "Xiangjun Ma",
      "Xianfu Lei",
      "P. Takis Mathiopoulos",
      "Kai Yu",
      "Xiaohu Tang"
    ],
    "author_ids": [],
    "abstract": "In this paper, an analytical framework for evaluating the performance of\nscalable cell-free massive MIMO (SCF-mMIMO) systems in which all user\nequipments (UEs) and access points (APs) employ finite resolution\ndigital-to-analog converters (DACs) and analog-to-digital converters (ADCs) and\noperates under correlated Rician fading, is presented. By using maximal-ratio\ncombining (MRC) detection, generic expressions for the uplink (UL) spectral\nefficiency (SE) for both distributed and centralized schemes are derived. In\norder to further reduce the computational complexity (CC) of the original local\npartial MMSE (LP-MMSE) and partial MMSE (P-MMSE) detectors, two novel scalable\nlow complexity MMSE detectors are proposed for distributed and centralized\nschemes respectively, which achieves very similar SE performance. Furthermore,\nfor the distributed scheme a novel partial large-scale fading decoding (P-LSFD)\nweighting vector is introduced and its analytical SE performance is very\nsimilar to the performance of an equivalent unscalable LSFD vector. Finally, a\nscalable algorithm jointly consisting of AP cluster formation, pilot\nassignment, and power control is proposed, which outperforms the conventional\nrandom pilot assignment and user-group based pilot assignment policies and,\ncontrary to an equal power transmit strategy, it guarantees quality of service\n(QoS) fairness for all accessing UEs.",
    "published_date": "2021-09-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IT",
      "cs.NI",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.07061v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2109.06974v1",
    "title": "Algorithmic Auditing and Social Justice: Lessons from the History of Audit Studies",
    "authors": [
      "Briana Vecchione",
      "Solon Barocas",
      "Karen Levy"
    ],
    "author_ids": [],
    "abstract": "Algorithmic audits have been embraced as tools to investigate the functioning\nand consequences of sociotechnical systems. Though the term is used somewhat\nloosely in the algorithmic context and encompasses a variety of methods, it\nmaintains a close connection to audit studies in the social sciences--which\nhave, for decades, used experimental methods to measure the prevalence of\ndiscrimination across domains like housing and employment. In the social\nsciences, audit studies originated in a strong tradition of social justice and\nparticipatory action, often involving collaboration between researchers and\ncommunities; but scholars have argued that, over time, social science audits\nhave become somewhat distanced from these original goals and priorities. We\ndraw from this history in order to highlight difficult tensions that have\nshaped the development of social science audits, and to assess their\nimplications in the context of algorithmic auditing. In doing so, we put forth\nconsiderations to assist in the development of robust and engaged assessments\nof sociotechnical systems that draw from auditing's roots in racial equity and\nsocial justice.",
    "published_date": "2021-09-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "K.4.0; K.4.1; K.4.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.06974v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2109.06800v1",
    "title": "Rethinking Trust in Social Robotics",
    "authors": [
      "Rachele Carli",
      "Amro Najjar"
    ],
    "author_ids": [],
    "abstract": "In 2018 the European Commission highlighted the demand of a human-centered\napproach to AI. Such a claim is gaining even more relevance considering\ntechnologies specifically designed to directly interact and physically\ncollaborate with human users in the real world. This is notably the case of\nsocial robots. The domain of Human-Robot Interaction (HRI) emerged to\ninvestigate these issues. \"Human-robot trust\" has been highlighted as one of\nthe most challenging and intriguing factors influencing HRI. On the one hand,\nuser studies and technical experts underline how trust is a key element to\nfacilitate users' acceptance, consequently increasing the chances to pursue the\ngiven task. On the other hand, such a phenomenon raises also ethical and\nphilosophical concerns leading scholars in these domains to argue that humans\nshould not trust robots. However, trust in HRI is not an index of fragility, it\nis rooted in anthropomorphism, and it is a natural characteristic of every\nhuman being. Thus, instead of focusing solely on how to inspire user trust in\nsocial robots, this paper argues that what should be investigated is to what\nextent and for which purpose it is suitable to trust robots. Such an endeavour\nrequires an interdisciplinary approach taking into account (i) technical needs\nand (ii) psychological implications.",
    "published_date": "2021-09-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.06800v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.06710v1",
    "title": "Fast Federated Edge Learning with Overlapped Communication and Computation and Channel-Aware Fair Client Scheduling",
    "authors": [
      "Mehmet Emre Ozfatura",
      "Junlin Zhao",
      "Deniz Gündüz"
    ],
    "author_ids": [],
    "abstract": "We consider federated edge learning (FEEL) over wireless fading channels\ntaking into account the downlink and uplink channel latencies, and the random\ncomputation delays at the clients. We speed up the training process by\noverlapping the communication with computation. With fountain coded\ntransmission of the global model update, clients receive the global model\nasynchronously, and start performing local computations right away. Then, we\npropose a dynamic client scheduling policy, called MRTP, for uploading local\nmodel updates to the parameter server (PS), which, at any time, schedules the\nclient with the minimum remaining upload time. However, MRTP can lead to biased\nparticipation of clients in the update process, resulting in performance\ndegradation in non-iid data scenarios. To overcome this, we propose two\nalternative schemes with fairness considerations, termed as age-aware MRTP\n(A-MRTP), and opportunistically fair MRTP (OF-MRTP). In A-MRTP, the remaining\nclients are scheduled according to the ratio between their remaining\ntransmission time and the update age, while in OF-MRTP, the selection mechanism\nutilizes the long term average channel rate of the clients to further reduce\nthe latency while ensuring fair participation of the clients. It is shown\nthrough numerical simulations that OF-MRTP provides significant reduction in\nlatency without sacrificing test accuracy.",
    "published_date": "2021-09-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IT",
      "cs.LG",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.06710v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.06598v1",
    "title": "Just What do You Think You're Doing, Dave?' A Checklist for Responsible Data Use in NLP",
    "authors": [
      "Anna Rogers",
      "Tim Baldwin",
      "Kobi Leins"
    ],
    "author_ids": [],
    "abstract": "A key part of the NLP ethics movement is responsible use of data, but exactly\nwhat that means or how it can be best achieved remain unclear. This position\npaper discusses the core legal and ethical principles for collection and\nsharing of textual data, and the tensions between them. We propose a potential\nchecklist for responsible data (re-)use that could both standardise the peer\nreview of conference submissions, as well as enable a more in-depth view of\npublished research across the community. Our proposal aims to contribute to the\ndevelopment of a consistent standard for data (re-)use, embraced across NLP\nconferences.",
    "published_date": "2021-09-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.06598v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.08020v1",
    "title": "The pitfalls of using open data to develop deep learning solutions for COVID-19 detection in chest X-rays",
    "authors": [
      "Rachael Harkness",
      "Geoff Hall",
      "Alejandro F Frangi",
      "Nishant Ravikumar",
      "Kieran Zucker"
    ],
    "author_ids": [],
    "abstract": "Since the emergence of COVID-19, deep learning models have been developed to\nidentify COVID-19 from chest X-rays. With little to no direct access to\nhospital data, the AI community relies heavily on public data comprising\nnumerous data sources. Model performance results have been exceptional when\ntraining and testing on open-source data, surpassing the reported capabilities\nof AI in pneumonia-detection prior to the COVID-19 outbreak. In this study\nimpactful models are trained on a widely used open-source data and tested on an\nexternal test set and a hospital dataset, for the task of classifying chest\nX-rays into one of three classes: COVID-19, non-COVID pneumonia and\nno-pneumonia. Classification performance of the models investigated is\nevaluated through ROC curves, confusion matrices and standard classification\nmetrics. Explainability modules are implemented to explore the image features\nmost important to classification. Data analysis and model evaluations show that\nthe popular open-source dataset COVIDx is not representative of the real\nclinical problem and that results from testing on this are inflated. Dependence\non open-source data can leave models vulnerable to bias and confounding\nvariables, requiring careful analysis to develop clinically useful/viable AI\ntools for COVID-19 detection in chest X-rays.",
    "published_date": "2021-09-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.08020v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.06565v1",
    "title": "Variation-Incentive Loss Re-weighting for Regression Analysis on Biased Data",
    "authors": [
      "Wentai Wu",
      "Ligang He",
      "Weiwei Lin"
    ],
    "author_ids": [],
    "abstract": "Both classification and regression tasks are susceptible to the biased\ndistribution of training data. However, existing approaches are focused on the\nclass-imbalanced learning and cannot be applied to the problems of numerical\nregression where the learning targets are continuous values rather than\ndiscrete labels. In this paper, we aim to improve the accuracy of the\nregression analysis by addressing the data skewness/bias during model training.\nWe first introduce two metrics, uniqueness and abnormality, to reflect the\nlocalized data distribution from the perspectives of their feature (i.e.,\ninput) space and target (i.e., output) space. Combining these two metrics we\npropose a Variation-Incentive Loss re-weighting method (VILoss) to optimize the\ngradient descent-based model training for regression analysis. We have\nconducted comprehensive experiments on both synthetic and real-world data sets.\nThe results show significant improvement in the model quality (reduction in\nerror by up to 11.9%) when using VILoss as the loss criterion in training.",
    "published_date": "2021-09-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "I.2.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.06565v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.06437v1",
    "title": "Uncovering Implicit Gender Bias in Narratives through Commonsense Inference",
    "authors": [
      "Tenghao Huang",
      "Faeze Brahman",
      "Vered Shwartz",
      "Snigdha Chaturvedi"
    ],
    "author_ids": [],
    "abstract": "Pre-trained language models learn socially harmful biases from their training\ncorpora, and may repeat these biases when used for generation. We study gender\nbiases associated with the protagonist in model-generated stories. Such biases\nmay be expressed either explicitly (\"women can't park\") or implicitly (e.g. an\nunsolicited male character guides her into a parking space). We focus on\nimplicit biases, and use a commonsense reasoning engine to uncover them.\nSpecifically, we infer and analyze the protagonist's motivations, attributes,\nmental states, and implications on others. Our findings regarding implicit\nbiases are in line with prior work that studied explicit biases, for example\nshowing that female characters' portrayal is centered around appearance, while\nmale figures' focus on intellect.",
    "published_date": "2021-09-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.06437v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.06416v2",
    "title": "MMCoVaR: Multimodal COVID-19 Vaccine Focused Data Repository for Fake News Detection and a Baseline Architecture for Classification",
    "authors": [
      "Mingxuan Chen",
      "Xinqiao Chu",
      "K. P. Subbalakshmi"
    ],
    "author_ids": [],
    "abstract": "The outbreak of COVID-19 has resulted in an \"infodemic\" that has encouraged\nthe propagation of misinformation about COVID-19 and cure methods which, in\nturn, could negatively affect the adoption of recommended public health\nmeasures in the larger population. In this paper, we provide a new multimodal\n(consisting of images, text and temporal information) labeled dataset\ncontaining news articles and tweets on the COVID-19 vaccine. We collected 2,593\nnews articles from 80 publishers for one year between Feb 16th 2020 to May 8th\n2021 and 24184 Twitter posts (collected between April 17th 2021 to May 8th\n2021). We combine ratings from two news media ranking sites: Medias Bias Chart\nand Media Bias/Fact Check (MBFC) to classify the news dataset into two levels\nof credibility: reliable and unreliable. The combination of two filters allows\nfor higher precision of labeling. We also propose a stance detection mechanism\nto annotate tweets into three levels of credibility: reliable, unreliable and\ninconclusive. We provide several statistics as well as other analytics like,\npublisher distribution, publication date distribution, topic analysis, etc. We\nalso provide a novel architecture that classifies the news data into\nmisinformation or truth to provide a baseline performance for this dataset. We\nfind that the proposed architecture has an F-Score of 0.919 and accuracy of\n0.882 for fake news detection. Furthermore, we provide benchmark performance\nfor misinformation detection on tweet dataset. This new multimodal dataset can\nbe used in research on COVID-19 vaccine, including misinformation detection,\ninfluence of fake COVID-19 vaccine information, etc.",
    "published_date": "2021-09-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.06416v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.06407v2",
    "title": "Neural Networks with Physics-Informed Architectures and Constraints for Dynamical Systems Modeling",
    "authors": [
      "Franck Djeumou",
      "Cyrus Neary",
      "Eric Goubault",
      "Sylvie Putot",
      "Ufuk Topcu"
    ],
    "author_ids": [],
    "abstract": "Effective inclusion of physics-based knowledge into deep neural network\nmodels of dynamical systems can greatly improve data efficiency and\ngeneralization. Such a-priori knowledge might arise from physical principles\n(e.g., conservation laws) or from the system's design (e.g., the Jacobian\nmatrix of a robot), even if large portions of the system dynamics remain\nunknown. We develop a framework to learn dynamics models from trajectory data\nwhile incorporating a-priori system knowledge as inductive bias. More\nspecifically, the proposed framework uses physics-based side information to\ninform the structure of the neural network itself, and to place constraints on\nthe values of the outputs and the internal states of the model. It represents\nthe system's vector field as a composition of known and unknown functions, the\nlatter of which are parametrized by neural networks. The physics-informed\nconstraints are enforced via the augmented Lagrangian method during the model's\ntraining. We experimentally demonstrate the benefits of the proposed approach\non a variety of dynamical systems -- including a benchmark suite of robotics\nenvironments featuring large state spaces, non-linear dynamics, external\nforces, contact forces, and control inputs. By exploiting a-priori system\nknowledge during training, the proposed approach learns to predict the system\ndynamics two orders of magnitude more accurately than a baseline approach that\ndoes not include prior knowledge, given the same training dataset.",
    "published_date": "2021-09-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.06407v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.06323v1",
    "title": "Stochastic Observer for SLAM on the Lie Group",
    "authors": [
      "Marium Tawhid",
      "Ajay Singh Ludher",
      "Hashim A. Hashim"
    ],
    "author_ids": [],
    "abstract": "A robust nonlinear stochastic observer for simultaneous localization and\nmapping (SLAM) is proposed using the available uncertain measurements of\nangular velocity, translational velocity, and features. The proposed observer\nis posed on the Lie Group of $\\mathbb{SLAM}_{n}\\left(3\\right)$ to mimic the\ntrue stochastic SLAM dynamics. The proposed approach considers the velocity\nmeasurements to be attached with an unknown bias and an unknown Gaussian noise.\nThe proposed SLAM observer ensures that the closed loop error signals are\nsemi-globally uniformly ultimately bounded. Simulation results demonstrates the\nefficiency and robustness of the proposed approach, revealing its ability to\nlocalize the unknown vehicle, as well as mapping the unknown environment given\nmeasurements obtained from low-cost units.",
    "published_date": "2021-09-13T00:00:00",
    "year": 2021,
    "categories": [
      "eess.SY",
      "cs.RO",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.06323v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2109.06321v1",
    "title": "Mitigating Sampling Bias and Improving Robustness in Active Learning",
    "authors": [
      "Ranganath Krishnan",
      "Alok Sinha",
      "Nilesh Ahuja",
      "Mahesh Subedar",
      "Omesh Tickoo",
      "Ravi Iyer"
    ],
    "author_ids": [],
    "abstract": "This paper presents simple and efficient methods to mitigate sampling bias in\nactive learning while achieving state-of-the-art accuracy and model robustness.\nWe introduce supervised contrastive active learning by leveraging the\ncontrastive loss for active learning under a supervised setting. We propose an\nunbiased query strategy that selects informative data samples of diverse\nfeature representations with our methods: supervised contrastive active\nlearning (SCAL) and deep feature modeling (DFM). We empirically demonstrate our\nproposed methods reduce sampling bias, achieve state-of-the-art accuracy and\nmodel calibration in an active learning setup with the query computation 26x\nfaster than Bayesian active learning by disagreement and 11x faster than\nCoreSet. The proposed SCAL method outperforms by a big margin in robustness to\ndataset shift and out-of-distribution.",
    "published_date": "2021-09-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.06321v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.06309v1",
    "title": "Fairness and Data Protection Impact Assessments",
    "authors": [
      "Atoosa Kasirzadeh",
      "Damian Clifford"
    ],
    "author_ids": [],
    "abstract": "In this paper, we critically examine the effectiveness of the requirement to\nconduct a Data Protection Impact Assessment (DPIA) in Article 35 of the General\nData Protection Regulation (GDPR) in light of fairness metrics. Through this\nanalysis, we explore the role of the fairness principle as introduced in\nArticle 5(1)(a) and its multifaceted interpretation in the obligation to\nconduct a DPIA. Our paper argues that although there is a significant\ntheoretical role for the considerations of fairness in the DPIA process, an\nanalysis of the various guidance documents issued by data protection\nauthorities on the obligation to conduct a DPIA reveals that they rarely\nmention the fairness principle in practice.",
    "published_date": "2021-09-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.06309v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2109.06253v1",
    "title": "Multi-Sentence Resampling: A Simple Approach to Alleviate Dataset Length Bias and Beam-Search Degradation",
    "authors": [
      "Ivan Provilkov",
      "Andrey Malinin"
    ],
    "author_ids": [],
    "abstract": "Neural Machine Translation (NMT) is known to suffer from a beam-search\nproblem: after a certain point, increasing beam size causes an overall drop in\ntranslation quality. This effect is especially pronounced for long sentences.\nWhile much work was done analyzing this phenomenon, primarily for\nautoregressive NMT models, there is still no consensus on its underlying cause.\nIn this work, we analyze errors that cause major quality degradation with large\nbeams in NMT and Automatic Speech Recognition (ASR). We show that a factor that\nstrongly contributes to the quality degradation with large beams is\n\\textit{dataset length-bias} - \\textit{NMT datasets are strongly biased towards\nshort sentences}. To mitigate this issue, we propose a new data augmentation\ntechnique -- \\textit{Multi-Sentence Resampling (MSR)}. This technique extends\nthe training examples by concatenating several sentences from the original\ndataset to make a long training example. We demonstrate that MSR significantly\nreduces degradation with growing beam size and improves final translation\nquality on the IWSTL$15$ En-Vi, IWSTL$17$ En-Fr, and WMT$14$ En-De datasets.",
    "published_date": "2021-09-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.06253v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.06232v2",
    "title": "The Emergence of the Shape Bias Results from Communicative Efficiency",
    "authors": [
      "Eva Portelance",
      "Michael C. Frank",
      "Dan Jurafsky",
      "Alessandro Sordoni",
      "Romain Laroche"
    ],
    "author_ids": [],
    "abstract": "By the age of two, children tend to assume that new word categories are based\non objects' shape, rather than their color or texture; this assumption is\ncalled the shape bias. They are thought to learn this bias by observing that\ntheir caregiver's language is biased towards shape based categories. This\npresents a chicken and egg problem: if the shape bias must be present in the\nlanguage in order for children to learn it, how did it arise in language in the\nfirst place? In this paper, we propose that communicative efficiency explains\nboth how the shape bias emerged and why it persists across generations. We\nmodel this process with neural emergent language agents that learn to\ncommunicate about raw pixelated images. First, we show that the shape bias\nemerges as a result of efficient communication strategies employed by agents.\nSecond, we show that pressure brought on by communicative need is also\nnecessary for it to persist across generations; simply having a shape bias in\nan agent's input language is insufficient. These results suggest that, over and\nabove the operation of other learning strategies, the shape bias in human\nlearners may emerge and be sustained by communicative pressures.",
    "published_date": "2021-09-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.IT",
      "cs.NE",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.06232v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.06152v5",
    "title": "Enumerating independent sets in Abelian Cayley graphs",
    "authors": [
      "Aditya Potukuchi",
      "Liana Yepremyan"
    ],
    "author_ids": [],
    "abstract": "We show that any connected Cayley graph $\\Gamma$ on an Abelian group of order\n$2n$ and degree $\\tilde{\\Omega}(\\log n)$ has at most $2^{n+1}(1 + o(1))$\nindependent sets. This bound is tight up to to the $o(1)$ term when $\\Gamma$ is\nbipartite. Our proof is based on Sapozhenko's graph container method and uses\nthe Pl\\\"{u}nnecke-Rusza-Petridis inequality from additive combinatorics.",
    "published_date": "2021-09-13T00:00:00",
    "year": 2021,
    "categories": [
      "math.CO",
      "cs.DM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.06152v5",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2109.06141v3",
    "title": "On Tilted Losses in Machine Learning: Theory and Applications",
    "authors": [
      "Tian Li",
      "Ahmad Beirami",
      "Maziar Sanjabi",
      "Virginia Smith"
    ],
    "author_ids": [],
    "abstract": "Exponential tilting is a technique commonly used in fields such as\nstatistics, probability, information theory, and optimization to create\nparametric distribution shifts. Despite its prevalence in related fields,\ntilting has not seen widespread use in machine learning. In this work, we aim\nto bridge this gap by exploring the use of tilting in risk minimization. We\nstudy a simple extension to ERM -- tilted empirical risk minimization (TERM) --\nwhich uses exponential tilting to flexibly tune the impact of individual\nlosses. The resulting framework has several useful properties: We show that\nTERM can increase or decrease the influence of outliers, respectively, to\nenable fairness or robustness; has variance-reduction properties that can\nbenefit generalization; and can be viewed as a smooth approximation to the tail\nprobability of losses. Our work makes rigorous connections between TERM and\nrelated objectives, such as Value-at-Risk, Conditional Value-at-Risk, and\ndistributionally robust optimization (DRO). We develop batch and stochastic\nfirst-order optimization methods for solving TERM, provide convergence\nguarantees for the solvers, and show that the framework can be efficiently\nsolved relative to common alternatives. Finally, we demonstrate that TERM can\nbe used for a multitude of applications in machine learning, such as enforcing\nfairness between subgroups, mitigating the effect of outliers, and handling\nclass imbalance. Despite the straightforward modification TERM makes to\ntraditional ERM objectives, we find that the framework can consistently\noutperform ERM and deliver competitive performance with state-of-the-art,\nproblem-specific approaches.",
    "published_date": "2021-09-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.IT",
      "math.IT",
      "math.OC",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.06141v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.06096v3",
    "title": "The Grammar-Learning Trajectories of Neural Language Models",
    "authors": [
      "Leshem Choshen",
      "Guy Hacohen",
      "Daphna Weinshall",
      "Omri Abend"
    ],
    "author_ids": [],
    "abstract": "The learning trajectories of linguistic phenomena in humans provide insight\ninto linguistic representation, beyond what can be gleaned from inspecting the\nbehavior of an adult speaker. To apply a similar approach to analyze neural\nlanguage models (NLM), it is first necessary to establish that different models\nare similar enough in the generalizations they make. In this paper, we show\nthat NLMs with different initialization, architecture, and training data\nacquire linguistic phenomena in a similar order, despite their different end\nperformance. These findings suggest that there is some mutual inductive bias\nthat underlies these models' learning of linguistic phenomena. Taking\ninspiration from psycholinguistics, we argue that studying this inductive bias\nis an opportunity to study the linguistic representation implicit in NLMs.\n  Leveraging these findings, we compare the relative performance on different\nphenomena at varying learning stages with simpler reference models. Results\nsuggest that NLMs exhibit consistent \"developmental\" stages. Moreover, we find\nthe learning trajectory to be approximately one-dimensional: given an NLM with\na certain overall performance, it is possible to predict what linguistic\ngeneralizations it has already acquired. Initial analysis of these stages\npresents phenomena clusters (notably morphological ones), whose performance\nprogresses in unison, suggesting a potential link between the generalizations\nbehind them.",
    "published_date": "2021-09-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.06096v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.06037v1",
    "title": "Correcting the User Feedback-Loop Bias for Recommendation Systems",
    "authors": [
      "Weishen Pan",
      "Sen Cui",
      "Hongyi Wen",
      "Kun Chen",
      "Changshui Zhang",
      "Fei Wang"
    ],
    "author_ids": [],
    "abstract": "Selection bias is prevalent in the data for training and evaluating\nrecommendation systems with explicit feedback. For example, users tend to rate\nitems they like. However, when rating an item concerning a specific user, most\nof the recommendation algorithms tend to rely too much on his/her rating\n(feedback) history. This introduces implicit bias on the recommendation system,\nwhich is referred to as user feedback-loop bias in this paper. We propose a\nsystematic and dynamic way to correct such bias and to obtain more diverse and\nobjective recommendations by utilizing temporal rating information.\nSpecifically, our method includes a deep-learning component to learn each\nuser's dynamic rating history embedding for the estimation of the probability\ndistribution of the items that the user rates sequentially. These estimated\ndynamic exposure probabilities are then used as propensity scores to train an\ninverse-propensity-scoring (IPS) rating predictor. We empirically validated the\nexistence of such user feedback-loop bias in real world recommendation systems\nand compared the performance of our method with the baseline models that are\neither without de-biasing or with propensity scores estimated by other methods.\nThe results show the superiority of our approach.",
    "published_date": "2021-09-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.06037v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.05931v1",
    "title": "FaiREO: User Group Fairness for Equality of Opportunity in Course Recommendation",
    "authors": [
      "Agoritsa Polyzou",
      "Maria Kalantzi",
      "George Karypis"
    ],
    "author_ids": [],
    "abstract": "Course selection is challenging for students in higher educational\ninstitutions. Existing course recommendation systems make relevant suggestions\nto the students and help them in exploring the available courses. The\nrecommended courses can influence students' choice of degree program, future\nemployment, and even their socioeconomic status. This paper focuses on\nidentifying and alleviating biases that might be present in a course\nrecommender system. We strive to promote balanced opportunities with our\nsuggestions to all groups of students. At the same time, we need to make\nrecommendations of good quality to all protected groups. We formulate our\napproach as a multi-objective optimization problem and study the trade-offs\nbetween equal opportunity and quality. We evaluate our methods using both\nreal-world and synthetic datasets. The results indicate that we can\nconsiderably improve fairness regarding equality of opportunity, but we will\nintroduce some quality loss. Out of the four methods we tested, GHC-Inc and\nGHC-Tabu are the best performing ones with different advantageous\ncharacteristics.",
    "published_date": "2021-09-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.05931v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.05829v1",
    "title": "Zeroth-order non-convex learning via hierarchical dual averaging",
    "authors": [
      "Amélie Héliou",
      "Matthieu Martin",
      "Panayotis Mertikopoulos",
      "Thibaud Rahier"
    ],
    "author_ids": [],
    "abstract": "We propose a hierarchical version of dual averaging for zeroth-order online\nnon-convex optimization - i.e., learning processes where, at each stage, the\noptimizer is facing an unknown non-convex loss function and only receives the\nincurred loss as feedback. The proposed class of policies relies on the\nconstruction of an online model that aggregates loss information as it arrives,\nand it consists of two principal components: (a) a regularizer adapted to the\nFisher information metric (as opposed to the metric norm of the ambient space);\nand (b) a principled exploration of the problem's state space based on an\nadapted hierarchical schedule. This construction enables sharper control of the\nmodel's bias and variance, and allows us to derive tight bounds for both the\nlearner's static and dynamic regret - i.e., the regret incurred against the\nbest dynamic policy in hindsight over the horizon of play.",
    "published_date": "2021-09-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "math.OC",
      "Primary 68Q32, 90C56, secondary 90C15, 90C26"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.05829v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.05810v1",
    "title": "Truthful and Fair Mechanisms for Matroid-Rank Valuations",
    "authors": [
      "Siddharth Barman",
      "Paritosh Verma"
    ],
    "author_ids": [],
    "abstract": "We study the problem of allocating indivisible goods among strategic agents.\nWe focus on settings wherein monetary transfers are not available and each\nagent's private valuation is a submodular function with binary marginals, i.e.,\nthe agents' valuations are matroid-rank functions. In this setup, we establish\na notable dichotomy between two of the most well-studied fairness notions in\ndiscrete fair division; specifically, between envy-freeness up to one good\n(EF1) and maximin shares (MMS).\n  First, we show that a Pareto-efficient mechanism of Babaioff et al. (2021) is\ngroup strategy-proof for finding EF1 allocations, under matroid-rank\nvaluations. The group strategy-proofness guarantee strengthens the result of\nBabaioff et al. (2021), that establishes truthfulness (individually for each\nagent) in the same context. Our result also generalizes a work of Halpern et\nal. (2020), from binary additive valuations to the matroid-rank case.\n  Next, we establish that an analogous positive result cannot be achieved for\nMMS, even when considering truthfulness on an individual level. Specifically,\nwe prove that, for matroid-rank valuations, there does not exist a truthful\nmechanism that is index oblivious, Pareto efficient, and maximin fair.\n  For establishing our results, we develop a characterization of truthful\nmechanisms for matroid-rank functions. This characterization in fact holds for\na broader class of valuations (specifically, holds for binary XOS functions)\nand might be of independent interest.",
    "published_date": "2021-09-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.05810v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2109.05792v1",
    "title": "Perceptions of Fairness and Trustworthiness Based on Explanations in Human vs. Automated Decision-Making",
    "authors": [
      "Jakob Schoeffer",
      "Yvette Machowski",
      "Niklas Kuehl"
    ],
    "author_ids": [],
    "abstract": "Automated decision systems (ADS) have become ubiquitous in many high-stakes\ndomains. Those systems typically involve sophisticated yet opaque artificial\nintelligence (AI) techniques that seldom allow for full comprehension of their\ninner workings, particularly for affected individuals. As a result, ADS are\nprone to deficient oversight and calibration, which can lead to undesirable\n(e.g., unfair) outcomes. In this work, we conduct an online study with 200\nparticipants to examine people's perceptions of fairness and trustworthiness\ntowards ADS in comparison to a scenario where a human instead of an ADS makes a\nhigh-stakes decision -- and we provide thorough identical explanations\nregarding decisions in both cases. Surprisingly, we find that people perceive\nADS as fairer than human decision-makers. Our analyses also suggest that\npeople's AI literacy affects their perceptions, indicating that people with\nhigher AI literacy favor ADS more strongly over human decision-makers, whereas\nlow-AI-literacy people exhibit no significant differences in their perceptions.",
    "published_date": "2021-09-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.05792v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.05767v1",
    "title": "Computation Rate Maximum for Mobile Terminals in UAV-assisted Wireless Powered MEC Networks with Fairness Constraint",
    "authors": [
      "Xiaoyi Zhou",
      "Liang Huang",
      "Tong Ye",
      "Weiqiang Sun"
    ],
    "author_ids": [],
    "abstract": "This paper investigates an unmanned aerial vehicle (UAV)-assisted wireless\npowered mobile-edge computing (MEC) system, where the UAV powers the mobile\nterminals by wireless power transfer (WPT) and provides computation service for\nthem. We aim to maximize the computation rate of terminals while ensuring\nfairness among them. Considering the random trajectories of mobile terminals,\nwe propose a soft actor-critic (SAC)-based UAV trajectory planning and resource\nallocation (SAC-TR) algorithm, which combines off-policy and maximum entropy\nreinforcement learning to promote the convergence of the algorithm. We design\nthe reward as a heterogeneous function of computation rate, fairness, and\nreaching of destination. Simulation results show that SAC-TR can quickly adapt\nto varying network environments and outperform representative benchmarks in a\nvariety of situations.",
    "published_date": "2021-09-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.05767v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.05721v2",
    "title": "ADNet: Leveraging Error-Bias Towards Normal Direction in Face Alignment",
    "authors": [
      "Yangyu Huang",
      "Hao Yang",
      "Chong Li",
      "Jongyoo Kim",
      "Fangyun Wei"
    ],
    "author_ids": [],
    "abstract": "The recent progress of CNN has dramatically improved face alignment\nperformance. However, few works have paid attention to the error-bias with\nrespect to error distribution of facial landmarks. In this paper, we\ninvestigate the error-bias issue in face alignment, where the distributions of\nlandmark errors tend to spread along the tangent line to landmark curves. This\nerror-bias is not trivial since it is closely connected to the ambiguous\nlandmark labeling task. Inspired by this observation, we seek a way to leverage\nthe error-bias property for better convergence of CNN model. To this end, we\npropose anisotropic direction loss (ADL) and anisotropic attention module (AAM)\nfor coordinate and heatmap regression, respectively. ADL imposes strong binding\nforce in normal direction for each landmark point on facial boundaries. On the\nother hand, AAM is an attention module which can get anisotropic attention mask\nfocusing on the region of point and its local edge connected by adjacent\npoints, it has a stronger response in tangent than in normal, which means\nrelaxed constraints in the tangent. These two methods work in a complementary\nmanner to learn both facial structures and texture details. Finally, we\nintegrate them into an optimized end-to-end training pipeline named ADNet. Our\nADNet achieves state-of-the-art results on 300W, WFLW and COFW datasets, which\ndemonstrates the effectiveness and robustness.",
    "published_date": "2021-09-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.05721v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.07908v2",
    "title": "Auditing the Imputation Effect on Fairness of Predictive Analytics in Higher Education",
    "authors": [
      "Hadis Anahideh",
      "Parian Haghighat",
      "Nazanin Nezami",
      "Denisa G`andara"
    ],
    "author_ids": [],
    "abstract": "Colleges and universities use predictive analytics in a variety of ways to\nincrease student success rates. Despite the potential for predictive analytics,\ntwo major barriers exist to their adoption in higher education: (a) the lack of\ndemocratization in deployment, and (b) the potential to exacerbate\ninequalities. Education researchers and policymakers encounter numerous\nchallenges in deploying predictive modeling in practice. These challenges\npresent in different steps of modeling including data preparation, model\ndevelopment, and evaluation. Nevertheless, each of these steps can introduce\nadditional bias to the system if not appropriately performed. Most large-scale\nand nationally representative education data sets suffer from a significant\nnumber of incomplete responses from the research participants. While many\neducation-related studies addressed the challenges of missing data, little is\nknown about the impact of handling missing values on the fairness of predictive\noutcomes in practice. In this paper, we set out to first assess the disparities\nin predictive modeling outcomes for college-student success, then investigate\nthe impact of imputation techniques on the model performance and fairness using\na commonly used set of metrics. We conduct a prospective evaluation to provide\na less biased estimation of future performance and fairness than an evaluation\nof historical data. Our comprehensive analysis of a real large-scale education\ndataset reveals key insights on modeling disparities and how imputation\ntechniques impact the fairness of the student-success predictive outcome under\ndifferent testing scenarios. Our results indicate that imputation introduces\nbias if the testing set follows the historical distribution. However, if the\ninjustice in society is addressed and consequently the upcoming batch of\nobservations is equalized, the model would be less biased.",
    "published_date": "2021-09-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.07908v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.05704v2",
    "title": "Mitigating Language-Dependent Ethnic Bias in BERT",
    "authors": [
      "Jaimeen Ahn",
      "Alice Oh"
    ],
    "author_ids": [],
    "abstract": "BERT and other large-scale language models (LMs) contain gender and racial\nbias. They also exhibit other dimensions of social bias, most of which have not\nbeen studied in depth, and some of which vary depending on the language. In\nthis paper, we study ethnic bias and how it varies across languages by\nanalyzing and mitigating ethnic bias in monolingual BERT for English, German,\nSpanish, Korean, Turkish, and Chinese. To observe and quantify ethnic bias, we\ndevelop a novel metric called Categorical Bias score. Then we propose two\nmethods for mitigation; first using a multilingual model, and second using\ncontextual word alignment of two monolingual models. We compare our proposed\nmethods with monolingual BERT and show that these methods effectively alleviate\nthe ethnic bias. Which of the two methods works better depends on the amount of\nNLP resources available for that language. We additionally experiment with\nArabic and Greek to verify that our proposed methods work for a wider variety\nof languages.",
    "published_date": "2021-09-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.05704v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.05697v2",
    "title": "Finding Representative Group Fairness Metrics Using Correlation Estimations",
    "authors": [
      "Hadis Anahideh",
      "Nazanin Nezami",
      "Abolfazl Asudeh"
    ],
    "author_ids": [],
    "abstract": "It is of critical importance to be aware of the historical discrimination\nembedded in the data and to consider a fairness measure to reduce bias\nthroughout the predictive modeling pipeline. Given various notions of fairness\ndefined in the literature, investigating the correlation and interaction among\nmetrics is vital for addressing unfairness. Practitioners and data scientists\nshould be able to comprehend each metric and examine their impact on one\nanother given the context, use case, and regulations. Exploring the\ncombinatorial space of different metrics for such examination is burdensome. To\nalleviate the burden of selecting fairness notions for consideration, we\npropose a framework that estimates the correlation among fairness notions. Our\nframework consequently identifies a set of diverse and semantically distinct\nmetrics as representative for a given context. We propose a Monte-Carlo\nsampling technique for computing the correlations between fairness metrics by\nindirect and efficient perturbation in the model space. Using the estimated\ncorrelations, we then find a subset of representative metrics. The paper\nproposes a generic method that can be generalized to any arbitrary set of\nfairness metrics. We showcase the validity of the proposal using comprehensive\nexperiments on real-world benchmark datasets.",
    "published_date": "2021-09-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.05697v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.05677v1",
    "title": "An Adaptive Boosting Technique to Mitigate Popularity Bias in Recommender System",
    "authors": [
      "Ajay Gangwar",
      "Shweta Jain"
    ],
    "author_ids": [],
    "abstract": "The observed ratings in most recommender systems are subjected to popularity\nbias and are thus not randomly missing. Due to this, only a few popular items\nare recommended, and a vast number of non-popular items are hardly recommended.\nNot suggesting the non-popular items lead to fewer products dominating the\nmarket and thus offering fewer opportunities for creativity and innovation. In\nthe literature, several fair algorithms have been proposed which mainly focused\non improving the accuracy of the recommendation system. However, a typical\naccuracy measure is biased towards popular items, i.e., it promotes better\naccuracy for popular items compared to non-popular items. This paper considers\na metric that measures the popularity bias as the difference in error on\npopular items and non-popular items. Motivated by the fair boosting algorithm\non classification, we propose an algorithm that reduces the popularity bias\npresent in the data while maintaining accuracy within acceptable limits. The\nmain idea of our algorithm is that it lifts the weights of the non-popular\nitems, which are generally underrepresented in the data. With the help of\ncomprehensive experiments on real-world datasets, we show that our proposed\nalgorithm outperforms the existing algorithms on the proposed popularity bias\nmetric.",
    "published_date": "2021-09-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.05677v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.05671v1",
    "title": "Shape-Biased Domain Generalization via Shock Graph Embeddings",
    "authors": [
      "Maruthi Narayanan",
      "Vickram Rajendran",
      "Benjamin Kimia"
    ],
    "author_ids": [],
    "abstract": "There is an emerging sense that the vulnerability of Image Convolutional\nNeural Networks (CNN), i.e., sensitivity to image corruptions, perturbations,\nand adversarial attacks, is connected with Texture Bias. This relative lack of\nShape Bias is also responsible for poor performance in Domain Generalization\n(DG). The inclusion of a role of shape alleviates these vulnerabilities and\nsome approaches have achieved this by training on negative images, images\nendowed with edge maps, or images with conflicting shape and texture\ninformation. This paper advocates an explicit and complete representation of\nshape using a classical computer vision approach, namely, representing the\nshape content of an image with the shock graph of its contour map. The\nresulting graph and its descriptor is a complete representation of contour\ncontent and is classified using recent Graph Neural Network (GNN) methods. The\nexperimental results on three domain shift datasets, Colored MNIST, PACS, and\nVLCS demonstrate that even without using appearance the shape-based approach\nexceeds classical Image CNN based methods in domain generalization.",
    "published_date": "2021-09-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.05671v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.05659v1",
    "title": "Source Inference Attacks in Federated Learning",
    "authors": [
      "Hongsheng Hu",
      "Zoran Salcic",
      "Lichao Sun",
      "Gillian Dobbie",
      "Xuyun Zhang"
    ],
    "author_ids": [],
    "abstract": "Federated learning (FL) has emerged as a promising privacy-aware paradigm\nthat allows multiple clients to jointly train a model without sharing their\nprivate data. Recently, many studies have shown that FL is vulnerable to\nmembership inference attacks (MIAs) that can distinguish the training members\nof the given model from the non-members. However, existing MIAs ignore the\nsource of a training member, i.e., the information of which client owns the\ntraining member, while it is essential to explore source privacy in FL beyond\nmembership privacy of examples from all clients. The leakage of source\ninformation can lead to severe privacy issues. For example, identification of\nthe hospital contributing to the training of an FL model for COVID-19 pandemic\ncan render the owner of a data record from this hospital more prone to\ndiscrimination if the hospital is in a high risk region. In this paper, we\npropose a new inference attack called source inference attack (SIA), which can\nderive an optimal estimation of the source of a training member. Specifically,\nwe innovatively adopt the Bayesian perspective to demonstrate that an\nhonest-but-curious server can launch an SIA to steal non-trivial source\ninformation of the training members without violating the FL protocol. The\nserver leverages the prediction loss of local models on the training members to\nachieve the attack effectively and non-intrusively. We conduct extensive\nexperiments on one synthetic and five real datasets to evaluate the key factors\nin an SIA, and the results show the efficacy of the proposed source inference\nattack.",
    "published_date": "2021-09-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CR",
      "cs.DC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.05659v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.05658v1",
    "title": "Measurement as governance in and for responsible AI",
    "authors": [
      "Abigail Z. Jacobs"
    ],
    "author_ids": [],
    "abstract": "Measurement of social phenomena is everywhere, unavoidably, in sociotechnical\nsystems. This is not (only) an academic point: Fairness-related harms emerge\nwhen there is a mismatch in the measurement process between the thing we\npurport to be measuring and the thing we actually measure. However, the\nmeasurement process -- where social, cultural, and political values are\nimplicitly encoded in sociotechnical systems -- is almost always obscured.\nFurthermore, this obscured process is where important governance decisions are\nencoded: governance about which systems are fair, which individuals belong in\nwhich categories, and so on. We can then use the language of measurement, and\nthe tools of construct validity and reliability, to uncover hidden governance\ndecisions. In particular, we highlight two types of construct validity, content\nvalidity and consequential validity, that are useful to elicit and characterize\nthe feedback loops between the measurement, social construction, and\nenforcement of social categories. We then explore the constructs of fairness,\nrobustness, and responsibility in the context of governance in and for\nresponsible AI. Together, these perspectives help us unpack how measurement\nacts as a hidden governance process in sociotechnical systems. Understanding\nmeasurement as governance supports a richer understanding of the governance\nprocesses already happening in AI -- responsible or otherwise -- revealing\npaths to more effective interventions.",
    "published_date": "2021-09-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.05658v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.05602v2",
    "title": "Good-Enough Example Extrapolation",
    "authors": [
      "Jason Wei"
    ],
    "author_ids": [],
    "abstract": "This paper asks whether extrapolating the hidden space distribution of text\nexamples from one class onto another is a valid inductive bias for data\naugmentation. To operationalize this question, I propose a simple data\naugmentation protocol called \"good-enough example extrapolation\" (GE3). GE3 is\nlightweight and has no hyperparameters. Applied to three text classification\ndatasets for various data imbalance scenarios, GE3 improves performance more\nthan upsampling and other hidden-space data augmentation methods.",
    "published_date": "2021-09-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.05602v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.07906v1",
    "title": "Ethics of AI: A Systematic Literature Review of Principles and Challenges",
    "authors": [
      "Arif Ali Khan",
      "Sher Badshah",
      "Peng Liang",
      "Bilal Khan",
      "Muhammad Waseem",
      "Mahmood Niazi",
      "Muhammad Azeem Akbar"
    ],
    "author_ids": [],
    "abstract": "Ethics in AI becomes a global topic of interest for both policymakers and\nacademic researchers. In the last few years, various research organizations,\nlawyers, think tankers and regulatory bodies get involved in developing AI\nethics guidelines and principles. However, there is still debate about the\nimplications of these principles. We conducted a systematic literature review\n(SLR) study to investigate the agreement on the significance of AI principles\nand identify the challenging factors that could negatively impact the adoption\nof AI ethics principles. The results reveal that the global convergence set\nconsists of 22 ethical principles and 15 challenges. Transparency, privacy,\naccountability and fairness are identified as the most common AI ethics\nprinciples. Similarly, lack of ethical knowledge and vague principles are\nreported as the significant challenges for considering ethics in AI. The\nfindings of this study are the preliminary inputs for proposing a maturity\nmodel that assess the ethical capabilities of AI systems and provide best\npractices for further improvements.",
    "published_date": "2021-09-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.07906v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.05433v1",
    "title": "Are Gender-Neutral Queries Really Gender-Neutral? Mitigating Gender Bias in Image Search",
    "authors": [
      "Jialu Wang",
      "Yang Liu",
      "Xin Eric Wang"
    ],
    "author_ids": [],
    "abstract": "Internet search affects people's cognition of the world, so mitigating biases\nin search results and learning fair models is imperative for social good. We\nstudy a unique gender bias in image search in this work: the search images are\noften gender-imbalanced for gender-neutral natural language queries. We\ndiagnose two typical image search models, the specialized model trained on\nin-domain datasets and the generalized representation model pre-trained on\nmassive image and text data across the internet. Both models suffer from severe\ngender bias. Therefore, we introduce two novel debiasing approaches: an\nin-processing fair sampling method to address the gender imbalance issue for\ntraining models, and a post-processing feature clipping method base on mutual\ninformation to debias multimodal representations of pre-trained models.\nExtensive experiments on MS-COCO and Flickr30K benchmarks show that our methods\nsignificantly reduce the gender bias in image search models.",
    "published_date": "2021-09-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.CL",
      "I.2.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.05433v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.05389v1",
    "title": "Omnipredictors",
    "authors": [
      "Parikshit Gopalan",
      "Adam Tauman Kalai",
      "Omer Reingold",
      "Vatsal Sharan",
      "Udi Wieder"
    ],
    "author_ids": [],
    "abstract": "Loss minimization is a dominant paradigm in machine learning, where a\npredictor is trained to minimize some loss function that depends on an\nuncertain event (e.g., \"will it rain tomorrow?''). Different loss functions\nimply different learning algorithms and, at times, very different predictors.\nWhile widespread and appealing, a clear drawback of this approach is that the\nloss function may not be known at the time of learning, requiring the algorithm\nto use a best-guess loss function. We suggest a rigorous new paradigm for loss\nminimization in machine learning where the loss function can be ignored at the\ntime of learning and only be taken into account when deciding an action.\n  We introduce the notion of an (${\\mathcal{L}},\\mathcal{C}$)-omnipredictor,\nwhich could be used to optimize any loss in a family ${\\mathcal{L}}$. Once the\nloss function is set, the outputs of the predictor can be post-processed (a\nsimple univariate data-independent transformation of individual predictions) to\ndo well compared with any hypothesis from the class $\\mathcal{C}$. The post\nprocessing is essentially what one would perform if the outputs of the\npredictor were true probabilities of the uncertain events. In a sense,\nomnipredictors extract all the predictive power from the class $\\mathcal{C}$,\nirrespective of the loss function in $\\mathcal{L}$.\n  We show that such \"loss-oblivious'' learning is feasible through a connection\nto multicalibration, a notion introduced in the context of algorithmic\nfairness. In addition, we show how multicalibration can be viewed as a solution\nconcept for agnostic boosting, shedding new light on past results. Finally, we\ntransfer our insights back to the context of algorithmic fairness by providing\nomnipredictors for multi-group loss minimization.",
    "published_date": "2021-09-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.05389v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.05346v1",
    "title": "BGT-Net: Bidirectional GRU Transformer Network for Scene Graph Generation",
    "authors": [
      "Naina Dhingra",
      "Florian Ritter",
      "Andreas Kunz"
    ],
    "author_ids": [],
    "abstract": "Scene graphs are nodes and edges consisting of objects and object-object\nrelationships, respectively. Scene graph generation (SGG) aims to identify the\nobjects and their relationships. We propose a bidirectional GRU (BiGRU)\ntransformer network (BGT-Net) for the scene graph generation for images. This\nmodel implements novel object-object communication to enhance the object\ninformation using a BiGRU layer. Thus, the information of all objects in the\nimage is available for the other objects, which can be leveraged later in the\nobject prediction step. This object information is used in a transformer\nencoder to predict the object class as well as to create object-specific edge\ninformation via the use of another transformer encoder. To handle the dataset\nbias induced by the long-tailed relationship distribution, softening with a\nlog-softmax function and adding a bias adaptation term to regulate the bias for\nevery relation prediction individually showed to be an effective approach. We\nconducted an elaborate study on experiments and ablations using open-source\ndatasets, i.e., Visual Genome, Open-Images, and Visual Relationship Detection\ndatasets, demonstrating the effectiveness of the proposed model over state of\nthe art.",
    "published_date": "2021-09-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.05346v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.05317v1",
    "title": "Bayesian Topic Regression for Causal Inference",
    "authors": [
      "Maximilian Ahrens",
      "Julian Ashwin",
      "Jan-Peter Calliess",
      "Vu Nguyen"
    ],
    "author_ids": [],
    "abstract": "Causal inference using observational text data is becoming increasingly\npopular in many research areas. This paper presents the Bayesian Topic\nRegression (BTR) model that uses both text and numerical information to model\nan outcome variable. It allows estimation of both discrete and continuous\ntreatment effects. Furthermore, it allows for the inclusion of additional\nnumerical confounding factors next to text data. To this end, we combine a\nsupervised Bayesian topic model with a Bayesian regression framework and\nperform supervised representation learning for the text features jointly with\nthe regression parameter training, respecting the Frisch-Waugh-Lovell theorem.\nOur paper makes two main contributions. First, we provide a regression\nframework that allows causal inference in settings when both text and numerical\nconfounders are of relevance. We show with synthetic and semi-synthetic\ndatasets that our joint approach recovers ground truth with lower bias than any\nbenchmark model, when text and numerical features are correlated. Second,\nexperiments on two real-world datasets demonstrate that a joint and supervised\nlearning strategy also yields superior prediction results compared to\nstrategies that estimate regression weights for text and non-text features\nseparately, being even competitive with more complex deep neural networks.",
    "published_date": "2021-09-11T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.05317v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.05267v1",
    "title": "Utility Fairness for the Differentially Private Federated Learning",
    "authors": [
      "Sheeraz A. Alvi",
      "Yi Hong",
      "Salman Durrani"
    ],
    "author_ids": [],
    "abstract": "Federated learning (FL) allows predictive model training on the sensed data\nin a wireless Internet of things (IoT) network evading data collection cost in\nterms of energy, time, and privacy. In this paper, for a FL setting, we model\nthe learning gain achieved by an IoT device against its participation cost as\nits utility. The local model quality and the associated cost differs from\ndevice to device due to the device-heterogeneity which could be time-varying.\nWe identify that this results in utility unfairness because the same global\nmodel is shared among the devices. In the vanilla FL setting, the master is\nunaware of devices' local model computation and transmission costs, thus it is\nunable to address the utility unfairness problem. In addition, a device may\nexploit this lack of knowledge at the master to intentionally reduce its\nexpenditure and thereby boost its utility. We propose to control the quality of\nthe global model shared with the devices, in each round, based on their\ncontribution and expenditure. This is achieved by employing differential\nprivacy to curtail global model divulgence based on the learning contribution.\nFurthermore, we devise adaptive computation and transmission policies for each\ndevice to control its expenditure in order to mitigate utility unfairness. Our\nresults show that the proposed scheme reduces the standard deviation of the\nenergy cost of devices by 99% in comparison to the benchmark scheme, while the\nstandard deviation of the training loss of devices varies around 0.103.",
    "published_date": "2021-09-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.05267v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.05252v1",
    "title": "XCoref: Cross-document Coreference Resolution in the Wild",
    "authors": [
      "Anastasia Zhukova",
      "Felix Hamborg",
      "Karsten Donnay",
      "Bela Gipp"
    ],
    "author_ids": [],
    "abstract": "Datasets and methods for cross-document coreference resolution (CDCR) focus\non events or entities with strict coreference relations. They lack, however,\nannotating and resolving coreference mentions with more abstract or loose\nrelations that may occur when news articles report about controversial and\npolarized events. Bridging and loose coreference relations trigger associations\nthat may lead to exposing news readers to bias by word choice and labeling. For\nexample, coreferential mentions of \"direct talks between U.S. President Donald\nTrump and Kim\" such as \"an extraordinary meeting following months of heated\nrhetoric\" or \"great chance to solve a world problem\" form a more positive\nperception of this event. A step towards bringing awareness of bias by word\nchoice and labeling is the reliable resolution of coreferences with high\nlexical diversity. We propose an unsupervised method named XCoref, which is a\nCDCR method that capably resolves not only previously prevalent entities, such\nas persons, e.g., \"Donald Trump,\" but also abstractly defined concepts, such as\ngroups of persons, \"caravan of immigrants,\" events and actions, e.g., \"marching\nto the U.S. border.\" In an extensive evaluation, we compare the proposed XCoref\nto a state-of-the-art CDCR method and a previous method TCA that resolves such\ncomplex coreference relations and find that XCoref outperforms these methods.\nOutperforming an established CDCR model shows that the new CDCR models need to\nbe evaluated on semantically complex mentions with more loose coreference\nrelations to indicate their applicability of models to resolve mentions in the\n\"wild\" of political news articles.",
    "published_date": "2021-09-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.05252v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.05234v1",
    "title": "Prior Omission of Dissimilar Source Domain(s) for Cost-Effective Few-Shot Learning",
    "authors": [
      "Zezhong Wang",
      "Hongru Wang",
      "Kwan Wai Chung",
      "Jia Zhu",
      "Gabriel Pui Cheong Fung",
      "Kam-Fai Wong"
    ],
    "author_ids": [],
    "abstract": "Few-shot slot tagging is an emerging research topic in the field of Natural\nLanguage Understanding (NLU). With sufficient annotated data from source\ndomains, the key challenge is how to train and adapt the model to another\ntarget domain which only has few labels. Conventional few-shot approaches use\nall the data from the source domains without considering inter-domain relations\nand implicitly assume each sample in the domain contributes equally. However,\nour experiments show that the data distribution bias among different domains\nwill significantly affect the adaption performance. Moreover, transferring\nknowledge from dissimilar domains will even introduce some extra noises so that\naffect the performance of models. To tackle this problem, we propose an\neffective similarity-based method to select data from the source domains. In\naddition, we propose a Shared-Private Network (SP-Net) for the few-shot slot\ntagging task. The words from the same class would have some shared features. We\nextract those shared features from the limited annotated data on the target\ndomain and merge them together as the label embedding to help us predict other\nunlabelled data on the target domain. The experiment shows that our method\noutperforms the state-of-the-art approaches with fewer source data. The result\nalso proves that some training data from dissimilar sources are redundant and\neven negative for the adaption.",
    "published_date": "2021-09-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.05234v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.05178v1",
    "title": "College Student Retention Risk Analysis From Educational Database using Multi-Task Multi-Modal Neural Fusion",
    "authors": [
      "Mohammad Arif Ul Alam"
    ],
    "author_ids": [],
    "abstract": "We develop a Multimodal Spatiotemporal Neural Fusion network for Multi-Task\nLearning (MSNF-MTCL) to predict 5 important students' retention risks: future\ndropout, next semester dropout, type of dropout, duration of dropout and cause\nof dropout. First, we develop a general purpose multi-modal neural fusion\nnetwork model MSNF for learning students' academic information representation\nby fusing spatial and temporal unstructured advising notes with spatiotemporal\nstructured data. MSNF combines a Bidirectional Encoder Representations from\nTransformers (BERT)-based document embedding framework to represent each\nadvising note, Long-Short Term Memory (LSTM) network to model temporal advising\nnote embeddings, LSTM network to model students' temporal performance variables\nand students' static demographics altogether. The final fused representation\nfrom MSNF has been utilized on a Multi-Task Cascade Learning (MTCL) model\ntowards building MSNF-MTCL for predicting 5 student retention risks. We\nevaluate MSNFMTCL on a large educational database consists of 36,445 college\nstudents over 18 years period of time that provides promising performances\ncomparing with the nearest state-of-art models. Additionally, we test the\nfairness of such model given the existence of biases.",
    "published_date": "2021-09-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.05178v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.04999v1",
    "title": "Fairness without the sensitive attribute via Causal Variational Autoencoder",
    "authors": [
      "Vincent Grari",
      "Sylvain Lamprier",
      "Marcin Detyniecki"
    ],
    "author_ids": [],
    "abstract": "In recent years, most fairness strategies in machine learning models focus on\nmitigating unwanted biases by assuming that the sensitive information is\nobserved. However this is not always possible in practice. Due to privacy\npurposes and var-ious regulations such as RGPD in EU, many personal sensitive\nattributes are frequently not collected. We notice a lack of approaches for\nmitigating bias in such difficult settings, in particular for achieving\nclassical fairness objectives such as Demographic Parity and Equalized Odds. By\nleveraging recent developments for approximate inference, we propose an\napproach to fill this gap. Based on a causal graph, we rely on a new\nvariational auto-encoding based framework named SRCVAE to infer a sensitive\ninformation proxy, that serve for bias mitigation in an adversarial fairness\napproach. We empirically demonstrate significant improvements over existing\nworks in the field. We observe that the generated proxy's latent space recovers\nsensitive information and that our approach achieves a higher accuracy while\nobtaining the same level of fairness on two real datasets, as measured using\ncom-mon fairness definitions.",
    "published_date": "2021-09-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.04999v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.04976v1",
    "title": "Optimal bounds for bit-sizes of stationary distributions in finite Markov chains",
    "authors": [
      "Mateusz Skomra"
    ],
    "author_ids": [],
    "abstract": "An irreducible stochastic matrix with rational entries has a stationary\ndistribution given by a vector of rational numbers. We give an upper bound on\nthe lowest common denominator of the entries of this vector. Bounds of this\nkind are used to study the complexity of algorithms for solving stochastic mean\npayoff games. They are usually derived using the Hadamard inequality, but this\nleads to suboptimal results. We replace the Hadamard inequality with the Markov\nchain tree formula in order to obtain optimal bounds. We also adapt our\napproach to obtain bounds on the absorption probabilities of finite Markov\nchains and on the gains and bias vectors of Markov chains with rewards.",
    "published_date": "2021-09-10T00:00:00",
    "year": 2021,
    "categories": [
      "math.CO",
      "cs.GT",
      "math.PR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.04976v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2109.04813v3",
    "title": "TACS: Taxonomy Adaptive Cross-Domain Semantic Segmentation",
    "authors": [
      "Rui Gong",
      "Martin Danelljan",
      "Dengxin Dai",
      "Danda Pani Paudel",
      "Ajad Chhatkuli",
      "Fisher Yu",
      "Luc Van Gool"
    ],
    "author_ids": [],
    "abstract": "Traditional domain adaptive semantic segmentation addresses the task of\nadapting a model to a novel target domain under limited or no additional\nsupervision. While tackling the input domain gap, the standard domain\nadaptation settings assume no domain change in the output space. In semantic\nprediction tasks, different datasets are often labeled according to different\nsemantic taxonomies. In many real-world settings, the target domain task\nrequires a different taxonomy than the one imposed by the source domain. We\ntherefore introduce the more general taxonomy adaptive cross-domain semantic\nsegmentation (TACS) problem, allowing for inconsistent taxonomies between the\ntwo domains. We further propose an approach that jointly addresses the\nimage-level and label-level domain adaptation. On the label-level, we employ a\nbilateral mixed sampling strategy to augment the target domain, and a\nrelabelling method to unify and align the label spaces. We address the\nimage-level domain gap by proposing an uncertainty-rectified contrastive\nlearning method, leading to more domain-invariant and class-discriminative\nfeatures. We extensively evaluate the effectiveness of our framework under\ndifferent TACS settings: open taxonomy, coarse-to-fine taxonomy, and\nimplicitly-overlapping taxonomy. Our approach outperforms the previous\nstate-of-the-art by a large margin, while being capable of adapting to target\ntaxonomies. Our implementation is publicly available at\nhttps://github.com/ETHRuiGong/TADA.",
    "published_date": "2021-09-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.04813v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.04732v1",
    "title": "Assessing the Reliability of Word Embedding Gender Bias Measures",
    "authors": [
      "Yupei Du",
      "Qixiang Fang",
      "Dong Nguyen"
    ],
    "author_ids": [],
    "abstract": "Various measures have been proposed to quantify human-like social biases in\nword embeddings. However, bias scores based on these measures can suffer from\nmeasurement error. One indication of measurement quality is reliability,\nconcerning the extent to which a measure produces consistent results. In this\npaper, we assess three types of reliability of word embedding gender bias\nmeasures, namely test-retest reliability, inter-rater consistency and internal\nconsistency. Specifically, we investigate the consistency of bias scores across\ndifferent choices of random seeds, scoring rules and words. Furthermore, we\nanalyse the effects of various factors on these measures' reliability scores.\nOur findings inform better design of word embedding gender bias measures.\nMoreover, we urge researchers to be more critical about the application of such\nmeasures.",
    "published_date": "2021-09-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.04732v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.04727v1",
    "title": "A Simple and Effective Method To Eliminate the Self Language Bias in Multilingual Representations",
    "authors": [
      "Ziyi Yang",
      "Yinfei Yang",
      "Daniel Cer",
      "Eric Darve"
    ],
    "author_ids": [],
    "abstract": "Language agnostic and semantic-language information isolation is an emerging\nresearch direction for multilingual representations models. We explore this\nproblem from a novel angle of geometric algebra and semantic space. A simple\nbut highly effective method \"Language Information Removal (LIR)\" factors out\nlanguage identity information from semantic related components in multilingual\nrepresentations pre-trained on multi-monolingual data. A post-training and\nmodel-agnostic method, LIR only uses simple linear operations, e.g. matrix\nfactorization and orthogonal projection. LIR reveals that for weak-alignment\nmultilingual systems, the principal components of semantic spaces primarily\nencodes language identity information. We first evaluate the LIR on a\ncross-lingual question answer retrieval task (LAReQA), which requires the\nstrong alignment for the multilingual embedding space. Experiment shows that\nLIR is highly effectively on this task, yielding almost 100% relative\nimprovement in MAP for weak-alignment models. We then evaluate the LIR on\nAmazon Reviews and XEVAL dataset, with the observation that removing language\ninformation is able to improve the cross-lingual transfer performance.",
    "published_date": "2021-09-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.04727v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.04624v1",
    "title": "Style Pooling: Automatic Text Style Obfuscation for Improved Classification Fairness",
    "authors": [
      "Fatemehsadat Mireshghallah",
      "Taylor Berg-Kirkpatrick"
    ],
    "author_ids": [],
    "abstract": "Text style can reveal sensitive attributes of the author (e.g. race or age)\nto the reader, which can, in turn, lead to privacy violations and bias in both\nhuman and algorithmic decisions based on text. For example, the style of\nwriting in job applications might reveal protected attributes of the candidate\nwhich could lead to bias in hiring decisions, regardless of whether hiring\ndecisions are made algorithmically or by humans. We propose a VAE-based\nframework that obfuscates stylistic features of human-generated text through\nstyle transfer by automatically re-writing the text itself. Our framework\noperationalizes the notion of obfuscated style in a flexible way that enables\ntwo distinct notions of obfuscated style: (1) a minimal notion that effectively\nintersects the various styles seen in training, and (2) a maximal notion that\nseeks to obfuscate by adding stylistic features of all sensitive attributes to\ntext, in effect, computing a union of styles. Our style-obfuscation framework\ncan be used for multiple purposes, however, we demonstrate its effectiveness in\nimproving the fairness of downstream classifiers. We also conduct a\ncomprehensive study on style pooling's effect on fluency, semantic consistency,\nand attribute removal from text, in two and three domain style obfuscation.",
    "published_date": "2021-09-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.04624v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.04554v2",
    "title": "Feature-based Individual Fairness in k-Clustering",
    "authors": [
      "Debajyoti Kar",
      "Mert Kosan",
      "Debmalya Mandal",
      "Sourav Medya",
      "Arlei Silva",
      "Palash Dey",
      "Swagato Sanyal"
    ],
    "author_ids": [],
    "abstract": "Ensuring fairness in machine learning algorithms is a challenging and\nessential task. We consider the problem of clustering a set of points while\nsatisfying fairness constraints. While there have been several attempts to\ncapture group fairness in the $k$-clustering problem, fairness at an individual\nlevel is relatively less explored. We introduce a new notion of individual\nfairness in $k$-clustering based on features not necessarily used for\nclustering. We show that this problem is NP-hard and does not admit a constant\nfactor approximation. Therefore, we design a randomized algorithm that\nguarantees approximation both in terms of minimizing the clustering distance\nobjective and individual fairness under natural restrictions on the distance\nmetric and fairness constraints. Finally, our experimental results against six\ncompeting baselines validate that our algorithm produces individually fairer\nclusters than the fairest baseline by 12.5% on average while also being less\ncostly in terms of the clustering objective than the best baseline by 34.5% on\naverage.",
    "published_date": "2021-09-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.04554v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.04517v2",
    "title": "Prediction and Prevention of Pandemics via Graphical Model Inference and Convex Programming",
    "authors": [
      "Mikhail Krechetov",
      "Amir Mohammad Esmaieeli Sikaroudi",
      "Alon Efrat",
      "Valentin Polishchuk",
      "Michael Chertkov"
    ],
    "author_ids": [],
    "abstract": "Hard-to-predict bursts of COVID-19 pandemic revealed significance of\nstatistical modeling which would resolve spatio-temporal correlations over\ngeographical areas, for example spread of the infection over a city with census\ntract granularity. In this manuscript, we provide algorithmic answers to the\nfollowing two inter-related public health challenges. (1) Inference Challenge:\nassuming that there are $N$ census blocks (nodes) in the city, and given an\ninitial infection at any set of nodes, what is the probability for a subset of\ncensus blocks to become infected by the time the spread of the infection burst\nis stabilized? (2) Prevention Challenge: What is the minimal control action one\ncan take to minimize the infected part of the stabilized state footprint? To\nanswer the challenges, we build a Graphical Model of pandemic of the attractive\nIsing (pair-wise, binary) type, where each node represents a census track and\neach edge factor represents the strength of the pairwise interaction between a\npair of nodes. We show that almost all attractive Ising Models on dense graphs\nresult in either of the two modes for the most probable state: either all nodes\nwhich were not infected initially became infected, or all the initially\nuninfected nodes remain uninfected. This bi-modal solution of the Inference\nChallenge allows us to re-state the Prevention Challenge as the following\ntractable convex programming: for the bare Ising Model with pair-wise and bias\nfactors representing the system without prevention measures, such that the MAP\nstate is fully infected for at least one of the initial infection patterns,\nfind the closest, in $l_1$ norm, set of factors resulting in all the MAP states\nof the Ising model, with the optimal prevention measures applied, to become\nsafe.",
    "published_date": "2021-09-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.04517v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2109.04399v2",
    "title": "Gradual (In)Compatibility of Fairness Criteria",
    "authors": [
      "Corinna Hertweck",
      "Tim Räz"
    ],
    "author_ids": [],
    "abstract": "Impossibility results show that important fairness measures (independence,\nseparation, sufficiency) cannot be satisfied at the same time under reasonable\nassumptions. This paper explores whether we can satisfy and/or improve these\nfairness measures simultaneously to a certain degree. We introduce\ninformation-theoretic formulations of the fairness measures and define degrees\nof fairness based on these formulations. The information-theoretic formulations\nsuggest unexplored theoretical relations between the three fairness measures.\nIn the experimental part, we use the information-theoretic expressions as\nregularizers to obtain fairness-regularized predictors for three standard\ndatasets. Our experiments show that a) fairness regularization directly\nincreases fairness measures, in line with existing work, and b) some fairness\nregularizations indirectly increase other fairness measures, as suggested by\nour theoretical findings. This establishes that it is possible to increase the\ndegree to which some fairness measures are satisfied at the same time -- some\nfairness measures are gradually compatible.",
    "published_date": "2021-09-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.04399v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.04374v4",
    "title": "IFBiD: Inference-Free Bias Detection",
    "authors": [
      "Ignacio Serna",
      "Daniel DeAlcala",
      "Aythami Morales",
      "Julian Fierrez",
      "Javier Ortega-Garcia"
    ],
    "author_ids": [],
    "abstract": "This paper is the first to explore an automatic way to detect bias in deep\nconvolutional neural networks by simply looking at their weights. Furthermore,\nit is also a step towards understanding neural networks and how they work. We\nshow that it is indeed possible to know if a model is biased or not simply by\nlooking at its weights, without the model inference for an specific input. We\nanalyze how bias is encoded in the weights of deep networks through a toy\nexample using the Colored MNIST database and we also provide a realistic case\nstudy in gender detection from face images using state-of-the-art methods and\nexperimental resources. To do so, we generated two databases with 36K and 48K\nbiased models each. In the MNIST models we were able to detect whether they\npresented a strong or low bias with more than 99% accuracy, and we were also\nable to classify between four levels of bias with more than 70% accuracy. For\nthe face models, we achieved 90% accuracy in distinguishing between models\nbiased towards Asian, Black, or Caucasian ethnicity.",
    "published_date": "2021-09-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.04374v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.04284v1",
    "title": "Towards Robust Cross-domain Image Understanding with Unsupervised Noise Removal",
    "authors": [
      "Lei Zhu",
      "Zhaojing Luo",
      "Wei Wang",
      "Meihui Zhang",
      "Gang Chen",
      "Kaiping Zheng"
    ],
    "author_ids": [],
    "abstract": "Deep learning models usually require a large amount of labeled data to\nachieve satisfactory performance. In multimedia analysis, domain adaptation\nstudies the problem of cross-domain knowledge transfer from a label rich source\ndomain to a label scarce target domain, thus potentially alleviates the\nannotation requirement for deep learning models. However, we find that\ncontemporary domain adaptation methods for cross-domain image understanding\nperform poorly when source domain is noisy. Weakly Supervised Domain Adaptation\n(WSDA) studies the domain adaptation problem under the scenario where source\ndata can be noisy. Prior methods on WSDA remove noisy source data and align the\nmarginal distribution across domains without considering the fine-grained\nsemantic structure in the embedding space, which have the problem of class\nmisalignment, e.g., features of cats in the target domain might be mapped near\nfeatures of dogs in the source domain. In this paper, we propose a novel\nmethod, termed Noise Tolerant Domain Adaptation, for WSDA. Specifically, we\nadopt the cluster assumption and learn cluster discriminatively with class\nprototypes in the embedding space. We propose to leverage the location\ninformation of the data points in the embedding space and model the location\ninformation with a Gaussian mixture model to identify noisy source data. We\nthen design a network which incorporates the Gaussian mixture noise model as a\nsub-module for unsupervised noise removal and propose a novel cluster-level\nadversarial adaptation method which aligns unlabeled target data with the less\nnoisy class prototypes for mapping the semantic structure across domains. We\nconduct extensive experiments to evaluate the effectiveness of our method on\nboth general images and medical images from COVID-19 and e-commerce datasets.\nThe results show that our method significantly outperforms state-of-the-art\nWSDA methods.",
    "published_date": "2021-09-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.04284v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.04230v1",
    "title": "A Systematic Approach to Group Fairness in Automated Decision Making",
    "authors": [
      "Corinna Hertweck",
      "Christoph Heitz"
    ],
    "author_ids": [],
    "abstract": "While the field of algorithmic fairness has brought forth many ways to\nmeasure and improve the fairness of machine learning models, these findings are\nstill not widely used in practice. We suspect that one reason for this is that\nthe field of algorithmic fairness came up with a lot of definitions of\nfairness, which are difficult to navigate. The goal of this paper is to provide\ndata scientists with an accessible introduction to group fairness metrics and\nto give some insight into the philosophical reasoning for caring about these\nmetrics. We will do this by considering in which sense socio-demographic groups\nare compared for making a statement on fairness.",
    "published_date": "2021-09-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.04230v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.04141v1",
    "title": "Nonlocal reaction traffic flow model with on-off ramps",
    "authors": [
      "F. A. Chiarello",
      "H. D. Contreras",
      "L. M. Villada"
    ],
    "author_ids": [],
    "abstract": "We present a non-local version of a scalar balance law modeling traffic flow\nwith on-ramps and off-ramps. The source term is used to describe the traffic\nflow over the on-ramp and off-ramps. We approximate the problem using an\nupwind-type numerical scheme and we provide L^\\infty and BV estimates for the\nsequence of approximate solutions. Together with a discrete entropy inequality,\nwe also show the well-posedness of the considered class of scalar balance laws.\nSome numerical simulations illustrate the behaviour of solutions in sample\ncases.",
    "published_date": "2021-09-09T00:00:00",
    "year": 2021,
    "categories": [
      "math.AP",
      "cs.NA",
      "math.NA",
      "35L65, 65M12, 90B20"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.04141v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2109.04114v2",
    "title": "Fixing exposure bias with imitation learning needs powerful oracles",
    "authors": [
      "Luca Hormann",
      "Artem Sokolov"
    ],
    "author_ids": [],
    "abstract": "We apply imitation learning (IL) to tackle the NMT exposure bias problem with\nerror-correcting oracles, and evaluate an SMT lattice-based oracle which,\ndespite its excellent performance in an unconstrained oracle translation task,\nturned out to be too pruned and idiosyncratic to serve as the oracle for IL.",
    "published_date": "2021-09-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.04114v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.04095v1",
    "title": "Debiasing Methods in Natural Language Understanding Make Bias More Accessible",
    "authors": [
      "Michael Mendelson",
      "Yonatan Belinkov"
    ],
    "author_ids": [],
    "abstract": "Model robustness to bias is often determined by the generalization on\ncarefully designed out-of-distribution datasets. Recent debiasing methods in\nnatural language understanding (NLU) improve performance on such datasets by\npressuring models into making unbiased predictions. An underlying assumption\nbehind such methods is that this also leads to the discovery of more robust\nfeatures in the model's inner representations. We propose a general\nprobing-based framework that allows for post-hoc interpretation of biases in\nlanguage models, and use an information-theoretic approach to measure the\nextractability of certain biases from the model's representations. We\nexperiment with several NLU datasets and known biases, and show that,\ncounter-intuitively, the more a language model is pushed towards a debiased\nregime, the more bias is actually encoded in its inner representations.",
    "published_date": "2021-09-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "68T50",
      "I.2.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.04095v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.03952v1",
    "title": "Attributing Fair Decisions with Attention Interventions",
    "authors": [
      "Ninareh Mehrabi",
      "Umang Gupta",
      "Fred Morstatter",
      "Greg Ver Steeg",
      "Aram Galstyan"
    ],
    "author_ids": [],
    "abstract": "The widespread use of Artificial Intelligence (AI) in consequential domains,\nsuch as healthcare and parole decision-making systems, has drawn intense\nscrutiny on the fairness of these methods. However, ensuring fairness is often\ninsufficient as the rationale for a contentious decision needs to be audited,\nunderstood, and defended. We propose that the attention mechanism can be used\nto ensure fair outcomes while simultaneously providing feature attributions to\naccount for how a decision was made. Toward this goal, we design an\nattention-based model that can be leveraged as an attribution framework. It can\nidentify features responsible for both performance and fairness of the model\nthrough attention interventions and attention weight manipulation. Using this\nattribution framework, we then design a post-processing bias mitigation\nstrategy and compare it with a suite of baselines. We demonstrate the\nversatility of our approach by conducting experiments on two distinct data\ntypes, tabular and textual.",
    "published_date": "2021-09-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.03952v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.03946v1",
    "title": "Quantitative form of Ball's Cube slicing in $\\mathbb{R}^n$ and equality cases in the min-entropy power inequality",
    "authors": [
      "James Melbourne",
      "Cyril Roberto"
    ],
    "author_ids": [],
    "abstract": "We prove a quantitative form of the celebrated Ball's theorem on cube slicing\nin $\\mathbb{R}^n$ and obtain, as a consequence, equality cases in the\nmin-entropy power inequality. Independently, we also give a quantitative form\nof Khintchine's inequality in the special case $p=1$.",
    "published_date": "2021-09-08T00:00:00",
    "year": 2021,
    "categories": [
      "math.PR",
      "cs.IT",
      "math.FA",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.03946v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2109.03930v1",
    "title": "Matrix Completion of World Trade",
    "authors": [
      "Gnecco Giorgio",
      "Nutarelli Federico",
      "Riccaboni Massimo"
    ],
    "author_ids": [],
    "abstract": "This work applies Matrix Completion (MC) -- a class of machine-learning\nmethods commonly used in the context of recommendation systems -- to analyse\neconomic complexity. MC is applied to reconstruct the Revealed Comparative\nAdvantage (RCA) matrix, whose elements express the relative advantage of\ncountries in given classes of products, as evidenced by yearly trade flows. A\nhigh-accuracy binary classifier is derived from the application of MC, with the\naim of discriminating between elements of the RCA matrix that are,\nrespectively, higher or lower than one. We introduce a novel Matrix cOmpletion\niNdex of Economic complexitY (MONEY) based on MC, which is related to the\npredictability of countries' RCA (the lower the predictability, the higher the\ncomplexity). Differently from previously-developed indices of economic\ncomplexity, the MONEY index takes into account the various singular vectors of\nthe matrix reconstructed by MC, whereas other indices are based only on one/two\neigenvectors of a suitable symmetric matrix, derived from the RCA matrix.\nFinally, MC is compared with a state-of-the-art economic complexity index\n(GENEPY). We show that the false positive rate per country of a binary\nclassifier constructed starting from the average entry-wise output of MC can be\nused as a proxy of GENEPY.",
    "published_date": "2021-09-08T00:00:00",
    "year": 2021,
    "categories": [
      "econ.GN",
      "cs.LG",
      "q-fin.EC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.03930v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.03858v2",
    "title": "Collecting a Large-Scale Gender Bias Dataset for Coreference Resolution and Machine Translation",
    "authors": [
      "Shahar Levy",
      "Koren Lazar",
      "Gabriel Stanovsky"
    ],
    "author_ids": [],
    "abstract": "Recent works have found evidence of gender bias in models of machine\ntranslation and coreference resolution using mostly synthetic diagnostic\ndatasets. While these quantify bias in a controlled experiment, they often do\nso on a small scale and consist mostly of artificial, out-of-distribution\nsentences. In this work, we find grammatical patterns indicating stereotypical\nand non-stereotypical gender-role assignments (e.g., female nurses versus male\ndancers) in corpora from three domains, resulting in a first large-scale gender\nbias dataset of 108K diverse real-world English sentences. We manually verify\nthe quality of our corpus and use it to evaluate gender bias in various\ncoreference resolution and machine translation models. We find that all tested\nmodels tend to over-rely on gender stereotypes when presented with natural\ninputs, which may be especially harmful when deployed in commercial systems.\nFinally, we show that our dataset lends itself to finetuning a coreference\nresolution model, finding it mitigates bias on a held out set. Our dataset and\nmodels are publicly available at www.github.com/SLAB-NLP/BUG. We hope they will\nspur future research into gender bias evaluation mitigation techniques in\nrealistic settings.",
    "published_date": "2021-09-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.03858v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.03646v1",
    "title": "Sustainable Modular Debiasing of Language Models",
    "authors": [
      "Anne Lauscher",
      "Tobias Lüken",
      "Goran Glavaš"
    ],
    "author_ids": [],
    "abstract": "Unfair stereotypical biases (e.g., gender, racial, or religious biases)\nencoded in modern pretrained language models (PLMs) have negative ethical\nimplications for widespread adoption of state-of-the-art language technology.\nTo remedy for this, a wide range of debiasing techniques have recently been\nintroduced to remove such stereotypical biases from PLMs. Existing debiasing\nmethods, however, directly modify all of the PLMs parameters, which -- besides\nbeing computationally expensive -- comes with the inherent risk of\n(catastrophic) forgetting of useful language knowledge acquired in pretraining.\nIn this work, we propose a more sustainable modular debiasing approach based on\ndedicated debiasing adapters, dubbed ADELE. Concretely, we (1) inject adapter\nmodules into the original PLM layers and (2) update only the adapters (i.e., we\nkeep the original PLM parameters frozen) via language modeling training on a\ncounterfactually augmented corpus. We showcase ADELE, in gender debiasing of\nBERT: our extensive evaluation, encompassing three intrinsic and two extrinsic\nbias measures, renders ADELE, very effective in bias mitigation. We further\nshow that -- due to its modular nature -- ADELE, coupled with task adapters,\nretains fairness even after large-scale downstream training. Finally, by means\nof multilingual BERT, we successfully transfer ADELE, to six target languages.",
    "published_date": "2021-09-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.03646v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.03604v1",
    "title": "Power to the Relational Inductive Bias: Graph Neural Networks in Electrical Power Grids",
    "authors": [
      "Martin Ringsquandl",
      "Houssem Sellami",
      "Marcel Hildebrandt",
      "Dagmar Beyer",
      "Sylwia Henselmeyer",
      "Sebastian Weber",
      "Mitchell Joblin"
    ],
    "author_ids": [],
    "abstract": "The application of graph neural networks (GNNs) to the domain of electrical\npower grids has high potential impact on smart grid monitoring. Even though\nthere is a natural correspondence of power flow to message-passing in GNNs,\ntheir performance on power grids is not well-understood. We argue that there is\na gap between GNN research driven by benchmarks which contain graphs that\ndiffer from power grids in several important aspects. Additionally, inductive\nlearning of GNNs across multiple power grid topologies has not been explored\nwith real-world data. We address this gap by means of (i) defining power grid\ngraph datasets in inductive settings, (ii) an exploratory analysis of graph\nproperties, and (iii) an empirical study of the concrete learning task of state\nestimation on real-world power grids. Our results show that GNNs are more\nrobust to noise with up to 400% lower error compared to baselines. Furthermore,\ndue to the unique properties of electrical grids, we do not observe the well\nknown over-smoothing phenomenon of GNNs and find the best performing models to\nbe exceptionally deep with up to 13 layers. This is in stark contrast to\nexisting benchmark datasets where the consensus is that 2 to 3 layer GNNs\nperform best. Our results demonstrate that a key challenge in this domain is to\neffectively handle long-range dependence.",
    "published_date": "2021-09-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "eess.SP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.03604v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.03300v1",
    "title": "Hi, my name is Martha: Using names to measure and mitigate bias in generative dialogue models",
    "authors": [
      "Eric Michael Smith",
      "Adina Williams"
    ],
    "author_ids": [],
    "abstract": "All AI models are susceptible to learning biases in data that they are\ntrained on. For generative dialogue models, being trained on real human\nconversations containing unbalanced gender and race/ethnicity references can\nlead to models that display learned biases, which we define here broadly as any\nmeasurable differences in the distributions of words or semantic content of\nconversations based on demographic groups. We measure the strength of such\nbiases by producing artificial conversations between two copies of a dialogue\nmodel, conditioning one conversational partner to state a name commonly\nassociated with a certain gender and/or race/ethnicity. We find that larger\ncapacity models tend to exhibit more gender bias and greater stereotyping of\noccupations by gender. We show that several methods of tuning these dialogue\nmodels, specifically name scrambling, controlled generation, and unlikelihood\ntraining, are effective in reducing bias in conversation, including on a\ndownstream conversational task. Name scrambling is also effective in lowering\ndifferences in token usage across conversations where partners have names\nassociated with different genders or races/ethnicities.",
    "published_date": "2021-09-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.03300v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.03285v1",
    "title": "Amazon SageMaker Clarify: Machine Learning Bias Detection and Explainability in the Cloud",
    "authors": [
      "Michaela Hardt",
      "Xiaoguang Chen",
      "Xiaoyi Cheng",
      "Michele Donini",
      "Jason Gelman",
      "Satish Gollaprolu",
      "John He",
      "Pedro Larroy",
      "Xinyu Liu",
      "Nick McCarthy",
      "Ashish Rathi",
      "Scott Rees",
      "Ankit Siva",
      "ErhYuan Tsai",
      "Keerthan Vasist",
      "Pinar Yilmaz",
      "Muhammad Bilal Zafar",
      "Sanjiv Das",
      "Kevin Haas",
      "Tyler Hill",
      "Krishnaram Kenthapadi"
    ],
    "author_ids": [],
    "abstract": "Understanding the predictions made by machine learning (ML) models and their\npotential biases remains a challenging and labor-intensive task that depends on\nthe application, the dataset, and the specific model. We present Amazon\nSageMaker Clarify, an explainability feature for Amazon SageMaker that launched\nin December 2020, providing insights into data and ML models by identifying\nbiases and explaining predictions. It is deeply integrated into Amazon\nSageMaker, a fully managed service that enables data scientists and developers\nto build, train, and deploy ML models at any scale. Clarify supports bias\ndetection and feature importance computation across the ML lifecycle, during\ndata preparation, model evaluation, and post-deployment monitoring. We outline\nthe desiderata derived from customer input, the modular architecture, and the\nmethodology for bias and explanation computations. Further, we describe the\ntechnical challenges encountered and the tradeoffs we had to make. For\nillustration, we discuss two customer use cases. We present our deployment\nresults including qualitative customer feedback and a quantitative evaluation.\nFinally, we summarize lessons learned, and discuss best practices for the\nsuccessful adoption of fairness and explanation tools in practice.",
    "published_date": "2021-09-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.03285v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.03229v4",
    "title": "Rethinking Common Assumptions to Mitigate Racial Bias in Face Recognition Datasets",
    "authors": [
      "Matthew Gwilliam",
      "Srinidhi Hegde",
      "Lade Tinubu",
      "Alex Hanson"
    ],
    "author_ids": [],
    "abstract": "Many existing works have made great strides towards reducing racial bias in\nface recognition. However, most of these methods attempt to rectify bias that\nmanifests in models during training instead of directly addressing a major\nsource of the bias, the dataset itself. Exceptions to this are\nBUPT-Balancedface/RFW and Fairface, but these works assume that primarily\ntraining on a single race or not racially balancing the dataset are inherently\ndisadvantageous. We demonstrate that these assumptions are not necessarily\nvalid. In our experiments, training on only African faces induced less bias\nthan training on a balanced distribution of faces and distributions skewed to\ninclude more African faces produced more equitable models. We additionally\nnotice that adding more images of existing identities to a dataset in place of\nadding new identities can lead to accuracy boosts across racial categories. Our\ncode is available at\nhttps://github.com/j-alex-hanson/rethinking-race-face-datasets.",
    "published_date": "2021-09-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.03229v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.03201v6",
    "title": "nnFormer: Interleaved Transformer for Volumetric Segmentation",
    "authors": [
      "Hong-Yu Zhou",
      "Jiansen Guo",
      "Yinghao Zhang",
      "Lequan Yu",
      "Liansheng Wang",
      "Yizhou Yu"
    ],
    "author_ids": [],
    "abstract": "Transformer, the model of choice for natural language processing, has drawn\nscant attention from the medical imaging community. Given the ability to\nexploit long-term dependencies, transformers are promising to help atypical\nconvolutional neural networks to overcome their inherent shortcomings of\nspatial inductive bias. However, most of recently proposed transformer-based\nsegmentation approaches simply treated transformers as assisted modules to help\nencode global context into convolutional representations. To address this\nissue, we introduce nnFormer, a 3D transformer for volumetric medical image\nsegmentation. nnFormer not only exploits the combination of interleaved\nconvolution and self-attention operations, but also introduces local and global\nvolume-based self-attention mechanism to learn volume representations.\nMoreover, nnFormer proposes to use skip attention to replace the traditional\nconcatenation/summation operations in skip connections in U-Net like\narchitecture. Experiments show that nnFormer significantly outperforms previous\ntransformer-based counterparts by large margins on three public datasets.\nCompared to nnUNet, nnFormer produces significantly lower HD95 and comparable\nDSC results. Furthermore, we show that nnFormer and nnUNet are highly\ncomplementary to each other in model ensembling.",
    "published_date": "2021-09-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.03201v6",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.02944v1",
    "title": "Dutch Comfort: The limits of AI governance through municipal registers",
    "authors": [
      "Corinne Cath",
      "Fieke Jansen"
    ],
    "author_ids": [],
    "abstract": "In this commentary, we respond to a recent editorial letter by Professor\nLuciano Floridi entitled 'AI as a public service: Learning from Amsterdam and\nHelsinki'. Here, Floridi considers the positive impact of these municipal AI\nregisters, which collect a limited number of algorithmic systems used by the\ncity of Amsterdam and Helsinki. There are a number of assumptions about AI\nregisters as a governance model for automated systems that we seek to question.\nStarting with recent attempts to normalize AI by decontextualizing and\ndepoliticizing it, which is a fraught political project that encourages what we\ncall 'ethics theater' given the proven dangers of using these systems in the\ncontext of the digital welfare state. We agree with Floridi that much can be\nlearned from these registers about the role of AI systems in municipal city\nmanagement. Yet, the lessons we draw, on the basis of our extensive\nethnographic engagement with digital well-fare states are distinctly less\noptimistic.",
    "published_date": "2021-09-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.DB",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.02944v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.02866v1",
    "title": "Readying Medical Students for Medical AI: The Need to Embed AI Ethics Education",
    "authors": [
      "Thomas P Quinn",
      "Simon Coghlan"
    ],
    "author_ids": [],
    "abstract": "Medical students will almost inevitably encounter powerful medical AI systems\nearly in their careers. Yet, contemporary medical education does not adequately\nequip students with the basic clinical proficiency in medical AI needed to use\nthese tools safely and effectively. Education reform is urgently needed, but\nnot easily implemented, largely due to an already jam-packed medical curricula.\nIn this article, we propose an education reform framework as an effective and\nefficient solution, which we call the Embedded AI Ethics Education Framework.\nUnlike other calls for education reform to accommodate AI teaching that are\nmore radical in scope, our framework is modest and incremental. It leverages\nexisting bioethics or medical ethics curricula to develop and deliver content\non the ethical issues associated with medical AI, especially the harms of\ntechnology misuse, disuse, and abuse that affect the risk-benefit analyses at\nthe heart of healthcare. In doing so, the framework provides a simple tool for\ngoing beyond the \"What?\" and the \"Why?\" of medical AI ethics education, to\nanswer the \"How?\", giving universities, course directors, and/or professors a\nbroad road-map for equipping their students with the necessary clinical\nproficiency in medical AI.",
    "published_date": "2021-09-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.02866v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.02840v1",
    "title": "CIM: Class-Irrelevant Mapping for Few-Shot Classification",
    "authors": [
      "Shuai Shao",
      "Lei Xing",
      "Yixin Chen",
      "Yan-Jiang Wang",
      "Bao-Di Liu",
      "Yicong Zhou"
    ],
    "author_ids": [],
    "abstract": "Few-shot classification (FSC) is one of the most concerned hot issues in\nrecent years. The general setting consists of two phases: (1) Pre-train a\nfeature extraction model (FEM) with base data (has large amounts of labeled\nsamples). (2) Use the FEM to extract the features of novel data (with few\nlabeled samples and totally different categories from base data), then classify\nthem with the to-be-designed classifier. The adaptability of pre-trained FEM to\nnovel data determines the accuracy of novel features, thereby affecting the\nfinal classification performances. To this end, how to appraise the pre-trained\nFEM is the most crucial focus in the FSC community. It sounds like traditional\nClass Activate Mapping (CAM) based methods can achieve this by overlaying\nweighted feature maps. However, due to the particularity of FSC (e.g., there is\nno backpropagation when using the pre-trained FEM to extract novel features),\nwe cannot activate the feature map with the novel classes. To address this\nchallenge, we propose a simple, flexible method, dubbed as Class-Irrelevant\nMapping (CIM). Specifically, first, we introduce dictionary learning theory and\nview the channels of the feature map as the bases in a dictionary. Then we\nutilize the feature map to fit the feature vector of an image to achieve the\ncorresponding channel weights. Finally, we overlap the weighted feature map for\nvisualization to appraise the ability of pre-trained FEM on novel data. For\nfair use of CIM in evaluating different models, we propose a new measurement\nindex, called Feature Localization Accuracy (FLA). In experiments, we first\ncompare our CIM with CAM in regular tasks and achieve outstanding performances.\nNext, we use our CIM to appraise several classical FSC frameworks without\nconsidering the classification results and discuss them.",
    "published_date": "2021-09-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.02840v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.02691v1",
    "title": "SS-BERT: Mitigating Identity Terms Bias in Toxic Comment Classification by Utilising the Notion of \"Subjectivity\" and \"Identity Terms\"",
    "authors": [
      "Zhixue Zhao",
      "Ziqi Zhang",
      "Frank Hopfgartner"
    ],
    "author_ids": [],
    "abstract": "Toxic comment classification models are often found biased toward identity\nterms which are terms characterizing a specific group of people such as\n\"Muslim\" and \"black\". Such bias is commonly reflected in false-positive\npredictions, i.e. non-toxic comments with identity terms. In this work, we\npropose a novel approach to tackle such bias in toxic comment classification,\nleveraging the notion of subjectivity level of a comment and the presence of\nidentity terms. We hypothesize that when a comment is made about a group of\npeople that is characterized by an identity term, the likelihood of that\ncomment being toxic is associated with the subjectivity level of the comment,\ni.e. the extent to which the comment conveys personal feelings and opinions.\nBuilding upon the BERT model, we propose a new structure that is able to\nleverage these features, and thoroughly evaluate our model on 4 datasets of\nvarying sizes and representing different social media platforms. The results\nshow that our model can consistently outperform BERT and a SOTA model devised\nto address identity term bias in a different way, with a maximum improvement in\nF1 of 2.43% and 1.91% respectively.",
    "published_date": "2021-09-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.02691v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.02538v2",
    "title": "Bounding Means of Discrete Distributions",
    "authors": [
      "Eric Bax",
      "Frédéric Ouimet"
    ],
    "author_ids": [],
    "abstract": "We introduce methods to bound the mean of a discrete distribution (or finite\npopulation) based on sample data, for random variables with a known set of\npossible values. In particular, the methods can be applied to categorical data\nwith known category-based values. For small sample sizes, we show how to\nleverage the knowledge of the set of possible values to compute bounds that are\nstronger than for general random variables such as standard concentration\ninequalities.",
    "published_date": "2021-09-06T00:00:00",
    "year": 2021,
    "categories": [
      "math.ST",
      "cs.IT",
      "math.IT",
      "math.PR",
      "stat.TH",
      "62G15, 62G05"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.02538v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2109.02645v1",
    "title": "Backpropagation and fuzzy algorithm Modelling to Resolve Blood Supply Chain Issues in the Covid-19 Pandemic",
    "authors": [
      "Aan Erlansari",
      "Rusdi Effendi",
      "Funny Farady C",
      "Andang Wijanarko",
      "Boko Susilo",
      "Reza Hardiansyah"
    ],
    "author_ids": [],
    "abstract": "Bloodstock shortages and its uncertain demand has become a major problem for\nall countries worldwide. Therefore, this study aims to provide solution to the\nissues of blood distribution during the Covid-19 Pandemic at Bengkulu,\nIndonesia. The Backpropagation algorithm was used to improve the possibility of\ndiscovering available and potential donors. Furthermore, the distances, age,\nand length of donation were measured to obtain the right person to donate blood\nwhen it needed. The Backpropagation uses three input layers to classify\neligible donors, namely age, body, weight, and bias. In addition, the system\nthrough its query automatically counts the variables via the Fuzzy Tahani and\nsimultaneously access the vast database.",
    "published_date": "2021-09-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.02645v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.02431v3",
    "title": "On Length Divergence Bias in Textual Matching Models",
    "authors": [
      "Lan Jiang",
      "Tianshu Lyu",
      "Yankai Lin",
      "Meng Chong",
      "Xiaoyong Lyu",
      "Dawei Yin"
    ],
    "author_ids": [],
    "abstract": "Despite the remarkable success deep models have achieved in Textual Matching\n(TM) tasks, it still remains unclear whether they truly understand language or\nmeasure the semantic similarity of texts by exploiting statistical bias in\ndatasets. In this work, we provide a new perspective to study this issue -- via\nthe length divergence bias. We find the length divergence heuristic widely\nexists in prevalent TM datasets, providing direct cues for prediction. To\ndetermine whether TM models have adopted such heuristic, we introduce an\nadversarial evaluation scheme which invalidates the heuristic. In this\nadversarial setting, all TM models perform worse, indicating they have indeed\nadopted this heuristic. Through a well-designed probing experiment, we\nempirically validate that the bias of TM models can be attributed in part to\nextracting the text length information during training. To alleviate the length\ndivergence bias, we propose an adversarial training method. The results\ndemonstrate we successfully improve the robustness and generalization ability\nof models at the same time.",
    "published_date": "2021-09-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.02431v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.02403v2",
    "title": "Eliminating Sentiment Bias for Aspect-Level Sentiment Classification with Unsupervised Opinion Extraction",
    "authors": [
      "Bo Wang",
      "Tao Shen",
      "Guodong Long",
      "Tianyi Zhou",
      "Yi Chang"
    ],
    "author_ids": [],
    "abstract": "Aspect-level sentiment classification (ALSC) aims at identifying the\nsentiment polarity of a specified aspect in a sentence. ALSC is a practical\nsetting in aspect-based sentiment analysis due to no opinion term labeling\nneeded, but it fails to interpret why a sentiment polarity is derived for the\naspect. To address this problem, recent works fine-tune pre-trained Transformer\nencoders for ALSC to extract an aspect-centric dependency tree that can locate\nthe opinion words. However, the induced opinion words only provide an intuitive\ncue far below human-level interpretability. Besides, the pre-trained encoder\ntends to internalize an aspect's intrinsic sentiment, causing sentiment bias\nand thus affecting model performance. In this paper, we propose a span-based\nanti-bias aspect representation learning framework. It first eliminates the\nsentiment bias in the aspect embedding by adversarial learning against aspects'\nprior sentiment. Then, it aligns the distilled opinion candidates with the\naspect by span-based dependency modeling to highlight the interpretable opinion\nterms. Our method achieves new state-of-the-art performance on five benchmarks,\nwith the capability of unsupervised opinion extraction.",
    "published_date": "2021-09-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.02403v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.02357v2",
    "title": "Fighting Selection Bias in Statistical Learning: Application to Visual Recognition from Biased Image Databases",
    "authors": [
      "Stephan Clémençon",
      "Pierre Laforgue",
      "Robin Vogel"
    ],
    "author_ids": [],
    "abstract": "In practice, and especially when training deep neural networks, visual\nrecognition rules are often learned based on various sources of information. On\nthe other hand, the recent deployment of facial recognition systems with uneven\nperformances on different population segments has highlighted the\nrepresentativeness issues induced by a naive aggregation of the datasets. In\nthis paper, we show how biasing models can remedy these problems. Based on the\n(approximate) knowledge of the biasing mechanisms at work, our approach\nconsists in reweighting the observations, so as to form a nearly debiased\nestimator of the target distribution. One key condition is that the supports of\nthe biased distributions must partly overlap, and cover the support of the\ntarget distribution. In order to meet this requirement in practice, we propose\nto use a low dimensional image representation, shared across the image\ndatabases. Finally, we provide numerical experiments highlighting the relevance\nof our approach.",
    "published_date": "2021-09-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.CY",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.02357v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.02355v1",
    "title": "A Farewell to the Bias-Variance Tradeoff? An Overview of the Theory of Overparameterized Machine Learning",
    "authors": [
      "Yehuda Dar",
      "Vidya Muthukumar",
      "Richard G. Baraniuk"
    ],
    "author_ids": [],
    "abstract": "The rapid recent progress in machine learning (ML) has raised a number of\nscientific questions that challenge the longstanding dogma of the field. One of\nthe most important riddles is the good empirical generalization of\noverparameterized models. Overparameterized models are excessively complex with\nrespect to the size of the training dataset, which results in them perfectly\nfitting (i.e., interpolating) the training data, which is usually noisy. Such\ninterpolation of noisy data is traditionally associated with detrimental\noverfitting, and yet a wide range of interpolating models -- from simple linear\nmodels to deep neural networks -- have recently been observed to generalize\nextremely well on fresh test data. Indeed, the recently discovered double\ndescent phenomenon has revealed that highly overparameterized models often\nimprove over the best underparameterized model in test performance.\n  Understanding learning in this overparameterized regime requires new theory\nand foundational empirical studies, even for the simplest case of the linear\nmodel. The underpinnings of this understanding have been laid in very recent\nanalyses of overparameterized linear regression and related statistical\nlearning tasks, which resulted in precise analytic characterizations of double\ndescent. This paper provides a succinct overview of this emerging theory of\noverparameterized ML (henceforth abbreviated as TOPML) that explains these\nrecent findings through a statistical signal processing perspective. We\nemphasize the unique aspects that define the TOPML research area as a subfield\nof modern ML theory and outline interesting open questions that remain.",
    "published_date": "2021-09-06T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.02355v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.02351v3",
    "title": "F3: Fair and Federated Face Attribute Classification with Heterogeneous Data",
    "authors": [
      "Samhita Kanaparthy",
      "Manisha Padala",
      "Sankarshan Damle",
      "Ravi Kiran Sarvadevabhatla",
      "Sujit Gujar"
    ],
    "author_ids": [],
    "abstract": "Fairness across different demographic groups is an essential criterion for\nface-related tasks, Face Attribute Classification (FAC) being a prominent\nexample. Apart from this trend, Federated Learning (FL) is increasingly gaining\ntraction as a scalable paradigm for distributed training. Existing FL\napproaches require data homogeneity to ensure fairness. However, this\nassumption is too restrictive in real-world settings. We propose F3, a novel FL\nframework for fair FAC under data heterogeneity. F3 adopts multiple heuristics\nto improve fairness across different demographic groups without requiring data\nhomogeneity assumption. We demonstrate the efficacy of F3 by reporting\nempirically observed fairness measures and accuracy guarantees on popular face\ndatasets. Our results suggest that F3 strikes a practical balance between\naccuracy and fairness for FAC.",
    "published_date": "2021-09-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CV",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.02351v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.02202v1",
    "title": "Fairness via AI: Bias Reduction in Medical Information",
    "authors": [
      "Shiri Dori-Hacohen",
      "Roberto Montenegro",
      "Fabricio Murai",
      "Scott A. Hale",
      "Keen Sung",
      "Michela Blain",
      "Jennifer Edwards-Johnson"
    ],
    "author_ids": [],
    "abstract": "Most Fairness in AI research focuses on exposing biases in AI systems. A\nbroader lens on fairness reveals that AI can serve a greater aspiration:\nrooting out societal inequities from their source. Specifically, we focus on\ninequities in health information, and aim to reduce bias in that domain using\nAI. The AI algorithms under the hood of search engines and social media, many\nof which are based on recommender systems, have an outsized impact on the\nquality of medical and health information online. Therefore, embedding bias\ndetection and reduction into these recommender systems serving up medical and\nhealth content online could have an outsized positive impact on patient\noutcomes and wellbeing.\n  In this position paper, we offer the following contributions: (1) we propose\na novel framework of Fairness via AI, inspired by insights from medical\neducation, sociology and antiracism; (2) we define a new term, bisinformation,\nwhich is related to, but distinct from, misinformation, and encourage\nresearchers to study it; (3) we propose using AI to study, detect and mitigate\nbiased, harmful, and/or false health information that disproportionately hurts\nminority groups in society; and (4) we suggest several pillars and pose several\nopen problems in order to seed inquiry in this new space. While part (3) of\nthis work specifically focuses on the health domain, the fundamental computer\nscience advances and contributions stemming from research efforts in bias\nreduction and Fairness via AI have broad implications in all areas of society.",
    "published_date": "2021-09-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.IR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.02202v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.03150v3",
    "title": "Recommendation Fairness: From Static to Dynamic",
    "authors": [
      "Dell Zhang",
      "Jun Wang"
    ],
    "author_ids": [],
    "abstract": "Driven by the need to capture users' evolving interests and optimize their\nlong-term experiences, more and more recommender systems have started to model\nrecommendation as a Markov decision process and employ reinforcement learning\nto address the problem. Shouldn't research on the fairness of recommender\nsystems follow the same trend from static evaluation and one-shot intervention\nto dynamic monitoring and non-stop control? In this paper, we portray the\nrecent developments in recommender systems first and then discuss how fairness\ncould be baked into the reinforcement learning techniques for recommendation.\nMoreover, we argue that in order to make further progress in recommendation\nfairness, we may want to consider multi-agent (game-theoretic) optimization,\nmulti-objective (Pareto) optimization, and simulation-based optimization, in\nthe general framework of stochastic games.",
    "published_date": "2021-09-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.03150v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.02058v1",
    "title": "Detecting Communities from Heterogeneous Graphs: A Context Path-based Graph Neural Network Model",
    "authors": [
      "Linhao Luo",
      "Yixiang Fang",
      "Xin Cao",
      "Xiaofeng Zhang",
      "Wenjie Zhang"
    ],
    "author_ids": [],
    "abstract": "Community detection, aiming to group the graph nodes into clusters with dense\ninner-connection, is a fundamental graph mining task. Recently, it has been\nstudied on the heterogeneous graph, which contains multiple types of nodes and\nedges, posing great challenges for modeling the high-order relationship between\nnodes. With the surge of graph embedding mechanism, it has also been adopted to\ncommunity detection. A remarkable group of works use the meta-path to capture\nthe high-order relationship between nodes and embed them into nodes' embedding\nto facilitate community detection. However, defining meaningful meta-paths\nrequires much domain knowledge, which largely limits their applications,\nespecially on schema-rich heterogeneous graphs like knowledge graphs. To\nalleviate this issue, in this paper, we propose to exploit the context path to\ncapture the high-order relationship between nodes, and build a Context\nPath-based Graph Neural Network (CP-GNN) model. It recursively embeds the\nhigh-order relationship between nodes into the node embedding with attention\nmechanisms to discriminate the importance of different relationships. By\nmaximizing the expectation of the co-occurrence of nodes connected by context\npaths, the model can learn the nodes' embeddings that both well preserve the\nhigh-order relationship between nodes and are helpful for community detection.\nExtensive experimental results on four real-world datasets show that CP-GNN\noutperforms the state-of-the-art community detection methods.",
    "published_date": "2021-09-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.02058v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.01992v1",
    "title": "Balanced House Allocation",
    "authors": [
      "Xinghua Long",
      "Rodrigo A. Velez"
    ],
    "author_ids": [],
    "abstract": "We introduce balancedness a fairness axiom in house allocation problems. It\nrequires a mechanism to assign the top choice, the second top choice, and so\non, on the same number of profiles for each agent. This axiom guarantees equal\ntreatment of all agents at the stage in which the mechanism is announced when\nall preference profiles are equally likely. We show that, with an interesting\nexception for the three-agent case, Top Trading Cycles from individual\nendowments is the only mechanism that is balanced, efficient, and group\nstrategy-proof.",
    "published_date": "2021-09-05T00:00:00",
    "year": 2021,
    "categories": [
      "econ.TH",
      "cs.GT",
      "91A99"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.01992v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2109.01902v6",
    "title": "Barycentric-alignment and reconstruction loss minimization for domain generalization",
    "authors": [
      "Boyang Lyu",
      "Thuan Nguyen",
      "Prakash Ishwar",
      "Matthias Scheutz",
      "Shuchin Aeron"
    ],
    "author_ids": [],
    "abstract": "This paper advances the theory and practice of Domain Generalization (DG) in\nmachine learning. We consider the typical DG setting where the hypothesis is\ncomposed of a representation mapping followed by a labeling function. Within\nthis setting, the majority of popular DG methods aim to jointly learn the\nrepresentation and the labeling functions by minimizing a well-known upper\nbound for the classification risk in the unseen domain. In practice, however,\nmethods based on this theoretical upper bound ignore a term that cannot be\ndirectly optimized due to its dual dependence on both the representation\nmapping and the unknown optimal labeling function in the unseen domain. To\nbridge this gap between theory and practice, we introduce a new upper bound\nthat is free of terms having such dual dependence, resulting in a fully\noptimizable risk upper bound for the unseen domain. Our derivation leverages\nclassical and recent transport inequalities that link optimal transport metrics\nwith information-theoretic measures. Compared to previous bounds, our bound\nintroduces two new terms: (i) the Wasserstein-2 barycenter term that aligns\ndistributions between domains, and (ii) the reconstruction loss term that\nassesses the quality of representation in reconstructing the original data.\nBased on this new upper bound, we propose a novel DG algorithm named\nWasserstein Barycenter Auto-Encoder (WBAE) that simultaneously minimizes the\nclassification loss, the barycenter loss, and the reconstruction loss.\nNumerical results demonstrate that the proposed method outperforms current\nstate-of-the-art DG algorithms on several datasets.",
    "published_date": "2021-09-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.01902v6",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.01838v3",
    "title": "RAMA: A Rapid Multicut Algorithm on GPU",
    "authors": [
      "Ahmed Abbas",
      "Paul Swoboda"
    ],
    "author_ids": [],
    "abstract": "We propose a highly parallel primal-dual algorithm for the multicut (a.k.a.\ncorrelation clustering) problem, a classical graph clustering problem widely\nused in machine learning and computer vision. Our algorithm consists of three\nsteps executed recursively: (1) Finding conflicted cycles that correspond to\nviolated inequalities of the underlying multicut relaxation, (2) Performing\nmessage passing between the edges and cycles to optimize the Lagrange\nrelaxation coming from the found violated cycles producing reduced costs and\n(3) Contracting edges with high reduced costs through matrix-matrix\nmultiplications. Our algorithm produces primal solutions and lower bounds that\nestimate the distance to optimum. We implement our algorithm on GPUs and show\nresulting one to two orders-of-magnitudes improvements in execution speed\nwithout sacrificing solution quality compared to traditional sequential\nalgorithms that run on CPUs. We can solve very large scale benchmark problems\nwith up to $\\mathcal{O}(10^8)$ variables in a few seconds with small\nprimal-dual gaps. Our code is available at\nhttps://github.com/pawelswoboda/RAMA.",
    "published_date": "2021-09-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DC",
      "cs.CV",
      "cs.DS",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.01838v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.01806v1",
    "title": "On Faster Convergence of Scaled Sign Gradient Descent",
    "authors": [
      "Xiuxian Li",
      "Kuo-Yi Lin",
      "Li Li",
      "Yiguang Hong",
      "Jie Chen"
    ],
    "author_ids": [],
    "abstract": "Communication has been seen as a significant bottleneck in industrial\napplications over large-scale networks. To alleviate the communication burden,\nsign-based optimization algorithms have gained popularity recently in both\nindustrial and academic communities, which is shown to be closely related to\nadaptive gradient methods, such as Adam. Along this line, this paper\ninvestigates faster convergence for a variant of sign-based gradient descent,\ncalled scaled signGD, in three cases: 1) the objective function is strongly\nconvex; 2) the objective function is nonconvex but satisfies the\nPolyak-Lojasiewicz (PL) inequality; 3) the gradient is stochastic, called\nscaled signGD in this case. For the first two cases, it can be shown that the\nscaled signGD converges at a linear rate. For case 3), the algorithm is shown\nto converge linearly to a neighborhood of the optimal value when a constant\nlearning rate is employed, and the algorithm converges at a rate of $O(1/k)$\nwhen using a diminishing learning rate, where $k$ is the iteration number. The\nresults are also extended to the distributed setting by majority vote in a\nparameter-server framework. Finally, numerical experiments on logistic\nregression are performed to corroborate the theoretical findings.",
    "published_date": "2021-09-04T00:00:00",
    "year": 2021,
    "categories": [
      "math.OC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.01806v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.06062v2",
    "title": "Semantics-Guided Contrastive Network for Zero-Shot Object detection",
    "authors": [
      "Caixia Yan",
      "Xiaojun Chang",
      "Minnan Luo",
      "Huan Liu",
      "Xiaoqin Zhang",
      "Qinghua Zheng"
    ],
    "author_ids": [],
    "abstract": "Zero-shot object detection (ZSD), the task that extends conventional\ndetection models to detecting objects from unseen categories, has emerged as a\nnew challenge in computer vision. Most existing approaches tackle the ZSD task\nwith a strict mapping-transfer strategy, which may lead to suboptimal ZSD\nresults: 1) the learning process of those models ignores the available unseen\nclass information, and thus can be easily biased towards the seen categories;\n2) the original visual feature space is not well-structured and lack of\ndiscriminative information. To address these issues, we develop a novel\nSemantics-Guided Contrastive Network for ZSD, named ContrastZSD, a detection\nframework that first brings contrastive learning mechanism into the realm of\nzero-shot detection. Particularly, ContrastZSD incorporates two\nsemantics-guided contrastive learning subnets that contrast between\nregion-category and region-region pairs respectively. The pairwise contrastive\ntasks take advantage of additional supervision signals derived from both ground\ntruth label and pre-defined class similarity distribution. Under the guidance\nof those explicit semantic supervision, the model can learn more knowledge\nabout unseen categories to avoid the bias problem to seen concepts, while\noptimizing the data structure of visual features to be more discriminative for\nbetter visual-semantic alignment. Extensive experiments are conducted on two\npopular benchmarks for ZSD, i.e., PASCAL VOC and MS COCO. Results show that our\nmethod outperforms the previous state-of-the-art on both ZSD and generalized\nZSD tasks.",
    "published_date": "2021-09-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.06062v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.01295v1",
    "title": "Information Symmetry Matters: A Modal-Alternating Propagation Network for Few-Shot Learning",
    "authors": [
      "Zhong Ji",
      "Zhishen Hou",
      "Xiyao Liu",
      "Yanwei Pang",
      "Jungong Han"
    ],
    "author_ids": [],
    "abstract": "Semantic information provides intra-class consistency and inter-class\ndiscriminability beyond visual concepts, which has been employed in Few-Shot\nLearning (FSL) to achieve further gains. However, semantic information is only\navailable for labeled samples but absent for unlabeled samples, in which the\nembeddings are rectified unilaterally by guiding the few labeled samples with\nsemantics. Therefore, it is inevitable to bring a cross-modal bias between\nsemantic-guided samples and nonsemantic-guided samples, which results in an\ninformation asymmetry problem. To address this problem, we propose a\nModal-Alternating Propagation Network (MAP-Net) to supplement the absent\nsemantic information of unlabeled samples, which builds information symmetry\namong all samples in both visual and semantic modalities. Specifically, the\nMAP-Net transfers the neighbor information by the graph propagation to generate\nthe pseudo-semantics for unlabeled samples guided by the completed visual\nrelationships and rectify the feature embeddings. In addition, due to the large\ndiscrepancy between visual and semantic modalities, we design a Relation\nGuidance (RG) strategy to guide the visual relation vectors via semantics so\nthat the propagated information is more beneficial. Extensive experimental\nresults on three semantic-labeled datasets, i.e., Caltech-UCSD-Birds 200-2011,\nSUN Attribute Database, and Oxford 102 Flower, have demonstrated that our\nproposed method achieves promising performance and outperforms the\nstate-of-the-art approaches, which indicates the necessity of information\nsymmetry.",
    "published_date": "2021-09-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.01295v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.01190v2",
    "title": "Assisting Decision Making in Scholarly Peer Review: A Preference Learning Perspective",
    "authors": [
      "Nils Dycke",
      "Edwin Simpson",
      "Ilia Kuznetsov",
      "Iryna Gurevych"
    ],
    "author_ids": [],
    "abstract": "Peer review is the primary means of quality control in academia; as an\noutcome of a peer review process, program and area chairs make acceptance\ndecisions for each paper based on the review reports and scores they received.\nQuality of scientific work is multi-faceted; coupled with the subjectivity of\nreviewing, this makes final decision making difficult and time-consuming. To\nsupport this final step of peer review, we formalize it as a paper ranking\nproblem. We introduce a novel, multi-faceted generic evaluation framework for\nranking submissions based on peer reviews that takes into account\neffectiveness, efficiency and fairness. We propose a preference learning\nperspective on the task that considers both review texts and scores to\nalleviate the inevitable bias and noise in reviews. Our experiments on peer\nreview data from the ACL 2018 conference demonstrate the superiority of our\npreference-learning-based approach over baselines and prior work, while\nhighlighting the importance of using both review texts and scores to rank\nsubmissions.",
    "published_date": "2021-09-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.01190v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.01182v1",
    "title": "COVID-19 Vaccine Hesitancy and Information Diffusion: An Agent-based Modeling Approach",
    "authors": [
      "Pooria Taghizadeh Naderi",
      "Ali Asgary",
      "Jude Kong",
      "Jianhong Wu",
      "Fattaneh Taghiyareh"
    ],
    "author_ids": [],
    "abstract": "Despite the unprecedented success in the rapid development of several\neffective vaccines against the Cov-SARS-2, global vaccination rollout efforts\nsuffer from vaccine distribution inequality and vaccine acceptance, leading to\ninsufficient public immunity provided by the vaccine products. While a major\ncurrent focus in vaccine acceptance research is how to model and inform vaccine\nacceptance based on social-demographic parameters, characteristics of vaccine\nacceptance are not well understood and in particular, it is not known whether\nand how information diffusion influences vaccine acceptance. This study\nexamines how information diffusion can change vaccine acceptance by developing\na comprehensive computational model with an agent-based simulation technique to\novercome the modeling and quantification complexity associated with\nsocio-demographics, vaccine types, population statistics, and information\ndiffusion. Our analyses, calibrated by the vaccine acceptance survey data from\nthe provinces and territories of Canada, provide clear evidence that the\npropagation of information can greatly influence vaccine acceptance rates. The\nresults illustrate that spread of negative messages about the COVID-19 vaccines\ncan cause significant vaccine hesitancy that challenges the goal of a high\npublic immunity provided by the vaccines. Our findings might help solve the\nvaccine hesitancy problem by focusing more on individuals' opinions and\nbehavior.",
    "published_date": "2021-09-02T00:00:00",
    "year": 2021,
    "categories": [
      "physics.soc-ph",
      "cs.MA",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.01182v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2109.00874v1",
    "title": "Universal and Tight Online Algorithms for Generalized-Mean Welfare",
    "authors": [
      "Siddharth Barman",
      "Arindam Khan",
      "Arnab Maiti"
    ],
    "author_ids": [],
    "abstract": "We study fair and efficient allocation of divisible goods, in an online\nmanner, among $n$ agents. The goods arrive online in a sequence of $T$ time\nperiods. The agents' values for a good are revealed only after its arrival, and\nthe online algorithm needs to fractionally allocate the good, immediately and\nirrevocably, among the agents. Towards a unifying treatment of fairness and\neconomic efficiency objectives, we develop an algorithmic framework for finding\nonline allocations to maximize the generalized mean of the values received by\nthe agents. In particular, working with the assumption that each agent's value\nfor the grand bundle of goods is appropriately scaled, we address online\nmaximization of $p$-mean welfare. Parameterized by an exponent term $p \\in\n(-\\infty, 1]$, these means encapsulate a range of welfare functions, including\nsocial welfare ($p=1$), egalitarian welfare ($p \\to -\\infty$), and Nash social\nwelfare ($p \\to 0$).\n  We present a simple algorithmic template that takes a threshold as input and,\nwith judicious choices for this threshold, leads to both universal and tailored\ncompetitive guarantees. First, we show that one can compute online a single\nallocation that $O (\\sqrt{n} \\log n)$-approximates the optimal $p$-mean welfare\nfor all $p\\le 1$. The existence of such a universal allocation is interesting\nin and of itself. Moreover, this universal guarantee achieves essentially tight\ncompetitive ratios for specific values of $p$.\n  Next, we obtain improved competitive ratios for different ranges of $p$ by\nexecuting our algorithm with $p$-specific thresholds, e.g., we provide $O(\\log\n^3 n)$-competitive ratio for all $p\\in (\\frac{-1}{\\log 2n},1)$.\n  We complement our positive results by establishing lower bounds to show that\nour guarantees are essentially tight for a wide range of the exponent\nparameter.",
    "published_date": "2021-09-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.00874v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2109.00725v2",
    "title": "Causal Inference in Natural Language Processing: Estimation, Prediction, Interpretation and Beyond",
    "authors": [
      "Amir Feder",
      "Katherine A. Keith",
      "Emaad Manzoor",
      "Reid Pryzant",
      "Dhanya Sridhar",
      "Zach Wood-Doughty",
      "Jacob Eisenstein",
      "Justin Grimmer",
      "Roi Reichart",
      "Margaret E. Roberts",
      "Brandon M. Stewart",
      "Victor Veitch",
      "Diyi Yang"
    ],
    "author_ids": [],
    "abstract": "A fundamental goal of scientific research is to learn about causal\nrelationships. However, despite its critical role in the life and social\nsciences, causality has not had the same importance in Natural Language\nProcessing (NLP), which has traditionally placed more emphasis on predictive\ntasks. This distinction is beginning to fade, with an emerging area of\ninterdisciplinary research at the convergence of causal inference and language\nprocessing. Still, research on causality in NLP remains scattered across\ndomains without unified definitions, benchmark datasets and clear articulations\nof the challenges and opportunities in the application of causal inference to\nthe textual domain, with its unique properties. In this survey, we consolidate\nresearch across academic areas and situate it in the broader NLP landscape. We\nintroduce the statistical challenge of estimating causal effects with text,\nencompassing settings where text is used as an outcome, treatment, or to\naddress confounding. In addition, we explore potential uses of causal inference\nto improve the robustness, fairness, and interpretability of NLP models. We\nthus provide a unified overview of causal inference for the NLP community.",
    "published_date": "2021-09-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.00725v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.00708v3",
    "title": "Efficient Algorithms For Fair Clustering with a New Fairness Notion",
    "authors": [
      "Shivam Gupta",
      "Ganesh Ghalme",
      "Narayanan C. Krishnan",
      "Shweta Jain"
    ],
    "author_ids": [],
    "abstract": "We revisit the problem of fair clustering, first introduced by Chierichetti\net al., that requires each protected attribute to have approximately equal\nrepresentation in every cluster; i.e., a balance property. Existing solutions\nto fair clustering are either not scalable or do not achieve an optimal\ntrade-off between clustering objective and fairness. In this paper, we propose\na new notion of fairness, which we call $tau$-fair fairness, that strictly\ngeneralizes the balance property and enables a fine-grained efficiency vs.\nfairness trade-off. Furthermore, we show that simple greedy round-robin based\nalgorithms achieve this trade-off efficiently. Under a more general setting of\nmulti-valued protected attributes, we rigorously analyze the theoretical\nproperties of the our algorithms. Our experimental results suggest that the\nproposed solution outperforms all the state-of-the-art algorithms and works\nexceptionally well even for a large number of clusters.",
    "published_date": "2021-09-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.00708v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.00644v3",
    "title": "RIFLE: Imputation and Robust Inference from Low Order Marginals",
    "authors": [
      "Sina Baharlouei",
      "Kelechi Ogudu",
      "Sze-chuan Suen",
      "Meisam Razaviyayn"
    ],
    "author_ids": [],
    "abstract": "The ubiquity of missing values in real-world datasets poses a challenge for\nstatistical inference and can prevent similar datasets from being analyzed in\nthe same study, precluding many existing datasets from being used for new\nanalyses. While an extensive collection of packages and algorithms have been\ndeveloped for data imputation, the overwhelming majority perform poorly if\nthere are many missing values and low sample sizes, which are unfortunately\ncommon characteristics in empirical data. Such low-accuracy estimations\nadversely affect the performance of downstream statistical models. We develop a\nstatistical inference framework for regression and classification in the\npresence of missing data without imputation. Our framework, RIFLE (Robust\nInFerence via Low-order moment Estimations), estimates low-order moments of the\nunderlying data distribution with corresponding confidence intervals to learn a\ndistributionally robust model. We specialize our framework to linear regression\nand normal discriminant analysis, and we provide convergence and performance\nguarantees. This framework can also be adapted to impute missing data. In\nnumerical experiments, we compare RIFLE to several state-of-the-art approaches\n(including MICE, Amelia, MissForest, KNN-imputer, MIDA, and Mean Imputer) for\nimputation and inference in the presence of missing values. Our experiments\ndemonstrate that RIFLE outperforms other benchmark algorithms when the\npercentage of missing values is high and/or when the number of data points is\nrelatively small. RIFLE is publicly available at\nhttps://github.com/optimization-for-data-driven-science/RIFLE.",
    "published_date": "2021-09-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.MS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.00644v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.00582v3",
    "title": "Information-theoretic Classification Accuracy: A Criterion that Guides Data-driven Combination of Ambiguous Outcome Labels in Multi-class Classification",
    "authors": [
      "Chihao Zhang",
      "Yiling Elaine Chen",
      "Shihua Zhang",
      "Jingyi Jessica Li"
    ],
    "author_ids": [],
    "abstract": "Outcome labeling ambiguity and subjectivity are ubiquitous in real-world\ndatasets. While practitioners commonly combine ambiguous outcome labels for all\ndata points (instances) in an ad hoc way to improve the accuracy of multi-class\nclassification, there lacks a principled approach to guide the label\ncombination for all data points by any optimality criterion. To address this\nproblem, we propose the information-theoretic classification accuracy (ITCA), a\ncriterion that balances the trade-off between prediction accuracy (how well do\npredicted labels agree with actual labels) and classification resolution (how\nmany labels are predictable), to guide practitioners on how to combine\nambiguous outcome labels. To find the optimal label combination indicated by\nITCA, we propose two search strategies: greedy search and breadth-first search.\nNotably, ITCA and the two search strategies are adaptive to all\nmachine-learning classification algorithms. Coupled with a classification\nalgorithm and a search strategy, ITCA has two uses: improving prediction\naccuracy and identifying ambiguous labels. We first verify that ITCA achieves\nhigh accuracy with both search strategies in finding the correct label\ncombinations on synthetic and real data. Then we demonstrate the effectiveness\nof ITCA in diverse applications including medical prognosis, cancer survival\nprediction, user demographics prediction, and cell type classification. We also\nprovide theoretical insights into ITCA by studying the oracle and the linear\ndiscriminant analysis classification algorithms. Python package itca (available\nat https://github.com/JSB-UCLA/ITCA) implements ITCA and search strategies.",
    "published_date": "2021-09-01T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.ME",
      "62-08"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.00582v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.01164v1",
    "title": "Scalable Data Annotation Pipeline for High-Quality Large Speech Datasets Development",
    "authors": [
      "Mingkuan Liu",
      "Chi Zhang",
      "Hua Xing",
      "Chao Feng",
      "Monchu Chen",
      "Judith Bishop",
      "Grace Ngapo"
    ],
    "author_ids": [],
    "abstract": "This paper introduces a human-in-the-loop (HITL) data annotation pipeline to\ngenerate high-quality, large-scale speech datasets. The pipeline combines human\nand machine advantages to more quickly, accurately, and cost-effectively\nannotate datasets with machine pre-labeling and fully manual auditing. Quality\ncontrol mechanisms such as blind testing, behavior monitoring, and data\nvalidation have been adopted in the annotation pipeline to mitigate potential\nbias introduced by machine-generated labels. Our A/B testing and pilot results\ndemonstrated the HITL pipeline can improve annotation speed and capacity by at\nleast 80% and quality is comparable to or higher than manual double pass\nannotation. We are leveraging this scalable pipeline to create and continuously\ngrow ultra-high volume off-the-shelf (UHV-OTS) speech corpora for multiple\nlanguages, with the capability to expand to 10,000+ hours per language\nannually. Customized datasets can be produced from the UHV-OTS corpora using\ndynamic packaging. UHV-OTS is a long-term Appen project to support commercial\nand academic research data needs in speech processing. Appen will donate a\nnumber of free speech datasets from the UHV-OTS each year to support academic\nand open source community research under the CC-BY-SA license. We are also\nreleasing the code of the data pre-processing and pre-tagging pipeline under\nthe Apache 2.0 license to allow reproduction of the results reported in the\npaper.",
    "published_date": "2021-09-01T00:00:00",
    "year": 2021,
    "categories": [
      "eess.AS",
      "cs.LG",
      "cs.SD"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.01164v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.00545v2",
    "title": "Fair Representation: Guaranteeing Approximate Multiple Group Fairness for Unknown Tasks",
    "authors": [
      "Xudong Shen",
      "Yongkang Wong",
      "Mohan Kankanhalli"
    ],
    "author_ids": [],
    "abstract": "Motivated by scenarios where data is used for diverse prediction tasks, we\nstudy whether fair representation can be used to guarantee fairness for unknown\ntasks and for multiple fairness notions simultaneously. We consider seven group\nfairness notions that cover the concepts of independence, separation, and\ncalibration. Against the backdrop of the fairness impossibility results, we\nexplore approximate fairness. We prove that, although fair representation might\nnot guarantee fairness for all prediction tasks, it does guarantee fairness for\nan important subset of tasks -- the tasks for which the representation is\ndiscriminative. Specifically, all seven group fairness notions are linearly\ncontrolled by fairness and discriminativeness of the representation. When an\nincompatibility exists between different fairness notions, fair and\ndiscriminative representation hits the sweet spot that approximately satisfies\nall notions. Motivated by our theoretical findings, we propose to learn both\nfair and discriminative representations using pretext loss which\nself-supervises learning, and Maximum Mean Discrepancy as a fair regularizer.\nExperiments on tabular, image, and face datasets show that using the learned\nrepresentation, downstream predictions that we are unaware of when learning the\nrepresentation indeed become fairer for seven group fairness notions, and the\nfairness guarantees computed from our theoretical results are all valid.",
    "published_date": "2021-09-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CV",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.00545v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.00484v2",
    "title": "Impossibility Results in AI: A Survey",
    "authors": [
      "Mario Brcic",
      "Roman V. Yampolskiy"
    ],
    "author_ids": [],
    "abstract": "An impossibility theorem demonstrates that a particular problem or set of\nproblems cannot be solved as described in the claim. Such theorems put limits\non what is possible to do concerning artificial intelligence, especially the\nsuper-intelligent one. As such, these results serve as guidelines, reminders,\nand warnings to AI safety, AI policy, and governance researchers. These might\nenable solutions to some long-standing questions in the form of formalizing\ntheories in the framework of constraint satisfaction without committing to one\noption. We strongly believe this to be the most prudent approach to long-term\nAI safety initiatives. In this paper, we have categorized impossibility\ntheorems applicable to AI into five mechanism-based categories: deduction,\nindistinguishability, induction, tradeoffs, and intractability. We found that\ncertain theorems are too specific or have implicit assumptions that limit\napplication. Also, we added new results (theorems) such as the unfairness of\nexplainability, the first explainability-related result in the induction\ncategory. The remaining results deal with misalignment between the clones and\nput a limit to the self-awareness of agents. We concluded that deductive\nimpossibilities deny 100%-guarantees for security. In the end, we give some\nideas that hold potential in explainability, controllability, value alignment,\nethics, and group decision-making. They can be deepened by further\ninvestigation.",
    "published_date": "2021-09-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.CY",
      "K.4; I.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.00484v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.00482v2",
    "title": "Looking at the whole picture: constrained unsupervised anomaly segmentation",
    "authors": [
      "Julio Silva-Rodríguez",
      "Valery Naranjo",
      "Jose Dolz"
    ],
    "author_ids": [],
    "abstract": "Current unsupervised anomaly localization approaches rely on generative\nmodels to learn the distribution of normal images, which is later used to\nidentify potential anomalous regions derived from errors on the reconstructed\nimages. However, a main limitation of nearly all prior literature is the need\nof employing anomalous images to set a class-specific threshold to locate the\nanomalies. This limits their usability in realistic scenarios, where only\nnormal data is typically accessible. Despite this major drawback, only a\nhandful of works have addressed this limitation, by integrating supervision on\nattention maps during training. In this work, we propose a novel formulation\nthat does not require accessing images with abnormalities to define the\nthreshold. Furthermore, and in contrast to very recent work, the proposed\nconstraint is formulated in a more principled manner, leveraging well-known\nknowledge in constrained optimization. In particular, the equality constraint\non the attention maps in prior work is replaced by an inequality constraint,\nwhich allows more flexibility. In addition, to address the limitations of\npenalty-based functions we employ an extension of the popular log-barrier\nmethods to handle the constraint. Comprehensive experiments on the popular\nBRATS'19 dataset demonstrate that the proposed approach substantially\noutperforms relevant literature, establishing new state-of-the-art results for\nunsupervised lesion segmentation.",
    "published_date": "2021-09-01T00:00:00",
    "year": 2021,
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.00482v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.00435v3",
    "title": "Proceedings of KDD 2020 Workshop on Data-driven Humanitarian Mapping: Harnessing Human-Machine Intelligence for High-Stake Public Policy and Resilience Planning",
    "authors": [
      "Snehalkumar",
      "S. Gaikwad",
      "Shankar Iyer",
      "Dalton Lunga",
      "Yu-Ru Lin"
    ],
    "author_ids": [],
    "abstract": "Humanitarian challenges, including natural disasters, food insecurity,\nclimate change, racial and gender violence, environmental crises, the COVID-19\ncoronavirus pandemic, human rights violations, and forced displacements,\ndisproportionately impact vulnerable communities worldwide. According to UN\nOCHA, 235 million people will require humanitarian assistance in 2021 . Despite\nthese growing perils, there remains a notable paucity of data science research\nto scientifically inform equitable public policy decisions for improving the\nlivelihood of at-risk populations. Scattered data science efforts exist to\naddress these challenges, but they remain isolated from practice and prone to\nalgorithmic harms concerning lack of privacy, fairness, interpretability,\naccountability, transparency, and ethics. Biases in data-driven methods carry\nthe risk of amplifying inequalities in high-stakes policy decisions that impact\nthe livelihood of millions of people. Consequently, proclaimed benefits of\ndata-driven innovations remain inaccessible to policymakers, practitioners, and\nmarginalized communities at the core of humanitarian actions and global\ndevelopment. To help fill this gap, we propose the Data-driven Humanitarian\nMapping Research Program, which focuses on developing novel data science\nmethodologies that harness human-machine intelligence for high-stakes public\npolicy and resilience planning.\n  The proceedings of the 1st Data-driven Humanitarian Mapping workshop at the\n26th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, August 24th,\n2020.",
    "published_date": "2021-09-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "econ.GN",
      "q-fin.EC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.00435v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.00377v1",
    "title": "New Proofs of Extremal Inequalities With Applications",
    "authors": [
      "Yinfei Xu",
      "Guojun Chen"
    ],
    "author_ids": [],
    "abstract": "The extremal inequality approach plays a key role in network information\ntheory problems. In this paper, we propose a novel monotone path construction\nin product probability space. The optimality of Gaussian distribution is then\nestablished by standard perturbation arguments. The proofs of Liu-Viswanath\nextremal and vector Generalization of Costa's entropy power inequality are\nillustrated into the unified framework. As applications, capacity region of the\nmultiple-input multiple-output (MIMO) Gaussian broadcast channel and\nrate-distortion-equivocation function of the vector Gaussian secure source\ncoding are revisited through our proposed extremal inequality approach.",
    "published_date": "2021-09-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.00377v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2109.00328v1",
    "title": "Memory-Free Generative Replay For Class-Incremental Learning",
    "authors": [
      "Xiaomeng Xin",
      "Yiran Zhong",
      "Yunzhong Hou",
      "Jinjun Wang",
      "Liang Zheng"
    ],
    "author_ids": [],
    "abstract": "Regularization-based methods are beneficial to alleviate the catastrophic\nforgetting problem in class-incremental learning. With the absence of old task\nimages, they often assume that old knowledge is well preserved if the\nclassifier produces similar output on new images. In this paper, we find that\ntheir effectiveness largely depends on the nature of old classes: they work\nwell on classes that are easily distinguishable between each other but may fail\non more fine-grained ones, e.g., boy and girl. In spirit, such methods project\nnew data onto the feature space spanned by the weight vectors in the fully\nconnected layer, corresponding to old classes. The resulting projections would\nbe similar on fine-grained old classes, and as a consequence the new classifier\nwill gradually lose the discriminative ability on these classes. To address\nthis issue, we propose a memory-free generative replay strategy to preserve the\nfine-grained old classes characteristics by generating representative old\nimages directly from the old classifier and combined with new data for new\nclassifier training. To solve the homogenization problem of the generated\nsamples, we also propose a diversity loss that maximizes Kullback Leibler (KL)\ndivergence between generated samples. Our method is best complemented by prior\nregularization-based methods proved to be effective for easily distinguishable\nold classes. We validate the above design and insights on CUB-200-2011,\nCaltech-101, CIFAR-100 and Tiny ImageNet and show that our strategy outperforms\nexisting memory-free methods with a clear margin. Code is available at\nhttps://github.com/xmengxin/MFGR",
    "published_date": "2021-09-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.00328v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.00521v1",
    "title": "Don't Discard All the Biased Instances: Investigating a Core Assumption in Dataset Bias Mitigation Techniques",
    "authors": [
      "Hossein Amirkhani",
      "Mohammad Taher Pilehvar"
    ],
    "author_ids": [],
    "abstract": "Existing techniques for mitigating dataset bias often leverage a biased model\nto identify biased instances. The role of these biased instances is then\nreduced during the training of the main model to enhance its robustness to\nout-of-distribution data. A common core assumption of these techniques is that\nthe main model handles biased instances similarly to the biased model, in that\nit will resort to biases whenever available. In this paper, we show that this\nassumption does not hold in general. We carry out a critical investigation on\ntwo well-known datasets in the domain, MNLI and FEVER, along with two biased\ninstance detection methods, partial-input and limited-capacity models. Our\nexperiments show that in around a third to a half of instances, the biased\nmodel is unable to predict the main model's behavior, highlighted by the\nsignificantly different parts of the input on which they base their decisions.\nBased on a manual validation, we also show that this estimate is highly in line\nwith human interpretation. Our findings suggest that down-weighting of\ninstances detected by bias detection methods, which is a widely-practiced\nprocedure, is an unnecessary waste of training data. We release our code to\nfacilitate reproducibility and future research.",
    "published_date": "2021-09-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.00521v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.00207v1",
    "title": "Fairness based Multi-Preference Resource Allocation in Decentralised Open Markets",
    "authors": [
      "Pankaj Mishra",
      "Ahmed Moustafa",
      "Takayuki Ito"
    ],
    "author_ids": [],
    "abstract": "In this work, we focus on resource allocation in a decentralised open market.\nIn decentralised open markets consists of multiple vendors and multiple\ndynamically-arriving buyers, thus makes the market complex and dynamic.\nBecause, in these markets, negotiations among vendors and buyers take place\nover multiple conflicting issues such as price, scalability, robustness, delay,\netc. As a result, optimising the resource allocation in such open markets\nbecomes directly dependent on two key decisions, which are; incorporating a\ndifferent kind of buyers' preferences, and fairness based vendor elicitation\nstrategy. Towards this end, in this work, we propose a three-step resource\nallocation approach that employs a reverse-auction paradigm. At the first step,\npriority label is attached to each bidding vendor based on the proposed\npriority mechanism. Then, at the second step, the preference score is\ncalculated for all the different kinds of preferences of the buyers. Finally,\nat the third step, based on the priority label of the vendor and the preference\nscore winner is determined. Finally, we compare the proposed approach with two\nstate-of-the-art resource pricing and allocation strategies. The experimental\nresults show that the proposed approach outperforms the other two resource\nallocation approaches in terms of the independent utilities of buyers and the\noverall utility of the open market.",
    "published_date": "2021-09-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT",
      "cs.LG",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.00207v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.00150v1",
    "title": "Federated Reconnaissance: Efficient, Distributed, Class-Incremental Learning",
    "authors": [
      "Sean M. Hendryx",
      "Dharma Raj KC",
      "Bradley Walls",
      "Clayton T. Morrison"
    ],
    "author_ids": [],
    "abstract": "We describe federated reconnaissance, a class of learning problems in which\ndistributed clients learn new concepts independently and communicate that\nknowledge efficiently. In particular, we propose an evaluation framework and\nmethodological baseline for a system in which each client is expected to learn\na growing set of classes and communicate knowledge of those classes efficiently\nwith other clients, such that, after knowledge merging, the clients should be\nable to accurately discriminate between classes in the superset of classes\nobserved by the set of clients. We compare a range of learning algorithms for\nthis problem and find that prototypical networks are a strong approach in that\nthey are robust to catastrophic forgetting while incorporating new information\nefficiently. Furthermore, we show that the online averaging of prototype\nvectors is effective for client model merging and requires only a small amount\nof communication overhead, memory, and update time per class with no\ngradient-based learning or hyperparameter tuning. Additionally, to put our\nresults in context, we find that a simple, prototypical network with four\nconvolutional layers significantly outperforms complex, state of the art\ncontinual learning algorithms, increasing the accuracy by over 22% after\nlearning 600 Omniglot classes and over 33% after learning 20 mini-ImageNet\nclasses incrementally. These results have important implications for federated\nreconnaissance and continual learning more generally by demonstrating that\ncommunicating feature vectors is an efficient, robust, and effective means for\ndistributed, continual learning.",
    "published_date": "2021-09-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.00150v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.00100v4",
    "title": "Proceedings of KDD 2021 Workshop on Data-driven Humanitarian Mapping: Harnessing Human-Machine Intelligence for High-Stake Public Policy and Resilience Planning",
    "authors": [
      "Snehalkumar",
      "S. Gaikwad",
      "Shankar Iyer",
      "Dalton Lunga",
      "Elizabeth Bondi"
    ],
    "author_ids": [],
    "abstract": "Humanitarian challenges, including natural disasters, food insecurity,\nclimate change, racial and gender violence, environmental crises, the COVID-19\ncoronavirus pandemic, human rights violations, and forced displacements,\ndisproportionately impact vulnerable communities worldwide. According to UN\nOCHA, 235 million people will require humanitarian assistance in 2021. Despite\nthese growing perils, there remains a notable paucity of data science research\nto scientifically inform equitable public policy decisions for improving the\nlivelihood of at-risk populations. Scattered data science efforts exist to\naddress these challenges, but they remain isolated from practice and prone to\nalgorithmic harms concerning lack of privacy, fairness, interpretability,\naccountability, transparency, and ethics. Biases in data-driven methods carry\nthe risk of amplifying inequalities in high-stakes policy decisions that impact\nthe livelihood of millions of people. Consequently, proclaimed benefits of\ndata-driven innovations remain inaccessible to policymakers, practitioners, and\nmarginalized communities at the core of humanitarian actions and global\ndevelopment. To help fill this gap, we propose the Data-driven Humanitarian\nMapping Research Program, which focuses on developing novel data science\nmethodologies that harness human-machine intelligence for high-stakes public\npolicy and resilience planning.\n  The proceedings of the 2nd Data-driven Humanitarian Mapping workshop at the\n27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. August 15th,\n2021",
    "published_date": "2021-08-31T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "econ.GN",
      "q-fin.EC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.00100v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2109.00024v1",
    "title": "Machine-Learning media bias",
    "authors": [
      "Samantha D'Alonzo",
      "Max Tegmark"
    ],
    "author_ids": [],
    "abstract": "We present an automated method for measuring media bias. Inferring which\nnewspaper published a given article, based only on the frequencies with which\nit uses different phrases, leads to a conditional probability distribution\nwhose analysis lets us automatically map newspapers and phrases into a bias\nspace. By analyzing roughly a million articles from roughly a hundred\nnewspapers for bias in dozens of news topics, our method maps newspapers into a\ntwo-dimensional bias landscape that agrees well with previous bias\nclassifications based on human judgement. One dimension can be interpreted as\ntraditional left-right bias, the other as establishment bias. This means that\nalthough news bias is inherently political, its measurement need not be.",
    "published_date": "2021-08-31T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.00024v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.14006v1",
    "title": "A Generative Approach for Mitigating Structural Biases in Natural Language Inference",
    "authors": [
      "Dimion Asael",
      "Zachary Ziegler",
      "Yonatan Belinkov"
    ],
    "author_ids": [],
    "abstract": "Many natural language inference (NLI) datasets contain biases that allow\nmodels to perform well by only using a biased subset of the input, without\nconsidering the remainder features. For instance, models are able to make a\nclassification decision by only using the hypothesis, without learning the true\nrelationship between it and the premise. These structural biases lead\ndiscriminative models to learn unintended superficial features and to\ngeneralize poorly out of the training distribution. In this work, we\nreformulate the NLI task as a generative task, where a model is conditioned on\nthe biased subset of the input and the label and generates the remaining subset\nof the input. We show that by imposing a uniform prior, we obtain a provably\nunbiased model. Through synthetic experiments, we find that this approach is\nhighly robust to large amounts of bias. We then demonstrate empirically on two\ntypes of natural bias that this approach leads to fully unbiased models in\npractice. However, we find that generative models are difficult to train and\nthey generally perform worse than discriminative baselines. We highlight the\ndifficulty of the generative modeling task in the context of NLI as a cause for\nthis worse performance. Finally, by fine-tuning the generative model with a\ndiscriminative objective, we reduce the performance gap between the generative\nmodel and the discriminative baseline, while allowing for a small amount of\nbias.",
    "published_date": "2021-08-31T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "I.2.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.14006v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.13843v1",
    "title": "Self-Supervised Learning Based Domain Adaptation for Robust Speaker Verification",
    "authors": [
      "Zhengyang Chen",
      "Shuai Wang",
      "Yanmin Qian"
    ],
    "author_ids": [],
    "abstract": "Large performance degradation is often observed for speaker ver-ification\nsystems when applied to a new domain dataset. Givenan unlabeled target-domain\ndataset, unsupervised domain adaptation(UDA) methods, which usually leverage\nadversarial training strate-gies, are commonly used to bridge the performance\ngap caused bythe domain mismatch. However, such adversarial training\nstrategyonly uses the distribution information of target domain data and cannot\nensure the performance improvement on the target domain. Inthis paper, we\nincorporate self-supervised learning strategy to the un-supervised domain\nadaptation system and proposed a self-supervisedlearning based domain\nadaptation approach (SSDA). Compared tothe traditional UDA method, the new SSDA\ntraining strategy canfully leverage the potential label information from target\ndomainand adapt the speaker discrimination ability from source\ndomainsimultaneously. We evaluated the proposed approach on the Vox-Celeb\n(labeled source domain) and CnCeleb (unlabeled target do-main) datasets, and\nthe best SSDA system obtains 10.2% Equal ErrorRate (EER) on the CnCeleb dataset\nwithout using any speaker labelson CnCeleb, which also can achieve the\nstate-of-the-art results onthis corpus.",
    "published_date": "2021-08-31T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SD",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.13843v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.13653v1",
    "title": "Explaining Classes through Word Attribution",
    "authors": [
      "Samuel Rönnqvist",
      "Amanda Myntti",
      "Aki-Juhani Kyröläinen",
      "Sampo Pyysalo",
      "Veronika Laippala",
      "Filip Ginter"
    ],
    "author_ids": [],
    "abstract": "In recent years, several methods have been proposed for explaining individual\npredictions of deep learning models, yet there has been little study of how to\naggregate these predictions to explain how such models view classes as a whole\nin text classification tasks. In this work, we propose a method for explaining\nclasses using deep learning models and the Integrated Gradients feature\nattribution technique by aggregating explanations of individual examples in\ntext classification to general descriptions of the classes. We demonstrate the\napproach on Web register (genre) classification using the XML-R model and the\nCorpus of Online Registers of English (CORE), finding that the method\nidentifies plausible and discriminative keywords characterizing all but the\nsmallest class.",
    "published_date": "2021-08-31T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.13653v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.13628v2",
    "title": "Learning Optimal Prescriptive Trees from Observational Data",
    "authors": [
      "Nathanael Jo",
      "Sina Aghaei",
      "Andrés Gómez",
      "Phebe Vayanos"
    ],
    "author_ids": [],
    "abstract": "We consider the problem of learning an optimal prescriptive tree (i.e., an\ninterpretable treatment assignment policy in the form of a binary tree) of\nmoderate depth, from observational data. This problem arises in numerous\nsocially important domains such as public health and personalized medicine,\nwhere interpretable and data-driven interventions are sought based on data\ngathered in deployment -- through passive collection of data -- rather than\nfrom randomized trials. We propose a method for learning optimal prescriptive\ntrees using mixed-integer optimization (MIO) technology. We show that under\nmild conditions our method is asymptotically exact in the sense that it\nconverges to an optimal out-of-sample treatment assignment policy as the number\nof historical data samples tends to infinity. Contrary to existing literature,\nour approach: 1) does not require data to be randomized, 2) does not impose\nstringent assumptions on the learned trees, and 3) has the ability to model\ndomain specific constraints. Through extensive computational experiments, we\ndemonstrate that our asymptotic guarantees translate to significant performance\nimprovements in finite samples, as well as showcase our uniquely flexible\nmodeling power by incorporating budget and fairness constraints.",
    "published_date": "2021-08-31T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.13628v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.13597v1",
    "title": "Self-balanced Learning For Domain Generalization",
    "authors": [
      "Jin Kim",
      "Jiyoung Lee",
      "Jungin Park",
      "Dongbo Min",
      "Kwanghoon Sohn"
    ],
    "author_ids": [],
    "abstract": "Domain generalization aims to learn a prediction model on multi-domain source\ndata such that the model can generalize to a target domain with unknown\nstatistics. Most existing approaches have been developed under the assumption\nthat the source data is well-balanced in terms of both domain and class.\nHowever, real-world training data collected with different composition biases\noften exhibits severe distribution gaps for domain and class, leading to\nsubstantial performance degradation. In this paper, we propose a self-balanced\ndomain generalization framework that adaptively learns the weights of losses to\nalleviate the bias caused by different distributions of the multi-domain source\ndata. The self-balanced scheme is based on an auxiliary reweighting network\nthat iteratively updates the weight of loss conditioned on the domain and class\ninformation by leveraging balanced meta data. Experimental results demonstrate\nthe effectiveness of our method overwhelming state-of-the-art works for domain\ngeneralization.",
    "published_date": "2021-08-31T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.13597v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.13555v1",
    "title": "Adaptive Label Smoothing To Regularize Large-Scale Graph Training",
    "authors": [
      "Kaixiong Zhou",
      "Ninghao Liu",
      "Fan Yang",
      "Zirui Liu",
      "Rui Chen",
      "Li Li",
      "Soo-Hyun Choi",
      "Xia Hu"
    ],
    "author_ids": [],
    "abstract": "Graph neural networks (GNNs), which learn the node representations by\nrecursively aggregating information from its neighbors, have become a\npredominant computational tool in many domains. To handle large-scale graphs,\nmost of the existing methods partition the input graph into multiple sub-graphs\n(e.g., through node clustering) and apply batch training to save memory cost.\nHowever, such batch training will lead to label bias within each batch, and\nthen result in over-confidence in model predictions. Since the connected nodes\nwith positively related labels tend to be assigned together, the traditional\ncross-entropy minimization process will attend on the predictions of biased\nclasses in the batch, and may intensify the overfitting issue. To overcome the\nlabel bias problem, we propose the adaptive label smoothing (ALS) method to\nreplace the one-hot hard labels with smoothed ones, which learns to allocate\nlabel confidences from the biased classes to the others. Specifically, ALS\npropagates node labels to aggregate the neighborhood label distribution in a\npre-processing step, and then updates the optimal smoothed labels online to\nadapt to specific graph structure. Experiments on the real-world datasets\ndemonstrate that ALS can be generally applied to the main scalable learning\nframeworks to calibrate the biased labels and improve generalization\nperformances.",
    "published_date": "2021-08-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.13555v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.13477v2",
    "title": "Toward an Actionable Socioeconomic-Aware HCI",
    "authors": [
      "Margaret Burnett",
      "Abrar Fallatah",
      "Catherine Hu",
      "Christopher Perdriau",
      "Christopher Mendez",
      "Caroline Gao",
      "Anita Sarma"
    ],
    "author_ids": [],
    "abstract": "Although inequities for individuals in different socioeconomic situations are\nstarting to capture widespread attention, less attention has been given to the\nsocioeconomic inequities that saturate socioeconomic-diverse individuals' user\nexperiences. To enable HCI practitioners to attend to such inequities and avoid\nunwittingly introducing them, in this paper we consider a wide body of research\nrelevant to how an individual's socioeconomic status (SES) can affect their\nuser experiences with technology. We synthesize this foundational research to\nproduce a core set of 6 evidence-based SES \"facets\" (attribute types and value\nranges) that directly relate to user experiences for individuals in different\nSES strata. We then harness these SES facets to produce actionable paths\nforward -- including a new structured method we call SocioeconomicMag -- by\nwhich HCI researchers and practitioners can bring new socioeconomic-aware\npractices into their everyday HCI work.",
    "published_date": "2021-08-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.13477v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.13025v2",
    "title": "Transport-based Counterfactual Models",
    "authors": [
      "Lucas de Lara",
      "Alberto González-Sanz",
      "Nicholas Asher",
      "Laurent Risser",
      "Jean-Michel Loubes"
    ],
    "author_ids": [],
    "abstract": "Counterfactual frameworks have grown popular in machine learning for both\nexplaining algorithmic decisions but also defining individual notions of\nfairness, more intuitive than typical group fairness conditions. However,\nstate-of-the-art models to compute counterfactuals are either unrealistic or\nunfeasible. In particular, while Pearl's causal inference provides appealing\nrules to calculate counterfactuals, it relies on a model that is unknown and\nhard to discover in practice. We address the problem of designing realistic and\nfeasible counterfactuals in the absence of a causal model. We define\ntransport-based counterfactual models as collections of joint probability\ndistributions between observable distributions, and show their connection to\ncausal counterfactuals. More specifically, we argue that optimal-transport\ntheory defines relevant transport-based counterfactual models, as they are\nnumerically feasible, statistically-faithful, and can coincide under some\nassumptions with causal counterfactual models. Finally, these models make\ncounterfactual approaches to fairness feasible, and we illustrate their\npracticality and efficiency on fair learning. With this paper, we aim at laying\nout the theoretical foundations for a new, implementable approach to\ncounterfactual thinking.",
    "published_date": "2021-08-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.13025v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.12963v2",
    "title": "Scheduled Sampling Based on Decoding Steps for Neural Machine Translation",
    "authors": [
      "Yijin Liu",
      "Fandong Meng",
      "Yufeng Chen",
      "Jinan Xu",
      "Jie Zhou"
    ],
    "author_ids": [],
    "abstract": "Scheduled sampling is widely used to mitigate the exposure bias problem for\nneural machine translation. Its core motivation is to simulate the inference\nscene during training by replacing ground-truth tokens with predicted tokens,\nthus bridging the gap between training and inference. However, vanilla\nscheduled sampling is merely based on training steps and equally treats all\ndecoding steps. Namely, it simulates an inference scene with uniform error\nrates, which disobeys the real inference scene, where larger decoding steps\nusually have higher error rates due to error accumulations. To alleviate the\nabove discrepancy, we propose scheduled sampling methods based on decoding\nsteps, increasing the selection chance of predicted tokens with the growth of\ndecoding steps. Consequently, we can more realistically simulate the inference\nscene during training, thus better bridging the gap between training and\ninference. Moreover, we investigate scheduled sampling based on both training\nsteps and decoding steps for further improvements. Experimentally, our\napproaches significantly outperform the Transformer baseline and vanilla\nscheduled sampling on three large-scale WMT tasks. Additionally, our approaches\nalso generalize well to the text summarization task on two popular benchmarks.",
    "published_date": "2021-08-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.12963v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.12673v4",
    "title": "Tracing app technology: An ethical review in the COVID-19 era and directions for post-COVID-19",
    "authors": [
      "Saleh Afroogh",
      "Amir Esmalian",
      "Ali Mostafavi",
      "Ali Akbari",
      "Kambiz Rasoulkhani",
      "Shahriar Esmaeili",
      "Ehsan Hajiramezanali"
    ],
    "author_ids": [],
    "abstract": "We conducted a systematic literature review on the ethical considerations of\nthe use of contact tracing app technology, which was extensively implemented\nduring the COVID-19 pandemic. The rapid and extensive use of this technology\nduring the COVID-19 pandemic, while benefiting the public well-being by\nproviding information about people's mobility and movements to control the\nspread of the virus, raised several ethical concerns for the post-COVID-19 era.\nTo investigate these concerns for the post-pandemic situation and provide\ndirection for future events, we analyzed the current ethical frameworks,\nresearch, and case studies about the ethical usage of tracing app technology.\nThe results suggest there are seven essential ethical considerations, namely\nprivacy, security, acceptability, government surveillance, transparency,\njustice, and voluntariness in the ethical use of contact tracing technology. In\nthis paper, we explain and discuss these considerations and how they are needed\nfor the ethical usage of this technology. The findings also highlight the\nimportance of developing integrated guidelines and frameworks for\nimplementation of such technology in the post-COVID-19 world.",
    "published_date": "2021-08-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.12673v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.12611v1",
    "title": "Stagewise Unsupervised Domain Adaptation with Adversarial Self-Training for Road Segmentation of Remote Sensing Images",
    "authors": [
      "Lefei Zhang",
      "Meng Lan",
      "Jing Zhang",
      "Dacheng Tao"
    ],
    "author_ids": [],
    "abstract": "Road segmentation from remote sensing images is a challenging task with wide\nranges of application potentials. Deep neural networks have advanced this field\nby leveraging the power of large-scale labeled data, which, however, are\nextremely expensive and time-consuming to acquire. One solution is to use cheap\navailable data to train a model and deploy it to directly process the data from\na specific application domain. Nevertheless, the well-known domain shift (DS)\nissue prevents the trained model from generalizing well on the target domain.\nIn this paper, we propose a novel stagewise domain adaptation model called\nRoadDA to address the DS issue in this field. In the first stage, RoadDA adapts\nthe target domain features to align with the source ones via generative\nadversarial networks (GAN) based inter-domain adaptation. Specifically, a\nfeature pyramid fusion module is devised to avoid information loss of long and\nthin roads and learn discriminative and robust features. Besides, to address\nthe intra-domain discrepancy in the target domain, in the second stage, we\npropose an adversarial self-training method. We generate the pseudo labels of\ntarget domain using the trained generator and divide it to labeled easy split\nand unlabeled hard split based on the road confidence scores. The features of\nhard split are adapted to align with the easy ones using adversarial learning\nand the intra-domain adaptation process is repeated to progressively improve\nthe segmentation performance. Experiment results on two benchmarks demonstrate\nthat RoadDA can efficiently reduce the domain gap and outperforms\nstate-of-the-art methods.",
    "published_date": "2021-08-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.12611v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.12601v1",
    "title": "Mitigation of Diachronic Bias in Fake News Detection Dataset",
    "authors": [
      "Taichi Murayama",
      "Shoko Wakamiya",
      "Eiji Aramaki"
    ],
    "author_ids": [],
    "abstract": "Fake news causes significant damage to society.To deal with these fake news,\nseveral studies on building detection models and arranging datasets have been\nconducted. Most of the fake news datasets depend on a specific time period.\nConsequently, the detection models trained on such a dataset have difficulty\ndetecting novel fake news generated by political changes and social changes;\nthey may possibly result in biased output from the input, including specific\nperson names and organizational names. We refer to this problem as\n\\textbf{Diachronic Bias} because it is caused by the creation date of news in\neach dataset. In this study, we confirm the bias, especially proper nouns\nincluding person names, from the deviation of phrase appearances in each\ndataset. Based on these findings, we propose masking methods using Wikidata to\nmitigate the influence of person names and validate whether they make fake news\ndetection models robust through experiments with in-domain and out-of-domain\ndata.",
    "published_date": "2021-08-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.12601v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.12409v2",
    "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
    "authors": [
      "Ofir Press",
      "Noah A. Smith",
      "Mike Lewis"
    ],
    "author_ids": [],
    "abstract": "Since the introduction of the transformer model by Vaswani et al. (2017), a\nfundamental question has yet to be answered: how does a model achieve\nextrapolation at inference time for sequences that are longer than it saw\nduring training? We first show that extrapolation can be enabled by simply\nchanging the position representation method, though we find that current\nmethods do not allow for efficient extrapolation. We therefore introduce a\nsimpler and more efficient position method, Attention with Linear Biases\n(ALiBi). ALiBi does not add positional embeddings to word embeddings; instead,\nit biases query-key attention scores with a penalty that is proportional to\ntheir distance. We show that this method trains a 1.3 billion parameter model\non input sequences of length 1024 that extrapolates to input sequences of\nlength 2048, achieving the same perplexity as a sinusoidal position embedding\nmodel trained on inputs of length 2048 but training 11% faster and using 11%\nless memory. ALiBi's inductive bias towards recency also leads it to outperform\nmultiple strong position methods on the WikiText-103 benchmark.",
    "published_date": "2021-08-27T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.12409v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.12397v1",
    "title": "Prior Signal Editing for Graph Filter Posterior Fairness Constraints",
    "authors": [
      "Emmanouil Krasanakis",
      "Symeon Papadopoulos",
      "Ioannis Kompatsiaris",
      "Andreas Symeonidis"
    ],
    "author_ids": [],
    "abstract": "Graph filters are an emerging paradigm that systematizes information\npropagation in graphs as transformation of prior node values, called graph\nsignals, to posterior scores. In this work, we study the problem of mitigating\ndisparate impact, i.e. posterior score differences between a protected set of\nsensitive nodes and the rest, while minimally editing scores to preserve\nrecommendation quality. To this end, we develop a scheme that respects\npropagation mechanisms by editing graph signal priors according to their\nposteriors and node sensitivity, where a small number of editing parameters can\nbe tuned to constrain or eliminate disparate impact. We also theoretically\nexplain that coarse prior editing can locally optimize posteriors objectives\nthanks to graph filter robustness. We experiment on a diverse collection of 12\ngraphs with varying number of nodes, where our approach performs equally well\nor better than previous ones in minimizing disparate impact and preserving\nposterior AUC under fairness constraints.",
    "published_date": "2021-08-27T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.12397v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.12250v2",
    "title": "A comparison of approaches to improve worst-case predictive model performance over patient subpopulations",
    "authors": [
      "Stephen R. Pfohl",
      "Haoran Zhang",
      "Yizhe Xu",
      "Agata Foryciarz",
      "Marzyeh Ghassemi",
      "Nigam H. Shah"
    ],
    "author_ids": [],
    "abstract": "Predictive models for clinical outcomes that are accurate on average in a\npatient population may underperform drastically for some subpopulations,\npotentially introducing or reinforcing inequities in care access and quality.\nModel training approaches that aim to maximize worst-case model performance\nacross subpopulations, such as distributionally robust optimization (DRO),\nattempt to address this problem without introducing additional harms. We\nconduct a large-scale empirical study of DRO and several variations of standard\nlearning procedures to identify approaches for model development and selection\nthat consistently improve disaggregated and worst-case performance over\nsubpopulations compared to standard approaches for learning predictive models\nfrom electronic health records data. In the course of our evaluation, we\nintroduce an extension to DRO approaches that allows for specification of the\nmetric used to assess worst-case performance. We conduct the analysis for\nmodels that predict in-hospital mortality, prolonged length of stay, and 30-day\nreadmission for inpatient admissions, and predict in-hospital mortality using\nintensive care data. We find that, with relatively few exceptions, no approach\nperforms better, for each patient subpopulation examined, than standard\nlearning procedures using the entire training dataset. These results imply that\nwhen it is of interest to improve model performance for patient subpopulations\nbeyond what can be achieved with standard practices, it may be necessary to do\nso via data collection techniques that increase the effective sample size or\nreduce the level of noise in the prediction problem.",
    "published_date": "2021-08-27T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.12250v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.12198v1",
    "title": "Deep Reinforcement Learning for Wireless Resource Allocation Using Buffer State Information",
    "authors": [
      "Eike-Manuel Bansbach",
      "Victor Eliachevitch",
      "Laurent Schmalen"
    ],
    "author_ids": [],
    "abstract": "As the number of user equipments (UEs) with various data rate and latency\nrequirements increases in wireless networks, the resource allocation problem\nfor orthogonal frequency-division multiple access (OFDMA) becomes challenging.\nIn particular, varying requirements lead to a non-convex optimization problem\nwhen maximizing the systems data rate while preserving fairness between UEs. In\nthis paper, we solve the non-convex optimization problem using deep\nreinforcement learning (DRL). We outline, train and evaluate a DRL agent, which\nperforms the task of media access control scheduling for a downlink OFDMA\nscenario. To kickstart training of our agent, we introduce mimicking learning.\nFor improvement of scheduling performance, full buffer state information at the\nbase station (e.g. packet age, packet size) is taken into account. Techniques\nlike input feature compression, packet shuffling and age capping further\nimprove the performance of the agent. We train and evaluate our agents using\nNokia's wireless suite and evaluate against different benchmark agents. We show\nthat our agents clearly outperform the benchmark agents.",
    "published_date": "2021-08-27T00:00:00",
    "year": 2021,
    "categories": [
      "cs.NI",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.12198v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.12100v1",
    "title": "A framework for massive scale personalized promotion",
    "authors": [
      "Yitao Shen",
      "Yue Wang",
      "Xingyu Lu",
      "Feng Qi",
      "Jia Yan",
      "Yixiang Mu",
      "Yao Yang",
      "YiFan Peng",
      "Jinjie Gu"
    ],
    "author_ids": [],
    "abstract": "Technology companies building consumer-facing platforms may have access to\nmassive-scale user population. In recent years, promotion with quantifiable\nincentive has become a popular approach for increasing active users on such\nplatforms. On one hand, increased user activities can introduce network effect,\nbring in advertisement audience, and produce other benefits. On the other hand,\nmassive-scale promotion causes massive cost. Therefore making promotion\ncampaigns efficient in terms of return-on-investment (ROI) is of great interest\nto many companies.\n  This paper proposes a practical two-stage framework that can optimize the ROI\nof various massive-scale promotion campaigns. In the first stage, users'\npersonal promotion-response curves are modeled by machine learning techniques.\nIn the second stage, business objectives and resource constraints are\nformulated into an optimization problem, the decision variables of which are\nhow much incentive to give to each user. In order to do effective optimization\nin the second stage, counterfactual prediction and noise-reduction are\nessential for the first stage. We leverage existing counterfactual prediction\ntechniques to correct treatment bias in data. We also introduce a novel deep\nneural network (DNN) architecture, the deep-isotonic-promotion-network (DIPN),\nto reduce noise in the promotion response curves. The DIPN architecture\nincorporates our prior knowledge of response curve shape, by enforcing\nisotonicity and smoothness. It out-performed regular DNN and other\nstate-of-the-art shape-constrained models in our experiments.",
    "published_date": "2021-08-27T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.12100v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.12026v2",
    "title": "Semantic-Based Self-Critical Training For Question Generation",
    "authors": [
      "Loïc",
      "Kwate Dassi"
    ],
    "author_ids": [],
    "abstract": "Question generation is a conditioned language generation task that consists\nin generating a context-aware question given a context and the targeted answer.\nTrain language modelling with a mere likelihood maximization has been widely\nused while suffering from exposure bias and the discordance between the\ntraining and the test metrics. In the way of addressing this issue, The\npresented work portrays a fully Transformer-based reinforcement learning\ngenerator-evaluation architecture for neural question generation. To edge the\nflexibility of the generation, a semantic-based reward score was externally\ninfused during the training to drive the training of the language model. The\nglobal architecture is laid out in a generator-evaluator fashion optimized\ndirectly to n-gram and semantic-based metrics. Evaluation metrics for language\nmodelling only based on n-gram overlapping do not consider semantic relations\nbetween reference and candidate sequences. To improve the evaluation step, a\ntwo-fold evaluation was carried out. On the one side, an n-gram overlapping\nevaluation using the BLEU score. On the other side, a semantic-based assessment\nusing BERTScore and NUBIA. The results were corroborated by a binary human\nevaluation of the semantic relatedness of the generated question and the ground\ntruth. The results obtained showed that use a semantic-based REINFORCE\nalgorithm for the question generation syntactically reshapes the generated\nquestions while preserving their underlying semantic meaning. Many downstream\napplications can be drawn from a successful question generation including the\nenlargement of question answering datasets, the improvement of conversational\nsystems, the enhancement of autonomous educational assessment systems, and so\nforth.",
    "published_date": "2021-08-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.12026v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.11939v2",
    "title": "Understanding and Accelerating Neural Architecture Search with Training-Free and Theory-Grounded Metrics",
    "authors": [
      "Wuyang Chen",
      "Xinyu Gong",
      "Junru Wu",
      "Yunchao Wei",
      "Humphrey Shi",
      "Zhicheng Yan",
      "Yi Yang",
      "Zhangyang Wang"
    ],
    "author_ids": [],
    "abstract": "This work targets designing a principled and unified training-free framework\nfor Neural Architecture Search (NAS), with high performance, low cost, and\nin-depth interpretation. NAS has been explosively studied to automate the\ndiscovery of top-performer neural networks, but suffers from heavy resource\nconsumption and often incurs search bias due to truncated training or\napproximations. Recent NAS works start to explore indicators that can predict a\nnetwork's performance without training. However, they either leveraged limited\nproperties of deep networks, or the benefits of their training-free indicators\nare not applied to more extensive search methods. By rigorous correlation\nanalysis, we present a unified framework to understand and accelerate NAS, by\ndisentangling \"TEG\" characteristics of searched networks - Trainability,\nExpressivity, Generalization - all assessed in a training-free manner. The TEG\nindicators could be scaled up and integrated with various NAS search methods,\nincluding both supernet and single-path approaches. Extensive studies validate\nthe effective and efficient guidance from our TEG-NAS framework, leading to\nboth improved search accuracy and over 56% reduction in search time cost.\nMoreover, we visualize search trajectories on three landscapes of \"TEG\"\ncharacteristics, observing that while a good local minimum is easier to find on\nNAS-Bench-201 given its simple topology, balancing \"TEG\" characteristics is\nmuch harder on the DARTS search space due to its complex landscape geometry.\nOur code is available at https://github.com/VITA-Group/TEGNAS.",
    "published_date": "2021-08-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.11939v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.11892v2",
    "title": "Dynamics of Wealth Inequality in Simple Artificial Societies",
    "authors": [
      "John C. Stevenson"
    ],
    "author_ids": [],
    "abstract": "A simple generative model of a foraging society generates significant wealth\ninequalities from identical agents on an equal opportunity landscape. These\ninequalities arise in both equilibrium and non-equilibrium regimes with some\nsocieties essentially never reaching equilibrium. Reproduction costs mitigate\ninequality beyond their affect on intrinsic growth rate. The highest levels of\ninequality are found during non-equilibrium regimes. Inequality in dynamic\nregimes is driven by factors different than those driving steady state\ninequality. Evolutionary pressures drive the intrinsic growth rate as high as\npossible, leading to a tragedy of the commons.",
    "published_date": "2021-08-26T00:00:00",
    "year": 2021,
    "categories": [
      "physics.soc-ph",
      "cs.MA",
      "econ.GN",
      "q-fin.EC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.11892v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.11846v1",
    "title": "Alleviating Exposure Bias via Contrastive Learning for Abstractive Text Summarization",
    "authors": [
      "Shichao Sun",
      "Wenjie Li"
    ],
    "author_ids": [],
    "abstract": "Encoder-decoder models have achieved remarkable success in abstractive text\nsummarization, which aims to compress one or more documents into a shorter\nversion without the loss of the essential content. Unfortunately, these models\nmostly suffer a discrepancy between training and inference, i.e., the exposure\nbias problem. During the training stage, with teacher forcing these models are\noptimized to maximize the likelihood of the gold summary given the gold summary\ntokens as input to the decoder, while at inference the given tokens are\nreplaced by the generated tokens. Consequently, low-quality summaries are very\nlikely to be generated. To remedy this problem, we propose to leverage\ncontrastive learning to decrease the likelihood of these low-quality summaries,\nand meanwhile increase the likelihood of the gold summary. Since our solution\nexpands the states that the model perceives during training, we expect that the\nexposure bias problem can be alleviated. We experimentally demonstrate that our\nmethod effectively improves the performance of the state-of-the-art model on\ndifferent datasets.",
    "published_date": "2021-08-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.11846v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.11844v1",
    "title": "AI at work -- Mitigating safety and discriminatory risk with technical standards",
    "authors": [
      "Nikolas Becker",
      "Pauline Junginger",
      "Lukas Martinez",
      "Daniel Krupka",
      "Leonie Beining"
    ],
    "author_ids": [],
    "abstract": "The use of artificial intelligence (AI) and AI methods in the workplace holds\nboth great opportunities as well as risks to occupational safety and\ndiscrimination. In addition to legal regulation, technical standards will play\na key role in mitigating such risk by defining technical requirements for\ndevelopment and testing of AI systems. This paper provides an overview and\nassessment of existing international, European and German standards as well as\nthose currently under development. The paper is part of the research project\n\"ExamAI - Testing and Auditing of AI systems\" and focusses on the use of AI in\nan industrial production environment as well as in the realm of human resource\nmanagement (HR).",
    "published_date": "2021-08-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "K.4.1; K.6.1; K.6.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.11844v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.11767v1",
    "title": "A Comparison of Deep Saliency Map Generators on Multispectral Data in Object Detection",
    "authors": [
      "Jens Bayer",
      "David Münch",
      "Michael Arens"
    ],
    "author_ids": [],
    "abstract": "Deep neural networks, especially convolutional deep neural networks, are\nstate-of-the-art methods to classify, segment or even generate images, movies,\nor sounds. However, these methods lack of a good semantic understanding of what\nhappens internally. The question, why a COVID-19 detector has classified a\nstack of lung-ct images as positive, is sometimes more interesting than the\noverall specificity and sensitivity. Especially when human domain expert\nknowledge disagrees with the given output. This way, human domain experts could\nalso be advised to reconsider their choice, regarding the information pointed\nout by the system. In addition, the deep learning model can be controlled, and\na present dataset bias can be found. Currently, most explainable AI methods in\nthe computer vision domain are purely used on image classification, where the\nimages are ordinary images in the visible spectrum. As a result, there is no\ncomparison on how the methods behave with multimodal image data, as well as\nmost methods have not been investigated on how they behave when used for object\ndetection. This work tries to close the gaps. Firstly, investigating three\nsaliency map generator methods on how their maps differ across the different\nspectra. This is achieved via accurate and systematic training. Secondly, we\nexamine how they behave when used for object detection. As a practical problem,\nwe chose object detection in the infrared and visual spectrum for autonomous\ndriving. The dataset used in this work is the Multispectral Object Detection\nDataset, where each scene is available in the FIR, MIR and NIR as well as\nvisual spectrum. The results show that there are differences between the\ninfrared and visual activation maps. Further, an advanced training with both,\nthe infrared and visual data not only improves the network's output, it also\nleads to more focused spots in the saliency maps.",
    "published_date": "2021-08-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.11767v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.11489v3",
    "title": "The Interplay Between Implicit Bias and Benign Overfitting in Two-Layer Linear Networks",
    "authors": [
      "Niladri S. Chatterji",
      "Philip M. Long",
      "Peter L. Bartlett"
    ],
    "author_ids": [],
    "abstract": "The recent success of neural network models has shone light on a rather\nsurprising statistical phenomenon: statistical models that perfectly fit noisy\ndata can generalize well to unseen test data. Understanding this phenomenon of\n$\\textit{benign overfitting}$ has attracted intense theoretical and empirical\nstudy. In this paper, we consider interpolating two-layer linear neural\nnetworks trained with gradient flow on the squared loss and derive bounds on\nthe excess risk when the covariates satisfy sub-Gaussianity and\nanti-concentration properties, and the noise is independent and sub-Gaussian.\nBy leveraging recent results that characterize the implicit bias of this\nestimator, our bounds emphasize the role of both the quality of the\ninitialization as well as the properties of the data covariance matrix in\nachieving low excess risk.",
    "published_date": "2021-08-25T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.ST",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.11489v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.11186v1",
    "title": "Hierarchical Optimization-Based Model Predictive Control for a Class of Discrete Fuzzy Large-Scale Systems Considering Time-Varying Delays and Disturbances",
    "authors": [
      "Mohammad Sarbaz",
      "Iman Zamani",
      "Mohammad Manthouri",
      "Asier Ibeas"
    ],
    "author_ids": [],
    "abstract": "Abstract-In this manuscript, model predictive control for class of discrete\nfuzzy large-scale systems subjected to bounded time-varying delay and\ndisturbances is studied. The considered method is Razumikhin for time-varying\ndelay large-scale systems, in which it includes a Lyapunov function associated\nwith the original non-augmented state space of system dynamics in comparison\nwith the Krasovskii method. As a rule, the Razumikhin method has a perfect\npotential to avoid the inherent complexity of the Krasovskii method especially\nin the presence of large delays and disturbances. The considered large-scale\nsystem in this manuscript is decomposed into several subsystems, each of which\nis represented by a fuzzy Takagi-Sugeno (T-S) model and the interconnection\nbetween any two subsystems is considered. Because the main section of the model\npredictive control is optimization, the hierarchical scheme is performed for\nthe optimization problem. Furthermore, persistent disturbances are considered\nthat robust positive invariance and input-to-state stability under such\ncircumstances are studied. The linear matrix inequalities (LMIs) method is\nperformed for our computations. So the closed-loop large-scale system is\nasymptotically stable. Ultimately, by two examples, the effectiveness of the\nproposed method is illustrated, and a comparison with other papers is made by\nremarks.",
    "published_date": "2021-08-25T00:00:00",
    "year": 2021,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.11186v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.10992v1",
    "title": "OOWL500: Overcoming Dataset Collection Bias in the Wild",
    "authors": [
      "Brandon Leung",
      "Chih-Hui Ho",
      "Amir Persekian",
      "David Orozco",
      "Yen Chang",
      "Erik Sandstrom",
      "Bo Liu",
      "Nuno Vasconcelos"
    ],
    "author_ids": [],
    "abstract": "The hypothesis that image datasets gathered online \"in the wild\" can produce\nbiased object recognizers, e.g. preferring professional photography or certain\nviewing angles, is studied. A new \"in the lab\" data collection infrastructure\nis proposed consisting of a drone which captures images as it circles around\nobjects. Crucially, the control provided by this setup and the natural camera\nshake inherent to flight mitigate many biases. It's inexpensive and easily\nreplicable nature may also potentially lead to a scalable data collection\neffort by the vision community. The procedure's usefulness is demonstrated by\ncreating a dataset of Objects Obtained With fLight (OOWL). Denoted as OOWL500,\nit contains 120,000 images of 500 objects and is the largest \"in the lab\" image\ndataset available when both number of classes and objects per class are\nconsidered. Furthermore, it has enabled several of new insights on object\nrecognition. First, a novel adversarial attack strategy is proposed, where\nimage perturbations are defined in terms of semantic properties such as camera\nshake and pose. Indeed, experiments have shown that ImageNet has considerable\namounts of pose and professional photography bias. Second, it is used to show\nthat the augmentation of in the wild datasets, such as ImageNet, with in the\nlab data, such as OOWL500, can significantly decrease these biases, leading to\nobject recognizers of improved generalization. Third, the dataset is used to\nstudy questions on \"best procedures\" for dataset collection. It is revealed\nthat data augmentation with synthetic images does not suffice to eliminate in\nthe wild datasets biases, and that camera shake and pose diversity play a more\nimportant role in object recognition robustness than previously thought.",
    "published_date": "2021-08-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.10992v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.10934v3",
    "title": "Mitigating Statistical Bias within Differentially Private Synthetic Data",
    "authors": [
      "Sahra Ghalebikesabi",
      "Harrison Wilde",
      "Jack Jewson",
      "Arnaud Doucet",
      "Sebastian Vollmer",
      "Chris Holmes"
    ],
    "author_ids": [],
    "abstract": "Increasing interest in privacy-preserving machine learning has led to new and\nevolved approaches for generating private synthetic data from undisclosed real\ndata. However, mechanisms of privacy preservation can significantly reduce the\nutility of synthetic data, which in turn impacts downstream tasks such as\nlearning predictive models or inference. We propose several re-weighting\nstrategies using privatised likelihood ratios that not only mitigate\nstatistical bias of downstream estimators but also have general applicability\nto differentially private generative models. Through large-scale empirical\nevaluation, we show that private importance weighting provides simple and\neffective privacy-compliant augmentation for general applications of synthetic\ndata.",
    "published_date": "2021-08-24T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.CR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.10934v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.10803v1",
    "title": "Reducing Exposure Bias in Training Recurrent Neural Network Transducers",
    "authors": [
      "Xiaodong Cui",
      "Brian Kingsbury",
      "George Saon",
      "David Haws",
      "Zoltan Tuske"
    ],
    "author_ids": [],
    "abstract": "When recurrent neural network transducers (RNNTs) are trained using the\ntypical maximum likelihood criterion, the prediction network is trained only on\nground truth label sequences. This leads to a mismatch during inference, known\nas exposure bias, when the model must deal with label sequences containing\nerrors. In this paper we investigate approaches to reducing exposure bias in\ntraining to improve the generalization of RNNT models for automatic speech\nrecognition (ASR). A label-preserving input perturbation to the prediction\nnetwork is introduced. The input token sequences are perturbed using SwitchOut\nand scheduled sampling based on an additional token language model. Experiments\nconducted on the 300-hour Switchboard dataset demonstrate their effectiveness.\nBy reducing the exposure bias, we show that we can further improve the accuracy\nof a high-performance RNNT ASR model and obtain state-of-the-art results on the\n300-hour Switchboard dataset.",
    "published_date": "2021-08-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.10803v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.10761v1",
    "title": "Federated Learning for Privacy-Preserving Open Innovation Future on Digital Health",
    "authors": [
      "Guodong Long",
      "Tao Shen",
      "Yue Tan",
      "Leah Gerrard",
      "Allison Clarke",
      "Jing Jiang"
    ],
    "author_ids": [],
    "abstract": "Privacy protection is an ethical issue with broad concern in Artificial\nIntelligence (AI). Federated learning is a new machine learning paradigm to\nlearn a shared model across users or organisations without direct access to the\ndata. It has great potential to be the next-general AI model training framework\nthat offers privacy protection and therefore has broad implications for the\nfuture of digital health and healthcare informatics. Implementing an open\ninnovation framework in the healthcare industry, namely open health, is to\nenhance innovation and creative capability of health-related organisations by\nbuilding a next-generation collaborative framework with partner organisations\nand the research community. In particular, this game-changing collaborative\nframework offers knowledge sharing from diverse data with a privacy-preserving.\nThis chapter will discuss how federated learning can enable the development of\nan open health ecosystem with the support of AI. Existing challenges and\nsolutions for federated learning will be discussed.",
    "published_date": "2021-08-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.10761v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.10744v1",
    "title": "Interpretable deep-learning models to help achieve the Sustainable Development Goals",
    "authors": [
      "Ricardo Vinuesa",
      "Beril Sirmacek"
    ],
    "author_ids": [],
    "abstract": "We discuss our insights into interpretable artificial-intelligence (AI)\nmodels, and how they are essential in the context of developing ethical AI\nsystems, as well as data-driven solutions compliant with the Sustainable\nDevelopment Goals (SDGs). We highlight the potential of extracting\ntruly-interpretable models from deep-learning methods, for instance via\nsymbolic models obtained through inductive biases, to ensure a sustainable\ndevelopment of AI.",
    "published_date": "2021-08-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.10744v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.10665v1",
    "title": "Sharing Practices for Datasets Related to Accessibility and Aging",
    "authors": [
      "Rie Kamikubo",
      "Utkarsh Dwivedi",
      "Hernisa Kacorri"
    ],
    "author_ids": [],
    "abstract": "Datasets sourced from people with disabilities and older adults play an\nimportant role in innovation, benchmarking, and mitigating bias for both\nassistive and inclusive AI-infused applications. However, they are scarce. We\nconduct a systematic review of 137 accessibility datasets manually located\nacross different disciplines over the last 35 years. Our analysis highlights\nhow researchers navigate tensions between benefits and risks in data collection\nand sharing. We uncover patterns in data collection purpose, terminology,\nsample size, data types, and data sharing practices across communities of\nfocus. We conclude by critically reflecting on challenges and opportunities\nrelated to locating and sharing accessibility datasets calling for technical,\nlegal, and institutional privacy frameworks that are more attuned to concerns\nfrom these communities.",
    "published_date": "2021-08-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.10665v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.11320v1",
    "title": "The Effect of Noise Level on Causal Identification with Additive Noise Models",
    "authors": [
      "Benjamin Kap"
    ],
    "author_ids": [],
    "abstract": "In recent years a lot of research has been conducted within the area of\ncausal inference and causal learning. Many methods have been developed to\nidentify the cause-effect pairs in models and have been successfully applied to\nobservational real-world data in order to determine the direction of causal\nrelationships. Many of these methods require simplifying assumptions, such as\nabsence of confounding, cycles, and selection bias. Yet in bivariate situations\ncausal discovery problems remain challenging. One class of such methods, that\nalso allows tackling the bivariate case, is based on Additive Noise Models\n(ANMs). Unfortunately, one aspect of these methods has not received much\nattention until now: what is the impact of different noise levels on the\nability of these methods to identify the direction of the causal relationship.\nThis work aims to bridge this gap with the help of an empirical study. For this\nwork, we considered bivariate cases, which is the most elementary form of a\ncausal discovery problem where one needs to decide whether X causes Y or Y\ncauses X, given joint distributions of two variables X, Y. Furthermore, two\nspecific methods have been selected, \\textit{Regression with Subsequent\nIndependence Test} and \\textit{Identification using Conditional Variances},\nwhich have been tested with an exhaustive range of ANMs where the additive\nnoises' levels gradually change from 1% to 10000% of the causes' noise level\n(the latter remains fixed). Additionally, the experiments in this work consider\nseveral different types of distributions as well as linear and non-linear ANMs.\nThe results of the experiments show that these methods can fail to capture the\ntrue causal direction for some levels of noise.",
    "published_date": "2021-08-24T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG",
      "62G86",
      "G.3.7; G.3.11"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.11320v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.10643v2",
    "title": "Morality-based Assertion and Homophily on Social Media: A Cultural Comparison between English and Japanese Languages",
    "authors": [
      "Maneet Singh",
      "Rishemjit Kaur",
      "Akiko Matsuo",
      "S. R. S. Iyengar",
      "Kazutoshi Sasahara"
    ],
    "author_ids": [],
    "abstract": "Moral psychology is a domain that deals with moral identity, appraisals and\nemotions. Previous work has primarily focused on moral development and the\nassociated role of culture. Knowing that language is an inherent element of a\nculture, we used the social media platform Twitter to compare moral behaviors\nof Japanese tweets with English tweets. The five basic moral foundations, i.e.,\nCare, Fairness, Ingroup, Authority and Purity, along with the associated\nemotional valence were compared between English and Japanese tweets. The tweets\nfrom Japanese users depicted relatively higher Fairness, Ingroup, and Purity,\nwhereas English tweets expressed more positive emotions for all moral\ndimensions. Considering moral similarities in connecting users on social media,\nwe quantified homophily concerning different moral dimensions using our\nproposed method. The moral dimensions Care, Authority and Purity for English\nand Ingroup, Authority and Purity for Japanese depicted homophily on Twitter.\nOverall, our study uncovers the underlying cultural differences with respect to\nmoral behavior in English- and Japanese-speaking users.",
    "published_date": "2021-08-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI",
      "J.4; I.2.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.10643v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.10549v1",
    "title": "StyleAugment: Learning Texture De-biased Representations by Style Augmentation without Pre-defined Textures",
    "authors": [
      "Sanghyuk Chun",
      "Song Park"
    ],
    "author_ids": [],
    "abstract": "Recent powerful vision classifiers are biased towards textures, while shape\ninformation is overlooked by the models. A simple attempt by augmenting\ntraining images using the artistic style transfer method, called Stylized\nImageNet, can reduce the texture bias. However, Stylized ImageNet approach has\ntwo drawbacks in fidelity and diversity. First, the generated images show low\nimage quality due to the significant semantic gap betweeen natural images and\nartistic paintings. Also, Stylized ImageNet training samples are pre-computed\nbefore training, resulting in showing the lack of diversity for each sample. We\npropose a StyleAugment by augmenting styles from the mini-batch. StyleAugment\ndoes not rely on the pre-defined style references, but generates augmented\nimages on-the-fly by natural images in the mini-batch for the references.\nHence, StyleAugment let the model observe abundant confounding cues for each\nimage by on-the-fly the augmentation strategy, while the augmented images are\nmore realistic than artistic style transferred images. We validate the\neffectiveness of StyleAugment in the ImageNet dataset with robustness\nbenchmarks, such as texture de-biased accuracy, corruption robustness, natural\nadversarial samples, and occlusion robustness. StyleAugment shows better\ngeneralization performances than previous unsupervised de-biasing methods and\nstate-of-the-art data augmentation methods in our experiments.",
    "published_date": "2021-08-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.10549v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.10442v2",
    "title": "Evaluating Fairness in Argument Retrieval",
    "authors": [
      "Sachin Pathiyan Cherumanal",
      "Damiano Spina",
      "Falk Scholer",
      "W. Bruce Croft"
    ],
    "author_ids": [],
    "abstract": "Existing commercial search engines often struggle to represent different\nperspectives of a search query. Argument retrieval systems address this\nlimitation of search engines and provide both positive (PRO) and negative (CON)\nperspectives about a user's information need on a controversial topic (e.g.,\nclimate change). The effectiveness of such argument retrieval systems is\ntypically evaluated based on topical relevance and argument quality, without\ntaking into account the often differing number of documents shown for the\nargument stances (PRO or CON). Therefore, systems may retrieve relevant\npassages, but with a biased exposure of arguments. In this work, we analyze a\nrange of non-stochastic fairness-aware ranking and diversity metrics to\nevaluate the extent to which argument stances are fairly exposed in argument\nretrieval systems.\n  Using the official runs of the argument retrieval task Touch\\'e at CLEF 2020,\nas well as synthetic data to control the amount and order of argument stances\nin the rankings, we show that systems with the best effectiveness in terms of\ntopical relevance are not necessarily the most fair or the most diverse in\nterms of argument stance. The relationships we found between (un)fairness and\ndiversity metrics shed light on how to evaluate group fairness -- in addition\nto topical relevance -- in argument retrieval settings.",
    "published_date": "2021-08-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.10442v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.10379v1",
    "title": "Examining Covert Gender Bias: A Case Study in Turkish and English Machine Translation Models",
    "authors": [
      "Chloe Ciora",
      "Nur Iren",
      "Malihe Alikhani"
    ],
    "author_ids": [],
    "abstract": "As Machine Translation (MT) has become increasingly more powerful,\naccessible, and widespread, the potential for the perpetuation of bias has\ngrown alongside its advances. While overt indicators of bias have been studied\nin machine translation, we argue that covert biases expose a problem that is\nfurther entrenched. Through the use of the gender-neutral language Turkish and\nthe gendered language English, we examine cases of both overt and covert gender\nbias in MT models. Specifically, we introduce a method to investigate\nasymmetrical gender markings. We also assess bias in the attribution of\npersonhood and examine occupational and personality stereotypes through overt\nbias indicators in MT models. Our work explores a deeper layer of bias in MT\nmodels and demonstrates the continued need for language-specific,\ninterdisciplinary methodology in MT model development.",
    "published_date": "2021-08-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.10379v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.10360v2",
    "title": "Interpreting Face Inference Models using Hierarchical Network Dissection",
    "authors": [
      "Divyang Teotia",
      "Agata Lapedriza",
      "Sarah Ostadabbas"
    ],
    "author_ids": [],
    "abstract": "This paper presents Hierarchical Network Dissection, a general pipeline to\ninterpret the internal representation of face-centric inference models. Using a\nprobabilistic formulation, our pipeline pairs units of the model with concepts\nin our \"Face Dictionary\", a collection of facial concepts with corresponding\nsample images. Our pipeline is inspired by Network Dissection, a popular\ninterpretability model for object-centric and scene-centric models. However,\nour formulation allows to deal with two important challenges of face-centric\nmodels that Network Dissection cannot address: (1) spacial overlap of concepts:\nthere are different facial concepts that simultaneously occur in the same\nregion of the image, like \"nose\" (facial part) and \"pointy nose\" (facial\nattribute); and (2) global concepts: there are units with affinity to concepts\nthat do not refer to specific locations of the face (e.g. apparent age). We use\nHierarchical Network Dissection to dissect different face-centric inference\nmodels trained on widely-used facial datasets. The results show models trained\nfor different tasks learned different internal representations. Furthermore,\nthe interpretability results can reveal some biases in the training data and\nsome interesting characteristics of the face-centric inference tasks. Finally,\nwe conduct controlled experiments on biased data to showcase the potential of\nHierarchical Network Dissection for bias discovery. The results illustrate how\nHierarchical Network Dissection can be used to discover and quantify bias in\nthe training data that is also encoded in the model.",
    "published_date": "2021-08-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.10360v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.10343v1",
    "title": "Gender-based occupational segregation: a bit string approach",
    "authors": [
      "Joana Passinhas",
      "Tanya Araújo"
    ],
    "author_ids": [],
    "abstract": "The systematic differences of gender representation across occupations,\ngender-based occupational segregation, has been suggested as one of the most\nimportant determinants of the still existing gender wage gap. Despite some\nsigns of a decreasing trend, there is evidence that occupational gendered\nsegregation is persistent even though gender differences in human capital\nvariables have been disappearing. Using an agent-based model we provide a\nframework that introduces discriminatory behavior based on labour market\ntheories of discrimination where workers and firms can exhibit gendered\npreferences. The introduction of discriminatory behavior transforms the\notherwise random dynamics of occupational choice into a persistent gender-based\noccupational segregation consistent with empirical evidence.",
    "published_date": "2021-08-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI",
      "cs.MA",
      "nlin.AO",
      "68U99",
      "I.6"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.10343v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.10265v1",
    "title": "Exploring Biases and Prejudice of Facial Synthesis via Semantic Latent Space",
    "authors": [
      "Xuyang Shen",
      "Jo Plested",
      "Sabrina Caldwell",
      "Tom Gedeon"
    ],
    "author_ids": [],
    "abstract": "Deep learning (DL) models are widely used to provide a more convenient and\nsmarter life. However, biased algorithms will negatively influence us. For\ninstance, groups targeted by biased algorithms will feel unfairly treated and\neven fearful of negative consequences of these biases. This work targets biased\ngenerative models' behaviors, identifying the cause of the biases and\neliminating them. We can (as expected) conclude that biased data causes biased\npredictions of face frontalization models. Varying the proportions of male and\nfemale faces in the training data can have a substantial effect on behavior on\nthe test data: we found that the seemingly obvious choice of 50:50 proportions\nwas not the best for this dataset to reduce biased behavior on female faces,\nwhich was 71% unbiased as compared to our top unbiased rate of 84%. Failure in\ngeneration and generating incorrect gender faces are two behaviors of these\nmodels. In addition, only some layers in face frontalization models are\nvulnerable to biased datasets. Optimizing the skip-connections of the generator\nin face frontalization models can make models less biased. We conclude that it\nis likely to be impossible to eliminate all training bias without an unlimited\nsize dataset, and our experiments show that the bias can be reduced and\nquantified. We believe the next best to a perfect unbiased predictor is one\nthat has minimized the remaining known bias.",
    "published_date": "2021-08-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.10265v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.10108v1",
    "title": "Integrating Transductive And Inductive Embeddings Improves Link Prediction Accuracy",
    "authors": [
      "Chitrank Gupta",
      "Yash Jain",
      "Abir De",
      "Soumen Chakrabarti"
    ],
    "author_ids": [],
    "abstract": "In recent years, inductive graph embedding models, \\emph{viz.}, graph neural\nnetworks (GNNs) have become increasingly accurate at link prediction (LP) in\nonline social networks. The performance of such networks depends strongly on\nthe input node features, which vary across networks and applications. Selecting\nappropriate node features remains application-dependent and generally an open\nquestion. Moreover, owing to privacy and ethical issues, use of personalized\nnode features is often restricted. In fact, many publicly available data from\nonline social network do not contain any node features (e.g., demography). In\nthis work, we provide a comprehensive experimental analysis which shows that\nharnessing a transductive technique (e.g., Node2Vec) for obtaining initial node\nrepresentations, after which an inductive node embedding technique takes over,\nleads to substantial improvements in link prediction accuracy. We demonstrate\nthat, for a wide variety of GNN variants, node representation vectors obtained\nfrom Node2Vec serve as high quality input features to GNNs, thereby improving\nLP performance.",
    "published_date": "2021-08-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.10108v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.10096v1",
    "title": "The Ethical Implications of Digital Contact Tracing for LGBTQIA+ Communities",
    "authors": [
      "Izak van Zyl",
      "Nyx McLean"
    ],
    "author_ids": [],
    "abstract": "The onset of COVID-19 has led to the introduction of far-reaching digital\ninterventions in the interest of public health. Among these, digital contact\ntracing has been proposed as a viable means of targeted control in countries\nacross the globe, including on the African continent. This, in turn, creates\nsignificant ethical challenges for vulnerable communities, including LGBTQIA+\npersons. In this research paper, we explore some of the ethical implications of\ndigital contact tracing for the LGBTQIA+ community. We refer specifically to\nthe digital infringement of freedoms, and ground our discussion in the\ndiscourse of data colonisation and Big Tech. We propose a critical\nintersectional feminism towards developing inclusive technology that is\ndecentralised and user controlled. This approach is informed by a feminist\nethics of care that emphasises multiple lived experiences.",
    "published_date": "2021-08-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.10096v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.10093v1",
    "title": "Artificial Intelligence in the Global South (AI4D): Potential and Risks",
    "authors": [
      "P. J. Wall",
      "Deepak Saxena",
      "Suzana Brown"
    ],
    "author_ids": [],
    "abstract": "Artificial intelligence is becoming more widely available in all parts of the\nworld. This has created many previously unforeseen possibilities for addressing\nthe challenges outlined in the Sustainable Development Goals in the Global\nSouth. However, the use of AI in such contexts brings with it a unique set of\nrisks and challenges. Among these are the potential for Governments to use such\ntechnologies to suppress their own people, and the ethical questions arising\nfrom implementing AI primarily designed and developed in the Global North into\nvastly different social, cultural, and political environments in the Global\nSouth. This paper examines the key issues and questions arising in the emerging\nsub-field of AI for global development (AI4D) and the potential and risks\nassociated with using such technologies in the Global South. We propose that\nalthough there are many risks associated with the use of AI, the potential\nbenefits are enough to warrant detailed research and investigation of the most\nappropriate and effective ways to design, develop, implement, and use such\ntechnologies in the Global South. We conclude by calling for the wider ICT4D\ncommunity to continue to conduct detailed research and investigation of all\naspects of AI4D.",
    "published_date": "2021-08-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.10093v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.10089v1",
    "title": "Gender Data 4 Girls?: A Postcolonial Feminist Participatory Study in Bangladesh",
    "authors": [
      "Isobel Talks"
    ],
    "author_ids": [],
    "abstract": "Premised on the logic that more, high-quality information on majority world\nwomen's lives will improve the effectiveness of interventions addressing gender\ninequality, mainstream development institutions have invested heavily in gender\ndata initiatives of late. However, critical empirical and theoretical\ninvestigations into gender data for development policy and practice are\nlacking. Postcolonial feminist theory has long provided a critical lens through\nwhich to analyse international development projects that target women in the\nmajority world. However, postcolonial feminism remains underutilised for\ncritically investigating data for development projects. This paper addresses\nthese gaps through presenting the findings from a participatory action research\nproject with young women involved in a gender data for development project in\nBangladesh. Echoing postcolonial feminist concerns with development, the\n'DataGirls' had some concerns that data was being extracted from their\ncommunities, representing the priorities of external NGOs to a greater extent\nthan their own. However, through collaborating to develop and deliver community\nevents on child marriage with the 'DataGirls', this research demonstrates that\nparticipatory approaches can address some postcolonial feminist criticisms of\n(data for) development, by ensuring that gender data is enacted by and for\nmajority world women rather than Western development institutions.",
    "published_date": "2021-08-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.10089v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.10008v1",
    "title": "BiaSwap: Removing dataset bias with bias-tailored swapping augmentation",
    "authors": [
      "Eungyeup Kim",
      "Jihyeon Lee",
      "Jaegul Choo"
    ],
    "author_ids": [],
    "abstract": "Deep neural networks often make decisions based on the spurious correlations\ninherent in the dataset, failing to generalize in an unbiased data\ndistribution. Although previous approaches pre-define the type of dataset bias\nto prevent the network from learning it, recognizing the bias type in the real\ndataset is often prohibitive. This paper proposes a novel bias-tailored\naugmentation-based approach, BiaSwap, for learning debiased representation\nwithout requiring supervision on the bias type. Assuming that the bias\ncorresponds to the easy-to-learn attributes, we sort the training images based\non how much a biased classifier can exploits them as shortcut and divide them\ninto bias-guiding and bias-contrary samples in an unsupervised manner.\nAfterwards, we integrate the style-transferring module of the image translation\nmodel with the class activation maps of such biased classifier, which enables\nto primarily transfer the bias attributes learned by the classifier. Therefore,\ngiven the pair of bias-guiding and bias-contrary, BiaSwap generates the\nbias-swapped image which contains the bias attributes from the bias-contrary\nimages, while preserving bias-irrelevant ones in the bias-guiding images. Given\nsuch augmented images, BiaSwap demonstrates the superiority in debiasing\nagainst the existing baselines over both synthetic and real-world datasets.\nEven without careful supervision on the bias, BiaSwap achieves a remarkable\nperformance on both unbiased and bias-guiding samples, implying the improved\ngeneralization capability of the model.",
    "published_date": "2021-08-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.10008v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.09959v1",
    "title": "Artificial Intelligence Ethics: An Inclusive Global Discourse?",
    "authors": [
      "Cathy Roche",
      "Dave Lewis",
      "P. J. Wall"
    ],
    "author_ids": [],
    "abstract": "It is widely accepted that technology is ubiquitous across the planet and has\nthe potential to solve many of the problems existing in the Global South.\nMoreover, the rapid advancement of artificial intelligence (AI) brings with it\nthe potential to address many of the challenges outlined in the Sustainable\nDevelopment Goals (SDGs) in ways which were never before possible. However,\nthere are many questions about how such advanced technologies should be managed\nand governed, and whether or not the emerging ethical frameworks and standards\nfor AI are dominated by the Global North. This research examines the growing\nbody of documentation on AI ethics to examine whether or not there is equality\nof participation in the ongoing global discourse. Specifically, it seeks to\ndiscover if both countries in the Global South and women are underrepresented\nin this discourse. Findings indicate a dearth of references to both of these\nthemes in the AI ethics documents, suggesting that the associated ethical\nimplications and risks are being neglected. Without adequate input from both\ncountries in the Global South and from women, such ethical frameworks and\nstandards may be discriminatory with the potential to reinforce\nmarginalisation.",
    "published_date": "2021-08-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.09959v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.09958v1",
    "title": "Earth Observation and the New African Rural Datascapes: Defining an Agenda for Critical Research",
    "authors": [
      "Rose Pritchard",
      "Wilhelm Kiwango",
      "Andy Challinor"
    ],
    "author_ids": [],
    "abstract": "The increasing availability of Earth Observation data could transform the use\nand governance of African rural landscapes, with major implications for the\nlivelihoods and wellbeing of people living in those landscapes. Recent years\nhave seen a rapid increase in the development of EO data applications targeted\nat stakeholders in African agricultural systems. But there is still relatively\nlittle critical scholarship questioning how EO data are accessed, presented,\ndisseminated and used in different socio-political contexts, or of whether this\nincreases or decreases the wellbeing of poorer and marginalized peoples. We\nhighlight three neglected areas in existing EO-for-development research: (i)\nthe imaginaries of 'ideal' future landscapes informing deployments of EO data;\n(ii) how power relationships in larger EO-for-development networks shape the\ndistribution of costs and benefits; and (iii) how these larger-scale political\ndynamics interact with local-scale inequalities to influence the resilience of\nmarginalised peoples. We then propose a framework for critical\nEO-for-development research drawing on recent thinking in critical data\nstudies, ICT4D and political ecology.",
    "published_date": "2021-08-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.09958v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.09955v1",
    "title": "Students' Engagement in Anonymous Peer Review: Using the Open-Source Sakai Platform",
    "authors": [
      "Fazlyn Petersen",
      "Bradley Groenewald"
    ],
    "author_ids": [],
    "abstract": "There is a need to provide quality education without discrimination or\nprejudice to all students. However, there are challenges in implementing\nquality education in large classes, especially during remote learning.\nLiterature indicates that providing lecturer feedback can become a tedious\ntask, especially in large classes. Literature states that involving students in\nthe peer review process can improve the quality of their submissions. This\nresearch used a case study and thematic analysis. Qualitative data were\ncollected from 179 third-year Information Systems students who used the\nOpensource Sakai Platform. Students reviewed another student's report, without\nknowing their identity. The research used self-determination theory as a\ntheoretical basis. The achievement of perceived autonomy is supported as an\nanonymous peer review helped students to empower themselves. Perceived\ncompetence was also achieved as the anonymous peer review improved the quality\nof work submitted and the development of workplace skills. Perceived\nrelatedness is supported as students indicated that the anonymous peer review\nallowed them to learn from their peers. It also improved their understanding\nand the ability to see errors in their work. Despite the negative aspects\nidentified using the Sakai platform, it may provide a viable alternative for\nproviding feedback remotely, especially during the Covid-19 pandemic.",
    "published_date": "2021-08-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.09955v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.09947v1",
    "title": "For Better or for Worse? A Framework for Critical Analysis of ICT4D for Women",
    "authors": [
      "Abhipsa Pal",
      "Rahul De'"
    ],
    "author_ids": [],
    "abstract": "Diffusion of ICTs provide possibilities for women empowerment by greater\nparticipation and enhanced gender-based digital equality. However, a critical\nanalysis reveals that as ICT diffusion widens, there is a persistent threat of\nwidening the gender-based digital divide and exposes women to online sexual\nabuses, predominantly in developing countries characterized by the gendered\nnature of the social structure. Instead of accepting ICT as the facilitator to\nwomen empowerment, in this paper, we develop a critical research framework for\na gender-focused examination of ICT4D studies. Critical research methodology\nprovides the appropriate ontology unveiling social realities through\nchallenging the status quo and exposing the deeper societal inequalities. Using\nthe critical research framework developed, we investigate past ICT4D\ninitiatives and artifacts from literature and draw critical conclusions of its\nbenefits and issues. This study would aid future ICT4D research to investigate\nareas of gender discrimination and understand the role of ICTs in a critical\nlight.",
    "published_date": "2021-08-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.09947v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.09932v1",
    "title": "Federated Learning Meets Fairness and Differential Privacy",
    "authors": [
      "Manisha Padala",
      "Sankarshan Damle",
      "Sujit Gujar"
    ],
    "author_ids": [],
    "abstract": "Deep learning's unprecedented success raises several ethical concerns ranging\nfrom biased predictions to data privacy. Researchers tackle these issues by\nintroducing fairness metrics, or federated learning, or differential privacy. A\nfirst, this work presents an ethical federated learning model, incorporating\nall three measures simultaneously. Experiments on the Adult, Bank and Dutch\ndatasets highlight the resulting ``empirical interplay\" between accuracy,\nfairness, and privacy.",
    "published_date": "2021-08-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.09932v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.09864v1",
    "title": "A Round-Robin Packet Scheduler for Hierarchical Max-Min Fairness",
    "authors": [
      "Natchanon Luangsomboon",
      "Jörg Liebeherr"
    ],
    "author_ids": [],
    "abstract": "Hierarchical link sharing addresses the demand for fine-grain traffic control\nat multiple levels of aggregation. At present, packet schedulers that can\nsupport hierarchical link sharing are not suitable for an implementation at\nline rates, and deployed schedulers perform poorly when distributing excess\ncapacity to classes that need additional bandwidth. We present HLS, a packet\nscheduler that ensures a hierarchical max-min fair allocation of the link\nbandwidth. HLS supports minimum rate guarantees and isolation between classes.\nSince it is realized as a non-hierarchical round robin scheduler, it is\nsuitable to operate at high rates. We implement HLS in the Linux kernel and\nevaluate it with respect to achieved rate allocations and overhead. We compare\nthe results with those obtained for CBQ and HTB, the existing scheduling\nalgorithms in Linux for hierarchical link sharing. We show that the overhead of\nHLS is comparable to that of other classful packet schedulers.",
    "published_date": "2021-08-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.NI",
      "C.2.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.09864v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.12255v1",
    "title": "Assessing Gender Bias in the Information Systems Field: An Analysis of the Impact on Citations",
    "authors": [
      "Silvia Masiero",
      "Aleksi Aaltonen"
    ],
    "author_ids": [],
    "abstract": "Gender bias, a systemic and unfair difference in how men and women are\ntreated in a given domain, is widely studied across different academic fields.\nYet, there are barely any studies of the phenomenon in the field of academic\ninformation systems (IS), which is surprising especially in the light of the\nproliferation of such studies in the Science, Technology, Mathematics and\nTechnology (STEM) disciplines. To assess potential gender bias in the IS field,\nthis paper outlines a study to estimate the impact of scholarly citations that\nfemale IS academics accumulate vis-\\`a-vis their male colleagues. Drawing on a\nscientometric study of the 7,260 papers published in the most prestigious IS\njournals (known as the AIS Basket of Eight), our analysis aims to unveil\npotential bias in the accumulation of citations between genders in the field.\nWe use panel regression to estimate the gendered citations accumulation in the\nfield. By doing so we propose to contribute knowledge on a core dimension of\ngender bias in academia, which is, so far, almost completely unexplored in the\nIS field.",
    "published_date": "2021-08-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.12255v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.09788v1",
    "title": "Gendering of Smartphone Ownership and Autonomy among Youth: Narratives from Rural India",
    "authors": [
      "Renza Iqbal"
    ],
    "author_ids": [],
    "abstract": "This study delves into the research question: how does gender influence\nsmartphone ownership and autonomy in using the internet among the youth in\nrural India? This paper explores the influence of local culture on smartphone\nownership and autonomy through an ethnographic study among rural Indian youth\nby analysing the intersection of gender with other identity axes. The findings\nshow that young people's smartphone ownership and autonomy is shaped by their\nsocial and cultural setting, and could lead to various inequalities in their\ninternet usage. This study shows that gender paves way for various disparities\nwith regard to smartphone ownership and internet usage. Decolonisation of the\nunderstanding of smartphone ownership and internet usage patterns of the youth\nin the Global South suggests a reconsideration of the user experience designs\nand platform policies.",
    "published_date": "2021-08-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.09788v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.09783v1",
    "title": "From Digital Divide to Digital Justice in the Global South: Conceptualising Adverse Digital Incorporation",
    "authors": [
      "Richard Heeks"
    ],
    "author_ids": [],
    "abstract": "The connection between digital and inequality has traditionally been\nunderstood in terms of the digital divide or of forms of digital inequality\nwhose core conceptualisation is exclusion. This paper argues that, as the\nglobal South moves into a digital development paradigm of growing breadth and\ndepth of digital engagement, an exclusion worldview is no longer sufficient.\nDrawing from ideas in the development studies literature on chronic poverty,\nthe paper argues the need for a new concept: \"adverse digital incorporation\",\nmeaning inclusion in a digital system that enables a more-advantaged group to\nextract disproportionate value from the work or resources of another,\nless-advantaged group. This explains why inequality persists - even grows - in\na digital development paradigm. To help ground future research and practice on\nthis issue, the paper inductively builds a conceptual model of adverse digital\nincorporation with three main component sets: the processes, the drivers, and\nthe causes of adverse digital incorporation. The paper concludes with thoughts\non a future research and practice agenda that seeks to deliver digital justice\nin the global South: a necessary reconfiguration of the broader components of\npower that currently shape the inclusionary connection between digital and\ninequality.",
    "published_date": "2021-08-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.09783v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.09782v1",
    "title": "The Liberalities and Tyrannies of ICTs for Vulnerable Migrants: The Status Quo, Gaps and Directions",
    "authors": [
      "Yidnekachew Redda Haile"
    ],
    "author_ids": [],
    "abstract": "Information and communication technologies (ICTs) have increasingly become\nvital for people on the move including the nearly 80 million displaced due to\nconflict, violence, and human right violations globally. However, existing\nresearch on ICTs and migrants, which almost entirely focused on migrants' ICT\nuse 'en route' or within developed economies principally in the perspectives of\nresearchers from these regions, is very fragmented posing a difficulty in\nunderstanding the key objects of research. Moreover, ICTs are often celebrated\nas liberating and exploitable at migrants' rational discretion even though they\nare 'double-edged swords' with significant risks, burdens, pressures and\ninequality challenges particularly for vulnerable migrants including those\nforcefully displaced and trafficked. Towards addressing these limitations and\nilluminating future directions, this paper, first, scrutinises the existing\nresearch vis-a-vis ICTs' liberating and authoritarian role particularly for\nvulnerable migrants whereby explicating key issues in the research domain.\nSecond, it identifies key gaps and opportunities for future research. Using a\ntailored methodology, broad literature relating to ICTs and\nmigration/development published in the period 1990-2020 was surveyed resulting\nin 157 selected publications which were critically appraised vis-a-vis the key\nthemes, major technologies dealt with, and methodologies and theories/concepts\nadopted. Furthermore, key insights, trends, gaps, and future research\nopportunities pertaining to both the existing and missing objects of research\nin ICTs and migration/development are spotlighted.",
    "published_date": "2021-08-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.09782v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.09712v1",
    "title": "Resilient ICT4D: Building and Sustaining our Community in Pandemic Times",
    "authors": [
      "Silvia Masiero",
      "Petter Nielsen"
    ],
    "author_ids": [],
    "abstract": "The impacts of the COVID-19 pandemic, disproportionally affecting vulnerable\npeople and deepening pre-existing inequalities (Dreze, 2020; Qureshi, 2021),\nhave interested the very same \"development\" processes that the IFIP Working\nGroup 9.4 on the Implications of Information and Digital Technologies for\nDevelopment has dealt with over time. A global development paradigm (Oldekop et\nal., 2020) has emerged in response to the global nature of the crisis, infusing\nnew meaning in the spirit of \"making a better world\" with ICTs (Walsham, 2012)\nthat always have characterised ICT4D research. Such a new meaning\ncontextualises our research in the landscape of the first pandemic of the\ndatafied society (Milan & Trere, 2020), coming to terms with the silencing of\nnarratives from the margins within the pandemic (Milan et al., 2021) - in\nQureshi's (2021) words, a \"pandemics within the pandemic\" producing new\nsocio-economic inequities in a state of global emergency.",
    "published_date": "2021-08-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.09712v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.09646v2",
    "title": "A Systematic Review of Automated Query Reformulations in Source Code Search",
    "authors": [
      "Mohammad Masudur Rahman",
      "Chanchal K. Roy"
    ],
    "author_ids": [],
    "abstract": "Fixing software bugs and adding new features are two of the major maintenance\ntasks. Software bugs and features are reported as change requests. Developers\nconsult these requests and often choose a few keywords from them as an ad hoc\nquery. Then they execute the query with a search engine to find the exact\nlocations within software code that need to be changed. Unfortunately, even\nexperienced developers often fail to choose appropriate queries, which leads to\ncostly trials and errors during a code search. Over the years, many studies\nattempt to reformulate the ad hoc queries from developers to support them. In\nthis systematic literature review, we carefully select 70 primary studies on\nquery reformulations from 2,970 candidate studies, perform an in-depth\nqualitative analysis (e.g., Grounded Theory), and then answer seven research\nquestions with major findings. First, to date, eight major methodologies (e.g.,\nterm weighting, term co-occurrence analysis, thesaurus lookup) have been\nadopted to reformulate queries. Second, the existing studies suffer from\nseveral major limitations (e.g., lack of generalizability, vocabulary mismatch\nproblem, subjective bias) that might prevent their wide adoption. Finally, we\ndiscuss the best practices and future opportunities to advance the state of\nresearch in search query reformulations.",
    "published_date": "2021-08-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SE",
      "cs.IR",
      "cs.LG",
      "cs.NE",
      "D.2.5; D.2.1; D.2.7; D.2.13"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.09646v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.09643v2",
    "title": "Bias for the Trace of the Resolvent and Its Application on Non-Gaussian and Non-centered MIMO Channels",
    "authors": [
      "Xin Zhang",
      "S. H. Song"
    ],
    "author_ids": [],
    "abstract": "The mutual information (MI) of Gaussian multi-input multi-output (MIMO)\nchannels has been evaluated by utilizing random matrix theory (RMT) and shown\nto asymptotically follow Gaussian distribution, where the ergodic mutual\ninformation (EMI) converges to a deterministic quantity. However, with\nnon-Gaussian channels, there is a bias between the EMI and its deterministic\nequivalent (DE), whose evaluation is not available in the literature. This bias\nof the EMI is related to the bias for the trace of the resolvent in large RMT.\nIn this paper, we first derive the bias for the trace of the resolvent, which\nis further extended to compute the bias for the linear spectral statistics\n(LSS). Then, we apply the above results on non-Gaussian MIMO channels to\ndetermine the bias for the EMI. It is also proved that the bias for the EMI is\n$-0.5$ times of that for the variance of the MI. Finally, the derived bias is\nutilized to modify the central limit theory (CLT) and calculate the outage\nprobability. Numerical results show that the modified CLT significantly\noutperforms previous methods in approximating the distribution of the MI and\nimproves the accuracy for the outage probability evaluation.",
    "published_date": "2021-08-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.09643v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.09534v2",
    "title": "Theoretical Analysis and Evaluation of NoCs with Weighted Round-Robin Arbitration",
    "authors": [
      "Sumit K. Mandal",
      "Jie Tong",
      "Raid Ayoub",
      "Michael Kishinevsky",
      "Ahmed Abousamra",
      "Umit Y. Ogras"
    ],
    "author_ids": [],
    "abstract": "Fast and accurate performance analysis techniques are essential in early\ndesign space exploration and pre-silicon evaluations, including software\neco-system development. In particular, on-chip communication continues to play\nan increasingly important role as the many-core processors scale up. This paper\npresents the first performance analysis technique that targets networks-on-chip\n(NoCs) that employ weighted round-robin (WRR) arbitration. Besides fairness,\nWRR arbitration provides flexibility in allocating bandwidth proportionally to\nthe importance of the traffic classes, unlike basic round-robin and\npriority-based arbitration. The proposed approach first estimates the effective\nservice time of the packets in the queue due to WRR arbitration. Then, it uses\nthe effective service time to compute the average waiting time of the packets.\nNext, we incorporate a decomposition technique to extend the analytical model\nto handle NoC of any size. The proposed approach achieves less than 5% error\nwhile executing real applications and 10% error under challenging synthetic\ntraffic with different burstiness levels.",
    "published_date": "2021-08-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.PF",
      "math.PR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.09534v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.09440v4",
    "title": "Unsupervised Local Discrimination for Medical Images",
    "authors": [
      "Huai Chen",
      "Renzhen Wang",
      "Xiuying Wang",
      "Jieyu Li",
      "Qu Fang",
      "Hui Li",
      "Jianhao Bai",
      "Qing Peng",
      "Deyu Meng",
      "Lisheng Wang"
    ],
    "author_ids": [],
    "abstract": "Contrastive learning, which aims to capture general representation from\nunlabeled images to initialize the medical analysis models, has been proven\neffective in alleviating the high demand for expensive annotations. Current\nmethods mainly focus on instance-wise comparisons to learn the global\ndiscriminative features, however, pretermitting the local details to\ndistinguish tiny anatomical structures, lesions, and tissues. To address this\nchallenge, in this paper, we propose a general unsupervised representation\nlearning framework, named local discrimination (LD), to learn local\ndiscriminative features for medical images by closely embedding semantically\nsimilar pixels and identifying regions of similar structures across different\nimages. Specifically, this model is equipped with an embedding module for\npixel-wise embedding and a clustering module for generating segmentation. And\nthese two modules are unified through optimizing our novel region\ndiscrimination loss function in a mutually beneficial mechanism, which enables\nour model to reflect structure information as well as measure pixel-wise and\nregion-wise similarity. Furthermore, based on LD, we propose a center-sensitive\none-shot landmark localization algorithm and a shape-guided cross-modality\nsegmentation model to foster the generalizability of our model. When\ntransferred to downstream tasks, the learned representation by our method shows\na better generalization, outperforming representation from 18 state-of-the-art\n(SOTA) methods and winning 9 out of all 12 downstream tasks. Especially for the\nchallenging lesion segmentation tasks, the proposed method achieves\nsignificantly better performances. The source codes are publicly available at\nhttps://github.com/HuaiChen-1994/LDLearning.",
    "published_date": "2021-08-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.09440v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.09435v1",
    "title": "Fairness-Aware Online Meta-learning",
    "authors": [
      "Chen Zhao",
      "Feng Chen",
      "Bhavani Thuraisingham"
    ],
    "author_ids": [],
    "abstract": "In contrast to offline working fashions, two research paradigms are devised\nfor online learning: (1) Online Meta Learning (OML) learns good priors over\nmodel parameters (or learning to learn) in a sequential setting where tasks are\nrevealed one after another. Although it provides a sub-linear regret bound,\nsuch techniques completely ignore the importance of learning with fairness\nwhich is a significant hallmark of human intelligence. (2) Online\nFairness-Aware Learning. This setting captures many classification problems for\nwhich fairness is a concern. But it aims to attain zero-shot generalization\nwithout any task-specific adaptation. This therefore limits the capability of a\nmodel to adapt onto newly arrived data. To overcome such issues and bridge the\ngap, in this paper for the first time we proposed a novel online meta-learning\nalgorithm, namely FFML, which is under the setting of unfairness prevention.\nThe key part of FFML is to learn good priors of an online fair classification\nmodel's primal and dual parameters that are associated with the model's\naccuracy and fairness, respectively. The problem is formulated in the form of a\nbi-level convex-concave optimization. Theoretic analysis provides sub-linear\nupper bounds for loss regret and for violation of cumulative fairness\nconstraints. Our experiments demonstrate the versatility of FFML by applying it\nto classification on three real-world datasets and show substantial\nimprovements over the best prior work on the tradeoff between fairness and\nclassification accuracy",
    "published_date": "2021-08-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.09435v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.09233v1",
    "title": "Detour Dual Optimal Inequalities for Column Generation with Application to Routing and Location",
    "authors": [
      "Julian Yarkony",
      "Naveed Haghani",
      "Amelia Regan"
    ],
    "author_ids": [],
    "abstract": "We consider the problem of accelerating column generation (CG) for logistics\noptimization problems using vehicle routing as an example. Without loss of\ngenerality, we focus on the Capacitated Vehicle Routing Problem (CVRP) via the\naddition of a new class of dual optimal inequalities (DOI) that incorporate\ninformation about detours from the vehicle routes. These inequalities extend\nthe Smooth-DOI recently introduced in the literature for the solution of\ncertain classes of set-covering problems by CG. The Detour-DOI introduced in\nthis article permit low cost swap operations between items on a given active\nroute with items near to other items on that route to estimate (and bound) the\nvalues of the dual variables. Smooth-DOI in contrast only permit low cost swap\noperations between nearby items. The use of Detour-DOI permits a faster\nconvergence of CG without weakening the linear programming relaxation. We then\nargue that these DOI can also be conveniently applied to single source\ncapacitated facility location problems. These problems have been shown to be\nequivalent to a broad class of logistics optimization problems that include,\nfor example telecommunication network design and production planning. The\nimportance of developing vastly more efficient column generation solvers cannot\nbe overstated. Detour-DOI, which permit large numbers of columns to be\nexpressed with a finite set of variables, contributes to this important\nendeavor.",
    "published_date": "2021-08-20T00:00:00",
    "year": 2021,
    "categories": [
      "math.OC",
      "cs.DS",
      "F.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.09233v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.09136v2",
    "title": "Unsupervised Domain-adaptive Hash for Networks",
    "authors": [
      "Tao He",
      "Lianli Gao",
      "Jingkuan Song",
      "Yuan-Fang Li"
    ],
    "author_ids": [],
    "abstract": "Abundant real-world data can be naturally represented by large-scale\nnetworks, which demands efficient and effective learning algorithms. At the\nsame time, labels may only be available for some networks, which demands these\nalgorithms to be able to adapt to unlabeled networks. Domain-adaptive hash\nlearning has enjoyed considerable success in the computer vision community in\nmany practical tasks due to its lower cost in both retrieval time and storage\nfootprint. However, it has not been applied to multiple-domain networks. In\nthis work, we bridge this gap by developing an unsupervised domain-adaptive\nhash learning method for networks, dubbed UDAH. Specifically, we develop four\n{task-specific yet correlated} components: (1) network structure preservation\nvia a hard groupwise contrastive loss, (2) relaxation-free supervised hashing,\n(3) cross-domain intersected discriminators, and (4) semantic center alignment.\nWe conduct a wide range of experiments to evaluate the effectiveness and\nefficiency of our method on a range of tasks including link prediction, node\nclassification, and neighbor recommendation. Our evaluation results demonstrate\nthat our model achieves better performance than the state-of-the-art\nconventional discrete embedding methods over all the tasks.",
    "published_date": "2021-08-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CV",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.09136v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.09098v2",
    "title": "A fuzzy-rough uncertainty measure to discover bias encoded explicitly or implicitly in features of structured pattern classification datasets",
    "authors": [
      "Gonzalo Nápoles",
      "Lisa Koutsoviti Koumeri"
    ],
    "author_ids": [],
    "abstract": "The need to measure bias encoded in tabular data that are used to solve\npattern recognition problems is widely recognized by academia, legislators and\nenterprises alike. In previous work, we proposed a bias quantification measure,\ncalled fuzzy-rough uncer-tainty, which relies on the fuzzy-rough set theory.\nThe intuition dictates that protected features should not change the\nfuzzy-rough boundary regions of a decision class significantly. The extent to\nwhich this happens is a proxy for bias expressed as uncertainty in\nadecision-making context. Our measure's main advantage is that it does not\ndepend on any machine learning prediction model but adistance function. In this\npaper, we extend our study by exploring the existence of bias encoded\nimplicitly in non-protected featuresas defined by the correlation between\nprotected and unprotected attributes. This analysis leads to four scenarios\nthat domain experts should evaluate before deciding how to tackle bias. In\naddition, we conduct a sensitivity analysis to determine the fuzzy operatorsand\ndistance function that best capture change in the boundary regions.",
    "published_date": "2021-08-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.09098v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.10697v1",
    "title": "Does Adversarial Oversampling Help us?",
    "authors": [
      "Tanmoy Dam",
      "Md Meftahul Ferdaus",
      "Sreenatha G. Anavatti",
      "Senthilnath Jayavelu",
      "Hussein A. Abbass"
    ],
    "author_ids": [],
    "abstract": "Traditional oversampling methods are generally employed to handle class\nimbalance in datasets. This oversampling approach is independent of the\nclassifier; thus, it does not offer an end-to-end solution. To overcome this,\nwe propose a three-player adversarial game-based end-to-end method, where a\ndomain-constraints mixture of generators, a discriminator, and a multi-class\nclassifier are used. Rather than adversarial minority oversampling, we propose\nan adversarial oversampling (AO) and a data-space oversampling (DO) approach.\nIn AO, the generator updates by fooling both the classifier and discriminator,\nhowever, in DO, it updates by favoring the classifier and fooling the\ndiscriminator. While updating the classifier, it considers both the real and\nsynthetically generated samples in AO. But, in DO, it favors the real samples\nand fools the subset class-specific generated samples. To mitigate the biases\nof a classifier towards the majority class, minority samples are over-sampled\nat a fractional rate. Such implementation is shown to provide more robust\nclassification boundaries. The effectiveness of our proposed method has been\nvalidated with high-dimensional, highly imbalanced and large-scale multi-class\ntabular datasets. The results as measured by average class specific accuracy\n(ACSA) clearly indicate that the proposed method provides better classification\naccuracy (improvement in the range of 0.7% to 49.27%) as compared to the\nbaseline classifier.",
    "published_date": "2021-08-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.10697v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.08995v1",
    "title": "Discriminative Domain-Invariant Adversarial Network for Deep Domain Generalization",
    "authors": [
      "Mohammad Mahfujur Rahman",
      "Clinton Fookes",
      "Sridha Sridharan"
    ],
    "author_ids": [],
    "abstract": "Domain generalization approaches aim to learn a domain invariant prediction\nmodel for unknown target domains from multiple training source domains with\ndifferent distributions. Significant efforts have recently been committed to\nbroad domain generalization, which is a challenging and topical problem in\nmachine learning and computer vision communities. Most previous domain\ngeneralization approaches assume that the conditional distribution across the\ndomains remain the same across the source domains and learn a domain invariant\nmodel by minimizing the marginal distributions. However, the assumption of a\nstable conditional distribution of the training source domains does not really\nhold in practice. The hyperplane learned from the source domains will easily\nmisclassify samples scattered at the boundary of clusters or far from their\ncorresponding class centres. To address the above two drawbacks, we propose a\ndiscriminative domain-invariant adversarial network (DDIAN) for domain\ngeneralization. The discriminativeness of the features are guaranteed through a\ndiscriminative feature module and domain-invariant features are guaranteed\nthrough the global domain and local sub-domain alignment modules. Extensive\nexperiments on several benchmarks show that DDIAN achieves better prediction on\nunseen target data during training compared to state-of-the-art domain\ngeneralization approaches.",
    "published_date": "2021-08-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.08995v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.08874v2",
    "title": "Towards A Fairer Landmark Recognition Dataset",
    "authors": [
      "Zu Kim",
      "André Araujo",
      "Bingyi Cao",
      "Cam Askew",
      "Jack Sim",
      "Mike Green",
      "N'Mah Fodiatu Yilla",
      "Tobias Weyand"
    ],
    "author_ids": [],
    "abstract": "We introduce a new landmark recognition dataset, which is created with a\nfocus on fair worldwide representation. While previous work proposes to collect\nas many images as possible from web repositories, we instead argue that such\napproaches can lead to biased data. To create a more comprehensive and\nequitable dataset, we start by defining the fair relevance of a landmark to the\nworld population. These relevances are estimated by combining anonymized Google\nMaps user contribution statistics with the contributors' demographic\ninformation. We present a stratification approach and analysis which leads to a\nmuch fairer coverage of the world, compared to existing datasets. The resulting\ndatasets are used to evaluate computer vision models as part of the the Google\nLandmark Recognition and RetrievalChallenges 2021.",
    "published_date": "2021-08-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.08874v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.08839v1",
    "title": "PoinTr: Diverse Point Cloud Completion with Geometry-Aware Transformers",
    "authors": [
      "Xumin Yu",
      "Yongming Rao",
      "Ziyi Wang",
      "Zuyan Liu",
      "Jiwen Lu",
      "Jie Zhou"
    ],
    "author_ids": [],
    "abstract": "Point clouds captured in real-world applications are often incomplete due to\nthe limited sensor resolution, single viewpoint, and occlusion. Therefore,\nrecovering the complete point clouds from partial ones becomes an indispensable\ntask in many practical applications. In this paper, we present a new method\nthat reformulates point cloud completion as a set-to-set translation problem\nand design a new model, called PoinTr that adopts a transformer encoder-decoder\narchitecture for point cloud completion. By representing the point cloud as a\nset of unordered groups of points with position embeddings, we convert the\npoint cloud to a sequence of point proxies and employ the transformers for\npoint cloud generation. To facilitate transformers to better leverage the\ninductive bias about 3D geometric structures of point clouds, we further devise\na geometry-aware block that models the local geometric relationships\nexplicitly. The migration of transformers enables our model to better learn\nstructural knowledge and preserve detailed information for point cloud\ncompletion. Furthermore, we propose two more challenging benchmarks with more\ndiverse incomplete point clouds that can better reflect the real-world\nscenarios to promote future research. Experimental results show that our method\noutperforms state-of-the-art methods by a large margin on both the new\nbenchmarks and the existing ones. Code is available at\nhttps://github.com/yuxumin/PoinTr",
    "published_date": "2021-08-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.08839v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.08818v1",
    "title": "Discriminating modelling approaches for Point in Time Economic Scenario Generation",
    "authors": [
      "Rui Wang"
    ],
    "author_ids": [],
    "abstract": "We introduce the notion of Point in Time Economic Scenario Generation (PiT\nESG) with a clear mathematical problem formulation to unify and compare\neconomic scenario generation approaches conditional on forward looking market\ndata. Such PiT ESGs should provide quicker and more flexible reactions to\nsudden economic changes than traditional ESGs calibrated solely to long periods\nof historical data. We specifically take as economic variable the S&P500 Index\nwith the VIX Index as forward looking market data to compare the nonparametric\nfiltered historical simulation, GARCH model with joint likelihood estimation\n(parametric), Restricted Boltzmann Machine and the conditional Variational\nAutoencoder (Generative Networks) for their suitability as PiT ESG. Our\nevaluation consists of statistical tests for model fit and benchmarking the out\nof sample forecasting quality with a strategy backtest using model output as\nstop loss criterion. We find that both Generative Networks outperform the\nnonparametric and classic parametric model in our tests, but that the CVAE\nseems to be particularly well suited for our purposes: yielding more robust\nperformance and being computationally lighter.",
    "published_date": "2021-08-19T00:00:00",
    "year": 2021,
    "categories": [
      "q-fin.CP",
      "cs.LG",
      "q-fin.RM",
      "q-fin.ST"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.08818v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.08800v1",
    "title": "EqGNN: Equalized Node Opportunity in Graphs",
    "authors": [
      "Uriel Singer",
      "Kira Radinsky"
    ],
    "author_ids": [],
    "abstract": "Graph neural networks (GNNs), has been widely used for supervised learning\ntasks in graphs reaching state-of-the-art results. However, little work was\ndedicated to creating unbiased GNNs, i.e., where the classification is\nuncorrelated with sensitive attributes, such as race or gender. Some ignore the\nsensitive attributes or optimize for the criteria of statistical parity for\nfairness. However, it has been shown that neither approaches ensure fairness,\nbut rather cripple the utility of the prediction task. In this work, we present\na GNN framework that allows optimizing representations for the notion of\nEqualized Odds fairness criteria. The architecture is composed of three\ncomponents: (1) a GNN classifier predicting the utility class, (2) a sampler\nlearning the distribution of the sensitive attributes of the nodes given their\nlabels. It generates samples fed into a (3) discriminator that discriminates\nbetween true and sampled sensitive attributes using a novel \"permutation loss\"\nfunction. Using these components, we train a model to neglect information\nregarding the sensitive attribute only with respect to its label. To the best\nof our knowledge, we are the first to optimize GNNs for the equalized odds\ncriteria. We evaluate our classifier over several graph datasets and sensitive\nattributes and show our algorithm reaches state-of-the-art results.",
    "published_date": "2021-08-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.08800v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.08656v1",
    "title": "Max-min Fairness Based Faucet Design for Blockchains",
    "authors": [
      "Serdar Metin",
      "Can Özturan"
    ],
    "author_ids": [],
    "abstract": "In order to have transactions executed and recorded on blockchains such as\nthe Ethereum Mainnet, fees expressed in crypto-currency units of the blockchain\nmust be paid. One can buy crypto-currency called Ether of the Ethereum\nblockchain from exchanges and pay for the transaction fees. In the case of test\nnetworks (such as Rinkeby) or scientific research blockchains (such as\nBloxberg), free crypto-currency, Ether, is distributed to users via faucets.\nSince transaction slots on the blocks, storage and smart contract executions\nare consuming blockchain resources, Ethers are distributed by fixed small\namounts to users. Users may have different amount of Ether requirements; some\nsmall amounts and some large amounts during different times. As a result,\nrather than allowing the user to get a fixed small amount of Ether, a more\ngeneral distribution mechanism that allows a user to demand and claim arbitrary\namounts of Ether, while satisfying fairness among users, is needed. For this\nend, Max-min Fairness based schemes have been used in centralized settings. Our\nwork contributes a Max-min Fairness based algorithm and its Solidity smart\ncontract implementation that requires low transaction costs independent of the\nnumber of users. This is important on the Ethereum blockchain, since a smart\ncontract execution with transaction costs depending on the number of users\nwould mean block gas limit exhaustion problem will eventually be met, making\nthe smart contract ineffective. We report tests which confirm that the low\ntransaction cost aims have been achieved by our algorithm.",
    "published_date": "2021-08-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DC",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.08656v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.08562v1",
    "title": "Concurrent Discrimination and Alignment for Self-Supervised Feature Learning",
    "authors": [
      "Anjan Dutta",
      "Massimiliano Mancini",
      "Zeynep Akata"
    ],
    "author_ids": [],
    "abstract": "Existing self-supervised learning methods learn representation by means of\npretext tasks which are either (1) discriminating that explicitly specify which\nfeatures should be separated or (2) aligning that precisely indicate which\nfeatures should be closed together, but ignore the fact how to jointly and\nprincipally define which features to be repelled and which ones to be\nattracted. In this work, we combine the positive aspects of the discriminating\nand aligning methods, and design a hybrid method that addresses the above\nissue. Our method explicitly specifies the repulsion and attraction mechanism\nrespectively by discriminative predictive task and concurrently maximizing\nmutual information between paired views sharing redundant information. We\nqualitatively and quantitatively show that our proposed model learns better\nfeatures that are more effective for the diverse downstream tasks ranging from\nclassification to semantic segmentation. Our experiments on nine established\nbenchmarks show that the proposed model consistently outperforms the existing\nstate-of-the-art results of self-supervised and transfer learning protocol.",
    "published_date": "2021-08-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.08562v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.08538v1",
    "title": "Mixture-Based Correction for Position and Trust Bias in Counterfactual Learning to Rank",
    "authors": [
      "Ali Vardasbi",
      "Maarten de Rijke",
      "Ilya Markov"
    ],
    "author_ids": [],
    "abstract": "In counterfactual learning to rank (CLTR) user interactions are used as a\nsource of supervision. Since user interactions come with bias, an important\nfocus of research in this field lies in developing methods to correct for the\nbias of interactions. Inverse propensity scoring (IPS) is a popular method\nsuitable for correcting position bias. Affine correction (AC) is a\ngeneralization of IPS that corrects for position bias and trust bias. IPS and\nAC provably remove bias, conditioned on an accurate estimation of the bias\nparameters. Estimating the bias parameters, in turn, requires an accurate\nestimation of the relevance probabilities. This cyclic dependency introduces\npractical limitations in terms of sensitivity, convergence and efficiency.\n  We propose a new correction method for position and trust bias in CLTR in\nwhich, unlike the existing methods, the correction does not rely on relevance\nestimation. Our proposed method, mixture-based correction (MBC), is based on\nthe assumption that the distribution of the CTRs over the items being ranked is\na mixture of two distributions: the distribution of CTRs for relevant items and\nthe distribution of CTRs for non-relevant items. We prove that our method is\nunbiased. The validity of our proof is not conditioned on accurate bias\nparameter estimation. Our experiments show that MBC, when used in different\nbias settings and accompanied by different LTR algorithms, outperforms AC, the\nstate-of-the-art method for correcting position and trust bias, in some\nsettings, while performing on par in other settings. Furthermore, MBC is orders\nof magnitude more efficient than AC in terms of the training time.",
    "published_date": "2021-08-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.08538v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.08504v1",
    "title": "Understanding and Mitigating Annotation Bias in Facial Expression Recognition",
    "authors": [
      "Yunliang Chen",
      "Jungseock Joo"
    ],
    "author_ids": [],
    "abstract": "The performance of a computer vision model depends on the size and quality of\nits training data. Recent studies have unveiled previously-unknown composition\nbiases in common image datasets which then lead to skewed model outputs, and\nhave proposed methods to mitigate these biases. However, most existing works\nassume that human-generated annotations can be considered gold-standard and\nunbiased. In this paper, we reveal that this assumption can be problematic, and\nthat special care should be taken to prevent models from learning such\nannotation biases. We focus on facial expression recognition and compare the\nlabel biases between lab-controlled and in-the-wild datasets. We demonstrate\nthat many expression datasets contain significant annotation biases between\ngenders, especially when it comes to the happy and angry expressions, and that\ntraditional methods cannot fully mitigate such biases in trained models. To\nremove expression annotation bias, we propose an AU-Calibrated Facial\nExpression Recognition (AUC-FER) framework that utilizes facial action units\n(AUs) and incorporates the triplet loss into the objective function.\nExperimental results suggest that the proposed method is more effective in\nremoving expression annotation bias than existing techniques.",
    "published_date": "2021-08-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.08504v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.08435v3",
    "title": "Addressing Algorithmic Disparity and Performance Inconsistency in Federated Learning",
    "authors": [
      "Sen Cui",
      "Weishen Pan",
      "Jian Liang",
      "Changshui Zhang",
      "Fei Wang"
    ],
    "author_ids": [],
    "abstract": "Federated learning (FL) has gain growing interests for its capability of\nlearning from distributed data sources collectively without the need of\naccessing the raw data samples across different sources. So far FL research has\nmostly focused on improving the performance, how the algorithmic disparity will\nbe impacted for the model learned from FL and the impact of algorithmic\ndisparity on the utility inconsistency are largely unexplored. In this paper,\nwe propose an FL framework to jointly consider performance consistency and\nalgorithmic fairness across different local clients (data sources). We derive\nour framework from a constrained multi-objective optimization perspective, in\nwhich we learn a model satisfying fairness constraints on all clients with\nconsistent performance. Specifically, we treat the algorithm prediction loss at\neach local client as an objective and maximize the worst-performing client with\nfairness constraints through optimizing a surrogate maximum function with all\nobjectives involved. A gradient-based procedure is employed to achieve the\nPareto optimality of this optimization problem. Theoretical analysis is\nprovided to prove that our method can converge to a Pareto solution that\nachieves the min-max performance with fairness constraints on all clients.\nComprehensive experiments on synthetic and real-world datasets demonstrate the\nsuperiority that our approach over baselines and its effectiveness in achieving\nboth fairness and consistency across all local clients.",
    "published_date": "2021-08-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.08435v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.08407v1",
    "title": "Show or Tell? Visual and Verbal Representations Bias Position Recall",
    "authors": [
      "Cristina R. Ceja",
      "Cindy Xiong"
    ],
    "author_ids": [],
    "abstract": "When we view visualizations, we not only have a visual representation of the\ndata, but also a verbal one. Recent work has shown that these visual\nrepresentations of data can be biased, such that the position of a line in a\nchart will be consistently underestimated. But are the verbal representations\nof position encodings also biased in the same manner, or is this a purely\nvisual bias that can be mitigated with verbal context? We explored the bias in\nposition reproductions for simple uniform lines for both visual and verbal\nrepresentations. We find that the direction of the bias changed depending on\nthe response modality, with visual reproductions showing a position\nunderestimation while verbal responses showed overestimation. This finding\nindicates that, even for simple line charts, biases are still present for both\nvisual and verbal representations, although the directionality of this bias\ndepends on the modality.",
    "published_date": "2021-08-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.08407v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.08233v2",
    "title": "Research on Gender-related Fingerprint Features",
    "authors": [
      "Yong Qi",
      "Yanping Li",
      "Huawei Lin",
      "Jiashu Chen",
      "Huaiguang Lei"
    ],
    "author_ids": [],
    "abstract": "Fingerprint is an important biological feature of human body, which contains\nabundant gender information. At present, the academic research of fingerprint\ngender characteristics is generally at the level of understanding, while the\nstandardization research is quite limited. In this work, we propose a more\nrobust method, Dense Dilated Convolution ResNet (DDC-ResNet) to extract valid\ngender information from fingerprints. By replacing the normal convolution\noperations with the atrous convolution in the backbone, prior knowledge is\nprovided to keep the edge details and the global reception field can be\nextended. We explored the results in 3 ways: 1) The efficiency of the\nDDC-ResNet. 6 typical methods of automatic feature extraction coupling with 9\nmainstream classifiers are evaluated in our dataset with fair implementation\ndetails. Experimental results demonstrate that the combination of our approach\noutperforms other combinations in terms of average accuracy and separate-gender\naccuracy. It reaches 96.5% for average and 0.9752 (males)/0.9548 (females) for\nseparate-gender accuracy. 2) The effect of fingers. It is found that the best\nperformance of classifying gender with separate fingers is achieved by the\nright ring finger. 3) The effect of specific features. Based on the\nobservations of the concentrations of fingerprints visualized by our approach,\nit can be inferred that loops and whorls (level 1), bifurcations (level 2), as\nwell as line shapes (level 3) are connected with gender. Finally, we will open\nsource the dataset that contains 6000 fingerprint images",
    "published_date": "2021-08-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.08233v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.08229v1",
    "title": "Tilted Platforms: Rental Housing Technology and the Rise of Urban Big Data Oligopolies",
    "authors": [
      "Geoff Boeing",
      "Max Besbris",
      "David Wachsmuth",
      "Jake Wegmann"
    ],
    "author_ids": [],
    "abstract": "This article interprets emerging scholarship on rental housing platforms --\nparticularly the most well-known and used short- and long-term rental housing\nplatforms - and considers how the technological processes connecting both\nshort-term and long-term rentals to the platform economy are transforming\ncities. It discusses potential policy approaches to more equitably distribute\nbenefits and mitigate harms. We argue that information technology is not\nvalue-neutral. While rental housing platforms may empower data analysts and\ncertain market participants, the same cannot be said for all users or society\nat large. First, user-generated online data frequently reproduce the systematic\nbiases found in traditional sources of housing information. Evidence is growing\nthat the information broadcasting potential of rental housing platforms may\nincrease rather than mitigate sociospatial inequality. Second, technology\nplatforms curate and shape information according to their creators' own\nfinancial and political interests. The question of which data -- and people --\nare hidden or marginalized on these platforms is just as important as the\nquestion of which data are available. Finally, important differences in\nbenefits and drawbacks exist between short-term and long-term rental housing\nplatforms, but are underexplored in the literature: this article unpacks these\ndifferences and proposes policy recommendations.",
    "published_date": "2021-08-18T00:00:00",
    "year": 2021,
    "categories": [
      "econ.GN",
      "cs.CY",
      "q-fin.EC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.08229v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.13475v1",
    "title": "An Analysis Of Entire Space Multi-Task Models For Post-Click Conversion Prediction",
    "authors": [
      "Conor O'Brien",
      "Kin Sum Liu",
      "James Neufeld",
      "Rafael Barreto",
      "Jonathan J Hunt"
    ],
    "author_ids": [],
    "abstract": "Industrial recommender systems are frequently tasked with approximating\nprobabilities for multiple, often closely related, user actions. For example,\npredicting if a user will click on an advertisement and if they will then\npurchase the advertised product. The conceptual similarity between these tasks\nhas promoted the use of multi-task learning: a class of algorithms that aim to\nbring positive inductive transfer from related tasks. Here, we empirically\nevaluate multi-task learning approaches with neural networks for an online\nadvertising task. Specifically, we consider approximating the probability of\npost-click conversion events (installs) (CVR) for mobile app advertising on a\nlarge-scale advertising platform, using the related click events (CTR) as an\nauxiliary task. We use an ablation approach to systematically study recent\napproaches that incorporate both multitask learning and \"entire space modeling\"\nwhich train the CVR on all logged examples rather than learning a conditional\nlikelihood of conversion given clicked. Based on these results we show that\nseveral different approaches result in similar levels of positive transfer from\nthe data-abundant CTR task to the CVR task and offer some insight into how the\nmulti-task design choices address the two primary problems affecting the CVR\ntask: data sparsity and data bias. Our findings add to the growing body of\nevidence suggesting that standard multi-task learning is a sensible approach to\nmodelling related events in real-world large-scale applications and suggest the\nspecific multitask approach can be guided by ease of implementation in an\nexisting system.",
    "published_date": "2021-08-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.13475v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.07996v1",
    "title": "VerSaChI: Finding Statistically Significant Subgraph Matches using Chebyshev's Inequality",
    "authors": [
      "Shubhangi Agarwal",
      "Sourav Dutta",
      "Arnab Bhattacharya"
    ],
    "author_ids": [],
    "abstract": "Approximate subgraph matching, which is an important primitive for many\napplications like question answering, community detection, and motif discovery,\noften involves large labeled graphs such as knowledge graphs, social networks,\nand protein sequences. Effective methods for extracting matching subgraphs, in\nterms of label and structural similarities to a query, should depict accuracy,\ncomputational efficiency, and robustness to noise. In this paper, we propose\nVerSaChI for finding the top-k most similar subgraphs based on 2-hop label and\nstructural overlap similarity with the query. The similarity is characterized\nusing Chebyshev's inequality to compute the chi-square statistical significance\nfor measuring the degree of matching of the subgraphs. Experiments on real-life\ngraph datasets showcase significant improvements in terms of accuracy compared\nto state-of-the-art methods, as well as robustness to noise.",
    "published_date": "2021-08-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DB"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.07996v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.07954v1",
    "title": "Self-Supervised Visual Representations Learning by Contrastive Mask Prediction",
    "authors": [
      "Yucheng Zhao",
      "Guangting Wang",
      "Chong Luo",
      "Wenjun Zeng",
      "Zheng-Jun Zha"
    ],
    "author_ids": [],
    "abstract": "Advanced self-supervised visual representation learning methods rely on the\ninstance discrimination (ID) pretext task. We point out that the ID task has an\nimplicit semantic consistency (SC) assumption, which may not hold in\nunconstrained datasets. In this paper, we propose a novel contrastive mask\nprediction (CMP) task for visual representation learning and design a mask\ncontrast (MaskCo) framework to implement the idea. MaskCo contrasts\nregion-level features instead of view-level features, which makes it possible\nto identify the positive sample without any assumptions. To solve the domain\ngap between masked and unmasked features, we design a dedicated mask prediction\nhead in MaskCo. This module is shown to be the key to the success of the CMP.\nWe evaluated MaskCo on training datasets beyond ImageNet and compare its\nperformance with MoCo V2. Results show that MaskCo achieves comparable\nperformance with MoCo V2 using ImageNet training dataset, but demonstrates a\nstronger performance across a range of downstream tasks when COCO or Conceptual\nCaptions are used for training. MaskCo provides a promising alternative to the\nID-based methods for self-supervised learning in the wild.",
    "published_date": "2021-08-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.07954v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.07898v3",
    "title": "Searching For or Reviewing Evidence Improves Crowdworkers' Misinformation Judgments and Reduces Partisan Bias",
    "authors": [
      "Paul Resnick",
      "Aljohara Alfayez",
      "Jane Im",
      "Eric Gilbert"
    ],
    "author_ids": [],
    "abstract": "Can crowd workers be trusted to judge whether news-like articles circulating\non the Internet are misleading, or does partisanship and inexperience get in\nthe way? And can the task be structured in a way that reduces partisanship? We\nassembled pools of both liberal and conservative crowd raters and tested three\nways of asking them to make judgments about 374 articles. In a no research\ncondition, they were just asked to view the article and then render a judgment.\nIn an individual research condition, they were also asked to search for\ncorroborating evidence and provide a link to the best evidence they found. In a\ncollective research condition, they were not asked to search, but instead to\nreview links collected from workers in the individual research condition. Both\nresearch conditions reduced partisan disagreement in judgments. The individual\nresearch condition was most effective at producing alignment with journalists'\nassessments. In this condition, the judgments of a panel of sixteen or more\ncrowd workers were better than that of a panel of three expert journalists, as\nmeasured by alignment with a held out journalist's ratings.",
    "published_date": "2021-08-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.07898v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.07865v1",
    "title": "Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE 2021): Workshop and Shared Task Report",
    "authors": [
      "Ali Hürriyetoğlu",
      "Hristo Tanev",
      "Vanni Zavarella",
      "Jakub Piskorski",
      "Reyyan Yeniterzi",
      "Erdem Yörük"
    ],
    "author_ids": [],
    "abstract": "This workshop is the fourth issue of a series of workshops on automatic\nextraction of socio-political events from news, organized by the Emerging\nMarket Welfare Project, with the support of the Joint Research Centre of the\nEuropean Commission and with contributions from many other prominent scholars\nin this field. The purpose of this series of workshops is to foster research\nand development of reliable, valid, robust, and practical solutions for\nautomatically detecting descriptions of socio-political events, such as\nprotests, riots, wars and armed conflicts, in text streams. This year workshop\ncontributors make use of the state-of-the-art NLP technologies, such as Deep\nLearning, Word Embeddings and Transformers and cover a wide range of topics\nfrom text classification to news bias detection. Around 40 teams have\nregistered and 15 teams contributed to three tasks that are i) multilingual\nprotest news detection, ii) fine-grained classification of socio-political\nevents, and iii) discovering Black Lives Matter protest events. The workshop\nalso highlights two keynote and four invited talks about various aspects of\ncreating event data sets and multi- and cross-lingual machine learning in few-\nand zero-shot settings.",
    "published_date": "2021-08-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.07865v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.07779v1",
    "title": "Appearance Based Deep Domain Adaptation for the Classification of Aerial Images",
    "authors": [
      "Dennis Wittich",
      "Franz Rottensteiner"
    ],
    "author_ids": [],
    "abstract": "This paper addresses domain adaptation for the pixel-wise classification of\nremotely sensed data using deep neural networks (DNN) as a strategy to reduce\nthe requirements of DNN with respect to the availability of training data. We\nfocus on the setting in which labelled data are only available in a source\ndomain DS, but not in a target domain DT. Our method is based on adversarial\ntraining of an appearance adaptation network (AAN) that transforms images from\nDS such that they look like images from DT. Together with the original label\nmaps from DS, the transformed images are used to adapt a DNN to DT. We propose\na joint training strategy of the AAN and the classifier, which constrains the\nAAN to transform the images such that they are correctly classified. In this\nway, objects of a certain class are changed such that they resemble objects of\nthe same class in DT. To further improve the adaptation performance, we propose\na new regularization loss for the discriminator network used in domain\nadversarial training. We also address the problem of finding the optimal values\nof the trained network parameters, proposing an unsupervised entropy based\nparameter selection criterion which compensates for the fact that there is no\nvalidation set in DT that could be monitored. As a minor contribution, we\npresent a new weighting strategy for the cross-entropy loss, addressing the\nproblem of imbalanced class distributions. Our method is evaluated in 42\nadaptation scenarios using datasets from 7 cities, all consisting of\nhigh-resolution digital orthophotos and height data. It achieves a positive\ntransfer in all cases, and on average it improves the performance in the target\ndomain by 4.3% in overall accuracy. In adaptation scenarios between datasets\nfrom the ISPRS semantic labelling benchmark our method outperforms those from\nrecent publications by 10-20% with respect to the mean intersection over union.",
    "published_date": "2021-08-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.07779v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.07593v1",
    "title": "MigrationsKB: A Knowledge Base of Public Attitudes towards Migrations and their Driving Factors",
    "authors": [
      "Yiyi Chen",
      "Harald Sack",
      "Mehwish Alam"
    ],
    "author_ids": [],
    "abstract": "With the increasing trend in the topic of migration in Europe, the public is\nnow more engaged in expressing their opinions through various platforms such as\nTwitter. Understanding the online discourses is therefore essential to capture\nthe public opinion. The goal of this study is the analysis of social media\nplatform to quantify public attitudes towards migrations and the identification\nof different factors causing these attitudes. The tweets spanning from 2013 to\nJul-2021 in the European countries which are hosts to immigrants are collected,\npre-processed, and filtered using advanced topic modeling technique. BERT-based\nentity linking and sentiment analysis, and attention-based hate speech\ndetection are performed to annotate the curated tweets. Moreover, the external\ndatabases are used to identify the potential social and economic factors\ncausing negative attitudes of the people about migration. To further promote\nresearch in the interdisciplinary fields of social science and computer\nscience, the outcomes are integrated into a Knowledge Base (KB), i.e.,\nMigrationsKB which significantly extends the existing models to take into\naccount the public attitudes towards migrations and the economic indicators.\nThis KB is made public using FAIR principles, which can be queried through\nSPARQL endpoint. Data dumps are made available on Zenodo.",
    "published_date": "2021-08-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI",
      "68T50, 68T07"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.07593v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.07582v1",
    "title": "Self-Supervised Pretraining and Controlled Augmentation Improve Rare Wildlife Recognition in UAV Images",
    "authors": [
      "Xiaochen Zheng",
      "Benjamin Kellenberger",
      "Rui Gong",
      "Irena Hajnsek",
      "Devis Tuia"
    ],
    "author_ids": [],
    "abstract": "Automated animal censuses with aerial imagery are a vital ingredient towards\nwildlife conservation. Recent models are generally based on deep learning and\nthus require vast amounts of training data. Due to their scarcity and minuscule\nsize, annotating animals in aerial imagery is a highly tedious process. In this\nproject, we present a methodology to reduce the amount of required training\ndata by resorting to self-supervised pretraining. In detail, we examine a\ncombination of recent contrastive learning methodologies like Momentum Contrast\n(MoCo) and Cross-Level Instance-Group Discrimination (CLD) to condition our\nmodel on the aerial images without the requirement for labels. We show that a\ncombination of MoCo, CLD, and geometric augmentations outperforms conventional\nmodels pre-trained on ImageNet by a large margin. Crucially, our method still\nyields favorable results even if we reduce the number of training animals to\njust 10%, at which point our best model scores double the recall of the\nbaseline at similar precision. This effectively allows reducing the number of\nrequired annotations to a fraction while still being able to train\nhigh-accuracy models in such highly challenging settings.",
    "published_date": "2021-08-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.07582v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.07437v1",
    "title": "Social influence leads to the formation of diverse local trends",
    "authors": [
      "Ziv Epstein",
      "Matthew Groh",
      "Abhimanyu Dubey",
      "Alex \"Sandy\" Pentland"
    ],
    "author_ids": [],
    "abstract": "How does the visual design of digital platforms impact user behavior and the\nresulting environment? A body of work suggests that introducing social signals\nto content can increase both the inequality and unpredictability of its\nsuccess, but has only been shown in the context of music listening. To further\nexamine the effect of social influence on media popularity, we extend this\nresearch to the context of algorithmically-generated images by re-adapting\nSalganik et al's Music Lab experiment. On a digital platform where participants\ndiscover and curate AI-generated hybrid animals, we randomly assign both the\nknowledge of other participants' behavior and the visual presentation of the\ninformation. We successfully replicate the Music Lab's findings in the context\nof images, whereby social influence leads to an unpredictable winner-take-all\nmarket. However, we also find that social influence can lead to the emergence\nof local cultural trends that diverge from the status quo and are ultimately\nmore diverse. We discuss the implications of these results for platform\ndesigners and animal conservation efforts.",
    "published_date": "2021-08-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC",
      "cs.AI",
      "J.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.07437v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.07403v2",
    "title": "FARF: A Fair and Adaptive Random Forests Classifier",
    "authors": [
      "Wenbin Zhang",
      "Albert Bifet",
      "Xiangliang Zhang",
      "Jeremy C. Weiss",
      "Wolfgang Nejdl"
    ],
    "author_ids": [],
    "abstract": "As Artificial Intelligence (AI) is used in more applications, the need to\nconsider and mitigate biases from the learned models has followed. Most works\nin developing fair learning algorithms focus on the offline setting. However,\nin many real-world applications data comes in an online fashion and needs to be\nprocessed on the fly. Moreover, in practical application, there is a trade-off\nbetween accuracy and fairness that needs to be accounted for, but current\nmethods often have multiple hyperparameters with non-trivial interaction to\nachieve fairness. In this paper, we propose a flexible ensemble algorithm for\nfair decision-making in the more challenging context of evolving online\nsettings. This algorithm, called FARF (Fair and Adaptive Random Forests), is\nbased on using online component classifiers and updating them according to the\ncurrent distribution, that also accounts for fairness and a single\nhyperparameters that alters fairness-accuracy balance. Experiments on\nreal-world discriminated data streams demonstrate the utility of FARF.",
    "published_date": "2021-08-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.07403v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.07258v3",
    "title": "On the Opportunities and Risks of Foundation Models",
    "authors": [
      "Rishi Bommasani",
      "Drew A. Hudson",
      "Ehsan Adeli",
      "Russ Altman",
      "Simran Arora",
      "Sydney von Arx",
      "Michael S. Bernstein",
      "Jeannette Bohg",
      "Antoine Bosselut",
      "Emma Brunskill",
      "Erik Brynjolfsson",
      "Shyamal Buch",
      "Dallas Card",
      "Rodrigo Castellon",
      "Niladri Chatterji",
      "Annie Chen",
      "Kathleen Creel",
      "Jared Quincy Davis",
      "Dora Demszky",
      "Chris Donahue",
      "Moussa Doumbouya",
      "Esin Durmus",
      "Stefano Ermon",
      "John Etchemendy",
      "Kawin Ethayarajh",
      "Li Fei-Fei",
      "Chelsea Finn",
      "Trevor Gale",
      "Lauren Gillespie",
      "Karan Goel",
      "Noah Goodman",
      "Shelby Grossman",
      "Neel Guha",
      "Tatsunori Hashimoto",
      "Peter Henderson",
      "John Hewitt",
      "Daniel E. Ho",
      "Jenny Hong",
      "Kyle Hsu",
      "Jing Huang",
      "Thomas Icard",
      "Saahil Jain",
      "Dan Jurafsky",
      "Pratyusha Kalluri",
      "Siddharth Karamcheti",
      "Geoff Keeling",
      "Fereshte Khani",
      "Omar Khattab",
      "Pang Wei Koh",
      "Mark Krass",
      "Ranjay Krishna",
      "Rohith Kuditipudi",
      "Ananya Kumar",
      "Faisal Ladhak",
      "Mina Lee",
      "Tony Lee",
      "Jure Leskovec",
      "Isabelle Levent",
      "Xiang Lisa Li",
      "Xuechen Li",
      "Tengyu Ma",
      "Ali Malik",
      "Christopher D. Manning",
      "Suvir Mirchandani",
      "Eric Mitchell",
      "Zanele Munyikwa",
      "Suraj Nair",
      "Avanika Narayan",
      "Deepak Narayanan",
      "Ben Newman",
      "Allen Nie",
      "Juan Carlos Niebles",
      "Hamed Nilforoshan",
      "Julian Nyarko",
      "Giray Ogut",
      "Laurel Orr",
      "Isabel Papadimitriou",
      "Joon Sung Park",
      "Chris Piech",
      "Eva Portelance",
      "Christopher Potts",
      "Aditi Raghunathan",
      "Rob Reich",
      "Hongyu Ren",
      "Frieda Rong",
      "Yusuf Roohani",
      "Camilo Ruiz",
      "Jack Ryan",
      "Christopher Ré",
      "Dorsa Sadigh",
      "Shiori Sagawa",
      "Keshav Santhanam",
      "Andy Shih",
      "Krishnan Srinivasan",
      "Alex Tamkin",
      "Rohan Taori",
      "Armin W. Thomas",
      "Florian Tramèr",
      "Rose E. Wang",
      "William Wang",
      "Bohan Wu",
      "Jiajun Wu",
      "Yuhuai Wu",
      "Sang Michael Xie",
      "Michihiro Yasunaga",
      "Jiaxuan You",
      "Matei Zaharia",
      "Michael Zhang",
      "Tianyi Zhang",
      "Xikun Zhang",
      "Yuhui Zhang",
      "Lucia Zheng",
      "Kaitlyn Zhou",
      "Percy Liang"
    ],
    "author_ids": [],
    "abstract": "AI is undergoing a paradigm shift with the rise of models (e.g., BERT,\nDALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a\nwide range of downstream tasks. We call these models foundation models to\nunderscore their critically central yet incomplete character. This report\nprovides a thorough account of the opportunities and risks of foundation\nmodels, ranging from their capabilities (e.g., language, vision, robotics,\nreasoning, human interaction) and technical principles(e.g., model\narchitectures, training procedures, data, systems, security, evaluation,\ntheory) to their applications (e.g., law, healthcare, education) and societal\nimpact (e.g., inequity, misuse, economic and environmental impact, legal and\nethical considerations). Though foundation models are based on standard deep\nlearning and transfer learning, their scale results in new emergent\ncapabilities,and their effectiveness across so many tasks incentivizes\nhomogenization. Homogenization provides powerful leverage but demands caution,\nas the defects of the foundation model are inherited by all the adapted models\ndownstream. Despite the impending widespread deployment of foundation models,\nwe currently lack a clear understanding of how they work, when they fail, and\nwhat they are even capable of due to their emergent properties. To tackle these\nquestions, we believe much of the critical research on foundation models will\nrequire deep interdisciplinary collaboration commensurate with their\nfundamentally sociotechnical nature.",
    "published_date": "2021-08-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.07258v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.06973v1",
    "title": "Analyzing Item Popularity Bias of Music Recommender Systems: Are Different Genders Equally Affected?",
    "authors": [
      "Oleg Lesota",
      "Alessandro B. Melchiorre",
      "Navid Rekabsaz",
      "Stefan Brandl",
      "Dominik Kowald",
      "Elisabeth Lex",
      "Markus Schedl"
    ],
    "author_ids": [],
    "abstract": "Several studies have identified discrepancies between the popularity of items\nin user profiles and the corresponding recommendation lists. Such behavior,\nwhich concerns a variety of recommendation algorithms, is referred to as\npopularity bias. Existing work predominantly adopts simple statistical\nmeasures, such as the difference of mean or median popularity, to quantify\npopularity bias. Moreover, it does so irrespective of user characteristics\nother than the inclination to popular content. In this work, in contrast, we\npropose to investigate popularity differences (between the user profile and\nrecommendation list) in terms of median, a variety of statistical moments, as\nwell as similarity measures that consider the entire popularity distributions\n(Kullback-Leibler divergence and Kendall's tau rank-order correlation). This\nresults in a more detailed picture of the characteristics of popularity bias.\nFurthermore, we investigate whether such algorithmic popularity bias affects\nusers of different genders in the same way. We focus on music recommendation\nand conduct experiments on the recently released standardized LFM-2b dataset,\ncontaining listening profiles of Last.fm users. We investigate the algorithmic\npopularity bias of seven common recommendation algorithms (five collaborative\nfiltering and two baselines). Our experiments show that (1) the studied metrics\nprovide novel insights into popularity bias in comparison with only using\naverage differences, (2) algorithms less inclined towards popularity bias\namplification do not necessarily perform worse in terms of utility (NDCG), (3)\nthe majority of the investigated recommenders intensify the popularity bias of\nthe female users.",
    "published_date": "2021-08-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.06973v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.06918v1",
    "title": "Legal perspective on possible fairness measures - A legal discussion using the example of hiring decisions (preprint)",
    "authors": [
      "Marc P Hauer",
      "Johannes Kevekordes",
      "Maryam Amir Haeri"
    ],
    "author_ids": [],
    "abstract": "With the increasing use of AI in algorithmic decision making (e.g. based on\nneural networks), the question arises how bias can be excluded or mitigated.\nThere are some promising approaches, but many of them are based on a \"fair\"\nground truth, others are based on a subjective goal to be reached, which leads\nto the usual problem of how to define and compute \"fairness\". The different\nfunctioning of algorithmic decision making in contrast to human decision making\nleads to a shift from a process-oriented to a result-oriented discrimination\nassessment. We argue that with such a shift society needs to determine which\nkind of fairness is the right one to choose for which certain scenario. To\nunderstand the implications of such a determination we explain the different\nkinds of fairness concepts that might be applicable for the specific\napplication of hiring decisions, analyze their pros and cons with regard to the\nrespective fairness interpretation and evaluate them from a legal perspective\n(based on EU law).",
    "published_date": "2021-08-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.06918v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.06830v1",
    "title": "'Walking Into a Fire Hoping You Don't Catch': Strategies and Designs to Facilitate Cross-Partisan Online Discussions",
    "authors": [
      "Ashwin Rajadesingan",
      "Carolyn Duran",
      "Paul Resnick",
      "Ceren Budak"
    ],
    "author_ids": [],
    "abstract": "While cross-partisan conversations are central to a vibrant democracy, these\nare hard conversations to have, especially in the United States amidst\nunprecedented levels of partisan animosity. Such interactions often devolve\ninto name-calling and personal attacks. We report on a qualitative study of 17\nUS residents who have engaged with outpartisans on Reddit, to understand their\nexpectations and the strategies they adopt in such interactions. We find that\nusers have multiple, sometimes contradictory expectations of these\nconversations, ranging from deliberative discussions to entertainment and\nbanter, which adds to the challenge of finding conversations they like. Through\nexperience, users have refined multiple strategies to foster good\ncross-partisan engagement. Contrary to offline settings where knowing about the\ninterlocutor can help manage disagreements, on Reddit, some users look to\nactively learn as little as possible about their outpartisan interlocutors for\nfear that such information may bias their interactions. Through design probes\nabout hypothetical features intended to reduce partisan hostility, we find that\nusers are actually open to knowing certain kinds of information about their\ninterlocutors, such as non-political subreddits that they both participate in,\nand to having that information made visible to their interlocutors. However,\nmaking other information visible, such as the other subreddits that they\nparticipate in or previous comments they posted, though potentially humanizing,\nraises concerns around privacy and misuse of that information for personal\nattacks.",
    "published_date": "2021-08-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.06830v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.06702v1",
    "title": "Deepfake Representation with Multilinear Regression",
    "authors": [
      "Sara Abdali",
      "M. Alex O. Vasilescu",
      "Evangelos E. Papalexakis"
    ],
    "author_ids": [],
    "abstract": "Generative neural network architectures such as GANs, may be used to generate\nsynthetic instances to compensate for the lack of real data. However, they may\nbe employed to create media that may cause social, political or economical\nupheaval. One emerging media is \"Deepfake\".Techniques that can discriminate\nbetween such media is indispensable. In this paper, we propose a modified\nmultilinear (tensor) method, a combination of linear and multilinear\nregressions for representing fake and real data. We test our approach by\nrepresenting Deepfakes with our modified multilinear (tensor) approach and\nperform SVM classification with encouraging results.",
    "published_date": "2021-08-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.06702v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.06624v1",
    "title": "Equity-Directed Bootstrapping: Examples and Analysis",
    "authors": [
      "Harish S. Bhat",
      "Majerle E. Reeves",
      "Sidra Goldman-Mellor"
    ],
    "author_ids": [],
    "abstract": "When faced with severely imbalanced binary classification problems, we often\ntrain models on bootstrapped data in which the number of instances of each\nclass occur in a more favorable ratio, e.g., one. We view algorithmic inequity\nthrough the lens of imbalanced classification: in order to balance the\nperformance of a classifier across groups, we can bootstrap to achieve training\nsets that are balanced with respect to both labels and group identity. For an\nexample problem with severe class imbalance---prediction of suicide death from\nadministrative patient records---we illustrate how an equity-directed bootstrap\ncan bring test set sensitivities and specificities much closer to satisfying\nthe equal odds criterion. In the context of na\\\"ive Bayes and logistic\nregression, we analyze the equity-directed bootstrap, demonstrating that it\nworks by bringing odds ratios close to one, and linking it to methods involving\nintercept adjustment, thresholding, and weighting.",
    "published_date": "2021-08-14T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.CO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.06624v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.06603v1",
    "title": "Algorithmic correspondence for relevance logics, bunched implication logics, and relation algebras: the algorithm PEARL and its implementation (Technical Report)",
    "authors": [
      "Willem Conradie",
      "Valntin Goranko",
      "Peter Jipsen"
    ],
    "author_ids": [],
    "abstract": "The non-deterministic algorithmic procedure PEARL (an acronym for\n`Propositional variables Elimination Algorithm for Relevance Logic') has been\nrecently developed for computing first-order equivalents of formulas of the\nlanguage of relevance logics RL in terms of the standard Routley-Meyer\nrelational semantics. It succeeds on a large class of axioms of relevance\nlogics, including all so-called inductive formulas. In the present work we\nre-interpret PEARL from an algebraic perspective, with its rewrite rules seen\nas manipulating quasi-inequalities interpreted over Urquhart's relevant\nalgebras, and report on its recent Python implementation. We also show that all\nformulae on which PEARL succeeds are canonical, i.e., preserved under canonical\nextensions of relevant algebras. This generalizes the \"canonicity via\ncorrespondence\" result in Urquhart's 1996 paper. We also indicate that, with\nminor modifications, PEARL can be applied to bunched implication algebras and\nrelation algebras.",
    "published_date": "2021-08-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LO",
      "math.LO",
      "03B47",
      "F.4.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.06603v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.10132v2",
    "title": "TRAPDOOR: Repurposing backdoors to detect dataset bias in machine learning-based genomic analysis",
    "authors": [
      "Esha Sarkar",
      "Michail Maniatakos"
    ],
    "author_ids": [],
    "abstract": "Machine Learning (ML) has achieved unprecedented performance in several\napplications including image, speech, text, and data analysis. Use of ML to\nunderstand underlying patterns in gene mutations (genomics) has far-reaching\nresults, not only in overcoming diagnostic pitfalls, but also in designing\ntreatments for life-threatening diseases like cancer. Success and\nsustainability of ML algorithms depends on the quality and diversity of data\ncollected and used for training. Under-representation of groups (ethnic groups,\ngender groups, etc.) in such a dataset can lead to inaccurate predictions for\ncertain groups, which can further exacerbate systemic discrimination issues.\n  In this work, we propose TRAPDOOR, a methodology for identification of biased\ndatasets by repurposing a technique that has been mostly proposed for nefarious\npurposes: Neural network backdoors. We consider a typical collaborative\nlearning setting of the genomics supply chain, where data may come from\nhospitals, collaborative projects, or research institutes to a central cloud\nwithout awareness of bias against a sensitive group. In this context, we\ndevelop a methodology to leak potential bias information of the collective data\nwithout hampering the genuine performance using ML backdooring catered for\ngenomic applications. Using a real-world cancer dataset, we analyze the dataset\nwith the bias that already existed towards white individuals and also\nintroduced biases in datasets artificially, and our experimental result show\nthat TRAPDOOR can detect the presence of dataset bias with 100% accuracy, and\nfurthermore can also extract the extent of bias by recovering the percentage\nwith a small error.",
    "published_date": "2021-08-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.10132v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.06581v1",
    "title": "Unravelling the Effect of Image Distortions for Biased Prediction of Pre-trained Face Recognition Models",
    "authors": [
      "Puspita Majumdar",
      "Surbhi Mittal",
      "Richa Singh",
      "Mayank Vatsa"
    ],
    "author_ids": [],
    "abstract": "Identifying and mitigating bias in deep learning algorithms has gained\nsignificant popularity in the past few years due to its impact on the society.\nResearchers argue that models trained on balanced datasets with good\nrepresentation provide equal and unbiased performance across subgroups.\nHowever, \\textit{can seemingly unbiased pre-trained model become biased when\ninput data undergoes certain distortions?} For the first time, we attempt to\nanswer this question in the context of face recognition. We provide a\nsystematic analysis to evaluate the performance of four state-of-the-art deep\nface recognition models in the presence of image distortions across different\n\\textit{gender} and \\textit{race} subgroups. We have observed that image\ndistortions have a relationship with the performance gap of the model across\ndifferent subgroups.",
    "published_date": "2021-08-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.06581v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.06536v1",
    "title": "Exploiting a Joint Embedding Space for Generalized Zero-Shot Semantic Segmentation",
    "authors": [
      "Donghyeon Baek",
      "Youngmin Oh",
      "Bumsub Ham"
    ],
    "author_ids": [],
    "abstract": "We address the problem of generalized zero-shot semantic segmentation (GZS3)\npredicting pixel-wise semantic labels for seen and unseen classes. Most GZS3\nmethods adopt a generative approach that synthesizes visual features of unseen\nclasses from corresponding semantic ones (e.g., word2vec) to train novel\nclassifiers for both seen and unseen classes. Although generative methods show\ndecent performance, they have two limitations: (1) the visual features are\nbiased towards seen classes; (2) the classifier should be retrained whenever\nnovel unseen classes appear. We propose a discriminative approach to address\nthese limitations in a unified framework. To this end, we leverage visual and\nsemantic encoders to learn a joint embedding space, where the semantic encoder\ntransforms semantic features to semantic prototypes that act as centers for\nvisual features of corresponding classes. Specifically, we introduce\nboundary-aware regression (BAR) and semantic consistency (SC) losses to learn\ndiscriminative features. Our approach to exploiting the joint embedding space,\ntogether with BAR and SC terms, alleviates the seen bias problem. At test time,\nwe avoid the retraining process by exploiting semantic prototypes as a\nnearest-neighbor (NN) classifier. To further alleviate the bias problem, we\nalso propose an inference technique, dubbed Apollonius calibration (AC), that\nmodulates the decision boundary of the NN classifier to the Apollonius circle\nadaptively. Experimental results demonstrate the effectiveness of our\nframework, achieving a new state of the art on standard benchmarks.",
    "published_date": "2021-08-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.06536v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.06503v2",
    "title": "Packaging research artefacts with RO-Crate",
    "authors": [
      "Stian Soiland-Reyes",
      "Peter Sefton",
      "Mercè Crosas",
      "Leyla Jael Castro",
      "Frederik Coppens",
      "José M. Fernández",
      "Daniel Garijo",
      "Björn Grüning",
      "Marco La Rosa",
      "Simone Leo",
      "Eoghan Ó Carragáin",
      "Marc Portier",
      "Ana Trisovic",
      "RO-Crate Community",
      "Paul Groth",
      "Carole Goble"
    ],
    "author_ids": [],
    "abstract": "An increasing number of researchers support reproducibility by including\npointers to and descriptions of datasets, software and methods in their\npublications. However, scientific articles may be ambiguous, incomplete and\ndifficult to process by automated systems. In this paper we introduce RO-Crate,\nan open, community-driven, and lightweight approach to packaging research\nartefacts along with their metadata in a machine readable manner. RO-Crate is\nbased on Schema$.$org annotations in JSON-LD, aiming to establish best\npractices to formally describe metadata in an accessible and practical way for\ntheir use in a wide variety of situations.\n  An RO-Crate is a structured archive of all the items that contributed to a\nresearch outcome, including their identifiers, provenance, relations and\nannotations. As a general purpose packaging approach for data and their\nmetadata, RO-Crate is used across multiple areas, including bioinformatics,\ndigital humanities and regulatory sciences. By applying \"just enough\" Linked\nData standards, RO-Crate simplifies the process of making research outputs FAIR\nwhile also enhancing research reproducibility.\n  An RO-Crate for this article is available at\nhttps://w3id.org/ro/doi/10.5281/zenodo.5146227",
    "published_date": "2021-08-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DL",
      "H.1.1; H.3.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.06503v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.06500v1",
    "title": "Appropriate Fairness Perceptions? On the Effectiveness of Explanations in Enabling People to Assess the Fairness of Automated Decision Systems",
    "authors": [
      "Jakob Schoeffer",
      "Niklas Kuehl"
    ],
    "author_ids": [],
    "abstract": "It is often argued that one goal of explaining automated decision systems\n(ADS) is to facilitate positive perceptions (e.g., fairness or trustworthiness)\nof users towards such systems. This viewpoint, however, makes the implicit\nassumption that a given ADS is fair and trustworthy, to begin with. If the ADS\nissues unfair outcomes, then one might expect that explanations regarding the\nsystem's workings will reveal its shortcomings and, hence, lead to a decrease\nin fairness perceptions. Consequently, we suggest that it is more meaningful to\nevaluate explanations against their effectiveness in enabling people to\nappropriately assess the quality (e.g., fairness) of an associated ADS. We\nargue that for an effective explanation, perceptions of fairness should\nincrease if and only if the underlying ADS is fair. In this in-progress work,\nwe introduce the desideratum of appropriate fairness perceptions, propose a\nnovel study design for evaluating it, and outline next steps towards a\ncomprehensive experiment.",
    "published_date": "2021-08-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.06500v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.06487v1",
    "title": "Investigating Bias In Automatic Toxic Comment Detection: An Empirical Study",
    "authors": [
      "Ayush Kumar",
      "Pratik Kumar"
    ],
    "author_ids": [],
    "abstract": "With surge in online platforms, there has been an upsurge in the user\nengagement on these platforms via comments and reactions. A large portion of\nsuch textual comments are abusive, rude and offensive to the audience. With\nmachine learning systems in-place to check such comments coming onto platform,\nbiases present in the training data gets passed onto the classifier leading to\ndiscrimination against a set of classes, religion and gender. In this work, we\nevaluate different classifiers and feature to estimate the bias in these\nclassifiers along with their performance on downstream task of toxicity\nclassification. Results show that improvement in performance of automatic toxic\ncomment detection models is positively correlated to mitigating biases in these\nmodels. In our work, LSTM with attention mechanism proved to be a better\nmodelling strategy than a CNN model. Further analysis shows that fasttext\nembeddings is marginally preferable than glove embeddings on training models\nfor toxicity comment detection. Deeper analysis reveals the findings that such\nautomatic models are particularly biased to specific identity groups even\nthough the model has a high AUC score. Finally, in effort to mitigate bias in\ntoxicity detection models, a multi-task setup trained with auxiliary task of\ntoxicity sub-types proved to be useful leading to upto 0.26% (6% relative) gain\nin AUC scores.",
    "published_date": "2021-08-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.06487v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.06415v1",
    "title": "The Sharpe predictor for fairness in machine learning",
    "authors": [
      "Suyun Liu",
      "Luis Nunes Vicente"
    ],
    "author_ids": [],
    "abstract": "In machine learning (ML) applications, unfair predictions may discriminate\nagainst a minority group. Most existing approaches for fair machine learning\n(FML) treat fairness as a constraint or a penalization term in the optimization\nof a ML model, which does not lead to the discovery of the complete landscape\nof the trade-offs among learning accuracy and fairness metrics, and does not\nintegrate fairness in a meaningful way.\n  Recently, we have introduced a new paradigm for FML based on Stochastic\nMulti-Objective Optimization (SMOO), where accuracy and fairness metrics stand\nas conflicting objectives to be optimized simultaneously. The entire trade-offs\nrange is defined as the Pareto front of the SMOO problem, which can then be\nefficiently computed using stochastic-gradient type algorithms. SMOO also\nallows defining and computing new meaningful predictors for FML, a novel one\nbeing the Sharpe predictor that we introduce and explore in this paper, and\nwhich gives the highest ratio of accuracy-to-unfairness. Inspired from SMOO in\nfinance, the Sharpe predictor for FML provides the highest prediction return\n(accuracy) per unit of prediction risk (unfairness).",
    "published_date": "2021-08-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.06415v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.06295v1",
    "title": "Diachronic Analysis of German Parliamentary Proceedings: Ideological Shifts through the Lens of Political Biases",
    "authors": [
      "Tobias Walter",
      "Celina Kirschner",
      "Steffen Eger",
      "Goran Glavaš",
      "Anne Lauscher",
      "Simone Paolo Ponzetto"
    ],
    "author_ids": [],
    "abstract": "We analyze bias in historical corpora as encoded in diachronic distributional\nsemantic models by focusing on two specific forms of bias, namely a political\n(i.e., anti-communism) and racist (i.e., antisemitism) one. For this, we use a\nnew corpus of German parliamentary proceedings, DeuPARL, spanning the period\n1867--2020. We complement this analysis of historical biases in diachronic word\nembeddings with a novel measure of bias on the basis of term co-occurrences and\ngraph-based label propagation. The results of our bias measurements align with\ncommonly perceived historical trends of antisemitic and anti-communist biases\nin German politics in different time periods, thus indicating the viability of\nanalyzing historical bias trends using semantic spaces induced from historical\ncorpora.",
    "published_date": "2021-08-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.DL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.06295v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.06231v1",
    "title": "Online Fairness-Aware Learning with Imbalanced Data Streams",
    "authors": [
      "Vasileios Iosifidis",
      "Wenbin Zhang",
      "Eirini Ntoutsi"
    ],
    "author_ids": [],
    "abstract": "Data-driven learning algorithms are employed in many online applications, in\nwhich data become available over time, like network monitoring, stock price\nprediction, job applications, etc. The underlying data distribution might\nevolve over time calling for model adaptation as new instances arrive and old\ninstances become obsolete. In such dynamic environments, the so-called data\nstreams, fairness-aware learning cannot be considered as a one-off requirement,\nbut rather it should comprise a continual requirement over the stream. Recent\nfairness-aware stream classifiers ignore the problem of class imbalance, which\nmanifests in many real-life applications, and mitigate discrimination mainly\nbecause they \"reject\" minority instances at large due to their inability to\neffectively learn all classes.\n  In this work, we propose \\ours, an online fairness-aware approach that\nmaintains a valid and fair classifier over the stream. \\ours~is an online\nboosting approach that changes the training distribution in an online fashion\nby monitoring stream's class imbalance and tweaks its decision boundary to\nmitigate discriminatory outcomes over the stream. Experiments on 8 real-world\nand 1 synthetic datasets from different domains with varying class imbalance\ndemonstrate the superiority of our method over state-of-the-art fairness-aware\nstream approaches with a range (relative) increase [11.2\\%-14.2\\%] in balanced\naccuracy, [22.6\\%-31.8\\%] in gmean, [42.5\\%-49.6\\%] in recall, [14.3\\%-25.7\\%]\nin kappa and [89.4\\%-96.6\\%] in statistical parity (fairness).",
    "published_date": "2021-08-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.06231v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.06069v1",
    "title": "Zero-shot Task Transfer for Invoice Extraction via Class-aware QA Ensemble",
    "authors": [
      "Prithiviraj Damodaran",
      "Prabhkaran Singh",
      "Josemon Achankuju"
    ],
    "author_ids": [],
    "abstract": "We present VESPA, an intentionally simple yet novel zero-shot system for\nlayout, locale, and domain agnostic document extraction. In spite of the\navailability of large corpora of documents, the lack of labeled and validated\ndatasets makes it a challenge to discriminatively train document extraction\nmodels for enterprises. We show that this problem can be addressed by simply\ntransferring the information extraction (IE) task to a natural language\nQuestion-Answering (QA) task without engineering task-specific architectures.\nWe demonstrate the effectiveness of our system by evaluating on a closed corpus\nof real-world retail and tax invoices with multiple complex layouts, domains,\nand geographies. The empirical evaluation shows that our system outperforms 4\nprominent commercial invoice solutions that use discriminatively trained models\nwith architectures specifically crafted for invoice extraction. We extracted 6\nfields with zero upfront human annotation or training with an Avg. F1 of 87.50.",
    "published_date": "2021-08-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.06069v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.06049v3",
    "title": "Limitations of Local Quantum Algorithms on Random Max-k-XOR and Beyond",
    "authors": [
      "Chi-Ning Chou",
      "Peter J. Love",
      "Juspreet Singh Sandhu",
      "Jonathan Shi"
    ],
    "author_ids": [],
    "abstract": "We introduce a notion of \\emph{generic local algorithm} which strictly\ngeneralizes existing frameworks of local algorithms such as \\emph{factors of\ni.i.d.} by capturing local \\emph{quantum} algorithms such as the Quantum\nApproximate Optimization Algorithm (QAOA).\n  Motivated by a question of Farhi et al. [arXiv:1910.08187, 2019] we then show\nlimitations of generic local algorithms including QAOA on random instances of\nconstraint satisfaction problems (CSPs). Specifically, we show that any generic\nlocal algorithm whose assignment to a vertex depends only on a local\nneighborhood with $o(n)$ other vertices (such as the QAOA at depth less than\n$\\epsilon\\log(n)$) cannot arbitrarily-well approximate boolean CSPs if the\nproblem satisfies a geometric property from statistical physics called the\ncoupled overlap-gap property (OGP) [Chen et al., Annals of Probability, 47(3),\n2019]. We show that the random MAX-k-XOR problem has this property when\n$k\\geq4$ is even by extending the corresponding result for diluted $k$-spin\nglasses.\n  Our concentration lemmas confirm a conjecture of Brandao et al.\n[arXiv:1812.04170, 2018] asserting that the landscape independence of QAOA\nextends to logarithmic depth -- in other words, for every fixed choice of QAOA\nangle parameters, the algorithm at logarithmic depth performs almost equally\nwell on almost all instances. One of these concentration lemmas is a\nstrengthening of McDiarmid's inequality, applicable when the random variables\nhave a highly biased distribution, and may be of independent interest.",
    "published_date": "2021-08-13T00:00:00",
    "year": 2021,
    "categories": [
      "quant-ph",
      "cs.CC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.06049v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.05720v1",
    "title": "Semantic Concentration for Domain Adaptation",
    "authors": [
      "Shuang Li",
      "Mixue Xie",
      "Fangrui Lv",
      "Chi Harold Liu",
      "Jian Liang",
      "Chen Qin",
      "Wei Li"
    ],
    "author_ids": [],
    "abstract": "Domain adaptation (DA) paves the way for label annotation and dataset bias\nissues by the knowledge transfer from a label-rich source domain to a related\nbut unlabeled target domain. A mainstream of DA methods is to align the feature\ndistributions of the two domains. However, the majority of them focus on the\nentire image features where irrelevant semantic information, e.g., the messy\nbackground, is inevitably embedded. Enforcing feature alignments in such case\nwill negatively influence the correct matching of objects and consequently lead\nto the semantically negative transfer due to the confusion of irrelevant\nsemantics. To tackle this issue, we propose Semantic Concentration for Domain\nAdaptation (SCDA), which encourages the model to concentrate on the most\nprincipal features via the pair-wise adversarial alignment of prediction\ndistributions. Specifically, we train the classifier to class-wisely maximize\nthe prediction distribution divergence of each sample pair, which enables the\nmodel to find the region with large differences among the same class of\nsamples. Meanwhile, the feature extractor attempts to minimize that\ndiscrepancy, which suppresses the features of dissimilar regions among the same\nclass of samples and accentuates the features of principal parts. As a general\nmethod, SCDA can be easily integrated into various DA methods as a regularizer\nto further boost their performance. Extensive experiments on the cross-domain\nbenchmarks show the efficacy of SCDA.",
    "published_date": "2021-08-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.05720v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.05635v2",
    "title": "Memory-based Semantic Segmentation for Off-road Unstructured Natural Environments",
    "authors": [
      "Youngsaeng Jin",
      "David K. Han",
      "Hanseok Ko"
    ],
    "author_ids": [],
    "abstract": "With the availability of many datasets tailored for autonomous driving in\nreal-world urban scenes, semantic segmentation for urban driving scenes\nachieves significant progress. However, semantic segmentation for off-road,\nunstructured environments is not widely studied. Directly applying existing\nsegmentation networks often results in performance degradation as they cannot\novercome intrinsic problems in such environments, such as illumination changes.\nIn this paper, a built-in memory module for semantic segmentation is proposed\nto overcome these problems. The memory module stores significant\nrepresentations of training images as memory items. In addition to the encoder\nembedding like items together, the proposed memory module is specifically\ndesigned to cluster together instances of the same class even when there are\nsignificant variances in embedded features. Therefore, it makes segmentation\nnetworks better deal with unexpected illumination changes. A triplet loss is\nused in training to minimize redundancy in storing discriminative\nrepresentations of the memory module. The proposed memory module is general so\nthat it can be adopted in a variety of networks. We conduct experiments on the\nRobot Unstructured Ground Driving (RUGD) dataset and RELLIS dataset, which are\ncollected from off-road, unstructured natural environments. Experimental\nresults show that the proposed memory module improves the performance of\nexisting segmentation networks and contributes to capturing unclear objects\nover various off-road, unstructured natural scenes with equivalent\ncomputational cost and network parameters. As the proposed method can be\nintegrated into compact networks, it presents a viable approach for\nresource-limited small autonomous platforms.",
    "published_date": "2021-08-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.05635v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.05523v2",
    "title": "Fair Decision-Making for Food Inspections",
    "authors": [
      "Shubham Singh",
      "Bhuvni Shah",
      "Chris Kanich",
      "Ian A. Kash"
    ],
    "author_ids": [],
    "abstract": "Data and algorithms are essential and complementary parts of a large-scale\ndecision-making process. However, their injudicious use can lead to unforeseen\nconsequences, as has been observed by researchers and activists alike in the\nrecent past. In this paper, we revisit the application of predictive models by\nthe Chicago Department of Public Health to schedule restaurant inspections and\nprioritize the detection of critical food code violations. We perform the first\nanalysis of the model's fairness to the population served by the restaurants in\nterms of average time to find a critical violation. We find that the model\ntreats inspections unequally based on the sanitarian who conducted the\ninspection and that, in turn, there are geographic disparities in the benefits\nof the model. We examine four alternate methods of model training and two\nalternative ways of scheduling using the model and find that the latter\ngenerate more desirable results. The challenges from this application point to\nimportant directions for future work around fairness with collective entities\nrather than individuals, the use of critical violations as a proxy, and the\ndisconnect between fair classification and fairness in the dynamic scheduling\nsystem.",
    "published_date": "2021-08-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.05523v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.05449v2",
    "title": "Learning Bias-Invariant Representation by Cross-Sample Mutual Information Minimization",
    "authors": [
      "Wei Zhu",
      "Haitian Zheng",
      "Haofu Liao",
      "Weijian Li",
      "Jiebo Luo"
    ],
    "author_ids": [],
    "abstract": "Deep learning algorithms mine knowledge from the training data and thus would\nlikely inherit the dataset's bias information. As a result, the obtained model\nwould generalize poorly and even mislead the decision process in real-life\napplications. We propose to remove the bias information misused by the target\ntask with a cross-sample adversarial debiasing (CSAD) method. CSAD explicitly\nextracts target and bias features disentangled from the latent representation\ngenerated by a feature extractor and then learns to discover and remove the\ncorrelation between the target and bias features. The correlation measurement\nplays a critical role in adversarial debiasing and is conducted by a\ncross-sample neural mutual information estimator. Moreover, we propose joint\ncontent and local structural representation learning to boost mutual\ninformation estimation for better performance. We conduct thorough experiments\non publicly available datasets to validate the advantages of the proposed\nmethod over state-of-the-art approaches.",
    "published_date": "2021-08-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.05449v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.05412v1",
    "title": "Analyzing Race and Country of Citizenship Bias in Wikidata",
    "authors": [
      "Zaina Shaik",
      "Filip Ilievski",
      "Fred Morstatter"
    ],
    "author_ids": [],
    "abstract": "As an open and collaborative knowledge graph created by users and bots, it is\npossible that the knowledge in Wikidata is biased in regards to multiple\nfactors such as gender, race, and country of citizenship. Previous work has\nmostly studied the representativeness of Wikidata knowledge in terms of genders\nof people. In this paper, we examine the race and citizenship bias in general\nand in regards to STEM representation for scientists, software developers, and\nengineers. By comparing Wikidata queries to real-world datasets, we identify\nthe differences in representation to characterize the biases present in\nWikidata. Through this analysis, we discovered that there is an\noverrepresentation of white individuals and those with citizenship in Europe\nand North America; the rest of the groups are generally underrepresented. Based\non these findings, we have found and linked to Wikidata additional data about\nSTEM scientists from the minorities. This data is ready to be inserted into\nWikidata with a bot. Increasing representation of minority race and country of\ncitizenship groups can create a more accurate portrayal of individuals in STEM.",
    "published_date": "2021-08-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.05412v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.06217v1",
    "title": "Beyond Fairness Metrics: Roadblocks and Challenges for Ethical AI in Practice",
    "authors": [
      "Jiahao Chen",
      "Victor Storchan",
      "Eren Kurshan"
    ],
    "author_ids": [],
    "abstract": "We review practical challenges in building and deploying ethical AI at the\nscale of contemporary industrial and societal uses. Apart from the purely\ntechnical concerns that are the usual focus of academic research, the\noperational challenges of inconsistent regulatory pressures, conflicting\nbusiness goals, data quality issues, development processes, systems integration\npractices, and the scale of deployment all conspire to create new ethical\nrisks. Such ethical concerns arising from these practical considerations are\nnot adequately addressed by existing research results. We argue that a holistic\nconsideration of ethics in the development and deployment of AI systems is\nnecessary for building ethical AI in practice, and exhort researchers to\nconsider the full operational contexts of AI systems when assessing ethical\nrisks.",
    "published_date": "2021-08-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "91G45",
      "I.2.6; J.4; K.5.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.06217v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.05335v1",
    "title": "Explaining Algorithmic Fairness Through Fairness-Aware Causal Path Decomposition",
    "authors": [
      "Weishen Pan",
      "Sen Cui",
      "Jiang Bian",
      "Changshui Zhang",
      "Fei Wang"
    ],
    "author_ids": [],
    "abstract": "Algorithmic fairness has aroused considerable interests in data mining and\nmachine learning communities recently. So far the existing research has been\nmostly focusing on the development of quantitative metrics to measure algorithm\ndisparities across different protected groups, and approaches for adjusting the\nalgorithm output to reduce such disparities. In this paper, we propose to study\nthe problem of identification of the source of model disparities. Unlike\nexisting interpretation methods which typically learn feature importance, we\nconsider the causal relationships among feature variables and propose a novel\nframework to decompose the disparity into the sum of contributions from\nfairness-aware causal paths, which are paths linking the sensitive attribute\nand the final predictions, on the graph. We also consider the scenario when the\ndirections on certain edges within those paths cannot be determined. Our\nframework is also model agnostic and applicable to a variety of quantitative\ndisparity measures. Empirical evaluations on both synthetic and real-world data\nsets are provided to show that our method can provide precise and comprehensive\nexplanations to the model disparities.",
    "published_date": "2021-08-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.05335v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.05315v2",
    "title": "Fairness Through Counterfactual Utilities",
    "authors": [
      "Jack Blandin",
      "Ian Kash"
    ],
    "author_ids": [],
    "abstract": "Group fairness definitions such as Demographic Parity and Equal Opportunity\nmake assumptions about the underlying decision-problem that restrict them to\nclassification problems. Prior work has translated these definitions to other\nmachine learning environments, such as unsupervised learning and reinforcement\nlearning, by implementing their closest mathematical equivalent. As a result,\nthere are numerous bespoke interpretations of these definitions. Instead, we\nprovide a generalized set of group fairness definitions that unambiguously\nextend to all machine learning environments while still retaining their\noriginal fairness notions. We derive two fairness principles that enable such a\ngeneralized framework. First, our framework measures outcomes in terms of\nutilities, rather than predictions, and does so for both the decision-algorithm\nand the individual. Second, our framework considers counterfactual outcomes,\nrather than just observed outcomes, thus preventing loopholes where fairness\ncriteria are satisfied through self-fulfilling prophecies. We provide concrete\nexamples of how our counterfactual utility fairness framework resolves known\nfairness issues in classification, clustering, and reinforcement learning\nproblems. We also show that many of the bespoke interpretations of Demographic\nParity and Equal Opportunity fit nicely as special cases of our framework.",
    "published_date": "2021-08-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.05315v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.05297v3",
    "title": "Uniform Complete Observability of Mass and Rotational Inertial Parameters in Adaptive Identification of Rigid-Body Plant Dynamics",
    "authors": [
      "Tyler M. Paine",
      "Louis L. Whitcomb"
    ],
    "author_ids": [],
    "abstract": "This paper addresses the long-standing open problem of observability of mass\nand inertia plant parameters in the adaptive identification (AID) of\nsecond-order nonlinear models of 6 degree-of-freedom rigid-body dynamical\nsystems subject to externally applied forces and moments. Although stable\nmethods for AID of plant parameters for this class of systems, as well numerous\napproaches to stable model-based direct adaptive trajectory-tracking control of\nsuch systems, have been reported, these studies have been unable to prove\nanalytically that the adaptive parameter estimates converge to the true plant\nparameter values. This paper reports necessary and sufficient conditions for\nthe uniform complete observability (UCO) of 6-DOF plant inertial parameters for\na stable adaptive identifier for this class of systems. When the UCO condition\nis satisfied, the adaptive parameter estimates are shown to converge to the\ntrue plant parameter values. To the best of our knowledge this is the first\nreported proof for this class of systems of UCO of plant parameters and for\nconvergence of adaptive parameter estimates to true parameter values.\n  We also report a numerical simulation study of this AID approach which shows\nthat (a) the UCO condition can be met for fully-actuated plants as well as\nunderactuated plants with the proper choice of control input and (b)\nconvergence of adaptive parameter estimates to the true parameter values. We\nconjecture that this approach can be extended to include other parameters that\nappear rigid body plant models including parameters for drag, buoyancy, added\nmass, bias, and actuators.",
    "published_date": "2021-08-11T00:00:00",
    "year": 2021,
    "categories": [
      "eess.SY",
      "cs.RO",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.05297v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.05233v2",
    "title": "EDITS: Modeling and Mitigating Data Bias for Graph Neural Networks",
    "authors": [
      "Yushun Dong",
      "Ninghao Liu",
      "Brian Jalaian",
      "Jundong Li"
    ],
    "author_ids": [],
    "abstract": "Graph Neural Networks (GNNs) have shown superior performance in analyzing\nattributed networks in various web-based applications such as social\nrecommendation and web search. Nevertheless, in high-stake decision-making\nscenarios such as online fraud detection, there is an increasing societal\nconcern that GNNs could make discriminatory decisions towards certain\ndemographic groups. Despite recent explorations on fair GNNs, these works are\ntailored for a specific GNN model. However, myriads of GNN variants have been\nproposed for different applications, and it is costly to fine-tune existing\ndebiasing algorithms for each specific GNN architecture. Different from\nexisting works that debias GNN models, we aim to debias the input attributed\nnetwork to achieve fairer GNNs through feeding GNNs with less biased data.\nSpecifically, we propose novel definitions and metrics to measure the bias in\nan attributed network, which leads to the optimization objective to mitigate\nbias. We then develop a framework EDITS to mitigate the bias in attributed\nnetworks while maintaining the performance of GNNs in downstream tasks. EDITS\nworks in a model-agnostic manner, i.e., it is independent of any specific GNN.\nExperiments demonstrate the validity of the proposed bias metrics and the\nsuperiority of EDITS on both bias mitigation and utility maintenance.\nOpen-source implementation: https://github.com/yushundong/EDITS.",
    "published_date": "2021-08-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.05233v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.05187v1",
    "title": "Discriminative Distillation to Reduce Class Confusion in Continual Learning",
    "authors": [
      "Changhong Zhong",
      "Zhiying Cui",
      "Ruixuan Wang",
      "Wei-Shi Zheng"
    ],
    "author_ids": [],
    "abstract": "Successful continual learning of new knowledge would enable intelligent\nsystems to recognize more and more classes of objects. However, current\nintelligent systems often fail to correctly recognize previously learned\nclasses of objects when updated to learn new classes. It is widely believed\nthat such downgraded performance is solely due to the catastrophic forgetting\nof previously learned knowledge. In this study, we argue that the class\nconfusion phenomena may also play a role in downgrading the classification\nperformance during continual learning, i.e., the high similarity between new\nclasses and any previously learned classes would also cause the classifier to\nmake mistakes in recognizing these old classes, even if the knowledge of these\nold classes is not forgotten. To alleviate the class confusion issue, we\npropose a discriminative distillation strategy to help the classify well learn\nthe discriminative features between confusing classes during continual\nlearning. Experiments on multiple natural image classification tasks support\nthat the proposed distillation strategy, when combined with existing methods,\nis effective in further improving continual learning.",
    "published_date": "2021-08-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.05187v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.05152v1",
    "title": "Estimation of Fair Ranking Metrics with Incomplete Judgments",
    "authors": [
      "Ömer Kırnap",
      "Fernando Diaz",
      "Asia Biega",
      "Michael Ekstrand",
      "Ben Carterette",
      "Emine Yılmaz"
    ],
    "author_ids": [],
    "abstract": "There is increasing attention to evaluating the fairness of search system\nranking decisions. These metrics often consider the membership of items to\nparticular groups, often identified using protected attributes such as gender\nor ethnicity. To date, these metrics typically assume the availability and\ncompleteness of protected attribute labels of items. However, the protected\nattributes of individuals are rarely present, limiting the application of fair\nranking metrics in large scale systems. In order to address this problem, we\npropose a sampling strategy and estimation technique for four fair ranking\nmetrics. We formulate a robust and unbiased estimator which can operate even\nwith very limited number of labeled items. We evaluate our approach using both\nsimulated and real world data. Our experimental results demonstrate that our\nmethod can estimate this family of fair ranking metrics and provides a robust,\nreliable alternative to exhaustive or random data annotation.",
    "published_date": "2021-08-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.05152v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.05135v1",
    "title": "Overview of the TREC 2020 Fair Ranking Track",
    "authors": [
      "Asia J. Biega",
      "Fernando Diaz",
      "Michael D. Ekstrand",
      "Sergey Feldman",
      "Sebastian Kohlmeier"
    ],
    "author_ids": [],
    "abstract": "This paper provides an overview of the NIST TREC 2020 Fair Ranking track. For\n2020, we again adopted an academic search task, where we have a corpus of\nacademic article abstracts and queries submitted to a production academic\nsearch engine. The central goal of the Fair Ranking track is to provide fair\nexposure to different groups of authors (a group fairness framing). We\nrecognize that there may be multiple group definitions (e.g. based on\ndemographics, stature, topic) and hoped for the systems to be robust to these.\nWe expected participants to develop systems that optimize for fairness and\nrelevance for arbitrary group definitions, and did not reveal the exact group\ndefinitions until after the evaluation runs were submitted.The track contains\ntwo tasks,reranking and retrieval, with a shared evaluation.",
    "published_date": "2021-08-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.05135v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.05080v4",
    "title": "FakeAVCeleb: A Novel Audio-Video Multimodal Deepfake Dataset",
    "authors": [
      "Hasam Khalid",
      "Shahroz Tariq",
      "Minha Kim",
      "Simon S. Woo"
    ],
    "author_ids": [],
    "abstract": "While the significant advancements have made in the generation of deepfakes\nusing deep learning technologies, its misuse is a well-known issue now.\nDeepfakes can cause severe security and privacy issues as they can be used to\nimpersonate a person's identity in a video by replacing his/her face with\nanother person's face. Recently, a new problem of generating synthesized human\nvoice of a person is emerging, where AI-based deep learning models can\nsynthesize any person's voice requiring just a few seconds of audio. With the\nemerging threat of impersonation attacks using deepfake audios and videos, a\nnew generation of deepfake detectors is needed to focus on both video and audio\ncollectively. To develop a competent deepfake detector, a large amount of\nhigh-quality data is typically required to capture real-world (or practical)\nscenarios. Existing deepfake datasets either contain deepfake videos or audios,\nwhich are racially biased as well. As a result, it is critical to develop a\nhigh-quality video and audio deepfake dataset that can be used to detect both\naudio and video deepfakes simultaneously. To fill this gap, we propose a novel\nAudio-Video Deepfake dataset, FakeAVCeleb, which contains not only deepfake\nvideos but also respective synthesized lip-synced fake audios. We generate this\ndataset using the most popular deepfake generation methods. We selected real\nYouTube videos of celebrities with four ethnic backgrounds to develop a more\nrealistic multimodal dataset that addresses racial bias, and further help\ndevelop multimodal deepfake detectors. We performed several experiments using\nstate-of-the-art detection methods to evaluate our deepfake dataset and\ndemonstrate the challenges and usefulness of our multimodal Audio-Video\ndeepfake dataset.",
    "published_date": "2021-08-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.MM",
      "cs.SD",
      "eess.AS",
      "I.4.9; I.5.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.05080v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.05028v2",
    "title": "Boosting the Generalization Capability in Cross-Domain Few-shot Learning via Noise-enhanced Supervised Autoencoder",
    "authors": [
      "Hanwen Liang",
      "Qiong Zhang",
      "Peng Dai",
      "Juwei Lu"
    ],
    "author_ids": [],
    "abstract": "State of the art (SOTA) few-shot learning (FSL) methods suffer significant\nperformance drop in the presence of domain differences between source and\ntarget datasets. The strong discrimination ability on the source dataset does\nnot necessarily translate to high classification accuracy on the target\ndataset. In this work, we address this cross-domain few-shot learning (CDFSL)\nproblem by boosting the generalization capability of the model. Specifically,\nwe teach the model to capture broader variations of the feature distributions\nwith a novel noise-enhanced supervised autoencoder (NSAE). NSAE trains the\nmodel by jointly reconstructing inputs and predicting the labels of inputs as\nwell as their reconstructed pairs. Theoretical analysis based on intra-class\ncorrelation (ICC) shows that the feature embeddings learned from NSAE have\nstronger discrimination and generalization abilities in the target domain. We\nalso take advantage of NSAE structure and propose a two-step fine-tuning\nprocedure that achieves better adaption and improves classification performance\nin the target domain. Extensive experiments and ablation studies are conducted\nto demonstrate the effectiveness of the proposed method. Experimental results\nshow that our proposed method consistently outperforms SOTA methods under\nvarious conditions.",
    "published_date": "2021-08-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.05028v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.04983v1",
    "title": "Learning Fair Face Representation With Progressive Cross Transformer",
    "authors": [
      "Yong Li",
      "Yufei Sun",
      "Zhen Cui",
      "Shiguang Shan",
      "Jian Yang"
    ],
    "author_ids": [],
    "abstract": "Face recognition (FR) has made extraordinary progress owing to the\nadvancement of deep convolutional neural networks. However, demographic bias\namong different racial cohorts still challenges the practical face recognition\nsystem. The race factor has been proven to be a dilemma for fair FR (FFR) as\nthe subject-related specific attributes induce the classification bias whilst\ncarrying some useful cues for FR. To mitigate racial bias and meantime preserve\nrobust FR, we abstract face identity-related representation as a signal\ndenoising problem and propose a progressive cross transformer (PCT) method for\nfair face recognition. Originating from the signal decomposition theory, we\nattempt to decouple face representation into i) identity-related components and\nii) noisy/identity-unrelated components induced by race. As an extension of\nsignal subspace decomposition, we formulate face decoupling as a generalized\nfunctional expression model to cross-predict face identity and race\ninformation. The face expression model is further concretized by designing dual\ncross-transformers to distill identity-related components and suppress racial\nnoises. In order to refine face representation, we take a progressive face\ndecoupling way to learn identity/race-specific transformations, so that\nidentity-unrelated components induced by race could be better disentangled. We\nevaluate the proposed PCT on the public fair face recognition benchmarks (BFW,\nRFW) and verify that PCT is capable of mitigating bias in face recognition\nwhile achieving state-of-the-art FR performance. Besides, visualization results\nalso show that the attention maps in PCT can well reveal the\nrace-related/biased facial regions.",
    "published_date": "2021-08-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.04983v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.04931v3",
    "title": "Toward Systematic Considerations of Missingness in Visual Analytics",
    "authors": [
      "Maoyuan Sun",
      "Yue Ma",
      "Yuanxin Wang",
      "Tianyi Li",
      "Jian Zhao",
      "Yujun Liu",
      "Ping-Shou Zhong"
    ],
    "author_ids": [],
    "abstract": "Data-driven decision making has been a common task in today's big data era,\nfrom simple choices such as finding a fast way to drive home, to complex\ndecisions on medical treatment. It is often supported by visual analytics. For\nvarious reasons (e.g., system failure, interrupted network, intentional\ninformation hiding, or bias), visual analytics for sensemaking of data involves\nmissingness (e.g., data loss and incomplete analysis), which impacts human\ndecisions. For example, missing data can cost a business millions of dollars,\nand failing to recognize key evidence can put an innocent person in jail. Being\naware of missingness is critical to avoid such catastrophes. To fulfill this,\nas an initial step, we consider missingness in visual analytics from two\naspects: data-centric and human-centric. The former emphasizes missingness in\nthree data-related categories: data composition, data relationship, and data\nusage. The latter focuses on the human-perceived missingness at three levels:\nobserved-level, inferred-level, and ignored-level. Based on them, we discuss\npossible roles of visualizations for handling missingness, and conclude our\ndiscussion with future research opportunities.",
    "published_date": "2021-08-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC",
      "H.5.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.04931v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.04884v3",
    "title": "Retiring Adult: New Datasets for Fair Machine Learning",
    "authors": [
      "Frances Ding",
      "Moritz Hardt",
      "John Miller",
      "Ludwig Schmidt"
    ],
    "author_ids": [],
    "abstract": "Although the fairness community has recognized the importance of data,\nresearchers in the area primarily rely on UCI Adult when it comes to tabular\ndata. Derived from a 1994 US Census survey, this dataset has appeared in\nhundreds of research papers where it served as the basis for the development\nand comparison of many algorithmic fairness interventions. We reconstruct a\nsuperset of the UCI Adult data from available US Census sources and reveal\nidiosyncrasies of the UCI Adult dataset that limit its external validity. Our\nprimary contribution is a suite of new datasets derived from US Census surveys\nthat extend the existing data ecosystem for research on fair machine learning.\nWe create prediction tasks relating to income, employment, health,\ntransportation, and housing. The data span multiple years and all states of the\nUnited States, allowing researchers to study temporal shift and geographic\nvariation. We highlight a broad initial sweep of new empirical insights\nrelating to trade-offs between fairness criteria, performance of algorithmic\ninterventions, and the role of distribution shift based on our new datasets.\nOur findings inform ongoing debates, challenge some existing narratives, and\npoint to future research directions. Our datasets are available at\nhttps://github.com/zykls/folktables.",
    "published_date": "2021-08-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.04884v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.07714v1",
    "title": "Harnessing value from data science in business: ensuring explainability and fairness of solutions",
    "authors": [
      "Krzysztof Chomiak",
      "Michał Miktus"
    ],
    "author_ids": [],
    "abstract": "The paper introduces concepts of fairness and explainability (XAI) in\nartificial intelligence, oriented to solve a sophisticated business problems.\nFor fairness, the authors discuss the bias-inducing specifics, as well as\nrelevant mitigation methods, concluding with a set of recipes for introducing\nfairness in data-driven organizations. Additionally, for XAI, the authors audit\nspecific algorithms paired with demonstrational business use-cases, discuss a\nplethora of techniques of explanations quality quantification and provide an\noverview of future research avenues.",
    "published_date": "2021-08-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.07714v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.04436v1",
    "title": "A Generalizable Model-and-Data Driven Approach for Open-Set RFF Authentication",
    "authors": [
      "Renjie Xie",
      "Wei Xu",
      "Yanzhi Chen",
      "Jiabao Yu",
      "Aiqun Hu",
      "Derrick Wing Kwan Ng",
      "A. Lee Swindlehurst"
    ],
    "author_ids": [],
    "abstract": "Radio-frequency fingerprints~(RFFs) are promising solutions for realizing\nlow-cost physical layer authentication. Machine learning-based methods have\nbeen proposed for RFF extraction and discrimination. However, most existing\nmethods are designed for the closed-set scenario where the set of devices is\nremains unchanged. These methods can not be generalized to the RFF\ndiscrimination of unknown devices. To enable the discrimination of RFF from\nboth known and unknown devices, we propose a new end-to-end deep learning\nframework for extracting RFFs from raw received signals. The proposed framework\ncomprises a novel preprocessing module, called neural synchronization~(NS),\nwhich incorporates the data-driven learning with signal processing priors as an\ninductive bias from communication-model based processing. Compared to\ntraditional carrier synchronization techniques, which are static, this module\nestimates offsets by two learnable deep neural networks jointly trained by the\nRFF extractor. Additionally, a hypersphere representation is proposed to\nfurther improve the discrimination of RFF. Theoretical analysis shows that such\na data-and-model framework can better optimize the mutual information between\ndevice identity and the RFF, which naturally leads to better performance.\nExperimental results verify that the proposed RFF significantly outperforms\npurely data-driven DNN-design and existing handcrafted RFF methods in terms of\nboth discrimination and network generalizability.",
    "published_date": "2021-08-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.04436v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.04384v3",
    "title": "RaftMLP: How Much Can Be Done Without Attention and with Less Spatial Locality?",
    "authors": [
      "Yuki Tatsunami",
      "Masato Taki"
    ],
    "author_ids": [],
    "abstract": "For the past ten years, CNN has reigned supreme in the world of computer\nvision, but recently, Transformer has been on the rise. However, the quadratic\ncomputational cost of self-attention has become a serious problem in practice\napplications. There has been much research on architectures without CNN and\nself-attention in this context. In particular, MLP-Mixer is a simple\narchitecture designed using MLPs and hit an accuracy comparable to the Vision\nTransformer. However, the only inductive bias in this architecture is the\nembedding of tokens. This leaves open the possibility of incorporating a\nnon-convolutional (or non-local) inductive bias into the architecture, so we\nused two simple ideas to incorporate inductive bias into the MLP-Mixer while\ntaking advantage of its ability to capture global correlations. A way is to\ndivide the token-mixing block vertically and horizontally. Another way is to\nmake spatial correlations denser among some channels of token-mixing. With this\napproach, we were able to improve the accuracy of the MLP-Mixer while reducing\nits parameters and computational complexity. The small model that is RaftMLP-S\nis comparable to the state-of-the-art global MLP-based model in terms of\nparameters and efficiency per calculation. In addition, we tackled the problem\nof fixed input image resolution for global MLP-based models by utilizing\nbicubic interpolation. We demonstrated that these models could be applied as\nthe backbone of architectures for downstream tasks such as object detection.\nHowever, it did not have significant performance and mentioned the need for\nMLP-specific architectures for downstream tasks for global MLP-based models.\nThe source code in PyTorch version is available at\n\\url{https://github.com/okojoalg/raft-mlp}.",
    "published_date": "2021-08-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.04384v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.04022v1",
    "title": "Towards Automated Fatigue Assessment using Wearable Sensing and Mixed-Effects Models",
    "authors": [
      "Yang Bai",
      "Yu Guan",
      "Jian Qing Shi",
      "Wan-Fai Ng"
    ],
    "author_ids": [],
    "abstract": "Fatigue is a broad, multifactorial concept that includes the subjective\nperception of reduced physical and mental energy levels. It is also one of the\nkey factors that strongly affect patients' health-related quality of life. To\ndate, most fatigue assessment methods were based on self-reporting, which may\nsuffer from many factors such as recall bias. To address this issue, in this\nwork, we recorded multi-modal physiological data (including ECG, accelerometer,\nskin temperature and respiratory rate, as well as demographic information such\nas age, BMI) in free-living environments and developed automated fatigue\nassessment models. Specifically, we extracted features from each modality and\nemployed the random forest-based mixed-effects models, which can take advantage\nof the demographic information for improved performance. We conducted\nexperiments on our collected dataset, and very promising preliminary results\nwere achieved. Our results suggested ECG played an important role in the\nfatigue assessment tasks.",
    "published_date": "2021-08-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.04022v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.03929v1",
    "title": "The State of AI Ethics Report (Volume 5)",
    "authors": [
      "Abhishek Gupta",
      "Connor Wright",
      "Marianna Bergamaschi Ganapini",
      "Masa Sweidan",
      "Renjie Butalid"
    ],
    "author_ids": [],
    "abstract": "This report from the Montreal AI Ethics Institute covers the most salient\nprogress in research and reporting over the second quarter of 2021 in the field\nof AI ethics with a special emphasis on \"Environment and AI\", \"Creativity and\nAI\", and \"Geopolitics and AI.\" The report also features an exclusive piece\ntitled \"Critical Race Quantum Computer\" that applies ideas from quantum physics\nto explain the complexities of human characteristics and how they can and\nshould shape our interactions with each other. The report also features special\ncontributions on the subject of pedagogy in AI ethics, sociology and AI ethics,\nand organizational challenges to implementing AI ethics in practice. Given\nMAIEI's mission to highlight scholars from around the world working on AI\nethics issues, the report also features two spotlights sharing the work of\nscholars operating in Singapore and Mexico helping to shape policy measures as\nthey relate to the responsible use of technology. The report also has an\nextensive section covering the gamut of issues when it comes to the societal\nimpacts of AI covering areas of bias, privacy, transparency, accountability,\nfairness, interpretability, disinformation, policymaking, law, regulations, and\nmoral philosophy.",
    "published_date": "2021-08-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "K.4; I.2; A.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.03929v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.03856v2",
    "title": "BenchENAS: A Benchmarking Platform for Evolutionary Neural Architecture Search",
    "authors": [
      "Xiangning Xie",
      "Yuqiao Liu",
      "Yanan Sun",
      "Gary G. Yen",
      "Bing Xue",
      "Mengjie Zhang"
    ],
    "author_ids": [],
    "abstract": "Neural architecture search (NAS), which automatically designs the\narchitectures of deep neural networks, has achieved breakthrough success over\nmany applications in the past few years. Among different classes of NAS\nmethods, evolutionary computation based NAS (ENAS) methods have recently gained\nmuch attention. Unfortunately, the issues of fair comparisons and efficient\nevaluations have hindered the development of ENAS. The current benchmark\narchitecture datasets designed for fair comparisons only provide the datasets,\nnot the ENAS algorithms or the platform to run the algorithms. The existing\nefficient evaluation methods are either not suitable for the population-based\nENAS algorithm or are too complex to use. This paper develops a platform named\nBenchENAS to address these issues. BenchENAS aims to achieve fair comparisons\nby running different algorithms in the same environment and with the same\nsettings. To achieve efficient evaluation in a common lab environment,\nBenchENAS designs a parallel component and a cache component with high\nmaintainability. Furthermore, BenchENAS is easy to install and highly\nconfigurable and modular, which brings benefits in good usability and easy\nextensibility. The paper conducts efficient comparison experiments on eight\nENAS algorithms with high GPU utilization on this platform. The experiments\nvalidate that the fair comparison issue does exist, and BenchENAS can alleviate\nthis issue. A website has been built to promote BenchENAS at\nhttps://benchenas.com, where interested researchers can obtain the source code\nand document of BenchENAS for free.",
    "published_date": "2021-08-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.NE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.03856v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.03778v1",
    "title": "Velocity-adaptive Access Scheme for MEC-assisted Platooning Networks: Access Fairness Via Data Freshness",
    "authors": [
      "Qiong Wu",
      "Ziyang Wan",
      "Qiang Fan",
      "Pingyi Fan",
      "Jiangzhou Wang"
    ],
    "author_ids": [],
    "abstract": "Platooning strategy is an important part of autonomous driving technology.\nDue to the limited resource of autonomous vehicles in platoons, mobile edge\ncomputing (MEC) is usually used to assist vehicles in platoons to obtain useful\ninformation, increasing its safety. Specifically, vehicles usually adopt the\nIEEE 802.11 distributed coordination function (DCF) mechanism to transmit large\namount of data to the base station (BS) through vehicle-to-infrastructure (V2I)\ncommunications, where the useful information can be extracted by the edge\nserver connected to the BS and then sent back to the vehicles to make correct\ndecisions in time. However, vehicles may be moving on different lanes with\ndifferent velocities, which incurs the unfair access due to the characteristics\nof platoons, i.e., vehicles on different lanes transmit different amount of\ndata to the BS when they pass through the coverage of the BS, which also\nresults in the different amount of useful information received by various\nvehicles. Moreover, age of information (AoI) is an important performance metric\nto measure the freshness of the data. Large average age of data implies not\nreceiving the useful information in time. It is necessary to design an access\nscheme to jointly optimize the fairness and data freshness. In this paper, we\nformulate a joint optimization problem in the MEC-assisted V2I networks and\npresent a multi-objective optimization scheme to solve the problem through\nadjusting the minimum contention window under the IEEE 802.11 DCF mode\naccording to the velocities of vehicles. The effectiveness of the scheme has\nbeen demonstrated by simulation.",
    "published_date": "2021-08-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.03778v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.03766v1",
    "title": "The Weighted Average Illusion: Biases in Perceived Mean Position in Scatterplots",
    "authors": [
      "Matt-Heun Hong",
      "Jessica K. Witt",
      "Danielle Albers Szafir"
    ],
    "author_ids": [],
    "abstract": "Scatterplots can encode a third dimension by using additional channels like\nsize or color (e.g. bubble charts). We explore a potential misinterpretation of\ntrivariate scatterplots, which we call the weighted average illusion, where\nlocations of larger and darker points are given more weight toward x- and\ny-mean estimates. This systematic bias is sensitive to a designer's choice of\nsize or lightness ranges mapped onto the data. In this paper, we quantify this\nbias against varying size/lightness ranges and data correlations. We discuss\npossible explanations for its cause by measuring attention given to individual\ndata points using a vision science technique called the centroid method. Our\nwork illustrates how ensemble processing mechanisms and mental shortcuts can\nsignificantly distort visual summaries of data, and can lead to misconceptions\nlike the demonstrated weighted average illusion.",
    "published_date": "2021-08-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.03766v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.03764v1",
    "title": "PASS: Protected Attribute Suppression System for Mitigating Bias in Face Recognition",
    "authors": [
      "Prithviraj Dhar",
      "Joshua Gleason",
      "Aniket Roy",
      "Carlos D. Castillo",
      "Rama Chellappa"
    ],
    "author_ids": [],
    "abstract": "Face recognition networks encode information about sensitive attributes while\nbeing trained for identity classification. Such encoding has two major issues:\n(a) it makes the face representations susceptible to privacy leakage (b) it\nappears to contribute to bias in face recognition. However, existing bias\nmitigation approaches generally require end-to-end training and are unable to\nachieve high verification accuracy. Therefore, we present a descriptor-based\nadversarial de-biasing approach called `Protected Attribute Suppression System\n(PASS)'. PASS can be trained on top of descriptors obtained from any previously\ntrained high-performing network to classify identities and simultaneously\nreduce encoding of sensitive attributes. This eliminates the need for\nend-to-end training. As a component of PASS, we present a novel discriminator\ntraining strategy that discourages a network from encoding protected attribute\ninformation. We show the efficacy of PASS to reduce gender and skintone\ninformation in descriptors from SOTA face recognition networks like Arcface. As\na result, PASS descriptors outperform existing baselines in reducing gender and\nskintone bias on the IJB-C dataset, while maintaining a high verification\naccuracy.",
    "published_date": "2021-08-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.03764v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.03609v1",
    "title": "Enabling Plug-and-Play and Crowdsourcing SLAM in Wireless Communication Systems",
    "authors": [
      "Jie Yang",
      "Chao-Kai Wen",
      "Shi Jin",
      "Xiao Li"
    ],
    "author_ids": [],
    "abstract": "Simultaneous localization and mapping (SLAM) during communication is\nemerging. This technology promises to provide information on propagation\nenvironments and transceivers' location, thus creating several new services and\napplications for the Internet of Things and environment-aware communication.\nUsing crowdsourcing data collected by multiple agents appears to be much\npotential for enhancing SLAM performance. However, the measurement\nuncertainties in practice and biased estimations from multiple agents may\nresult in serious errors. This study develops a robust SLAM method with\nmeasurement plug-and-play and crowdsourcing mechanisms to address the above\nproblems. First, we divide measurements into different categories according to\ntheir unknown biases and realize a measurement plug-and-play mechanism by\nextending the classic belief propagation (BP)-based SLAM method. The proposed\nmechanism can obtain the time-varying agent location, radio features, and\ncorresponding measurement biases (such as clock bias, orientation bias, and\nreceived signal strength model parameters), with high accuracy and robustness\nin challenging scenarios without any prior information on anchors and agents.\nNext, we establish a probabilistic crowdsourcing-based SLAM mechanism, in which\nmultiple agents cooperate to construct and refine the radio map in a\ndecentralized manner. Our study presents the first BP-based crowdsourcing that\nresolves the \"double count\" and \"data reliability\" problems through the\nflexible application of probabilistic data association methods. Numerical\nresults reveal that the crowdsourcing mechanism can further improve the\naccuracy of the mapping result, which, in turn, ensures the decimeter-level\nlocalization accuracy of each agent in a challenging propagation environment.",
    "published_date": "2021-08-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IT",
      "eess.SP",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.03609v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.03536v2",
    "title": "Left, Right, and Gender: Exploring Interaction Traces to Mitigate Human Biases",
    "authors": [
      "Emily Wall",
      "Arpit Narechania",
      "Adam Coscia",
      "Jamal Paden",
      "Alex Endert"
    ],
    "author_ids": [],
    "abstract": "Human biases impact the way people analyze data and make decisions. Recent\nwork has shown that some visualization designs can better support cognitive\nprocesses and mitigate cognitive biases (i.e., errors that occur due to the use\nof mental \"shortcuts\"). In this work, we explore how visualizing a user's\ninteraction history (i.e., which data points and attributes a user has\ninteracted with) can be used to mitigate potential biases that drive decision\nmaking by promoting conscious reflection of one's analysis process. Given an\ninteractive scatterplot-based visualization tool, we showed interaction history\nin real-time while exploring data (by coloring points in the scatterplot that\nthe user has interacted with), and in a summative format after a decision has\nbeen made (by comparing the distribution of user interactions to the underlying\ndistribution of the data). We conducted a series of in-lab experiments and a\ncrowd-sourced experiment to evaluate the effectiveness of interaction history\ninterventions toward mitigating bias. We contextualized this work in a\npolitical scenario in which participants were instructed to choose a committee\nof 10 fictitious politicians to review a recent bill passed in the U.S. state\nof Georgia banning abortion after 6 weeks, where things like gender bias or\npolitical party bias may drive one's analysis process. We demonstrate the\ngeneralizability of this approach by evaluating a second decision making\nscenario related to movies. Our results are inconclusive for the effectiveness\nof interaction history (henceforth referred to as interaction traces) toward\nmitigating biased decision making. However, we find some mixed support that\ninteraction traces, particularly in a summative format, can increase awareness\nof potential unconscious biases.",
    "published_date": "2021-08-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.03536v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.03531v2",
    "title": "Learning to Transfer with von Neumann Conditional Divergence",
    "authors": [
      "Ammar Shaker",
      "Shujian Yu",
      "Daniel Oñoro-Rubio"
    ],
    "author_ids": [],
    "abstract": "The similarity of feature representations plays a pivotal role in the success\nof problems related to domain adaptation. Feature similarity includes both the\ninvariance of marginal distributions and the closeness of conditional\ndistributions given the desired response $y$ (e.g., class labels).\nUnfortunately, traditional methods always learn such features without fully\ntaking into consideration the information in $y$, which in turn may lead to a\nmismatch of the conditional distributions or the mix-up of discriminative\nstructures underlying data distributions. In this work, we introduce the\nrecently proposed von Neumann conditional divergence to improve the\ntransferability across multiple domains. We show that this new divergence is\ndifferentiable and eligible to easily quantify the functional dependence\nbetween features and $y$. Given multiple source tasks, we integrate this\ndivergence to capture discriminative information in $y$ and design novel\nlearning objectives assuming those source tasks are observed either\nsimultaneously or sequentially. In both scenarios, we obtain favorable\nperformance against state-of-the-art methods in terms of smaller generalization\nerror on new tasks and less catastrophic forgetting on source tasks (in the\nsequential setup).",
    "published_date": "2021-08-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.03531v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.03440v1",
    "title": "Unbiased Cascade Bandits: Mitigating Exposure Bias in Online Learning to Rank Recommendation",
    "authors": [
      "Masoud Mansoury",
      "Himan Abdollahpouri",
      "Bamshad Mobasher",
      "Mykola Pechenizkiy",
      "Robin Burke",
      "Milad Sabouri"
    ],
    "author_ids": [],
    "abstract": "Exposure bias is a well-known issue in recommender systems where items and\nsuppliers are not equally represented in the recommendation results. This is\nespecially problematic when bias is amplified over time as a few popular items\nare repeatedly over-represented in recommendation lists. This phenomenon can be\nviewed as a recommendation feedback loop: the system repeatedly recommends\ncertain items at different time points and interactions of users with those\nitems will amplify bias towards those items over time. This issue has been\nextensively studied in the literature on model-based or neighborhood-based\nrecommendation algorithms, but less work has been done on online recommendation\nmodels such as those based on multi-armed Bandit algorithms. In this paper, we\nstudy exposure bias in a class of well-known bandit algorithms known as Linear\nCascade Bandits. We analyze these algorithms on their ability to handle\nexposure bias and provide a fair representation for items and suppliers in the\nrecommendation results. Our analysis reveals that these algorithms fail to\ntreat items and suppliers fairly and do not sufficiently explore the item space\nfor each user. To mitigate this bias, we propose a discounting factor and\nincorporate it into these algorithms that controls the exposure of items at\neach time step. To show the effectiveness of the proposed discounting factor on\nmitigating exposure bias, we perform experiments on two datasets using three\ncascading bandit algorithms and our experimental results show that the proposed\nmethod improves the exposure fairness for items and suppliers.",
    "published_date": "2021-08-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.03440v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.03439v2",
    "title": "Towards Discriminative Representation Learning for Unsupervised Person Re-identification",
    "authors": [
      "Takashi Isobe",
      "Dong Li",
      "Lu Tian",
      "Weihua Chen",
      "Yi Shan",
      "Shengjin Wang"
    ],
    "author_ids": [],
    "abstract": "In this work, we address the problem of unsupervised domain adaptation for\nperson re-ID where annotations are available for the source domain but not for\ntarget. Previous methods typically follow a two-stage optimization pipeline,\nwhere the network is first pre-trained on source and then fine-tuned on target\nwith pseudo labels created by feature clustering. Such methods sustain two main\nlimitations. (1) The label noise may hinder the learning of discriminative\nfeatures for recognizing target classes. (2) The domain gap may hinder\nknowledge transferring from source to target. We propose three types of\ntechnical schemes to alleviate these issues. First, we propose a cluster-wise\ncontrastive learning algorithm (CCL) by iterative optimization of feature\nlearning and cluster refinery to learn noise-tolerant representations in the\nunsupervised manner. Second, we adopt a progressive domain adaptation (PDA)\nstrategy to gradually mitigate the domain gap between source and target data.\nThird, we propose Fourier augmentation (FA) for further maximizing the class\nseparability of re-ID models by imposing extra constraints in the Fourier\nspace. We observe that these proposed schemes are capable of facilitating the\nlearning of discriminative feature representations. Experiments demonstrate\nthat our method consistently achieves notable improvements over the\nstate-of-the-art unsupervised re-ID methods on multiple benchmarks, e.g.,\nsurpassing MMT largely by 8.1\\%, 9.9\\%, 11.4\\% and 11.1\\% mAP on the\nMarket-to-Duke, Duke-to-Market, Market-to-MSMT and Duke-to-MSMT tasks,\nrespectively.",
    "published_date": "2021-08-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.03439v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.03372v4",
    "title": "Neighborhood Consensus Contrastive Learning for Backward-Compatible Representation",
    "authors": [
      "Shengsen Wu",
      "Liang Chen",
      "Yihang Lou",
      "Yan Bai",
      "Tao Bai",
      "Minghua Deng",
      "Lingyu Duan"
    ],
    "author_ids": [],
    "abstract": "In object re-identification (ReID), the development of deep learning\ntechniques often involves model updates and deployment. It is unbearable to\nre-embedding and re-index with the system suspended when deploying new models.\nTherefore, backward-compatible representation is proposed to enable \"new\"\nfeatures to be compared with \"old\" features directly, which means that the\ndatabase is active when there are both \"new\" and \"old\" features in it. Thus we\ncan scroll-refresh the database or even do nothing on the database to update.\n  The existing backward-compatible methods either require a strong overlap\nbetween old and new training data or simply conduct constraints at the instance\nlevel. Thus they are difficult in handling complicated cluster structures and\nare limited in eliminating the impact of outliers in old embeddings, resulting\nin a risk of damaging the discriminative capability of new features. In this\nwork, we propose a Neighborhood Consensus Contrastive Learning (NCCL) method.\nWith no assumptions about the new training data, we estimate the sub-cluster\nstructures of old embeddings. A new embedding is constrained with multiple old\nembeddings in both embedding space and discrimination space at the sub-class\nlevel. The effect of outliers diminished, as the multiple samples serve as\n\"mean teachers\". Besides, we also propose a scheme to filter the old embeddings\nwith low credibility, further improving the compatibility robustness. Our\nmethod ensures backward compatibility without impairing the accuracy of the new\nmodel. And it can even improve the new model's accuracy in most scenarios.",
    "published_date": "2021-08-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.03372v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.03362v2",
    "title": "On Measures of Biases and Harms in NLP",
    "authors": [
      "Sunipa Dev",
      "Emily Sheng",
      "Jieyu Zhao",
      "Aubrie Amstutz",
      "Jiao Sun",
      "Yu Hou",
      "Mattie Sanseverino",
      "Jiin Kim",
      "Akihiro Nishi",
      "Nanyun Peng",
      "Kai-Wei Chang"
    ],
    "author_ids": [],
    "abstract": "Recent studies show that Natural Language Processing (NLP) technologies\npropagate societal biases about demographic groups associated with attributes\nsuch as gender, race, and nationality. To create interventions and mitigate\nthese biases and associated harms, it is vital to be able to detect and measure\nsuch biases. While existing works propose bias evaluation and mitigation\nmethods for various tasks, there remains a need to cohesively understand the\nbiases and the specific harms they measure, and how different measures compare\nwith each other. To address this gap, this work presents a practical framework\nof harms and a series of questions that practitioners can answer to guide the\ndevelopment of bias measures. As a validation of our framework and\ndocumentation questions, we also present several case studies of how existing\nbias measures in NLP -- both intrinsic measures of bias in representations and\nextrinsic measures of bias of downstream applications -- can be aligned with\ndifferent harms and how our proposed documentation questions facilitates more\nholistic understanding of what bias measures are measuring.",
    "published_date": "2021-08-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.03362v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.03318v1",
    "title": "OHPL: One-shot Hand-eye Policy Learner",
    "authors": [
      "Changjae Oh",
      "Yik Lung Pang",
      "Andrea Cavallaro"
    ],
    "author_ids": [],
    "abstract": "The control of a robot for manipulation tasks generally relies on object\ndetection and pose estimation. An attractive alternative is to learn control\npolicies directly from raw input data. However, this approach is time-consuming\nand expensive since learning the policy requires many trials with robot actions\nin the physical environment. To reduce the training cost, the policy can be\nlearned in simulation with a large set of synthetic images. The limit of this\napproach is the domain gap between the simulation and the robot workspace. In\nthis paper, we propose to learn a policy for robot reaching movements from a\nsingle image captured directly in the robot workspace from a camera placed on\nthe end-effector (a hand-eye camera). The idea behind the proposed policy\nlearner is that view changes seen from the hand-eye camera produced by actions\nin the robot workspace are analogous to locating a region-of-interest in a\nsingle image by performing sequential object localisation. This similar view\nchange enables training of object reaching policies using\nreinforcement-learning-based sequential object localisation. To facilitate the\nadaptation of the policy to view changes in the robot workspace, we further\npresent a dynamic filter that learns to bias an input state to remove\nirrelevant information for an action decision. The proposed policy learner can\nbe used as a powerful representation for robotic tasks, and we validate it on\nstatic and moving object reaching tasks.",
    "published_date": "2021-08-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.03318v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.02953v1",
    "title": "Few-shot Unsupervised Domain Adaptation with Image-to-class Sparse Similarity Encoding",
    "authors": [
      "Shengqi Huang",
      "Wanqi Yang",
      "Lei Wang",
      "Luping Zhou",
      "Ming Yang"
    ],
    "author_ids": [],
    "abstract": "This paper investigates a valuable setting called few-shot unsupervised\ndomain adaptation (FS-UDA), which has not been sufficiently studied in the\nliterature. In this setting, the source domain data are labelled, but with\nfew-shot per category, while the target domain data are unlabelled. To address\nthe FS-UDA setting, we develop a general UDA model to solve the following two\nkey issues: the few-shot labeled data per category and the domain adaptation\nbetween support and query sets. Our model is general in that once trained it\nwill be able to be applied to various FS-UDA tasks from the same source and\ntarget domains. Inspired by the recent local descriptor based few-shot learning\n(FSL), our general UDA model is fully built upon local descriptors (LDs) for\nimage classification and domain adaptation. By proposing a novel concept called\nsimilarity patterns (SPs), our model not only effectively considers the spatial\nrelationship of LDs that was ignored in previous FSL methods, but also makes\nthe learned image similarity better serve the required domain alignment.\nSpecifically, we propose a novel IMage-to-class sparse Similarity Encoding\n(IMSE) method. It learns SPs to extract the local discriminative information\nfor classification and meanwhile aligns the covariance matrix of the SPs for\ndomain adaptation. Also, domain adversarial training and multi-scale local\nfeature matching are performed upon LDs. Extensive experiments conducted on a\nmulti-domain benchmark dataset DomainNet demonstrates the state-of-the-art\nperformance of our IMSE for the novel setting of FS-UDA. In addition, for FSL,\nour IMSE can also show better performance than most of recent FSL methods on\nminiImageNet.",
    "published_date": "2021-08-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.02953v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.02943v2",
    "title": "Unsupervised Learning of Debiased Representations with Pseudo-Attributes",
    "authors": [
      "Seonguk Seo",
      "Joon-Young Lee",
      "Bohyung Han"
    ],
    "author_ids": [],
    "abstract": "Dataset bias is a critical challenge in machine learning since it often leads\nto a negative impact on a model due to the unintended decision rules captured\nby spurious correlations. Although existing works often handle this issue based\non human supervision, the availability of the proper annotations is impractical\nand even unrealistic. To better tackle the limitation, we propose a simple but\neffective unsupervised debiasing technique. Specifically, we first identify\npseudo-attributes based on the results from clustering performed in the feature\nembedding space even without an explicit bias attribute supervision. Then, we\nemploy a novel cluster-wise reweighting scheme to learn debiased\nrepresentation; the proposed method prevents minority groups from being\ndiscounted for minimizing the overall loss, which is desirable for worst-case\ngeneralization. The extensive experiments demonstrate the outstanding\nperformance of our approach on multiple standard benchmarks, even achieving the\ncompetitive accuracy to the supervised counterpart.",
    "published_date": "2021-08-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.02943v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.02922v2",
    "title": "Mitigating Dataset Harms Requires Stewardship: Lessons from 1000 Papers",
    "authors": [
      "Kenny Peng",
      "Arunesh Mathur",
      "Arvind Narayanan"
    ],
    "author_ids": [],
    "abstract": "Machine learning datasets have elicited concerns about privacy, bias, and\nunethical applications, leading to the retraction of prominent datasets such as\nDukeMTMC, MS-Celeb-1M, and Tiny Images. In response, the machine learning\ncommunity has called for higher ethical standards in dataset creation. To help\ninform these efforts, we studied three influential but ethically problematic\nface and person recognition datasets -- Labeled Faces in the Wild (LFW),\nMS-Celeb-1M, and DukeMTM -- by analyzing nearly 1000 papers that cite them. We\nfound that the creation of derivative datasets and models, broader\ntechnological and social change, the lack of clarity of licenses, and dataset\nmanagement practices can introduce a wide range of ethical concerns. We\nconclude by suggesting a distributed approach to harm mitigation that considers\nthe entire life cycle of a dataset.",
    "published_date": "2021-08-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.02922v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.07706v1",
    "title": "Sentiment Analysis on the News to Improve Mental Health",
    "authors": [
      "Saurav Kumar",
      "Rushil Jayant",
      "Nihaar Charagulla"
    ],
    "author_ids": [],
    "abstract": "The popularization of the internet created a revitalized digital media. With\nmonetization driven by clicks, journalists have reprioritized their content for\nthe highly competitive atmosphere of online news. The resulting negativity bias\nis harmful and can lead to anxiety and mood disturbance. We utilized a pipeline\nof 4 sentiment analysis models trained on various datasets - using Sequential,\nLSTM, BERT, and SVM models. When combined, the application, a mobile app,\nsolely displays uplifting and inspiring stories for users to read. Results have\nbeen successful - 1,300 users rate the app at 4.9 stars, and 85% report\nimproved mental health by using it.",
    "published_date": "2021-08-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.07706v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.02741v2",
    "title": "GIFAIR-FL: A Framework for Group and Individual Fairness in Federated Learning",
    "authors": [
      "Xubo Yue",
      "Maher Nouiehed",
      "Raed Al Kontar"
    ],
    "author_ids": [],
    "abstract": "In this paper we propose \\texttt{GIFAIR-FL}: a framework that imposes\n\\textbf{G}roup and \\textbf{I}ndividual \\textbf{FAIR}ness to \\textbf{F}ederated\n\\textbf{L}earning settings. By adding a regularization term, our algorithm\npenalizes the spread in the loss of client groups to drive the optimizer to\nfair solutions. Our framework \\texttt{GIFAIR-FL} can accommodate both global\nand personalized settings. Theoretically, we show convergence in non-convex and\nstrongly convex settings. Our convergence guarantees hold for both $i.i.d.$ and\nnon-$i.i.d.$ data. To demonstrate the empirical performance of our algorithm,\nwe apply our method to image classification and text prediction tasks. Compared\nto existing algorithms, our method shows improved fairness results while\nretaining superior or similar prediction accuracy.",
    "published_date": "2021-08-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.02741v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.02707v3",
    "title": "Fairness Properties of Face Recognition and Obfuscation Systems",
    "authors": [
      "Harrison Rosenberg",
      "Brian Tang",
      "Kassem Fawaz",
      "Somesh Jha"
    ],
    "author_ids": [],
    "abstract": "The proliferation of automated face recognition in the commercial and\ngovernment sectors has caused significant privacy concerns for individuals. One\napproach to address these privacy concerns is to employ evasion attacks against\nthe metric embedding networks powering face recognition systems: Face\nobfuscation systems generate imperceptibly perturbed images that cause face\nrecognition systems to misidentify the user. Perturbed faces are generated on\nmetric embedding networks, which are known to be unfair in the context of face\nrecognition. A question of demographic fairness naturally follows: are there\ndemographic disparities in face obfuscation system performance? We answer this\nquestion with an analytical and empirical exploration of recent face\nobfuscation systems. Metric embedding networks are found to be demographically\naware: face embeddings are clustered by demographic. We show how this\nclustering behavior leads to reduced face obfuscation utility for faces in\nminority groups. An intuitive analytical model yields insight into these\nphenomena.",
    "published_date": "2021-08-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.02707v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.02662v1",
    "title": "Reducing Unintended Bias of ML Models on Tabular and Textual Data",
    "authors": [
      "Guilherme Alves",
      "Maxime Amblard",
      "Fabien Bernier",
      "Miguel Couceiro",
      "Amedeo Napoli"
    ],
    "author_ids": [],
    "abstract": "Unintended biases in machine learning (ML) models are among the major\nconcerns that must be addressed to maintain public trust in ML. In this paper,\nwe address process fairness of ML models that consists in reducing the\ndependence of models on sensitive features, without compromising their\nperformance. We revisit the framework FixOut that is inspired in the approach\n\"fairness through unawareness\" to build fairer models. We introduce several\nimprovements such as automating the choice of FixOut's parameters. Also, FixOut\nwas originally proposed to improve fairness of ML models on tabular data. We\nalso demonstrate the feasibility of FixOut's workflow for models on textual\ndata. We present several experimental results that illustrate the fact that\nFixOut improves process fairness on different classification settings.",
    "published_date": "2021-08-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "I.2.0; J.1; J.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.02662v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.07700v1",
    "title": "In Oxford Handbook on AI Governance: The Role of Workers in AI Ethics and Governance",
    "authors": [
      "Nataliya Nedzhvetskaya",
      "JS Tan"
    ],
    "author_ids": [],
    "abstract": "While the role of states, corporations, and international organizations in AI\ngovernance has been extensively theorized, the role of workers has received\ncomparatively little attention. This chapter looks at the role that workers\nplay in identifying and mitigating harms from AI technologies. Harms are the\ncausally assessed impacts of technologies. They arise despite technical\nreliability and are not a result of technical negligence but rather of\nnormative uncertainty around questions of safety and fairness in complex social\nsystems. There is high consensus in the AI ethics community on the benefits of\nreducing harms but less consensus on mechanisms for determining or addressing\nharms. This lack of consensus has resulted in a number of collective actions by\nworkers protesting how harms are identified and addressed in their workplace.\nWe theorize the role of workers within AI governance and construct a model of\nharm reporting processes in AI workplaces. The harm reporting process involves\nthree steps, identification, the governance decision, and the response. Workers\ndraw upon three types of claims to argue for jurisdiction over questions of AI\ngovernance, subjection, control over the product of labor, and proximate\nknowledge of systems. Examining the past decade of AI related worker activism\nallows us to understand how different types of workers are positioned within a\nworkplace that produces AI systems, how their position informs their claims,\nand the place of collective action in staking their claims. This chapter argues\nthat workers occupy a unique role in identifying and mitigating harms caused by\nAI systems.",
    "published_date": "2021-08-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.07700v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.02501v3",
    "title": "Locally Interpretable One-Class Anomaly Detection for Credit Card Fraud Detection",
    "authors": [
      "Tungyu Wu",
      "Youting Wang"
    ],
    "author_ids": [],
    "abstract": "For the highly imbalanced credit card fraud detection problem, most existing\nmethods either use data augmentation methods or conventional machine learning\nmodels, while neural network-based anomaly detection approaches are lacking.\nFurthermore, few studies have employed AI interpretability tools to investigate\nthe feature importance of transaction data, which is crucial for the black-box\nfraud detection module. Considering these two points together, we propose a\nnovel anomaly detection framework for credit card fraud detection as well as a\nmodel-explaining module responsible for prediction explanations. The fraud\ndetection model is composed of two deep neural networks, which are trained in\nan unsupervised and adversarial manner. Precisely, the generator is an\nAutoEncoder aiming to reconstruct genuine transaction data, while the\ndiscriminator is a fully-connected network for fraud detection. The explanation\nmodule has three white-box explainers in charge of interpretations of the\nAutoEncoder, discriminator, and the whole detection model, respectively.\nExperimental results show the state-of-the-art performances of our fraud\ndetection model on the benchmark dataset compared with baselines. In addition,\nprediction analyses by three explainers are presented, offering a clear\nperspective on how each feature of an instance of interest contributes to the\nfinal model output.",
    "published_date": "2021-08-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.02501v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.04035v2",
    "title": "Mixture of Linear Models Co-supervised by Deep Neural Networks",
    "authors": [
      "Beomseok Seo",
      "Lin Lin",
      "Jia Li"
    ],
    "author_ids": [],
    "abstract": "Deep neural network (DNN) models have achieved phenomenal success for\napplications in many domains, ranging from academic research in science and\nengineering to industry and business. The modeling power of DNN is believed to\nhave come from the complexity and over-parameterization of the model, which on\nthe other hand has been criticized for the lack of interpretation. Although\ncertainly not true for every application, in some applications, especially in\neconomics, social science, healthcare industry, and administrative decision\nmaking, scientists or practitioners are resistant to use predictions made by a\nblack-box system for multiple reasons. One reason is that a major purpose of a\nstudy can be to make discoveries based upon the prediction function, e.g., to\nreveal the relationships between measurements. Another reason can be that the\ntraining dataset is not large enough to make researchers feel completely sure\nabout a purely data-driven result. Being able to examine and interpret the\nprediction function will enable researchers to connect the result with existing\nknowledge or gain insights about new directions to explore. Although classic\nstatistical models are much more explainable, their accuracy often falls\nconsiderably below DNN. In this paper, we propose an approach to fill the gap\nbetween relatively simple explainable models and DNN such that we can more\nflexibly tune the trade-off between interpretability and accuracy. Our main\nidea is a mixture of discriminative models that is trained with the guidance\nfrom a DNN. Although mixtures of discriminative models have been studied\nbefore, our way of generating the mixture is quite different.",
    "published_date": "2021-08-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.04035v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.02234v5",
    "title": "Multi-Branch with Attention Network for Hand-Based Person Recognition",
    "authors": [
      "Nathanael L. Baisa",
      "Bryan Williams",
      "Hossein Rahmani",
      "Plamen Angelov",
      "Sue Black"
    ],
    "author_ids": [],
    "abstract": "In this paper, we propose a novel hand-based person recognition method for\nthe purpose of criminal investigations since the hand image is often the only\navailable information in cases of serious crime such as sexual abuse. Our\nproposed method, Multi-Branch with Attention Network (MBA-Net), incorporates\nboth channel and spatial attention modules in branches in addition to a global\n(without attention) branch to capture global structural information for\ndiscriminative feature learning. The attention modules focus on the relevant\nfeatures of the hand image while suppressing the irrelevant backgrounds. In\norder to overcome the weakness of the attention mechanisms, equivariant to\npixel shuffling, we integrate relative positional encodings into the spatial\nattention module to capture the spatial positions of pixels. Extensive\nevaluations on two large multi-ethnic and publicly available hand datasets\ndemonstrate that our proposed method achieves state-of-the-art performance,\nsurpassing the existing hand-based identification methods.",
    "published_date": "2021-08-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.02234v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.02214v2",
    "title": "A FAIR and AI-ready Higgs boson decay dataset",
    "authors": [
      "Yifan Chen",
      "E. A. Huerta",
      "Javier Duarte",
      "Philip Harris",
      "Daniel S. Katz",
      "Mark S. Neubauer",
      "Daniel Diaz",
      "Farouk Mokhtar",
      "Raghav Kansal",
      "Sang Eon Park",
      "Volodymyr V. Kindratenko",
      "Zhizhen Zhao",
      "Roger Rusack"
    ],
    "author_ids": [],
    "abstract": "To enable the reusability of massive scientific datasets by humans and\nmachines, researchers aim to adhere to the principles of findability,\naccessibility, interoperability, and reusability (FAIR) for data and artificial\nintelligence (AI) models. This article provides a domain-agnostic, step-by-step\nassessment guide to evaluate whether or not a given dataset meets these\nprinciples. We demonstrate how to use this guide to evaluate the FAIRness of an\nopen simulated dataset produced by the CMS Collaboration at the CERN Large\nHadron Collider. This dataset consists of Higgs boson decays and quark and\ngluon background, and is available through the CERN Open Data Portal. We use\nadditional available tools to assess the FAIRness of this dataset, and\nincorporate feedback from members of the FAIR community to validate our\nresults. This article is accompanied by a Jupyter notebook to visualize and\nexplore this dataset. This study marks the first in a planned series of\narticles that will guide scientists in the creation of FAIR AI models and\ndatasets in high energy particle physics.",
    "published_date": "2021-08-04T00:00:00",
    "year": 2021,
    "categories": [
      "hep-ex",
      "cs.AI",
      "cs.DB",
      "hep-ph",
      "I.2; J.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.02214v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.02137v1",
    "title": "Under the Radar -- Auditing Fairness in ML for Humanitarian Mapping",
    "authors": [
      "Lukas Kondmann",
      "Xiao Xiang Zhu"
    ],
    "author_ids": [],
    "abstract": "Humanitarian mapping from space with machine learning helps policy-makers to\ntimely and accurately identify people in need. However, recent concerns around\nfairness and transparency of algorithmic decision-making are a significant\nobstacle for applying these methods in practice. In this paper, we study if\nhumanitarian mapping approaches from space are prone to bias in their\npredictions. We map village-level poverty and electricity rates in India based\non nighttime lights (NTLs) with linear regression and random forest and analyze\nif the predictions systematically show prejudice against scheduled caste or\ntribe communities. To achieve this, we design a causal approach to measure\ncounterfactual fairness based on propensity score matching. This allows to\ncompare villages within a community of interest to synthetic counterfactuals.\nOur findings indicate that poverty is systematically overestimated and\nelectricity systematically underestimated for scheduled tribes in comparison to\na synthetic counterfactual group of villages. The effects have the opposite\ndirection for scheduled castes where poverty is underestimated and\nelectrification overestimated. These results are a warning sign for a variety\nof applications in humanitarian mapping where fairness issues would compromise\npolicy goals.",
    "published_date": "2021-08-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.02137v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.04134v1",
    "title": "Fairness in Algorithmic Profiling: A German Case Study",
    "authors": [
      "Christoph Kern",
      "Ruben L. Bach",
      "Hannah Mautner",
      "Frauke Kreuter"
    ],
    "author_ids": [],
    "abstract": "Algorithmic profiling is increasingly used in the public sector as a means to\nallocate limited public resources effectively and objectively. One example is\nthe prediction-based statistical profiling of job seekers to guide the\nallocation of support measures by public employment services. However,\nempirical evaluations of potential side-effects such as unintended\ndiscrimination and fairness concerns are rare. In this study, we compare and\nevaluate statistical models for predicting job seekers' risk of becoming\nlong-term unemployed with respect to prediction performance, fairness metrics,\nand vulnerabilities to data analysis decisions. Focusing on Germany as a use\ncase, we evaluate profiling models under realistic conditions by utilizing\nadministrative data on job seekers' employment histories that are routinely\ncollected by German public employment services. Besides showing that these data\ncan be used to predict long-term unemployment with competitive levels of\naccuracy, we highlight that different classification policies have very\ndifferent fairness implications. We therefore call for rigorous auditing\nprocesses before such models are put to practice.",
    "published_date": "2021-08-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.LG",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.04134v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.01987v1",
    "title": "Core-Stable Committees under Restricted Domains",
    "authors": [
      "Grzegorz Pierczyński",
      "Piotr Skowron"
    ],
    "author_ids": [],
    "abstract": "We study the setting of committee elections, where a group of individuals\nneeds to collectively select a given size subset of available objects. This\nmodel is relevant for a number of real-life scenarios including political\nelections, participatory budgeting, and facility-location. We focus on the core\n-- the classic notion of proportionality, stability and fairness. We show that\nfor a number of restricted domains including voter-interval,\ncandidate-interval, single-peaked, and single-crossing preferences the core is\nnon-empty and can be found in polynomial time. We show that the core might be\nempty for strict top-monotonic preferences, yet we introduce a relaxation of\nthis class, which guarantees non-emptiness of the core. Our algorithms work\nboth in the randomized and discrete models. We also show that the classic known\nproportional rules do not return committees from the core even for the most\nrestrictive domains among those we consider (in particular for 1D-Euclidean\npreferences). We additionally prove a number of structural results that give\nbetter insights into the nature of some of the restricted domains, and which in\nparticular give a better intuitive understanding of the class of top-monotonic\npreferences.",
    "published_date": "2021-08-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.01987v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.01764v1",
    "title": "Q-Pain: A Question Answering Dataset to Measure Social Bias in Pain Management",
    "authors": [
      "Cécile Logé",
      "Emily Ross",
      "David Yaw Amoah Dadey",
      "Saahil Jain",
      "Adriel Saporta",
      "Andrew Y. Ng",
      "Pranav Rajpurkar"
    ],
    "author_ids": [],
    "abstract": "Recent advances in Natural Language Processing (NLP), and specifically\nautomated Question Answering (QA) systems, have demonstrated both impressive\nlinguistic fluency and a pernicious tendency to reflect social biases. In this\nstudy, we introduce Q-Pain, a dataset for assessing bias in medical QA in the\ncontext of pain management, one of the most challenging forms of clinical\ndecision-making. Along with the dataset, we propose a new, rigorous framework,\nincluding a sample experimental design, to measure the potential biases present\nwhen making treatment decisions. We demonstrate its use by assessing two\nreference Question-Answering systems, GPT-2 and GPT-3, and find statistically\nsignificant differences in treatment between intersectional race-gender\nsubgroups, thus reaffirming the risks posed by AI in medical settings, and the\nneed for datasets like ours to ensure safety before medical AI applications are\ndeployed.",
    "published_date": "2021-08-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.01764v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.01721v1",
    "title": "Improving Counterfactual Generation for Fair Hate Speech Detection",
    "authors": [
      "Aida Mostafazadeh Davani",
      "Ali Omrani",
      "Brendan Kennedy",
      "Mohammad Atari",
      "Xiang Ren",
      "Morteza Dehghani"
    ],
    "author_ids": [],
    "abstract": "Bias mitigation approaches reduce models' dependence on sensitive features of\ndata, such as social group tokens (SGTs), resulting in equal predictions across\nthe sensitive features. In hate speech detection, however, equalizing model\npredictions may ignore important differences among targeted social groups, as\nhate speech can contain stereotypical language specific to each SGT. Here, to\ntake the specific language about each SGT into account, we rely on\ncounterfactual fairness and equalize predictions among counterfactuals,\ngenerated by changing the SGTs. Our method evaluates the similarity in sentence\nlikelihoods (via pre-trained language models) among counterfactuals, to treat\nSGTs equally only within interchangeable contexts. By applying logit pairing to\nequalize outcomes on the restricted set of counterfactuals for each instance,\nwe improve fairness metrics while preserving model performance on hate speech\ndetection.",
    "published_date": "2021-08-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.01721v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.07699v1",
    "title": "Digital Divide: Mapping the geodemographics of internet accessibility across Great Britain",
    "authors": [
      "Claire Powell",
      "Luke Burns"
    ],
    "author_ids": [],
    "abstract": "Aim: This research proposes the first solely sociodemographic measure of\ndigital accessibility for Great Britain. Digital inaccessibility affects circa\n10 million people who are unable to access or make full use of the internet,\nparticularly impacting the disadvantaged in society. Method: A geodemographic\nclassification is developed, analysing literature-guided sociodemographic\nvariables at the district level. Analysis: Resultant clusters are analysed\nagainst their sociodemographic variables and spatial extent. Findings suggest\nthree at-risk clusters exist, \"Metropolitan Minority Struggle\", \"Indian\nMetropolitan Living\" and \"Pakistani-Bangladeshi Inequality\". These are\nvalidated through nationwide Ofcom telecommunications performance data and\nspecific case studies using Office for National Statistics internet usage data.\nConclusion: Using solely contemporary and open-source sociodemographic\nvariables, this paper enhances previous digital accessibility research. The\nidentification of digitally inaccessible areas allows focussed local and\nnational government resource and policy targeting, particularly important as a\nkey data source and methodology post-2021, following the expected final\nnationwide census.",
    "published_date": "2021-08-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.07699v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.01250v3",
    "title": "Your fairness may vary: Pretrained language model fairness in toxic text classification",
    "authors": [
      "Ioana Baldini",
      "Dennis Wei",
      "Karthikeyan Natesan Ramamurthy",
      "Mikhail Yurochkin",
      "Moninder Singh"
    ],
    "author_ids": [],
    "abstract": "The popularity of pretrained language models in natural language processing\nsystems calls for a careful evaluation of such models in down-stream tasks,\nwhich have a higher potential for societal impact. The evaluation of such\nsystems usually focuses on accuracy measures. Our findings in this paper call\nfor attention to be paid to fairness measures as well. Through the analysis of\nmore than a dozen pretrained language models of varying sizes on two toxic text\nclassification tasks (English), we demonstrate that focusing on accuracy\nmeasures alone can lead to models with wide variation in fairness\ncharacteristics. Specifically, we observe that fairness can vary even more than\naccuracy with increasing training data size and different random\ninitializations. At the same time, we find that little of the fairness\nvariation is explained by model size, despite claims in the literature. To\nimprove model fairness without retraining, we show that two post-processing\nmethods developed for structured, tabular data can be successfully applied to a\nrange of pretrained language models. Warning: This paper contains samples of\noffensive text.",
    "published_date": "2021-08-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.01250v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.01174v1",
    "title": "Knowledge-intensive Language Understanding for Explainable AI",
    "authors": [
      "Amit Sheth",
      "Manas Gaur",
      "Kaushik Roy",
      "Keyur Faldu"
    ],
    "author_ids": [],
    "abstract": "AI systems have seen significant adoption in various domains. At the same\ntime, further adoption in some domains is hindered by inability to fully trust\nan AI system that it will not harm a human. Besides the concerns for fairness,\nprivacy, transparency, and explainability are key to developing trusts in AI\nsystems. As stated in describing trustworthy AI \"Trust comes through\nunderstanding. How AI-led decisions are made and what determining factors were\nincluded are crucial to understand.\" The subarea of explaining AI systems has\ncome to be known as XAI. Multiple aspects of an AI system can be explained;\nthese include biases that the data might have, lack of data points in a\nparticular region of the example space, fairness of gathering the data, feature\nimportances, etc. However, besides these, it is critical to have human-centered\nexplanations that are directly related to decision-making similar to how a\ndomain expert makes decisions based on \"domain knowledge,\" that also include\nwell-established, peer-validated explicit guidelines. To understand and\nvalidate an AI system's outcomes (such as classification, recommendations,\npredictions), that lead to developing trust in the AI system, it is necessary\nto involve explicit domain knowledge that humans understand and use.",
    "published_date": "2021-08-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.01174v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.07354v1",
    "title": "Private Delivery Networks -- Extended Abstract",
    "authors": [
      "Alex Berke",
      "Nicolas Lee",
      "Patrick Chwalek"
    ],
    "author_ids": [],
    "abstract": "The past decade has seen tremendous shifts in how people live, work, and buy\ngoods, with an increased reliance on e-commerce and deliveries. Purchase\nhistories generated through e-commerce can be highly personal, revealing\nidentifying information about individuals and households. Constructing profiles\nfrom these data allows for the targeting of individuals and communities through\npractices such as targeted marketing and information campaigns. Furthermore,\nwhen purchase profiles are connected with delivery addresses, these data can\nmeasure the demographics of a local community and allow for individualized\ntargeting to reach beyond the digital realm to the physical one. Events that\naccelerated shifts towards e-commerce, such as an infectious disease epidemic,\nhave also widened equity gaps. This work is about alternative e-commerce\ndelivery network models that address both rising privacy and wealth inequality\nconcerns. This includes strategies that mask and add noise to purchase\nhistories, and allow people to \"buy privacy\" through charitable contributions.",
    "published_date": "2021-08-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.07354v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.00807v3",
    "title": "Application of Blockchain in Healthcare and Health Insurance Sector",
    "authors": [
      "Debendranath Das"
    ],
    "author_ids": [],
    "abstract": "Technology has evolved over the years, making our lives easier. It has\nimpacted the healthcare sector, increasing the average life expectancy of human\nbeings. Still, there are gaps that remain unaddressed. There is a lack of\ntransparency in the healthcare system, which results in inherent trust problems\nbetween patients and hospitals. In the present day, a patient does not know\nwhether he or she will get the proper treatment from the hospital for the fee\ncharged. A patient can claim reimbursement of the medical bill from any\ninsurance company. However, today there is minimal scope for the Insurance\nCompany to verify the validity of such bills or medical records. A patient can\nprovide fake details to get financial benefits from the insurance company.\nAgain, there are trust issues between the patient (i.e., the insurance claimer)\nand the insurance company. Blockchain integrated with the smart contract is a\nwell-known disruptive technology that builds trust by providing transparency to\nthe system. In this paper, we propose a blockchain-enabled Secure and Smart\nHealthCare System. Fairness of all the entities: patient, hospital, or\ninsurance company involved in the system is guaranteed with no one trusting\neach other. Privacy and security of patients' medical data are ensured as well.\nWe also propose a method for privacy-preserving sharing of aggregated data with\nthe research community for their own purpose. Shared data must not be\npersonally identifiable, i.e, no one can link the acquired data to the identity\nof any patient or their medical history. We have implemented the prototype in\nthe Ethereum platform and Ropsten test network, and have included the analysis\nas well.",
    "published_date": "2021-08-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.00807v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.00768v1",
    "title": "Cross-cultural Mood Perception in Pop Songs and its Alignment with Mood Detection Algorithms",
    "authors": [
      "Harin Lee",
      "Frank Hoeger",
      "Marc Schoenwiesner",
      "Minsu Park",
      "Nori Jacoby"
    ],
    "author_ids": [],
    "abstract": "Do people from different cultural backgrounds perceive the mood in music the\nsame way? How closely do human ratings across different cultures approximate\nautomatic mood detection algorithms that are often trained on corpora of\npredominantly Western popular music? Analyzing 166 participants responses from\nBrazil, South Korea, and the US, we examined the similarity between the ratings\nof nine categories of perceived moods in music and estimated their alignment\nwith four popular mood detection algorithms. We created a dataset of 360 recent\npop songs drawn from major music charts of the countries and constructed\nsemantically identical mood descriptors across English, Korean, and Portuguese\nlanguages. Multiple participants from the three countries rated their\nfamiliarity, preference, and perceived moods for a given song. Ratings were\nhighly similar within and across cultures for basic mood attributes such as\nsad, cheerful, and energetic. However, we found significant cross-cultural\ndifferences for more complex characteristics such as dreamy and love. To our\nsurprise, the results of mood detection algorithms were uniformly correlated\nacross human ratings from all three countries and did not show a detectable\nbias towards any particular culture. Our study thus suggests that the mood\ndetection algorithms can be considered as an objective measure at least within\nthe popular music context.",
    "published_date": "2021-08-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "cs.SD",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.00768v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.00713v2",
    "title": "Cohort Bias Adaptation in Aggregated Datasets for Lesion Segmentation",
    "authors": [
      "Brennan Nichyporuk",
      "Jillian Cardinell",
      "Justin Szeto",
      "Raghav Mehta",
      "Sotirios Tsaftaris",
      "Douglas L. Arnold",
      "Tal Arbel"
    ],
    "author_ids": [],
    "abstract": "Many automatic machine learning models developed for focal pathology (e.g.\nlesions, tumours) detection and segmentation perform well, but do not\ngeneralize as well to new patient cohorts, impeding their widespread adoption\ninto real clinical contexts. One strategy to create a more diverse,\ngeneralizable training set is to naively pool datasets from different cohorts.\nSurprisingly, training on this \\it{big data} does not necessarily increase, and\nmay even reduce, overall performance and model generalizability, due to the\nexistence of cohort biases that affect label distributions. In this paper, we\npropose a generalized affine conditioning framework to learn and account for\ncohort biases across multi-source datasets, which we call Source-Conditioned\nInstance Normalization (SCIN). Through extensive experimentation on three\ndifferent, large scale, multi-scanner, multi-centre Multiple Sclerosis (MS)\nclinical trial MRI datasets, we show that our cohort bias adaptation method (1)\nimproves performance of the network on pooled datasets relative to naively\npooling datasets and (2) can quickly adapt to a new cohort by fine-tuning the\ninstance normalization parameters, thus learning the new cohort bias with only\n10 labelled samples.",
    "published_date": "2021-08-02T00:00:00",
    "year": 2021,
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.00713v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.00682v2",
    "title": "Asymptotic bias of inexact Markov Chain Monte Carlo methods in high dimension",
    "authors": [
      "Alain Oliviero Durmus",
      "Andreas Eberle"
    ],
    "author_ids": [],
    "abstract": "Inexact Markov Chain Monte Carlo methods rely on Markov chains that do not\nexactly preserve the target distribution. Examples include the unadjusted\nLangevin algorithm (ULA) and unadjusted Hamiltonian Monte Carlo (uHMC). This\npaper establishes bounds on Wasserstein distances between the invariant\nprobability measures of inexact MCMC methods and their target distributions\nwith a focus on understanding the precise dependence of this asymptotic bias on\nboth dimension and discretization step size. Assuming Wasserstein bounds on the\nconvergence to equilibrium of either the exact or the approximate dynamics, we\nshow that for both ULA and uHMC, the asymptotic bias depends on key quantities\nrelated to the target distribution or the stationary probability measure of the\nscheme. As a corollary, we conclude that for models with a limited amount of\ninteractions such as mean-field models, finite range graphical models, and\nperturbations thereof, the asymptotic bias has a similar dependence on the step\nsize and the dimension as for product measures.",
    "published_date": "2021-08-02T00:00:00",
    "year": 2021,
    "categories": [
      "math.PR",
      "cs.NA",
      "math.NA",
      "stat.CO",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.00682v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2108.00574v1",
    "title": "Ab-initio experimental violation of Bell inequalities",
    "authors": [
      "Davide Poderini",
      "Emanuele Polino",
      "Giovanni Rodari",
      "Alessia Suprano",
      "Rafael Chaves",
      "Fabio Sciarrino"
    ],
    "author_ids": [],
    "abstract": "The violation of a Bell inequality is the paradigmatic example of\ndevice-independent quantum information: the nonclassicality of the data is\ncertified without the knowledge of the functioning of devices. In practice,\nhowever, all Bell experiments rely on the precise understanding of the\nunderlying physical mechanisms. Given that, it is natural to ask: Can one\nwitness nonclassical behaviour in a truly black-box scenario? Here we propose\nand implement, computationally and experimentally, a solution to this ab-initio\ntask. It exploits a robust automated optimization approach based on the\nStochastic Nelder-Mead algorithm. Treating preparation and measurement devices\nas black-boxes, and relying on the observed statistics only, our adaptive\nprotocol approaches the optimal Bell inequality violation after a limited\nnumber of iterations for a variety photonic states, measurement responses and\nBell scenarios. In particular, we exploit it for randomness certification from\nunknown states and measurements. Our results demonstrate the power of automated\nalgorithms, opening a new venue for the experimental implementation of\ndevice-independent quantum technologies.",
    "published_date": "2021-08-02T00:00:00",
    "year": 2021,
    "categories": [
      "quant-ph",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.00574v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.00295v2",
    "title": "Fair Representation Learning using Interpolation Enabled Disentanglement",
    "authors": [
      "Akshita Jha",
      "Bhanukiran Vinzamuri",
      "Chandan K. Reddy"
    ],
    "author_ids": [],
    "abstract": "With the growing interest in the machine learning community to solve\nreal-world problems, it has become crucial to uncover the hidden reasoning\nbehind their decisions by focusing on the fairness and auditing the predictions\nmade by these black-box models. In this paper, we propose a novel method to\naddress two key issues: (a) Can we simultaneously learn fair disentangled\nrepresentations while ensuring the utility of the learned representation for\ndownstream tasks, and (b)Can we provide theoretical insights into when the\nproposed approach will be both fair and accurate. To address the former, we\npropose the method FRIED, Fair Representation learning using Interpolation\nEnabled Disentanglement. In our architecture, by imposing a critic-based\nadversarial framework, we enforce the interpolated points in the latent space\nto be more realistic. This helps in capturing the data manifold effectively and\nenhances the utility of the learned representation for downstream prediction\ntasks. We address the latter question by developing a theory on\nfairness-accuracy trade-offs using classifier-based conditional mutual\ninformation estimation. We demonstrate the effectiveness of FRIED on datasets\nof different modalities - tabular, text, and image datasets. We observe that\nthe representations learned by FRIED are overall fairer in comparison to\nexisting baselines and also accurate for downstream prediction tasks.\nAdditionally, we evaluate FRIED on a real-world healthcare claims dataset where\nwe conduct an expert aided model auditing study providing useful insights into\nopioid ad-diction patterns.",
    "published_date": "2021-07-31T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.00295v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.00250v1",
    "title": "Bayesian analysis of the prevalence bias: learning and predicting from imbalanced data",
    "authors": [
      "Loic Le Folgoc",
      "Vasileios Baltatzis",
      "Amir Alansary",
      "Sujal Desai",
      "Anand Devaraj",
      "Sam Ellis",
      "Octavio E. Martinez Manzanera",
      "Fahdi Kanavati",
      "Arjun Nair",
      "Julia Schnabel",
      "Ben Glocker"
    ],
    "author_ids": [],
    "abstract": "Datasets are rarely a realistic approximation of the target population. Say,\nprevalence is misrepresented, image quality is above clinical standards, etc.\nThis mismatch is known as sampling bias. Sampling biases are a major hindrance\nfor machine learning models. They cause significant gaps between model\nperformance in the lab and in the real world. Our work is a solution to\nprevalence bias. Prevalence bias is the discrepancy between the prevalence of a\npathology and its sampling rate in the training dataset, introduced upon\ncollecting data or due to the practioner rebalancing the training batches. This\npaper lays the theoretical and computational framework for training models, and\nfor prediction, in the presence of prevalence bias. Concretely a bias-corrected\nloss function, as well as bias-corrected predictive rules, are derived under\nthe principles of Bayesian risk minimization. The loss exhibits a direct\nconnection to the information gain. It offers a principled alternative to\nheuristic training losses and complements test-time procedures based on\nselecting an operating point from summary curves. It integrates seamlessly in\nthe current paradigm of (deep) learning using stochastic backpropagation and\nnaturally with Bayesian models.",
    "published_date": "2021-07-31T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "q-bio.QM",
      "stat.AP",
      "stat.ME",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.00250v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.00117v2",
    "title": "Margin-Aware Intra-Class Novelty Identification for Medical Images",
    "authors": [
      "Xiaoyuan Guo",
      "Judy Wawira Gichoya",
      "Saptarshi Purkayastha",
      "Imon Banerjee"
    ],
    "author_ids": [],
    "abstract": "Traditional anomaly detection methods focus on detecting inter-class\nvariations while medical image novelty identification is inherently an\nintra-class detection problem. For example, a machine learning model trained\nwith normal chest X-ray and common lung abnormalities, is expected to discover\nand flag idiopathic pulmonary fibrosis which a rare lung disease and unseen by\nthe model during training. The nuances from intra-class variations and lack of\nrelevant training data in medical image analysis pose great challenges for\nexisting anomaly detection methods. To tackle the challenges, we propose a\nhybrid model - Transformation-based Embedding learning for Novelty Detection\n(TEND) which without any out-of-distribution training data, performs novelty\nidentification by combining both autoencoder-based and classifier-based method.\nWith a pre-trained autoencoder as image feature extractor, TEND learns to\ndiscriminate the feature embeddings of in-distribution data from the\ntransformed counterparts as fake out-of-distribution inputs. To enhance the\nseparation, a distance objective is optimized to enforce a margin between the\ntwo classes. Extensive experimental results on both natural image datasets and\nmedical image datasets are presented and our method out-performs\nstate-of-the-art approaches.",
    "published_date": "2021-07-31T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.00117v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.00071v1",
    "title": "Foundations of data imbalance and solutions for a data democracy",
    "authors": [
      "Ajay Kulkarni",
      "Deri Chong",
      "Feras A. Batarseh"
    ],
    "author_ids": [],
    "abstract": "Dealing with imbalanced data is a prevalent problem while performing\nclassification on the datasets. Many times, this problem contributes to bias\nwhile making decisions or implementing policies. Thus, it is vital to\nunderstand the factors which cause imbalance in the data (or class imbalance).\nSuch hidden biases and imbalances can lead to data tyranny and a major\nchallenge to a data democracy. In this chapter, two essential statistical\nelements are resolved: the degree of class imbalance and the complexity of the\nconcept; solving such issues helps in building the foundations of a data\ndemocracy. Furthermore, statistical measures which are appropriate in these\nscenarios are discussed and implemented on a real-life dataset (car insurance\nclaims). In the end, popular data-level methods such as random oversampling,\nrandom undersampling, synthetic minority oversampling technique, Tomek link,\nand others are implemented in Python, and their performance is compared.",
    "published_date": "2021-07-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.00071v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.00049v2",
    "title": "Object-aware Contrastive Learning for Debiased Scene Representation",
    "authors": [
      "Sangwoo Mo",
      "Hyunwoo Kang",
      "Kihyuk Sohn",
      "Chun-Liang Li",
      "Jinwoo Shin"
    ],
    "author_ids": [],
    "abstract": "Contrastive self-supervised learning has shown impressive results in learning\nvisual representations from unlabeled images by enforcing invariance against\ndifferent data augmentations. However, the learned representations are often\ncontextually biased to the spurious scene correlations of different objects or\nobject and background, which may harm their generalization on the downstream\ntasks. To tackle the issue, we develop a novel object-aware contrastive\nlearning framework that first (a) localizes objects in a self-supervised manner\nand then (b) debias scene correlations via appropriate data augmentations\nconsidering the inferred object locations. For (a), we propose the contrastive\nclass activation map (ContraCAM), which finds the most discriminative regions\n(e.g., objects) in the image compared to the other images using the\ncontrastively trained models. We further improve the ContraCAM to detect\nmultiple objects and entire shapes via an iterative refinement procedure. For\n(b), we introduce two data augmentations based on ContraCAM, object-aware\nrandom crop and background mixup, which reduce contextual and background biases\nduring contrastive self-supervised learning, respectively. Our experiments\ndemonstrate the effectiveness of our representation learning framework,\nparticularly when trained under multi-object images or evaluated under the\nbackground (and distribution) shifted images.",
    "published_date": "2021-07-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.00049v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.00048v1",
    "title": "Controlling Weather Field Synthesis Using Variational Autoencoders",
    "authors": [
      "Dario Augusto Borges Oliveira",
      "Jorge Guevara Diaz",
      "Bianca Zadrozny",
      "Campbell Watson"
    ],
    "author_ids": [],
    "abstract": "One of the consequences of climate change is anobserved increase in the\nfrequency of extreme cli-mate events. That poses a challenge for\nweatherforecast and generation algorithms, which learnfrom historical data but\nshould embed an often un-certain bias to create correct scenarios. This\npaperinvestigates how mapping climate data to a knowndistribution using\nvariational autoencoders mighthelp explore such biases and control the\nsynthesisof weather fields towards more extreme climatescenarios. We\nexperimented using a monsoon-affected precipitation dataset from southwest\nIn-dia, which should give a roughly stable pattern ofrainy days and ease our\ninvestigation. We reportcompelling results showing that mapping complexweather\ndata to a known distribution implementsan efficient control for weather field\nsynthesis to-wards more (or less) extreme scenarios.",
    "published_date": "2021-07-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.00048v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.00046v3",
    "title": "On the finite element approximation of a semicoercive Stokes variational inequality arising in glaciology",
    "authors": [
      "Gonzalo G. de Diego",
      "Patrick E. Farrell",
      "Ian J. Hewitt"
    ],
    "author_ids": [],
    "abstract": "Stokes variational inequalities arise in the formulation of glaciological\nproblems involving contact. We consider the problem of a two-dimensional marine\nice sheet with a grounding line, although the analysis presented here is\nextendable to other contact problems in glaciology, such as that of subglacial\ncavitation. The analysis of this problem and its discretisation is complicated\nby the nonlinear rheology commonly used for modelling ice, the enforcement of a\nfriction boundary condition given by a power law, and the presence of rigid\nmodes in the velocity space, which render the variational inequality\nsemicoercive. In this work, we consider a mixed formulation of this variational\ninequality involving a Lagrange multiplier and provide an analysis of its\nfinite element approximation. Error estimates in the presence of rigid modes\nare obtained by means of a specially-built projection operator onto the\nsubspace of rigid modes and a Korn-type inequality. These proofs rely on the\nfact that the subspace of rigid modes is at most one-dimensional. Numerical\nresults are reported to validate the error estimates.",
    "published_date": "2021-07-30T00:00:00",
    "year": 2021,
    "categories": [
      "math.NA",
      "cs.NA",
      "65N12, 65N15, 65N30, 86A40"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.00046v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2107.14768v1",
    "title": "Debiased Explainable Pairwise Ranking from Implicit Feedback",
    "authors": [
      "Khalil Damak",
      "Sami Khenissi",
      "Olfa Nasraoui"
    ],
    "author_ids": [],
    "abstract": "Recent work in recommender systems has emphasized the importance of fairness,\nwith a particular interest in bias and transparency, in addition to predictive\naccuracy. In this paper, we focus on the state of the art pairwise ranking\nmodel, Bayesian Personalized Ranking (BPR), which has previously been found to\noutperform pointwise models in predictive accuracy, while also being able to\nhandle implicit feedback. Specifically, we address two limitations of BPR: (1)\nBPR is a black box model that does not explain its outputs, thus limiting the\nuser's trust in the recommendations, and the analyst's ability to scrutinize a\nmodel's outputs; and (2) BPR is vulnerable to exposure bias due to the data\nbeing Missing Not At Random (MNAR). This exposure bias usually translates into\nan unfairness against the least popular items because they risk being\nunder-exposed by the recommender system. In this work, we first propose a novel\nexplainable loss function and a corresponding Matrix Factorization-based model\ncalled Explainable Bayesian Personalized Ranking (EBPR) that generates\nrecommendations along with item-based explanations. Then, we theoretically\nquantify additional exposure bias resulting from the explainability, and use it\nas a basis to propose an unbiased estimator for the ideal EBPR loss. The result\nis a ranking model that aptly captures both debiased and explainable user\npreferences. Finally, we perform an empirical study on three real-world\ndatasets that demonstrate the advantages of our proposed models.",
    "published_date": "2021-07-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.14768v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.14653v1",
    "title": "DadaGP: A Dataset of Tokenized GuitarPro Songs for Sequence Models",
    "authors": [
      "Pedro Sarmento",
      "Adarsh Kumar",
      "CJ Carr",
      "Zack Zukowski",
      "Mathieu Barthet",
      "Yi-Hsuan Yang"
    ],
    "author_ids": [],
    "abstract": "Originating in the Renaissance and burgeoning in the digital era, tablatures\nare a commonly used music notation system which provides explicit\nrepresentations of instrument fingerings rather than pitches. GuitarPro has\nestablished itself as a widely used tablature format and software enabling\nmusicians to edit and share songs for musical practice, learning, and\ncomposition. In this work, we present DadaGP, a new symbolic music dataset\ncomprising 26,181 song scores in the GuitarPro format covering 739 musical\ngenres, along with an accompanying tokenized format well-suited for generative\nsequence models such as the Transformer. The tokenized format is inspired by\nevent-based MIDI encodings, often used in symbolic music generation models. The\ndataset is released with an encoder/decoder which converts GuitarPro files to\ntokens and back. We present results of a use case in which DadaGP is used to\ntrain a Transformer-based model to generate new songs in GuitarPro format. We\ndiscuss other relevant use cases for the dataset (guitar-bass transcription,\nmusic style transfer and artist/genre classification) as well as ethical\nimplications. DadaGP opens up the possibility to train GuitarPro score\ngenerators, fine-tune models on custom data, create new styles of music,\nAI-powered songwriting apps, and human-AI improvisation.",
    "published_date": "2021-07-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SD",
      "cs.LG",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.14653v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.14425v1",
    "title": "Enhancing Social Relation Inference with Concise Interaction Graph and Discriminative Scene Representation",
    "authors": [
      "Xiaotian Yu",
      "Hanling Yi",
      "Yi Yu",
      "Ling Xing",
      "Shiliang Zhang",
      "Xiaoyu Wang"
    ],
    "author_ids": [],
    "abstract": "There has been a recent surge of research interest in attacking the problem\nof social relation inference based on images. Existing works classify social\nrelations mainly by creating complicated graphs of human interactions, or\nlearning the foreground and/or background information of persons and objects,\nbut ignore holistic scene context. The holistic scene refers to the\nfunctionality of a place in images, such as dinning room, playground and\noffice. In this paper, by mimicking human understanding on images, we propose\nan approach of \\textbf{PR}actical \\textbf{I}nference in \\textbf{S}ocial\nr\\textbf{E}lation (PRISE), which concisely learns interactive features of\npersons and discriminative features of holistic scenes. Technically, we develop\na simple and fast relational graph convolutional network to capture interactive\nfeatures of all persons in one image. To learn the holistic scene feature, we\nelaborately design a contrastive learning task based on image scene\nclassification. To further boost the performance in social relation inference,\nwe collect and distribute a new large-scale dataset, which consists of about\n240 thousand unlabeled images. The extensive experimental results show that our\nnovel learning framework significantly beats the state-of-the-art methods,\ne.g., PRISE achieves 6.8$\\%$ improvement for domain classification in PIPA\ndataset.",
    "published_date": "2021-07-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.14425v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.14399v1",
    "title": "Self-Supervised Regional and Temporal Auxiliary Tasks for Facial Action Unit Recognition",
    "authors": [
      "Jingwei Yan",
      "Jingjing Wang",
      "Qiang Li",
      "Chunmao Wang",
      "Shiliang Pu"
    ],
    "author_ids": [],
    "abstract": "Automatic facial action unit (AU) recognition is a challenging task due to\nthe scarcity of manual annotations. To alleviate this problem, a large amount\nof efforts has been dedicated to exploiting various methods which leverage\nnumerous unlabeled data. However, many aspects with regard to some unique\nproperties of AUs, such as the regional and relational characteristics, are not\nsufficiently explored in previous works. Motivated by this, we take the AU\nproperties into consideration and propose two auxiliary AU related tasks to\nbridge the gap between limited annotations and the model performance in a\nself-supervised manner via the unlabeled data. Specifically, to enhance the\ndiscrimination of regional features with AU relation embedding, we design a\ntask of RoI inpainting to recover the randomly cropped AU patches. Meanwhile, a\nsingle image based optical flow estimation task is proposed to leverage the\ndynamic change of facial muscles and encode the motion information into the\nglobal feature representation. Based on these two self-supervised auxiliary\ntasks, local features, mutual relation and motion cues of AUs are better\ncaptured in the backbone network with the proposed regional and temporal based\nauxiliary task learning (RTATL) framework. Extensive experiments on BP4D and\nDISFA demonstrate the superiority of our method and new state-of-the-art\nperformances are achieved.",
    "published_date": "2021-07-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.14399v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.14321v1",
    "title": "LPV Delay-Dependent Sampled-Data Output-Feedback Control of Fueling in Spark Ignition Engines",
    "authors": [
      "Shahin Tasoujian",
      "Karolos Grigoriadis",
      "Matthew Franchek"
    ],
    "author_ids": [],
    "abstract": "We propose a delay-dependent sampled-data output-feedback LPV control\ntechnique to address the air-fuel ratio (AFR) regulation problem in spark\nignition (SI) engines. AFR control and advanced fueling strategies are\nessential for maximizing fuel economy while minimizing harmful exhaust\nemissions. The fuel path of the SI engine, as well as the three-way catalyst\n(TWC) simplified dynamics, have been captured by a continuous-time linear\nparameter-varying (LPV) system with varying time delay, where the system\ndynamics rely on the engine speed, defined as the system's scheduling\nparameter. The interconnection of the continuous-time plant and a digital\ncontroller through analog-to-digital and digital-to-analog converter devices\nforms a hybrid closed-loop configuration. Therefore, in order to benefit from\ncontinuous-time control synthesis tools, the input-delay method has been\nemployed to transform the hybrid closed-loop system into the continuous-time\ndomain with system inherent time delay and an additional delay imposed by the\nmapping approach. The designed sampled-data gain scheduled output-feedback\ncontroller seeks to establish the closed-loop asymptotic stability and a\nprescribed level of performance for the LPV system with an arbitrarily varying\ntime delay and varying sampling time, where the synthesis results are provided\nin a convex linear matrix inequality (LMI) constraint setting. Finally, several\nclosed-loop simulation scenarios are conducted, and comparisons are provided to\ndemonstrate the proposed methodology's performance in achieving precise\nreference AFR tracking and disturbance attenuation.",
    "published_date": "2021-07-29T00:00:00",
    "year": 2021,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.14321v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2107.14204v2",
    "title": "Personalized Trajectory Prediction via Distribution Discrimination",
    "authors": [
      "Guangyi Chen",
      "Junlong Li",
      "Nuoxing Zhou",
      "Liangliang Ren",
      "Jiwen Lu"
    ],
    "author_ids": [],
    "abstract": "Trajectory prediction is confronted with the dilemma to capture the\nmulti-modal nature of future dynamics with both diversity and accuracy. In this\npaper, we present a distribution discrimination (DisDis) method to predict\npersonalized motion patterns by distinguishing the potential distributions.\nMotivated by that the motion pattern of each person is personalized due to\nhis/her habit, our DisDis learns the latent distribution to represent different\nmotion patterns and optimize it by the contrastive discrimination. This\ndistribution discrimination encourages latent distributions to be more\ndiscriminative. Our method can be integrated with existing multi-modal\nstochastic predictive models as a plug-and-play module to learn the more\ndiscriminative latent distribution. To evaluate the latent distribution, we\nfurther propose a new metric, probability cumulative minimum distance (PCMD)\ncurve, which cumulatively calculates the minimum distance on the sorted\nprobabilities. Experimental results on the ETH and UCY datasets show the\neffectiveness of our method.",
    "published_date": "2021-07-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.14204v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.13876v1",
    "title": "Understanding the Effects of Adversarial Personalized Ranking Optimization Method on Recommendation Quality",
    "authors": [
      "Vito Walter Anelli",
      "Yashar Deldjoo",
      "Tommaso Di Noia",
      "Felice Antonio Merra"
    ],
    "author_ids": [],
    "abstract": "Recommender systems (RSs) employ user-item feedback, e.g., ratings, to match\ncustomers to personalized lists of products. Approaches to top-k recommendation\nmainly rely on Learning-To-Rank algorithms and, among them, the most widely\nadopted is Bayesian Personalized Ranking (BPR), which bases on a pair-wise\noptimization approach. Recently, BPR has been found vulnerable against\nadversarial perturbations of its model parameters. Adversarial Personalized\nRanking (APR) mitigates this issue by robustifying BPR via an adversarial\ntraining procedure. The empirical improvements of APR's accuracy performance on\nBPR have led to its wide use in several recommender models. However, a key\noverlooked aspect has been the beyond-accuracy performance of APR, i.e.,\nnovelty, coverage, and amplification of popularity bias, considering that\nrecent results suggest that BPR, the building block of APR, is sensitive to the\nintensification of biases and reduction of recommendation novelty. In this\nwork, we model the learning characteristics of the BPR and APR optimization\nframeworks to give mathematical evidence that, when the feedback data have a\ntailed distribution, APR amplifies the popularity bias more than BPR due to an\nunbalanced number of received positive updates from short-head items. Using\nmatrix factorization (MF), we empirically validate the theoretical results by\nperforming preliminary experiments on two public datasets to compare BPR-MF and\nAPR-MF performance on accuracy and beyond-accuracy metrics. The experimental\nresults consistently show the degradation of novelty and coverage measures and\na worrying amplification of bias.",
    "published_date": "2021-07-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.13876v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.13807v1",
    "title": "FREE: Feature Refinement for Generalized Zero-Shot Learning",
    "authors": [
      "Shiming Chen",
      "Wenjie Wang",
      "Beihao Xia",
      "Qinmu Peng",
      "Xinge You",
      "Feng Zheng",
      "Ling Shao"
    ],
    "author_ids": [],
    "abstract": "Generalized zero-shot learning (GZSL) has achieved significant progress, with\nmany efforts dedicated to overcoming the problems of visual-semantic domain gap\nand seen-unseen bias. However, most existing methods directly use feature\nextraction models trained on ImageNet alone, ignoring the cross-dataset bias\nbetween ImageNet and GZSL benchmarks. Such a bias inevitably results in\npoor-quality visual features for GZSL tasks, which potentially limits the\nrecognition performance on both seen and unseen classes. In this paper, we\npropose a simple yet effective GZSL method, termed feature refinement for\ngeneralized zero-shot learning (FREE), to tackle the above problem. FREE\nemploys a feature refinement (FR) module that incorporates\n\\textit{semantic$\\rightarrow$visual} mapping into a unified generative model to\nrefine the visual features of seen and unseen class samples. Furthermore, we\npropose a self-adaptive margin center loss (SAMC-loss) that cooperates with a\nsemantic cycle-consistency loss to guide FR to learn class- and\nsemantically-relevant representations, and concatenate the features in FR to\nextract the fully refined features. Extensive experiments on five benchmark\ndatasets demonstrate the significant performance gain of FREE over its baseline\nand current state-of-the-art methods. Our codes are available at\nhttps://github.com/shiming-chen/FREE .",
    "published_date": "2021-07-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.13807v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.13734v1",
    "title": "An Ethical Framework for Guiding the Development of Affectively-Aware Artificial Intelligence",
    "authors": [
      "Desmond C. Ong"
    ],
    "author_ids": [],
    "abstract": "The recent rapid advancements in artificial intelligence research and\ndeployment have sparked more discussion about the potential ramifications of\nsocially- and emotionally-intelligent AI. The question is not if research can\nproduce such affectively-aware AI, but when it will. What will it mean for\nsociety when machines -- and the corporations and governments they serve -- can\n\"read\" people's minds and emotions? What should developers and operators of\nsuch AI do, and what should they not do? The goal of this article is to\npre-empt some of the potential implications of these developments, and propose\na set of guidelines for evaluating the (moral and) ethical consequences of\naffectively-aware AI, in order to guide researchers, industry professionals,\nand policy-makers. We propose a multi-stakeholder analysis framework that\nseparates the ethical responsibilities of AI Developers vis-\\`a-vis the\nentities that deploy such AI -- which we term Operators. Our analysis produces\ntwo pillars that clarify the responsibilities of each of these stakeholders:\nProvable Beneficence, which rests on proving the effectiveness of the AI, and\nResponsible Stewardship, which governs responsible collection, use, and storage\nof data and the decisions made from such data. We end with recommendations for\nresearchers, developers, operators, as well as regulators and law-makers.",
    "published_date": "2021-07-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.13734v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.13625v3",
    "title": "Adaptation and Generalization for Unknown Sensitive Factors of Variations",
    "authors": [
      "William Paul",
      "Philippe Burlina"
    ],
    "author_ids": [],
    "abstract": "Assured AI in unrestricted settings is a critical problem. Our framework\naddresses AI assurance challenges lying at the intersection of domain\nadaptation, fairness, and counterfactuals analysis, operating via the discovery\nand intervention on factors of variations in data (e.g. weather or illumination\nconditions) that significantly affect the robustness of AI models. Robustness\nis understood here as insensitivity of the model performance to variations in\nsensitive factors. Sensitive factors are traditionally set in a supervised\nsetting, whereby factors are known a-priori (e.g. for fairness this could be\nfactors like sex or race). In contrast, our motivation is real-life scenarios\nwhere less, or nothing, is actually known a-priori about certain factors that\ncause models to fail. This leads us to consider various settings (unsupervised,\ndomain generalization, semi-supervised) that correspond to different degrees of\nincomplete knowledge about those factors. Therefore, our two step approach\nworks by a) discovering sensitive factors that cause AI systems to fail in a\nunsupervised fashion, and then b) intervening models to lessen these factor's\ninfluence. Our method considers 3 interventions consisting of Augmentation,\nCoherence, and Adversarial Interventions (ACAI). We demonstrate the ability for\ninterventions on discovered/source factors to generalize to target/real\nfactors. We also demonstrate how adaptation to real factors of variations can\nbe performed in the semi-supervised case where some target factor labels are\nknown, via automated intervention selection. Experiments show that our approach\nimproves on baseline models, with regard to achieving optimal utility vs.\nsensitivity/robustness tradeoffs.",
    "published_date": "2021-07-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.13625v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.13484v1",
    "title": "Inferring bias and uncertainty in camera calibration",
    "authors": [
      "Annika Hagemann",
      "Moritz Knorr",
      "Holger Janssen",
      "Christoph Stiller"
    ],
    "author_ids": [],
    "abstract": "Accurate camera calibration is a precondition for many computer vision\napplications. Calibration errors, such as wrong model assumptions or imprecise\nparameter estimation, can deteriorate a system's overall performance, making\nthe reliable detection and quantification of these errors critical. In this\nwork, we introduce an evaluation scheme to capture the fundamental error\nsources in camera calibration: systematic errors (biases) and uncertainty\n(variance). The proposed bias detection method uncovers smallest systematic\nerrors and thereby reveals imperfections of the calibration setup and provides\nthe basis for camera model selection. A novel resampling-based uncertainty\nestimator enables uncertainty estimation under non-ideal conditions and thereby\nextends the classical covariance estimator. Furthermore, we derive a simple\nuncertainty metric that is independent of the camera model. In combination, the\nproposed methods can be used to assess the accuracy of individual calibrations,\nbut also to benchmark new calibration algorithms, camera models, or calibration\nsetups. We evaluate the proposed methods with simulations and real cameras.",
    "published_date": "2021-07-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.13484v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.13361v1",
    "title": "Snippet Policy Network for Multi-class Varied-length ECG Early Classification",
    "authors": [
      "Yu Huang",
      "Gary G. Yen",
      "Vincent S. Tseng"
    ],
    "author_ids": [],
    "abstract": "Arrhythmia detection from ECG is an important research subject in the\nprevention and diagnosis of cardiovascular diseases. The prevailing studies\nformulate arrhythmia detection from ECG as a time series classification\nproblem. Meanwhile, early detection of arrhythmia presents a real-world demand\nfor early prevention and diagnosis. In this paper, we address a problem of\ncardiovascular disease early classification, which is a varied-length and\nlong-length time series early classification problem as well. For solving this\nproblem, we propose a deep reinforcement learning-based framework, namely\nSnippet Policy Network (SPN), consisting of four modules, snippet generator,\nbackbone network, controlling agent, and discriminator. Comparing to the\nexisting approaches, the proposed framework features flexible input length,\nsolves the dual-optimization solution of the earliness and accuracy goals.\nExperimental results demonstrate that SPN achieves an excellent performance of\nover 80\\% in terms of accuracy. Compared to the state-of-the-art methods, at\nleast 7% improvement on different metrics, including the precision, recall,\nF1-score, and harmonic mean, is delivered by the proposed SPN. To the best of\nour knowledge, this is the first work focusing on solving the cardiovascular\nearly classification problem based on varied-length ECG data. Based on these\nexcellent features from SPN, it offers a good exemplification for addressing\nall kinds of varied-length time series early classification problems.",
    "published_date": "2021-07-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "eess.SP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.13361v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.13327v1",
    "title": "Ranker-agnostic Contextual Position Bias Estimation",
    "authors": [
      "Oriol Barbany Mayor",
      "Vito Bellini",
      "Alexander Buchholz",
      "Giuseppe Di Benedetto",
      "Diego Marco Granziol",
      "Matteo Ruffini",
      "Yannik Stein"
    ],
    "author_ids": [],
    "abstract": "Learning-to-rank (LTR) algorithms are ubiquitous and necessary to explore the\nextensive catalogs of media providers. To avoid the user examining all the\nresults, its preferences are used to provide a subset of relatively small size.\nThe user preferences can be inferred from the interactions with the presented\ncontent if explicit ratings are unavailable. However, directly using implicit\nfeedback can lead to learning wrong relevance models and is known as biased\nLTR. The mismatch between implicit feedback and true relevances is due to\nvarious nuisances, with position bias one of the most relevant. Position bias\nmodels consider that the lack of interaction with a presented item is not only\nattributed to the item being irrelevant but because the item was not examined.\nThis paper introduces a method for modeling the probability of an item being\nseen in different contexts, e.g., for different users, with a single estimator.\nOur suggested method, denoted as contextual (EM)-based regression, is\nranker-agnostic and able to correctly learn the latent examination\nprobabilities while only using implicit feedback. Our empirical results\nindicate that the method introduced in this paper outperforms other existing\nposition bias estimators in terms of relative error when the examination\nprobability varies across queries. Moreover, the estimated values provide a\nranking performance boost when used to debias the implicit ranking data even if\nthere is no context dependency on the examination probabilities.",
    "published_date": "2021-07-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.13327v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2107.13277v1",
    "title": "A Novel CropdocNet for Automated Potato Late Blight Disease Detection from the Unmanned Aerial Vehicle-based Hyperspectral Imagery",
    "authors": [
      "Yue Shi",
      "Liangxiu Han",
      "Anthony Kleerekoper",
      "Sheng Chang",
      "Tongle Hu"
    ],
    "author_ids": [],
    "abstract": "Late blight disease is one of the most destructive diseases in potato crop,\nleading to serious yield losses globally. Accurate diagnosis of the disease at\nearly stage is critical for precision disease control and management. Current\nfarm practices in crop disease diagnosis are based on manual visual inspection,\nwhich is costly, time consuming, subject to individual bias. Recent advances in\nimaging sensors (e.g. RGB, multiple spectral and hyperspectral cameras), remote\nsensing and machine learning offer the opportunity to address this challenge.\nParticularly, hyperspectral imagery (HSI) combining with machine learning/deep\nlearning approaches is preferable for accurately identifying specific plant\ndiseases because the HSI consists of a wide range of high-quality reflectance\ninformation beyond human vision, capable of capturing both spectral-spatial\ninformation. The proposed method considers the potential disease specific\nreflectance radiation variance caused by the canopy structural diversity,\nintroduces the multiple capsule layers to model the hierarchical structure of\nthe spectral-spatial disease attributes with the encapsulated features to\nrepresent the various classes and the rotation invariance of the disease\nattributes in the feature space. We have evaluated the proposed method with the\nreal UAV-based HSI data under the controlled field conditions. The\neffectiveness of the hierarchical features has been quantitatively assessed and\ncompared with the existing representative machine learning/deep learning\nmethods. The experiment results show that the proposed model significantly\nimproves the accuracy performance when considering hierarchical-structure of\nspectral-spatial features, comparing to the existing methods only using\nspectral, or spatial or spectral-spatial features without consider\nhierarchical-structure of spectral-spatial features.",
    "published_date": "2021-07-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.13277v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.13173v1",
    "title": "New Metrics to Evaluate the Performance and Fairness of Personalized Federated Learning",
    "authors": [
      "Siddharth Divi",
      "Yi-Shan Lin",
      "Habiba Farrukh",
      "Z. Berkay Celik"
    ],
    "author_ids": [],
    "abstract": "In Federated Learning (FL), the clients learn a single global model (FedAvg)\nthrough a central aggregator. In this setting, the non-IID distribution of the\ndata across clients restricts the global FL model from delivering good\nperformance on the local data of each client. Personalized FL aims to address\nthis problem by finding a personalized model for each client. Recent works\nwidely report the average personalized model accuracy on a particular data\nsplit of a dataset to evaluate the effectiveness of their methods. However,\nconsidering the multitude of personalization approaches proposed, it is\ncritical to study the per-user personalized accuracy and the accuracy\nimprovements among users with an equitable notion of fairness. To address these\nissues, we present a set of performance and fairness metrics intending to\nassess the quality of personalized FL methods. We apply these metrics to four\nrecently proposed personalized FL methods, PersFL, FedPer, pFedMe, and\nPer-FedAvg, on three different data splits of the CIFAR-10 dataset. Our\nevaluations show that the personalized model with the highest average accuracy\nacross users may not necessarily be the fairest. Our code is available at\nhttps://tinyurl.com/1hp9ywfa for public use.",
    "published_date": "2021-07-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.13173v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.13118v1",
    "title": "Divide-and-Assemble: Learning Block-wise Memory for Unsupervised Anomaly Detection",
    "authors": [
      "Jinlei Hou",
      "Yingying Zhang",
      "Qiaoyong Zhong",
      "Di Xie",
      "Shiliang Pu",
      "Hong Zhou"
    ],
    "author_ids": [],
    "abstract": "Reconstruction-based methods play an important role in unsupervised anomaly\ndetection in images. Ideally, we expect a perfect reconstruction for normal\nsamples and poor reconstruction for abnormal samples. Since the\ngeneralizability of deep neural networks is difficult to control, existing\nmodels such as autoencoder do not work well. In this work, we interpret the\nreconstruction of an image as a divide-and-assemble procedure. Surprisingly, by\nvarying the granularity of division on feature maps, we are able to modulate\nthe reconstruction capability of the model for both normal and abnormal\nsamples. That is, finer granularity leads to better reconstruction, while\ncoarser granularity leads to poorer reconstruction. With proper granularity,\nthe gap between the reconstruction error of normal and abnormal samples can be\nmaximized. The divide-and-assemble framework is implemented by embedding a\nnovel multi-scale block-wise memory module into an autoencoder network.\nBesides, we introduce adversarial learning and explore the semantic latent\nrepresentation of the discriminator, which improves the detection of subtle\nanomaly. We achieve state-of-the-art performance on the challenging MVTec AD\ndataset. Remarkably, we improve the vanilla autoencoder model by 10.1% in terms\nof the AUROC score.",
    "published_date": "2021-07-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.13118v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.12977v3",
    "title": "The social dilemma in artificial intelligence development and why we have to solve it",
    "authors": [
      "Inga Strümke",
      "Marija Slavkovik",
      "Vince I. Madai"
    ],
    "author_ids": [],
    "abstract": "While the demand for ethical artificial intelligence (AI) systems increases,\nthe number of unethical uses of AI accelerates, even though there is no\nshortage of ethical guidelines. We argue that a possible underlying cause for\nthis is that AI developers face a social dilemma in AI development ethics,\npreventing the widespread adaptation of ethical best practices. We define the\nsocial dilemma for AI development and describe why the current crisis in AI\ndevelopment ethics cannot be solved without relieving AI developers of their\nsocial dilemma. We argue that AI development must be professionalised to\novercome the social dilemma, and discuss how medicine can be used as a template\nin this process.",
    "published_date": "2021-07-27T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.12977v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.12845v2",
    "title": "A Storytelling Robot managing Persuasive and Ethical Stances via ACT-R: an Exploratory Study",
    "authors": [
      "Agnese Augello",
      "Giuseppe Città",
      "Manuel Gentile",
      "Antonio Lieto"
    ],
    "author_ids": [],
    "abstract": "We present a storytelling robot, controlled via the ACT-R cognitive\narchitecture, able to adopt different persuasive techniques and ethical stances\nwhile conversing about some topics concerning COVID-19. The main contribution\nof the paper consists in the proposal of a needs-driven model that guides and\nevaluates, during the dialogue, the use (if any) of persuasive techniques\navailable in the agent procedural memory. The portfolio of persuasive\ntechniques tested in such a model ranges from the use of storytelling, to\nframing techniques and rhetorical-based arguments. To the best of our\nknowledge, this represents the first attempt of building a persuasive agent\nable to integrate a mix of explicitly grounded cognitive assumptions about\ndialogue management, storytelling and persuasive techniques as well as ethical\nattitudes. The paper presents the results of an exploratory evaluation of the\nsystem on 63 participants",
    "published_date": "2021-07-27T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.12845v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.12826v1",
    "title": "Adversarial Stacked Auto-Encoders for Fair Representation Learning",
    "authors": [
      "Patrik Joslin Kenfack",
      "Adil Mehmood Khan",
      "Rasheed Hussain",
      "S. M. Ahsan Kazmi"
    ],
    "author_ids": [],
    "abstract": "Training machine learning models with the only accuracy as a final goal may\npromote prejudices and discriminatory behaviors embedded in the data. One\nsolution is to learn latent representations that fulfill specific fairness\nmetrics. Different types of learning methods are employed to map data into the\nfair representational space. The main purpose is to learn a latent\nrepresentation of data that scores well on a fairness metric while maintaining\nthe usability for the downstream task. In this paper, we propose a new fair\nrepresentation learning approach that leverages different levels of\nrepresentation of data to tighten the fairness bounds of the learned\nrepresentation. Our results show that stacking different auto-encoders and\nenforcing fairness at different latent spaces result in an improvement of\nfairness compared to other existing approaches.",
    "published_date": "2021-07-27T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.12826v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.12783v1",
    "title": "Statistical Guarantees for Fairness Aware Plug-In Algorithms",
    "authors": [
      "Drona Khurana",
      "Srinivasan Ravichandran",
      "Sparsh Jain",
      "Narayanan Unny Edakunni"
    ],
    "author_ids": [],
    "abstract": "A plug-in algorithm to estimate Bayes Optimal Classifiers for fairness-aware\nbinary classification has been proposed in (Menon & Williamson, 2018). However,\nthe statistical efficacy of their approach has not been established. We prove\nthat the plug-in algorithm is statistically consistent. We also derive finite\nsample guarantees associated with learning the Bayes Optimal Classifiers via\nthe plug-in algorithm. Finally, we propose a protocol that modifies the plug-in\napproach, so as to simultaneously guarantee fairness and differential privacy\nwith respect to a binary feature deemed sensitive.",
    "published_date": "2021-07-27T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.12783v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.12753v1",
    "title": "Discriminative-Generative Representation Learning for One-Class Anomaly Detection",
    "authors": [
      "Xuan Xia",
      "Xizhou Pan",
      "Xing He",
      "Jingfei Zhang",
      "Ning Ding",
      "Lin Ma"
    ],
    "author_ids": [],
    "abstract": "As a kind of generative self-supervised learning methods, generative\nadversarial nets have been widely studied in the field of anomaly detection.\nHowever, the representation learning ability of the generator is limited since\nit pays too much attention to pixel-level details, and generator is difficult\nto learn abstract semantic representations from label prediction pretext tasks\nas effective as discriminator. In order to improve the representation learning\nability of generator, we propose a self-supervised learning framework combining\ngenerative methods and discriminative methods. The generator no longer learns\nrepresentation by reconstruction error, but the guidance of discriminator, and\ncould benefit from pretext tasks designed for discriminative methods. Our\ndiscriminative-generative representation learning method has performance close\nto discriminative methods and has a great advantage in speed. Our method used\nin one-class anomaly detection task significantly outperforms several\nstate-of-the-arts on multiple benchmark data sets, increases the performance of\nthe top-performing GAN-based baseline by 6% on CIFAR-10 and 2% on MVTAD.",
    "published_date": "2021-07-27T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.12753v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.12711v3",
    "title": "Reason Against the Machine: Future Directions for Mass Online Deliberation",
    "authors": [
      "Ruth Shortall",
      "Anatol Itten",
      "Michiel van der Meer",
      "Pradeep K. Murukannaiah",
      "Catholijn M. Jonker"
    ],
    "author_ids": [],
    "abstract": "Designers of online deliberative platforms aim to counter the degrading\nquality of online debates. Support technologies such as machine learning and\nnatural language processing open avenues for widening the circle of people\ninvolved in deliberation, moving from small groups to \"crowd\" scale. Numerous\ndesign features of large-scale online discussion systems allow larger numbers\nof people to discuss shared problems, enhance critical thinking, and formulate\nsolutions. We review the transdisciplinary literature on the design of digital\nmass deliberation platforms and examine the commonly featured design aspects\n(e.g., argumentation support, automated facilitation, and gamification) that\nattempt to facilitate scaling up. We find that the literature is largely\nfocused on developing technical fixes for scaling up deliberation, but may\nneglect the more nuanced requirements of high quality deliberation. Current\ndesign research is carried out with a small, atypical segment of the world's\npopulation, and much research is still needed on how to facilitate and\naccommodate different genders or cultures in deliberation, how to deal with the\nimplications of pre-existing social inequalities, how to build motivation and\nself-efficacy in certain groups, and how to deal with differences in cognitive\nabilities and cultural or linguistic differences. Few studies bridge\ndisciplines between deliberative theory, design and engineering. As a result,\nscaling up deliberation will likely advance in separate systemic siloes. We\nmake design and process recommendations to correct this course and suggest\navenues for future research",
    "published_date": "2021-07-27T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.12711v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.12530v3",
    "title": "Convergence of Deep ReLU Networks",
    "authors": [
      "Yuesheng Xu",
      "Haizhang Zhang"
    ],
    "author_ids": [],
    "abstract": "We explore convergence of deep neural networks with the popular ReLU\nactivation function, as the depth of the networks tends to infinity. To this\nend, we introduce the notion of activation domains and activation matrices of a\nReLU network. By replacing applications of the ReLU activation function by\nmultiplications with activation matrices on activation domains, we obtain an\nexplicit expression of the ReLU network. We then identify the convergence of\nthe ReLU networks as convergence of a class of infinite products of matrices.\nSufficient and necessary conditions for convergence of these infinite products\nof matrices are studied. As a result, we establish necessary conditions for\nReLU networks to converge that the sequence of weight matrices converges to the\nidentity matrix and the sequence of the bias vectors converges to zero as the\ndepth of ReLU networks increases to infinity. Moreover, we obtain sufficient\nconditions in terms of the weight matrices and bias vectors at hidden layers\nfor pointwise convergence of deep ReLU networks. These results provide\nmathematical insights to the design strategy of the well-known deep residual\nnetworks in image classification.",
    "published_date": "2021-07-27T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "math.FA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.12530v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.12438v4",
    "title": "Debiasing In-Sample Policy Performance for Small-Data, Large-Scale Optimization",
    "authors": [
      "Vishal Gupta",
      "Michael Huang",
      "Paat Rusmevichientong"
    ],
    "author_ids": [],
    "abstract": "Motivated by the poor performance of cross-validation in settings where data\nare scarce, we propose a novel estimator of the out-of-sample performance of a\npolicy in data-driven optimization.Our approach exploits the optimization\nproblem's sensitivity analysis to estimate the gradient of the optimal\nobjective value with respect to the amount of noise in the data and uses the\nestimated gradient to debias the policy's in-sample performance. Unlike\ncross-validation techniques, our approach avoids sacrificing data for a test\nset, utilizes all data when training and, hence, is well-suited to settings\nwhere data are scarce. We prove bounds on the bias and variance of our\nestimator for optimization problems with uncertain linear objectives but known,\npotentially non-convex, feasible regions. For more specialized optimization\nproblems where the feasible region is \"weakly-coupled\" in a certain sense, we\nprove stronger results. Specifically, we provide explicit high-probability\nbounds on the error of our estimator that hold uniformly over a policy class\nand depends on the problem's dimension and policy class's complexity. Our\nbounds show that under mild conditions, the error of our estimator vanishes as\nthe dimension of the optimization problem grows, even if the amount of\navailable data remains small and constant. Said differently, we prove our\nestimator performs well in the small-data, large-scale regime. Finally, we\nnumerically compare our proposed method to state-of-the-art approaches through\na case-study on dispatching emergency medical response services using real\ndata. Our method provides more accurate estimates of out-of-sample performance\nand learns better-performing policies.",
    "published_date": "2021-07-26T00:00:00",
    "year": 2021,
    "categories": [
      "math.OC",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.12438v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.12053v2",
    "title": "A Frequency-based Parent Selection for Reducing the Effect of Evaluation Time Bias in Asynchronous Parallel Multi-objective Evolutionary Algorithms",
    "authors": [
      "Tomohiro Harada"
    ],
    "author_ids": [],
    "abstract": "Parallel evolutionary algorithms (PEAs) have been studied for reducing the\nexecution time of evolutionary algorithms by utilizing parallel computing. An\nasynchronous PEA (APEA) is a scheme of PEAs that increases computational\nefficiency by generating a new solution immediately after a solution evaluation\ncompletes without the idling time of computing nodes. However, because APEA\ngives more search opportunities to solutions with shorter evaluation times, the\nevaluation time bias of solutions negatively affects the search performance. To\novercome this drawback, this paper proposes a new parent selection method to\nreduce the effect of evaluation time bias in APEAs. The proposed method\nconsiders the search frequency of solutions and selects the parent solutions so\nthat the search progress in the population is uniform regardless of the\nevaluation time bias. This paper conducts experiments on multi-objective\noptimization problems that simulate the evaluation time bias. The experiments\nuse NSGA-III, a well-known multi-objective evolutionary algorithm, and compare\nthe proposed method with the conventional synchronous/asynchronous\nparallelization. The experimental results reveal that the proposed method can\nreduce the effect of the evaluation time bias while reducing the computing time\nof the parallel NSGA-III.",
    "published_date": "2021-07-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.NE",
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.12053v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.12049v2",
    "title": "SVEva Fair: A Framework for Evaluating Fairness in Speaker Verification",
    "authors": [
      "Wiebke Toussaint",
      "Aaron Yi Ding"
    ],
    "author_ids": [],
    "abstract": "Despite the success of deep neural networks (DNNs) in enabling on-device\nvoice assistants, increasing evidence of bias and discrimination in machine\nlearning is raising the urgency of investigating the fairness of these systems.\nSpeaker verification is a form of biometric identification that gives access to\nvoice assistants. Due to a lack of fairness metrics and evaluation frameworks\nthat are appropriate for testing the fairness of speaker verification\ncomponents, little is known about how model performance varies across\nsubgroups, and what factors influence performance variation. To tackle this\nemerging challenge, we design and develop SVEva Fair, an accessible, actionable\nand model-agnostic framework for evaluating the fairness of speaker\nverification components. The framework provides evaluation measures and\nvisualisations to interrogate model performance across speaker subgroups and\ncompare fairness between models. We demonstrate SVEva Fair in a case study with\nend-to-end DNNs trained on the VoxCeleb datasets to reveal potential bias in\nexisting embedded speech recognition systems based on the demographic\nattributes of speakers. Our evaluation shows that publicly accessible benchmark\nmodels are not fair and consistently produce worse predictions for some\nnationalities, and for female speakers of most nationalities. To pave the way\nfor fair and reliable embedded speaker verification, SVEva Fair has been\nimplemented as an open-source python library and can be integrated into the\nembedded ML development pipeline to facilitate developers and researchers in\ntroubleshooting unreliable speaker verification performance, and selecting high\nimpact approaches for mitigating fairness challenges",
    "published_date": "2021-07-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.12049v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.12028v2",
    "title": "Parametric Contrastive Learning",
    "authors": [
      "Jiequan Cui",
      "Zhisheng Zhong",
      "Shu Liu",
      "Bei Yu",
      "Jiaya Jia"
    ],
    "author_ids": [],
    "abstract": "In this paper, we propose Parametric Contrastive Learning (PaCo) to tackle\nlong-tailed recognition. Based on theoretical analysis, we observe supervised\ncontrastive loss tends to bias on high-frequency classes and thus increases the\ndifficulty of imbalanced learning. We introduce a set of parametric class-wise\nlearnable centers to rebalance from an optimization perspective. Further, we\nanalyze our PaCo loss under a balanced setting. Our analysis demonstrates that\nPaCo can adaptively enhance the intensity of pushing samples of the same class\nclose as more samples are pulled together with their corresponding centers and\nbenefit hard example learning. Experiments on long-tailed CIFAR, ImageNet,\nPlaces, and iNaturalist 2018 manifest the new state-of-the-art for long-tailed\nrecognition. On full ImageNet, models trained with PaCo loss surpass supervised\ncontrastive learning across various ResNet backbones, e.g., our ResNet-200\nachieves 81.8% top-1 accuracy. Our code is available at\nhttps://github.com/dvlab-research/Parametric-Contrastive-Learning.",
    "published_date": "2021-07-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.12028v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.11913v2",
    "title": "Measuring Ethics in AI with AI: A Methodology and Dataset Construction",
    "authors": [
      "Pedro H. C. Avelar",
      "Rafael B. Audibert",
      "Anderson R. Tavares",
      "Luís C. Lamb"
    ],
    "author_ids": [],
    "abstract": "Recently, the use of sound measures and metrics in Artificial Intelligence\nhas become the subject of interest of academia, government, and industry.\nEfforts towards measuring different phenomena have gained traction in the AI\ncommunity, as illustrated by the publication of several influential field\nreports and policy documents. These metrics are designed to help decision\ntakers to inform themselves about the fast-moving and impacting influences of\nkey advances in Artificial Intelligence in general and Machine Learning in\nparticular. In this paper we propose to use such newfound capabilities of AI\ntechnologies to augment our AI measuring capabilities. We do so by training a\nmodel to classify publications related to ethical issues and concerns. In our\nmethodology we use an expert, manually curated dataset as the training set and\nthen evaluate a large set of research papers. Finally, we highlight the\nimplications of AI metrics, in particular their contribution towards developing\ntrustful and fair AI-based tools and technologies. Keywords: AI Ethics; AI\nFairness; AI Measurement. Ethics in Computer Science.",
    "published_date": "2021-07-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "I.2; K.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.11913v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.11906v1",
    "title": "H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences",
    "authors": [
      "Zhenhai Zhu",
      "Radu Soricut"
    ],
    "author_ids": [],
    "abstract": "We describe an efficient hierarchical method to compute attention in the\nTransformer architecture. The proposed attention mechanism exploits a matrix\nstructure similar to the Hierarchical Matrix (H-Matrix) developed by the\nnumerical analysis community, and has linear run time and memory complexity. We\nperform extensive experiments to show that the inductive bias embodied by our\nhierarchical attention is effective in capturing the hierarchical structure in\nthe sequences typical for natural language and vision tasks. Our method is\nsuperior to alternative sub-quadratic proposals by over +6 points on average on\nthe Long Range Arena benchmark. It also sets a new SOTA test perplexity on\nOne-Billion Word dataset with 5x fewer model parameters than that of the\nprevious-best Transformer-based models.",
    "published_date": "2021-07-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.11906v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.11832v1",
    "title": "A Holistic Analysis of Datacenter Operations: Resource Usage, Energy, and Workload Characterization -- Extended Technical Report",
    "authors": [
      "Laurens Versluis",
      "Mehmet Cetin",
      "Caspar Greeven",
      "Kristian Laursen",
      "Damian Podareanu",
      "Valeriu Codreanu",
      "Alexandru Uta",
      "Alexandru Iosup"
    ],
    "author_ids": [],
    "abstract": "Improving datacenter operations is vital for the digital society. We posit\nthat doing so requires our community to shift, from operational aspects taken\nin isolation to holistic analysis of datacenter resources, energy, and\nworkloads. In turn, this shift will require new analysis methods, and\nopen-access, FAIR datasets with fine temporal and spatial granularity. We\nleverage in this work one of the (rare) public datasets providing fine-grained\ninformation on datacenter operations. Using it, we show strong evidence that\nfine-grained information reveals new operational aspects. We then propose a\nmethod for holistic analysis of datacenter operations, providing statistical\ncharacterization of node, energy, and workload aspects. We demonstrate the\nbenefits of our holistic analysis method by applying it to the operations of a\ndatacenter infrastructure with over 300 nodes. Our analysis reveals both\ngeneric and ML-specific aspects, and further details how the operational\nbehavior of the datacenter changed during the 2020 COVID-19 pandemic. We make\nover 30 main observations, providing holistic insight into the long-term\noperation of a large-scale, public scientific infrastructure. We suggest such\nobservations can help immediately with performance engineering tasks such as\npredicting future datacenter load, and also long-term with the design of\ndatacenter infrastructure.",
    "published_date": "2021-07-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.11832v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2107.11625v1",
    "title": "Discrete Denoising Flows",
    "authors": [
      "Alexandra Lindt",
      "Emiel Hoogeboom"
    ],
    "author_ids": [],
    "abstract": "Discrete flow-based models are a recently proposed class of generative models\nthat learn invertible transformations for discrete random variables. Since they\ndo not require data dequantization and maximize an exact likelihood objective,\nthey can be used in a straight-forward manner for lossless compression. In this\npaper, we introduce a new discrete flow-based model for categorical random\nvariables: Discrete Denoising Flows (DDFs). In contrast with other discrete\nflow-based models, our model can be locally trained without introducing\ngradient bias. We show that DDFs outperform Discrete Flows on modeling a toy\nexample, binary MNIST and Cityscapes segmentation maps, measured in\nlog-likelihood.",
    "published_date": "2021-07-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.11625v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.11610v1",
    "title": "Context-aware Adversarial Training for Name Regularity Bias in Named Entity Recognition",
    "authors": [
      "Abbas Ghaddar",
      "Philippe Langlais",
      "Ahmad Rashid",
      "Mehdi Rezagholizadeh"
    ],
    "author_ids": [],
    "abstract": "In this work, we examine the ability of NER models to use contextual\ninformation when predicting the type of an ambiguous entity. We introduce NRB,\na new testbed carefully designed to diagnose Name Regularity Bias of NER\nmodels. Our results indicate that all state-of-the-art models we tested show\nsuch a bias; BERT fine-tuned models significantly outperforming feature-based\n(LSTM-CRF) ones on NRB, despite having comparable (sometimes lower) performance\non standard benchmarks.\n  To mitigate this bias, we propose a novel model-agnostic training method that\nadds learnable adversarial noise to some entity mentions, thus enforcing models\nto focus more strongly on the contextual signal, leading to significant gains\non NRB. Combining it with two other training strategies, data augmentation and\nparameter freezing, leads to further gains.",
    "published_date": "2021-07-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.11610v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.11584v1",
    "title": "Extending Challenge Sets to Uncover Gender Bias in Machine Translation: Impact of Stereotypical Verbs and Adjectives",
    "authors": [
      "Jonas-Dario Troles",
      "Ute Schmid"
    ],
    "author_ids": [],
    "abstract": "Human gender bias is reflected in language and text production. Because\nstate-of-the-art machine translation (MT) systems are trained on large corpora\nof text, mostly generated by humans, gender bias can also be found in MT. For\ninstance when occupations are translated from a language like English, which\nmostly uses gender neutral words, to a language like German, which mostly uses\na feminine and a masculine version for an occupation, a decision must be made\nby the MT System. Recent research showed that MT systems are biased towards\nstereotypical translation of occupations. In 2019 the first, and so far only,\nchallenge set, explicitly designed to measure the extent of gender bias in MT\nsystems has been published. In this set measurement of gender bias is solely\nbased on the translation of occupations. In this paper we present an extension\nof this challenge set, called WiBeMT, with gender-biased adjectives and adds\nsentences with gender-biased verbs. The resulting challenge set consists of\nover 70, 000 sentences and has been translated with three commercial MT\nsystems: DeepL Translator, Microsoft Translator, and Google Translate. Results\nshow a gender bias for all three MT systems. This gender bias is to a great\nextent significantly influenced by adjectives and to a lesser extent by verbs.",
    "published_date": "2021-07-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.11584v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.11496v1",
    "title": "Training multi-objective/multi-task collocation physics-informed neural network with student/teachers transfer learnings",
    "authors": [
      "Bahador Bahmani",
      "WaiChing Sun"
    ],
    "author_ids": [],
    "abstract": "This paper presents a PINN training framework that employs (1) pre-training\nsteps that accelerates and improve the robustness of the training of\nphysics-informed neural network with auxiliary data stored in point clouds, (2)\na net-to-net knowledge transfer algorithm that improves the weight\ninitialization of the neural network and (3) a multi-objective optimization\nalgorithm that may improve the performance of a physical-informed neural\nnetwork with competing constraints. We consider the training and transfer and\nmulti-task learning of physics-informed neural network (PINN) as\nmulti-objective problems where the physics constraints such as the governing\nequation, boundary conditions, thermodynamic inequality, symmetry, and\ninvariant properties, as well as point cloud used for pre-training can\nsometimes lead to conflicts and necessitating the seek of the Pareto optimal\nsolution. In these situations, weighted norms commonly used to handle multiple\nconstraints may lead to poor performance, while other multi-objective\nalgorithms may scale poorly with increasing dimensionality. To overcome this\ntechnical barrier, we adopt the concept of vectorized objective function and\nmodify a gradient descent approach to handle the issue of conflicting\ngradients. Numerical experiments are compared the benchmark boundary value\nproblems solved via PINN. The performance of the proposed paradigm is compared\nagainst the classical equal-weighted norm approach. Our numerical experiments\nindicate that the brittleness and lack of robustness demonstrated in some PINN\nimplementations can be overcome with the proposed strategy.",
    "published_date": "2021-07-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.11496v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.11279v2",
    "title": "Re-distributing Biased Pseudo Labels for Semi-supervised Semantic Segmentation: A Baseline Investigation",
    "authors": [
      "Ruifei He",
      "Jihan Yang",
      "Xiaojuan Qi"
    ],
    "author_ids": [],
    "abstract": "While self-training has advanced semi-supervised semantic segmentation, it\nseverely suffers from the long-tailed class distribution on real-world semantic\nsegmentation datasets that make the pseudo-labeled data bias toward majority\nclasses. In this paper, we present a simple and yet effective Distribution\nAlignment and Random Sampling (DARS) method to produce unbiased pseudo labels\nthat match the true class distribution estimated from the labeled data.\nBesides, we also contribute a progressive data augmentation and labeling\nstrategy to facilitate model training with pseudo-labeled data. Experiments on\nboth Cityscapes and PASCAL VOC 2012 datasets demonstrate the effectiveness of\nour approach. Albeit simple, our method performs favorably in comparison with\nstate-of-the-art approaches. Code will be available at\nhttps://github.com/CVMI-Lab/DARS.",
    "published_date": "2021-07-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.11279v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.11170v3",
    "title": "Bias Loss for Mobile Neural Networks",
    "authors": [
      "Lusine Abrahamyan",
      "Valentin Ziatchin",
      "Yiming Chen",
      "Nikos Deligiannis"
    ],
    "author_ids": [],
    "abstract": "Compact convolutional neural networks (CNNs) have witnessed exceptional\nimprovements in performance in recent years. However, they still fail to\nprovide the same predictive power as CNNs with a large number of parameters.\nThe diverse and even abundant features captured by the layers is an important\ncharacteristic of these successful CNNs. However, differences in this\ncharacteristic between large CNNs and their compact counterparts have rarely\nbeen investigated. In compact CNNs, due to the limited number of parameters,\nabundant features are unlikely to be obtained, and feature diversity becomes an\nessential characteristic. Diverse features present in the activation maps\nderived from a data point during model inference may indicate the presence of a\nset of unique descriptors necessary to distinguish between objects of different\nclasses. In contrast, data points with low feature diversity may not provide a\nsufficient amount of unique descriptors to make a valid prediction; we refer to\nthem as random predictions. Random predictions can negatively impact the\noptimization process and harm the final performance. This paper proposes\naddressing the problem raised by random predictions by reshaping the standard\ncross-entropy to make it biased toward data points with a limited number of\nunique descriptive features. Our novel Bias Loss focuses the training on a set\nof valuable data points and prevents the vast number of samples with poor\nlearning features from misleading the optimization process. Furthermore, to\nshow the importance of diversity, we present a family of SkipNet models whose\narchitectures are brought to boost the number of unique descriptors in the last\nlayers. Our Skipnet-M can achieve 1% higher classification accuracy than\nMobileNetV3 Large.",
    "published_date": "2021-07-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.11170v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.11159v1",
    "title": "Learning Discriminative Representations for Multi-Label Image Recognition",
    "authors": [
      "Mohammed Hassanin",
      "Ibrahim Radwan",
      "Salman Khan",
      "Murat Tahtali"
    ],
    "author_ids": [],
    "abstract": "Multi-label recognition is a fundamental, and yet is a challenging task in\ncomputer vision. Recently, deep learning models have achieved great progress\ntowards learning discriminative features from input images. However,\nconventional approaches are unable to model the inter-class discrepancies among\nfeatures in multi-label images, since they are designed to work for image-level\nfeature discrimination. In this paper, we propose a unified deep network to\nlearn discriminative features for the multi-label task. Given a multi-label\nimage, the proposed method first disentangles features corresponding to\ndifferent classes. Then, it discriminates between these classes via increasing\nthe inter-class distance while decreasing the intra-class differences in the\noutput space. By regularizing the whole network with the proposed loss, the\nperformance of applying the wellknown ResNet-101 is improved significantly.\nExtensive experiments have been performed on COCO-2014, VOC2007 and VOC2012\ndatasets, which demonstrate that the proposed method outperforms\nstate-of-the-art approaches by a significant margin of 3:5% on large-scale COCO\ndataset. Moreover, analysis of the discriminative feature learning approach\nshows that it can be plugged into various types of multi-label methods as a\ngeneral module.",
    "published_date": "2021-07-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.11159v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.10718v1",
    "title": "Segmentation of Cardiac Structures via Successive Subspace Learning with Saab Transform from Cine MRI",
    "authors": [
      "Xiaofeng Liu",
      "Fangxu Xing",
      "Hanna K. Gaggin",
      "Weichung Wang",
      "C. -C. Jay Kuo",
      "Georges El Fakhri",
      "Jonghye Woo"
    ],
    "author_ids": [],
    "abstract": "Assessment of cardiovascular disease (CVD) with cine magnetic resonance\nimaging (MRI) has been used to non-invasively evaluate detailed cardiac\nstructure and function. Accurate segmentation of cardiac structures from cine\nMRI is a crucial step for early diagnosis and prognosis of CVD, and has been\ngreatly improved with convolutional neural networks (CNN). There, however, are\na number of limitations identified in CNN models, such as limited\ninterpretability and high complexity, thus limiting their use in clinical\npractice. In this work, to address the limitations, we propose a lightweight\nand interpretable machine learning model, successive subspace learning with the\nsubspace approximation with adjusted bias (Saab) transform, for accurate and\nefficient segmentation from cine MRI. Specifically, our segmentation framework\nis comprised of the following steps: (1) sequential expansion of near-to-far\nneighborhood at different resolutions; (2) channel-wise subspace approximation\nusing the Saab transform for unsupervised dimension reduction; (3) class-wise\nentropy guided feature selection for supervised dimension reduction; (4)\nconcatenation of features and pixel-wise classification with gradient boost;\nand (5) conditional random field for post-processing. Experimental results on\nthe ACDC 2017 segmentation database, showed that our framework performed better\nthan state-of-the-art U-Net models with 200$\\times$ fewer parameters in\ndelineating the left ventricle, right ventricle, and myocardium, thus showing\nits potential to be used in clinical practice.",
    "published_date": "2021-07-22T00:00:00",
    "year": 2021,
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.10718v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.10715v1",
    "title": "Philosophical Specification of Empathetic Ethical Artificial Intelligence",
    "authors": [
      "Michael Timothy Bennett",
      "Yoshihiro Maruyama"
    ],
    "author_ids": [],
    "abstract": "In order to construct an ethical artificial intelligence (AI) two complex\nproblems must be overcome. Firstly, humans do not consistently agree on what is\nor is not ethical. Second, contemporary AI and machine learning methods tend to\nbe blunt instruments which either search for solutions within the bounds of\npredefined rules, or mimic behaviour. An ethical AI must be capable of\ninferring unspoken rules, interpreting nuance and context, possess and be able\nto infer intent, and explain not just its actions but its intent. Using\nenactivism, semiotics, perceptual symbol systems and symbol emergence, we\nspecify an agent that learns not just arbitrary relations between signs but\ntheir meaning in terms of the perceptual states of its sensorimotor system.\nSubsequently it can learn what is meant by a sentence and infer the intent of\nothers in terms of its own experiences. It has malleable intent because the\nmeaning of symbols changes as it learns, and its intent is represented\nsymbolically as a goal. As such it may learn a concept of what is most likely\nto be considered ethical by the majority within a population of humans, which\nmay then be used as a goal. The meaning of abstract symbols is expressed using\nperceptual symbols of raw sensorimotor stimuli as the weakest (consistent with\nOckham's Razor) necessary and sufficient concept, an intensional definition\nlearned from an ostensive definition, from which the extensional definition or\ncategory of all ethical decisions may be obtained. Because these abstract\nsymbols are the same for both situation and response, the same symbol is used\nwhen either performing or observing an action. This is akin to mirror neurons\nin the human brain. Mirror symbols may allow the agent to empathise, because\nits own experiences are associated with the symbol, which is also associated\nwith the observation of another agent experiencing something that symbol\nrepresents.",
    "published_date": "2021-07-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.10715v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.10516v1",
    "title": "The Stationary Prophet Inequality Problem",
    "authors": [
      "Kristen Kessel",
      "Amin Saberi",
      "Ali Shameli",
      "David Wajc"
    ],
    "author_ids": [],
    "abstract": "We study a continuous and infinite time horizon counterpart to the classic\nprophet inequality, which we term the stationary prophet inequality problem.\nHere, copies of a good arrive and perish according to Poisson point processes.\nBuyers arrive similarly and make take-it-or-leave-it offers for unsold items.\nThe objective is to maximize the (infinite) time average revenue of the seller.\n  Our main results are pricing-based policies which (i) achieve a\n$1/2$-approximation of the optimal offline policy, which is best possible, and\n(ii) achieve a better than $(1-1/e)$-approximation of the optimal online\npolicy. Result (i) improves upon bounds implied by recent work of Collina et\nal. (WINE'20), and is the first optimal prophet inequality for a stationary\nproblem. Result (ii) improves upon a $1-1/e$ bound implied by recent work of\nAouad and Sarita\\c{c} (EC'20), and shows that this prevalent bound in online\nalgorithms is not optimal for this problem.",
    "published_date": "2021-07-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.10516v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2107.10251v2",
    "title": "Using Adversarial Debiasing to Remove Bias from Word Embeddings",
    "authors": [
      "Dana Kenna"
    ],
    "author_ids": [],
    "abstract": "Word Embeddings have been shown to contain the societal biases present in the\noriginal corpora. Existing methods to deal with this problem have been shown to\nonly remove superficial biases. The method of Adversarial Debiasing was\npresumed to be similarly superficial, but this is was not verified in previous\nworks. Using the experiments that demonstrated the shallow removal in other\nmethods, I show results that suggest Adversarial Debiasing is more effective at\nremoving bias and thus motivate further investigation on the utility of\nAdversarial Debiasing.",
    "published_date": "2021-07-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.10251v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.10181v2",
    "title": "Debiasing Multilingual Word Embeddings: A Case Study of Three Indian Languages",
    "authors": [
      "Srijan Bansal",
      "Vishal Garimella",
      "Ayush Suhane",
      "Animesh Mukherjee"
    ],
    "author_ids": [],
    "abstract": "In this paper, we advance the current state-of-the-art method for debiasing\nmonolingual word embeddings so as to generalize well in a multilingual setting.\nWe consider different methods to quantify bias and different debiasing\napproaches for monolingual as well as multilingual settings. We demonstrate the\nsignificance of our bias-mitigation approach on downstream NLP applications.\nOur proposed methods establish the state-of-the-art performance for debiasing\nmultilingual embeddings for three Indian languages - Hindi, Bengali, and Telugu\nin addition to English. We believe that our work will open up new opportunities\nin building unbiased downstream NLP applications that are inherently dependent\non the quality of the word embeddings used.",
    "published_date": "2021-07-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.10181v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.10171v1",
    "title": "Leave-one-out Unfairness",
    "authors": [
      "Emily Black",
      "Matt Fredrikson"
    ],
    "author_ids": [],
    "abstract": "We introduce leave-one-out unfairness, which characterizes how likely a\nmodel's prediction for an individual will change due to the inclusion or\nremoval of a single other person in the model's training data. Leave-one-out\nunfairness appeals to the idea that fair decisions are not arbitrary: they\nshould not be based on the chance event of any one person's inclusion in the\ntraining data. Leave-one-out unfairness is closely related to algorithmic\nstability, but it focuses on the consistency of an individual point's\nprediction outcome over unit changes to the training data, rather than the\nerror of the model in aggregate. Beyond formalizing leave-one-out unfairness,\nwe characterize the extent to which deep models behave leave-one-out unfairly\non real data, including in cases where the generalization error is small.\nFurther, we demonstrate that adversarial training and randomized smoothing\ntechniques have opposite effects on leave-one-out fairness, which sheds light\non the relationships between robustness, memorization, individual fairness, and\nleave-one-out fairness in deep models. Finally, we discuss salient practical\napplications that may be negatively affected by leave-one-out unfairness.",
    "published_date": "2021-07-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY",
      "I.2.0; K.4.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.10171v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.10146v1",
    "title": "A Deep Reinforcement Learning Approach for Fair Traffic Signal Control",
    "authors": [
      "Majid Raeis",
      "Alberto Leon-Garcia"
    ],
    "author_ids": [],
    "abstract": "Traffic signal control is one of the most effective methods of traffic\nmanagement in urban areas. In recent years, traffic control methods based on\ndeep reinforcement learning (DRL) have gained attention due to their ability to\nexploit real-time traffic data, which is often poorly used by the traditional\nhand-crafted methods. While most recent DRL-based methods have focused on\nmaximizing the throughput or minimizing the average travel time of the\nvehicles, the fairness of the traffic signal controllers has often been\nneglected. This is particularly important as neglecting fairness can lead to\nsituations where some vehicles experience extreme waiting times, or where the\nthroughput of a particular traffic flow is highly impacted by the fluctuations\nof another conflicting flow at the intersection. In order to address these\nissues, we introduce two notions of fairness: delay-based and throughput-based\nfairness, which correspond to the two issues mentioned above. Furthermore, we\npropose two DRL-based traffic signal control methods for implementing these\nfairness notions, that can achieve a high throughput as well. We evaluate the\nperformance of our proposed methods using three traffic arrival distributions,\nand find that our methods outperform the baselines in the tested scenarios.",
    "published_date": "2021-07-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.10146v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.10139v2",
    "title": "Generative Models for Security: Attacks, Defenses, and Opportunities",
    "authors": [
      "Luke A. Bauer",
      "Vincent Bindschaedler"
    ],
    "author_ids": [],
    "abstract": "Generative models learn the distribution of data from a sample dataset and\ncan then generate new data instances. Recent advances in deep learning has\nbrought forth improvements in generative model architectures, and some\nstate-of-the-art models can (in some cases) produce outputs realistic enough to\nfool humans.\n  We survey recent research at the intersection of security and privacy and\ngenerative models. In particular, we discuss the use of generative models in\nadversarial machine learning, in helping automate or enhance existing attacks,\nand as building blocks for defenses in contexts such as intrusion detection,\nbiometrics spoofing, and malware obfuscation. We also describe the use of\ngenerative models in diverse applications such as fairness in machine learning,\nprivacy-preserving data synthesis, and steganography. Finally, we discuss new\nthreats due to generative models: the creation of synthetic media such as\ndeepfakes that can be used for disinformation.",
    "published_date": "2021-07-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.10139v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.10093v2",
    "title": "Incentivizing Compliance with Algorithmic Instruments",
    "authors": [
      "Daniel Ngo",
      "Logan Stapleton",
      "Vasilis Syrgkanis",
      "Zhiwei Steven Wu"
    ],
    "author_ids": [],
    "abstract": "Randomized experiments can be susceptible to selection bias due to potential\nnon-compliance by the participants. While much of the existing work has studied\ncompliance as a static behavior, we propose a game-theoretic model to study\ncompliance as dynamic behavior that may change over time. In rounds, a social\nplanner interacts with a sequence of heterogeneous agents who arrive with their\nunobserved private type that determines both their prior preferences across the\nactions (e.g., control and treatment) and their baseline rewards without taking\nany treatment. The planner provides each agent with a randomized recommendation\nthat may alter their beliefs and their action selection. We develop a novel\nrecommendation mechanism that views the planner's recommendation as a form of\ninstrumental variable (IV) that only affects an agents' action selection, but\nnot the observed rewards. We construct such IVs by carefully mapping the\nhistory -- the interactions between the planner and the previous agents -- to a\nrandom recommendation. Even though the initial agents may be completely\nnon-compliant, our mechanism can incentivize compliance over time, thereby\nenabling the estimation of the treatment effect of each treatment, and\nminimizing the cumulative regret of the planner whose goal is to identify the\noptimal treatment.",
    "published_date": "2021-07-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.10093v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.10037v3",
    "title": "A novel method for assessing and measuring homophily in networks through second-order statistics",
    "authors": [
      "Nicola Apollonio",
      "Paolo Giulio Franciosa",
      "Daniele Santoni"
    ],
    "author_ids": [],
    "abstract": "We present a new method for assessing and measuring homophily in networks\nwhose nodes have categorical attributes, namely when the nodes of networks come\npartitioned into classes (colors). We probe this method in two different\nclasses of networks: i) protein-protein interaction (PPI) networks, where nodes\ncorrespond to proteins, partitioned according to their functional role, and\nedges represent functional interactions between proteins ii) Pokec on-line\nsocial network, where nodes correspond to users, partitioned according to their\nage, and edges respresent friendship between users. Similarly to other\nclassical and well consolidated approaches, our method compares the relative\nedge density of the subgraphs induced by each class with the corresponding\nexpected relative edge density under a null model. The novelty of our approach\nconsists in prescribing an endogenous null model, namely, the sample space of\nthe null model is built on the input network itself. This allows us to give\nexact explicit expression for the z-score of the relative edge density of each\nclass as well as other related statistics. The z-scores directly quantify the\nstatistical significance of the observed homophily via Tchebycheff inequality.\nThe expression of each z-score is entered by the network structure through\nbasic combinatorial invariant such as the number of subgraphs with two spanning\nedges. Each z-score is computed in O(n + m) time for a network with n nodes and\nm edges. This leads to an overall efficient computational method for assesing\nhomophily. We complement the analysis of homophily/heterophily by considering\nz-scores of the number of isolated nodes in the subgraphs induced by each\nclass, that are computed in O(nm) time. Theoretical results are then exploited\nto show that, as expected, both the analyzed network classes are significantly\nhomophilic with respect to the considered node properties.",
    "published_date": "2021-07-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DM",
      "68R02",
      "G.3; G.2.2; J.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.10037v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2107.10025v2",
    "title": "Fairness-aware Maximal Clique in Large Graphs: Concepts and Algorithms",
    "authors": [
      "Qi Zhang",
      "Rong-Hua Li",
      "Minjia Pan",
      "Yongheng Dai",
      "Qun Tian",
      "Guoren Wang"
    ],
    "author_ids": [],
    "abstract": "Cohesive subgraph mining on attributed graphs is a fundamental problem in\ngraph data analysis. Existing cohesive subgraph mining algorithms on attributed\ngraphs do not consider the fairness of attributes in the subgraph. In this\npaper, we, for the first time, introduce fairness into the widely-used clique\nmodel to mine fairness-aware cohesive subgraphs. In particular, we propose\nthree novel fairness-aware maximal clique models on attributed graphs, called\nweak fair clique, strong fair clique and relative fair clique, respectively. To\nenumerate all weak fair cliques, we develop an efficient backtracking algorithm\ncalled WFCEnum equipped with a novel colorful k-core based pruning technique.\nWe also propose an efficient enumeration algorithm called SFCEnum to find all\nstrong fair cliques based on a new attribute-alternatively-selection search\ntechnique. To further improve the efficiency, we also present several\nnon-trivial ordering techniques for both weak and strong fair clique\nenumerations. To enumerate all relative fair cliques, we design an enhanced\ncolorful k-core based pruning technique for 2D attribute, and then develop two\nefficient search algorithms: RFCRefineEnum and RFCAlterEnum based on the ideas\nof WFCEnum and SFCEnum for arbitrary dimension attribute. The results of\nextensive experiments on four real-world graphs demonstrate the efficiency,\nscalability and effectiveness of the proposed algorithms.",
    "published_date": "2021-07-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DB"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.10025v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2107.09922v1",
    "title": "Auditing the Biases Enacted by YouTube for Political Topics in Germany",
    "authors": [
      "Hendrik Heuer",
      "Hendrik Hoch",
      "Andreas Breiter",
      "Yannis Theocharis"
    ],
    "author_ids": [],
    "abstract": "With YouTube's growing importance as a news platform, its recommendation\nsystem came under increased scrutiny. Recognizing YouTube's recommendation\nsystem as a broadcaster of media, we explore the applicability of laws that\nrequire broadcasters to give important political, ideological, and social\ngroups adequate opportunity to express themselves in the broadcasted program of\nthe service. We present audits as an important tool to enforce such laws and to\nensure that a system operates in the public's interest. To examine whether\nYouTube is enacting certain biases, we collected video recommendations about\npolitical topics by following chains of ten recommendations per video. Our\nfindings suggest that YouTube's recommendation system is enacting important\nbiases. We find that YouTube is recommending increasingly popular but topically\nunrelated videos. The sadness evoked by the recommended videos decreases while\nthe happiness increases. We discuss the strong popularity bias we identified\nand analyze the link between the popularity of content and emotions. We also\ndiscuss how audits empower researchers and civic hackers to monitor complex\nmachine learning (ML)-based systems like YouTube's recommendation system.",
    "published_date": "2021-07-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.09922v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.13076v1",
    "title": "Interactive Storytelling for Children: A Case-study of Design and Development Considerations for Ethical Conversational AI",
    "authors": [
      "ennifer Chubba",
      "Sondess Missaouib",
      "Shauna Concannonc",
      "Liam Maloneyb",
      "James Alfred Walker"
    ],
    "author_ids": [],
    "abstract": "Conversational Artificial Intelligence (CAI) systems and Intelligent Personal\nAssistants (IPA), such as Alexa, Cortana, Google Home and Siri are becoming\nubiquitous in our lives, including those of children, the implications of which\nis receiving increased attention, specifically with respect to the effects of\nthese systems on children's cognitive, social and linguistic development.\nRecent advances address the implications of CAI with respect to privacy,\nsafety, security, and access. However, there is a need to connect and embed the\nethical and technical aspects in the design. Using a case-study of a research\nand development project focused on the use of CAI in storytelling for children,\nthis paper reflects on the social context within a specific case of technology\ndevelopment, as substantiated and supported by argumentation from within the\nliterature. It describes the decision making process behind the recommendations\nmade on this case for their adoption in the creative industries. Further\nresearch that engages with developers and stakeholders in the ethics of\nstorytelling through CAI is highlighted as a matter of urgency.",
    "published_date": "2021-07-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.13076v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.09546v2",
    "title": "Responsible and Regulatory Conform Machine Learning for Medicine: A Survey of Challenges and Solutions",
    "authors": [
      "Eike Petersen",
      "Yannik Potdevin",
      "Esfandiar Mohammadi",
      "Stephan Zidowitz",
      "Sabrina Breyer",
      "Dirk Nowotka",
      "Sandra Henn",
      "Ludwig Pechmann",
      "Martin Leucker",
      "Philipp Rostalski",
      "Christian Herzog"
    ],
    "author_ids": [],
    "abstract": "Machine learning is expected to fuel significant improvements in medical\ncare. To ensure that fundamental principles such as beneficence, respect for\nhuman autonomy, prevention of harm, justice, privacy, and transparency are\nrespected, medical machine learning systems must be developed responsibly. Many\nhigh-level declarations of ethical principles have been put forth for this\npurpose, but there is a severe lack of technical guidelines explicating the\npractical consequences for medical machine learning. Similarly, there is\ncurrently considerable uncertainty regarding the exact regulatory requirements\nplaced upon medical machine learning systems. This survey provides an overview\nof the technical and procedural challenges involved in creating medical machine\nlearning systems responsibly and in conformity with existing regulations, as\nwell as possible solutions to address these challenges. First, a brief review\nof existing regulations affecting medical machine learning is provided, showing\nthat properties such as safety, robustness, reliability, privacy, security,\ntransparency, explainability, and nondiscrimination are all demanded already by\nexisting law and regulations - albeit, in many cases, to an uncertain degree.\nNext, the key technical obstacles to achieving these desirable properties are\ndiscussed, as well as important techniques to overcome these obstacles in the\nmedical context. We notice that distribution shift, spurious correlations,\nmodel underspecification, uncertainty quantification, and data scarcity\nrepresent severe challenges in the medical context. Promising solution\napproaches include the use of large and representative datasets and federated\nlearning as a means to that end, the careful exploitation of domain knowledge,\nthe use of inherently transparent models, comprehensive out-of-distribution\nmodel testing and verification, as well as algorithmic impact assessments.",
    "published_date": "2021-07-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY",
      "J.3; I.5; I.2.6"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.09546v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.13073v1",
    "title": "Loss of New Ideas: Potentially Long-lasting Effects of the Pandemic on Scientists",
    "authors": [
      "Jian Gao",
      "Yian Yin",
      "Kyle R. Myers",
      "Karim R. Lakhani",
      "Dashun Wang"
    ],
    "author_ids": [],
    "abstract": "Extensive research has documented the immediate impacts of the COVID-19\npandemic on scientists, yet it remains unclear if and how such impacts have\nshifted over time. Here we compare results from two surveys of principal\ninvestigators, conducted between April 2020 and January 2021, along with\nanalyses of large-scale publication data. We find that there has been a clear\nsign of recovery in some regards, as scientists' time spent on their work has\nalmost returned to pre-pandemic levels. However, the latest data also reveals a\nnew dimension in which the pandemic is affecting the scientific workforce: the\nrate of initiating new research projects. Except for the small fraction of\nscientists who directly engaged in COVID-related research, most scientists\nstarted significantly fewer new research projects in 2020. This decline is most\npronounced amongst the same demographic groups of scientists who reported the\nlargest initial disruptions: female scientists and those with young children.\nYet in sharp contrast to the earlier phase of the pandemic, when there were\nlarge disparities across scientific fields, this loss of new projects appears\nremarkably homogeneous across fields. Analyses of large-scale publication data\nreveal a global decline in the rate of new collaborations, especially in\nnon-COVID-related preprints, which is consistent with the reported decline in\nnew projects. Overall, these findings highlight that, while the end of the\npandemic may appear in sight in some countries, its large and unequal impact on\nthe scientific workforce may be enduring, which may have broad implications for\ninequality and the long-term vitality of science.",
    "published_date": "2021-07-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DL",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.13073v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2107.09211v1",
    "title": "Understanding Gender and Racial Disparities in Image Recognition Models",
    "authors": [
      "Rohan Mahadev",
      "Anindya Chakravarti"
    ],
    "author_ids": [],
    "abstract": "Large scale image classification models trained on top of popular datasets\nsuch as Imagenet have shown to have a distributional skew which leads to\ndisparities in prediction accuracies across different subsections of population\ndemographics. A lot of approaches have been made to solve for this\ndistributional skew using methods that alter the model pre, post and during\ntraining. We investigate one such approach - which uses a multi-label softmax\nloss with cross-entropy as the loss function instead of a binary cross-entropy\non a multi-label classification problem on the Inclusive Images dataset which\nis a subset of the OpenImages V6 dataset. We use the MR2 dataset, which\ncontains images of people with self-identified gender and race attributes to\nevaluate the fairness in the model outcomes and try to interpret the mistakes\nby looking at model activations and suggest possible fixes.",
    "published_date": "2021-07-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.09211v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.07627v1",
    "title": "Independent Ethical Assessment of Text Classification Models: A Hate Speech Detection Case Study",
    "authors": [
      "Amitoj Singh",
      "Jingshu Chen",
      "Lihao Zhang",
      "Amin Rasekh",
      "Ilana Golbin",
      "Anand Rao"
    ],
    "author_ids": [],
    "abstract": "An independent ethical assessment of an artificial intelligence system is an\nimpartial examination of the system's development, deployment, and use in\nalignment with ethical values. System-level qualitative frameworks that\ndescribe high-level requirements and component-level quantitative metrics that\nmeasure individual ethical dimensions have been developed over the past few\nyears. However, there exists a gap between the two, which hinders the execution\nof independent ethical assessments in practice. This study bridges this gap and\ndesigns a holistic independent ethical assessment process for a text\nclassification model with a special focus on the task of hate speech detection.\nThe assessment is further augmented with protected attributes mining and\ncounterfactual-based analysis to enhance bias assessment. It covers assessments\nof technical performance, data bias, embedding bias, classification bias, and\ninterpretability. The proposed process is demonstrated through an assessment of\na deep hate speech detection model.",
    "published_date": "2021-07-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.CL",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.07627v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.09163v1",
    "title": "Diversity in Sociotechnical Machine Learning Systems",
    "authors": [
      "Sina Fazelpour",
      "Maria De-Arteaga"
    ],
    "author_ids": [],
    "abstract": "There has been a surge of recent interest in sociocultural diversity in\nmachine learning (ML) research, with researchers (i) examining the benefits of\ndiversity as an organizational solution for alleviating problems with\nalgorithmic bias, and (ii) proposing measures and methods for implementing\ndiversity as a design desideratum in the construction of predictive algorithms.\nCurrently, however, there is a gap between discussions of measures and benefits\nof diversity in ML, on the one hand, and the broader research on the underlying\nconcepts of diversity and the precise mechanisms of its functional benefits, on\nthe other. This gap is problematic because diversity is not a monolithic\nconcept. Rather, different concepts of diversity are based on distinct\nrationales that should inform how we measure diversity in a given context.\nSimilarly, the lack of specificity about the precise mechanisms underpinning\ndiversity's potential benefits can result in uninformative generalities,\ninvalid experimental designs, and illicit interpretations of findings. In this\nwork, we draw on research in philosophy, psychology, and social and\norganizational sciences to make three contributions: First, we introduce a\ntaxonomy of different diversity concepts from philosophy of science, and\nexplicate the distinct epistemic and political rationales underlying these\nconcepts. Second, we provide an overview of mechanisms by which diversity can\nbenefit group performance. Third, we situate these taxonomies--of concepts and\nmechanisms--in the lifecycle of sociotechnical ML systems and make a case for\ntheir usefulness in fair and accountable ML. We do so by illustrating how they\nclarify the discourse around diversity in the context of ML systems, promote\nthe formulation of more precise research questions about diversity's impact,\nand provide conceptual tools to further advance research and practice.",
    "published_date": "2021-07-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.09163v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.08987v3",
    "title": "Analysis of training and seed bias in small molecules generated with a conditional graph-based variational autoencoder -- Insights for practical AI-driven molecule generation",
    "authors": [
      "Seung-gu Kang",
      "Joseph A. Morrone",
      "Jeffrey K. Weber",
      "Wendy D. Cornell"
    ],
    "author_ids": [],
    "abstract": "The application of deep learning to generative molecule design has shown\nearly promise for accelerating lead series development. However, questions\nremain concerning how factors like training, dataset, and seed bias impact the\ntechnology's utility to medicine and computational chemists. In this work, we\nanalyze the impact of seed and training bias on the output of an\nactivity-conditioned graph-based variational autoencoder (VAE). Leveraging a\nmassive, labeled dataset corresponding to the dopamine D2 receptor, our\ngraph-based generative model is shown to excel in producing desired conditioned\nactivities and favorable unconditioned physical properties in generated\nmolecules. We implement an activity swapping method that allows for the\nactivation, deactivation, or retention of activity of molecular seeds, and we\napply independent deep learning classifiers to verify the generative results.\nOverall, we uncover relationships between noise, molecular seeds, and training\nset selection across a range of latent-space sampling procedures, providing\nimportant insights for practical AI-driven molecule generation.",
    "published_date": "2021-07-19T00:00:00",
    "year": 2021,
    "categories": [
      "q-bio.BM",
      "cs.LG",
      "physics.bio-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.08987v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2108.04329v1",
    "title": "Class dependency based learning using Bi-LSTM coupled with the transfer learning of VGG16 for the diagnosis of Tuberculosis from chest x-rays",
    "authors": [
      "G Jignesh Chowdary",
      "Suganya G",
      "Premalatha M",
      "Karunamurthy K"
    ],
    "author_ids": [],
    "abstract": "Tuberculosis is an infectious disease that is leading to the death of\nmillions of people across the world. The mortality rate of this disease is high\nin patients suffering from immuno-compromised disorders. The early diagnosis of\nthis disease can save lives and can avoid further complications. But the\ndiagnosis of TB is a very complex task. The standard diagnostic tests still\nrely on traditional procedures developed in the last century. These procedures\nare slow and expensive. So this paper presents an automatic approach for the\ndiagnosis of TB from posteroanterior chest x-rays. This is a two-step approach,\nwhere in the first step the lung regions are segmented from the chest x-rays\nusing the graph cut method, and then in the second step the transfer learning\nof VGG16 combined with Bi-directional LSTM is used for extracting high-level\ndiscriminative features from the segmented lung regions and then classification\nis performed using a fully connected layer. The proposed model is evaluated\nusing data from two publicly available databases namely Montgomery Country set\nand Schezien set. The proposed model achieved accuracy and sensitivity of\n97.76%, 97.01% and 96.42%, 94.11% on Schezien and Montgomery county datasets.\nThis model enhanced the diagnostic accuracy of TB by 0.7% and 11.68% on\nSchezien and Montgomery county datasets.",
    "published_date": "2021-07-19T00:00:00",
    "year": 2021,
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.04329v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.08928v2",
    "title": "Introducing a Family of Synthetic Datasets for Research on Bias in Machine Learning",
    "authors": [
      "William Blanzeisky",
      "Pádraig Cunningham",
      "Kenneth Kennedy"
    ],
    "author_ids": [],
    "abstract": "A significant impediment to progress in research on bias in machine learning\n(ML) is the availability of relevant datasets. This situation is unlikely to\nchange much given the sensitivity of such data. For this reason, there is a\nrole for synthetic data in this research. In this short paper, we present one\nsuch family of synthetic data sets. We provide an overview of the data,\ndescribe how the level of bias can be varied, and present a simple example of\nan experiment on the data.",
    "published_date": "2021-07-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.08928v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.08892v1",
    "title": "Unsupervised Embedding Learning from Uncertainty Momentum Modeling",
    "authors": [
      "Jiahuan Zhou",
      "Yansong Tang",
      "Bing Su",
      "Ying Wu"
    ],
    "author_ids": [],
    "abstract": "Existing popular unsupervised embedding learning methods focus on enhancing\nthe instance-level local discrimination of the given unlabeled images by\nexploring various negative data. However, the existed sample outliers which\nexhibit large intra-class divergences or small inter-class variations severely\nlimit their learning performance. We justify that the performance limitation is\ncaused by the gradient vanishing on these sample outliers. Moreover, the\nshortage of positive data and disregard for global discrimination consideration\nalso pose critical issues for unsupervised learning but are always ignored by\nexisting methods. To handle these issues, we propose a novel solution to\nexplicitly model and directly explore the uncertainty of the given unlabeled\nlearning samples. Instead of learning a deterministic feature point for each\nsample in the embedding space, we propose to represent a sample by a stochastic\nGaussian with the mean vector depicting its space localization and covariance\nvector representing the sample uncertainty. We leverage such uncertainty\nmodeling as momentum to the learning which is helpful to tackle the outliers.\nFurthermore, abundant positive candidates can be readily drawn from the learned\ninstance-specific distributions which are further adopted to mitigate the\naforementioned issues. Thorough rationale analyses and extensive experiments\nare presented to verify our superiority.",
    "published_date": "2021-07-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.08892v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.08714v1",
    "title": "CETransformer: Casual Effect Estimation via Transformer Based Representation Learning",
    "authors": [
      "Zhenyu Guo",
      "Shuai Zheng",
      "Zhizhe Liu",
      "Kun Yan",
      "Zhenfeng Zhu"
    ],
    "author_ids": [],
    "abstract": "Treatment effect estimation, which refers to the estimation of causal effects\nand aims to measure the strength of the causal relationship, is of great\nimportance in many fields but is a challenging problem in practice. As present,\ndata-driven causal effect estimation faces two main challenges, i.e., selection\nbias and the missing of counterfactual. To address these two issues, most of\nthe existing approaches tend to reduce the selection bias by learning a\nbalanced representation, and then to estimate the counterfactual through the\nrepresentation. However, they heavily rely on the finely hand-crafted metric\nfunctions when learning balanced representations, which generally doesn't work\nwell for the situations where the original distribution is complicated. In this\npaper, we propose a CETransformer model for casual effect estimation via\ntransformer based representation learning. To learn the representation of\ncovariates(features) robustly, a self-supervised transformer is proposed, by\nwhich the correlation between covariates can be well exploited through\nself-attention mechanism. In addition, an adversarial network is adopted to\nbalance the distribution of the treated and control groups in the\nrepresentation space. Experimental results on three real-world datasets\ndemonstrate the advantages of the proposed CETransformer, compared with the\nstate-of-the-art treatment effect estimation methods.",
    "published_date": "2021-07-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.08714v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.09642v1",
    "title": "Linear Invariants for Linear Systems",
    "authors": [
      "Ashish Tiwari"
    ],
    "author_ids": [],
    "abstract": "A central question in verification is characterizing when a system has\ninvariants of a certain form, and then synthesizing them. We say a system has a\n$k$ linear invariant, $k$-LI in short, if it has a conjunction of $k$ linear\n(non-strict) inequalities -- equivalently, an intersection of $k$ (closed) half\nspaces -- as an invariant. We present a sufficient condition -- solely in terms\nof eigenvalues of the $A$-matrix -- for an $n$-dimensional linear dynamical\nsystem to have a $k$-LI. Our proof of sufficiency is constructive, and we get a\nprocedure that computes a $k$-LI if the condition holds. We also present a\nnecessary condition, together with many example linear systems where either the\nsufficient condition, or the necessary is tight, and which show that the gap\nbetween the conditions is not easy to overcome. In practice, the gap implies\nthat using our procedure, we synthesize $k$-LI for a larger value of $k$ than\nwhat might be necessary. Our result enables analysis of continuous and hybrid\nsystems with linear dynamics in their modes solely using reasoning in the\ntheory of linear arithmetic (polygons), without needing reasoning over\nnonlinear arithmetic (ellipsoids).",
    "published_date": "2021-07-18T00:00:00",
    "year": 2021,
    "categories": [
      "math.DS",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.09642v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2107.08045v2",
    "title": "Desiderata for Explainable AI in statistical production systems of the European Central Bank",
    "authors": [
      "Carlos Mougan",
      "Georgios Kanellos",
      "Thomas Gottron"
    ],
    "author_ids": [],
    "abstract": "Explainable AI constitutes a fundamental step towards establishing fairness\nand addressing bias in algorithmic decision-making. Despite the large body of\nwork on the topic, the benefit of solutions is mostly evaluated from a\nconceptual or theoretical point of view and the usefulness for real-world use\ncases remains uncertain. In this work, we aim to state clear user-centric\ndesiderata for explainable AI reflecting common explainability needs\nexperienced in statistical production systems of the European Central Bank. We\nlink the desiderata to archetypical user roles and give examples of techniques\nand methods which can be used to address the user's needs. To this end, we\nprovide two concrete use cases from the domain of statistical data production\nin central banks: the detection of outliers in the Centralised Securities\nDatabase and the data-driven identification of data quality checks for the\nSupervisory Banking data system.",
    "published_date": "2021-07-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "I.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.08045v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.08362v1",
    "title": "Probabilistic Verification of Neural Networks Against Group Fairness",
    "authors": [
      "Bing Sun",
      "Jun Sun",
      "Ting Dai",
      "Lijun Zhang"
    ],
    "author_ids": [],
    "abstract": "Fairness is crucial for neural networks which are used in applications with\nimportant societal implication. Recently, there have been multiple attempts on\nimproving fairness of neural networks, with a focus on fairness testing (e.g.,\ngenerating individual discriminatory instances) and fairness training (e.g.,\nenhancing fairness through augmented training). In this work, we propose an\napproach to formally verify neural networks against fairness, with a focus on\nindependence-based fairness such as group fairness. Our method is built upon an\napproach for learning Markov Chains from a user-provided neural network (i.e.,\na feed-forward neural network or a recurrent neural network) which is\nguaranteed to facilitate sound analysis. The learned Markov Chain not only\nallows us to verify (with Probably Approximate Correctness guarantee) whether\nthe neural network is fair or not, but also facilities sensitivity analysis\nwhich helps to understand why fairness is violated. We demonstrate that with\nour analysis results, the neural weights can be optimized to improve fairness.\nOur approach has been evaluated with multiple models trained on benchmark\ndatasets and the experiment results show that our approach is effective and\nefficient.",
    "published_date": "2021-07-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.08362v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.08310v5",
    "title": "FairBalance: How to Achieve Equalized Odds With Data Pre-processing",
    "authors": [
      "Zhe Yu",
      "Joymallya Chakraborty",
      "Tim Menzies"
    ],
    "author_ids": [],
    "abstract": "This research seeks to benefit the software engineering society by providing\na simple yet effective pre-processing approach to achieve equalized odds\nfairness in machine learning software. Fairness issues have attracted\nincreasing attention since machine learning software is increasingly used for\nhigh-stakes and high-risk decisions. Amongst all the existing fairness notions,\nthis work specifically targets \"equalized odds\" given its advantage in always\nallowing perfect classifiers. Equalized odds requires that members of every\ndemographic group do not receive disparate mistreatment. Prior works either\noptimize for an equalized odds related metric during the learning process like\na black-box, or manipulate the training data following some intuition. This\nwork studies the root cause of the violation of equalized odds and how to\ntackle it. We found that equalizing the class distribution in each demographic\ngroup with sample weights is a necessary condition for achieving equalized odds\nwithout modifying the normal training process. In addition, an important\npartial condition for equalized odds (zero average odds difference) can be\nguaranteed when the class distributions are weighted to be not only equal but\nalso balanced (1:1). Based on these analyses, we proposed FairBalance, a\npre-processing algorithm which balances the class distribution in each\ndemographic group by assigning calculated weights to the training data. On\neight real-world datasets, our empirical results show that, at low\ncomputational overhead, the proposed pre-processing algorithm FairBalance can\nsignificantly improve equalized odds without much, if any damage to the\nutility. FairBalance also outperforms existing state-of-the-art approaches in\nterms of equalized odds. To facilitate reuse, reproduction, and validation, we\nmade our scripts available at https://github.com/hil-se/FairBalance.",
    "published_date": "2021-07-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.08310v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.08271v3",
    "title": "Fair Equilibria in Sponsored Search Auctions: The Advertisers' Perspective",
    "authors": [
      "Georgios Birmpas",
      "Andrea Celli",
      "Riccardo Colini-Baldeschi",
      "Stefano Leonardi"
    ],
    "author_ids": [],
    "abstract": "In this work we introduce a new class of mechanisms composed of a traditional\nGeneralized Second Price (GSP) auction and a fair division scheme, in order to\nachieve some desired level of fairness between groups of Bayesian strategic\nadvertisers. We propose two mechanisms, $\\beta$-Fair GSP and GSP-EFX, that\ncompose GSP with, respectively, an envy-free up to one item, and an envy-free\nup to any item fair division scheme. The payments of GSP are adjusted in order\nto compensate advertisers that suffer a loss of efficiency due the fair\ndivision stage. We investigate the strategic learning implications of the\ndeployment of sponsored search auction mechanisms that obey to such fairness\ncriteria. We prove that, for both mechanisms, if bidders play so as to minimize\ntheir external regret they are guaranteed to reach an equilibrium with good\nsocial welfare. We also prove that the mechanisms are budget balanced, so that\nthe payments charged by the traditional GSP mechanism are a good proxy of the\ntotal compensation offered to the advertisers. Finally, we evaluate the quality\nof the allocations through experiments on real-world data.",
    "published_date": "2021-07-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.08271v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2107.08176v2",
    "title": "Automatic Fairness Testing of Neural Classifiers through Adversarial Sampling",
    "authors": [
      "Peixin Zhang",
      "Jingyi Wang",
      "Jun Sun",
      "Xinyu Wang",
      "Guoliang Dong",
      "Xingen Wang",
      "Ting Dai",
      "Jin Song Dong"
    ],
    "author_ids": [],
    "abstract": "Although deep learning has demonstrated astonishing performance in many\napplications, there are still concerns about its dependability. One desirable\nproperty of deep learning applications with societal impact is fairness (i.e.,\nnon-discrimination). Unfortunately, discrimination might be intrinsically\nembedded into the models due to the discrimination in the training data. As a\ncountermeasure, fairness testing systemically identifies discriminatory\nsamples, which can be used to retrain the model and improve the model's\nfairness. Existing fairness testing approaches however have two major\nlimitations. Firstly, they only work well on traditional machine learning\nmodels and have poor performance (e.g., effectiveness and efficiency) on deep\nlearning models. Secondly, they only work on simple structured (e.g., tabular)\ndata and are not applicable for domains such as text. In this work, we bridge\nthe gap by proposing a scalable and effective approach for systematically\nsearching for discriminatory samples while extending existing fairness testing\napproaches to address a more challenging domain, i.e., text classification.\nCompared with state-of-the-art methods, our approach only employs lightweight\nprocedures like gradient computation and clustering, which is significantly\nmore scalable and effective. Experimental results show that on average, our\napproach explores the search space much more effectively (9.62 and 2.38 times\nmore than the state-of-the-art methods respectively on tabular and text\ndatasets) and generates much more discriminatory samples (24.95 and 2.68 times)\nwithin a same reasonable time. Moreover, the retrained models reduce\ndiscrimination by 57.2% and 60.2% respectively on average.",
    "published_date": "2021-07-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.08176v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.08122v1",
    "title": "Future Intelligent Autonomous Robots, Ethical by Design. Learning from Autonomous Cars Ethics",
    "authors": [
      "Gordana Dodig-Crnkovic",
      "Tobias Holstein",
      "Patrizio Pelliccione"
    ],
    "author_ids": [],
    "abstract": "Development of the intelligent autonomous robot technology presupposes its\nanticipated beneficial effect on the individuals and societies. In the case of\nsuch disruptive emergent technology, not only questions of how to build, but\nalso why to build and with what consequences are important. The field of ethics\nof intelligent autonomous robotic cars is a good example of research with\nactionable practical value, where a variety of stakeholders, including the\nlegal system and other societal and governmental actors, as well as companies\nand businesses, collaborate bringing about shared view of ethics and societal\naspects of technology. It could be used as a starting platform for the\napproaches to the development of intelligent autonomous robots in general,\nconsidering human-machine interfaces in different phases of the life cycle of\ntechnology - the development, implementation, testing, use and disposal.\nDrawing from our work on ethics of autonomous intelligent robocars, and the\nexisting literature on ethics of robotics, our contribution consists of a set\nof values and ethical principles with identified challenges and proposed\napproaches for meeting them. This may help stakeholders in the field of\nintelligent autonomous robotics to connect ethical principles with their\napplications. Our recommendations of ethical requirements for autonomous cars\ncan be used for other types of intelligent autonomous robots, with the caveat\nfor social robots that require more research regarding interactions with the\nusers. We emphasize that existing ethical frameworks need to be applied in a\ncontext-sensitive way, by assessments in interdisciplinary, multi-competent\nteams through multi-criteria analysis. Furthermore, we argue for the need of a\ncontinuous development of ethical principles, guidelines, and regulations,\ninformed by the progress of technologies and involving relevant stakeholders.",
    "published_date": "2021-07-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.08122v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.08090v2",
    "title": "Near-Optimal Algorithms for Linear Algebra in the Current Matrix Multiplication Time",
    "authors": [
      "Nadiia Chepurko",
      "Kenneth L. Clarkson",
      "Praneeth Kacham",
      "David P. Woodruff"
    ],
    "author_ids": [],
    "abstract": "In the numerical linear algebra community, it was suggested that to obtain\nnearly optimal bounds for various problems such as rank computation, finding a\nmaximal linearly independent subset of columns (a basis), regression, or\nlow-rank approximation, a natural way would be to resolve the main open\nquestion of Nelson and Nguyen (FOCS, 2013). This question is regarding the\nlogarithmic factors in the sketching dimension of existing oblivious subspace\nembeddings that achieve constant-factor approximation. We show how to bypass\nthis question using a refined sketching technique, and obtain optimal or nearly\noptimal bounds for these problems. A key technique we use is an explicit\nmapping of Indyk based on uncertainty principles and extractors, which after\nfirst applying known oblivious subspace embeddings, allows us to quickly spread\nout the mass of the vector so that sampling is now effective. We thereby avoid\na logarithmic factor in the sketching dimension that is standard in bounds\nproven using the matrix Chernoff inequality. For the fundamental problems of\nrank computation and finding a basis, our algorithms improve Cheung, Kwok, and\nLau (JACM, 2013), and are optimal to within a constant factor and a poly(log\nlog(n))-factor, respectively. Further, for constant-factor regression and\nlow-rank approximation we give the first optimal algorithms, for the current\nmatrix multiplication exponent.",
    "published_date": "2021-07-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DS",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.08090v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.07919v2",
    "title": "A Survey on Bias in Visual Datasets",
    "authors": [
      "Simone Fabbrizzi",
      "Symeon Papadopoulos",
      "Eirini Ntoutsi",
      "Ioannis Kompatsiaris"
    ],
    "author_ids": [],
    "abstract": "Computer Vision (CV) has achieved remarkable results, outperforming humans in\nseveral tasks. Nonetheless, it may result in significant discrimination if not\nhandled properly as CV systems highly depend on the data they are fed with and\ncan learn and amplify biases within such data. Thus, the problems of\nunderstanding and discovering biases are of utmost importance. Yet, there is no\ncomprehensive survey on bias in visual datasets. Hence, this work aims to: i)\ndescribe the biases that might manifest in visual datasets; ii) review the\nliterature on methods for bias discovery and quantification in visual datasets;\niii) discuss existing attempts to collect bias-aware visual datasets. A key\nconclusion of our study is that the problem of bias discovery and\nquantification in visual datasets is still open, and there is room for\nimprovement in terms of both methods and the range of biases that can be\naddressed. Moreover, there is no such thing as a bias-free dataset, so\nscientists and practitioners must become aware of the biases in their datasets\nand make them explicit. To this end, we propose a checklist to spot different\ntypes of bias during visual dataset collection.",
    "published_date": "2021-07-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.07919v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.07754v1",
    "title": "Measuring Fairness in Generative Models",
    "authors": [
      "Christopher T. H Teo",
      "Ngai-Man Cheung"
    ],
    "author_ids": [],
    "abstract": "Deep generative models have made much progress in improving training\nstability and quality of generated data. Recently there has been increased\ninterest in the fairness of deep-generated data. Fairness is important in many\napplications, e.g. law enforcement, as biases will affect efficacy. Central to\nfair data generation are the fairness metrics for the assessment and evaluation\nof different generative models. In this paper, we first review fairness metrics\nproposed in previous works and highlight potential weaknesses. We then discuss\na performance benchmark framework along with the assessment of alternative\nmetrics.",
    "published_date": "2021-07-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.07754v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.07705v1",
    "title": "Pseudo-labelling Enhanced Media Bias Detection",
    "authors": [
      "Qin Ruan",
      "Brian Mac Namee",
      "Ruihai Dong"
    ],
    "author_ids": [],
    "abstract": "Leveraging unlabelled data through weak or distant supervision is a\ncompelling approach to developing more effective text classification models.\nThis paper proposes a simple but effective data augmentation method, which\nleverages the idea of pseudo-labelling to select samples from noisy distant\nsupervision annotation datasets. The result shows that the proposed method\nimproves the accuracy of biased news detection models.",
    "published_date": "2021-07-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.IR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.07705v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.07691v1",
    "title": "Intersectional Bias in Causal Language Models",
    "authors": [
      "Liam Magee",
      "Lida Ghahremanlou",
      "Karen Soldatic",
      "Shanthi Robertson"
    ],
    "author_ids": [],
    "abstract": "To examine whether intersectional bias can be observed in language\ngeneration, we examine \\emph{GPT-2} and \\emph{GPT-NEO} models, ranging in size\nfrom 124 million to ~2.7 billion parameters. We conduct an experiment combining\nup to three social categories - gender, religion and disability - into\nunconditional or zero-shot prompts used to generate sentences that are then\nanalysed for sentiment. Our results confirm earlier tests conducted with\nauto-regressive causal models, including the \\emph{GPT} family of models. We\nalso illustrate why bias may be resistant to techniques that target single\ncategories (e.g. gender, religion and race), as it can also manifest, in often\nsubtle ways, in texts prompted by concatenated social categories. To address\nthese difficulties, we suggest technical and community-based approaches need to\ncombine to acknowledge and address complex and intersectional language model\nbias.",
    "published_date": "2021-07-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.07691v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.07540v3",
    "title": "Fast First-Order Algorithm for Large-Scale Max-Min Fair Multi-Group Multicast Beamforming",
    "authors": [
      "Chong Zhang",
      "Min Dong",
      "Ben Liang"
    ],
    "author_ids": [],
    "abstract": "We propose a first-order fast algorithm for the weighted max-min fair (MMF)\nmulti-group multicast beamforming problem in large-scale systems. Utilizing the\noptimal multicast beamforming structure obtained recently, we convert the\nnonconvex MMF problem into a min-max weight minimization problem and show that\nit is a weakly convex problem. We propose using the projected subgradient\nalgorithm (PSA) to solve the problem directly, instead of the conventional\nmethod that requires iteratively solving its inverse problem. We show that PSA\nfor our problem has closed-form updates and thus is computationally cheap.\nFurthermore, PSA converges to a near-stationary point of our problem within\nfinite time. Simulation results show that our PSA-based algorithm offers\nnear-optimal performance with considerably lower computational complexity than\nexisting methods for large-scale systems.",
    "published_date": "2021-07-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.07540v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2107.07356v2",
    "title": "DiRe Committee : Diversity and Representation Constraints in Multiwinner Elections",
    "authors": [
      "Kunal Relia"
    ],
    "author_ids": [],
    "abstract": "The study of fairness in multiwinner elections focuses on settings where\ncandidates have attributes. However, voters may also be divided into predefined\npopulations under one or more attributes (e.g., \"California\" and \"Illinois\"\npopulations under the \"state\" attribute), which may be same or different from\ncandidate attributes. The models that focus on candidate attributes alone may\nsystematically under-represent smaller voter populations. Hence, we develop a\nmodel, DiRe Committee WinnerDetermination (DRCWD), which delineates candidate\nand voter attributes to select a committee by specifying diversity and\nrepresentation constraints and a voting rule. We analyze its computational\ncomplexity, inapproximability, and parameterized complexity. We develop a\nheuristic-based algorithm, which finds the winning DiRe committee in under two\nminutes on 63% of the instances of synthetic datasets and on 100% of instances\nof real-world datasets. We present an empirical analysis of the running time,\nfeasibility, and utility traded-off.\n  Overall, DRCWD motivates that a study of multiwinner elections should\nconsider both its actors, namely candidates and voters, as candidate-specific\nmodels can unknowingly harm voter populations, and vice versa. Additionally,\neven when the attributes of candidates and voters coincide, it is important to\ntreat them separately as diversity does not imply representation and vice\nversa. This is to say that having a female candidate on the committee, for\nexample, is different from having a candidate on the committee who is preferred\nby the female voters, and who themselves may or may not be female.",
    "published_date": "2021-07-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.07356v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.07249v1",
    "title": "Empowered and Embedded: Ethics and Agile Processes",
    "authors": [
      "Niina Zuber",
      "Severin Kacianka",
      "Jan Gogoll",
      "Alexander Pretschner",
      "Julian Nida-Rümelin"
    ],
    "author_ids": [],
    "abstract": "In this article we focus on the structural aspects of the development of\nethical software, and argue that ethical considerations need to be embedded\ninto the (agile) software development process. In fact, we claim that agile\nprocesses of software development lend themselves specifically well for this\nendeavour. First, we contend that ethical evaluations need to go beyond the use\nof software products and include an evaluation of the software itself. This\nimplies that software engineers influence peoples' lives through the features\nof their designed products. Embedded values are thus approached best by\nsoftware engineers themselves. Therefore, we put emphasis on the possibility to\nimplement ethical deliberations in already existing and well established agile\nsoftware development processes. Our approach relies on software engineers\nmaking their own judgments throughout the entire development process to ensure\nthat technical features and ethical evaluation can be addressed adequately to\ntransport and foster desirable values and norms. We argue that agile software\ndevelopment processes may help the implementation of ethical deliberation for\nfive reasons: 1) agile methods are widely spread, 2) their emphasis on flat\nhierarchies promotes independent thinking, 3) their reliance on existing team\nstructures serve as an incubator for deliberation, 4) agile development\nenhances object-focused techno-ethical realism, and, finally, 5) agile\nstructures provide a salient endpoint to deliberation.",
    "published_date": "2021-07-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SE",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.07249v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2107.07087v3",
    "title": "Entropic Inequality Constraints from $e$-separation Relations in Directed Acyclic Graphs with Hidden Variables",
    "authors": [
      "Noam Finkelstein",
      "Beata Zjawin",
      "Elie Wolfe",
      "Ilya Shpitser",
      "Robert W. Spekkens"
    ],
    "author_ids": [],
    "abstract": "Directed acyclic graphs (DAGs) with hidden variables are often used to\ncharacterize causal relations between variables in a system. When some\nvariables are unobserved, DAGs imply a notoriously complicated set of\nconstraints on the distribution of observed variables. In this work, we present\nentropic inequality constraints that are implied by $e$-separation relations in\nhidden variable DAGs with discrete observed variables. The constraints can\nintuitively be understood to follow from the fact that the capacity of\nvariables along a causal pathway to convey information is restricted by their\nentropy; e.g. at the extreme case, a variable with entropy $0$ can convey no\ninformation. We show how these constraints can be used to learn about the true\ncausal model from an observed data distribution. In addition, we propose a\nmeasure of causal influence called the minimal mediary entropy, and demonstrate\nthat it can augment traditional measures such as the average causal effect.",
    "published_date": "2021-07-15T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.07087v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.07083v4",
    "title": "Combatting Gerrymandering with Social Choice: the Design of Multi-member Districts",
    "authors": [
      "Nikhil Garg",
      "Wes Gurnee",
      "David Rothschild",
      "David Shmoys"
    ],
    "author_ids": [],
    "abstract": "Every representative democracy must specify a mechanism under which voters\nchoose their representatives. The most common mechanism in the United States --\nWinner takes all single-member districts -- both enables substantial partisan\ngerrymandering and constrains `fair' redistricting, preventing proportional\nrepresentation in legislatures. We study the design of multi-member districts\n(MMDs), in which each district elects multiple representatives, potentially\nthrough a non-Winner takes all voting rule. We carry out large-scale empirical\nanalyses for the U.S. House of Representatives under MMDs with different social\nchoice functions, under algorithmically generated maps optimized for either\npartisan benefit or proportionality. Doing so requires efficiently\nincorporating predicted partisan outcomes -- under various multi-winner social\nchoice functions -- into an algorithm that optimizes over an ensemble of maps.\nWe find that with three-member districts using Single Transferable Vote,\nfairness-minded independent commissions would be able to achieve proportional\noutcomes in every state up to rounding, and advantage-seeking partisans would\nhave their power to gerrymander significantly curtailed. Simultaneously, such\ndistricts would preserve geographic cohesion, an arguably important aspect of\nrepresentative democracies. In the process, we advance a rich research agenda\nat the intersection of social choice and computational gerrymandering.",
    "published_date": "2021-07-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.07083v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2107.06944v2",
    "title": "On the impossibility of non-trivial accuracy under fairness constraints",
    "authors": [
      "Carlos Pinzón",
      "Catuscia Palamidessi",
      "Pablo Piantanida",
      "Frank Valencia"
    ],
    "author_ids": [],
    "abstract": "One of the main concerns about fairness in machine learning (ML) is that, in\norder to achieve it, one may have to trade off some accuracy. To overcome this\nissue, Hardt et al. proposed the notion of equality of opportunity (EO), which\nis compatible with maximal accuracy when the target label is deterministic with\nrespect to the input features.\n  In the probabilistic case, however, the issue is more complicated: It has\nbeen shown that under differential privacy constraints, there are data sources\nfor which EO can only be achieved at the total detriment of accuracy, in the\nsense that a classifier that satisfies EO cannot be more accurate than a\ntrivial (i.e., constant) classifier. In our paper we strengthen this result by\nremoving the privacy constraint. Namely, we show that for certain data sources,\nthe most accurate classifier that satisfies EO is a trivial classifier.\nFurthermore, we study the trade-off between accuracy and EO loss (opportunity\ndifference), and provide a sufficient condition on the data source under which\nEO and non-trivial accuracy are compatible.",
    "published_date": "2021-07-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.06944v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.14044v1",
    "title": "Ethical AI for Social Good",
    "authors": [
      "Ramya Akula",
      "Ivan Garibay"
    ],
    "author_ids": [],
    "abstract": "The concept of AI for Social Good(AI4SG) is gaining momentum in both\ninformation societies and the AI community. Through all the advancement of\nAI-based solutions, it can solve societal issues effectively. To date, however,\nthere is only a rudimentary grasp of what constitutes AI socially beneficial in\nprinciple, what constitutes AI4SG in reality, and what are the policies and\nregulations needed to ensure it. This paper fills the vacuum by addressing the\nethical aspects that are critical for future AI4SG efforts. Some of these\ncharacteristics are new to AI, while others have greater importance due to its\nusage.",
    "published_date": "2021-07-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.14044v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.14046v1",
    "title": "Audit and Assurance of AI Algorithms: A framework to ensure ethical algorithmic practices in Artificial Intelligence",
    "authors": [
      "Ramya Akula",
      "Ivan Garibay"
    ],
    "author_ids": [],
    "abstract": "Algorithms are becoming more widely used in business, and businesses are\nbecoming increasingly concerned that their algorithms will cause significant\nreputational or financial damage. We should emphasize that any of these damages\nstem from situations in which the United States lacks strict legislative\nprohibitions or specified protocols for measuring damages. As a result,\ngovernments are enacting legislation and enforcing prohibitions, regulators are\nfining businesses, and the judiciary is debating whether or not to make\nartificially intelligent computer models as the decision-makers in the eyes of\nthe law. From autonomous vehicles and banking to medical care, housing, and\nlegal decisions, there will soon be enormous amounts of algorithms that make\ndecisions with limited human interference. Governments, businesses, and society\nwould have an algorithm audit, which would have systematic verification that\nalgorithms are lawful, ethical, and secure, similar to financial audits. A\nmodern market, auditing, and assurance of algorithms developed to\nprofessionalize and industrialize AI, machine learning, and related algorithms.\nStakeholders of this emerging field include policymakers and regulators, along\nwith industry experts and entrepreneurs. In addition, we foresee audit\nthresholds and frameworks providing valuable information to all who are\nconcerned with governance and standardization. This paper aims to review the\ncritical areas required for auditing and assurance and spark discussion in this\nnovel field of study and practice.",
    "published_date": "2021-07-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.14046v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.06720v2",
    "title": "Fairness in Ranking under Uncertainty",
    "authors": [
      "Ashudeep Singh",
      "David Kempe",
      "Thorsten Joachims"
    ],
    "author_ids": [],
    "abstract": "Fairness has emerged as an important consideration in algorithmic\ndecision-making. Unfairness occurs when an agent with higher merit obtains a\nworse outcome than an agent with lower merit. Our central point is that a\nprimary cause of unfairness is uncertainty. A principal or algorithm making\ndecisions never has access to the agents' true merit, and instead uses proxy\nfeatures that only imperfectly predict merit (e.g., GPA, star ratings,\nrecommendation letters). None of these ever fully capture an agent's merit; yet\nexisting approaches have mostly been defining fairness notions directly based\non observed features and outcomes.\n  Our primary point is that it is more principled to acknowledge and model the\nuncertainty explicitly. The role of observed features is to give rise to a\nposterior distribution of the agents' merits. We use this viewpoint to define a\nnotion of approximate fairness in ranking. We call an algorithm $\\phi$-fair\n(for $\\phi \\in [0,1]$) if it has the following property for all agents $x$ and\nall $k$: if agent $x$ is among the top $k$ agents with respect to merit with\nprobability at least $\\rho$ (according to the posterior merit distribution),\nthen the algorithm places the agent among the top $k$ agents in its ranking\nwith probability at least $\\phi \\rho$.\n  We show how to compute rankings that optimally trade off approximate fairness\nagainst utility to the principal. In addition to the theoretical\ncharacterization, we present an empirical analysis of the potential impact of\nthe approach in simulation studies. For real-world validation, we applied the\napproach in the context of a paper recommendation system that we built and\nfielded at the KDD 2020 conference.",
    "published_date": "2021-07-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.06720v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.06678v2",
    "title": "Optimal Power Allocation in Downlink Multicarrier NOMA Systems: Theory and Fast Algorithms",
    "authors": [
      "Sepehr Rezvani",
      "Eduard A. Jorswieck",
      "Roghayeh Joda",
      "Halim Yanikomeroglu"
    ],
    "author_ids": [],
    "abstract": "In this work, we address the problem of finding globally optimal power\nallocation strategies to maximize the users sum-rate (SR) as well as system\nenergy efficiency (EE) in the downlink of single-cell multicarrier\nnon-orthogonal multiple access (MC-NOMA) systems. Each NOMA cluster includes a\nset of users in which the well-known superposition coding (SC) combined with\nsuccessive interference cancellation (SIC) technique is applied among them. By\nobtaining the closed-form expression of intra-cluster power allocation, we show\nthat MC-NOMA can be equivalently transformed to a virtual orthogonal multiple\naccess (OMA) system, where the effective channel gain of these virtual OMA\nusers is obtained in closed-form. Then, the SR and EE maximization problems are\nsolved by using very fast water-filling and Dinkelbach algorithms,\nrespectively. The equivalent transformation of MC-NOMA to the virtual OMA\nsystem brings new theoretical insights, which are discussed throughout the\npaper. The extensions of our analysis to other scenarios, such as considering\nusers rate fairness, admission control, long-term performance, and a number of\nfuture next-generation multiple access (NGMA) schemes enabling recent advanced\ntechnologies, e.g., reconfigurable intelligent surfaces are discussed.\nExtensive numerical results are provided to show the performance gaps between\nsingle-carrier NOMA (SC-NOMA), OMA-NOMA, and OMA.",
    "published_date": "2021-07-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IT",
      "eess.SP",
      "math.IT",
      "90C11",
      "G.1.6"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.06678v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2107.06468v1",
    "title": "Sampling on NISQ Devices: \"Who's the Fairest One of All?\"",
    "authors": [
      "Elijah Pelofske",
      "John Golden",
      "Andreas Bärtschi",
      "Daniel O'Malley",
      "Stephan Eidenbenz"
    ],
    "author_ids": [],
    "abstract": "Modern NISQ devices are subject to a variety of biases and sources of noise\nthat degrade the solution quality of computations carried out on these devices.\nA natural question that arises in the NISQ era, is how fairly do these devices\nsample ground state solutions. To this end, we run five fair sampling problems\n(each with at least three ground state solutions) that are based both on\nquantum annealing and on the Grover Mixer-QAOA algorithm for gate-based NISQ\nhardware. In particular, we use seven IBM~Q devices, the Aspen-9 Rigetti\ndevice, the IonQ device, and three D-Wave quantum annealers. For each of the\nfair sampling problems, we measure the ground state probability, the relative\nfairness of the frequency of each ground state solution with respect to the\nother ground state solutions, and the aggregate error as given by each hardware\nprovider. Overall, our results show that NISQ devices do not achieve fair\nsampling yet. We also observe differences in the software stack with a\nparticular focus on compilation techniques that illustrate what work will still\nneed to be done to achieve a seamless integration of frontend (i.e. quantum\ncircuit description) and backend compilation.",
    "published_date": "2021-07-14T00:00:00",
    "year": 2021,
    "categories": [
      "quant-ph",
      "cond-mat.stat-mech",
      "cs.ET"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.06468v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2107.06243v2",
    "title": "Fairness-aware Summarization for Justified Decision-Making",
    "authors": [
      "Moniba Keymanesh",
      "Tanya Berger-Wolf",
      "Micha Elsner",
      "Srinivasan Parthasarathy"
    ],
    "author_ids": [],
    "abstract": "In consequential domains such as recidivism prediction, facility inspection,\nand benefit assignment, it's important for individuals to know the\ndecision-relevant information for the model's prediction. In addition,\npredictions should be fair both in terms of the outcome and the justification\nof the outcome. In other words, decision-relevant features should provide\nsufficient information for the predicted outcome and should be independent of\nthe membership of individuals in protected groups such as race and gender. In\nthis work, we focus on the problem of (un)fairness in the justification of the\ntext-based neural models. We tie the explanatory power of the model to fairness\nin the outcome and propose a fairness-aware summarization mechanism to detect\nand counteract the bias in such models. Given a potentially biased natural\nlanguage explanation for a decision, we use a multi-task neural model and an\nattribution mechanism based on integrated gradients to extract high-utility and\nlow-bias justifications in form of a summary. The extracted summary is then\nused for training a model to make decisions for individuals. Results on several\nreal world datasets suggest that our method drastically limits the demographic\nleakage in the input (fairness in justification) while moderately enhancing the\nfairness in the outcome. Our model is also effective in detecting and\ncounteracting several types of data poisoning attacks that synthesize\nrace-coded reasoning or irrelevant justifications.",
    "published_date": "2021-07-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.06243v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.06197v1",
    "title": "Generative Adversarial Learning via Kernel Density Discrimination",
    "authors": [
      "Abdelhak Lemkhenter",
      "Adam Bielski",
      "Alp Eren Sari",
      "Paolo Favaro"
    ],
    "author_ids": [],
    "abstract": "We introduce Kernel Density Discrimination GAN (KDD GAN), a novel method for\ngenerative adversarial learning. KDD GAN formulates the training as a\nlikelihood ratio optimization problem where the data distributions are written\nexplicitly via (local) Kernel Density Estimates (KDE). This is inspired by the\nrecent progress in contrastive learning and its relation to KDE. We define the\nKDEs directly in feature space and forgo the requirement of invertibility of\nthe kernel feature mappings. In our approach, features are no longer optimized\nfor linear separability, as in the original GAN formulation, but for the more\ngeneral discrimination of distributions in the feature space. We analyze the\ngradient of our loss with respect to the feature representation and show that\nit is better behaved than that of the original hinge loss. We perform\nexperiments with the proposed KDE-based loss, used either as a training loss or\na regularization term, on both CIFAR10 and scaled versions of ImageNet. We use\nBigGAN/SA-GAN as a backbone and baseline, since our focus is not to design the\narchitecture of the networks. We show a boost in the quality of generated\nsamples with respect to FID from 10% to 40% compared to the baseline. Code will\nbe made available.",
    "published_date": "2021-07-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.06197v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.06028v2",
    "title": "Lifting the Convex Conjugate in Lagrangian Relaxations: A Tractable Approach for Continuous Markov Random Fields",
    "authors": [
      "Hartmut Bauermeister",
      "Emanuel Laude",
      "Thomas Möllenhoff",
      "Michael Moeller",
      "Daniel Cremers"
    ],
    "author_ids": [],
    "abstract": "Dual decomposition approaches in nonconvex optimization may suffer from a\nduality gap. This poses a challenge when applying them directly to nonconvex\nproblems such as MAP-inference in a Markov random field (MRF) with continuous\nstate spaces. To eliminate such gaps, this paper considers a reformulation of\nthe original nonconvex task in the space of measures. This infinite-dimensional\nreformulation is then approximated by a semi-infinite one, which is obtained\nvia a piecewise polynomial discretization in the dual. We provide a geometric\nintuition behind the primal problem induced by the dual discretization and draw\nconnections to optimization over moment spaces. In contrast to existing\ndiscretizations which suffer from a grid bias, we show that a piecewise\npolynomial discretization better preserves the continuous nature of our\nproblem. Invoking results from optimal transport theory and convex algebraic\ngeometry we reduce the semi-infinite program to a finite one and provide a\npractical implementation based on semidefinite programming. We show,\nexperimentally and in theory, that the approach successfully reduces the\nduality gap. To showcase the scalability of our approach, we apply it to the\nstereo matching problem between two images.",
    "published_date": "2021-07-13T00:00:00",
    "year": 2021,
    "categories": [
      "math.OC",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.06028v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.05987v1",
    "title": "Generating Gender Augmented Data for NLP",
    "authors": [
      "Nishtha Jain",
      "Maja Popovic",
      "Declan Groves",
      "Eva Vanmassenhove"
    ],
    "author_ids": [],
    "abstract": "Gender bias is a frequent occurrence in NLP-based applications, especially\npronounced in gender-inflected languages. Bias can appear through associations\nof certain adjectives and animate nouns with the natural gender of referents,\nbut also due to unbalanced grammatical gender frequencies of inflected words.\nThis type of bias becomes more evident in generating conversational utterances\nwhere gender is not specified within the sentence, because most current NLP\napplications still work on a sentence-level context. As a step towards more\ninclusive NLP, this paper proposes an automatic and generalisable rewriting\napproach for short conversational sentences. The rewriting method can be\napplied to sentences that, without extra-sentential context, have multiple\nequivalent alternatives in terms of gender. The method can be applied both for\ncreating gender balanced outputs as well as for creating gender balanced\ntraining data. The proposed approach is based on a neural machine translation\n(NMT) system trained to 'translate' from one gender alternative to another.\nBoth the automatic and manual analysis of the approach show promising results\nfor automatic generation of gender alternatives for conversational sentences in\nSpanish.",
    "published_date": "2021-07-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.05987v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.05978v1",
    "title": "DIVINE: Diverse Influential Training Points for Data Visualization and Model Refinement",
    "authors": [
      "Umang Bhatt",
      "Isabel Chien",
      "Muhammad Bilal Zafar",
      "Adrian Weller"
    ],
    "author_ids": [],
    "abstract": "As the complexity of machine learning (ML) models increases, resulting in a\nlack of prediction explainability, several methods have been developed to\nexplain a model's behavior in terms of the training data points that most\ninfluence the model. However, these methods tend to mark outliers as highly\ninfluential points, limiting the insights that practitioners can draw from\npoints that are not representative of the training data. In this work, we take\na step towards finding influential training points that also represent the\ntraining data well. We first review methods for assigning importance scores to\ntraining points. Given importance scores, we propose a method to select a set\nof DIVerse INfluEntial (DIVINE) training points as a useful explanation of\nmodel behavior. As practitioners might not only be interested in finding data\npoints influential with respect to model accuracy, but also with respect to\nother important metrics, we show how to evaluate training data points on the\nbasis of group fairness. Our method can identify unfairness-inducing training\npoints, which can be removed to improve fairness outcomes. Our quantitative\nexperiments and user studies show that visualizing DIVINE points helps\npractitioners understand and explain model behavior better than earlier\napproaches.",
    "published_date": "2021-07-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.05978v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.05913v2",
    "title": "Can Less be More? When Increasing-to-Balancing Label Noise Rates Considered Beneficial",
    "authors": [
      "Yang Liu",
      "Jialu Wang"
    ],
    "author_ids": [],
    "abstract": "In this paper, we answer the question of when inserting label noise (less\ninformative labels) can instead return us more accurate and fair models. We are\nprimarily inspired by three observations: 1) In contrast to reducing label\nnoise rates, increasing the noise rates is easy to implement; 2) Increasing a\ncertain class of instances' label noise to balance the noise rates\n(increasing-to-balancing) results in an easier learning problem; 3)\nIncreasing-to-balancing improves fairness guarantees against label bias. In\nthis paper, we first quantify the trade-offs introduced by increasing a certain\ngroup of instances' label noise rate w.r.t. the loss of label informativeness\nand the lowered learning difficulties. We analytically demonstrate when such an\nincrease is beneficial, in terms of either improved generalization power or the\nfairness guarantees. Then we present a method to insert label noise properly\nfor the task of learning with noisy labels, either without or with a fairness\nconstraint. The primary technical challenge we face is due to the fact that we\nwould not know which data instances are suffering from higher noise, and we\nwould not have the ground truth labels to verify any possible hypothesis. We\npropose a detection method that informs us which group of labels might suffer\nfrom higher noise without using ground truth labels. We formally establish the\neffectiveness of the proposed solution and demonstrate it with extensive\nexperiments.",
    "published_date": "2021-07-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.05913v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.05762v3",
    "title": "Strategic Instrumental Variable Regression: Recovering Causal Relationships From Strategic Responses",
    "authors": [
      "Keegan Harris",
      "Daniel Ngo",
      "Logan Stapleton",
      "Hoda Heidari",
      "Zhiwei Steven Wu"
    ],
    "author_ids": [],
    "abstract": "In settings where Machine Learning (ML) algorithms automate or inform\nconsequential decisions about people, individual decision subjects are often\nincentivized to strategically modify their observable attributes to receive\nmore favorable predictions. As a result, the distribution the assessment rule\nis trained on may differ from the one it operates on in deployment. While such\ndistribution shifts, in general, can hinder accurate predictions, our work\nidentifies a unique opportunity associated with shifts due to strategic\nresponses: We show that we can use strategic responses effectively to recover\ncausal relationships between the observable features and outcomes we wish to\npredict, even under the presence of unobserved confounding variables.\nSpecifically, our work establishes a novel connection between strategic\nresponses to ML models and instrumental variable (IV) regression by observing\nthat the sequence of deployed models can be viewed as an instrument that\naffects agents' observable features but does not directly influence their\noutcomes. We show that our causal recovery method can be utilized to improve\ndecision-making across several important criteria: individual fairness, agent\noutcomes, and predictive risk. In particular, we show that if decision subjects\ndiffer in their ability to modify non-causal attributes, any decision rule\ndeviating from the causal coefficients can lead to (potentially unbounded)\nindividual-level unfairness.",
    "published_date": "2021-07-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.05762v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.05704v1",
    "title": "How Could Equality and Data Protection Law Shape AI Fairness for People with Disabilities?",
    "authors": [
      "Reuben Binns",
      "Reuben Kirkham"
    ],
    "author_ids": [],
    "abstract": "This article examines the concept of 'AI fairness' for people with\ndisabilities from the perspective of data protection and equality law. This\nexamination demonstrates that there is a need for a distinctive approach to AI\nfairness that is fundamentally different to that used for other protected\ncharacteristics, due to the different ways in which discrimination and data\nprotection law applies in respect of Disability. We articulate this new agenda\nfor AI fairness for people with disabilities, explaining how combining data\nprotection and equality law creates new opportunities for disabled people's\norganisations and assistive technology researchers alike to shape the use of\nAI, as well as to challenge potential harmful uses.",
    "published_date": "2021-07-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.05704v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.06641v3",
    "title": "Trustworthy AI: A Computational Perspective",
    "authors": [
      "Haochen Liu",
      "Yiqi Wang",
      "Wenqi Fan",
      "Xiaorui Liu",
      "Yaxin Li",
      "Shaili Jain",
      "Yunhao Liu",
      "Anil K. Jain",
      "Jiliang Tang"
    ],
    "author_ids": [],
    "abstract": "In the past few decades, artificial intelligence (AI) technology has\nexperienced swift developments, changing everyone's daily life and profoundly\naltering the course of human society. The intention of developing AI is to\nbenefit humans, by reducing human labor, bringing everyday convenience to human\nlives, and promoting social good. However, recent research and AI applications\nshow that AI can cause unintentional harm to humans, such as making unreliable\ndecisions in safety-critical scenarios or undermining fairness by inadvertently\ndiscriminating against one group. Thus, trustworthy AI has attracted immense\nattention recently, which requires careful consideration to avoid the adverse\neffects that AI may bring to humans, so that humans can fully trust and live in\nharmony with AI technologies.\n  Recent years have witnessed a tremendous amount of research on trustworthy\nAI. In this survey, we present a comprehensive survey of trustworthy AI from a\ncomputational perspective, to help readers understand the latest technologies\nfor achieving trustworthy AI. Trustworthy AI is a large and complex area,\ninvolving various dimensions. In this work, we focus on six of the most crucial\ndimensions in achieving trustworthy AI: (i) Safety & Robustness, (ii)\nNon-discrimination & Fairness, (iii) Explainability, (iv) Privacy, (v)\nAccountability & Auditability, and (vi) Environmental Well-Being. For each\ndimension, we review the recent related technologies according to a taxonomy\nand summarize their applications in real-world systems. We also discuss the\naccordant and conflicting interactions among different dimensions and discuss\npotential aspects for trustworthy AI to investigate in the future.",
    "published_date": "2021-07-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.06641v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.05419v4",
    "title": "A New Approach for Active Automata Learning Based on Apartness",
    "authors": [
      "Frits Vaandrager",
      "Bharat Garhewal",
      "Jurriaan Rot",
      "Thorsten Wißmann"
    ],
    "author_ids": [],
    "abstract": "We present $L^{\\#}$, a new and simple approach to active automata learning.\nInstead of focusing on equivalence of observations, like the $L^{\\ast}$\nalgorithm and its descendants, $L^{\\#}$ takes a different perspective: it tries\nto establish apartness, a constructive form of inequality. $L^{\\#}$ does not\nrequire auxiliary notions such as observation tables or discrimination trees,\nbut operates directly on tree-shaped automata. $L^{\\#}$ has the same asymptotic\nquery and symbol complexities as the best existing learning algorithms, but we\nshow that adaptive distinguishing sequences can be naturally integrated to\nboost the performance of $L^{\\#}$ in practice. Experiments with a prototype\nimplementation, written in Rust, suggest that $L^{\\#}$ is competitive with\nexisting algorithms.",
    "published_date": "2021-07-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.FL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.05419v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2107.05348v4",
    "title": "Zero-shot Visual Question Answering using Knowledge Graph",
    "authors": [
      "Zhuo Chen",
      "Jiaoyan Chen",
      "Yuxia Geng",
      "Jeff Z. Pan",
      "Zonggang Yuan",
      "Huajun Chen"
    ],
    "author_ids": [],
    "abstract": "Incorporating external knowledge to Visual Question Answering (VQA) has\nbecome a vital practical need. Existing methods mostly adopt pipeline\napproaches with different components for knowledge matching and extraction,\nfeature learning, etc.However, such pipeline approaches suffer when some\ncomponent does not perform well, which leads to error propagation and poor\noverall performance. Furthermore, the majority of existing approaches ignore\nthe answer bias issue -- many answers may have never appeared during training\n(i.e., unseen answers) in real-word application. To bridge these gaps, in this\npaper, we propose a Zero-shot VQA algorithm using knowledge graphs and a\nmask-based learning mechanism for better incorporating external knowledge, and\npresent new answer-based Zero-shot VQA splits for the F-VQA dataset.\nExperiments show that our method can achieve state-of-the-art performance in\nZero-shot VQA with unseen answers, meanwhile dramatically augment existing\nend-to-end models on the normal F-VQA task.",
    "published_date": "2021-07-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.05348v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.05346v1",
    "title": "SimDem A Multi-agent Simulation Environment to Model Persons with Dementia and their Assistance",
    "authors": [
      "Muhammad Salman Shaukat",
      "Bjarne Christian Hiller",
      "Sebastian Bader",
      "Thomas Kirste"
    ],
    "author_ids": [],
    "abstract": "Developing artificial intelligence based assistive systems to aid Persons\nwith Dementia (PwD) requires large amounts of training data. However, data\ncollection poses ethical, legal, economic, and logistic issues. Synthetic data\ngeneration tools, in this regard, provide a potential solution. However, we\nbelieve that already available such tools do not adequately reflect cognitive\ndeficiencies in behavior simulation. To counter these issues we propose a\nsimulation model (SimDem ) that primarily focuses on cognitive impairments\nsuffered by PwD and can be easily configured and adapted by the users to model\nand evaluate assistive solutions.",
    "published_date": "2021-07-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.05346v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.05302v1",
    "title": "On Reward Sharing in Blockchain Mining Pools",
    "authors": [
      "Burak Can",
      "Jens Leth Hougaard",
      "Mohsen Pourpouneh"
    ],
    "author_ids": [],
    "abstract": "This paper proposes a conceptual framework for the analysis of reward sharing\nschemes in mining pools, such as those associated with Bitcoin. The framework\nis centered around the reported shares in a pool instead of agents and results\nin two new fairness criteria, absolute and relative redistribution. These\ncriteria impose that the addition of a share to the pool affects all previous\nshares in the same way, either in absolute amount or in relative ratio. We\ncharacterize two large classes of economically viable reward sharing schemes\ncorresponding to each of these fairness criteria in turn. We further show that\nthe intersection of these classes brings about a generalization of the\nwell-known proportional scheme, which also leads to a new characterization of\nthe proportional scheme as a corollary.",
    "published_date": "2021-07-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT",
      "econ.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.05302v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2107.05295v1",
    "title": "DaCy: A Unified Framework for Danish NLP",
    "authors": [
      "Kenneth Enevoldsen",
      "Lasse Hansen",
      "Kristoffer Nielbo"
    ],
    "author_ids": [],
    "abstract": "Danish natural language processing (NLP) has in recent years obtained\nconsiderable improvements with the addition of multiple new datasets and\nmodels. However, at present, there is no coherent framework for applying\nstate-of-the-art models for Danish. We present DaCy: a unified framework for\nDanish NLP built on SpaCy. DaCy uses efficient multitask models which obtain\nstate-of-the-art performance on named entity recognition, part-of-speech\ntagging, and dependency parsing. DaCy contains tools for easy integration of\nexisting models such as for polarity, emotion, or subjectivity detection. In\naddition, we conduct a series of tests for biases and robustness of Danish NLP\npipelines through augmentation of the test set of DaNE. DaCy large compares\nfavorably and is especially robust to long input lengths and spelling\nvariations and errors. All models except DaCy large display significant biases\nrelated to ethnicity while only Polyglot shows a significant gender bias. We\nargue that for languages with limited benchmark sets, data augmentation can be\nparticularly useful for obtaining more realistic and fine-grained performance\nestimates. We provide a series of augmenters as a first step towards a more\nthorough evaluation of language models for low and medium resource languages\nand encourage further development.",
    "published_date": "2021-07-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.LG",
      "I.2.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.05295v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.05175v6",
    "title": "Strategyproof Mechanisms For Group-Fair Facility Location Problems",
    "authors": [
      "Houyu Zhou",
      "Minming Li",
      "Hau Chan"
    ],
    "author_ids": [],
    "abstract": "We study the facility location problems where agents are located on a real\nline and divided into groups based on criteria such as ethnicity or age. Our\naim is to design mechanisms to locate a facility to approximately minimize the\ncosts of groups of agents to the facility fairly while eliciting the agents'\nlocations truthfully. We first explore various well-motivated group fairness\ncost objectives for the problems and show that many natural objectives have an\nunbounded approximation ratio. We then consider minimizing the maximum total\ngroup cost and minimizing the average group cost objectives. For these\nobjectives, we show that existing classical mechanisms (e.g., median) and new\ngroup-based mechanisms provide bounded approximation ratios, where the\ngroup-based mechanisms can achieve better ratios. We also provide lower bounds\nfor both objectives. To measure fairness between groups and within each group,\nwe study a new notion of intergroup and intragroup fairness (IIF) . We consider\ntwo IIF objectives and provide mechanisms with tight approximation ratios.",
    "published_date": "2021-07-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.05175v6",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2107.14052v1",
    "title": "The Role of Social Movements, Coalitions, and Workers in Resisting Harmful Artificial Intelligence and Contributing to the Development of Responsible AI",
    "authors": [
      "Susan von Struensee"
    ],
    "author_ids": [],
    "abstract": "There is mounting public concern over the influence that AI based systems has\nin our society. Coalitions in all sectors are acting worldwide to resist hamful\napplications of AI. From indigenous people addressing the lack of reliable\ndata, to smart city stakeholders, to students protesting the academic\nrelationships with sex trafficker and MIT donor Jeffery Epstein, the\nquestionable ethics and values of those heavily investing in and profiting from\nAI are under global scrutiny. There are biased, wrongful, and disturbing\nassumptions embedded in AI algorithms that could get locked in without\nintervention. Our best human judgment is needed to contain AI's harmful impact.\nPerhaps one of the greatest contributions of AI will be to make us ultimately\nunderstand how important human wisdom truly is in life on earth.",
    "published_date": "2021-07-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "econ.GN",
      "q-fin.EC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.14052v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.05039v1",
    "title": "Learning from Crowds with Sparse and Imbalanced Annotations",
    "authors": [
      "Ye Shi",
      "Shao-Yuan Li",
      "Sheng-Jun Huang"
    ],
    "author_ids": [],
    "abstract": "Traditional supervised learning requires ground truth labels for the training\ndata, whose collection can be difficult in many cases. Recently, crowdsourcing\nhas established itself as an efficient labeling solution through resorting to\nnon-expert crowds. To reduce the labeling error effects, one common practice is\nto distribute each instance to multiple workers, whereas each worker only\nannotates a subset of data, resulting in the {\\it sparse annotation}\nphenomenon. In this paper, we note that when meeting with class-imbalance,\ni.e., when the ground truth labels are {\\it class-imbalanced}, the sparse\nannotations are prone to be skewly distributed, which thus can severely bias\nthe learning algorithm. To combat this issue, we propose one self-training\nbased approach named {\\it Self-Crowd} by progressively adding confident\npseudo-annotations and rebalancing the annotation distribution. Specifically,\nwe propose one distribution aware confidence measure to select confident\npseudo-annotations, which adopts the resampling strategy to oversample the\nminority annotations and undersample the majority annotations. On one\nreal-world crowdsourcing image classification task, we show that the proposed\nmethod yields more balanced annotations throughout training than the\ndistribution agnostic methods and substantially improves the learning\nperformance at different annotation sparsity levels.",
    "published_date": "2021-07-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.05039v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.04952v2",
    "title": "Learn from Anywhere: Rethinking Generalized Zero-Shot Learning with Limited Supervision",
    "authors": [
      "Gaurav Bhatt",
      "Shivam Chandhok",
      "Vineeth N Balasubramanian"
    ],
    "author_ids": [],
    "abstract": "A common problem with most zero and few-shot learning approaches is they\nsuffer from bias towards seen classes resulting in sub-optimal performance.\nExisting efforts aim to utilize unlabeled images from unseen classes (i.e\ntransductive zero-shot) during training to enable generalization. However, this\nlimits their use in practical scenarios where data from target unseen classes\nis unavailable or infeasible to collect. In this work, we present a practical\nsetting of inductive zero and few-shot learning, where unlabeled images from\nother out-of-data classes, that do not belong to seen or unseen categories, can\nbe used to improve generalization in any-shot learning. We leverage a\nformulation based on product-of-experts and introduce a new AUD module that\nenables us to use unlabeled samples from out-of-data classes which are usually\neasily available and practically entail no annotation cost. In addition, we\nalso demonstrate the applicability of our model to address a more practical and\nchallenging, Generalized Zero-shot under a limited supervision setting, where\neven base seen classes do not have sufficient annotated samples.",
    "published_date": "2021-07-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.04952v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.04846v1",
    "title": "Propagation-aware Social Recommendation by Transfer Learning",
    "authors": [
      "Haodong Chang",
      "Yabo Chu"
    ],
    "author_ids": [],
    "abstract": "Social-aware recommendation approaches have been recognized as an effective\nway to solve the data sparsity issue of traditional recommender systems. The\nassumption behind is that the knowledge in social user-user connections can be\nshared and transferred to the domain of user-item interactions, whereby to help\nlearn user preferences. However, most existing approaches merely adopt the\nfirst-order connections among users during transfer learning, ignoring those\nconnections in higher orders. We argue that better recommendation performance\ncan also benefit from high-order social relations. In this paper, we propose a\nnovel Propagation-aware Transfer Learning Network (PTLN) based on the\npropagation of social relations. We aim to better mine the sharing knowledge\nhidden in social networks and thus further improve recommendation performance.\nSpecifically, we explore social influence in two aspects: (a) higher-order\nfriends have been taken into consideration by order bias; (b) different friends\nin the same order will have distinct importance for recommendation by an\nattention mechanism. Besides, we design a novel regularization to bridge the\ngap between social relations and user-item interactions. We conduct extensive\nexperiments on two real-world datasets and beat other counterparts in terms of\nranking accuracy, especially for the cold-start users with few historical\ninteractions.",
    "published_date": "2021-07-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.04846v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.04763v2",
    "title": "Hitting Weighted Even Cycles in Planar Graphs",
    "authors": [
      "Alexander Göke",
      "Jochen Koenemann",
      "Matthias Mnich",
      "Hao Sun"
    ],
    "author_ids": [],
    "abstract": "A classical branch of graph algorithms is graph transversals, where one seeks\na minimum-weight subset of nodes in a node-weighted graph $G$ which intersects\nall copies of subgraphs~$F$ from a fixed family $\\mathcal F$. Many such graph\ntransversal problems have been shown to admit polynomial-time approximation\nschemes (PTAS) for planar input graphs $G$, using a variety of techniques like\nthe shifting technique (Baker, J. ACM 1994), bidimensionality (Fomin et al.,\nSODA 2011), or connectivity domination (Cohen-Addad et al., STOC 2016). These\ntechniques do not seem to apply to graph transversals with parity constraints,\nwhich have recently received significant attention, but for which no PTASs are\nknown. In the even-cycle transversal (\\ECT) problem, the goal is to find a\nminimum-weight hitting set for the set of even cycles in an undirected graph.\n  For ECT, Fiorini et al. (IPCO 2010) showed that the integrality gap of the\nstandard covering LP relaxation is $\\Theta(\\log n)$, and that adding sparsity\ninequalities reduces the integrality gap to~10. Our main result is a\nprimal-dual algorithm that yields a $47/7\\approx6.71$-approximation for ECT on\nnode-weighted planar graphs, and an integrality gap of the same value for the\nstandard LP relaxation on node-weighted planar graphs.",
    "published_date": "2021-07-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DS",
      "cs.DM",
      "68W25",
      "F.2.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.04763v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2107.04711v1",
    "title": "Fragments of the Past: Curating Peer Support with Perpetrators of Domestic Violence",
    "authors": [
      "Rosanna Bellini",
      "Alexander Wilson",
      "Jan David Smeddinck"
    ],
    "author_ids": [],
    "abstract": "There is growing evidence that digital peer-support networks can have a\npositive influence on behaviour change and wellbeing outcomes for people who\nharm themselves and others. However, making and sustaining such networks are\nsubject to ethical and pragmatic challenges, particularly for perpetrators of\ndomestic violence whom pose unique risks when brought together. In this work we\nreport on a ten-month study where we worked with six support workers and\neighteen perpetrators in the design and deployment of Fragments of the Past; a\nsocio-material system that connects audio messages with tangible artefacts. We\nshare how crafting digitally-augmented artefacts - 'fragments' - of experiences\nof desisting from violence can translate messages for motivation and rapport\nbetween peers, without subjecting the process to risks inherent with direct\ninter-personal communication. These insights provide the basis for practical\nconsiderations for future network design with challenging populations.",
    "published_date": "2021-07-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.04711v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2107.04642v10",
    "title": "Escaping the Impossibility of Fairness: From Formal to Substantive Algorithmic Fairness",
    "authors": [
      "Ben Green"
    ],
    "author_ids": [],
    "abstract": "Efforts to promote equitable public policy with algorithms appear to be\nfundamentally constrained by the \"impossibility of fairness\" (an\nincompatibility between mathematical definitions of fairness). This technical\nlimitation raises a central question about algorithmic fairness: How can\ncomputer scientists and policymakers support equitable policy reforms with\nalgorithms? In this article, I argue that promoting justice with algorithms\nrequires reforming the methodology of algorithmic fairness. First, I diagnose\nthe problems of the current methodology for algorithmic fairness, which I call\n\"formal algorithmic fairness.\" Because formal algorithmic fairness restricts\nanalysis to isolated decision-making procedures, it leads to the impossibility\nof fairness and to models that exacerbate oppression despite appearing \"fair.\"\nSecond, I draw on theories of substantive equality from law and philosophy to\npropose an alternative methodology, which I call \"substantive algorithmic\nfairness.\" Because substantive algorithmic fairness takes a more expansive\nscope of analysis, it enables an escape from the impossibility of fairness and\nprovides a rigorous guide for alleviating injustice with algorithms. In sum,\nsubstantive algorithmic fairness presents a new direction for algorithmic\nfairness: away from formal mathematical models of \"fair\" decision-making and\ntoward substantive evaluations of whether and how algorithms can promote\njustice in practice.",
    "published_date": "2021-07-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.04642v10",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.04641v1",
    "title": "Training Over-parameterized Models with Non-decomposable Objectives",
    "authors": [
      "Harikrishna Narasimhan",
      "Aditya Krishna Menon"
    ],
    "author_ids": [],
    "abstract": "Many modern machine learning applications come with complex and nuanced\ndesign goals such as minimizing the worst-case error, satisfying a given\nprecision or recall target, or enforcing group-fairness constraints. Popular\ntechniques for optimizing such non-decomposable objectives reduce the problem\ninto a sequence of cost-sensitive learning tasks, each of which is then solved\nby re-weighting the training loss with example-specific costs. We point out\nthat the standard approach of re-weighting the loss to incorporate label costs\ncan produce unsatisfactory results when used to train over-parameterized\nmodels. As a remedy, we propose new cost-sensitive losses that extend the\nclassical idea of logit adjustment to handle more general cost matrices. Our\nlosses are calibrated, and can be further improved with distilled labels from a\nteacher model. Through experiments on benchmark image datasets, we showcase the\neffectiveness of our approach in training ResNet models with common robust and\nconstrained optimization objectives.",
    "published_date": "2021-07-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.04641v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.04461v1",
    "title": "On the Challenges of Open World Recognitionunder Shifting Visual Domains",
    "authors": [
      "Dario Fontanel",
      "Fabio Cermelli",
      "Massimiliano Mancini",
      "Barbara Caputo"
    ],
    "author_ids": [],
    "abstract": "Robotic visual systems operating in the wild must act in unconstrained\nscenarios, under different environmental conditions while facing a variety of\nsemantic concepts, including unknown ones. To this end, recent works tried to\nempower visual object recognition methods with the capability to i) detect\nunseen concepts and ii) extended their knowledge over time, as images of new\nsemantic classes arrive. This setting, called Open World Recognition (OWR), has\nthe goal to produce systems capable of breaking the semantic limits present in\nthe initial training set. However, this training set imposes to the system not\nonly its own semantic limits, but also environmental ones, due to its bias\ntoward certain acquisition conditions that do not necessarily reflect the high\nvariability of the real-world. This discrepancy between training and test\ndistribution is called domain-shift. This work investigates whether OWR\nalgorithms are effective under domain-shift, presenting the first benchmark\nsetup for assessing fairly the performances of OWR algorithms, with and without\ndomain-shift. We then use this benchmark to conduct analyses in various\nscenarios, showing how existing OWR algorithms indeed suffer a severe\nperformance degradation when train and test distributions differ. Our analysis\nshows that this degradation is only slightly mitigated by coupling OWR with\ndomain generalization techniques, indicating that the mere plug-and-play of\nexisting algorithms is not enough to recognize new and unknown categories in\nunseen domains. Our results clearly point toward open issues and future\nresearch directions, that need to be investigated for building robot visual\nsystems able to function reliably under these challenging yet very real\nconditions. Code available at\nhttps://github.com/DarioFontanel/OWR-VisualDomains",
    "published_date": "2021-07-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.04461v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.04423v2",
    "title": "Multiaccurate Proxies for Downstream Fairness",
    "authors": [
      "Emily Diana",
      "Wesley Gill",
      "Michael Kearns",
      "Krishnaram Kenthapadi",
      "Aaron Roth",
      "Saeed Sharifi-Malvajerdi"
    ],
    "author_ids": [],
    "abstract": "We study the problem of training a model that must obey demographic fairness\nconditions when the sensitive features are not available at training time -- in\nother words, how can we train a model to be fair by race when we don't have\ndata about race? We adopt a fairness pipeline perspective, in which an\n\"upstream\" learner that does have access to the sensitive features will learn a\nproxy model for these features from the other attributes. The goal of the proxy\nis to allow a general \"downstream\" learner -- with minimal assumptions on their\nprediction task -- to be able to use the proxy to train a model that is fair\nwith respect to the true sensitive features. We show that obeying multiaccuracy\nconstraints with respect to the downstream model class suffices for this\npurpose, provide sample- and oracle efficient-algorithms and generalization\nbounds for learning such proxies, and conduct an experimental evaluation. In\ngeneral, multiaccuracy is much easier to satisfy than classification accuracy,\nand can be satisfied even when the sensitive features are hard to predict.",
    "published_date": "2021-07-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.04423v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.04267v2",
    "title": "Can We Replicate Real Human Behaviour Using Artificial Neural Networks?",
    "authors": [
      "Georg Jäger",
      "Daniel Reisinger"
    ],
    "author_ids": [],
    "abstract": "Agent-based modelling is a powerful tool when simulating human systems, yet\nwhen human behaviour cannot be described by simple rules or maximising one's\nown profit, we quickly reach the limits of this methodology. Machine learning\nhas the potential to bridge this gap by providing a link between what people\nobserve and how they act in order to reach their goal. In this paper we use a\nframework for agent-based modelling that utilizes human values like fairness,\nconformity and altruism. Using this framework we simulate a public goods game\nand compare to experimental results. We can report good agreement between\nsimulation and experiment and furthermore find that the presented framework\noutperforms strict reinforcement learning. Both the framework and the utility\nfunction are generic enough that they can be used for arbitrary systems, which\nmakes this method a promising candidate for a foundation of a universal\nagent-based model.",
    "published_date": "2021-07-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.04267v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.04117v4",
    "title": "Crowd Sensing and Living Lab Outdoor Experimentation Made Easy",
    "authors": [
      "Evangelos Pournaras",
      "Atif Nabi Ghulam",
      "Renato Kunz",
      "Regula Hänggli"
    ],
    "author_ids": [],
    "abstract": "Living lab outdoor experimentation using pervasive computing provides new\nopportunities: higher realism, external validity and socio-spatio-temporal\nobservations in large scale. However, experimentation `in the wild' is complex\nand costly. Noise, biases, privacy concerns, compliance with standards of\nethical review boards, remote moderation, control of experimental conditions\nand equipment perplex the collection of high-quality data for causal inference.\nThis article introduces Smart Agora, a novel open-source software platform for\nrigorous systematic outdoor experimentation. Without writing a single line of\ncode, highly complex experimental scenarios are visually designed and\nautomatically deployed to smart phones. Novel geolocated survey and sensor data\nare collected subject of participants verifying desired experimental\nconditions, for instance, their localization at certain urban spots. This new\napproach drastically improves the quality and purposefulness of crowd sensing,\ntailored to conditions that confirm/reject hypotheses. The features that\nsupport this innovative functionality and the broad spectrum of its\napplicability are demonstrated.",
    "published_date": "2021-07-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC",
      "cs.CY",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.04117v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2107.03907v1",
    "title": "Remote Working Pre- and Post-COVID-19: An Analysis of New Threats and Risks to Security and Privacy",
    "authors": [
      "Jason R. C. Nurse",
      "Nikki Williams",
      "Emily Collins",
      "Niki Panteli",
      "John Blythe",
      "Ben Koppelman"
    ],
    "author_ids": [],
    "abstract": "COVID-19 has radically changed society as we know it. To reduce the spread of\nthe virus, millions across the globe have been forced to work remotely, often\nin make-shift home offices, and using a plethora of new, unfamiliar digital\ntechnologies. In this article, we critically analyse cyber security and privacy\nconcerns arising due to remote working during the coronavirus pandemic. Through\nour work, we discover a series of security risks emerging because of the\nrealities of this period. For instance, lack of remote-working security\ntraining, heightened stress and anxiety, rushed technology deployment, and the\npresence of untrusted individuals in a remote-working environment (e.g., in\nflatshares), can result in new cyber-risk. Simultaneously, we find that as\norganisations look to manage these and other risks posed by their remote\nworkforces, employee's privacy (including personal information and activities)\nis often compromised. This is apparent in the significant adoption of remote\nworkplace monitoring, management and surveillance technologies. Such\ntechnologies raise several privacy and ethical questions, and further highlight\nthe tension between security and privacy going forward.",
    "published_date": "2021-07-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CR",
      "cs.CY",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.03907v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2107.03742v1",
    "title": "Grid Partitioned Attention: Efficient TransformerApproximation with Inductive Bias for High Resolution Detail Generation",
    "authors": [
      "Nikolay Jetchev",
      "Gökhan Yildirim",
      "Christian Bracher",
      "Roland Vollgraf"
    ],
    "author_ids": [],
    "abstract": "Attention is a general reasoning mechanism than can flexibly deal with image\ninformation, but its memory requirements had made it so far impractical for\nhigh resolution image generation. We present Grid Partitioned Attention (GPA),\na new approximate attention algorithm that leverages a sparse inductive bias\nfor higher computational and memory efficiency in image domains: queries attend\nonly to few keys, spatially close queries attend to close keys due to\ncorrelations. Our paper introduces the new attention layer, analyzes its\ncomplexity and how the trade-off between memory usage and model power can be\ntuned by the hyper-parameters.We will show how such attention enables novel\ndeep learning architectures with copying modules that are especially useful for\nconditional image generation tasks like pose morphing. Our contributions are\n(i) algorithm and code1of the novel GPA layer, (ii) a novel deep\nattention-copying architecture, and (iii) new state-of-the art experimental\nresults in human pose morphing generation benchmarks.",
    "published_date": "2021-07-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.03742v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.14099v1",
    "title": "The ghost of AI governance past, present and future: AI governance in the European Union",
    "authors": [
      "Charlotte Stix"
    ],
    "author_ids": [],
    "abstract": "The received wisdom is that artificial intelligence (AI) is a competition\nbetween the US and China. In this chapter, the author will examine how the\nEuropean Union (EU) fits into that mix and what it can offer as a third way to\ngovern AI. The chapter presents this by exploring the past, present and future\nof AI governance in the EU. Section 1 serves to explore and evidence the EUs\ncoherent and comprehensive approach to AI governance. In short, the EU ensures\nand encourages ethical, trustworthy and reliable technological development.\nThis will cover a range of key documents and policy tools that lead to the most\ncrucial effort of the EU to date: to regulate AI. Section 2 maps the EUs drive\ntowards digital sovereignty through the lens of regulation and infrastructure.\nThis covers topics such as the trustworthiness of AI systems, cloud, compute\nand foreign direct investment. In Section 3, the chapter concludes by offering\nseveral considerations to achieve good AI governance in the EU.",
    "published_date": "2021-07-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.14099v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.03662v1",
    "title": "On Submodular Prophet Inequalities and Correlation Gap",
    "authors": [
      "Chandra Chekuri",
      "Vasilis Livanos"
    ],
    "author_ids": [],
    "abstract": "Prophet inequalities and secretary problems have been extensively studied in\nrecent years due to their elegance, connections to online algorithms,\nstochastic optimization, and mechanism design problems in game theoretic\nsettings. Rubinstein and Singla developed a notion of combinatorial prophet\ninequalities in order to generalize the standard prophet inequality setting to\ncombinatorial valuation functions such as submodular and subadditive functions.\nFor non-negative submodular functions they demonstrated a constant factor\nprophet inequality for matroid constraints. Along the way they showed a variant\nof the correlation gap for non-negative submodular functions.\n  In this paper we revisit their notion of correlation gap as well as the\nstandard notion of correlation gap and prove much tighter and cleaner bounds.\nVia these bounds and other insights we obtain substantially improved constant\nfactor combinatorial prophet inequalities for both monotone and non-monotone\nsubmodular functions over any constraint that admits an Online Contention\nResolution Scheme. In addition to improved bounds we describe efficient\npolynomial-time algorithms that achieve these bounds.",
    "published_date": "2021-07-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.03662v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2107.08792v2",
    "title": "Enhancing Stability in Training Conditional Generative Adversarial Networks via Selective Data Matching",
    "authors": [
      "Kyeongbo Kong",
      "Kyunghun Kim",
      "Suk-Ju Kang"
    ],
    "author_ids": [],
    "abstract": "Conditional generative adversarial networks (cGANs) have demonstrated\nremarkable success due to their class-wise controllability and superior quality\nfor complex generation tasks. Typical cGANs solve the joint distribution\nmatching problem by decomposing two easier sub-problems: marginal matching and\nconditional matching. In this paper, we proposes a simple but effective\ntraining methodology, selective focusing learning, which enforces the\ndiscriminator and generator to learn easy samples of each class rapidly while\nmaintaining diversity. Our key idea is to selectively apply conditional and\njoint matching for the data in each mini-batch.Specifically, we first select\nthe samples with the highest scores when sorted using the conditional term of\nthe discriminator outputs (real and generated samples). Then we optimize the\nmodel using the selected samples with only conditional matching and the other\nsamples with joint matching. From our toy experiments, we found that it is the\nbest to apply only conditional matching to certain samples due to the\ncontent-aware optimization of the discriminator. We conducted experiments on\nImageNet (64x64 and 128x128), CIFAR-10, CIFAR-100 datasets, and Mixture of\nGaussian, noisy label settings to demonstrate that the proposed method can\nsubstantially (up to 35.18% in terms of FID) improve all indicators with 10\nindependent trials. Code is available at\nhttps://github.com/pnu-cvsp/Enhancing-Stability-in-Training-Conditional-GAN-via-Selective-Data-Matching.",
    "published_date": "2021-07-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.08792v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.03483v1",
    "title": "Impossibility results for fair representations",
    "authors": [
      "Tosca Lechner",
      "Shai Ben-David",
      "Sushant Agarwal",
      "Nivasini Ananthakrishnan"
    ],
    "author_ids": [],
    "abstract": "With the growing awareness to fairness in machine learning and the\nrealization of the central role that data representation has in data processing\ntasks, there is an obvious interest in notions of fair data representations.\nThe goal of such representations is that a model trained on data under the\nrepresentation (e.g., a classifier) will be guaranteed to respect some fairness\nconstraints.\n  Such representations are useful when they can be fixed for training models on\nvarious different tasks and also when they serve as data filtering between the\nraw data (known to the representation designer) and potentially malicious\nagents that use the data under the representation to learn predictive models\nand make decisions.\n  A long list of recent research papers strive to provide tools for achieving\nthese goals.\n  However, we prove that this is basically a futile effort. Roughly stated, we\nprove that no representation can guarantee the fairness of classifiers for\ndifferent tasks trained using it; even the basic goal of achieving\nlabel-independent Demographic Parity fairness fails once the marginal data\ndistribution shifts. More refined notions of fairness, like Odds Equality,\ncannot be guaranteed by a representation that does not take into account the\ntask specific labeling rule with respect to which such fairness will be\nevaluated (even if the marginal data distribution is known a priory).\nFurthermore, except for trivial cases, no representation can guarantee Odds\nEquality fairness for any two different tasks, while allowing accurate label\npredictions for both.\n  While some of our conclusions are intuitive, we formulate (and prove) crisp\nstatements of such impossibilities, often contrasting impressions conveyed by\nmany recent works on fair representations.",
    "published_date": "2021-07-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML",
      "I.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.03483v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.03415v1",
    "title": "A Graph-based Approach for Mitigating Multi-sided Exposure Bias in Recommender Systems",
    "authors": [
      "Masoud Mansoury",
      "Himan Abdollahpouri",
      "Mykola Pechenizkiy",
      "Bamshad Mobasher",
      "Robin Burke"
    ],
    "author_ids": [],
    "abstract": "Fairness is a critical system-level objective in recommender systems that has\nbeen the subject of extensive recent research. A specific form of fairness is\nsupplier exposure fairness where the objective is to ensure equitable coverage\nof items across all suppliers in recommendations provided to users. This is\nespecially important in multistakeholder recommendation scenarios where it may\nbe important to optimize utilities not just for the end-user, but also for\nother stakeholders such as item sellers or producers who desire a fair\nrepresentation of their items. This type of supplier fairness is sometimes\naccomplished by attempting to increasing aggregate diversity in order to\nmitigate popularity bias and to improve the coverage of long-tail items in\nrecommendations. In this paper, we introduce FairMatch, a general graph-based\nalgorithm that works as a post processing approach after recommendation\ngeneration to improve exposure fairness for items and suppliers. The algorithm\niteratively adds high quality items that have low visibility or items from\nsuppliers with low exposure to the users' final recommendation lists. A\ncomprehensive set of experiments on two datasets and comparison with\nstate-of-the-art baselines show that FairMatch, while significantly improves\nexposure fairness and aggregate diversity, maintains an acceptable level of\nrelevance of the recommendations.",
    "published_date": "2021-07-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.03415v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.03207v1",
    "title": "Bias-Tolerant Fair Classification",
    "authors": [
      "Yixuan Zhang",
      "Feng Zhou",
      "Zhidong Li",
      "Yang Wang",
      "Fang Chen"
    ],
    "author_ids": [],
    "abstract": "The label bias and selection bias are acknowledged as two reasons in data\nthat will hinder the fairness of machine-learning outcomes. The label bias\noccurs when the labeling decision is disturbed by sensitive features, while the\nselection bias occurs when subjective bias exists during the data sampling.\nEven worse, models trained on such data can inherit or even intensify the\ndiscrimination. Most algorithmic fairness approaches perform an empirical risk\nminimization with predefined fairness constraints, which tends to trade-off\naccuracy for fairness. However, such methods would achieve the desired fairness\nlevel with the sacrifice of the benefits (receive positive outcomes) for\nindividuals affected by the bias. Therefore, we propose a\nBias-TolerantFAirRegularizedLoss (B-FARL), which tries to regain the benefits\nusing data affected by label bias and selection bias. B-FARL takes the biased\ndata as input, calls a model that approximates the one trained with fair but\nlatent data, and thus prevents discrimination without constraints required. In\naddition, we show the effective components by decomposing B-FARL, and we\nutilize the meta-learning framework for the B-FARL optimization. The\nexperimental results on real-world datasets show that our method is empirically\neffective in improving fairness towards the direction of true but latent\nlabels.",
    "published_date": "2021-07-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.03207v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.03057v1",
    "title": "BBRv2+:Towards Balancing Aggressiveness and Fairness with Delay-based Bandwidth Probing",
    "authors": [
      "Furong Yang",
      "Qinghua Wu",
      "Zhenyu Li",
      "Yanmei Liu",
      "Giovanni Pau",
      "Gaogang Xie"
    ],
    "author_ids": [],
    "abstract": "BBRv2, proposed by Google, aims at addressing BBR's shortcomings of\nunfairness against loss-based congestion control algorithms (CCAs) and\nexcessive retransmissions in shallow-buffered networks. In this paper, we first\ncomprehensively study BBRv2's performance under various network conditions and\nshow that BBRv2 mitigates the shortcomings of BBR. Nevertheless, BBRv2's\nbenefits come with several costs, including the slow responsiveness to\nbandwidth dynamics as well as the low resilience to random losses. We then\npropose BBRv2+ to address BBRv2's performance issues without sacrificing its\nadvantages over BBR. To this end, BBRv2+ incorporates delay information into\nits path model, which cautiously guides the aggressiveness of its bandwidth\nprobing to not reduce its fairness against loss-based CCAs. BBRv2+ also\nintegrates mechanisms for improved resilience to random losses as well as\nnetwork jitters. Extensive experiments demonstrate the effectiveness of BBRv2+.\nEspecially, it achieves 25% higher throughput and comparable queuing delay in\ncomparison with BBRv2 in high-mobility network scenarios.",
    "published_date": "2021-07-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.03057v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2107.03000v1",
    "title": "PoseRN: A 2D pose refinement network for bias-free multi-view 3D human pose estimation",
    "authors": [
      "Akihiko Sayo",
      "Diego Thomas",
      "Hiroshi Kawasaki",
      "Yuta Nakashima",
      "Katsushi Ikeuchi"
    ],
    "author_ids": [],
    "abstract": "We propose a new 2D pose refinement network that learns to predict the human\nbias in the estimated 2D pose. There are biases in 2D pose estimations that are\ndue to differences between annotations of 2D joint locations based on\nannotators' perception and those defined by motion capture (MoCap) systems.\nThese biases are crafted into publicly available 2D pose datasets and cannot be\nremoved with existing error reduction approaches. Our proposed pose refinement\nnetwork allows us to efficiently remove the human bias in the estimated 2D\nposes and achieve highly accurate multi-view 3D human pose estimation.",
    "published_date": "2021-07-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.03000v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.02959v1",
    "title": "Towards Achieving Trust Through Transparency and Ethics (Pre-Print)",
    "authors": [
      "David Kwan",
      "Luiz Marcio Cysneiros",
      "Julio Cesar Sampaio do Prado Leite"
    ],
    "author_ids": [],
    "abstract": "The ubiquitous presence of software in the products we use, together with\nArtificial Intelligence in these products, has led to an increasing need for\nconsumer trust. Consumers often lose faith in products, and the lack of Trust\npropagates to the companies behind them. This is even more so in\nmission-critical systems such as autonomous vehicles and clinical support\nsystems. This paper follows grounded theory principles to elicit knowledge\nrelated to Trust, Ethics, and Transparency. We approach these qualities as\nNon-Functional Requirements (NFRs), aiming to build catalogs to subsidize the\nconstruction of Socially Responsible Software. The corpus we have used was\nbuilt on a selected collection of literature on Corporate Social\nResponsibility, with an emphasis on Business Ethics. Our challenge is how to\nencode the social perspective knowledge, mainly through the view of Corporate\nSocial Responsibility, on how organizations or institutions achieve\ntrustworthiness. Since our ground perspective is that of NFRs, results are\npresented by a catalogue of Trust as a Non-Functional Requirement, represented\nas a Softgoal Interdependency Graph (SIG). The SIG language helps software\nengineers in understanding alternatives they have to improve Trust in software\nproducts.",
    "published_date": "2021-07-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.02959v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.03383v2",
    "title": "Assessing putative bias in prediction of anti-microbial resistance from real-world genotyping data under explicit causal assumptions",
    "authors": [
      "Mattia Prosperi",
      "Simone Marini",
      "Christina Boucher",
      "Jiang Bian"
    ],
    "author_ids": [],
    "abstract": "Whole genome sequencing (WGS) is quickly becoming the customary means for\nidentification of antimicrobial resistance (AMR) due to its ability to obtain\nhigh resolution information about the genes and mechanisms that are causing\nresistance and driving pathogen mobility. By contrast, traditional phenotypic\n(antibiogram) testing cannot easily elucidate such information. Yet development\nof AMR prediction tools from genotype-phenotype data can be biased, since\nsampling is non-randomized. Sample provenience, period of collection, and\nspecies representation can confound the association of genetic traits with AMR.\nThus, prediction models can perform poorly on new data with sampling\ndistribution shifts. In this work -- under an explicit set of causal\nassumptions -- we evaluate the effectiveness of propensity-based rebalancing\nand confounding adjustment on AMR prediction using genotype-phenotype AMR data\nfrom the Pathosystems Resource Integration Center (PATRIC). We select bacterial\ngenotypes (encoded as k-mer signatures, i.e. DNA fragments of length k),\ncountry, year, species, and AMR phenotypes for the tetracycline drug class,\npreparing test data with recent genomes coming from a single country. We test\nboosted logistic regression (BLR) and random forests (RF) with/without\nbias-handling. On 10,936 instances, we find evidence of species, location and\nyear imbalance with respect to the AMR phenotype. The crude versus\nbias-adjusted change in effect of genetic signatures on AMR varies but only\nmoderately (selecting the top 20,000 out of 40+ million k-mers). The area under\nthe receiver operating characteristic (AUROC) of the RF (0.95) is comparable to\nthat of BLR (0.94) on both out-of-bag samples from bootstrap and the external\ntest (n=1,085), where AUROCs do not decrease. We observe a 1%-5% gain in AUROC\nwith bias-handling compared to the sole use of genetic signatures. ...",
    "published_date": "2021-07-06T00:00:00",
    "year": 2021,
    "categories": [
      "q-bio.GN",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.03383v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.02718v4",
    "title": "Foreground-Aware Stylization and Consensus Pseudo-Labeling for Domain Adaptation of First-Person Hand Segmentation",
    "authors": [
      "Takehiko Ohkawa",
      "Takuma Yagi",
      "Atsushi Hashimoto",
      "Yoshitaka Ushiku",
      "Yoichi Sato"
    ],
    "author_ids": [],
    "abstract": "Hand segmentation is a crucial task in first-person vision. Since\nfirst-person images exhibit strong bias in appearance among different\nenvironments, adapting a pre-trained segmentation model to a new domain is\nrequired in hand segmentation. Here, we focus on appearance gaps for hand\nregions and backgrounds separately. We propose (i) foreground-aware image\nstylization and (ii) consensus pseudo-labeling for domain adaptation of hand\nsegmentation. We stylize source images independently for the foreground and\nbackground using target images as style. To resolve the domain shift that the\nstylization has not addressed, we apply careful pseudo-labeling by taking a\nconsensus between the models trained on the source and stylized source images.\nWe validated our method on domain adaptation of hand segmentation from real and\nsimulation images. Our method achieved state-of-the-art performance in both\nsettings. We also demonstrated promising results in challenging multi-target\ndomain adaptation and domain generalization settings. Code is available at\nhttps://github.com/ut-vision/FgSty-CPL.",
    "published_date": "2021-07-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.02718v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.02713v1",
    "title": "Predicate correlation learning for scene graph generation",
    "authors": [
      "Leitian Tao",
      "Li Mi",
      "Nannan Li",
      "Xianhang Cheng",
      "Yaosi Hu",
      "Zhenzhong Chen"
    ],
    "author_ids": [],
    "abstract": "For a typical Scene Graph Generation (SGG) method, there is often a large gap\nin the performance of the predicates' head classes and tail classes. This\nphenomenon is mainly caused by the semantic overlap between different\npredicates as well as the long-tailed data distribution. In this paper, a\nPredicate Correlation Learning (PCL) method for SGG is proposed to address the\nabove two problems by taking the correlation between predicates into\nconsideration. To describe the semantic overlap between strong-correlated\npredicate classes, a Predicate Correlation Matrix (PCM) is defined to quantify\nthe relationship between predicate pairs, which is dynamically updated to\nremove the matrix's long-tailed bias. In addition, PCM is integrated into a\nPredicate Correlation Loss function ($L_{PC}$) to reduce discouraging gradients\nof unannotated classes. The proposed method is evaluated on Visual Genome\nbenchmark, where the performance of the tail classes is significantly improved\nwhen built on the existing methods.",
    "published_date": "2021-07-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.02713v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.02597v1",
    "title": "Physics-informed regularization and structure preservation for learning stable reduced models from data with operator inference",
    "authors": [
      "Nihar Sawant",
      "Boris Kramer",
      "Benjamin Peherstorfer"
    ],
    "author_ids": [],
    "abstract": "Operator inference learns low-dimensional dynamical-system models with\npolynomial nonlinear terms from trajectories of high-dimensional physical\nsystems (non-intrusive model reduction). This work focuses on the large class\nof physical systems that can be well described by models with quadratic\nnonlinear terms and proposes a regularizer for operator inference that induces\na stability bias onto quadratic models. The proposed regularizer is physics\ninformed in the sense that it penalizes quadratic terms with large norms and so\nexplicitly leverages the quadratic model form that is given by the underlying\nphysics. This means that the proposed approach judiciously learns from data and\nphysical insights combined, rather than from either data or physics alone.\nAdditionally, a formulation of operator inference is proposed that enforces\nmodel constraints for preserving structure such as symmetry and definiteness in\nthe linear terms. Numerical results demonstrate that models learned with\noperator inference and the proposed regularizer and structure preservation are\naccurate and stable even in cases where using no regularization or Tikhonov\nregularization leads to models that are unstable.",
    "published_date": "2021-07-06T00:00:00",
    "year": 2021,
    "categories": [
      "math.NA",
      "cs.LG",
      "cs.NA",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.02597v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.02482v1",
    "title": "A Knowledge graph representation of baseline characteristics for the Dutch proton therapy research registry",
    "authors": [
      "Matthijs Sloep",
      "Petros Kalendralis",
      "Ananya Choudhury",
      "Lerau Seyben",
      "Jasper Snel",
      "Nibin Moni George",
      "Martijn Veening",
      "Johannes A. Langendijk",
      "Andre Dekker",
      "Johan van Soest",
      "Rianne Fijten"
    ],
    "author_ids": [],
    "abstract": "Cancer registries collect multisource data and provide valuable information\nthat can lead to unique research opportunities. In the Netherlands, a registry\nand model-based approach (MBA) are used for the selection of patients that are\neligible for proton therapy. We collected baseline characteristics including\ndemographic, clinical, tumour and treatment information. These data were\ntransformed into a machine readable format using the FAIR (Findable,\nAccessible, Interoperable, Reusable) data principles and resulted in a\nknowledge graph with baseline characteristics of proton therapy patients. With\nthis approach, we enable the possibility of linking external data sources and\noptimal flexibility to easily adapt the data structure of the existing\nknowledge graph to the needs of the clinic.",
    "published_date": "2021-07-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DB"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.02482v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2107.02237v1",
    "title": "Efficient First-Order Contextual Bandits: Prediction, Allocation, and Triangular Discrimination",
    "authors": [
      "Dylan J. Foster",
      "Akshay Krishnamurthy"
    ],
    "author_ids": [],
    "abstract": "A recurring theme in statistical learning, online learning, and beyond is\nthat faster convergence rates are possible for problems with low noise, often\nquantified by the performance of the best hypothesis; such results are known as\nfirst-order or small-loss guarantees. While first-order guarantees are\nrelatively well understood in statistical and online learning, adapting to low\nnoise in contextual bandits (and more broadly, decision making) presents major\nalgorithmic challenges. In a COLT 2017 open problem, Agarwal, Krishnamurthy,\nLangford, Luo, and Schapire asked whether first-order guarantees are even\npossible for contextual bandits and -- if so -- whether they can be attained by\nefficient algorithms. We give a resolution to this question by providing an\noptimal and efficient reduction from contextual bandits to online regression\nwith the logarithmic (or, cross-entropy) loss. Our algorithm is simple and\npractical, readily accommodates rich function classes, and requires no\ndistributional assumptions beyond realizability. In a large-scale empirical\nevaluation, we find that our approach typically outperforms comparable\nnon-first-order methods.\n  On the technical side, we show that the logarithmic loss and an\ninformation-theoretic quantity called the triangular discrimination play a\nfundamental role in obtaining first-order guarantees, and we combine this\nobservation with new refinements to the regression oracle reduction framework\nof Foster and Rakhlin. The use of triangular discrimination yields novel\nresults even for the classical statistical learning model, and we anticipate\nthat it will find broader use.",
    "published_date": "2021-07-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.02237v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.02189v1",
    "title": "Label noise in segmentation networks : mitigation must deal with bias",
    "authors": [
      "Eugene Vorontsov",
      "Samuel Kadoury"
    ],
    "author_ids": [],
    "abstract": "Imperfect labels limit the quality of predictions learned by deep neural\nnetworks. This is particularly relevant in medical image segmentation, where\nreference annotations are difficult to collect and vary significantly even\nacross expert annotators. Prior work on mitigating label noise focused on\nsimple models of mostly uniform noise. In this work, we explore biased and\nunbiased errors artificially introduced to brain tumour annotations on MRI\ndata. We found that supervised and semi-supervised segmentation methods are\nrobust or fairly robust to unbiased errors but sensitive to biased errors. It\nis therefore important to identify the sorts of errors expected in medical\nimage labels and especially mitigate the biased errors.",
    "published_date": "2021-07-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.02189v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.02170v2",
    "title": "On Model Calibration for Long-Tailed Object Detection and Instance Segmentation",
    "authors": [
      "Tai-Yu Pan",
      "Cheng Zhang",
      "Yandong Li",
      "Hexiang Hu",
      "Dong Xuan",
      "Soravit Changpinyo",
      "Boqing Gong",
      "Wei-Lun Chao"
    ],
    "author_ids": [],
    "abstract": "Vanilla models for object detection and instance segmentation suffer from the\nheavy bias toward detecting frequent objects in the long-tailed setting.\nExisting methods address this issue mostly during training, e.g., by\nre-sampling or re-weighting. In this paper, we investigate a largely overlooked\napproach -- post-processing calibration of confidence scores. We propose\nNorCal, Normalized Calibration for long-tailed object detection and instance\nsegmentation, a simple and straightforward recipe that reweighs the predicted\nscores of each class by its training sample size. We show that separately\nhandling the background class and normalizing the scores over classes for each\nproposal are keys to achieving superior performance. On the LVIS dataset,\nNorCal can effectively improve nearly all the baseline models not only on rare\nclasses but also on common and frequent classes. Finally, we conduct extensive\nanalysis and ablation studies to offer insights into various modeling choices\nand mechanisms of our approach. Our code is publicly available at\nhttps://github.com/tydpan/NorCal/.",
    "published_date": "2021-07-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.02170v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.02112v1",
    "title": "Recovering the Unbiased Scene Graphs from the Biased Ones",
    "authors": [
      "Meng-Jiun Chiou",
      "Henghui Ding",
      "Hanshu Yan",
      "Changhu Wang",
      "Roger Zimmermann",
      "Jiashi Feng"
    ],
    "author_ids": [],
    "abstract": "Given input images, scene graph generation (SGG) aims to produce\ncomprehensive, graphical representations describing visual relationships among\nsalient objects. Recently, more efforts have been paid to the long tail problem\nin SGG; however, the imbalance in the fraction of missing labels of different\nclasses, or reporting bias, exacerbating the long tail is rarely considered and\ncannot be solved by the existing debiasing methods. In this paper we show that,\ndue to the missing labels, SGG can be viewed as a \"Learning from Positive and\nUnlabeled data\" (PU learning) problem, where the reporting bias can be removed\nby recovering the unbiased probabilities from the biased ones by utilizing\nlabel frequencies, i.e., the per-class fraction of labeled, positive examples\nin all the positive examples. To obtain accurate label frequency estimates, we\npropose Dynamic Label Frequency Estimation (DLFE) to take advantage of\ntraining-time data augmentation and average over multiple training iterations\nto introduce more valid examples. Extensive experiments show that DLFE is more\neffective in estimating label frequencies than a naive variant of the\ntraditional estimate, and DLFE significantly alleviates the long tail and\nachieves state-of-the-art debiasing performance on the VG dataset. We also show\nqualitatively that SGG models with DLFE produce prominently more balanced and\nunbiased scene graphs.",
    "published_date": "2021-07-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.02112v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.02060v1",
    "title": "On the Complexity of the Escape Problem for Linear Dynamical Systems over Compact Semialgebraic Sets",
    "authors": [
      "Julian D'Costa",
      "Engel Lefaucheux",
      "Eike Neumann",
      "Joël Ouaknine",
      "James Worrell"
    ],
    "author_ids": [],
    "abstract": "We study the computational complexity of the Escape Problem for discrete-time\nlinear dynamical systems over compact semialgebraic sets, or equivalently the\nTermination Problem for affine loops with compact semialgebraic guard sets.\nConsider the fragment of the theory of the reals consisting of negation-free\n$\\exists \\forall$-sentences without strict inequalities. We derive several\nequivalent characterisations of the associated complexity class which\ndemonstrate its robustness and illustrate its expressive power. We show that\nthe Compact Escape Problem is complete for this class.",
    "published_date": "2021-07-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CC",
      "cs.LO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.02060v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2107.01816v1",
    "title": "Sets of Marginals and Pearson-Correlation-based CHSH Inequalities for a Two-Qubit System",
    "authors": [
      "Yuwen Huang",
      "Pascal O. Vontobel"
    ],
    "author_ids": [],
    "abstract": "Quantum mass functions (QMFs), which are tightly related to decoherence\nfunctionals, were introduced by Loeliger and Vontobel [IEEE Trans. Inf. Theory,\n2017, 2020] as a generalization of probability mass functions toward modeling\nquantum information processing setups in terms of factor graphs.\n  Simple quantum mass functions (SQMFs) are a special class of QMFs that do not\nexplicitly model classical random variables. Nevertheless, classical random\nvariables appear implicitly in an SQMF if some marginals of the SQMF satisfy\nsome conditions; variables of the SQMF corresponding to these \"emerging\" random\nvariables are called classicable variables. Of particular interest are jointly\nclassicable variables.\n  In this paper we initiate the characterization of the set of marginals given\nby the collection of jointly classicable variables of a graphical model and\ncompare them with other concepts associated with graphical models like the sets\nof realizable marginals and the local marginal polytope.\n  In order to further characterize this set of marginals given by the\ncollection of jointly classicable variables, we generalize the CHSH inequality\nbased on the Pearson correlation coefficients, and thereby prove a conjecture\nproposed by Pozsgay et al. A crucial feature of this inequality is its\nnonlinearity, which poses difficulties in the proof.",
    "published_date": "2021-07-05T00:00:00",
    "year": 2021,
    "categories": [
      "quant-ph",
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.01816v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2107.01624v1",
    "title": "Implicit Gender Bias in Computer Science -- A Qualitative Study",
    "authors": [
      "Aurélie Breidenbach",
      "Caroline Mahlow",
      "Andreas Schreiber"
    ],
    "author_ids": [],
    "abstract": "Gender diversity in the tech sector is - not yet? - sufficient to create a\nbalanced ratio of men and women. For many women, access to computer science is\nhampered by socialization-related, social, cultural and structural obstacles.\nThe so-called implicit gender bias has a great influence in this respect. The\nlack of contact in areas of computer science makes it difficult to develop or\nexpand potential interests. Female role models as well as more transparency of\nthe job description should help women to promote their - possible - interest in\nthe job description. However, gender diversity can also be promoted and\nfostered through adapted measures by leaders.",
    "published_date": "2021-07-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.SE",
      "K.4.2; K.7.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.01624v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2107.10648v3",
    "title": "DEAP-FAKED: Knowledge Graph based Approach for Fake News Detection",
    "authors": [
      "Mohit Mayank",
      "Shakshi Sharma",
      "Rajesh Sharma"
    ],
    "author_ids": [],
    "abstract": "Fake News on social media platforms has attracted a lot of attention in\nrecent times, primarily for events related to politics (2016 US Presidential\nelections), healthcare (infodemic during COVID-19), to name a few. Various\nmethods have been proposed for detecting Fake News. The approaches span from\nexploiting techniques related to network analysis, Natural Language Processing\n(NLP), and the usage of Graph Neural Networks (GNNs). In this work, we propose\nDEAP-FAKED, a knowleDgE grAPh FAKe nEws Detection framework for identifying\nFake News. Our approach is a combination of the NLP -- where we encode the news\ncontent, and the GNN technique -- where we encode the Knowledge Graph (KG). A\nvariety of these encodings provides a complementary advantage to our detector.\nWe evaluate our framework using two publicly available datasets containing\narticles from domains such as politics, business, technology, and healthcare.\nAs part of dataset pre-processing, we also remove the bias, such as the source\nof the articles, which could impact the performance of the models. DEAP-FAKED\nobtains an F1-score of 88% and 78% for the two datasets, which is an\nimprovement of 21%, and 3% respectively, which shows the effectiveness of the\napproach.",
    "published_date": "2021-07-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.10648v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.01443v2",
    "title": "Quantifying agent impacts on contact sequences in social interactions",
    "authors": [
      "Mark M. Dekker",
      "Tessa F. Blanken",
      "Fabian Dablander",
      "Jiamin Ou",
      "Denny Borsboom",
      "Debabrata Panja"
    ],
    "author_ids": [],
    "abstract": "Human social behavior plays a crucial role in how pathogens like SARS-CoV-2\nor fake news spread in a population. Social interactions determine the contact\nnetwork among individuals, while spreading, requiring individual-to-individual\ntransmission, takes place on top of the network. Studying the topological\naspects of a contact network, therefore, not only has the potential of leading\nto valuable insights into how the behavior of individuals impacts spreading\nphenomena, but it may also open up possibilities for devising effective\nbehavioral interventions. Because of the temporal nature of interactions -\nsince the topology of the network, containing who is in contact with whom,\nwhen, for how long, and in which precise sequence, varies (rapidly) in time -\nanalyzing them requires developing network methods and metrics that respect\ntemporal variability, in contrast to those developed for static (i.e.,\ntime-invariant) networks. Here, by means of event mapping, we propose a method\nto quantify how quickly agents mingle by transforming temporal network data of\nagent contacts. We define a novel measure called 'contact sequence centrality',\nwhich quantifies the impact of an individual on the contact sequences,\nreflecting the individual's behavioral potential for spreading. Comparing\ncontact sequence centrality across agents allows for ranking the impact of\nagents and identifying potential 'behavioral super-spreaders'. The method is\napplied to social interaction data collected at an art fair in Amsterdam. We\nrelate the measure to the existing network metrics, both temporal and static,\nand find that (mostly at longer time scales) traditional metrics lose their\nresemblance to contact sequence centrality. Our work highlights the importance\nof accounting for the sequential nature of contacts when analyzing social\ninteractions.",
    "published_date": "2021-07-03T00:00:00",
    "year": 2021,
    "categories": [
      "physics.soc-ph",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.01443v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2107.01407v2",
    "title": "Optimality Inductive Biases and Agnostic Guidelines for Offline Reinforcement Learning",
    "authors": [
      "Lionel Blondé",
      "Alexandros Kalousis",
      "Stéphane Marchand-Maillet"
    ],
    "author_ids": [],
    "abstract": "The performance of state-of-the-art offline RL methods varies widely over the\nspectrum of dataset qualities, ranging from far-from-optimal random data to\nclose-to-optimal expert demonstrations. We re-implement these methods to test\ntheir reproducibility, and show that when a given method outperforms the others\non one end of the spectrum, it never does on the other end. This prevents us\nfrom naming a victor across the board. We attribute the asymmetry to the amount\nof inductive bias injected into the agent to entice it to posit that the\nbehavior underlying the offline dataset is optimal for the task. Our\ninvestigations confirm that careless injections of such optimality inductive\nbiases make dominant agents subpar as soon as the offline policy is\nsub-optimal. To bridge this gap, we generalize importance-weighted regression\nmethods that have proved the most versatile across the spectrum of dataset\ngrades into a modular framework that allows for the design of methods that\nalign with how much we know about the dataset. This modularity enables\nqualitatively different injections of optimality inductive biases. We show that\ncertain orchestrations strike the right balance, improving the return on one\nend of the spectrum without harming it on the other end. While the formulation\nof guidelines for the design of an offline method reduces to aligning the\namount of optimality bias to inject with what we know about the quality of the\ndata, the design of an agnostic method for which we need not know the quality\nof the data beforehand is more nuanced. Only our framework allowed us to design\na method that performed well across the spectrum while remaining modular if\nmore information about the quality of the data ever becomes available.",
    "published_date": "2021-07-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.01407v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.01398v2",
    "title": "Traffic Generation for Benchmarking Data Centre Networks",
    "authors": [
      "Christopher W. F. Parsonson",
      "Joshua L. Benjamin",
      "Georgios Zervas"
    ],
    "author_ids": [],
    "abstract": "Benchmarking is commonly used in research fields, such as computer\narchitecture design and machine learning, as a powerful paradigm for rigorously\nassessing, comparing, and developing novel technologies. However, the data\ncentre networking community lacks a standard open-access benchmark. This is\ncurtailing the community's understanding of existing systems and hindering the\nability with which novel technologies can be developed, compared, and tested.\n  We present TrafPy; an open-access framework for generating both realistic and\ncustom data centre network traffic traces. TrafPy is compatible with any\nsimulation, emulation, or experimentation environment, and can be used for\nstandardised benchmarking and for investigating the properties and limitations\nof network systems such as schedulers, switches, routers, and resource\nmanagers. To demonstrate the efficacy of TrafPy, we use it to conduct a\nthorough investigation into the sensitivity of 4 canonical scheduling\nalgorithms (shortest remaining processing time, fair share, first fit, and\nrandom) to varying traffic trace characteristics. We show how the fundamental\nscheduler performance insights revealed by these tests translate to 4 realistic\ndata centre network types; University, Private Enterprise, Commercial Cloud,\nand Social Media Cloud. We then draw conclusions as to which types of\nscheduling policies are most suited to which types of network load conditions\nand traffic characteristics, leading to the possibility of application-informed\ndecision making at the design stage and new dynamically adaptable scheduling\npolicies. TrafPy is open-sourced via GitHub and all data associated with this\nmanuscript via RDR.",
    "published_date": "2021-07-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.01398v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.01372v2",
    "title": "Learning Debiased Representation via Disentangled Feature Augmentation",
    "authors": [
      "Jungsoo Lee",
      "Eungyeup Kim",
      "Juyoung Lee",
      "Jihyeon Lee",
      "Jaegul Choo"
    ],
    "author_ids": [],
    "abstract": "Image classification models tend to make decisions based on peripheral\nattributes of data items that have strong correlation with a target variable\n(i.e., dataset bias). These biased models suffer from the poor generalization\ncapability when evaluated on unbiased datasets. Existing approaches for\ndebiasing often identify and emphasize those samples with no such correlation\n(i.e., bias-conflicting) without defining the bias type in advance. However,\nsuch bias-conflicting samples are significantly scarce in biased datasets,\nlimiting the debiasing capability of these approaches. This paper first\npresents an empirical analysis revealing that training with \"diverse\"\nbias-conflicting samples beyond a given training set is crucial for debiasing\nas well as the generalization capability. Based on this observation, we propose\na novel feature-level data augmentation technique in order to synthesize\ndiverse bias-conflicting samples. To this end, our method learns the\ndisentangled representation of (1) the intrinsic attributes (i.e., those\ninherently defining a certain class) and (2) bias attributes (i.e., peripheral\nattributes causing the bias), from a large number of bias-aligned samples, the\nbias attributes of which have strong correlation with the target variable.\nUsing the disentangled representation, we synthesize bias-conflicting samples\nthat contain the diverse intrinsic attributes of bias-aligned samples by\nswapping their latent features. By utilizing these diversified bias-conflicting\nfeatures during the training, our approach achieves superior classification\naccuracy and debiasing results against the existing baselines on synthetic and\nreal-world datasets.",
    "published_date": "2021-07-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.01372v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.01325v1",
    "title": "Fair Decision Rules for Binary Classification",
    "authors": [
      "Connor Lawless",
      "Oktay Gunluk"
    ],
    "author_ids": [],
    "abstract": "In recent years, machine learning has begun automating decision making in\nfields as varied as college admissions, credit lending, and criminal\nsentencing. The socially sensitive nature of some of these applications\ntogether with increasing regulatory constraints has necessitated the need for\nalgorithms that are both fair and interpretable. In this paper we consider the\nproblem of building Boolean rule sets in disjunctive normal form (DNF), an\ninterpretable model for binary classification, subject to fairness constraints.\nWe formulate the problem as an integer program that maximizes classification\naccuracy with explicit constraints on two different measures of classification\nparity: equality of opportunity and equalized odds. Column generation\nframework, with a novel formulation, is used to efficiently search over\nexponentially many possible rules. When combined with faster heuristics, our\nmethod can deal with large data-sets. Compared to other fair and interpretable\nclassifiers, our method is able to find rule sets that meet stricter notions of\nfairness with a modest trade-off in accuracy.",
    "published_date": "2021-07-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.01325v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.03900v1",
    "title": "The Price of Diversity",
    "authors": [
      "Hari Bandi",
      "Dimitris Bertsimas"
    ],
    "author_ids": [],
    "abstract": "Systemic bias with respect to gender, race and ethnicity, often unconscious,\nis prevalent in datasets involving choices among individuals. Consequently,\nsociety has found it challenging to alleviate bias and achieve diversity in a\nway that maintains meritocracy in such settings. We propose (a) a novel\noptimization approach based on optimally flipping outcome labels and training\nclassification models simultaneously to discover changes to be made in the\nselection process so as to achieve diversity without significantly affecting\nmeritocracy, and (b) a novel implementation tool employing optimal\nclassification trees to provide insights on which attributes of individuals\nlead to flipping of their labels, and to help make changes in the current\nselection processes in a manner understandable by human decision makers. We\npresent case studies on three real-world datasets consisting of parole,\nadmissions to the bar and lending decisions, and demonstrate that the price of\ndiversity is low and sometimes negative, that is we can modify our selection\nprocesses in a way that enhances diversity without affecting meritocracy\nsignificantly, and sometimes improving it.",
    "published_date": "2021-07-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.03900v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.01284v1",
    "title": "A Novel Disaster Image Dataset and Characteristics Analysis using Attention Model",
    "authors": [
      "Fahim Faisal Niloy",
      "Arif",
      "Abu Bakar Siddik Nayem",
      "Anis Sarker",
      "Ovi Paul",
      "M. Ashraful Amin",
      "Amin Ahsan Ali",
      "Moinul Islam Zaber",
      "AKM Mahbubur Rahman"
    ],
    "author_ids": [],
    "abstract": "The advancement of deep learning technology has enabled us to develop systems\nthat outperform any other classification technique. However, success of any\nempirical system depends on the quality and diversity of the data available to\ntrain the proposed system. In this research, we have carefully accumulated a\nrelatively challenging dataset that contains images collected from various\nsources for three different disasters: fire, water and land. Besides this, we\nhave also collected images for various damaged infrastructure due to natural or\nman made calamities and damaged human due to war or accidents. We have also\naccumulated image data for a class named non-damage that contains images with\nno such disaster or sign of damage in them. There are 13,720 manually annotated\nimages in this dataset, each image is annotated by three individuals. We are\nalso providing discriminating image class information annotated manually with\nbounding box for a set of 200 test images. Images are collected from different\nnews portals, social media, and standard datasets made available by other\nresearchers. A three layer attention model (TLAM) is trained and average five\nfold validation accuracy of 95.88% is achieved. Moreover, on the 200 unseen\ntest images this accuracy is 96.48%. We also generate and compare attention\nmaps for these test images to determine the characteristics of the trained\nattention model. Our dataset is available at\nhttps://niloy193.github.io/Disaster-Dataset",
    "published_date": "2021-07-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.01284v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.01183v4",
    "title": "Ethics Sheets for AI Tasks",
    "authors": [
      "Saif M. Mohammad"
    ],
    "author_ids": [],
    "abstract": "Several high-profile events, such as the mass testing of emotion recognition\nsystems on vulnerable sub-populations and using question answering systems to\nmake moral judgments, have highlighted how technology will often lead to more\nadverse outcomes for those that are already marginalized. At issue here are not\njust individual systems and datasets, but also the AI tasks themselves. In this\nposition paper, I make a case for thinking about ethical considerations not\njust at the level of individual models and datasets, but also at the level of\nAI tasks. I will present a new form of such an effort, Ethics Sheets for AI\nTasks, dedicated to fleshing out the assumptions and ethical considerations\nhidden in how a task is commonly framed and in the choices we make regarding\nthe data, method, and evaluation. I will also present a template for ethics\nsheets with 50 ethical considerations, using the task of emotion recognition as\na running example. Ethics sheets are a mechanism to engage with and document\nethical considerations before building datasets and systems. Similar to survey\narticles, a small number of ethics sheets can serve numerous researchers and\ndevelopers.",
    "published_date": "2021-07-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.01183v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.01125v3",
    "title": "On Measuring and Controlling the Spectral Bias of the Deep Image Prior",
    "authors": [
      "Zenglin Shi",
      "Pascal Mettes",
      "Subhransu Maji",
      "Cees G. M. Snoek"
    ],
    "author_ids": [],
    "abstract": "The deep image prior showed that a randomly initialized network with a\nsuitable architecture can be trained to solve inverse imaging problems by\nsimply optimizing it's parameters to reconstruct a single degraded image.\nHowever, it suffers from two practical limitations. First, it remains unclear\nhow to control the prior beyond the choice of the network architecture. Second,\ntraining requires an oracle stopping criterion as during the optimization the\nperformance degrades after reaching an optimum value. To address these\nchallenges we introduce a frequency-band correspondence measure to characterize\nthe spectral bias of the deep image prior, where low-frequency image signals\nare learned faster and better than high-frequency counterparts. Based on our\nobservations, we propose techniques to prevent the eventual performance\ndegradation and accelerate convergence. We introduce a Lipschitz-controlled\nconvolution layer and a Gaussian-controlled upsampling layer as plug-in\nreplacements for layers used in the deep architectures. The experiments show\nthat with these changes the performance does not degrade during optimization,\nrelieving us from the need for an oracle stopping criterion. We further outline\na stopping criterion to avoid superfluous computation. Finally, we show that\nour approach obtains favorable results compared to current approaches across\nvarious denoising, deblocking, inpainting, super-resolution and detail\nenhancement tasks. Code is available at\n\\url{https://github.com/shizenglin/Measure-and-Control-Spectral-Bias}.",
    "published_date": "2021-07-02T00:00:00",
    "year": 2021,
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.01125v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.01085v2",
    "title": "Static output feedback stabilization of uncertain rational nonlinear systems with input saturation",
    "authors": [
      "Thiago Alves Lima",
      "Diego de. S. Madeira",
      "Valessa V. Viana",
      "Ricardo C. L. F. Oliveira"
    ],
    "author_ids": [],
    "abstract": "In this paper, the notion of robust strict QSR-dissipativity is applied to\nsolve the static output feedback control problem for a class of continuous-time\nnonlinear rational systems subject to input saturation and bounded parametric\nuncertainties. A local dissipativity condition is combined with generalized\nsector conditions to formulate the synthesis of a stabilizing controller in\nterms of linear matrix inequalities. The strategy applies to general static\noutput feedback design without any restrictions on the plant output equation.\nAn iterative algorithm based on linear matrix inequalities is proposed in order\nto compute the feedback gain matrix that maximizes the estimate of the\nclosed-loop region of attraction. Numerical examples are provided to illustrate\nthe applicability of this new approach in examples borrowed from the\nliterature.",
    "published_date": "2021-07-02T00:00:00",
    "year": 2021,
    "categories": [
      "math.OC",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.01085v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2107.00904v1",
    "title": "Video Conferencing and Flow-Rate Fairness: A First Look at Zoom and the Impact of Flow-Queuing AQM",
    "authors": [
      "Constantin Sander",
      "Ike Kunze",
      "Klaus Wehrle",
      "Jan Rüth"
    ],
    "author_ids": [],
    "abstract": "Congestion control is essential for the stability of the Internet and the\ncorresponding algorithms are commonly evaluated for interoperability based on\nflow-rate fairness. In contrast, video conferencing software such as Zoom uses\ncustom congestion control algorithms whose fairness behavior is mostly unknown.\nAggravatingly, video conferencing has recently seen a drastic increase in use -\npartly caused by the COVID-19 pandemic - and could hence negatively affect how\navailable Internet resources are shared. In this paper, we thus investigate the\nflow-rate fairness of video conferencing congestion control at the example of\nZoom and influences of deploying AQM. We find that Zoom is slow to react to\nbandwidth changes and uses two to three times the bandwidth of TCP in\nlow-bandwidth scenarios. Moreover, also when competing with delay aware\ncongestion control such as BBR, we see high queuing delays. AQM reduces these\nqueuing delays and can equalize the bandwidth use when used with flow-queuing.\nHowever, it then introduces high packet loss for Zoom, leaving the question how\ndelay and loss affect Zoom's QoE. We hence show a preliminary user study in the\nappendix which indicates that the QoE is at least not improved and should be\nstudied further.",
    "published_date": "2021-07-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.00904v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2107.00793v3",
    "title": "The Causal-Neural Connection: Expressiveness, Learnability, and Inference",
    "authors": [
      "Kevin Xia",
      "Kai-Zhan Lee",
      "Yoshua Bengio",
      "Elias Bareinboim"
    ],
    "author_ids": [],
    "abstract": "One of the central elements of any causal inference is an object called\nstructural causal model (SCM), which represents a collection of mechanisms and\nexogenous sources of random variation of the system under investigation (Pearl,\n2000). An important property of many kinds of neural networks is universal\napproximability: the ability to approximate any function to arbitrary\nprecision. Given this property, one may be tempted to surmise that a collection\nof neural nets is capable of learning any SCM by training on data generated by\nthat SCM. In this paper, we show this is not the case by disentangling the\nnotions of expressivity and learnability. Specifically, we show that the causal\nhierarchy theorem (Thm. 1, Bareinboim et al., 2020), which describes the limits\nof what can be learned from data, still holds for neural models. For instance,\nan arbitrarily complex and expressive neural net is unable to predict the\neffects of interventions given observational data alone. Given this result, we\nintroduce a special type of SCM called a neural causal model (NCM), and\nformalize a new type of inductive bias to encode structural constraints\nnecessary for performing causal inferences. Building on this new class of\nmodels, we focus on solving two canonical tasks found in the literature known\nas causal identification and estimation. Leveraging the neural toolbox, we\ndevelop an algorithm that is both sufficient and necessary to determine whether\na causal effect can be learned from data (i.e., causal identifiability); it\nthen estimates the effect whenever identifiability holds (causal estimation).\nSimulations corroborate the proposed approach.",
    "published_date": "2021-07-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.00793v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.00730v1",
    "title": "Normalizing Flow based Hidden Markov Models for Classification of Speech Phones with Explainability",
    "authors": [
      "Anubhab Ghosh",
      "Antoine Honoré",
      "Dong Liu",
      "Gustav Eje Henter",
      "Saikat Chatterjee"
    ],
    "author_ids": [],
    "abstract": "In pursuit of explainability, we develop generative models for sequential\ndata. The proposed models provide state-of-the-art classification results and\nrobust performance for speech phone classification. We combine modern neural\nnetworks (normalizing flows) and traditional generative models (hidden Markov\nmodels - HMMs). Normalizing flow-based mixture models (NMMs) are used to model\nthe conditional probability distribution given the hidden state in the HMMs.\nModel parameters are learned through judicious combinations of time-tested\nBayesian learning methods and contemporary neural network learning methods. We\nmainly combine expectation-maximization (EM) and mini-batch gradient descent.\nThe proposed generative models can compute likelihood of a data and hence\ndirectly suitable for maximum-likelihood (ML) classification approach. Due to\nstructural flexibility of HMMs, we can use different normalizing flow models.\nThis leads to different types of HMMs providing diversity in data modeling\ncapacity. The diversity provides an opportunity for easy decision fusion from\ndifferent models. For a standard speech phone classification setup involving 39\nphones (classes) and the TIMIT dataset, we show that the use of standard\nfeatures called mel-frequency-cepstral-coeffcients (MFCCs), the proposed\ngenerative models, and the decision fusion together can achieve $86.6\\%$\naccuracy by generative training only. This result is close to state-of-the-art\nresults, for examples, $86.2\\%$ accuracy of PyTorch-Kaldi toolkit [1], and\n$85.1\\%$ accuracy using light gated recurrent units [2]. We do not use any\ndiscriminative learning approach and related sophisticated features in this\narticle.",
    "published_date": "2021-07-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.00730v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.00593v3",
    "title": "Disaggregated Interventions to Reduce Inequality",
    "authors": [
      "Lucius E. J. Bynum",
      "Joshua R. Loftus",
      "Julia Stoyanovich"
    ],
    "author_ids": [],
    "abstract": "A significant body of research in the data sciences considers unfair\ndiscrimination against social categories such as race or gender that could\noccur or be amplified as a result of algorithmic decisions. Simultaneously,\nreal-world disparities continue to exist, even before algorithmic decisions are\nmade. In this work, we draw on insights from the social sciences brought into\nthe realm of causal modeling and constrained optimization, and develop a novel\nalgorithmic framework for tackling pre-existing real-world disparities. The\npurpose of our framework, which we call the \"impact remediation framework,\" is\nto measure real-world disparities and discover the optimal intervention\npolicies that could help improve equity or access to opportunity for those who\nare underserved with respect to an outcome of interest. We develop a\ndisaggregated approach to tackling pre-existing disparities that relaxes the\ntypical set of assumptions required for the use of social categories in\nstructural causal models. Our approach flexibly incorporates counterfactuals\nand is compatible with various ontological assumptions about the nature of\nsocial categories. We demonstrate impact remediation with a hypothetical case\nstudy and compare our disaggregated approach to an existing state-of-the-art\napproach, comparing its structure and resulting policy recommendations. In\ncontrast to most work on optimal policy learning, we explore disparity\nreduction itself as an objective, explicitly focusing the power of algorithms\non reducing inequality.",
    "published_date": "2021-07-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.AP",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.00593v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.00534v1",
    "title": "The Limit Order Book Recreation Model (LOBRM): An Extended Analysis",
    "authors": [
      "Zijian Shi",
      "John Cartlidge"
    ],
    "author_ids": [],
    "abstract": "The limit order book (LOB) depicts the fine-grained demand and supply\nrelationship for financial assets and is widely used in market microstructure\nstudies. Nevertheless, the availability and high cost of LOB data restrict its\nwider application. The LOB recreation model (LOBRM) was recently proposed to\nbridge this gap by synthesizing the LOB from trades and quotes (TAQ) data.\nHowever, in the original LOBRM study, there were two limitations: (1)\nexperiments were conducted on a relatively small dataset containing only one\nday of LOB data; and (2) the training and testing were performed in a\nnon-chronological fashion, which essentially re-frames the task as\ninterpolation and potentially introduces lookahead bias. In this study, we\nextend the research on LOBRM and further validate its use in real-world\napplication scenarios. We first advance the workflow of LOBRM by (1) adding a\ntime-weighted z-score standardization for the LOB and (2) substituting the\nordinary differential equation kernel with an exponential decay kernel to lower\ncomputation complexity. Experiments are conducted on the extended LOBSTER\ndataset in a chronological fashion, as it would be used in a real-world\napplication. We find that (1) LOBRM with decay kernel is superior to\ntraditional non-linear models, and module ensembling is effective; (2)\nprediction accuracy is negatively related to the volatility of order volumes\nresting in the LOB; (3) the proposed sparse encoding method for TAQ exhibits\ngood generalization ability and can facilitate manifold tasks; and (4) the\ninfluence of stochastic drift on prediction accuracy can be alleviated by\nincreasing historical samples.",
    "published_date": "2021-07-01T00:00:00",
    "year": 2021,
    "categories": [
      "q-fin.TR",
      "cs.LG",
      "q-fin.ST"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.00534v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.00439v3",
    "title": "What do End-to-End Speech Models Learn about Speaker, Language and Channel Information? A Layer-wise and Neuron-level Analysis",
    "authors": [
      "Shammur Absar Chowdhury",
      "Nadir Durrani",
      "Ahmed Ali"
    ],
    "author_ids": [],
    "abstract": "Deep neural networks are inherently opaque and challenging to interpret.\nUnlike hand-crafted feature-based models, we struggle to comprehend the\nconcepts learned and how they interact within these models. This understanding\nis crucial not only for debugging purposes but also for ensuring fairness in\nethical decision-making. In our study, we conduct a post-hoc functional\ninterpretability analysis of pretrained speech models using the probing\nframework [1]. Specifically, we analyze utterance-level representations of\nspeech models trained for various tasks such as speaker recognition and dialect\nidentification. We conduct layer and neuron-wise analyses, probing for speaker,\nlanguage, and channel properties. Our study aims to answer the following\nquestions: i) what information is captured within the representations? ii) how\nis it represented and distributed? and iii) can we identify a minimal subset of\nthe network that possesses this information?\n  Our results reveal several novel findings, including: i) channel and gender\ninformation are distributed across the network, ii) the information is\nredundantly available in neurons with respect to a task, iii) complex\nproperties such as dialectal information are encoded only in the task-oriented\npretrained network, iv) and is localised in the upper layers, v) we can extract\na minimal subset of neurons encoding the pre-defined property, vi) salient\nneurons are sometimes shared between properties, vii) our analysis highlights\nthe presence of biases (for example gender) in the network. Our\ncross-architectural comparison indicates that: i) the pretrained models capture\nspeaker-invariant information, and ii) CNN models are competitive with\nTransformer models in encoding various understudied properties.",
    "published_date": "2021-07-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.00439v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.00360v1",
    "title": "Towards Measuring Bias in Image Classification",
    "authors": [
      "Nina Schaaf",
      "Omar de Mitri",
      "Hang Beom Kim",
      "Alexander Windberger",
      "Marco F. Huber"
    ],
    "author_ids": [],
    "abstract": "Convolutional Neural Networks (CNN) have become de fact state-of-the-art for\nthe main computer vision tasks. However, due to the complex underlying\nstructure their decisions are hard to understand which limits their use in some\ncontext of the industrial world. A common and hard to detect challenge in\nmachine learning (ML) tasks is data bias. In this work, we present a systematic\napproach to uncover data bias by means of attribution maps. For this purpose,\nfirst an artificial dataset with a known bias is created and used to train\nintentionally biased CNNs. The networks' decisions are then inspected using\nattribution maps. Finally, meaningful metrics are used to measure the\nattribution maps' representativeness with respect to the known bias. The\nproposed study shows that some attribution map techniques highlight the\npresence of bias in the data better than others and metrics can support the\nidentification of bias.",
    "published_date": "2021-07-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.00360v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.00357v1",
    "title": "Prophet Inequality with Competing Agents",
    "authors": [
      "Tomer Ezra",
      "Michal Feldman",
      "Ron Kupfer"
    ],
    "author_ids": [],
    "abstract": "We introduce a model of competing agents in a prophet setting, where rewards\narrive online, and decisions are made immediately and irrevocably. The rewards\nare unknown from the outset, but they are drawn from a known probability\ndistribution. In the standard prophet setting, a single agent makes selection\ndecisions in an attempt to maximize her expected reward. The novelty of our\nmodel is the introduction of a competition setting, where multiple agents\ncompete over the arriving rewards, and make online selection decisions\nsimultaneously, as rewards arrive. If a given reward is selected by more than a\nsingle agent, ties are broken either randomly or by a fixed ranking of the\nagents. The consideration of competition turns the prophet setting from an\nonline decision making scenario to a multi-agent game.\n  For both random and ranked tie-breaking rules, we present simple threshold\nstrategies for the agents that give them high guarantees, independent of the\nstrategies taken by others. In particular, for random tie-breaking, every agent\ncan guarantee herself at least $\\frac{1}{k+1}$ of the highest reward, and at\nleast $\\frac{1}{2k}$ of the optimal social welfare. For ranked tie-breaking,\nthe $i$th ranked agent can guarantee herself at least a half of the $i$th\nhighest reward. We complement these results by matching upper bounds, even with\nrespect to equilibrium profiles. For ranked tie-breaking rule, we also show a\ncorrespondence between the equilibrium of the $k$-agent game and the optimal\nstrategy of a single decision maker who can select up to $k$ rewards.",
    "published_date": "2021-07-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.00357v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2107.00309v4",
    "title": "Adversarial Sample Detection for Speaker Verification by Neural Vocoders",
    "authors": [
      "Haibin Wu",
      "Po-chun Hsu",
      "Ji Gao",
      "Shanshan Zhang",
      "Shen Huang",
      "Jian Kang",
      "Zhiyong Wu",
      "Helen Meng",
      "Hung-yi Lee"
    ],
    "author_ids": [],
    "abstract": "Automatic speaker verification (ASV), one of the most important technology\nfor biometric identification, has been widely adopted in security-critical\napplications. However, ASV is seriously vulnerable to recently emerged\nadversarial attacks, yet effective countermeasures against them are limited. In\nthis paper, we adopt neural vocoders to spot adversarial samples for ASV. We\nuse the neural vocoder to re-synthesize audio and find that the difference\nbetween the ASV scores for the original and re-synthesized audio is a good\nindicator for discrimination between genuine and adversarial samples. This\neffort is, to the best of our knowledge, among the first to pursue such a\ntechnical direction for detecting time-domain adversarial samples for ASV, and\nhence there is a lack of established baselines for comparison. Consequently, we\nimplement the Griffin-Lim algorithm as the detection baseline. The proposed\napproach achieves effective detection performance that outperforms the\nbaselines in all the settings. We also show that the neural vocoder adopted in\nthe detection framework is dataset-independent. Our codes will be made\nopen-source for future works to do fair comparison.",
    "published_date": "2021-07-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SD",
      "cs.LG",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.00309v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.00297v1",
    "title": "Sonority Measurement Using System, Source, and Suprasegmental Information",
    "authors": [
      "Bidisha Sharma",
      "S. R. Mahadeva Prasanna"
    ],
    "author_ids": [],
    "abstract": "Sonorant sounds are characterized by regions with prominent formant\nstructure, high energy and high degree of periodicity. In this work, the\nvocal-tract system, excitation source and suprasegmental features derived from\nthe speech signal are analyzed to measure the sonority information present in\neach of them. Vocal-tract system information is extracted from the Hilbert\nenvelope of numerator of group delay function. It is derived from zero time\nwindowed speech signal that provides better resolution of the formants. A\nfive-dimensional feature set is computed from the estimated formants to measure\nthe prominence of the spectral peaks. A feature representing strength of\nexcitation is derived from the Hilbert envelope of linear prediction residual,\nwhich represents the source information. Correlation of speech over ten\nconsecutive pitch periods is used as the suprasegmental feature representing\nperiodicity information. The combination of evidences from the three different\naspects of speech provides better discrimination among different sonorant\nclasses, compared to the baseline MFCC features. The usefulness of the proposed\nsonority feature is demonstrated in the tasks of phoneme recognition and\nsonorant classification.",
    "published_date": "2021-07-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SD",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.00297v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2107.00143v1",
    "title": "One-class Steel Detector Using Patch GAN Discriminator for Visualising Anomalous Feature Map",
    "authors": [
      "Takato Yasuno",
      "Junichiro Fujii",
      "Sakura Fukami"
    ],
    "author_ids": [],
    "abstract": "For steel product manufacturing in indoor factories, steel defect detection\nis important for quality control. For example, a steel sheet is extremely\ndelicate, and must be accurately inspected. However, to maintain the painted\nsteel parts of the infrastructure around a severe outdoor environment,\ncorrosion detection is critical for predictive maintenance. In this paper, we\npropose a general-purpose application for steel anomaly detection that consists\nof the following four components. The first, a learner, is a unit image\nclassification network to determine whether the region of interest or\nbackground has been recognised, after dividing the original large sized image\ninto 256 square unit images. The second, an extractor, is a discriminator\nfeature encoder based on a pre-trained steel generator with a patch generative\nadversarial network discriminator(GAN). The third, an anomaly detector, is a\none-class support vector machine(SVM) to predict the anomaly score using the\ndiscriminator feature. The fourth, an indicator, is an anomalous probability\nmap used to visually explain the anomalous features. Furthermore, we\ndemonstrated our method through the inspection of steel sheet defects with\n13,774 unit images using high-speed cameras, and painted steel corrosion with\n19,766 unit images based on an eye inspection of the photographs. Finally, we\nvisualise anomalous feature maps of steel using a strip and painted steel\ninspection dataset",
    "published_date": "2021-06-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "eess.IV",
      "I.5.4; I.2.10"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.00143v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.00052v2",
    "title": "Stochastic Gradient Descent-Ascent and Consensus Optimization for Smooth Games: Convergence Analysis under Expected Co-coercivity",
    "authors": [
      "Nicolas Loizou",
      "Hugo Berard",
      "Gauthier Gidel",
      "Ioannis Mitliagkas",
      "Simon Lacoste-Julien"
    ],
    "author_ids": [],
    "abstract": "Two of the most prominent algorithms for solving unconstrained smooth games\nare the classical stochastic gradient descent-ascent (SGDA) and the recently\nintroduced stochastic consensus optimization (SCO) [Mescheder et al., 2017].\nSGDA is known to converge to a stationary point for specific classes of games,\nbut current convergence analyses require a bounded variance assumption. SCO is\nused successfully for solving large-scale adversarial problems, but its\nconvergence guarantees are limited to its deterministic variant. In this work,\nwe introduce the expected co-coercivity condition, explain its benefits, and\nprovide the first last-iterate convergence guarantees of SGDA and SCO under\nthis condition for solving a class of stochastic variational inequality\nproblems that are potentially non-monotone. We prove linear convergence of both\nmethods to a neighborhood of the solution when they use constant step-size, and\nwe propose insightful stepsize-switching rules to guarantee convergence to the\nexact solution. In addition, our convergence guarantees hold under the\narbitrary sampling paradigm, and as such, we give insights into the complexity\nof minibatching.",
    "published_date": "2021-06-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.GT",
      "math.OC",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.00052v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.00032v1",
    "title": "Agree to Disagree: Subjective Fairness in Privacy-Restricted Decentralised Conflict Resolution",
    "authors": [
      "Alex Raymond",
      "Matthew Malencia",
      "Guilherme Paulino-Passos",
      "Amanda Prorok"
    ],
    "author_ids": [],
    "abstract": "Fairness is commonly seen as a property of the global outcome of a system and\nassumes centralisation and complete knowledge. However, in real decentralised\napplications, agents only have partial observation capabilities. Under limited\ninformation, agents rely on communication to divulge some of their private (and\nunobservable) information to others. When an agent deliberates to resolve\nconflicts, limited knowledge may cause its perspective of a correct outcome to\ndiffer from the actual outcome of the conflict resolution. This is subjective\nunfairness.\n  To enable decentralised, fairness-aware conflict resolution under privacy\nconstraints, we have two contributions: (1) a novel interaction approach and\n(2) a formalism of the relationship between privacy and fairness. Our proposed\ninteraction approach is an architecture for privacy-aware explainable conflict\nresolution where agents engage in a dialogue of hypotheses and facts. To\nmeasure the privacy-fairness relationship, we define subjective and objective\nfairness on both the local and global scope and formalise the impact of partial\nobservability due to privacy in these different notions of fairness.\n  We first study our proposed architecture and the privacy-fairness\nrelationship in the abstract, testing different argumentation strategies on a\nlarge number of randomised cultures. We empirically demonstrate the trade-off\nbetween privacy, objective fairness, and subjective fairness and show that\nbetter strategies can mitigate the effects of privacy in distributed systems.\nIn addition to this analysis across a broad set of randomised abstract\ncultures, we analyse a case study for a specific scenario: we instantiate our\narchitecture in a multi-agent simulation of prioritised rule-aware collision\navoidance with limited information disclosure.",
    "published_date": "2021-06-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.MA",
      "cs.LO",
      "cs.RO",
      "I.2.11; I.2.3; I.2.9"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.00032v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.16225v2",
    "title": "Analytic Insights into Structure and Rank of Neural Network Hessian Maps",
    "authors": [
      "Sidak Pal Singh",
      "Gregor Bachmann",
      "Thomas Hofmann"
    ],
    "author_ids": [],
    "abstract": "The Hessian of a neural network captures parameter interactions through\nsecond-order derivatives of the loss. It is a fundamental object of study,\nclosely tied to various problems in deep learning, including model design,\noptimization, and generalization. Most prior work has been empirical, typically\nfocusing on low-rank approximations and heuristics that are blind to the\nnetwork structure. In contrast, we develop theoretical tools to analyze the\nrange of the Hessian map, providing us with a precise understanding of its rank\ndeficiency as well as the structural reasons behind it. This yields exact\nformulas and tight upper bounds for the Hessian rank of deep linear networks,\nallowing for an elegant interpretation in terms of rank deficiency. Moreover,\nwe demonstrate that our bounds remain faithful as an estimate of the numerical\nHessian rank, for a larger class of models such as rectified and hyperbolic\ntangent networks. Further, we also investigate the implications of model\narchitecture (e.g.~width, depth, bias) on the rank deficiency. Overall, our\nwork provides novel insights into the source and extent of redundancy in\noverparameterized networks.",
    "published_date": "2021-06-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.NE",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.16225v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.16163v2",
    "title": "The MultiBERTs: BERT Reproductions for Robustness Analysis",
    "authors": [
      "Thibault Sellam",
      "Steve Yadlowsky",
      "Jason Wei",
      "Naomi Saphra",
      "Alexander D'Amour",
      "Tal Linzen",
      "Jasmijn Bastings",
      "Iulia Turc",
      "Jacob Eisenstein",
      "Dipanjan Das",
      "Ian Tenney",
      "Ellie Pavlick"
    ],
    "author_ids": [],
    "abstract": "Experiments with pre-trained models such as BERT are often based on a single\ncheckpoint. While the conclusions drawn apply to the artifact tested in the\nexperiment (i.e., the particular instance of the model), it is not always clear\nwhether they hold for the more general procedure which includes the\narchitecture, training data, initialization scheme, and loss function. Recent\nwork has shown that repeating the pre-training process can lead to\nsubstantially different performance, suggesting that an alternate strategy is\nneeded to make principled statements about procedures. To enable researchers to\ndraw more robust conclusions, we introduce the MultiBERTs, a set of 25\nBERT-Base checkpoints, trained with similar hyper-parameters as the original\nBERT model but differing in random weight initialization and shuffling of\ntraining data. We also define the Multi-Bootstrap, a non-parametric bootstrap\nmethod for statistical inference designed for settings where there are multiple\npre-trained models and limited test data. To illustrate our approach, we\npresent a case study of gender bias in coreference resolution, in which the\nMulti-Bootstrap lets us measure effects that may not be detected with a single\ncheckpoint. We release our models and statistical library along with an\nadditional set of 140 intermediate checkpoints captured during pre-training to\nfacilitate research on learning dynamics.",
    "published_date": "2021-06-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.16163v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.16125v1",
    "title": "Affective Image Content Analysis: Two Decades Review and New Perspectives",
    "authors": [
      "Sicheng Zhao",
      "Xingxu Yao",
      "Jufeng Yang",
      "Guoli Jia",
      "Guiguang Ding",
      "Tat-Seng Chua",
      "Björn W. Schuller",
      "Kurt Keutzer"
    ],
    "author_ids": [],
    "abstract": "Images can convey rich semantics and induce various emotions in viewers.\nRecently, with the rapid advancement of emotional intelligence and the\nexplosive growth of visual data, extensive research efforts have been dedicated\nto affective image content analysis (AICA). In this survey, we will\ncomprehensively review the development of AICA in the recent two decades,\nespecially focusing on the state-of-the-art methods with respect to three main\nchallenges -- the affective gap, perception subjectivity, and label noise and\nabsence. We begin with an introduction to the key emotion representation models\nthat have been widely employed in AICA and description of available datasets\nfor performing evaluation with quantitative comparison of label noise and\ndataset bias. We then summarize and compare the representative approaches on\n(1) emotion feature extraction, including both handcrafted and deep features,\n(2) learning methods on dominant emotion recognition, personalized emotion\nprediction, emotion distribution learning, and learning from noisy data or few\nlabels, and (3) AICA based applications. Finally, we discuss some challenges\nand promising research directions in the future, such as image content and\ncontext understanding, group emotion clustering, and viewer-image interaction.",
    "published_date": "2021-06-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.16125v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.16122v2",
    "title": "Zombies in the Loop? Humans Trust Untrustworthy AI-Advisors for Ethical Decisions",
    "authors": [
      "Sebastian Krügel",
      "Andreas Ostermaier",
      "Matthias Uhl"
    ],
    "author_ids": [],
    "abstract": "Departing from the claim that AI needs to be trustworthy, we find that\nethical advice from an AI-powered algorithm is trusted even when its users know\nnothing about its training data and when they learn information about it that\nwarrants distrust. We conducted online experiments where the subjects took the\nrole of decision-makers who received advice from an algorithm on how to deal\nwith an ethical dilemma. We manipulated the information about the algorithm and\nstudied its influence. Our findings suggest that AI is overtrusted rather than\ndistrusted. We suggest digital literacy as a potential remedy to ensure the\nresponsible use of AI.",
    "published_date": "2021-06-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.16122v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.16050v1",
    "title": "Ethical AI-Powered Regression Test Selection",
    "authors": [
      "Per Erik Strandberg",
      "Mirgita Frasheri",
      "Eduard Paul Enoiu"
    ],
    "author_ids": [],
    "abstract": "Test automation is common in software development; often one tests repeatedly\nto identify regressions. If the amount of test cases is large, one may select a\nsubset and only use the most important test cases. The regression test\nselection (RTS) could be automated and enhanced with Artificial Intelligence\n(AI-RTS). This however could introduce ethical challenges. While such\nchallenges in AI are in general well studied, there is a gap with respect to\nethical AI-RTS. By exploring the literature and learning from our experiences\nof developing an industry AI-RTS tool, we contribute to the literature by\nidentifying three challenges (assigning responsibility, bias in decision-making\nand lack of participation) and three approaches (explicability, supervision and\ndiversity). Additionally, we provide a checklist for ethical AI-RTS to help\nguide the decision-making of the stakeholders involved in the process.",
    "published_date": "2021-06-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.16050v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.15917v1",
    "title": "Explaining Caste-based Digital Divide in India",
    "authors": [
      "R Vaidehi",
      "A Bheemeshwar Reddy",
      "Sudatta Banerjee"
    ],
    "author_ids": [],
    "abstract": "With the increasing importance of information and communication technologies\nin access to basic services like education and health, the question of the\ndigital divide based on caste assumes importance in India where large\nsocioeconomic disparities persist between different caste groups. Studies on\ncaste-based digital inequality are still scanty in India. Using nationally\nrepresentative survey data, this paper analyzes the first-level digital divide\n(ownership of computer and access to the internet) and the second-level digital\ndivide (individual's skill to use computer and the internet) between the\ndisadvantaged caste group and the others. Further, this paper identifies the\ncaste group-based differences in socioeconomic factors that contribute to the\ndigital divide between these groups using a non-linear decomposition method.\nThe results show that there exists a large first-level and second-level digital\ndivide between the disadvantaged caste groups and others in India. The\nnon-linear decomposition results indicate that the caste-based digital divide\nin India is rooted in historical socioeconomic deprivation of disadvantaged\ncaste groups. More than half of the caste-based digital gap is attributable to\ndifferences in educational attainment and income between the disadvantaged\ncaste groups and others. The findings of this study highlight the urgent need\nfor addressing educational and income inequality between the different caste\ngroups in India in order to bridge the digital divide.",
    "published_date": "2021-06-30T00:00:00",
    "year": 2021,
    "categories": [
      "econ.GN",
      "cs.CY",
      "q-fin.EC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.15917v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.15896v1",
    "title": "Whose Opinions Matter? Perspective-aware Models to Identify Opinions of Hate Speech Victims in Abusive Language Detection",
    "authors": [
      "Sohail Akhtar",
      "Valerio Basile",
      "Viviana Patti"
    ],
    "author_ids": [],
    "abstract": "Social media platforms provide users the freedom of expression and a medium\nto exchange information and express diverse opinions. Unfortunately, this has\nalso resulted in the growth of abusive content with the purpose of\ndiscriminating people and targeting the most vulnerable communities such as\nimmigrants, LGBT, Muslims, Jews and women. Because abusive language is\nsubjective in nature, there might be highly polarizing topics or events\ninvolved in the annotation of abusive contents such as hate speech (HS).\nTherefore, we need novel approaches to model conflicting perspectives and\nopinions coming from people with different personal and demographic\nbackgrounds. In this paper, we present an in-depth study to model polarized\nopinions coming from different communities under the hypothesis that similar\ncharacteristics (ethnicity, social background, culture etc.) can influence the\nperspectives of annotators on a certain phenomenon. We believe that by relying\non this information, we can divide the annotators into groups sharing similar\nperspectives. We can create separate gold standards, one for each group, to\ntrain state-of-the-art deep learning models. We can employ an ensemble approach\nto combine the perspective-aware classifiers from different groups to an\ninclusive model. We also propose a novel resource, a multi-perspective English\nlanguage dataset annotated according to different sub-categories relevant for\ncharacterising online abuse: hate speech, aggressiveness, offensiveness and\nstereotype. By training state-of-the-art deep learning models on this novel\nresource, we show how our approach improves the prediction performance of a\nstate-of-the-art supervised classifier.",
    "published_date": "2021-06-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.15896v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.15767v1",
    "title": "Unaware Fairness: Hierarchical Random Forest for Protected Classes",
    "authors": [
      "Xian Li"
    ],
    "author_ids": [],
    "abstract": "Procedural fairness has been a public concern, which leads to controversy\nwhen making decisions with respect to protected classes, such as race, social\nstatus, and disability. Some protected classes can be inferred according to\nsome safe proxies like surname and geolocation for the race. Hence, implicitly\nutilizing the predicted protected classes based on the related proxies when\nmaking decisions is an efficient approach to circumvent this issue and seek\njust decisions. In this article, we propose a hierarchical random forest model\nfor prediction without explicitly involving protected classes. Simulation\nexperiments are conducted to show the performance of the hierarchical random\nforest model. An example is analyzed from Boston police interview records to\nillustrate the usefulness of the proposed model.",
    "published_date": "2021-06-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.15767v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.01277v1",
    "title": "Non-Comparative Fairness for Human-Auditing and Its Relation to Traditional Fairness Notions",
    "authors": [
      "Mukund Telukunta",
      "Venkata Sriram Siddhardh Nadendla"
    ],
    "author_ids": [],
    "abstract": "Bias evaluation in machine-learning based services (MLS) based on traditional\nalgorithmic fairness notions that rely on comparative principles is practically\ndifficult, making it necessary to rely on human auditor feedback. However, in\nspite of taking rigorous training on various comparative fairness notions,\nhuman auditors are known to disagree on various aspects of fairness notions in\npractice, making it difficult to collect reliable feedback. This paper offers a\nparadigm shift to the domain of algorithmic fairness via proposing a new\nfairness notion based on the principle of non-comparative justice. In contrary\nto traditional fairness notions where the outcomes of two individuals/groups\nare compared, our proposed notion compares the MLS' outcome with a desired\noutcome for each input. This desired outcome naturally describes a human\nauditor's expectation, and can be easily used to evaluate MLS on crowd-auditing\nplatforms. We show that any MLS can be deemed fair from the perspective of\ncomparative fairness (be it in terms of individual fairness, statistical\nparity, equal opportunity or calibration) if it is non-comparatively fair with\nrespect to a fair auditor. We also show that the converse holds true in the\ncontext of individual fairness. Given that such an evaluation relies on the\ntrustworthiness of the auditor, we also present an approach to identify fair\nand reliable auditors by estimating their biases with respect to a given set of\nsensitive attributes, as well as quantify the uncertainty in the estimation of\nbiases within a given MLS. Furthermore, all of the above results are also\nvalidated on COMPAS, German credit and Adult Census Income datasets.",
    "published_date": "2021-06-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.01277v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.15535v2",
    "title": "Subgroup Generalization and Fairness of Graph Neural Networks",
    "authors": [
      "Jiaqi Ma",
      "Junwei Deng",
      "Qiaozhu Mei"
    ],
    "author_ids": [],
    "abstract": "Despite enormous successful applications of graph neural networks (GNNs),\ntheoretical understanding of their generalization ability, especially for\nnode-level tasks where data are not independent and identically-distributed\n(IID), has been sparse. The theoretical investigation of the generalization\nperformance is beneficial for understanding fundamental issues (such as\nfairness) of GNN models and designing better learning methods. In this paper,\nwe present a novel PAC-Bayesian analysis for GNNs under a non-IID\nsemi-supervised learning setup. Moreover, we analyze the generalization\nperformances on different subgroups of unlabeled nodes, which allows us to\nfurther study an accuracy-(dis)parity-style (un)fairness of GNNs from a\ntheoretical perspective. Under reasonable assumptions, we demonstrate that the\ndistance between a test subgroup and the training set can be a key factor\naffecting the GNN performance on that subgroup, which calls special attention\nto the training node selection for fair learning. Experiments across multiple\nGNN models and datasets support our theoretical results.",
    "published_date": "2021-06-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.15535v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.15454v1",
    "title": "Valid inequalities and a branch-and-cut algorithm for the routing and spectrum allocation problem",
    "authors": [
      "Marcelo Bianchetti",
      "Javier Marenco"
    ],
    "author_ids": [],
    "abstract": "One of the most promising solutions to deal with huge data traffic demands in\nlarge communication networks is given by flexible optical networking, in\nparticular the flexible grid (flexgrid) technology specified in the ITU-T\nstandard G.694.1. In this specification, the frequency spectrum of an optical\nfiber link is divided into narrow frequency slots. Any sequence of consecutive\nslots can be used as a simple channel, and such a channel can be switched in\nthe network nodes to create a lightpath. In this kind of networks, the problem\nof establishing lightpaths for a set of end-to-end demands that compete for\nspectrum resources is called the routing and spectrum allocation problem (RSA).\nDue to its relevance, RSA has been intensively studied in the last years. It\nhas been shown to be NP-hard and different solution approaches have been\nproposed for this problem. In this paper we present several families of valid\ninequalities, valid equations, and optimality cuts for a natural integer\nprogramming formulation of RSA and, based on these results, we develop a\nbranch-and-cut algorithm for this problem. Our computational experiments\nsuggest that such an approach is effective at tackling this problem.",
    "published_date": "2021-06-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DM",
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.15454v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.15355v1",
    "title": "On the Interaction of Belief Bias and Explanations",
    "authors": [
      "Ana Valeria Gonzalez",
      "Anna Rogers",
      "Anders Søgaard"
    ],
    "author_ids": [],
    "abstract": "A myriad of explainability methods have been proposed in recent years, but\nthere is little consensus on how to evaluate them. While automatic metrics\nallow for quick benchmarking, it isn't clear how such metrics reflect human\ninteraction with explanations. Human evaluation is of paramount importance, but\nprevious protocols fail to account for belief biases affecting human\nperformance, which may lead to misleading conclusions. We provide an overview\nof belief bias, its role in human evaluation, and ideas for NLP practitioners\non how to account for it. For two experimental paradigms, we present a case\nstudy of gradient-based explainability introducing simple ways to account for\nhumans' prior beliefs: models of varying quality and adversarial examples. We\nshow that conclusions about the highest performing methods change when\nintroducing such controls, pointing to the importance of accounting for belief\nbias in evaluation.",
    "published_date": "2021-06-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.15355v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.15299v2",
    "title": "Cells are Actors: Social Network Analysis with Classical ML for SOTA Histology Image Classification",
    "authors": [
      "Neda Zamanitajeddin",
      "Mostafa Jahanifar",
      "Nasir Rajpoot"
    ],
    "author_ids": [],
    "abstract": "Digitization of histology images and the advent of new computational methods,\nlike deep learning, have helped the automatic grading of colorectal\nadenocarcinoma cancer (CRA). Present automated CRA grading methods, however,\nusually use tiny image patches and thus fail to integrate the entire tissue\nmicro-architecture for grading purposes. To tackle these challenges, we propose\nto use a statistical network analysis method to describe the complex structure\nof the tissue micro-environment by modelling nuclei and their connections as a\nnetwork. We show that by analyzing only the interactions between the cells in a\nnetwork, we can extract highly discriminative statistical features for CRA\ngrading. Unlike other deep learning or convolutional graph-based approaches,\nour method is highly scalable (can be used for cell networks consist of\nmillions of nodes), completely explainable, and computationally inexpensive. We\ncreate cell networks on a broad CRC histology image dataset, experiment with\nour method, and report state-of-the-art performance for the prediction of\nthree-class CRA grading.",
    "published_date": "2021-06-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.15299v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.15252v1",
    "title": "AutoNovel: Automatically Discovering and Learning Novel Visual Categories",
    "authors": [
      "Kai Han",
      "Sylvestre-Alvise Rebuffi",
      "Sébastien Ehrhardt",
      "Andrea Vedaldi",
      "Andrew Zisserman"
    ],
    "author_ids": [],
    "abstract": "We tackle the problem of discovering novel classes in an image collection\ngiven labelled examples of other classes. We present a new approach called\nAutoNovel to address this problem by combining three ideas: (1) we suggest that\nthe common approach of bootstrapping an image representation using the labelled\ndata only introduces an unwanted bias, and that this can be avoided by using\nself-supervised learning to train the representation from scratch on the union\nof labelled and unlabelled data; (2) we use ranking statistics to transfer the\nmodel's knowledge of the labelled classes to the problem of clustering the\nunlabelled images; and, (3) we train the data representation by optimizing a\njoint objective function on the labelled and unlabelled subsets of the data,\nimproving both the supervised classification of the labelled data, and the\nclustering of the unlabelled data. Moreover, we propose a method to estimate\nthe number of classes for the case where the number of new categories is not\nknown a priori. We evaluate AutoNovel on standard classification benchmarks and\nsubstantially outperform current methods for novel category discovery. In\naddition, we also show that AutoNovel can be used for fully unsupervised image\nclustering, achieving promising results.",
    "published_date": "2021-06-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.15252v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.15167v1",
    "title": "Learning from Miscellaneous Other-Class Words for Few-shot Named Entity Recognition",
    "authors": [
      "Meihan Tong",
      "Shuai Wang",
      "Bin Xu",
      "Yixin Cao",
      "Minghui Liu",
      "Lei Hou",
      "Juanzi Li"
    ],
    "author_ids": [],
    "abstract": "Few-shot Named Entity Recognition (NER) exploits only a handful of\nannotations to identify and classify named entity mentions. Prototypical\nnetwork shows superior performance on few-shot NER. However, existing\nprototypical methods fail to differentiate rich semantics in other-class words,\nwhich will aggravate overfitting under few shot scenario. To address the issue,\nwe propose a novel model, Mining Undefined Classes from Other-class (MUCO),\nthat can automatically induce different undefined classes from the other class\nto improve few-shot NER. With these extra-labeled undefined classes, our method\nwill improve the discriminative ability of NER classifier and enhance the\nunderstanding of predefined classes with stand-by semantic knowledge.\nExperimental results demonstrate that our model outperforms five\nstate-of-the-art models in both 1-shot and 5-shots settings on four NER\nbenchmarks. We will release the code upon acceptance. The source code is\nreleased on https: //github.com/shuaiwa16/OtherClassNER.git.",
    "published_date": "2021-06-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.15167v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.15103v1",
    "title": "Sexism in the Judiciary",
    "authors": [
      "Noa Baker Gillis"
    ],
    "author_ids": [],
    "abstract": "We analyze 6.7 million case law documents to determine the presence of gender\nbias within our judicial system. We find that current bias detectino methods in\nNLP are insufficient to determine gender bias in our case law database and\npropose an alternative approach. We show that existing algorithms' inconsistent\nresults are consequences of prior research's definition of biases themselves.\nBias detection algorithms rely on groups of words to represent bias (e.g.,\n'salary,' 'job,' and 'boss' to represent employment as a potentially biased\ntheme against women in text). However, the methods to build these groups of\nwords have several weaknesses, primarily that the word lists are based on the\nresearchers' own intuitions. We suggest two new methods of automating the\ncreation of word lists to represent biases. We find that our methods outperform\ncurrent NLP bias detection methods. Our research improves the capabilities of\nNLP technology to detect bias and highlights gender biases present in\ninfluential case law. In order test our NLP bias detection method's\nperformance, we regress our results of bias in case law against U.S census data\nof women's participation in the workforce in the last 100 years.",
    "published_date": "2021-06-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.15103v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.14975v2",
    "title": "Design Considerations for Data Daemons: Co-creating Design Futures to Explore Ethical Personal Data Management",
    "authors": [
      "Wiebke Toussaint",
      "Alejandra Gomez Ortega",
      "Jered Vroon",
      "Julian Harty",
      "Gürkan Solmaz",
      "Olya Kudina",
      "Ella Peltonen",
      "Jacky Bourgeois",
      "Aaron Yi Ding"
    ],
    "author_ids": [],
    "abstract": "Mobile applications and online service providers track our virtual and\nphysical behaviour more actively and with a broader scope than ever before.\nThis has given rise to growing concerns about ethical personal data management.\nEven though regulation and awareness around data ethics are increasing,\nend-users are seldom engaged when defining and designing what a future with\nethical personal data management should look like. We explore a participatory\nprocess that uses design futures, the Future workshop method and design\nfictions to envision ethical personal data management with end-users and\ndesigners. To engage participants effectively, we needed to bridge their\ndifferential expertise and make the abstract concepts of data and ethics\ntangible. By concretely presenting personal data management and control as\nfictitious entities called Data Daemons, we created a shared understanding of\nthese abstract concepts, and empowered non-expert end-users and designers to\nbecome actively engaged in the design process.",
    "published_date": "2021-06-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.14975v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.14861v2",
    "title": "Doing good by fighting fraud: Ethical anti-fraud systems for mobile payments",
    "authors": [
      "Zainul Abi Din",
      "Hari Venugopalan",
      "Henry Lin",
      "Adam Wushensky",
      "Steven Liu",
      "Samuel T. King"
    ],
    "author_ids": [],
    "abstract": "App builders commonly use security challenges, a form of step-up\nauthentication, to add security to their apps. However, the ethical\nimplications of this type of architecture has not been studied previously. In\nthis paper, we present a large-scale measurement study of running an existing\nanti-fraud security challenge, Boxer, in real apps running on mobile devices.\nWe find that although Boxer does work well overall, it is unable to scan\neffectively on devices that run its machine learning models at less than one\nframe per second (FPS), blocking users who use inexpensive devices. With the\ninsights from our study, we design Daredevil, anew anti-fraud system for\nscanning payment cards that work swell across the broad range of performance\ncharacteristics and hardware configurations found on modern mobile devices.\nDaredevil reduces the number of devices that run at less than one FPS by an\norder of magnitude compared to Boxer, providing a more equitable system for\nfighting fraud. In total, we collect data from 5,085,444 real devices spread\nacross 496 real apps running production software and interacting with real\nusers.",
    "published_date": "2021-06-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.14861v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.14829v1",
    "title": "Dataset Bias Mitigation Through Analysis of CNN Training Scores",
    "authors": [
      "Ekberjan Derman"
    ],
    "author_ids": [],
    "abstract": "Training datasets are crucial for convolutional neural network-based\nalgorithms, which directly impact their overall performance. As such, using a\nwell-structured dataset that has minimum level of bias is always desirable. In\nthis paper, we proposed a novel, domain-independent approach, called\nscore-based resampling (SBR), to locate the under-represented samples of the\noriginal training dataset based on the model prediction scores obtained with\nthat training set. In our method, once trained, we use the same CNN model to\ninfer on its own training samples, obtain prediction scores, and based on the\ndistance between predicted and ground-truth, we identify samples that are far\naway from their ground-truth and augment them in the original training set. The\ntemperature term of the Sigmoid function is decreased to better differentiate\nscores. For experimental evaluation, we selected one Kaggle dataset for gender\nclassification. We first used a CNN-based classifier with relatively standard\nstructure, trained on the training images, and evaluated on the provided\nvalidation samples of the original dataset. Then, we assessed it on a totally\nnew test dataset consisting of light male, light female, dark male, and dark\nfemale groups. The obtained accuracies varied, revealing the existence of\ncategorical bias against certain groups in the original dataset. Subsequently,\nwe trained the model after resampling based on our proposed approach. We\ncompared our method with a previously proposed variational autoencoder (VAE)\nbased algorithm. The obtained results confirmed the validity of our proposed\nmethod regrading identifying under-represented samples among original dataset\nto decrease categorical bias of classifying certain groups. Although tested for\ngender classification, the proposed algorithm can be used for investigating\ndataset structure of any CNN-based tasks.",
    "published_date": "2021-06-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.14829v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.14749v1",
    "title": "A Theory-Driven Self-Labeling Refinement Method for Contrastive Representation Learning",
    "authors": [
      "Pan Zhou",
      "Caiming Xiong",
      "Xiao-Tong Yuan",
      "Steven Hoi"
    ],
    "author_ids": [],
    "abstract": "For an image query, unsupervised contrastive learning labels crops of the\nsame image as positives, and other image crops as negatives. Although\nintuitive, such a native label assignment strategy cannot reveal the underlying\nsemantic similarity between a query and its positives and negatives, and\nimpairs performance, since some negatives are semantically similar to the query\nor even share the same semantic class as the query. In this work, we first\nprove that for contrastive learning, inaccurate label assignment heavily\nimpairs its generalization for semantic instance discrimination, while accurate\nlabels benefit its generalization. Inspired by this theory, we propose a novel\nself-labeling refinement approach for contrastive learning. It improves the\nlabel quality via two complementary modules: (i) self-labeling refinery (SLR)\nto generate accurate labels and (ii) momentum mixup (MM) to enhance similarity\nbetween query and its positive. SLR uses a positive of a query to estimate\nsemantic similarity between a query and its positive and negatives, and\ncombines estimated similarity with vanilla label assignment in contrastive\nlearning to iteratively generate more accurate and informative soft labels. We\ntheoretically show that our SLR can exactly recover the true semantic labels of\nlabel-corrupted data, and supervises networks to achieve zero prediction error\non classification tasks. MM randomly combines queries and positives to increase\nsemantic similarity between the generated virtual queries and their positives\nso as to improves label accuracy. Experimental results on CIFAR10, ImageNet,\nVOC and COCO show the effectiveness of our method. PyTorch code and model will\nbe released online.",
    "published_date": "2021-06-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.14749v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.14574v1",
    "title": "Quantifying Social Biases in NLP: A Generalization and Empirical Comparison of Extrinsic Fairness Metrics",
    "authors": [
      "Paula Czarnowska",
      "Yogarshi Vyas",
      "Kashif Shah"
    ],
    "author_ids": [],
    "abstract": "Measuring bias is key for better understanding and addressing unfairness in\nNLP/ML models. This is often done via fairness metrics which quantify the\ndifferences in a model's behaviour across a range of demographic groups. In\nthis work, we shed more light on the differences and similarities between the\nfairness metrics used in NLP. First, we unify a broad range of existing metrics\nunder three generalized fairness metrics, revealing the connections between\nthem. Next, we carry out an extensive empirical comparison of existing metrics\nand demonstrate that the observed differences in bias measurement can be\nsystematically explained via differences in parameter choices for our\ngeneralized metrics.",
    "published_date": "2021-06-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.14574v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.14387v2",
    "title": "Political Ideology and Polarization of Policy Positions: A Multi-dimensional Approach",
    "authors": [
      "Barea Sinno",
      "Bernardo Oviedo",
      "Katherine Atwell",
      "Malihe Alikhani",
      "Junyi Jessy Li"
    ],
    "author_ids": [],
    "abstract": "Analyzing ideology and polarization is of critical importance in advancing\nour grasp of modern politics. Recent research has made great strides towards\nunderstanding the ideological bias (i.e., stance) of news media along the\nleft-right spectrum. In this work, we instead take a novel and more nuanced\napproach for the study of ideology based on its left or right positions on the\nissue being discussed. Aligned with the theoretical accounts in political\nscience, we treat ideology as a multi-dimensional construct, and introduce the\nfirst diachronic dataset of news articles whose ideological positions are\nannotated by trained political scientists and linguists at the paragraph level.\nWe showcase that, by controlling for the author's stance, our method allows for\nthe quantitative and temporal measurement and analysis of polarization as a\nmultidimensional ideological distance. We further present baseline models for\nideology prediction, outlining a challenging task distinct from stance\ndetection.",
    "published_date": "2021-06-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.14387v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.14384v2",
    "title": "Towards Model-informed Precision Dosing with Expert-in-the-loop Machine Learning",
    "authors": [
      "Yihuang Kang",
      "Yi-Wen Chiu",
      "Ming-Yen Lin",
      "Fang-yi Su",
      "Sheng-Tai Huang"
    ],
    "author_ids": [],
    "abstract": "Machine Learning (ML) and its applications have been transforming our lives\nbut it is also creating issues related to the development of fair, accountable,\ntransparent, and ethical Artificial Intelligence. As the ML models are not\nfully comprehensible yet, it is obvious that we still need humans to be part of\nalgorithmic decision-making processes. In this paper, we consider a ML\nframework that may accelerate model learning and improve its interpretability\nby incorporating human experts into the model learning loop. We propose a novel\nhuman-in-the-loop ML framework aimed at dealing with learning problems that the\ncost of data annotation is high and the lack of appropriate data to model the\nassociation between the target tasks and the input features. With an\napplication to precision dosing, our experimental results show that the\napproach can learn interpretable rules from data and may potentially lower\nexperts' workload by replacing data annotation with rule representation\nediting. The approach may also help remove algorithmic bias by introducing\nexperts' feedback into the iterative model learning process.",
    "published_date": "2021-06-28T00:00:00",
    "year": 2021,
    "categories": [
      "stat.AP",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.14384v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.14365v1",
    "title": "Integrating topic modeling and word embedding to characterize violent deaths",
    "authors": [
      "Alina Arseniev-Koehler",
      "Susan D. Cochran",
      "Vickie M. Mays",
      "Kai-Wei Chang",
      "Jacob Gates Foster"
    ],
    "author_ids": [],
    "abstract": "There is an escalating need for methods to identify latent patterns in text\ndata from many domains. We introduce a new method to identify topics in a\ncorpus and represent documents as topic sequences. Discourse Atom Topic\nModeling draws on advances in theoretical machine learning to integrate topic\nmodeling and word embedding, capitalizing on the distinct capabilities of each.\nWe first identify a set of vectors (\"discourse atoms\") that provide a sparse\nrepresentation of an embedding space. Atom vectors can be interpreted as latent\ntopics: Through a generative model, atoms map onto distributions over words;\none can also infer the topic that generated a sequence of words. We illustrate\nour method with a prominent example of underutilized text: the U.S. National\nViolent Death Reporting System (NVDRS). The NVDRS summarizes violent death\nincidents with structured variables and unstructured narratives. We identify\n225 latent topics in the narratives (e.g., preparation for death and physical\naggression); many of these topics are not captured by existing structured\nvariables. Motivated by known patterns in suicide and homicide by gender, and\nrecent research on gender biases in semantic space, we identify the gender bias\nof our topics (e.g., a topic about pain medication is feminine). We then\ncompare the gender bias of topics to their prevalence in narratives of female\nversus male victims. Results provide a detailed quantitative picture of\nreporting about lethal violence and its gendered nature. Our method offers a\nflexible and broadly applicable approach to model topics in text data.",
    "published_date": "2021-06-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.14365v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.14072v1",
    "title": "Detecting race and gender bias in visual representation of AI on web search engines",
    "authors": [
      "Mykola Makhortykh",
      "Aleksandra Urman",
      "Roberto Ulloa"
    ],
    "author_ids": [],
    "abstract": "Web search engines influence perception of social reality by filtering and\nranking information. However, their outputs are often subjected to bias that\ncan lead to skewed representation of subjects such as professional occupations\nor gender. In our paper, we use a mixed-method approach to investigate presence\nof race and gender bias in representation of artificial intelligence (AI) in\nimage search results coming from six different search engines. Our findings\nshow that search engines prioritize anthropomorphic images of AI that portray\nit as white, whereas non-white images of AI are present only in non-Western\nsearch engines. By contrast, gender representation of AI is more diverse and\nless skewed towards a specific gender that can be attributed to higher\nawareness about gender bias in search outputs. Our observations indicate both\nthe the need and the possibility for addressing bias in representation of\nsocietally relevant subjects, such as technological innovation, and emphasize\nthe importance of designing new approaches for detecting bias in information\nretrieval systems.",
    "published_date": "2021-06-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "H.3.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.14072v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.07341v2",
    "title": "Utilizing a digital swarm intelligence platform to improve consensus among radiologists and exploring its applications",
    "authors": [
      "Rutwik Shah",
      "Bruno Astuto",
      "Tyler Gleason",
      "Will Fletcher",
      "Justin Banaga",
      "Kevin Sweetwood",
      "Allen Ye",
      "Rina Patel",
      "Kevin McGill",
      "Thomas Link",
      "Jason Crane",
      "Valentina Pedoia",
      "Sharmila Majumdar"
    ],
    "author_ids": [],
    "abstract": "Radiologists today play a key role in making diagnostic decisions and\nlabeling images for training A.I. algorithms. Low inter-reader reliability\n(IRR) can be seen between experts when interpreting challenging cases. While\nteams-based decisions are known to outperform individual decisions,\ninter-personal biases often creep up in group interactions which limit\nnon-dominant participants from expressing true opinions. To overcome the dual\nproblems of low consensus and inter-personal bias, we explored a solution\nmodeled on biological swarms of bees. Two separate cohorts; three radiologists\nand five radiology residents collaborated on a digital swarm platform in real\ntime and in a blinded fashion, grading meniscal lesions on knee MR exams. These\nconsensus votes were benchmarked against clinical (arthroscopy) and\nradiological (senior-most radiologist) observations. The IRR of the consensus\nvotes was compared to the IRR of the majority and most confident votes of the\ntwo cohorts.The radiologist cohort saw an improvement of 23% in IRR of swarm\nvotes over majority vote. Similar improvement of 23% in IRR in 3-resident swarm\nvotes over majority vote, was observed. The 5-resident swarm had an even higher\nimprovement of 32% in IRR over majority vote. Swarm consensus votes also\nimproved specificity by up to 50%. The swarm consensus votes outperformed\nindividual and majority vote decisions in both the radiologists and resident\ncohorts. The 5-resident swarm had higher IRR than 3-resident swarm indicating\npositive effect of increased swarm size. The attending and resident swarms also\noutperformed predictions from a state-of-the-art A.I. algorithm. Utilizing a\ndigital swarm platform improved agreement and allows participants to express\njudgement free intent, resulting in superior clinical performance and robust\nA.I. training labels.",
    "published_date": "2021-06-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.DC",
      "cs.LG",
      "cs.NE",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.07341v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.13901v1",
    "title": "Building Bridges: Generative Artworks to Explore AI Ethics",
    "authors": [
      "Ramya Srinivasan",
      "Devi Parikh"
    ],
    "author_ids": [],
    "abstract": "In recent years, there has been an increased emphasis on understanding and\nmitigating adverse impacts of artificial intelligence (AI) technologies on\nsociety. Across academia, industry, and government bodies, a variety of\nendeavours are being pursued towards enhancing AI ethics. A significant\nchallenge in the design of ethical AI systems is that there are multiple\nstakeholders in the AI pipeline, each with their own set of constraints and\ninterests. These different perspectives are often not understood, due in part\nto communication gaps.For example, AI researchers who design and develop AI\nmodels are not necessarily aware of the instability induced in consumers' lives\nby the compounded effects of AI decisions. Educating different stakeholders\nabout their roles and responsibilities in the broader context becomes\nnecessary. In this position paper, we outline some potential ways in which\ngenerative artworks can play this role by serving as accessible and powerful\neducational tools for surfacing different perspectives. We hope to spark\ninterdisciplinary discussions about computational creativity broadly as a tool\nfor enhancing AI ethics.",
    "published_date": "2021-06-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.13901v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.13786v2",
    "title": "Data efficiency in graph networks through equivariance",
    "authors": [
      "Francesco Farina",
      "Emma Slade"
    ],
    "author_ids": [],
    "abstract": "We introduce a novel architecture for graph networks which is equivariant to\nany transformation in the coordinate embeddings that preserves the distance\nbetween neighbouring nodes. In particular, it is equivariant to the Euclidean\nand conformal orthogonal groups in $n$-dimensions. Thanks to its equivariance\nproperties, the proposed model is extremely more data efficient with respect to\nclassical graph architectures and also intrinsically equipped with a better\ninductive bias. We show that, learning on a minimal amount of data, the\narchitecture we propose can perfectly generalise to unseen data in a synthetic\nproblem, while much more training data are required from a standard model to\nreach comparable performance.",
    "published_date": "2021-06-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.13786v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.13746v2",
    "title": "On Incorporating Inductive Biases into VAEs",
    "authors": [
      "Ning Miao",
      "Emile Mathieu",
      "N. Siddharth",
      "Yee Whye Teh",
      "Tom Rainforth"
    ],
    "author_ids": [],
    "abstract": "We explain why directly changing the prior can be a surprisingly ineffective\nmechanism for incorporating inductive biases into VAEs, and introduce a simple\nand effective alternative approach: Intermediary Latent Space VAEs(InteL-VAEs).\nInteL-VAEs use an intermediary set of latent variables to control the\nstochasticity of the encoding process, before mapping these in turn to the\nlatent representation using a parametric function that encapsulates our desired\ninductive bias(es). This allows us to impose properties like sparsity or\nclustering on learned representations, and incorporate human knowledge into the\ngenerative model. Whereas changing the prior only indirectly encourages\nbehavior through regularizing the encoder, InteL-VAEs are able to directly\nenforce desired characteristics. Moreover, they bypass the computation and\nencoder design issues caused by non-Gaussian priors, while allowing for\nadditional flexibility through training of the parametric mapping function. We\nshow that these advantages, in turn, lead to both better generative models and\nbetter representations being learned.",
    "published_date": "2021-06-25T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.13746v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.13665v1",
    "title": "On Some Quasi-Variational Inequalities and Other Problems with Moving Sets",
    "authors": [
      "José-Luis Menaldi",
      "Carlos N. Rautenberg"
    ],
    "author_ids": [],
    "abstract": "Since its introduction over 50 years ago, the concept of Mosco convergence\nhas permeated through diverse areas of mathematics and applied sciences. These\ninclude applied analysis, the theory of partial differential equations,\nnumerical analysis, and infinite dimensional constrained optimization, among\nothers. In this paper we explore some of the consequences of Mosco convergence\non applied problems that involve moving sets, with some historical accounts,\nand modern trends and features. In particular, we focus on connections with\ndensity of convex intersections, finite element approximations,\nquasi-variational inequalities, and impulse problems.",
    "published_date": "2021-06-25T00:00:00",
    "year": 2021,
    "categories": [
      "math.OC",
      "cs.NA",
      "math.AP",
      "math.NA",
      "35J86, 35J60, 35R35, 65K10, 93E20"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.13665v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.13624v2",
    "title": "Chebyshev-Cantelli PAC-Bayes-Bennett Inequality for the Weighted Majority Vote",
    "authors": [
      "Yi-Shan Wu",
      "Andrés R. Masegosa",
      "Stephan S. Lorenzen",
      "Christian Igel",
      "Yevgeny Seldin"
    ],
    "author_ids": [],
    "abstract": "We present a new second-order oracle bound for the expected risk of a\nweighted majority vote. The bound is based on a novel parametric form of the\nChebyshev- Cantelli inequality (a.k.a. one-sided Chebyshev's), which is\namenable to efficient minimization. The new form resolves the optimization\nchallenge faced by prior oracle bounds based on the Chebyshev-Cantelli\ninequality, the C-bounds [Germain et al., 2015], and, at the same time, it\nimproves on the oracle bound based on second order Markov's inequality\nintroduced by Masegosa et al. [2020]. We also derive a new concentration of\nmeasure inequality, which we name PAC-Bayes-Bennett, since it combines\nPAC-Bayesian bounding with Bennett's inequality. We use it for empirical\nestimation of the oracle bound. The PAC-Bayes-Bennett inequality improves on\nthe PAC-Bayes-Bernstein inequality of Seldin et al. [2012]. We provide an\nempirical evaluation demonstrating that the new bounds can improve on the work\nof Masegosa et al. [2020]. Both the parametric form of the Chebyshev-Cantelli\ninequality and the PAC-Bayes-Bennett inequality may be of independent interest\nfor the study of concentration of measure in other domains.",
    "published_date": "2021-06-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.13624v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.13539v2",
    "title": "Dealing with Expert Bias in Collective Decision-Making",
    "authors": [
      "Axel Abels",
      "Tom Lenaerts",
      "Vito Trianni",
      "Ann Nowé"
    ],
    "author_ids": [],
    "abstract": "Quite some real-world problems can be formulated as decision-making problems\nwherein one must repeatedly make an appropriate choice from a set of\nalternatives. Multiple expert judgements, whether human or artificial, can help\nin taking correct decisions, especially when exploration of alternative\nsolutions is costly. As expert opinions might deviate, the problem of finding\nthe right alternative can be approached as a collective decision making problem\n(CDM) via aggregation of independent judgements. Current state-of-the-art\napproaches focus on efficiently finding the optimal expert, and thus perform\npoorly if all experts are not qualified or if they are overly biased, thereby\npotentially derailing the decision-making process. In this paper, we propose a\nnew algorithmic approach based on contextual multi-armed bandit problems (CMAB)\nto identify and counteract such biased expertise. We explore homogeneous,\nheterogeneous and polarised expert groups and show that this approach is able\nto effectively exploit the collective expertise, outperforming state-of-the-art\nmethods, especially when the quality of the provided expertise degrades. Our\nnovel CMAB-inspired approach achieves a higher final performance and does so\nwhile converging more rapidly than previous adaptive algorithms.",
    "published_date": "2021-06-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.13539v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.06071v2",
    "title": "aiSTROM -- A roadmap for developing a successful AI strategy",
    "authors": [
      "Dorien Herremans"
    ],
    "author_ids": [],
    "abstract": "A total of 34% of AI research and development projects fails or are\nabandoned, according to a recent survey by Rackspace Technology of 1,870\ncompanies. We propose a new strategic framework, aiSTROM, that empowers\nmanagers to create a successful AI strategy based on a thorough literature\nreview. This provides a unique and integrated approach that guides managers and\nlead developers through the various challenges in the implementation process.\nIn the aiSTROM framework, we start by identifying the top n potential projects\n(typically 3-5). For each of those, seven areas of focus are thoroughly\nanalysed. These areas include creating a data strategy that takes into account\nunique cross-departmental machine learning data requirements, security, and\nlegal requirements. aiSTROM then guides managers to think about how to put\ntogether an interdisciplinary artificial intelligence (AI) implementation team\ngiven the scarcity of AI talent. Once an AI team strategy has been established,\nit needs to be positioned within the organization, either cross-departmental or\nas a separate division. Other considerations include AI as a service (AIaas),\nor outsourcing development. Looking at new technologies, we have to consider\nchallenges such as bias, legality of black-box-models, and keeping humans in\nthe loop. Next, like any project, we need value-based key performance\nindicators (KPIs) to track and validate the progress. Depending on the\ncompany's risk-strategy, a SWOT analysis (strengths, weaknesses, opportunities,\nand threats) can help further classify the shortlisted projects. Finally, we\nshould make sure that our strategy includes continuous education of employees\nto enable a culture of adoption. This unique and comprehensive framework offers\na valuable, literature supported, tool for managers and lead developers.",
    "published_date": "2021-06-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "68Txx, 97Pxx",
      "K.5; K.6; C.5; D.m; H.2; K.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.06071v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.13456v2",
    "title": "Interpreting Criminal Charge Prediction and Its Algorithmic Bias via Quantum-Inspired Complex Valued Networks",
    "authors": [
      "Abdul Rafae Khan",
      "Jia Xu",
      "Peter Varsanyi",
      "Rachit Pabreja"
    ],
    "author_ids": [],
    "abstract": "While predictive policing has become increasingly common in assisting with\ndecisions in the criminal justice system, the use of these results is still\ncontroversial. Some software based on deep learning lacks accuracy (e.g., in\nF-1), and importantly many decision processes are not transparent, causing\ndoubt about decision bias, such as perceived racial and age disparities. This\npaper addresses bias issues with post-hoc explanations to provide a trustable\nprediction of whether a person will receive future criminal charges given one's\nprevious criminal records by learning temporal behavior patterns over twenty\nyears. Bi-LSTM relieves the vanishing gradient problem, attentional mechanisms\nallow learning and interpretation of feature importance, and complex-valued\nnetworks inspired quantum physics to facilitate a certain level of transparency\nin modeling the decision process. Our approach shows a consistent and reliable\nprediction precision and recall on a real-life dataset. Our analysis of the\nimportance of each input feature shows the critical causal impact on\ndecision-making, suggesting that criminal histories are statistically\nsignificant factors, while identifiers, such as race and age, are not. Finally,\nour algorithm indicates that a suspect tends to rather than suddenly increase\ncrime severity level over time gradually.",
    "published_date": "2021-06-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.13456v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.13455v2",
    "title": "Fairness Deconstructed: A Sociotechnical View of 'Fair' Algorithms in Criminal Justice",
    "authors": [
      "Rajiv Movva"
    ],
    "author_ids": [],
    "abstract": "Early studies of risk assessment algorithms used in criminal justice revealed\nwidespread racial biases. In response, machine learning researchers have\ndeveloped methods for fairness, many of which rely on equalizing empirical\nmetrics across protected attributes. Here, I recall sociotechnical perspectives\nto delineate the significant gap between fairness in theory and practice,\nfocusing on criminal justice. I (1) illustrate how social context can undermine\nanalyses that are restricted to an AI system's outputs, and (2) argue that much\nof the fair ML literature fails to account for epistemological issues with\nunderlying crime data. Instead of building AI that reifies power imbalances,\nlike risk assessment algorithms, I ask whether data science can be used to\nunderstand the root causes of structural marginalization.",
    "published_date": "2021-06-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "K.4.2; K.4.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.13455v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.13386v1",
    "title": "Balancing Accuracy and Fairness for Interactive Recommendation with Reinforcement Learning",
    "authors": [
      "Weiwen Liu",
      "Feng Liu",
      "Ruiming Tang",
      "Ben Liao",
      "Guangyong Chen",
      "Pheng Ann Heng"
    ],
    "author_ids": [],
    "abstract": "Fairness in recommendation has attracted increasing attention due to bias and\ndiscrimination possibly caused by traditional recommenders. In Interactive\nRecommender Systems (IRS), user preferences and the system's fairness status\nare constantly changing over time. Existing fairness-aware recommenders mainly\nconsider fairness in static settings. Directly applying existing methods to IRS\nwill result in poor recommendation. To resolve this problem, we propose a\nreinforcement learning based framework, FairRec, to dynamically maintain a\nlong-term balance between accuracy and fairness in IRS. User preferences and\nthe system's fairness status are jointly compressed into the state\nrepresentation to generate recommendations. FairRec aims at maximizing our\ndesigned cumulative reward that combines accuracy and fairness. Extensive\nexperiments validate that FairRec can improve fairness, while preserving good\nrecommendation quality.",
    "published_date": "2021-06-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.13386v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.13382v1",
    "title": "A Source-Criticism Debiasing Method for GloVe Embeddings",
    "authors": [
      "Hope McGovern"
    ],
    "author_ids": [],
    "abstract": "It is well-documented that word embeddings trained on large public corpora\nconsistently exhibit known human social biases. Although many methods for\ndebiasing exist, almost all fixate on completely eliminating biased information\nfrom the embeddings and often diminish training set size in the process. In\nthis paper, we present a simple yet effective method for debiasing GloVe word\nembeddings (Pennington et al., 2014) which works by incorporating explicit\ninformation about training set bias rather than removing biased data outright.\nOur method runs quickly and efficiently with the help of a fast bias gradient\napproximation method from Brunet et al. (2019). As our approach is akin to the\nnotion of 'source criticism' in the humanities, we term our method\nSource-Critical GloVe (SC-GloVe). We show that SC-GloVe reduces the effect size\non Word Embedding Association Test (WEAT) sets without sacrificing training\ndata or TOP-1 performance.",
    "published_date": "2021-06-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.13382v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.13346v1",
    "title": "What will it take to generate fairness-preserving explanations?",
    "authors": [
      "Jessica Dai",
      "Sohini Upadhyay",
      "Stephen H. Bach",
      "Himabindu Lakkaraju"
    ],
    "author_ids": [],
    "abstract": "In situations where explanations of black-box models may be useful, the\nfairness of the black-box is also often a relevant concern. However, the link\nbetween the fairness of the black-box model and the behavior of explanations\nfor the black-box is unclear. We focus on explanations applied to tabular\ndatasets, suggesting that explanations do not necessarily preserve the fairness\nproperties of the black-box algorithm. In other words, explanation algorithms\ncan ignore or obscure critical relevant properties, creating incorrect or\nmisleading explanations. More broadly, we propose future research directions\nfor evaluating and generating explanations such that they are informative and\nrelevant from a fairness perspective.",
    "published_date": "2021-06-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.13346v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.13272v1",
    "title": "Generalized One-Class Learning Using Pairs of Complementary Classifiers",
    "authors": [
      "Anoop Cherian",
      "Jue Wang"
    ],
    "author_ids": [],
    "abstract": "One-class learning is the classic problem of fitting a model to the data for\nwhich annotations are available only for a single class. In this paper, we\nexplore novel objectives for one-class learning, which we collectively refer to\nas Generalized One-class Discriminative Subspaces (GODS). Our key idea is to\nlearn a pair of complementary classifiers to flexibly bound the one-class data\ndistribution, where the data belongs to the positive half-space of one of the\nclassifiers in the complementary pair and to the negative half-space of the\nother. To avoid redundancy while allowing non-linearity in the classifier\ndecision surfaces, we propose to design each classifier as an orthonormal frame\nand seek to learn these frames via jointly optimizing for two conflicting\nobjectives, namely: i) to minimize the distance between the two frames, and ii)\nto maximize the margin between the frames and the data. The learned orthonormal\nframes will thus characterize a piecewise linear decision surface that allows\nfor efficient inference, while our objectives seek to bound the data within a\nminimal volume that maximizes the decision margin, thereby robustly capturing\nthe data distribution. We explore several variants of our formulation under\ndifferent constraints on the constituent classifiers, including kernelized\nfeature maps. We demonstrate the empirical benefits of our approach via\nexperiments on data from several applications in computer vision, such as\nanomaly detection in video sequences, human poses, and human activities. We\nalso explore the generality and effectiveness of GODS for non-vision tasks via\nexperiments on several UCI datasets, demonstrating state-of-the-art results.",
    "published_date": "2021-06-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.13272v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.13271v1",
    "title": "On Fairness and Interpretability",
    "authors": [
      "Deepak P",
      "Sanil V",
      "Joemon M. Jose"
    ],
    "author_ids": [],
    "abstract": "Ethical AI spans a gamut of considerations. Among these, the most popular\nones, fairness and interpretability, have remained largely distinct in\ntechnical pursuits. We discuss and elucidate the differences between fairness\nand interpretability across a variety of dimensions. Further, we develop two\nprinciples-based frameworks towards developing ethical AI for the future that\nembrace aspects of both fairness and interpretability. First, interpretability\nfor fairness proposes instantiating interpretability within the realm of\nfairness to develop a new breed of ethical AI. Second, fairness and\ninterpretability initiates deliberations on bringing the best aspects of both\ntogether. We hope that these two frameworks will contribute to intensifying\nscholarly discussions on new frontiers of ethical AI that brings together\nfairness and interpretability.",
    "published_date": "2021-06-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.13271v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.13230v1",
    "title": "Video Swin Transformer",
    "authors": [
      "Ze Liu",
      "Jia Ning",
      "Yue Cao",
      "Yixuan Wei",
      "Zheng Zhang",
      "Stephen Lin",
      "Han Hu"
    ],
    "author_ids": [],
    "abstract": "The vision community is witnessing a modeling shift from CNNs to\nTransformers, where pure Transformer architectures have attained top accuracy\non the major video recognition benchmarks. These video models are all built on\nTransformer layers that globally connect patches across the spatial and\ntemporal dimensions. In this paper, we instead advocate an inductive bias of\nlocality in video Transformers, which leads to a better speed-accuracy\ntrade-off compared to previous approaches which compute self-attention globally\neven with spatial-temporal factorization. The locality of the proposed video\narchitecture is realized by adapting the Swin Transformer designed for the\nimage domain, while continuing to leverage the power of pre-trained image\nmodels. Our approach achieves state-of-the-art accuracy on a broad range of\nvideo recognition benchmarks, including on action recognition (84.9 top-1\naccuracy on Kinetics-400 and 86.1 top-1 accuracy on Kinetics-600 with ~20x less\npre-training data and ~3x smaller model size) and temporal modeling (69.6 top-1\naccuracy on Something-Something v2). The code and models will be made publicly\navailable at https://github.com/SwinTransformer/Video-Swin-Transformer.",
    "published_date": "2021-06-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.13230v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.13219v1",
    "title": "Towards Understanding and Mitigating Social Biases in Language Models",
    "authors": [
      "Paul Pu Liang",
      "Chiyu Wu",
      "Louis-Philippe Morency",
      "Ruslan Salakhutdinov"
    ],
    "author_ids": [],
    "abstract": "As machine learning methods are deployed in real-world settings such as\nhealthcare, legal systems, and social science, it is crucial to recognize how\nthey shape social biases and stereotypes in these sensitive decision-making\nprocesses. Among such real-world deployments are large-scale pretrained\nlanguage models (LMs) that can be potentially dangerous in manifesting\nundesirable representational biases - harmful biases resulting from\nstereotyping that propagate negative generalizations involving gender, race,\nreligion, and other social constructs. As a step towards improving the fairness\nof LMs, we carefully define several sources of representational biases before\nproposing new benchmarks and metrics to measure them. With these tools, we\npropose steps towards mitigating social biases during text generation. Our\nempirical results and human evaluation demonstrate effectiveness in mitigating\nbias while retaining crucial contextual information for high-fidelity text\ngeneration, thereby pushing forward the performance-fairness Pareto frontier.",
    "published_date": "2021-06-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.13219v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.13186v1",
    "title": "CCC/Code 8.7: Applying AI in the Fight Against Modern Slavery",
    "authors": [
      "Nadya Bliss",
      "Mark Briers",
      "Alice Eckstein",
      "James Goulding",
      "Daniel P. Lopresti",
      "Anjali Mazumder",
      "Gavin Smith"
    ],
    "author_ids": [],
    "abstract": "On any given day, tens of millions of people find themselves trapped in\ninstances of modern slavery. The terms \"human trafficking,\" \"trafficking in\npersons,\" and \"modern slavery\" are sometimes used interchangeably to refer to\nboth sex trafficking and forced labor. Human trafficking occurs when a\ntrafficker compels someone to provide labor or services through the use of\nforce, fraud, and/or coercion. The wide range of stakeholders in human\ntrafficking presents major challenges. Direct stakeholders are law enforcement,\nNGOs and INGOs, businesses, local/planning government authorities, and\nsurvivors. Viewed from a very high level, all stakeholders share in a rich\nnetwork of interactions that produce and consume enormous amounts of\ninformation. The problems of making efficient use of such information for the\npurposes of fighting trafficking while at the same time adhering to community\nstandards of privacy and ethics are formidable. At the same time they help us,\ntechnologies that increase surveillance of populations can also undermine basic\nhuman rights.\n  In early March 2020, the Computing Community Consortium (CCC), in\ncollaboration with the Code 8.7 Initiative, brought together over fifty members\nof the computing research community along with anti-slavery practitioners and\nsurvivors to lay out a research roadmap. The primary goal was to explore ways\nin which long-range research in artificial intelligence (AI) could be applied\nto the fight against human trafficking. Building on the kickoff Code 8.7\nconference held at the headquarters of the United Nations in February 2019, the\nfocus for this workshop was to link the ambitious goals outlined in the A\n20-Year Community Roadmap for Artificial Intelligence Research in the US (AI\nRoadmap) to challenges vital in achieving the UN's Sustainable Development Goal\nTarget 8.7, the elimination of modern slavery.",
    "published_date": "2021-06-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.13186v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.13178v1",
    "title": "Differential Morph Face Detection using Discriminative Wavelet Sub-bands",
    "authors": [
      "Baaria Chaudhary",
      "Poorya Aghdaie",
      "Sobhan Soleymani",
      "Jeremy Dawson",
      "Nasser M. Nasrabadi"
    ],
    "author_ids": [],
    "abstract": "Face recognition systems are extremely vulnerable to morphing attacks, in\nwhich a morphed facial reference image can be successfully verified as two or\nmore distinct identities. In this paper, we propose a morph attack detection\nalgorithm that leverages an undecimated 2D Discrete Wavelet Transform (DWT) for\nidentifying morphed face images. The core of our framework is that artifacts\nresulting from the morphing process that are not discernible in the image\ndomain can be more easily identified in the spatial frequency domain. A\ndiscriminative wavelet sub-band can accentuate the disparity between a real and\na morphed image. To this end, multi-level DWT is applied to all images,\nyielding 48 mid and high-frequency sub-bands each. The entropy distributions\nfor each sub-band are calculated separately for both bona fide and morph\nimages. For some of the sub-bands, there is a marked difference between the\nentropy of the sub-band in a bona fide image and the identical sub-band's\nentropy in a morphed image. Consequently, we employ Kullback-Liebler Divergence\n(KLD) to exploit these differences and isolate the sub-bands that are the most\ndiscriminative. We measure how discriminative a sub-band is by its KLD value\nand the 22 sub-bands with the highest KLD values are chosen for network\ntraining. Then, we train a deep Siamese neural network using these 22 selected\nsub-bands for differential morph attack detection. We examine the efficacy of\ndiscriminative wavelet sub-bands for morph attack detection and show that a\ndeep neural network trained on these sub-bands can accurately identify morph\nimagery.",
    "published_date": "2021-06-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.13178v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.13122v2",
    "title": "Exploring Corruption Robustness: Inductive Biases in Vision Transformers and MLP-Mixers",
    "authors": [
      "Katelyn Morrison",
      "Benjamin Gilby",
      "Colton Lipchak",
      "Adam Mattioli",
      "Adriana Kovashka"
    ],
    "author_ids": [],
    "abstract": "Recently, vision transformers and MLP-based models have been developed in\norder to address some of the prevalent weaknesses in convolutional neural\nnetworks. Due to the novelty of transformers being used in this domain along\nwith the self-attention mechanism, it remains unclear to what degree these\narchitectures are robust to corruptions. Despite some works proposing that data\naugmentation remains essential for a model to be robust against corruptions, we\npropose to explore the impact that the architecture has on corruption\nrobustness. We find that vision transformer architectures are inherently more\nrobust to corruptions than the ResNet-50 and MLP-Mixers. We also find that\nvision transformers with 5 times fewer parameters than a ResNet-50 have more\nshape bias. Our code is available to reproduce.",
    "published_date": "2021-06-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.13122v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.13109v1",
    "title": "Machine learning to tame divergent density functional approximations: a new path to consensus materials design principles",
    "authors": [
      "Chenru Duan",
      "Shuxin Chen",
      "Michael G. Taylor",
      "Fang Liu",
      "Heather J. Kulik"
    ],
    "author_ids": [],
    "abstract": "Computational virtual high-throughput screening (VHTS) with density\nfunctional theory (DFT) and machine-learning (ML)-acceleration is essential in\nrapid materials discovery. By necessity, efficient DFT-based workflows are\ncarried out with a single density functional approximation (DFA). Nevertheless,\nproperties evaluated with different DFAs can be expected to disagree for the\ncases with challenging electronic structure (e.g., open shell transition metal\ncomplexes, TMCs) for which rapid screening is most needed and accurate\nbenchmarks are often unavailable. To quantify the effect of DFA bias, we\nintroduce an approach to rapidly obtain property predictions from 23\nrepresentative DFAs spanning multiple families and \"rungs\" (e.g., semi-local to\ndouble hybrid) and basis sets on over 2,000 TMCs. Although computed properties\n(e.g., spin-state ordering and frontier orbital gap) naturally differ by DFA,\nhigh linear correlations persist across all DFAs. We train independent ML\nmodels for each DFA and observe convergent trends in feature importance; these\nfeatures thus provide DFA-invariant, universal design rules. We devise a\nstrategy to train ML models informed by all 23 DFAs and use them to predict\nproperties (e.g., spin-splitting energy) of over 182k TMCs. By requiring\nconsensus of the ANN-predicted DFA properties, we improve correspondence of\nthese computational lead compounds with literature-mined, experimental\ncompounds over the single-DFA approach typically employed. Both feature\nanalysis and consensus-based ML provide efficient, alternative paths to\novercome accuracy limitations of practical DFT.",
    "published_date": "2021-06-24T00:00:00",
    "year": 2021,
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.LG",
      "physics.chem-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.13109v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.13095v1",
    "title": "Pre-training transformer-based framework on large-scale pediatric claims data for downstream population-specific tasks",
    "authors": [
      "Xianlong Zeng",
      "Simon Lin",
      "Chang Liu"
    ],
    "author_ids": [],
    "abstract": "The adoption of electronic health records (EHR) has become universal during\nthe past decade, which has afforded in-depth data-based research. By learning\nfrom the large amount of healthcare data, various data-driven models have been\nbuilt to predict future events for different medical tasks, such as auto\ndiagnosis and heart-attack prediction. Although EHR is abundant, the population\nthat satisfies specific criteria for learning population-specific tasks is\nscarce, making it challenging to train data-hungry deep learning models. This\nstudy presents the Claim Pre-Training (Claim-PT) framework, a generic\npre-training model that first trains on the entire pediatric claims dataset,\nfollowed by a discriminative fine-tuning on each population-specific task. The\nsemantic meaning of medical events can be captured in the pre-training stage,\nand the effective knowledge transfer is completed through the task-aware\nfine-tuning stage. The fine-tuning process requires minimal parameter\nmodification without changing the model architecture, which mitigates the data\nscarcity issue and helps train the deep learning model adequately on small\npatient cohorts. We conducted experiments on a real-world claims dataset with\nmore than one million patient records. Experimental results on two downstream\ntasks demonstrated the effectiveness of our method: our general task-agnostic\npre-training framework outperformed tailored task-specific models, achieving\nmore than 10\\% higher in model performance as compared to baselines. In\naddition, our framework showed a great generalizability potential to transfer\nlearned knowledge from one institution to another, paving the way for future\nhealthcare model pre-training across institutions.",
    "published_date": "2021-06-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.13095v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.13002v2",
    "title": "Loosely-Stabilizing Phase Clocks and the Adaptive Majority Problem",
    "authors": [
      "Petra Berenbrink",
      "Felix Biermeier",
      "Christopher Hahn",
      "Dominik Kaaser"
    ],
    "author_ids": [],
    "abstract": "We present a loosely-stabilizing phase clock for population protocols. In the\npopulation model we are given a system of $n$ identical agents which interact\nin a sequence of randomly chosen pairs. Our phase clock is leaderless and it\nrequires $O(\\log n)$ states. It runs forever and is, at any point of time, in a\nsynchronous state w.h.p. When started in an arbitrary configuration, it\nrecovers rapidly and enters a synchronous configuration within $O(n\\log n)$\ninteractions w.h.p. Once the clock is synchronized, it stays in a synchronous\nconfiguration for at least poly $n$ parallel time w.h.p.\n  We use our clock to design a loosely-stabilizing protocol that solves the\ncomparison problem introduced by Alistarh et al., 2021. In this problem, a\nsubset of agents has at any time either $A$ or $B$ as input. The goal is to\nkeep track which of the two opinions is (momentarily) the majority. We show\nthat if the majority has a support of at least $\\Omega(\\log n)$ agents and a\nsufficiently large bias is present, then the protocol converges to a correct\noutput within $O(n\\log n)$ interactions and stays in a correct configuration\nfor poly $n$ interactions, w.h.p.",
    "published_date": "2021-06-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.13002v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.12794v1",
    "title": "\"Part Man, Part Machine, All Cop\": Automation in Policing",
    "authors": [
      "Angelika Adensamer",
      "Lukas Daniel Klausner"
    ],
    "author_ids": [],
    "abstract": "Digitisation, automation and datafication permeate policing and justice more\nand more each year -- from predictive policing methods through recidivism\nprediction to automated biometric identification at the border. The\nsociotechnical issues surrounding the use of such systems raise questions and\nreveal problems, both old and new. Our article reviews contemporary issues\nsurrounding automation in policing and the legal system, finds common issues\nand themes in various different examples, introduces the distinction between\nhuman \"retail bias\" and algorithmic \"wholesale bias\", and argues for shifting\nthe viewpoint on the debate to focus on both workers' rights and organisational\nresponsibility as well as fundamental rights and the right to an effective\nremedy.",
    "published_date": "2021-06-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.12794v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.14655v1",
    "title": "GAN-MDF: A Method for Multi-fidelity Data Fusion in Digital Twins",
    "authors": [
      "Lixue Liu",
      "Chao Zhang",
      "Dacheng Tao"
    ],
    "author_ids": [],
    "abstract": "The Internet of Things (IoT) collects real-time data of physical systems,\nsuch as smart factory, intelligent robot and healtcare system, and provide\nnecessary support for digital twins. Depending on the quality and accuracy,\nthese multi-source data are divided into different fidelity levels.\nHigh-fidelity (HF) responses describe the system of interest accurately but are\ncomputed costly. In contrast, low-fidelity (LF) responses have a low\ncomputational cost but could not meet the required accuracy. Multi-fidelity\ndata fusion (MDF) methods aims to use massive LF samples and small amounts of\nHF samples to develop an accurate and efficient model for describing the system\nwith a reasonable computation burden. In this paper, we propose a novel\ngenerative adversarial network for MDF in digital twins (GAN-MDF). The\ngenerator of GAN-MDF is composed of two sub-networks: one extracts the LF\nfeatures from an input; and the other integrates the input and the extracted LF\nfeatures to form the input of the subsequent discriminator. The discriminator\nof GAN-MDF identifies whether the generator output is a real sample generated\nfrom HF model. To enhance the stability of GAN-MDF's training, we also\nintroduce the supervised-loss trick to refine the generator weights during each\niteration of the adversarial training. Compared with the state-of-the-art\nmethods, the proposed GAN-MDF has the following advantages: 1) it performs well\nin the case of either nested or unnested sample structure; 2) there is no\nspecific assumption on the data distribution; and 3) it has high robustness\neven when very few HF samples are provided. The experimental results also\nsupport the validity of GAN-MDF.",
    "published_date": "2021-06-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.14655v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.12785v2",
    "title": "Modelling Mutual Exclusion in a Process Algebra with Time-outs",
    "authors": [
      "Rob van Glabbeek"
    ],
    "author_ids": [],
    "abstract": "I show that in a standard process algebra extended with time-outs one can\ncorrectly model mutual exclusion in such a way that starvation-freedom holds\nwithout assuming fairness or justness, even when one makes the problem more\nchallenging by assuming memory accesses to be atomic. This can be achieved only\nwhen dropping the requirement of speed independence.",
    "published_date": "2021-06-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LO",
      "F.1.2; F.4.1; F.3.2; F.3.1; C.2.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.12785v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2107.01018v1",
    "title": "RAN Resource Slicing in 5G Using Multi-Agent Correlated Q-Learning",
    "authors": [
      "Hao Zhou",
      "Medhat Elsayed",
      "Melike Erol-Kantarci"
    ],
    "author_ids": [],
    "abstract": "5G is regarded as a revolutionary mobile network, which is expected to\nsatisfy a vast number of novel services, ranging from remote health care to\nsmart cities. However, heterogeneous Quality of Service (QoS) requirements of\ndifferent services and limited spectrum make the radio resource allocation a\nchallenging problem in 5G. In this paper, we propose a multi-agent\nreinforcement learning (MARL) method for radio resource slicing in 5G. We model\neach slice as an intelligent agent that competes for limited radio resources,\nand the correlated Q-learning is applied for inter-slice resource block (RB)\nallocation. The proposed correlated Q-learning based interslice RB allocation\n(COQRA) scheme is compared with Nash Q-learning (NQL),\nLatency-Reliability-Throughput Q-learning (LRTQ) methods, and the priority\nproportional fairness (PPF) algorithm. Our simulation results show that the\nproposed COQRA achieves 32.4% lower latency and 6.3% higher throughput when\ncompared with LRTQ, and 5.8% lower latency and 5.9% higher throughput than NQL.\nSignificantly higher throughput and lower packet drop rate (PDR) is observed in\ncomparison to PPF.",
    "published_date": "2021-06-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.NI",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.01018v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.12674v2",
    "title": "Fairness via Representation Neutralization",
    "authors": [
      "Mengnan Du",
      "Subhabrata Mukherjee",
      "Guanchu Wang",
      "Ruixiang Tang",
      "Ahmed Hassan Awadallah",
      "Xia Hu"
    ],
    "author_ids": [],
    "abstract": "Existing bias mitigation methods for DNN models primarily work on learning\ndebiased encoders. This process not only requires a lot of instance-level\nannotations for sensitive attributes, it also does not guarantee that all\nfairness sensitive information has been removed from the encoder. To address\nthese limitations, we explore the following research question: Can we reduce\nthe discrimination of DNN models by only debiasing the classification head,\neven with biased representations as inputs? To this end, we propose a new\nmitigation technique, namely, Representation Neutralization for Fairness (RNF)\nthat achieves fairness by debiasing only the task-specific classification head\nof DNN models. To this end, we leverage samples with the same ground-truth\nlabel but different sensitive attributes, and use their neutralized\nrepresentations to train the classification head of the DNN model. The key idea\nof RNF is to discourage the classification head from capturing spurious\ncorrelation between fairness sensitive information in encoder representations\nwith specific class labels. To address low-resource settings with no access to\nsensitive attribute annotations, we leverage a bias-amplified model to generate\nproxy annotations for sensitive attributes. Experimental results over several\nbenchmark datasets demonstrate our RNF framework to effectively reduce\ndiscrimination of DNN models with minimal degradation in task-specific\nperformance.",
    "published_date": "2021-06-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.12674v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.12563v2",
    "title": "Feature Attributions and Counterfactual Explanations Can Be Manipulated",
    "authors": [
      "Dylan Slack",
      "Sophie Hilgard",
      "Sameer Singh",
      "Himabindu Lakkaraju"
    ],
    "author_ids": [],
    "abstract": "As machine learning models are increasingly used in critical decision-making\nsettings (e.g., healthcare, finance), there has been a growing emphasis on\ndeveloping methods to explain model predictions. Such \\textit{explanations} are\nused to understand and establish trust in models and are vital components in\nmachine learning pipelines. Though explanations are a critical piece in these\nsystems, there is little understanding about how they are vulnerable to\nmanipulation by adversaries. In this paper, we discuss how two broad classes of\nexplanations are vulnerable to manipulation. We demonstrate how adversaries can\ndesign biased models that manipulate model agnostic feature attribution methods\n(e.g., LIME \\& SHAP) and counterfactual explanations that hill-climb during the\ncounterfactual search (e.g., Wachter's Algorithm \\& DiCE) into\n\\textit{concealing} the model's biases. These vulnerabilities allow an\nadversary to deploy a biased model, yet explanations will not reveal this bias,\nthereby deceiving stakeholders into trusting the model. We evaluate the\nmanipulations on real world data sets, including COMPAS and Communities \\&\nCrime, and find explanations can be manipulated in practice.",
    "published_date": "2021-06-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.12563v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.12556v4",
    "title": "Real-time Outdoor Localization Using Radio Maps: A Deep Learning Approach",
    "authors": [
      "Çağkan Yapar",
      "Ron Levie",
      "Gitta Kutyniok",
      "Giuseppe Caire"
    ],
    "author_ids": [],
    "abstract": "Global Navigation Satellite Systems typically perform poorly in urban\nenvironments, where the likelihood of line-of-sight conditions between devices\nand satellites is low. Therefore, alternative location methods are required to\nachieve good accuracy. We present LocUNet: A convolutional, end-to-end trained\nneural network (NN) for the localization task, which is able to estimate the\nposition of a user from the received signal strength (RSS) of a small number of\nBase Stations (BS). Using estimations of pathloss radio maps of the BSs and the\nRSS measurements of the users to be localized, LocUNet can localize users with\nstate-of-the-art accuracy and enjoys high robustness to inaccuracies in the\nestimations of radio maps. The proposed method does not require generating RSS\nfingerprints of each specific area where the localization task is performed and\nis suitable for real-time applications. Moreover, two novel datasets that allow\nfor numerical evaluations of RSS and ToA methods in realistic urban\nenvironments are presented and made publicly available for the research\ncommunity. By using these datasets, we also provide a fair comparison of\nstate-of-the-art RSS and ToA-based methods in the dense urban scenario and show\nnumerically that LocUNet outperforms all the compared methods.",
    "published_date": "2021-06-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.NI",
      "eess.SP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.12556v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.12548v1",
    "title": "Multi-Class Classification of Blood Cells -- End to End Computer Vision based diagnosis case study",
    "authors": [
      "Sai Sukruth Bezugam"
    ],
    "author_ids": [],
    "abstract": "The diagnosis of blood-based diseases often involves identifying and\ncharacterizing patient blood samples. Automated methods to detect and classify\nblood cell subtypes have important medical applications. Automated medical\nimage processing and analysis offers a powerful tool for medical diagnosis. In\nthis work we tackle the problem of white blood cell classification based on the\nmorphological characteristics of their outer contour, color. The work we would\nexplore a set of preprocessing and segmentation (Color-based segmentation,\nMorphological processing, contouring) algorithms along with a set of features\nextraction methods (Corner detection algorithms and Histogram of\nGradients(HOG)), dimensionality reduction algorithms (Principal Component\nAnalysis(PCA)) that are able to recognize and classify through various\nUnsupervised(k-nearest neighbors) and Supervised (Support Vector Machine,\nDecision Trees, Linear Discriminant Analysis, Quadratic Discriminant Analysis,\nNaive Bayes) algorithms different categories of white blood cells to\nEosinophil, Lymphocyte, Monocyte, and Neutrophil. We even take a step forwards\nto explore various Deep Convolutional Neural network architecture (Sqeezent,\nMobilenetV1,MobilenetV2, InceptionNet etc.) without preprocessing/segmentation\nand with preprocessing. We would like to explore many algorithms to identify\nthe robust algorithm with least time complexity and low resource requirement.\nThe outcome of this work can be a cue to selection of algorithms as per\nrequirement for automated blood cell classification.",
    "published_date": "2021-06-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI",
      "q-bio.CB",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.12548v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.12533v1",
    "title": "Publication Bias: A Detailed Analysis of Experiments Published in ESEM",
    "authors": [
      "Rolando P. Reyes",
      "Óscar Dieste",
      "Efraín R. Fonseca C.",
      "Natalia Juristo"
    ],
    "author_ids": [],
    "abstract": "Background: Publication bias is the failure to publish the results of a study\nbased on the direction or strength of the study findings. The existence of\npublication bias is firmly established in areas like medical research. Recent\nresearch suggests the existence of publication bias in Software Engineering.\nAims: Finding out whether experiments published in the International Workshop\non Empirical Software Engineering and Measurement (ESEM) are affected by\npublication bias. Method: We review experiments published in ESEM. We also\nsurvey with experimental researchers to triangulate our findings. Results: ESEM\nexperiments do not define hypotheses and frequently perform multiple testing.\nOne-tailed tests have a slightly higher rate of achieving statistically\nsignificant results. We could not find other practices associated with\npublication bias. Conclusions: Our results provide a more encouraging\nperspective of SE research than previous research: (1) ESEM publications do not\nseem to be strongly affected by biases and (2) we identify some practices that\ncould be associated with p-hacking, but it is more likely that they are related\nto the conduction of exploratory research.",
    "published_date": "2021-06-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.12533v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.12387v2",
    "title": "Fairness in Cardiac MR Image Analysis: An Investigation of Bias Due to Data Imbalance in Deep Learning Based Segmentation",
    "authors": [
      "Esther Puyol-Anton",
      "Bram Ruijsink",
      "Stefan K. Piechnik",
      "Stefan Neubauer",
      "Steffen E. Petersen",
      "Reza Razavi",
      "Andrew P. King"
    ],
    "author_ids": [],
    "abstract": "The subject of \"fairness\" in artificial intelligence (AI) refers to assessing\nAI algorithms for potential bias based on demographic characteristics such as\nrace and gender, and the development of algorithms to address this bias. Most\napplications to date have been in computer vision, although some work in\nhealthcare has started to emerge. The use of deep learning (DL) in cardiac MR\nsegmentation has led to impressive results in recent years, and such techniques\nare starting to be translated into clinical practice. However, no work has yet\ninvestigated the fairness of such models. In this work, we perform such an\nanalysis for racial/gender groups, focusing on the problem of training data\nimbalance, using a nnU-Net model trained and evaluated on cine short axis\ncardiac MR data from the UK Biobank dataset, consisting of 5,903 subjects from\n6 different racial groups. We find statistically significant differences in\nDice performance between different racial groups. To reduce the racial bias, we\ninvestigated three strategies: (1) stratified batch sampling, in which batch\nsampling is stratified to ensure balance between racial groups; (2) fair\nmeta-learning for segmentation, in which a DL classifier is trained to classify\nrace and jointly optimized with the segmentation model; and (3) protected group\nmodels, in which a different segmentation model is trained for each racial\ngroup. We also compared the results to the scenario where we have a perfectly\nbalanced database. To assess fairness we used the standard deviation (SD) and\nskewed error ratio (SER) of the average Dice values. Our results demonstrate\nthat the racial bias results from the use of imbalanced training data, and that\nall proposed bias mitigation strategies improved fairness, with the best SD and\nSER resulting from the use of protected group models.",
    "published_date": "2021-06-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.12387v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.12378v1",
    "title": "Co-advise: Cross Inductive Bias Distillation",
    "authors": [
      "Sucheng Ren",
      "Zhengqi Gao",
      "Tianyu Hua",
      "Zihui Xue",
      "Yonglong Tian",
      "Shengfeng He",
      "Hang Zhao"
    ],
    "author_ids": [],
    "abstract": "Transformers recently are adapted from the community of natural language\nprocessing as a promising substitute of convolution-based neural networks for\nvisual learning tasks. However, its supremacy degenerates given an insufficient\namount of training data (e.g., ImageNet). To make it into practical utility, we\npropose a novel distillation-based method to train vision transformers. Unlike\nprevious works, where merely heavy convolution-based teachers are provided, we\nintroduce lightweight teachers with different architectural inductive biases\n(e.g., convolution and involution) to co-advise the student transformer. The\nkey is that teachers with different inductive biases attain different knowledge\ndespite that they are trained on the same dataset, and such different knowledge\ncompounds and boosts the student's performance during distillation. Equipped\nwith this cross inductive bias distillation method, our vision transformers\n(termed as CivT) outperform all previous transformers of the same architecture\non ImageNet.",
    "published_date": "2021-06-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.12378v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.12242v2",
    "title": "A Unified Approach to Fair Online Learning via Blackwell Approachability",
    "authors": [
      "Evgenii Chzhen",
      "Christophe Giraud",
      "Gilles Stoltz"
    ],
    "author_ids": [],
    "abstract": "We provide a setting and a general approach to fair online learning with\nstochastic sensitive and non-sensitive contexts. The setting is a repeated game\nbetween the Player and Nature, where at each stage both pick actions based on\nthe contexts. Inspired by the notion of unawareness, we assume that the Player\ncan only access the non-sensitive context before making a decision, while we\ndiscuss both cases of Nature accessing the sensitive contexts and Nature\nunaware of the sensitive contexts. Adapting Blackwell's approachability theory\nto handle the case of an unknown contexts' distribution, we provide a general\nnecessary and sufficient condition for learning objectives to be compatible\nwith some fairness constraints. This condition is instantiated on (group-wise)\nno-regret and (group-wise) calibration objectives, and on demographic parity as\nan additional constraint. When the objective is not compatible with the\nconstraint, the provided framework permits to characterise the optimal\ntrade-off between the two.",
    "published_date": "2021-06-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.12242v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.13732v1",
    "title": "Recurrent Coupled Topic Modeling over Sequential Documents",
    "authors": [
      "Jinjin Guo",
      "Longbing Cao",
      "Zhiguo Gong"
    ],
    "author_ids": [],
    "abstract": "The abundant sequential documents such as online archival, social media and\nnews feeds are streamingly updated, where each chunk of documents is\nincorporated with smoothly evolving yet dependent topics. Such digital texts\nhave attracted extensive research on dynamic topic modeling to infer hidden\nevolving topics and their temporal dependencies. However, most of the existing\napproaches focus on single-topic-thread evolution and ignore the fact that a\ncurrent topic may be coupled with multiple relevant prior topics. In addition,\nthese approaches also incur the intractable inference problem when inferring\nlatent parameters, resulting in a high computational cost and performance\ndegradation. In this work, we assume that a current topic evolves from all\nprior topics with corresponding coupling weights, forming the\nmulti-topic-thread evolution. Our method models the dependencies between\nevolving topics and thoroughly encodes their complex multi-couplings across\ntime steps. To conquer the intractable inference challenge, a new solution with\na set of novel data augmentation techniques is proposed, which successfully\ndiscomposes the multi-couplings between evolving topics. A fully conjugate\nmodel is thus obtained to guarantee the effectiveness and efficiency of the\ninference technique. A novel Gibbs sampler with a backward-forward filter\nalgorithm efficiently learns latent timeevolving parameters in a closed-form.\nIn addition, the latent Indian Buffet Process (IBP) compound distribution is\nexploited to automatically infer the overall topic number and customize the\nsparse topic proportions for each sequential document without bias. The\nproposed method is evaluated on both synthetic and real-world datasets against\nthe competitive baselines, demonstrating its superiority over the baselines in\nterms of the low per-word perplexity, high coherent topics, and better document\ntime prediction.",
    "published_date": "2021-06-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.13732v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.12182v2",
    "title": "Fairness for Image Generation with Uncertain Sensitive Attributes",
    "authors": [
      "Ajil Jalal",
      "Sushrut Karmalkar",
      "Jessica Hoffmann",
      "Alexandros G. Dimakis",
      "Eric Price"
    ],
    "author_ids": [],
    "abstract": "This work tackles the issue of fairness in the context of generative\nprocedures, such as image super-resolution, which entail different definitions\nfrom the standard classification setting. Moreover, while traditional group\nfairness definitions are typically defined with respect to specified protected\ngroups -- camouflaging the fact that these groupings are artificial and carry\nhistorical and political motivations -- we emphasize that there are no ground\ntruth identities. For instance, should South and East Asians be viewed as a\nsingle group or separate groups? Should we consider one race as a whole or\nfurther split by gender? Choosing which groups are valid and who belongs in\nthem is an impossible dilemma and being \"fair\" with respect to Asians may\nrequire being \"unfair\" with respect to South Asians. This motivates the\nintroduction of definitions that allow algorithms to be \\emph{oblivious} to the\nrelevant groupings.\n  We define several intuitive notions of group fairness and study their\nincompatibilities and trade-offs. We show that the natural extension of\ndemographic parity is strongly dependent on the grouping, and \\emph{impossible}\nto achieve obliviously. On the other hand, the conceptually new definition we\nintroduce, Conditional Proportional Representation, can be achieved obliviously\nthrough Posterior Sampling. Our experiments validate our theoretical results\nand achieve fair image reconstruction using state-of-the-art generative models.",
    "published_date": "2021-06-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.12182v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.12056v1",
    "title": "On Positivity Bias in Negative Reviews",
    "authors": [
      "Madhusudhan Aithal",
      "Chenhao Tan"
    ],
    "author_ids": [],
    "abstract": "Prior work has revealed that positive words occur more frequently than\nnegative words in human expressions, which is typically attributed to\npositivity bias, a tendency for people to report positive views of reality. But\nwhat about the language used in negative reviews? Consistent with prior work,\nwe show that English negative reviews tend to contain more positive words than\nnegative words, using a variety of datasets. We reconcile this observation with\nprior findings on the pragmatics of negation, and show that negations are\ncommonly associated with positive words in negative reviews. Furthermore, in\nnegative reviews, the majority of sentences with positive words express\nnegative opinions based on sentiment classifiers, indicating some form of\nnegation.",
    "published_date": "2021-06-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.12056v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.12576v2",
    "title": "DP-SGD vs PATE: Which Has Less Disparate Impact on Model Accuracy?",
    "authors": [
      "Archit Uniyal",
      "Rakshit Naidu",
      "Sasikanth Kotti",
      "Sahib Singh",
      "Patrik Joslin Kenfack",
      "Fatemehsadat Mireshghallah",
      "Andrew Trask"
    ],
    "author_ids": [],
    "abstract": "Recent advances in differentially private deep learning have demonstrated\nthat application of differential privacy, specifically the DP-SGD algorithm,\nhas a disparate impact on different sub-groups in the population, which leads\nto a significantly high drop-in model utility for sub-populations that are\nunder-represented (minorities), compared to well-represented ones. In this\nwork, we aim to compare PATE, another mechanism for training deep learning\nmodels using differential privacy, with DP-SGD in terms of fairness. We show\nthat PATE does have a disparate impact too, however, it is much less severe\nthan DP-SGD. We draw insights from this observation on what might be promising\ndirections in achieving better fairness-privacy trade-offs.",
    "published_date": "2021-06-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.12576v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.02024v1",
    "title": "Statistical Analysis of Perspective Scores on Hate Speech Detection",
    "authors": [
      "Hadi Mansourifar",
      "Dana Alsagheer",
      "Weidong Shi",
      "Lan Ni",
      "Yan Huang"
    ],
    "author_ids": [],
    "abstract": "Hate speech detection has become a hot topic in recent years due to the\nexponential growth of offensive language in social media. It has proven that,\nstate-of-the-art hate speech classifiers are efficient only when tested on the\ndata with the same feature distribution as training data. As a consequence,\nmodel architecture plays the second role to improve the current results. In\nsuch a diverse data distribution relying on low level features is the main\ncause of deficiency due to natural bias in data. That's why we need to use high\nlevel features to avoid a biased judgement. In this paper, we statistically\nanalyze the Perspective Scores and their impact on hate speech detection. We\nshow that, different hate speech datasets are very similar when it comes to\nextract their Perspective Scores. Eventually, we prove that, over-sampling the\nPerspective Scores of a hate speech dataset can significantly improve the\ngeneralization performance when it comes to be tested on other hate speech\ndatasets.",
    "published_date": "2021-06-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.02024v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.11930v4",
    "title": "On the importance of cross-task features for class-incremental learning",
    "authors": [
      "Albin Soutif--Cormerais",
      "Marc Masana",
      "Joost van de Weijer",
      "Bartłomiej Twardowski"
    ],
    "author_ids": [],
    "abstract": "In class-incremental learning, an agent with limited resources needs to learn\na sequence of classification tasks, forming an ever growing classification\nproblem, with the constraint of not being able to access data from previous\ntasks. The main difference with task-incremental learning, where a task-ID is\navailable at inference time, is that the learner also needs to perform\ncross-task discrimination, i.e. distinguish between classes that have not been\nseen together. Approaches to tackle this problem are numerous and mostly make\nuse of an external memory (buffer) of non-negligible size. In this paper, we\nablate the learning of cross-task features and study its influence on the\nperformance of basic replay strategies used for class-IL. We also define a new\nforgetting measure for class-incremental learning, and see that forgetting is\nnot the principal cause of low performance. Our experimental results show that\nfuture algorithms for class-incremental learning should not only prevent\nforgetting, but also aim to improve the quality of the cross-task features, and\nthe knowledge transfer between tasks. This is especially important when tasks\ncontain limited amount of data.",
    "published_date": "2021-06-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.11930v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.11410v2",
    "title": "A Survey of Race, Racism, and Anti-Racism in NLP",
    "authors": [
      "Anjalie Field",
      "Su Lin Blodgett",
      "Zeerak Waseem",
      "Yulia Tsvetkov"
    ],
    "author_ids": [],
    "abstract": "Despite inextricable ties between race and language, little work has\nconsidered race in NLP research and development. In this work, we survey 79\npapers from the ACL anthology that mention race. These papers reveal various\ntypes of race-related bias in all stages of NLP model development, highlighting\nthe need for proactive consideration of how NLP systems can uphold racial\nhierarchies. However, persistent gaps in research on race and NLP remain: race\nhas been siloed as a niche topic and remains ignored in many NLP tasks; most\nwork operationalizes race as a fixed single-dimensional variable with a\nground-truth label, which risks reinforcing differences produced by historical\nracism; and the voices of historically marginalized people are nearly absent in\nNLP literature. By identifying where and how NLP literature has and has not\nconsidered race, especially in comparison to related fields, our work calls for\ninclusion and racial justice in NLP research practices.",
    "published_date": "2021-06-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.11410v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.11178v2",
    "title": "Thou Shalt Covet The Average Of Thy Neighbors' Cakes",
    "authors": [
      "Jamie Tucker-Foltz"
    ],
    "author_ids": [],
    "abstract": "We prove an $\\Omega(n^2)$ lower bound on the query complexity of local\nproportionality in the Robertson-Webb cake-cutting model. Local proportionality\nrequires that each agent prefer their allocation to the average of their\nneighbors' allocations in some undirected social network. It is a weaker\nfairness notion than envy-freeness, which also has query complexity\n$\\Omega(n^2)$, and generally incomparable to proportionality, which has query\ncomplexity $\\Theta(n \\log n)$. This result separates the complexity of local\nproportionality from that of ordinary proportionality, confirming the intuition\nthat finding a locally proportional allocation is a more difficult\ncomputational problem.",
    "published_date": "2021-06-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.11178v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.11003v1",
    "title": "Stochastic Model for Sunk Cost Bias",
    "authors": [
      "Jon Kleinberg",
      "Sigal Oren",
      "Manish Raghavan",
      "Nadav Sklar"
    ],
    "author_ids": [],
    "abstract": "We present a novel model for capturing the behavior of an agent exhibiting\nsunk-cost bias in a stochastic environment. Agents exhibiting sunk-cost bias\ntake into account the effort they have already spent on an endeavor when they\nevaluate whether to continue or abandon it. We model planning tasks in which an\nagent with this type of bias tries to reach a designated goal. Our model\nstructures this problem as a type of Markov decision process: loosely speaking,\nthe agent traverses a directed acyclic graph with probabilistic transitions,\npaying costs for its actions as it tries to reach a target node containing a\nspecified reward. The agent's sunk cost bias is modeled by a cost that it\nincurs for abandoning the traversal: if the agent decides to stop traversing\nthe graph, it incurs a cost of $\\lambda \\cdot C_{sunk}$, where ${\\lambda \\geq\n0}$ is a parameter that captures the extent of the bias and $C_{sunk}$ is the\nsum of costs already invested.\n  We analyze the behavior of two types of agents: naive agents that are unaware\nof their bias, and sophisticated agents that are aware of it. Since optimal\n(bias-free) behavior in this problem can involve abandoning the traversal\nbefore reaching the goal, the bias exhibited by these types of agents can\nresult in sub-optimal behavior by shifting their decisions about abandonment.\nWe show that in contrast to optimal agents, it is computationally hard to\ncompute the optimal policy for a sophisticated agent. Our main results quantify\nthe loss exhibited by these two types of agents with respect to an optimal\nagent. We present both general and topology-specific bounds.",
    "published_date": "2021-06-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.11003v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.10826v1",
    "title": "Does Robustness Improve Fairness? Approaching Fairness with Word Substitution Robustness Methods for Text Classification",
    "authors": [
      "Yada Pruksachatkun",
      "Satyapriya Krishna",
      "Jwala Dhamala",
      "Rahul Gupta",
      "Kai-Wei Chang"
    ],
    "author_ids": [],
    "abstract": "Existing bias mitigation methods to reduce disparities in model outcomes\nacross cohorts have focused on data augmentation, debiasing model embeddings,\nor adding fairness-based optimization objectives during training. Separately,\ncertified word substitution robustness methods have been developed to decrease\nthe impact of spurious features and synonym substitutions on model predictions.\nWhile their end goals are different, they both aim to encourage models to make\nthe same prediction for certain changes in the input. In this paper, we\ninvestigate the utility of certified word substitution robustness methods to\nimprove equality of odds and equality of opportunity on multiple text\nclassification tasks. We observe that certified robustness methods improve\nfairness, and using both robustness and bias mitigation methods in training\nresults in an improvement in both fronts",
    "published_date": "2021-06-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.10826v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.10814v2",
    "title": "On Simple Mechanisms for Dependent Items",
    "authors": [
      "Yang Cai",
      "Argyris Oikonomou"
    ],
    "author_ids": [],
    "abstract": "We study the problem of selling $n$ heterogeneous items to a single buyer,\nwhose values for different items are dependent. Under arbitrary dependence,\nHart and Nisan show that no simple mechanism can achieve a non-negligible\nfraction of the optimal revenue even with only two items. We consider the\nsetting where the buyer's type is drawn from a correlated distribution that can\nbe captured by a Markov Random Field, one of the most prominent frameworks for\nmodeling high-dimensional distributions with structure.\n  If the buyer's valuation is additive or unit-demand, we extend the result to\nall MRFs and show that max(SRev,BRev) can achieve an\n$\\Omega\\left(\\frac{1}{e^{O(\\Delta)}}\\right)$-fraction of the optimal revenue,\nwhere $\\Delta$ is a parameter of the MRF that is determined by how much the\nvalue of an item can be influenced by the values of the other items. We further\nshow that the exponential dependence on $\\Delta$ is unavoidable for our\napproach and a polynomial dependence on $\\Delta$ is unavoidable for any\napproach. When the buyer has a XOS valuation, we show that max(Srev,Brev)\nachieves at least an\n$\\Omega\\left(\\frac{1}{e^{O(\\Delta)}+\\frac{1}{\\sqrt{n\\gamma}}}\\right)$-fraction\nof the optimal revenue, where $\\gamma$ is the spectral gap of the Glauber\ndynamics of the MRF. Note that in the special case of independently distributed\nitems, $\\Delta=0$ and $\\frac{1}{n\\gamma}\\leq 1$, and our results recover the\nknown constant factor approximations for a XOS buyer. We further extend our\nparametric approximation to several other well-studied dependency measures such\nas the Dobrushin coefficient and the inverse temperature. Our results are based\non the Duality-Framework by Cai et al. and a new concentration inequality for\nXOS functions over dependent random variables.",
    "published_date": "2021-06-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.10814v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.10731v1",
    "title": "Neighborhood Contrastive Learning for Novel Class Discovery",
    "authors": [
      "Zhun Zhong",
      "Enrico Fini",
      "Subhankar Roy",
      "Zhiming Luo",
      "Elisa Ricci",
      "Nicu Sebe"
    ],
    "author_ids": [],
    "abstract": "In this paper, we address Novel Class Discovery (NCD), the task of unveiling\nnew classes in a set of unlabeled samples given a labeled dataset with known\nclasses. We exploit the peculiarities of NCD to build a new framework, named\nNeighborhood Contrastive Learning (NCL), to learn discriminative\nrepresentations that are important to clustering performance. Our contribution\nis twofold. First, we find that a feature extractor trained on the labeled set\ngenerates representations in which a generic query sample and its neighbors are\nlikely to share the same class. We exploit this observation to retrieve and\naggregate pseudo-positive pairs with contrastive learning, thus encouraging the\nmodel to learn more discriminative representations. Second, we notice that most\nof the instances are easily discriminated by the network, contributing less to\nthe contrastive loss. To overcome this issue, we propose to generate hard\nnegatives by mixing labeled and unlabeled samples in the feature space. We\nexperimentally demonstrate that these two ingredients significantly contribute\nto clustering performance and lead our model to outperform state-of-the-art\nmethods by a large margin (e.g., clustering accuracy +13% on CIFAR-100 and +8%\non ImageNet).",
    "published_date": "2021-06-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.10731v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.10706v1",
    "title": "Feedback Nash Equilibria in Differential Games with Impulse Control",
    "authors": [
      "Utsav Sadana",
      "Puduru Viswanadha Reddy",
      "Georges Zaccour"
    ],
    "author_ids": [],
    "abstract": "We study a class of deterministic finite-horizon two-player nonzero-sum\ndifferential games where players are endowed with different kinds of controls.\nWe assume that Player 1 uses piecewise-continuous controls, while Player 2 uses\nimpulse controls. For this class of games, we seek to derive conditions for the\nexistence of feedback Nash equilibrium strategies for the players. More\nspecifically, we provide a verification theorem for identifying such\nequilibrium strategies, using the Hamilton-Jacobi-Bellman (HJB) equations for\nPlayer 1 and the quasi-variational inequalities (QVIs) for Player 2. Further,\nwe show that the equilibrium number of interventions by Player 2 is upper\nbounded. Furthermore, we specialize the obtained results to a scalar two-player\nlinear-quadratic differential game. In this game, Player 1's objective is to\ndrive the state variable towards a specific target value, and Player 2 has a\nsimilar objective with a different target value. We provide, for the first\ntime, an analytical characterization of the feedback Nash equilibrium in a\nlinear-quadratic differential game with impulse control. We illustrate our\nresults using numerical experiments.",
    "published_date": "2021-06-20T00:00:00",
    "year": 2021,
    "categories": [
      "math.OC",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.10706v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.10649v1",
    "title": "CAMERAS: Enhanced Resolution And Sanity preserving Class Activation Mapping for image saliency",
    "authors": [
      "Mohammad A. A. K. Jalwana",
      "Naveed Akhtar",
      "Mohammed Bennamoun",
      "Ajmal Mian"
    ],
    "author_ids": [],
    "abstract": "Backpropagation image saliency aims at explaining model predictions by\nestimating model-centric importance of individual pixels in the input. However,\nclass-insensitivity of the earlier layers in a network only allows saliency\ncomputation with low resolution activation maps of the deeper layers, resulting\nin compromised image saliency. Remedifying this can lead to sanity failures. We\npropose CAMERAS, a technique to compute high-fidelity backpropagation saliency\nmaps without requiring any external priors and preserving the map sanity. Our\nmethod systematically performs multi-scale accumulation and fusion of the\nactivation maps and backpropagated gradients to compute precise saliency maps.\nFrom accurate image saliency to articulation of relative importance of input\nfeatures for different models, and precise discrimination between model\nperception of visually similar objects, our high-resolution mapping offers\nmultiple novel insights into the black-box deep visual models, which are\npresented in the paper. We also demonstrate the utility of our saliency maps in\nadversarial setup by drastically reducing the norm of attack signals by\nfocusing them on the precise regions identified by our maps. Our method also\ninspires new evaluation metrics and a sanity check for this developing research\ndirection. Code is available here https://github.com/VisMIL/CAMERAS",
    "published_date": "2021-06-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.10649v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.10415v1",
    "title": "Rényi divergence inequalities via interpolation, with applications to generalised entropic uncertainty relations",
    "authors": [
      "Alexander McKinlay"
    ],
    "author_ids": [],
    "abstract": "We investigate quantum R\\'enyi entropic quantities, specifically those\nderived from 'sandwiched' divergence. This divergence is one of several\nproposed R\\'enyi generalisations of the quantum relative entropy. We may define\nR\\'enyi generalisations of the quantum conditional entropy and mutual\ninformation in terms of this divergence, from which they inherit many desirable\nproperties. However, these quantities lack some of the convenient structure of\ntheir Shannon and von Neumann counterparts. We attempt to bridge this gap by\nestablishing divergence inequalities for valid combinations of R\\'enyi order\nwhich replicate the chain and decomposition rules of Shannon and von Neumann\nentropies. Although weaker in general, these inequalities recover equivalence\nwhen the R\\'enyi parameters tend to one.\n  To this end we present R\\'enyi mutual information decomposition rules, a new\napproach to the R\\'enyi conditional entropy tripartite chain rules and a more\ngeneral bipartite comparison. The derivation of these results relies on a novel\ncomplex interpolation approach for general spaces of linear operators.\n  These new comparisons allow us to employ techniques that until now were only\navailable for Shannon and von Neumann entropies. We can therefore directly\napply them to the derivation of R\\'enyi entropic uncertainty relations.\nAccordingly, we establish a family of R\\'enyi information exclusion relations\nand provide further generalisations and improvements to this and other known\nrelations, including the R\\'enyi bipartite uncertainty relations.",
    "published_date": "2021-06-19T00:00:00",
    "year": 2021,
    "categories": [
      "quant-ph",
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.10415v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.10408v1",
    "title": "Step Out of Your Comfort Zone: More Inclusive Content Recommendation for Networked Systems",
    "authors": [
      "Jiaxin Wu",
      "Supawit Chockchowwat"
    ],
    "author_ids": [],
    "abstract": "Networked systems are widely applicable in real-world scenarios such as\nsocial networks, infrastructure networks, and biological networks. Among those\napplications, we are interested in social networks due to their complexity and\npopularity. One crucial task on the social network is to recommend new content\nbased on special characteristics of the graph structure. In this project, we\naim to enhance the recommender systems by preventing the recommendations from\nleaning towards contents from closed communities. To counteract the bias, we\nwill consider information dissemination across network as a metric to assess\nthe recommendation for contents e.g. new connections and news feed. We use\nacademic collaboration network and user-item interaction datasets from Yelp to\nsimulate an environment for connection recommendations and to validate the\nproposed algorithm.",
    "published_date": "2021-06-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.10408v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.10407v2",
    "title": "When Efficiency meets Equity in Congestion Pricing and Revenue Refunding Schemes",
    "authors": [
      "Devansh Jalota",
      "Kiril Solovey",
      "Karthik Gopalakrishnan",
      "Stephen Zoepf",
      "Hamsa Balakrishnan",
      "Marco Pavone"
    ],
    "author_ids": [],
    "abstract": "Congestion pricing has long been hailed as a means to mitigate traffic\ncongestion; however, its practical adoption has been limited due to the\nresulting social inequity issue, e.g., low-income users are priced out off\ncertain roads. This issue has spurred interest in the design of equitable\nmechanisms that aim to refund the collected toll revenues as lump-sum transfers\nto users. Although revenue refunding has been extensively studied for over\nthree decades, there has been no thorough characterization of how such schemes\ncan be designed to simultaneously achieve system efficiency and equity\nobjectives. In this work, we bridge this gap through the study of\n\\emph{congestion pricing and revenue refunding} (CPRR) schemes in non-atomic\ncongestion games. We first develop CPRR schemes, which in comparison to the\nuntolled case, simultaneously increase system efficiency without worsening\nwealth inequality, while being \\emph{user-favorable}: irrespective of their\ninitial wealth or values-of-time (which may differ across users), users would\nexperience a lower travel cost after the implementation of the proposed scheme.\nWe then characterize the set of optimal user-favorable CPRR schemes that\nsimultaneously maximize system efficiency and minimize wealth inequality.\nFinally, we provide a concrete methodology for computing optimal CPRR schemes\nand also highlight additional equilibrium properties of these schemes under\ndifferent models of user behavior. Overall, our work demonstrates that through\nappropriate refunding policies we can design user-favorable CPRR schemes that\nmaximize system efficiency while reducing wealth inequality.",
    "published_date": "2021-06-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.10407v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.10374v1",
    "title": "Towards a Query-Optimal and Time-Efficient Algorithm for Clustering with a Faulty Oracle",
    "authors": [
      "Pan Peng",
      "Jiapeng Zhang"
    ],
    "author_ids": [],
    "abstract": "Motivated by applications in crowdsourced entity resolution in database,\nsigned edge prediction in social networks and correlation clustering, Mazumdar\nand Saha [NIPS 2017] proposed an elegant theoretical model for studying\nclustering with a faulty oracle. In this model, given a set of $n$ items which\nbelong to $k$ unknown groups (or clusters), our goal is to recover the clusters\nby asking pairwise queries to an oracle. This oracle can answer the query that\n``do items $u$ and $v$ belong to the same cluster?''. However, the answer to\neach pairwise query errs with probability $\\varepsilon$, for some\n$\\varepsilon\\in(0,\\frac12)$. Mazumdar and Saha provided two algorithms under\nthis model: one algorithm is query-optimal while time-inefficient (i.e.,\nrunning in quasi-polynomial time), the other is time efficient (i.e., in\npolynomial time) while query-suboptimal. Larsen, Mitzenmacher and Tsourakakis\n[WWW 2020] then gave a new time-efficient algorithm for the special case of $2$\nclusters, which is query-optimal if the bias $\\delta:=1-2\\varepsilon$ of the\nmodel is large. It was left as an open question whether one can obtain a\nquery-optimal, time-efficient algorithm for the general case of $k$ clusters\nand other regimes of $\\delta$.\n  In this paper, we make progress on the above question and provide a\ntime-efficient algorithm with nearly-optimal query complexity (up to a factor\nof $O(\\log^2 n)$) for all constant $k$ and any $\\delta$ in the regime when\ninformation-theoretic recovery is possible. Our algorithm is built on a\nconnection to the stochastic block model.",
    "published_date": "2021-06-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.DS",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.10374v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.10199v5",
    "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models",
    "authors": [
      "Elad Ben Zaken",
      "Shauli Ravfogel",
      "Yoav Goldberg"
    ],
    "author_ids": [],
    "abstract": "We introduce BitFit, a sparse-finetuning method where only the bias-terms of\nthe model (or a subset of them) are being modified. We show that with\nsmall-to-medium training data, applying BitFit on pre-trained BERT models is\ncompetitive with (and sometimes better than) fine-tuning the entire model. For\nlarger data, the method is competitive with other sparse fine-tuning methods.\nBesides their practical utility, these findings are relevant for the question\nof understanding the commonly-used process of finetuning: they support the\nhypothesis that finetuning is mainly about exposing knowledge induced by\nlanguage-modeling training, rather than learning new task-specific linguistic\nknowledge.",
    "published_date": "2021-06-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.10199v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.10165v2",
    "title": "The Principles of Deep Learning Theory",
    "authors": [
      "Daniel A. Roberts",
      "Sho Yaida",
      "Boris Hanin"
    ],
    "author_ids": [],
    "abstract": "This book develops an effective theory approach to understanding deep neural\nnetworks of practical relevance. Beginning from a first-principles\ncomponent-level picture of networks, we explain how to determine an accurate\ndescription of the output of trained networks by solving layer-to-layer\niteration equations and nonlinear learning dynamics. A main result is that the\npredictions of networks are described by nearly-Gaussian distributions, with\nthe depth-to-width aspect ratio of the network controlling the deviations from\nthe infinite-width Gaussian description. We explain how these effectively-deep\nnetworks learn nontrivial representations from training and more broadly\nanalyze the mechanism of representation learning for nonlinear models. From a\nnearly-kernel-methods perspective, we find that the dependence of such models'\npredictions on the underlying learning algorithm can be expressed in a simple\nand universal way. To obtain these results, we develop the notion of\nrepresentation group flow (RG flow) to characterize the propagation of signals\nthrough the network. By tuning networks to criticality, we give a practical\nsolution to the exploding and vanishing gradient problem. We further explain\nhow RG flow leads to near-universal behavior and lets us categorize networks\nbuilt from different activation functions into universality classes.\nAltogether, we show that the depth-to-width ratio governs the effective model\ncomplexity of the ensemble of trained networks. By using information-theoretic\ntechniques, we estimate the optimal aspect ratio at which we expect the network\nto be practically most useful and show how residual connections can be used to\npush this scale to arbitrary depths. With these tools, we can learn in detail\nabout the inductive bias of architectures, hyperparameters, and optimizers.",
    "published_date": "2021-06-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "hep-th",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.10165v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.10097v2",
    "title": "The Graphical Traveling Salesperson Problem has no Integer Programming Formulation in the Original Space",
    "authors": [
      "Matthias Walter"
    ],
    "author_ids": [],
    "abstract": "The Graphical Traveling Salesperson Problem (GTSP) is the problem of\nassigning, for a given weighted graph, a nonnegative number $x_e$ each edge $e$\nsuch that the induced multi-subgraph is of minimum weight among those that are\nspanning, connected and Eulerian. Naturally, known mixed-integer programming\nformulations use integer variables $x_e$ in addition to others. Denis Naddef\nposed the challenge of finding a (reasonably simple) mixed-integer programming\nformulation that has integrality constraints only on these edge variables.\nRecently, Carr and Simonetti (IPCO 2021) showed that such a formulation cannot\nconsist of polynomial-time certifyiable inequality classes unless\n$\\mathsf{NP}=\\mathsf{coNP}$. In this note we establish a more rigorous result,\nnamely that no such MIP formulation exists at all.",
    "published_date": "2021-06-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DM",
      "math.OC",
      "90C11",
      "G.1.6; G.2.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.10097v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.10084v1",
    "title": "Subjective Bias in Abstractive Summarization",
    "authors": [
      "Lei Li",
      "Wei Liu",
      "Marina Litvak",
      "Natalia Vanetik",
      "Jiacheng Pei",
      "Yinan Liu",
      "Siya Qi"
    ],
    "author_ids": [],
    "abstract": "Due to the subjectivity of the summarization, it is a good practice to have\nmore than one gold summary for each training document. However, many modern\nlarge-scale abstractive summarization datasets have only one-to-one samples\nwritten by different human with different styles. The impact of this phenomenon\nis understudied. We formulate the differences among possible multiple\nexpressions summarizing the same content as subjective bias and examine the\nrole of this bias in the context of abstractive summarization. In this paper a\nlightweight and effective method to extract the feature embeddings of\nsubjective styles is proposed. Results of summarization models trained on\nstyle-clustered datasets show that there are certain types of styles that lead\nto better convergence, abstraction and generalization. The reproducible code\nand generated summaries are available online.",
    "published_date": "2021-06-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.10084v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.04022v2",
    "title": "Immune Moral Models? Pro-Social Rule Breaking as a Moral Enhancement Approach for Ethical AI",
    "authors": [
      "Rajitha Ramanayake",
      "Philipp Wicke",
      "Vivek Nallur"
    ],
    "author_ids": [],
    "abstract": "We are moving towards a future where Artificial Intelligence (AI) based\nagents make many decisions on behalf of humans. From healthcare decision making\nto social media censoring, these agents face problems, and make decisions with\nethical and societal implications. Ethical behaviour is a critical\ncharacteristic that we would like in a human-centric AI. A common observation\nin human-centric industries, like the service industry and healthcare, is that\ntheir professionals tend to break rules, if necessary, for pro-social reasons.\nThis behaviour among humans is defined as pro-social rule breaking. To make AI\nagents more human centric, we argue that there is a need for a mechanism that\nhelps AI agents identify when to break rules set by their designers. To\nunderstand when AI agents need to break rules, we examine the conditions under\nwhich humans break rules for pro-social reasons. In this paper, we present a\nstudy that introduces a 'vaccination strategy dilemma' to human participants\nand analyses their responses. In this dilemma, one needs to decide whether they\nwould distribute Covid-19 vaccines only to members of a high-risk group (follow\nthe enforced rule) or, in selected cases, administer the vaccine to a few\nsocial influencers (break the rule), which might yield an overall greater\nbenefit to society. The results of the empirical study suggest a relationship\nbetween stakeholder utilities and pro-social rule breaking (PSRB), which\nneither deontological nor utilitarian ethics completely explain. Finally, the\npaper discusses the design characteristics of an ethical agent capable of PSRB\nand the future research directions on PSRB in the AI realm. We hope that this\nwill inform the design of future AI agents, and their decision-making\nbehaviour.",
    "published_date": "2021-06-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.04022v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.09762v2",
    "title": "Causal Bias Quantification for Continuous Treatments",
    "authors": [
      "Gianluca Detommaso",
      "Michael Brückner",
      "Philip Schulz",
      "Victor Chernozhukov"
    ],
    "author_ids": [],
    "abstract": "We extend the definition of the marginal causal effect to the continuous\ntreatment setting and develop a novel characterization of causal bias in the\nframework of structural causal models. We prove that our derived bias\nexpression is zero if, and only if, the causal effect is identifiable via\ncovariate adjustment. We show that under some restrictions on the structural\nequations, the causal bias can be estimated efficiently and allows for causal\nregularization of predictive probabilistic models. We demonstrate the\neffectiveness of our method for causal bias quantification in various settings\nwhere (not) controlling for certain covariates would introduce causal bias.",
    "published_date": "2021-06-17T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ME",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.09762v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.09645v2",
    "title": "Prototypical Graph Contrastive Learning",
    "authors": [
      "Shuai Lin",
      "Pan Zhou",
      "Zi-Yuan Hu",
      "Shuojia Wang",
      "Ruihui Zhao",
      "Yefeng Zheng",
      "Liang Lin",
      "Eric Xing",
      "Xiaodan Liang"
    ],
    "author_ids": [],
    "abstract": "Graph-level representations are critical in various real-world applications,\nsuch as predicting the properties of molecules. But in practice, precise graph\nannotations are generally very expensive and time-consuming. To address this\nissue, graph contrastive learning constructs instance discrimination task which\npulls together positive pairs (augmentation pairs of the same graph) and pushes\naway negative pairs (augmentation pairs of different graphs) for unsupervised\nrepresentation learning. However, since for a query, its negatives are\nuniformly sampled from all graphs, existing methods suffer from the critical\nsampling bias issue, i.e., the negatives likely having the same semantic\nstructure with the query, leading to performance degradation. To mitigate this\nsampling bias issue, in this paper, we propose a Prototypical Graph Contrastive\nLearning (PGCL) approach. Specifically, PGCL models the underlying semantic\nstructure of the graph data via clustering semantically similar graphs into the\nsame group, and simultaneously encourages the clustering consistency for\ndifferent augmentations of the same graph. Then given a query, it performs\nnegative sampling via drawing the graphs from those clusters that differ from\nthe cluster of query, which ensures the semantic difference between query and\nits negative samples. Moreover, for a query, PGCL further reweights its\nnegative samples based on the distance between their prototypes (cluster\ncentroids) and the query prototype such that those negatives having moderate\nprototype distance enjoy relatively large weights. This reweighting strategy is\nproved to be more effective than uniform sampling. Experimental results on\nvarious graph benchmarks testify the advantages of our PGCL over\nstate-of-the-art methods. Code is publicly available at\nhttps://github.com/ha-lins/PGCL.",
    "published_date": "2021-06-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.09645v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.09627v1",
    "title": "Towards Prevention of Sportsmen Burnout: Formal Analysis of Sub-Optimal Tournament Scheduling",
    "authors": [
      "Syed Rameez Naqvi",
      "Adnan Ahmad",
      "S. M. Riazul Islam",
      "Tallha Akram",
      "M. Abdullah-Al-Wadud",
      "Atif Alamri"
    ],
    "author_ids": [],
    "abstract": "Scheduling a sports tournament is a complex optimization problem, which\nrequires a large number of hard constraints to satisfy. Despite the\navailability of several such constraints in the literature, there remains a gap\nsince most of the new sports events pose their own unique set of requirements,\nand demand novel constraints. Specifically talking of the strictly time bound\nevents, ensuring fairness between the different teams in terms of their rest\ndays, traveling, and the number of successive games they play, becomes a\ndifficult task to resolve, and demands attention. In this work, we present a\nsimilar situation with a recently played sports event, where a suboptimal\nschedule favored some of the sides more than the others. We introduce various\ncompetitive parameters to draw a fairness comparison between the sides and\npropose a weighting criterion to point out the sides that enjoyed this schedule\nmore than the others. Furthermore, we use root mean squared error between an\nideal schedule and the actual ones for each side to determine unfairness in the\ndistribution of rest days across their entire schedules. The latter is crucial,\nsince successively playing a large number of games may lead to sportsmen\nburnout, which must be prevented.",
    "published_date": "2021-06-17T00:00:00",
    "year": 2021,
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.09627v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.09586v1",
    "title": "Prevalence and Propagation of Fake News",
    "authors": [
      "Banafsheh Behzad",
      "Bhavana Bheem",
      "Daniela Elizondo",
      "Deyana Marsh",
      "Susan Martonosi"
    ],
    "author_ids": [],
    "abstract": "In recent years, scholars have raised concerns on the effects that unreliable\nnews, or \"fake news,\" has on our political sphere, and our democracy as a\nwhole. For example, the propagation of fake news on social media is widely\nbelieved to have influenced the outcome of national elections, including the\n2016 U.S. Presidential Election, and the 2020 COVID-19 pandemic. What drives\nthe propagation of fake news on an individual level, and which interventions\ncould effectively reduce the propagation rate? Our model disentangles bias from\ntruthfulness of an article and examines the relationship between these two\nparameters and a reader's own beliefs. Using the model, we create policy\nrecommendations for both social media platforms and individual social media\nusers to reduce the spread of untruthful or highly biased news. We recommend\nthat platforms sponsor unbiased truthful news, focus fact-checking efforts on\nmild to moderately biased news, recommend friend suggestions across the\npolitical spectrum, and provide users with reports about the political\nalignment of their feed. We recommend that individual social media users fact\ncheck news that strongly aligns with their political bias and read articles of\nopposing political bias.",
    "published_date": "2021-06-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI",
      "math.OC",
      "stat.AP",
      "90B50",
      "J.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.09586v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.09524v3",
    "title": "Implicit Bias of SGD for Diagonal Linear Networks: a Provable Benefit of Stochasticity",
    "authors": [
      "Scott Pesme",
      "Loucas Pillaud-Vivien",
      "Nicolas Flammarion"
    ],
    "author_ids": [],
    "abstract": "Understanding the implicit bias of training algorithms is of crucial\nimportance in order to explain the success of overparametrised neural networks.\nIn this paper, we study the dynamics of stochastic gradient descent over\ndiagonal linear networks through its continuous time version, namely stochastic\ngradient flow. We explicitly characterise the solution chosen by the stochastic\nflow and prove that it always enjoys better generalisation properties than that\nof gradient flow. Quite surprisingly, we show that the convergence speed of the\ntraining loss controls the magnitude of the biasing effect: the slower the\nconvergence, the better the bias. To fully complete our analysis, we provide\nconvergence guarantees for the dynamics. We also give experimental results\nwhich support our theoretical claims. Our findings highlight the fact that\nstructured noise can induce better generalisation and they help explain the\ngreater performances observed in practice of stochastic gradient descent over\ngradient descent.",
    "published_date": "2021-06-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.09524v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.09467v1",
    "title": "Algorithmic Bias and Data Bias: Understanding the Relation between Distributionally Robust Optimization and Data Curation",
    "authors": [
      "Agnieszka Słowik",
      "Léon Bottou"
    ],
    "author_ids": [],
    "abstract": "Machine learning systems based on minimizing average error have been shown to\nperform inconsistently across notable subsets of the data, which is not exposed\nby a low average error for the entire dataset. In consequential social and\neconomic applications, where data represent people, this can lead to\ndiscrimination of underrepresented gender and ethnic groups. Given the\nimportance of bias mitigation in machine learning, the topic leads to\ncontentious debates on how to ensure fairness in practice (data bias versus\nalgorithmic bias). Distributionally Robust Optimization (DRO) seemingly\naddresses this problem by minimizing the worst expected risk across\nsubpopulations. We establish theoretical results that clarify the relation\nbetween DRO and the optimization of the same loss averaged on an adequately\nweighted training dataset. The results cover finite and infinite number of\ntraining distributions, as well as convex and non-convex loss functions. We\nshow that neither DRO nor curating the training set should be construed as a\ncomplete solution for bias mitigation: in the same way that there is no\nuniversally robust training set, there is no universal way to setup a DRO\nproblem and ensure a socially acceptable set of results. We then leverage these\ninsights to provide a mininal set of practical recommendations for addressing\nbias with DRO. Finally, we discuss ramifications of our results in other\nrelated applications of DRO, using an example of adversarial robustness. Our\nresults show that there is merit to both the algorithm-focused and the\ndata-focused side of the bias debate, as long as arguments in favor of these\npositions are precisely qualified and backed by relevant mathematics known\ntoday.",
    "published_date": "2021-06-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.09467v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.09462v3",
    "title": "pysentimiento: A Python Toolkit for Opinion Mining and Social NLP tasks",
    "authors": [
      "Juan Manuel Pérez",
      "Mariela Rajngewerc",
      "Juan Carlos Giudici",
      "Damián A. Furman",
      "Franco Luque",
      "Laura Alonso Alemany",
      "María Vanina Martínez"
    ],
    "author_ids": [],
    "abstract": "In recent years, the extraction of opinions and information from\nuser-generated text has attracted a lot of interest, largely due to the\nunprecedented volume of content in Social Media. However, social researchers\nface some issues in adopting cutting-edge tools for these tasks, as they are\nusually behind commercial APIs, unavailable for other languages than English,\nor very complex to use for non-experts. To address these issues, we present\npysentimiento, a comprehensive multilingual Python toolkit designed for opinion\nmining and other Social NLP tasks. This open-source library brings\nstate-of-the-art models for Spanish, English, Italian, and Portuguese in an\neasy-to-use Python library, allowing researchers to leverage these techniques.\nWe present a comprehensive assessment of performance for several pre-trained\nlanguage models across a variety of tasks, languages, and datasets, including\nan evaluation of fairness in the results.",
    "published_date": "2021-06-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.09462v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.09329v1",
    "title": "Network Science, Homophily and Who Reviews Who in the Linux Kernel?",
    "authors": [
      "José Apolinário Teixeira",
      "Ville Leppänen",
      "Sami Hyrynsalmi"
    ],
    "author_ids": [],
    "abstract": "In this research, we investigate peer review in the development of Linux by\ndrawing on network theory and network analysis. We frame an analytical model\nwhich integrates the sociological principle of homophily (i.e., the relational\ntendency of individuals to establish relationships with similar others) with\nprior research on peer-review in general and open-source software in\nparticular. We found a relatively strong homophily tendency for maintainers to\nreview other maintainers, but a comparable tendency is surprisingly absent\nregarding developers' organizational affiliation. Such results mirror the\ndocumented norms, beliefs, values, processes, policies, and social hierarchies\nthat characterize the Linux kernel development. Our results underline the power\nof generative mechanisms from network theory to explain the evolution of peer\nreview networks. Regarding practitioners' concern over the Linux\ncommercialization trend, no relational bias in peer review was found albeit the\nincreasing involvement of firms.",
    "published_date": "2021-06-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SE",
      "cs.SI",
      "D.2.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.09329v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.09316v3",
    "title": "Optimized Power Control Design for Over-the-Air Federated Edge Learning",
    "authors": [
      "Xiaowen Cao",
      "Guangxu Zhu",
      "Jie Xu",
      "Zhiqin Wang",
      "Shuguang Cui"
    ],
    "author_ids": [],
    "abstract": "This paper investigates the transmission power control in over-the-air\nfederated edge learning (Air-FEEL) system. Different from conventional power\ncontrol designs (e.g., to minimize the individual mean squared error (MSE) of\nthe over-the-air aggregation at each round), we consider a new power control\ndesign aiming at directly maximizing the convergence speed. Towards this end,\nwe first analyze the convergence behavior of Air-FEEL (in terms of the\noptimality gap) subject to aggregation errors at different communication\nrounds. It is revealed that if the aggregation estimates are unbiased, then the\ntraining algorithm would converge exactly to the optimal point with mild\nconditions; while if they are biased, then the algorithm would converge with an\nerror floor determined by the accumulated estimate bias over communication\nrounds. Next, building upon the convergence results, we optimize the power\ncontrol to directly minimize the derived optimality gaps under both biased and\nunbiased aggregations, subject to a set of average and maximum power\nconstraints at individual edge devices. We transform both problems into convex\nforms, and obtain their structured optimal solutions, both appearing in a form\nof regularized channel inversion, by using the Lagrangian duality method.\nFinally, numerical results show that the proposed power control policies\nachieve significantly faster convergence for Air-FEEL, as compared with\nbenchmark policies with fixed power transmission or conventional MSE\nminimization.",
    "published_date": "2021-06-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IT",
      "eess.SP",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.09316v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.09259v2",
    "title": "A Random CNN Sees Objects: One Inductive Bias of CNN and Its Applications",
    "authors": [
      "Yun-Hao Cao",
      "Jianxin Wu"
    ],
    "author_ids": [],
    "abstract": "This paper starts by revealing a surprising finding: without any learning, a\nrandomly initialized CNN can localize objects surprisingly well. That is, a CNN\nhas an inductive bias to naturally focus on objects, named as Tobias (\"The\nobject is at sight\") in this paper. This empirical inductive bias is further\nanalyzed and successfully applied to self-supervised learning (SSL). A CNN is\nencouraged to learn representations that focus on the foreground object, by\ntransforming every image into various versions with different backgrounds,\nwhere the foreground and background separation is guided by Tobias.\nExperimental results show that the proposed Tobias significantly improves\ndownstream tasks, especially for object detection. This paper also shows that\nTobias has consistent improvements on training sets of different sizes, and is\nmore resilient to changes in image augmentations. Code is available at\nhttps://github.com/CupidJay/Tobias.",
    "published_date": "2021-06-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.09259v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.09258v1",
    "title": "Knowledge Graphs and Machine Learning in biased C4I applications",
    "authors": [
      "Evangelos Paparidis",
      "Konstantinos Kotis"
    ],
    "author_ids": [],
    "abstract": "This paper introduces our position on the critical issue of bias that\nrecently appeared in AI applications. Specifically, we discuss the combination\nof current technologies used in AI applications i.e., Machine Learning and\nKnowledge Graphs, and point to their involvement in (de)biased applications of\nthe C4I domain. Although this is a wider problem that currently emerges from\ndifferent application domains, bias appears more critical in C4I than in others\ndue to its security-related nature. While proposing certain actions to be taken\ntowards debiasing C4I applications, we acknowledge the immature aspect of this\ntopic within the Knowledge Graph and Semantic Web communities.",
    "published_date": "2021-06-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.09258v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2107.14077v2",
    "title": "A Fair and Ethical Healthcare Artificial Intelligence System for Monitoring Driver Behavior and Preventing Road Accidents",
    "authors": [
      "Soraia Oueida",
      "Soaad Hossain",
      "Yehia Kotb",
      "Syed Ishtiaque Ahmed"
    ],
    "author_ids": [],
    "abstract": "This paper presents a new approach to prevent transportation accidents and\nmonitor driver's behavior using a healthcare AI system that incorporates\nfairness and ethics. Dangerous medical cases and unusual behavior of the driver\nare detected. Fairness algorithm is approached in order to improve\ndecision-making and address ethical issues such as privacy issues, and to\nconsider challenges that appear in the wild within AI in healthcare and\ndriving. A healthcare professional will be alerted about any unusual activity,\nand the driver's location when necessary, is provided in order to enable the\nhealthcare professional to immediately help to the unstable driver. Therefore,\nusing the healthcare AI system allows for accidents to be predicted and thus\nprevented and lives may be saved based on the built-in AI system inside the\nvehicle which interacts with the ER system.",
    "published_date": "2021-06-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "68Txx",
      "I.2.1; J.3; H.4.2; J.7; K.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.14077v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.08864v1",
    "title": "Multi-Class Classification from Single-Class Data with Confidences",
    "authors": [
      "Yuzhou Cao",
      "Lei Feng",
      "Senlin Shu",
      "Yitian Xu",
      "Bo An",
      "Gang Niu",
      "Masashi Sugiyama"
    ],
    "author_ids": [],
    "abstract": "Can we learn a multi-class classifier from only data of a single class? We\nshow that without any assumptions on the loss functions, models, and\noptimizers, we can successfully learn a multi-class classifier from only data\nof a single class with a rigorous consistency guarantee when confidences (i.e.,\nthe class-posterior probabilities for all the classes) are available.\nSpecifically, we propose an empirical risk minimization framework that is\nloss-/model-/optimizer-independent. Instead of constructing a boundary between\nthe given class and other classes, our method can conduct discriminative\nclassification between all the classes even if no data from the other classes\nare provided. We further theoretically and experimentally show that our method\ncan be Bayes-consistent with a simple modification even if the provided\nconfidences are highly noisy. Then, we provide an extension of our method for\nthe case where data from a subset of all the classes are available.\nExperimental results demonstrate the effectiveness of our methods.",
    "published_date": "2021-06-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.08864v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.08814v2",
    "title": "Silhouettes and quasi residual plots for neural nets and tree-based classifiers",
    "authors": [
      "Jakob Raymaekers",
      "Peter J. Rousseeuw"
    ],
    "author_ids": [],
    "abstract": "Classification by neural nets and by tree-based methods are powerful tools of\nmachine learning. There exist interesting visualizations of the inner workings\nof these and other classifiers. Here we pursue a different goal, which is to\nvisualize the cases being classified, either in training data or in test data.\nAn important aspect is whether a case has been classified to its given class\n(label) or whether the classifier wants to assign it to different class. This\nis reflected in the (conditional and posterior) probability of the alternative\nclass (PAC). A high PAC indicates label bias, i.e. the possibility that the\ncase was mislabeled. The PAC is used to construct a silhouette plot which is\nsimilar in spirit to the silhouette plot for cluster analysis (Rousseeuw,\n1987). The average silhouette width can be used to compare different\nclassifications of the same dataset. We will also draw quasi residual plots of\nthe PAC versus a data feature, which may lead to more insight in the data. One\nof these data features is how far each case lies from its given class. The\ngraphical displays are illustrated and interpreted on benchmark data sets\ncontaining images, mixed features, and tweets.",
    "published_date": "2021-06-16T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.08814v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.08812v2",
    "title": "Costs and Benefits of Fair Regression",
    "authors": [
      "Han Zhao"
    ],
    "author_ids": [],
    "abstract": "Real-world applications of machine learning tools in high-stakes domains are\noften regulated to be fair, in the sense that the predicted target should\nsatisfy some quantitative notion of parity with respect to a protected\nattribute. However, the exact tradeoff between fairness and accuracy with a\nreal-valued target is not entirely clear. In this paper, we characterize the\ninherent tradeoff between statistical parity and accuracy in the regression\nsetting by providing a lower bound on the error of any fair regressor. Our\nlower bound is sharp, algorithm-independent, and admits a simple\ninterpretation: when the moments of the target differ between groups, any fair\nalgorithm has to make an error on at least one of the groups. We further extend\nthis result to give a lower bound on the joint error of any (approximately)\nfair algorithm, using the Wasserstein distance to measure the quality of the\napproximation. With our novel lower bound, we also show that the price paid by\na fair regressor that does not take the protected attribute as input is less\nthan that of a fair regressor with explicit access to the protected attribute.\nOn the upside, we establish the first connection between individual fairness,\naccuracy parity, and the Wasserstein distance by showing that if a regressor is\nindividually fair, it also approximately verifies the accuracy parity, where\nthe gap is given by the Wasserstein distance between the two groups. Inspired\nby our theoretical results, we develop a practical algorithm for fair\nregression through the lens of representation learning, and conduct experiments\non a real-world dataset to corroborate our findings.",
    "published_date": "2021-06-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.08812v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.08799v2",
    "title": "Regularization-Induced Bias and Consistency in Recursive Least Squares",
    "authors": [
      "Brian Lai",
      "Syed Aseem Ul Islam",
      "Dennis S. Bernstein"
    ],
    "author_ids": [],
    "abstract": "Within the context of recursive least squares (RLS) parameter estimation, the\ngoal of the present paper is to study the effect of regularization-induced bias\non the transient and asymptotic accuracy of the parameter estimates. We\nconsider this question in three stages. First, we consider regression with\nrandom data, in which case persistency is guaranteed. Next, we apply RLS to\nfinite-impulse-response (FIR) system identification and, finally, to\ninfinite-impulse-response (IIR) system identification. For each case, we relate\nthe condition number of the regressor matrix to the transient response and rate\nof convergence of the parameter estimates.",
    "published_date": "2021-06-16T00:00:00",
    "year": 2021,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.08799v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.08737v1",
    "title": "Benefits, Challenges and Contributors to Success for National eHealth Systems Implementation: A Scoping Review",
    "authors": [
      "James Scheibner",
      "Marcello Ienca",
      "Joanna Sleigh",
      "Effy Vayena"
    ],
    "author_ids": [],
    "abstract": "Our scoping review aims to assess what legal, ethical, and socio-technical\nfactors contribute or inhibit the success of national eHealth system\nimplementations. In addition, our review seeks to describe the characteristics\nand benefits of eHealth systems. We conducted a scoping review of literature\npublished in English between January 2000 and 2020 using a keyword search on\nfive databases; PubMed, Scopus, Web of Science, IEEEXplore, and ProQuest. After\nremoval of duplicates, abstract screening and full-text filtering, 86 articles\nwere included from 8276 search results. We identified 17 stakeholder groups, 6\neHealth Systems areas, and 15 types of legal regimes and standards. In-depth\ntextual analysis revealed challenges mainly in implementation, followed by\nethico-legal and data related aspects. Key factors influencing success include\npromoting trust of the system, ensuring wider acceptance amongst users,\nreconciling the system with legal requirements and ensuring an adaptable\ntechnical platform. Results revealed support for decentralised implementations\nbecause they carry less implementation and engagement challenges than\ncentralised ones. Simultaneously, due to decentralised systems interoperability\nissues, federated implementations (with a set of national standards) might be\npreferable. This study identifies the primary socio-technical, legal and\nethical factors that challenge and contribute to the success of eHealth system\nimplementations. This study also describes the complexities and characteristics\nof existing eHealth implementation programs, and surmises suggested guidance\nfor resolving the identified challenges.",
    "published_date": "2021-06-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.08737v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.08729v1",
    "title": "Modeling and Accomplishing the BEREC Network Neutrality Policy",
    "authors": [
      "David S. Barreto",
      "Rafael F. Reale",
      "Joberto S. B. Martins"
    ],
    "author_ids": [],
    "abstract": "Network neutrality (NN) is a principle of equal treatment of data in network\ninfrastructures with fairness and universality being the primary outcomes of\nthe NN management practice. For networks, the accomplishment of NN management\npractice is essential to deal with heterogeneous user requirements and the\never-increasing data traffic. Current tools and methods address the NN problem\nby detecting network neutrality violations and detecting traffic\ndifferentiation. This paper proposes the NN-PCM (Network Neutrality Policy\nConformance Module) that deploys the BEREC network neutrality policy using a\nbandwidth allocation model (BAM). The NN-PCM new approach allocates bandwidth\nto network users and accomplishes the BEREC NN policy concomitantly. Network\nneutrality is achieved by grouping users with similar traffic requirements in\nclasses and leveraging the bandwidth allocation model's characteristics. The\nconceptual analysis and simulation results indicate that NN-PCM allocates\nbandwidth to users and accomplishes BEREC network neutrality conformance by\ndesign with transparent, non-discriminatory, exceptional, and proportional\nmanagement practices.",
    "published_date": "2021-06-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.NI",
      "cs.CY",
      "cs.PF",
      "68 (Primary) 68R12 (Secondary)",
      "I.6; C.2.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.08729v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.08684v2",
    "title": "Reliability of Content and Echo Chambers on YouTube during the COVID-19 Debate",
    "authors": [
      "Niccolò Di Marco",
      "Matteo Cinelli",
      "Walter Quattrociocchi"
    ],
    "author_ids": [],
    "abstract": "The spread of inaccurate and misleading information may alter behaviours and\ncomplicate crisis management, especially during an emergency like the COVID-19\npandemic. This paper aims to investigate information diffusion during the\nCOVID-19 pandemic by evaluating news consumption on YouTube. First, we analyse\nmore than 2 million users' engagement with 13,000 videos released by 68 YouTube\nchannels, labelled with a political bias and fact-checking index. Then, we\nstudy the relationship between each user\\~Os political preference and their\nconsumption of questionable (i.e., poorly fact-checked) and reliable\ninformation. Our results, quantified using measures from information theory,\nprovide evidence for the existence of echo chambers across two dimensions\nrepresented by political bias and the trustworthiness of information channels.\nWe observe that the echo chamber structure cannot be reproduced after properly\nrandomising the users' interaction patterns. Moreover, we observe a relation\nbetween the political bias of users and their tendency to consume highly\nquestionable news.",
    "published_date": "2021-06-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.08684v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.08680v1",
    "title": "Evaluating Gender Bias in Hindi-English Machine Translation",
    "authors": [
      "Gauri Gupta",
      "Krithika Ramesh",
      "Sanjay Singh"
    ],
    "author_ids": [],
    "abstract": "With language models being deployed increasingly in the real world, it is\nessential to address the issue of the fairness of their outputs. The word\nembedding representations of these language models often implicitly draw\nunwanted associations that form a social bias within the model. The nature of\ngendered languages like Hindi, poses an additional problem to the\nquantification and mitigation of bias, owing to the change in the form of the\nwords in the sentence, based on the gender of the subject. Additionally, there\nis sparse work done in the realm of measuring and debiasing systems for Indic\nlanguages. In our work, we attempt to evaluate and quantify the gender bias\nwithin a Hindi-English machine translation system. We implement a modified\nversion of the existing TGBI metric based on the grammatical considerations for\nHindi. We also compare and contrast the resulting bias measurements across\nmultiple metrics for pre-trained embeddings and the ones learned by our machine\ntranslation model.",
    "published_date": "2021-06-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.08680v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.08652v2",
    "title": "Maxmin-Fair Ranking: Individual Fairness under Group-Fairness Constraints",
    "authors": [
      "David Garcia-Soriano",
      "Francesco Bonchi"
    ],
    "author_ids": [],
    "abstract": "We study a novel problem of fairness in ranking aimed at minimizing the\namount of individual unfairness introduced when enforcing group-fairness\nconstraints. Our proposal is rooted in the distributional maxmin fairness\ntheory, which uses randomization to maximize the expected satisfaction of the\nworst-off individuals. We devise an exact polynomial-time algorithm to find\nmaxmin-fair distributions of general search problems (including, but not\nlimited to, ranking), and show that our algorithm can produce rankings which,\nwhile satisfying the given group-fairness constraints, ensure that the maximum\npossible value is brought to individuals.",
    "published_date": "2021-06-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.08652v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.08527v2",
    "title": "FAIR: Fairness-Aware Information Retrieval Evaluation",
    "authors": [
      "Ruoyuan Gao",
      "Yingqiang Ge",
      "Chirag Shah"
    ],
    "author_ids": [],
    "abstract": "With the emerging needs of creating fairness-aware solutions for search and\nrecommendation systems, a daunting challenge exists of evaluating such\nsolutions. While many of the traditional information retrieval (IR) metrics can\ncapture the relevance, diversity, and novelty for the utility with respect to\nusers, they are not suitable for inferring whether the presented results are\nfair from the perspective of responsible information exposure. On the other\nhand, existing fairness metrics do not account for user utility or do not\nmeasure it adequately. To address this problem, we propose a new metric called\nFAIR. By unifying standard IR metrics and fairness measures into an integrated\nmetric, this metric offers a new perspective for evaluating fairness-aware\nranking results. Based on this metric, we developed an effective ranking\nalgorithm that jointly optimized user utility and fairness. The experimental\nresults showed that our FAIR metric could highlight results with good user\nutility and fair information exposure. We showed how FAIR related to a set of\nexisting utility and fairness metrics and demonstrated the effectiveness of our\nFAIR-based algorithm. We believe our work opens up a new direction of pursuing\na metric for evaluating and implementing the FAIR systems.",
    "published_date": "2021-06-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.08527v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.08523v1",
    "title": "ECKPN: Explicit Class Knowledge Propagation Network for Transductive Few-shot Learning",
    "authors": [
      "Chaofan Chen",
      "Xiaoshan Yang",
      "Changsheng Xu",
      "Xuhui Huang",
      "Zhe Ma"
    ],
    "author_ids": [],
    "abstract": "Recently, the transductive graph-based methods have achieved great success in\nthe few-shot classification task. However, most existing methods ignore\nexploring the class-level knowledge that can be easily learned by humans from\njust a handful of samples. In this paper, we propose an Explicit Class\nKnowledge Propagation Network (ECKPN), which is composed of the comparison,\nsqueeze and calibration modules, to address this problem. Specifically, we\nfirst employ the comparison module to explore the pairwise sample relations to\nlearn rich sample representations in the instance-level graph. Then, we squeeze\nthe instance-level graph to generate the class-level graph, which can help\nobtain the class-level visual knowledge and facilitate modeling the relations\nof different classes. Next, the calibration module is adopted to characterize\nthe relations of the classes explicitly to obtain the more discriminative\nclass-level knowledge representations. Finally, we combine the class-level\nknowledge with the instance-level sample representations to guide the inference\nof the query samples. We conduct extensive experiments on four few-shot\nclassification benchmarks, and the experimental results show that the proposed\nECKPN significantly outperforms the state-of-the-art methods.",
    "published_date": "2021-06-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.08523v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.08503v2",
    "title": "Understanding and Evaluating Racial Biases in Image Captioning",
    "authors": [
      "Dora Zhao",
      "Angelina Wang",
      "Olga Russakovsky"
    ],
    "author_ids": [],
    "abstract": "Image captioning is an important task for benchmarking visual reasoning and\nfor enabling accessibility for people with vision impairments. However, as in\nmany machine learning settings, social biases can influence image captioning in\nundesirable ways. In this work, we study bias propagation pathways within image\ncaptioning, focusing specifically on the COCO dataset. Prior work has analyzed\ngender bias in captions using automatically-derived gender labels; here we\nexamine racial and intersectional biases using manual annotations. Our first\ncontribution is in annotating the perceived gender and skin color of 28,315 of\nthe depicted people after obtaining IRB approval. Using these annotations, we\ncompare racial biases present in both manual and automatically-generated image\ncaptions. We demonstrate differences in caption performance, sentiment, and\nword choice between images of lighter versus darker-skinned people. Further, we\nfind the magnitude of these differences to be greater in modern captioning\nsystems compared to older ones, thus leading to concerns that without proper\nconsideration and mitigation these differences will only become increasingly\nprevalent. Code and data is available at\nhttps://princetonvisualai.github.io/imagecaptioning-bias .",
    "published_date": "2021-06-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.08503v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.10241v1",
    "title": "An Analysis of the Deployment of Models Trained on Private Tabular Synthetic Data: Unexpected Surprises",
    "authors": [
      "Mayana Pereira",
      "Meghana Kshirsagar",
      "Sumit Mukherjee",
      "Rahul Dodhia",
      "Juan Lavista Ferres"
    ],
    "author_ids": [],
    "abstract": "Diferentially private (DP) synthetic datasets are a powerful approach for\ntraining machine learning models while respecting the privacy of individual\ndata providers. The effect of DP on the fairness of the resulting trained\nmodels is not yet well understood. In this contribution, we systematically\nstudy the effects of differentially private synthetic data generation on\nclassification. We analyze disparities in model utility and bias caused by the\nsynthetic dataset, measured through algorithmic fairness metrics. Our first set\nof results show that although there seems to be a clear negative correlation\nbetween privacy and utility (the more private, the less accurate) across all\ndata synthesizers we evaluated, more privacy does not necessarily imply more\nbias. Additionally, we assess the effects of utilizing synthetic datasets for\nmodel training and model evaluation. We show that results obtained on synthetic\ndata can misestimate the actual model performance when it is deployed on real\ndata. We hence advocate on the need for defining proper testing protocols in\nscenarios where differentially private synthetic datasets are utilized for\nmodel training and evaluation.",
    "published_date": "2021-06-15T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.10241v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.08334v2",
    "title": "Quantum-inspired event reconstruction with Tensor Networks: Matrix Product States",
    "authors": [
      "Jack Y. Araz",
      "Michael Spannowsky"
    ],
    "author_ids": [],
    "abstract": "Tensor Networks are non-trivial representations of high-dimensional tensors,\noriginally designed to describe quantum many-body systems. We show that Tensor\nNetworks are ideal vehicles to connect quantum mechanical concepts to machine\nlearning techniques, thereby facilitating an improved interpretability of\nneural networks. This study presents the discrimination of top quark signal\nover QCD background processes using a Matrix Product State classifier. We show\nthat entanglement entropy can be used to interpret what a network learns, which\ncan be used to reduce the complexity of the network and feature space without\nloss of generality or performance. For the optimisation of the network, we\ncompare the Density Matrix Renormalization Group (DMRG) algorithm to stochastic\ngradient descent (SGD) and propose a joined training algorithm to harness the\nexplainability of DMRG with the efficiency of SGD.",
    "published_date": "2021-06-15T00:00:00",
    "year": 2021,
    "categories": [
      "hep-ph",
      "cs.LG",
      "hep-ex",
      "physics.data-an"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.08334v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.08323v4",
    "title": "VidHarm: A Clip Based Dataset for Harmful Content Detection",
    "authors": [
      "Johan Edstedt",
      "Amanda Berg",
      "Michael Felsberg",
      "Johan Karlsson",
      "Francisca Benavente",
      "Anette Novak",
      "Gustav Grund Pihlgren"
    ],
    "author_ids": [],
    "abstract": "Automatically identifying harmful content in video is an important task with\na wide range of applications. However, there is a lack of professionally\nlabeled open datasets available. In this work VidHarm, an open dataset of 3589\nvideo clips from film trailers annotated by professionals, is presented. An\nanalysis of the dataset is performed, revealing among other things the relation\nbetween clip and trailer level annotations. Audiovisual models are trained on\nthe dataset and an in-depth study of modeling choices conducted. The results\nshow that performance is greatly improved by combining the visual and audio\nmodality, pre-training on large-scale video recognition datasets, and class\nbalanced sampling. Lastly, biases of the trained models are investigated using\ndiscrimination probing.\n  VidHarm is openly available, and further details are available at:\nhttps://vidharm.github.io",
    "published_date": "2021-06-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.08323v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.08315v3",
    "title": "Decentralized Local Stochastic Extra-Gradient for Variational Inequalities",
    "authors": [
      "Aleksandr Beznosikov",
      "Pavel Dvurechensky",
      "Anastasia Koloskova",
      "Valentin Samokhin",
      "Sebastian U Stich",
      "Alexander Gasnikov"
    ],
    "author_ids": [],
    "abstract": "We consider distributed stochastic variational inequalities (VIs) on\nunbounded domains with the problem data that is heterogeneous (non-IID) and\ndistributed across many devices. We make a very general assumption on the\ncomputational network that, in particular, covers the settings of fully\ndecentralized calculations with time-varying networks and centralized\ntopologies commonly used in Federated Learning. Moreover, multiple local\nupdates on the workers can be made for reducing the communication frequency\nbetween the workers. We extend the stochastic extragradient method to this very\ngeneral setting and theoretically analyze its convergence rate in the\nstrongly-monotone, monotone, and non-monotone (when a Minty solution exists)\nsettings. The provided rates explicitly exhibit the dependence on network\ncharacteristics (e.g., mixing time), iteration counter, data heterogeneity,\nvariance, number of devices, and other standard parameters. As a special case,\nour method and analysis apply to distributed stochastic saddle-point problems\n(SPP), e.g., to the training of Deep Generative Adversarial Networks (GANs) for\nwhich decentralized training has been reported to be extremely challenging. In\nexperiments for the decentralized training of GANs we demonstrate the\neffectiveness of our proposed approach.",
    "published_date": "2021-06-15T00:00:00",
    "year": 2021,
    "categories": [
      "math.OC",
      "cs.DC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.08315v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.08259v1",
    "title": "Fairness as Equality of Opportunity: Normative Guidance from Political Philosophy",
    "authors": [
      "Falaah Arif Khan",
      "Eleni Manis",
      "Julia Stoyanovich"
    ],
    "author_ids": [],
    "abstract": "Recent interest in codifying fairness in Automated Decision Systems (ADS) has\nresulted in a wide range of formulations of what it means for an algorithmic\nsystem to be fair. Most of these propositions are inspired by, but inadequately\ngrounded in, political philosophy scholarship. This paper aims to correct that\ndeficit. We introduce a taxonomy of fairness ideals using doctrines of Equality\nof Opportunity (EOP) from political philosophy, clarifying their conceptions in\nphilosophy and the proposed codification in fair machine learning. We arrange\nthese fairness ideals onto an EOP spectrum, which serves as a useful frame to\nguide the design of a fair ADS in a given context.\n  We use our fairness-as-EOP framework to re-interpret the impossibility\nresults from a philosophical perspective, as the in-compatibility between\ndifferent value systems, and demonstrate the utility of the framework with\nseveral real-world and hypothetical examples. Through our EOP-framework we hope\nto answer what it means for an ADS to be fair from a moral and political\nphilosophy standpoint, and to pave the way for similar scholarship from ethics\nand legal experts.",
    "published_date": "2021-06-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.08259v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.08161v4",
    "title": "Contrastive Mixture of Posteriors for Counterfactual Inference, Data Integration and Fairness",
    "authors": [
      "Adam Foster",
      "Árpi Vezér",
      "Craig A Glastonbury",
      "Páidí Creed",
      "Sam Abujudeh",
      "Aaron Sim"
    ],
    "author_ids": [],
    "abstract": "Learning meaningful representations of data that can address challenges such\nas batch effect correction and counterfactual inference is a central problem in\nmany domains including computational biology. Adopting a Conditional VAE\nframework, we show that marginal independence between the representation and a\ncondition variable plays a key role in both of these challenges. We propose the\nContrastive Mixture of Posteriors (CoMP) method that uses a novel misalignment\npenalty defined in terms of mixtures of the variational posteriors to enforce\nthis independence in latent space. We show that CoMP has attractive theoretical\nproperties compared to previous approaches, and we prove counterfactual\nidentifiability of CoMP under additional assumptions. We demonstrate\nstate-of-the-art performance on a set of challenging tasks including aligning\nhuman tumour samples with cancer cell-lines, predicting transcriptome-level\nperturbation responses, and batch correction on single-cell RNA sequencing\ndata. We also find parallels to fair representation learning and demonstrate\nthat CoMP is competitive on a common task in the field.",
    "published_date": "2021-06-15T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG",
      "q-bio.GN"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.08161v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.08077v3",
    "title": "Computer-aided Interpretable Features for Leaf Image Classification",
    "authors": [
      "Jayani P. G. Lakshika",
      "Thiyanga S. Talagala"
    ],
    "author_ids": [],
    "abstract": "Plant species identification is time consuming, costly, and requires lots of\nefforts, and expertise knowledge. In recent, many researchers use deep learning\nmethods to classify plants directly using plant images. While deep learning\nmodels have achieved a great success, the lack of interpretability limit their\nwidespread application. To overcome this, we explore the use of interpretable,\nmeasurable and computer-aided features extracted from plant leaf images. Image\nprocessing is one of the most challenging, and crucial steps in\nfeature-extraction. The purpose of image processing is to improve the leaf\nimage by removing undesired distortion. The main image processing steps of our\nalgorithm involves: i) Convert original image to RGB (Red-Green-Blue) image,\nii) Gray scaling, iii) Gaussian smoothing, iv) Binary thresholding, v) Remove\nstalk, vi) Closing holes, and vii) Resize image. The next step after image\nprocessing is to extract features from plant leaf images. We introduced 52\ncomputationally efficient features to classify plant species. These features\nare mainly classified into four groups as: i) shape-based features, ii)\ncolor-based features, iii) texture-based features, and iv) scagnostic features.\nLength, width, area, texture correlation, monotonicity and scagnostics are to\nname few of them. We explore the ability of features to discriminate the\nclasses of interest under supervised learning and unsupervised learning\nsettings. For that, supervised dimensionality reduction technique, Linear\nDiscriminant Analysis (LDA), and unsupervised dimensionality reduction\ntechnique, Principal Component Analysis (PCA) are used to convert and visualize\nthe images from digital-image space to feature space. The results show that the\nfeatures are sufficient to discriminate the classes of interest under both\nsupervised and unsupervised learning settings.",
    "published_date": "2021-06-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "stat.AP",
      "E.4; I.5.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.08077v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.08049v3",
    "title": "Demographic Fairness in Face Identification: The Watchlist Imbalance Effect",
    "authors": [
      "Pawel Drozdowski",
      "Christian Rathgeb",
      "Christoph Busch"
    ],
    "author_ids": [],
    "abstract": "Recently, different researchers have found that the gallery composition of a\nface database can induce performance differentials to facial identification\nsystems in which a probe image is compared against up to all stored reference\nimages to reach a biometric decision. This negative effect is referred to as\n\"watchlist imbalance effect\". In this work, we present a method to\ntheoretically estimate said effect for a biometric identification system given\nits verification performance across demographic groups and the composition of\nthe used gallery. Further, we report results for identification experiments on\ndifferently composed demographic subsets, i.e. females and males, of the public\nacademic MORPH database using the open-source ArcFace face recognition system.\nIt is shown that the database composition has a huge impact on performance\ndifferentials in biometric identification systems, even if performance\ndifferentials are less pronounced in the verification scenario. This study\nrepresents the first detailed analysis of the watchlist imbalance effect which\nis expected to be of high interest for future research in the field of facial\nrecognition.",
    "published_date": "2021-06-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.08049v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.07876v3",
    "title": "Vision-Language Navigation with Random Environmental Mixup",
    "authors": [
      "Chong Liu",
      "Fengda Zhu",
      "Xiaojun Chang",
      "Xiaodan Liang",
      "Zongyuan Ge",
      "Yi-Dong Shen"
    ],
    "author_ids": [],
    "abstract": "Vision-language Navigation (VLN) tasks require an agent to navigate\nstep-by-step while perceiving the visual observations and comprehending a\nnatural language instruction. Large data bias, which is caused by the disparity\nratio between the small data scale and large navigation space, makes the VLN\ntask challenging. Previous works have proposed various data augmentation\nmethods to reduce data bias. However, these works do not explicitly reduce the\ndata bias across different house scenes. Therefore, the agent would overfit to\nthe seen scenes and achieve poor navigation performance in the unseen scenes.\nTo tackle this problem, we propose the Random Environmental Mixup (REM) method,\nwhich generates cross-connected house scenes as augmented data via mixuping\nenvironment. Specifically, we first select key viewpoints according to the room\nconnection graph for each scene. Then, we cross-connect the key views of\ndifferent scenes to construct augmented scenes. Finally, we generate augmented\ninstruction-path pairs in the cross-connected scenes. The experimental results\non benchmark datasets demonstrate that our augmentation data via REM help the\nagent reduce its performance gap between the seen and unseen environment and\nimprove the overall performance, making our model the best existing approach on\nthe standard VLN benchmark. The code have released:\nhttps://github.com/LCFractal/VLNREM.",
    "published_date": "2021-06-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.07876v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.07849v1",
    "title": "Simon Says: Evaluating and Mitigating Bias in Pruned Neural Networks with Knowledge Distillation",
    "authors": [
      "Cody Blakeney",
      "Nathaniel Huish",
      "Yan Yan",
      "Ziliang Zong"
    ],
    "author_ids": [],
    "abstract": "In recent years the ubiquitous deployment of AI has posed great concerns in\nregards to algorithmic bias, discrimination, and fairness. Compared to\ntraditional forms of bias or discrimination caused by humans, algorithmic bias\ngenerated by AI is more abstract and unintuitive therefore more difficult to\nexplain and mitigate. A clear gap exists in the current literature on\nevaluating and mitigating bias in pruned neural networks. In this work, we\nstrive to tackle the challenging issues of evaluating, mitigating, and\nexplaining induced bias in pruned neural networks. Our paper makes three\ncontributions. First, we propose two simple yet effective metrics, Combined\nError Variance (CEV) and Symmetric Distance Error (SDE), to quantitatively\nevaluate the induced bias prevention quality of pruned models. Second, we\ndemonstrate that knowledge distillation can mitigate induced bias in pruned\nneural networks, even with unbalanced datasets. Third, we reveal that model\nsimilarity has strong correlations with pruning induced bias, which provides a\npowerful method to explain why bias occurs in pruned neural networks. Our code\nis available at https://github.com/codestar12/pruning-distilation-bias",
    "published_date": "2021-06-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.07849v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.07820v1",
    "title": "On Large-Cohort Training for Federated Learning",
    "authors": [
      "Zachary Charles",
      "Zachary Garrett",
      "Zhouyuan Huo",
      "Sergei Shmulyian",
      "Virginia Smith"
    ],
    "author_ids": [],
    "abstract": "Federated learning methods typically learn a model by iteratively sampling\nupdates from a population of clients. In this work, we explore how the number\nof clients sampled at each round (the cohort size) impacts the quality of the\nlearned model and the training dynamics of federated learning algorithms. Our\nwork poses three fundamental questions. First, what challenges arise when\ntrying to scale federated learning to larger cohorts? Second, what parallels\nexist between cohort sizes in federated learning and batch sizes in centralized\nlearning? Last, how can we design federated learning methods that effectively\nutilize larger cohort sizes? We give partial answers to these questions based\non extensive empirical evaluation. Our work highlights a number of challenges\nstemming from the use of larger cohorts. While some of these (such as\ngeneralization issues and diminishing returns) are analogs of large-batch\ntraining challenges, others (including training failures and fairness concerns)\nare unique to federated learning.",
    "published_date": "2021-06-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.07820v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.07677v4",
    "title": "Planning to Fairly Allocate: Probabilistic Fairness in the Restless Bandit Setting",
    "authors": [
      "Christine Herlihy",
      "Aviva Prins",
      "Aravind Srinivasan",
      "John P. Dickerson"
    ],
    "author_ids": [],
    "abstract": "Restless and collapsing bandits are often used to model budget-constrained\nresource allocation in settings where arms have action-dependent transition\nprobabilities, such as the allocation of health interventions among patients.\nHowever, state-of-the-art Whittle-index-based approaches to this planning\nproblem either do not consider fairness among arms, or incentivize fairness\nwithout guaranteeing it. We thus introduce ProbFair, a probabilistically fair\npolicy that maximizes total expected reward and satisfies the budget constraint\nwhile ensuring a strictly positive lower bound on the probability of being\npulled at each timestep. We evaluate our algorithm on a real-world application,\nwhere interventions support continuous positive airway pressure (CPAP) therapy\nadherence among patients, as well as on a broader class of synthetic transition\nmatrices. We find that ProbFair preserves utility while providing fairness\nguarantees.",
    "published_date": "2021-06-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.07677v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.07597v4",
    "title": "MLPerf Tiny Benchmark",
    "authors": [
      "Colby Banbury",
      "Vijay Janapa Reddi",
      "Peter Torelli",
      "Jeremy Holleman",
      "Nat Jeffries",
      "Csaba Kiraly",
      "Pietro Montino",
      "David Kanter",
      "Sebastian Ahmed",
      "Danilo Pau",
      "Urmish Thakker",
      "Antonio Torrini",
      "Peter Warden",
      "Jay Cordaro",
      "Giuseppe Di Guglielmo",
      "Javier Duarte",
      "Stephen Gibellini",
      "Videet Parekh",
      "Honson Tran",
      "Nhan Tran",
      "Niu Wenxu",
      "Xu Xuesong"
    ],
    "author_ids": [],
    "abstract": "Advancements in ultra-low-power tiny machine learning (TinyML) systems\npromise to unlock an entirely new class of smart applications. However,\ncontinued progress is limited by the lack of a widely accepted and easily\nreproducible benchmark for these systems. To meet this need, we present MLPerf\nTiny, the first industry-standard benchmark suite for ultra-low-power tiny\nmachine learning systems. The benchmark suite is the collaborative effort of\nmore than 50 organizations from industry and academia and reflects the needs of\nthe community. MLPerf Tiny measures the accuracy, latency, and energy of\nmachine learning inference to properly evaluate the tradeoffs between systems.\nAdditionally, MLPerf Tiny implements a modular design that enables benchmark\nsubmitters to show the benefits of their product, regardless of where it falls\non the ML deployment stack, in a fair and reproducible manner. The suite\nfeatures four benchmarks: keyword spotting, visual wake words, image\nclassification, and anomaly detection.",
    "published_date": "2021-06-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.07597v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.07560v4",
    "title": "Allocating Stimulus Checks in Times of Crisis",
    "authors": [
      "Marios Papachristou",
      "Jon Kleinberg"
    ],
    "author_ids": [],
    "abstract": "We study the problem of allocating bailouts (stimulus, subsidy allocations)\nto people participating in a financial network subject to income shocks. We\nbuild on the financial clearing framework of Eisenberg and Noe that allows the\nincorporation of a bailout policy that is based on discrete bailouts motivated\nby the types of stimulus checks people receive around the world as part of\nCOVID-19 economical relief plans. We show that optimally allocating such\nbailouts on a financial network in order to maximize a variety of social\nwelfare objectives of this form is a computationally intractable problem. We\ndevelop approximation algorithms to optimize these objectives and establish\nguarantees for their approximation rations. Then, we incorporate multiple\nfairness constraints in the optimization problems and establish relative bounds\non the solutions with versus without these constraints. Finally, we apply our\nmethodology to a variety of data, both in the context of a system of large\nfinancial institutions with real-world data, as well as in a realistic societal\ncontext with financial interactions between people and businesses for which we\nuse semi-artificial data derived from mobility patterns. Our results suggest\nthat the algorithms we develop and study have reasonable results in practice\nand outperform other network-based heuristics. We argue that the presented\nproblem through the societal-level lens could assist policymakers in making\ninformed decisions on issuing subsidies.",
    "published_date": "2021-06-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.07560v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.07483v1",
    "title": "Can Explainable AI Explain Unfairness? A Framework for Evaluating Explainable AI",
    "authors": [
      "Kiana Alikhademi",
      "Brianna Richardson",
      "Emma Drobina",
      "Juan E. Gilbert"
    ],
    "author_ids": [],
    "abstract": "Many ML models are opaque to humans, producing decisions too complex for\nhumans to easily understand. In response, explainable artificial intelligence\n(XAI) tools that analyze the inner workings of a model have been created.\nDespite these tools' strength in translating model behavior, critiques have\nraised concerns about the impact of XAI tools as a tool for `fairwashing` by\nmisleading users into trusting biased or incorrect models. In this paper, we\ncreated a framework for evaluating explainable AI tools with respect to their\ncapabilities for detecting and addressing issues of bias and fairness as well\nas their capacity to communicate these results to their users clearly. We found\nthat despite their capabilities in simplifying and explaining model behavior,\nmany prominent XAI tools lack features that could be critical in detecting\nbias. Developers can use our framework to suggest modifications needed in their\ntoolkits to reduce issues likes fairwashing.",
    "published_date": "2021-06-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.07483v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.07333v2",
    "title": "Deep Transfer Learning for Brain Magnetic Resonance Image Multi-class Classification",
    "authors": [
      "Yusuf Brima",
      "Mossadek Hossain Kamal Tushar",
      "Upama Kabir",
      "Tariqul Islam"
    ],
    "author_ids": [],
    "abstract": "Magnetic Resonance Imaging (MRI) is a principal diagnostic approach used in\nthe field of radiology to create images of the anatomical and physiological\nstructure of patients. MRI is the prevalent medical imaging practice to find\nabnormalities in soft tissues. Traditionally they are analyzed by a radiologist\nto detect abnormalities in soft tissues, especially the brain. The process of\ninterpreting a massive volume of patient's MRI is laborious. Hence, the use of\nMachine Learning methodologies can aid in detecting abnormalities in soft\ntissues with considerable accuracy. In this research, we have curated a novel\ndataset and developed a framework that uses Deep Transfer Learning to perform a\nmulti-classification of tumors in the brain MRI images. In this paper, we\nadopted the Deep Residual Convolutional Neural Network (ResNet50) architecture\nfor the experiments along with discriminative learning techniques to train the\nmodel. Using the novel dataset and two publicly available MRI brain datasets,\nthis proposed approach attained a classification accuracy of 86.40% on the\ncurated dataset, 93.80% on the Harvard Whole Brain Atlas dataset, and 97.05%\naccuracy on the School of Biomedical Engineering dataset. Results of our\nexperiments significantly demonstrate our proposed framework for transfer\nlearning is a potential and effective method for brain tumor\nmulti-classification tasks.",
    "published_date": "2021-06-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.07333v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.07255v1",
    "title": "Federated Myopic Community Detection with One-shot Communication",
    "authors": [
      "Chuyang Ke",
      "Jean Honorio"
    ],
    "author_ids": [],
    "abstract": "In this paper, we study the problem of recovering the community structure of\na network under federated myopic learning. Under this paradigm, we have several\nclients, each of them having a myopic view, i.e., observing a small subgraph of\nthe network. Each client sends a censored evidence graph to a central server.\nWe provide an efficient algorithm, which computes a consensus signed weighted\ngraph from clients evidence, and recovers the underlying network structure in\nthe central server. We analyze the topological structure conditions of the\nnetwork, as well as the signal and noise levels of the clients that allow for\nrecovery of the network structure. Our analysis shows that exact recovery is\npossible and can be achieved in polynomial time. We also provide\ninformation-theoretic limits for the central server to recover the network\nstructure from any single client evidence. Finally, as a byproduct of our\nanalysis, we provide a novel Cheeger-type inequality for general signed\nweighted graphs.",
    "published_date": "2021-06-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.07255v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.07171v1",
    "title": "Examining and Combating Spurious Features under Distribution Shift",
    "authors": [
      "Chunting Zhou",
      "Xuezhe Ma",
      "Paul Michel",
      "Graham Neubig"
    ],
    "author_ids": [],
    "abstract": "A central goal of machine learning is to learn robust representations that\ncapture the causal relationship between inputs features and output labels.\nHowever, minimizing empirical risk over finite or biased datasets often results\nin models latching on to spurious correlations between the training\ninput/output pairs that are not fundamental to the problem at hand. In this\npaper, we define and analyze robust and spurious representations using the\ninformation-theoretic concept of minimal sufficient statistics. We prove that\neven when there is only bias of the input distribution (i.e. covariate shift),\nmodels can still pick up spurious features from their training data. Group\ndistributionally robust optimization (DRO) provides an effective tool to\nalleviate covariate shift by minimizing the worst-case training loss over a set\nof pre-defined groups. Inspired by our analysis, we demonstrate that group DRO\ncan fail when groups do not directly account for various spurious correlations\nthat occur in the data. To address this, we further propose to minimize the\nworst-case losses over a more flexible set of distributions that are defined on\nthe joint distribution of groups and instances, instead of treating each group\nas a whole at optimization time. Through extensive experiments on one image and\ntwo language tasks, we show that our model is significantly more robust than\ncomparable baselines under various partitions. Our code is available at\nhttps://github.com/violet-zct/group-conditional-DRO.",
    "published_date": "2021-06-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.07171v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.07112v2",
    "title": "User Acceptance of Gender Stereotypes in Automated Career Recommendations",
    "authors": [
      "Clarice Wang",
      "Kathryn Wang",
      "Andrew Bian",
      "Rashidul Islam",
      "Kamrun Naher Keya",
      "James Foulds",
      "Shimei Pan"
    ],
    "author_ids": [],
    "abstract": "Currently, there is a surge of interest in fair Artificial Intelligence (AI)\nand Machine Learning (ML) research which aims to mitigate discriminatory bias\nin AI algorithms, e.g. along lines of gender, age, and race. While most\nresearch in this domain focuses on developing fair AI algorithms, in this work,\nwe show that a fair AI algorithm on its own may be insufficient to achieve its\nintended results in the real world. Using career recommendation as a case\nstudy, we build a fair AI career recommender by employing gender debiasing\nmachine learning techniques. Our offline evaluation showed that the debiased\nrecommender makes fairer career recommendations without sacrificing its\naccuracy. Nevertheless, an online user study of more than 200 college students\nrevealed that participants on average prefer the original biased system over\nthe debiased system. Specifically, we found that perceived gender disparity is\na determining factor for the acceptance of a recommendation. In other words,\nour results demonstrate we cannot fully address the gender bias issue in AI\nrecommendations without addressing the gender bias in humans.",
    "published_date": "2021-06-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.07112v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.07057v4",
    "title": "FairCanary: Rapid Continuous Explainable Fairness",
    "authors": [
      "Avijit Ghosh",
      "Aalok Shanbhag",
      "Christo Wilson"
    ],
    "author_ids": [],
    "abstract": "Systems that offer continuous model monitoring have emerged in response to\n(1) well-documented failures of deployed Machine Learning (ML) and Artificial\nIntelligence (AI) models and (2) new regulatory requirements impacting these\nmodels. Existing monitoring systems continuously track the performance of\ndeployed ML models and compute feature importance (a.k.a. explanations) for\neach prediction to help developers identify the root causes of emergent model\nperformance problems.\n  We present Quantile Demographic Drift (QDD), a novel model bias\nquantification metric that uses quantile binning to measure differences in the\noverall prediction distributions over subgroups. QDD is ideal for continuous\nmonitoring scenarios, does not suffer from the statistical limitations of\nconventional threshold-based bias metrics, and does not require outcome labels\n(which may not be available at runtime). We incorporate QDD into a continuous\nmodel monitoring system, called FairCanary, that reuses existing explanations\ncomputed for each individual prediction to quickly compute explanations for the\nQDD bias metrics. This optimization makes FairCanary an order of magnitude\nfaster than previous work that has tried to generate feature-level bias\nexplanations.",
    "published_date": "2021-06-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.07057v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.07041v1",
    "title": "Correcting Exposure Bias for Link Recommendation",
    "authors": [
      "Shantanu Gupta",
      "Hao Wang",
      "Zachary C. Lipton",
      "Yuyang Wang"
    ],
    "author_ids": [],
    "abstract": "Link prediction methods are frequently applied in recommender systems, e.g.,\nto suggest citations for academic papers or friends in social networks.\nHowever, exposure bias can arise when users are systematically underexposed to\ncertain relevant items. For example, in citation networks, authors might be\nmore likely to encounter papers from their own field and thus cite them\npreferentially. This bias can propagate through naively trained link\npredictors, leading to both biased evaluation and high generalization error (as\nassessed by true relevance). Moreover, this bias can be exacerbated by feedback\nloops. We propose estimators that leverage known exposure probabilities to\nmitigate this bias and consequent feedback loops. Next, we provide a loss\nfunction for learning the exposure probabilities from data. Finally,\nexperiments on semi-synthetic data based on real-world citation networks, show\nthat our methods reliably identify (truly) relevant citations. Additionally,\nour methods lead to greater diversity in the recommended papers' fields of\nstudy. The code is available at\nhttps://github.com/shantanu95/exposure-bias-link-rec.",
    "published_date": "2021-06-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.07041v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.06811v1",
    "title": "Case Study on Detecting COVID-19 Health-Related Misinformation in Social Media",
    "authors": [
      "Mir Mehedi A. Pritom",
      "Rosana Montanez Rodriguez",
      "Asad Ali Khan",
      "Sebastian A. Nugroho",
      "Esra'a Alrashydah",
      "Beatrice N. Ruiz",
      "Anthony Rios"
    ],
    "author_ids": [],
    "abstract": "COVID-19 pandemic has generated what public health officials called an\ninfodemic of misinformation. As social distancing and stay-at-home orders came\ninto effect, many turned to social media for socializing. This increase in\nsocial media usage has made it a prime vehicle for the spreading of\nmisinformation. This paper presents a mechanism to detect COVID-19\nhealth-related misinformation in social media following an interdisciplinary\napproach. Leveraging social psychology as a foundation and existing\nmisinformation frameworks, we defined misinformation themes and associated\nkeywords incorporated into the misinformation detection mechanism using applied\nmachine learning techniques. Next, using the Twitter dataset, we explored the\nperformance of the proposed methodology using multiple state-of-the-art machine\nlearning classifiers. Our method shows promising results with at most 78%\naccuracy in classifying health-related misinformation versus true information\nusing uni-gram-based NLP feature generations from tweets and the Decision Tree\nclassifier. We also provide suggestions on alternatives for countering\nmisinformation and ethical consideration for the study.",
    "published_date": "2021-06-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.06811v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.06801v3",
    "title": "Contrastive Semi-Supervised Learning for 2D Medical Image Segmentation",
    "authors": [
      "Prashant Pandey",
      "Ajey Pai",
      "Nisarg Bhatt",
      "Prasenjit Das",
      "Govind Makharia",
      "Prathosh AP",
      "Mausam"
    ],
    "author_ids": [],
    "abstract": "Contrastive Learning (CL) is a recent representation learning approach, which\nencourages inter-class separability and intra-class compactness in learned\nimage representations. Since medical images often contain multiple semantic\nclasses in an image, using CL to learn representations of local features (as\nopposed to global) is important. In this work, we present a novel\nsemi-supervised 2D medical segmentation solution that applies CL on image\npatches, instead of full images. These patches are meaningfully constructed\nusing the semantic information of different classes obtained via pseudo\nlabeling. We also propose a novel consistency regularization (CR) scheme, which\nworks in synergy with CL. It addresses the problem of confirmation bias, and\nencourages better clustering in the feature space. We evaluate our method on\nfour public medical segmentation datasets and a novel histopathology dataset\nthat we introduce. Our method obtains consistent improvements over\nstate-of-the-art semi-supervised segmentation approaches for all datasets.",
    "published_date": "2021-06-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.06801v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.06770v2",
    "title": "What can linearized neural networks actually say about generalization?",
    "authors": [
      "Guillermo Ortiz-Jiménez",
      "Seyed-Mohsen Moosavi-Dezfooli",
      "Pascal Frossard"
    ],
    "author_ids": [],
    "abstract": "For certain infinitely-wide neural networks, the neural tangent kernel (NTK)\ntheory fully characterizes generalization, but for the networks used in\npractice, the empirical NTK only provides a rough first-order approximation.\nStill, a growing body of work keeps leveraging this approximation to\nsuccessfully analyze important deep learning phenomena and design algorithms\nfor new applications. In our work, we provide strong empirical evidence to\ndetermine the practical validity of such approximation by conducting a\nsystematic comparison of the behavior of different neural networks and their\nlinear approximations on different tasks. We show that the linear\napproximations can indeed rank the learning complexity of certain tasks for\nneural networks, even when they achieve very different performances. However,\nin contrast to what was previously reported, we discover that neural networks\ndo not always perform better than their kernel approximations, and reveal that\nthe performance gap heavily depends on architecture, dataset size and training\ntask. We discover that networks overfit to these tasks mostly due to the\nevolution of their kernel during training, thus, revealing a new type of\nimplicit bias.",
    "published_date": "2021-06-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.06770v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.06755v2",
    "title": "Tight FPT Approximation for Socially Fair Clustering",
    "authors": [
      "Dishant Goyal",
      "Ragesh Jaiswal"
    ],
    "author_ids": [],
    "abstract": "In this work, we study the socially fair $k$-median/$k$-means problem. We are\ngiven a set of points $P$ in a metric space $\\mathcal{X}$ with a distance\nfunction $d(.,.)$. There are $\\ell$ groups: $P_1,\\dotsc,P_{\\ell} \\subseteq P$.\nWe are also given a set $F$ of feasible centers in $\\mathcal{X}$. The goal in\nthe socially fair $k$-median problem is to find a set $C \\subseteq F$ of $k$\ncenters that minimizes the maximum average cost over all the groups. That is,\nfind $C$ that minimizes the objective function $\\Phi(C,P) \\equiv \\max_{j}\n\\Big\\{ \\sum_{x \\in P_j} d(C,x)/|P_j| \\Big\\}$, where $d(C,x)$ is the distance of\n$x$ to the closest center in $C$. The socially fair $k$-means problem is\ndefined similarly by using squared distances, i.e., $d^{2}(.,.)$ instead of\n$d(.,.)$. The current best approximation guarantee for both the problems is\n$O\\left( \\frac{\\log \\ell}{\\log \\log \\ell} \\right)$ due to Makarychev and\nVakilian [COLT 2021]. In this work, we study the fixed parameter tractability\nof the problems with respect to parameter $k$. We design $(3+\\varepsilon)$ and\n$(9 + \\varepsilon)$ approximation algorithms for the socially fair $k$-median\nand $k$-means problems, respectively, in FPT (fixed parameter tractable) time\n$f(k,\\varepsilon) \\cdot n^{O(1)}$, where $f(k,\\varepsilon) =\n(k/\\varepsilon)^{{O}(k)}$ and $n = |P \\cup F|$. Furthermore, we show that if\nGap-ETH holds, then better approximation guarantees are not possible in FPT\ntime.",
    "published_date": "2021-06-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DS",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.06755v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.06683v2",
    "title": "Assessing Multilingual Fairness in Pre-trained Multimodal Representations",
    "authors": [
      "Jialu Wang",
      "Yang Liu",
      "Xin Eric Wang"
    ],
    "author_ids": [],
    "abstract": "Recently pre-trained multimodal models, such as CLIP, have shown exceptional\ncapabilities towards connecting images and natural language. The textual\nrepresentations in English can be desirably transferred to multilingualism and\nsupport downstream multimodal tasks for different languages. Nevertheless, the\nprinciple of multilingual fairness is rarely scrutinized: do multilingual\nmultimodal models treat languages equally? Are their performances biased\ntowards particular languages? To answer these questions, we view language as\nthe fairness recipient and introduce two new fairness notions, multilingual\nindividual fairness and multilingual group fairness, for pre-trained multimodal\nmodels. Multilingual individual fairness requires that text snippets expressing\nsimilar semantics in different languages connect similarly to images, while\nmultilingual group fairness requires equalized predictive performance across\nlanguages. We characterize the extent to which pre-trained multilingual\nvision-and-language representations are individually fair across languages.\nHowever, extensive experiments demonstrate that multilingual representations do\nnot satisfy group fairness: (1) there is a severe multilingual accuracy\ndisparity issue; (2) the errors exhibit biases across languages conditioning\nthe group of people in the images, including race, gender and age.",
    "published_date": "2021-06-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.06683v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.06664v1",
    "title": "Rapid COVID-19 Risk Screening by Eye-region Manifestations",
    "authors": [
      "Yanwei Fu",
      "Lei Zhao",
      "Haojie Zheng",
      "Qiang Sun",
      "Li Yang",
      "Hong Li",
      "Jiao Xie",
      "Xiangyang Xue",
      "Feng Li",
      "Yuan Li",
      "Wei Wang",
      "Yantao Pei",
      "Jianmin Wang",
      "Xiuqi Wu",
      "Yanhua Zheng",
      "Hongxia Tian Mengwei Gu1"
    ],
    "author_ids": [],
    "abstract": "It is still nontrivial to develop a new fast COVID-19 screening method with\nthe easier access and lower cost, due to the technical and cost limitations of\nthe current testing methods in the medical resource-poor districts. On the\nother hand, there are more and more ocular manifestations that have been\nreported in the COVID-19 patients as growing clinical evidence[1]. This\ninspired this project. We have conducted the joint clinical research since\nJanuary 2021 at the ShiJiaZhuang City, Heibei province, China, which approved\nby the ethics committee of The fifth hospital of ShiJiaZhuang of Hebei Medical\nUniversity. We undertake several blind tests of COVID-19 patients by Union\nHospital, Tongji Medical College, Huazhong University of Science and\nTechnology, Wuhan, China. Meantime as an important part of the ongoing globally\nCOVID-19 eye test program by AIMOMICS since February 2020, we propose a new\nfast screening method of analyzing the eye-region images, captured by common\nCCD and CMOS cameras. This could reliably make a rapid risk screening of\nCOVID-19 with the sustainable stable high performance in different countries\nand races. Our model for COVID-19 rapid prescreening have the merits of the\nlower cost, fully self-performed, non-invasive, importantly real-time, and thus\nenables the continuous health surveillance. We further implement it as the open\naccessible APIs, and provide public service to the world. Our pilot experiments\nshow that our model is ready to be usable to all kinds of surveillance\nscenarios, such as infrared temperature measurement device at airports and\nstations, or directly pushing to the target people groups smartphones as a\npackaged application.",
    "published_date": "2021-06-12T00:00:00",
    "year": 2021,
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.06664v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.06616v2",
    "title": "Learning Competitive Equilibria in Exchange Economies with Bandit Feedback",
    "authors": [
      "Wenshuo Guo",
      "Kirthevasan Kandasamy",
      "Joseph E Gonzalez",
      "Michael I. Jordan",
      "Ion Stoica"
    ],
    "author_ids": [],
    "abstract": "The sharing of scarce resources among multiple rational agents is one of the\nclassical problems in economics. In exchange economies, which are used to model\nsuch situations, agents begin with an initial endowment of resources and\nexchange them in a way that is mutually beneficial until they reach a\ncompetitive equilibrium (CE). The allocations at a CE are Pareto efficient and\nfair. Consequently, they are used widely in designing mechanisms for fair\ndivision. However, computing CEs requires the knowledge of agent preferences\nwhich are unknown in several applications of interest. In this work, we explore\na new online learning mechanism, which, on each round, allocates resources to\nthe agents and collects stochastic feedback on their experience in using that\nallocation. Its goal is to learn the agent utilities via this feedback and\nimitate the allocations at a CE in the long run. We quantify CE behavior via\ntwo losses and propose a randomized algorithm which achieves sublinear loss\nunder a parametric class of utilities. Empirically, we demonstrate the\neffectiveness of this mechanism through numerical simulations.",
    "published_date": "2021-06-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.06616v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.06483v3",
    "title": "Towards Costless Model Selection in Contextual Bandits: A Bias-Variance Perspective",
    "authors": [
      "Sanath Kumar Krishnamurthy",
      "Adrienne Margaret Propp",
      "Susan Athey"
    ],
    "author_ids": [],
    "abstract": "Model selection in supervised learning provides costless guarantees as if the\nmodel that best balances bias and variance was known a priori. We study the\nfeasibility of similar guarantees for cumulative regret minimization in the\nstochastic contextual bandit setting. Recent work [Marinov and Zimmert, 2021]\nidentifies instances where no algorithm can guarantee costless regret bounds.\nNevertheless, we identify benign conditions where costless model selection is\nfeasible: gradually increasing class complexity, and diminishing marginal\nreturns for best-in-class policy value with increasing class complexity. Our\nalgorithm is based on a novel misspecification test, and our analysis\ndemonstrates the benefits of using model selection for reward estimation.\nUnlike prior work on model selection in contextual bandits, our algorithm\ncarefully adapts to the evolving bias-variance trade-off as more data is\ncollected. In particular, our algorithm and analysis go beyond adapting to the\ncomplexity of the simplest realizable class and instead adapt to the complexity\nof the simplest class whose estimation variance dominates the bias. For short\nhorizons, this provides improved regret guarantees that depend on the\ncomplexity of simpler classes.",
    "published_date": "2021-06-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.06483v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.06420v1",
    "title": "A Framework to Enhance Generalization of Deep Metric Learning methods using General Discriminative Feature Learning and Class Adversarial Neural Networks",
    "authors": [
      "Karrar Al-Kaabi",
      "Reza Monsefi",
      "Davood Zabihzadeh"
    ],
    "author_ids": [],
    "abstract": "Metric learning algorithms aim to learn a distance function that brings the\nsemantically similar data items together and keeps dissimilar ones at a\ndistance. The traditional Mahalanobis distance learning is equivalent to find a\nlinear projection. In contrast, Deep Metric Learning (DML) methods are proposed\nthat automatically extract features from data and learn a non-linear\ntransformation from input space to a semantically embedding space. Recently,\nmany DML methods are proposed focused to enhance the discrimination power of\nthe learned metric by providing novel sampling strategies or loss functions.\nThis approach is very helpful when both the training and test examples are\ncoming from the same set of categories. However, it is less effective in many\napplications of DML such as image retrieval and person-reidentification. Here,\nthe DML should learn general semantic concepts from observed classes and employ\nthem to rank or identify objects from unseen categories. Neglecting the\ngeneralization ability of the learned representation and just emphasizing to\nlearn a more discriminative embedding on the observed classes may lead to the\noverfitting problem. To address this limitation, we propose a framework to\nenhance the generalization power of existing DML methods in a Zero-Shot\nLearning (ZSL) setting by general yet discriminative representation learning\nand employing a class adversarial neural network. To learn a more general\nrepresentation, we propose to employ feature maps of intermediate layers in a\ndeep neural network and enhance their discrimination power through an attention\nmechanism. Besides, a class adversarial network is utilized to enforce the deep\nmodel to seek class invariant features for the DML task. We evaluate our work\non widely used machine vision datasets in a ZSL setting.",
    "published_date": "2021-06-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.IR",
      "cs.LG",
      "6804 (Primary)"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.06420v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.06258v1",
    "title": "DebiasGAN: Eliminating Position Bias in News Recommendation with Adversarial Learning",
    "authors": [
      "Chuhan Wu",
      "Fangzhao Wu",
      "Yongfeng Huang"
    ],
    "author_ids": [],
    "abstract": "News recommendation is important for improving news reading experience of\nusers. Users' news click behaviors are widely used for inferring user interests\nand predicting future clicks. However, click behaviors are heavily affected by\nthe biases brought by the positions of news displayed on the webpage. It is\nimportant to eliminate the effect of position biases on the recommendation\nmodel to accurately target user interests. In this paper, we propose a news\nrecommendation method named DebiasGAN that can effectively eliminate the effect\nof position biases via adversarial learning. We use a bias-aware click model to\ncapture the influence of position bias on click behaviors, and we use a\nbias-invariant click model with random candidate news positions to estimate the\nideally unbiased click scores. We apply adversarial learning techniques to the\nhidden representations learned by the two models to help the bias-invariant\nclick model capture the bias-independent interest of users on news.\nExperimental results on two real-world datasets show that DebiasGAN can\neffectively improve the accuracy of news recommendation by eliminating position\nbiases.",
    "published_date": "2021-06-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.06258v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.06007v1",
    "title": "Overcoming Difficulty in Obtaining Dark-skinned Subjects for Remote-PPG by Synthetic Augmentation",
    "authors": [
      "Yunhao Ba",
      "Zhen Wang",
      "Kerim Doruk Karinca",
      "Oyku Deniz Bozkurt",
      "Achuta Kadambi"
    ],
    "author_ids": [],
    "abstract": "Camera-based remote photoplethysmography (rPPG) provides a non-contact way to\nmeasure physiological signals (e.g., heart rate) using facial videos. Recent\ndeep learning architectures have improved the accuracy of such physiological\nmeasurement significantly, yet they are restricted by the diversity of the\nannotated videos. The existing datasets MMSE-HR, AFRL, and UBFC-RPPG contain\nroughly 10%, 0%, and 5% of dark-skinned subjects respectively. The unbalanced\ntraining sets result in a poor generalization capability to unseen subjects and\nlead to unwanted bias toward different demographic groups. In Western academia,\nit is regrettably difficult in a university setting to collect data on these\ndark-skinned subjects. Here we show a first attempt to overcome the lack of\ndark-skinned subjects by synthetic augmentation. A joint optimization framework\nis utilized to translate real videos from light-skinned subjects to dark skin\ntones while retaining their pulsatile signals. In the experiment, our method\nexhibits around 31% reduction in mean absolute error for the dark-skinned group\nand 46% improvement on bias mitigation for all the groups, as compared with the\nprevious work trained with just real samples.",
    "published_date": "2021-06-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.06007v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.06533v1",
    "title": "View Generalization for Single Image Textured 3D Models",
    "authors": [
      "Anand Bhattad",
      "Aysegul Dundar",
      "Guilin Liu",
      "Andrew Tao",
      "Bryan Catanzaro"
    ],
    "author_ids": [],
    "abstract": "Humans can easily infer the underlying 3D geometry and texture of an object\nonly from a single 2D image. Current computer vision methods can do this, too,\nbut suffer from view generalization problems - the models inferred tend to make\npoor predictions of appearance in novel views. As for generalization problems\nin machine learning, the difficulty is balancing single-view accuracy (cf.\ntraining error; bias) with novel view accuracy (cf. test error; variance). We\ndescribe a class of models whose geometric rigidity is easily controlled to\nmanage this tradeoff. We describe a cycle consistency loss that improves view\ngeneralization (roughly, a model from a generated view should predict the\noriginal view well). View generalization of textures requires that models share\ntexture information, so a car seen from the back still has headlights because\nother cars have headlights. We describe a cycle consistency loss that\nencourages model textures to be aligned, so as to encourage sharing. We compare\nour method against the state-of-the-art method and show both qualitative and\nquantitative improvements.",
    "published_date": "2021-06-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.06533v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.05964v2",
    "title": "Fair Classification with Adversarial Perturbations",
    "authors": [
      "L. Elisa Celis",
      "Anay Mehrotra",
      "Nisheeth K. Vishnoi"
    ],
    "author_ids": [],
    "abstract": "We study fair classification in the presence of an omniscient adversary that,\ngiven an $\\eta$, is allowed to choose an arbitrary $\\eta$-fraction of the\ntraining samples and arbitrarily perturb their protected attributes. The\nmotivation comes from settings in which protected attributes can be incorrect\ndue to strategic misreporting, malicious actors, or errors in imputation; and\nprior approaches that make stochastic or independence assumptions on errors may\nnot satisfy their guarantees in this adversarial setting. Our main contribution\nis an optimization framework to learn fair classifiers in this adversarial\nsetting that comes with provable guarantees on accuracy and fairness. Our\nframework works with multiple and non-binary protected attributes, is designed\nfor the large class of linear-fractional fairness metrics, and can also handle\nperturbations besides protected attributes. We prove near-tightness of our\nframework's guarantees for natural hypothesis classes: no algorithm can have\nsignificantly better accuracy and any algorithm with better fairness must have\nlower accuracy. Empirically, we evaluate the classifiers produced by our\nframework for statistical rate on real-world and synthetic datasets for a\nfamily of adversaries.",
    "published_date": "2021-06-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.DS",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.05964v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.05937v2",
    "title": "Fair Normalizing Flows",
    "authors": [
      "Mislav Balunović",
      "Anian Ruoss",
      "Martin Vechev"
    ],
    "author_ids": [],
    "abstract": "Fair representation learning is an attractive approach that promises fairness\nof downstream predictors by encoding sensitive data. Unfortunately, recent work\nhas shown that strong adversarial predictors can still exhibit unfairness by\nrecovering sensitive attributes from these representations. In this work, we\npresent Fair Normalizing Flows (FNF), a new approach offering more rigorous\nfairness guarantees for learned representations. Specifically, we consider a\npractical setting where we can estimate the probability density for sensitive\ngroups. The key idea is to model the encoder as a normalizing flow trained to\nminimize the statistical distance between the latent representations of\ndifferent groups. The main advantage of FNF is that its exact likelihood\ncomputation allows us to obtain guarantees on the maximum unfairness of any\npotentially adversarial downstream predictor. We experimentally demonstrate the\neffectiveness of FNF in enforcing various group fairness notions, as well as\nother attractive properties such as interpretability and transfer learning, on\na variety of challenging real-world datasets.",
    "published_date": "2021-06-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.05937v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.05891v1",
    "title": "Temporal and Object Quantification Networks",
    "authors": [
      "Jiayuan Mao",
      "Zhezheng Luo",
      "Chuang Gan",
      "Joshua B. Tenenbaum",
      "Jiajun Wu",
      "Leslie Pack Kaelbling",
      "Tomer D. Ullman"
    ],
    "author_ids": [],
    "abstract": "We present Temporal and Object Quantification Networks (TOQ-Nets), a new\nclass of neuro-symbolic networks with a structural bias that enables them to\nlearn to recognize complex relational-temporal events. This is done by\nincluding reasoning layers that implement finite-domain quantification over\nobjects and time. The structure allows them to generalize directly to input\ninstances with varying numbers of objects in temporal sequences of varying\nlengths. We evaluate TOQ-Nets on input domains that require recognizing\nevent-types in terms of complex temporal relational patterns. We demonstrate\nthat TOQ-Nets can generalize from small amounts of data to scenarios containing\nmore objects than were present during training and to temporal warpings of\ninput sequences.",
    "published_date": "2021-06-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.05891v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.05797v2",
    "title": "Linear Classifiers Under Infinite Imbalance",
    "authors": [
      "Paul Glasserman",
      "Mike Li"
    ],
    "author_ids": [],
    "abstract": "We study the behavior of linear discriminant functions for binary\nclassification in the infinite-imbalance limit, where the sample size of one\nclass grows without bound while the sample size of the other remains fixed. The\ncoefficients of the classifier minimize an empirical loss specified through a\nweight function. We show that for a broad class of weight functions, the\nintercept diverges but the rest of the coefficient vector has a finite almost\nsure limit under infinite imbalance, extending prior work on logistic\nregression. The limit depends on the left-tail growth rate of the weight\nfunction, for which we distinguish two cases: subexponential and exponential.\nThe limiting coefficient vectors reflect robustness or conservatism properties\nin the sense that they optimize against certain worst-case alternatives. In the\nsubexponential case, the limit is equivalent to an implicit choice of\nupsampling distribution for the minority class. We apply these ideas in a\ncredit risk setting, with particular emphasis on performance in the\nhigh-sensitivity and high-specificity regions.",
    "published_date": "2021-06-10T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG",
      "q-fin.RM",
      "stat.ME"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.05797v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.05727v3",
    "title": "Cooperative Multi-Agent Fairness and Equivariant Policies",
    "authors": [
      "Niko A. Grupen",
      "Bart Selman",
      "Daniel D. Lee"
    ],
    "author_ids": [],
    "abstract": "We study fairness through the lens of cooperative multi-agent learning. Our\nwork is motivated by empirical evidence that naive maximization of team reward\nyields unfair outcomes for individual team members. To address fairness in\nmulti-agent contexts, we introduce team fairness, a group-based fairness\nmeasure for multi-agent learning. We then prove that it is possible to enforce\nteam fairness during policy optimization by transforming the team's joint\npolicy into an equivariant map. We refer to our multi-agent learning strategy\nas Fairness through Equivariance (Fair-E) and demonstrate its effectiveness\nempirically. We then introduce Fairness through Equivariance Regularization\n(Fair-ER) as a soft-constraint version of Fair-E and show that it reaches\nhigher levels of utility than Fair-E and fairer outcomes than non-equivariant\npolicies. Finally, we present novel findings regarding the fairness-utility\ntrade-off in multi-agent settings; showing that the magnitude of the trade-off\nis dependent on agent skill.",
    "published_date": "2021-06-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.05727v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.05682v2",
    "title": "DASO: Distribution-Aware Semantics-Oriented Pseudo-label for Imbalanced Semi-Supervised Learning",
    "authors": [
      "Youngtaek Oh",
      "Dong-Jin Kim",
      "In So Kweon"
    ],
    "author_ids": [],
    "abstract": "The capability of the traditional semi-supervised learning (SSL) methods is\nfar from real-world application due to severely biased pseudo-labels caused by\n(1) class imbalance and (2) class distribution mismatch between labeled and\nunlabeled data. This paper addresses such a relatively under-explored problem.\nFirst, we propose a general pseudo-labeling framework that class-adaptively\nblends the semantic pseudo-label from a similarity-based classifier to the\nlinear one from the linear classifier, after making the observation that both\ntypes of pseudo-labels have complementary properties in terms of bias. We\nfurther introduce a novel semantic alignment loss to establish balanced feature\nrepresentation to reduce the biased predictions from the classifier. We term\nthe whole framework as Distribution-Aware Semantics-Oriented (DASO)\nPseudo-label. We conduct extensive experiments in a wide range of imbalanced\nbenchmarks: CIFAR10/100-LT, STL10-LT, and large-scale long-tailed Semi-Aves\nwith open-set class, and demonstrate that, the proposed DASO framework reliably\nimproves SSL learners with unlabeled data especially when both (1) class\nimbalance and (2) distribution mismatch dominate.",
    "published_date": "2021-06-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.05682v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.11022v1",
    "title": "Hard Choices in Artificial Intelligence",
    "authors": [
      "Roel Dobbe",
      "Thomas Krendl Gilbert",
      "Yonatan Mintz"
    ],
    "author_ids": [],
    "abstract": "As AI systems are integrated into high stakes social domains, researchers now\nexamine how to design and operate them in a safe and ethical manner. However,\nthe criteria for identifying and diagnosing safety risks in complex social\ncontexts remain unclear and contested. In this paper, we examine the vagueness\nin debates about the safety and ethical behavior of AI systems. We show how\nthis vagueness cannot be resolved through mathematical formalism alone, instead\nrequiring deliberation about the politics of development as well as the context\nof deployment. Drawing from a new sociotechnical lexicon, we redefine vagueness\nin terms of distinct design challenges at key stages in AI system development.\nThe resulting framework of Hard Choices in Artificial Intelligence (HCAI)\nempowers developers by 1) identifying points of overlap between design\ndecisions and major sociotechnical challenges; 2) motivating the creation of\nstakeholder feedback channels so that safety issues can be exhaustively\naddressed. As such, HCAI contributes to a timely debate about the status of AI\ndevelopment in democratic societies, arguing that deliberation should be the\ngoal of AI Safety, not just the procedure by which it is ensured.",
    "published_date": "2021-06-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.SY",
      "eess.SY",
      "I.2; K.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.11022v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.05522v5",
    "title": "A Mathematical Foundation for Robust Machine Learning based on Bias-Variance Trade-off",
    "authors": [
      "Ou Wu",
      "Weiyao Zhu",
      "Yingjun Deng",
      "Haixiang Zhang",
      "Qinghu Hou"
    ],
    "author_ids": [],
    "abstract": "A common assumption in machine learning is that samples are independently and\nidentically distributed (i.i.d). However, the contributions of different\nsamples are not identical in training. Some samples are difficult to learn and\nsome samples are noisy. The unequal contributions of samples has a considerable\neffect on training performances. Studies focusing on unequal sample\ncontributions (e.g., easy, hard, noisy) in learning usually refer to these\ncontributions as robust machine learning (RML). Weighing and regularization are\ntwo common techniques in RML. Numerous learning algorithms have been proposed\nbut the strategies for dealing with easy/hard/noisy samples differ or even\ncontradict with different learning algorithms. For example, some strategies\ntake the hard samples first, whereas some strategies take easy first.\nConducting a clear comparison for existing RML algorithms in dealing with\ndifferent samples is difficult due to lack of a unified theoretical framework\nfor RML. This study attempts to construct a mathematical foundation for RML\nbased on the bias-variance trade-off theory. A series of definitions and\nproperties are presented and proved. Several classical learning algorithms are\nalso explained and compared. Improvements of existing methods are obtained\nbased on the comparison. A unified method that combines two classical learning\nstrategies is proposed.",
    "published_date": "2021-06-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.05522v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.05519v1",
    "title": "Consistent Instance False Positive Improves Fairness in Face Recognition",
    "authors": [
      "Xingkun Xu",
      "Yuge Huang",
      "Pengcheng Shen",
      "Shaoxin Li",
      "Jilin Li",
      "Feiyue Huang",
      "Yong Li",
      "Zhen Cui"
    ],
    "author_ids": [],
    "abstract": "Demographic bias is a significant challenge in practical face recognition\nsystems. Existing methods heavily rely on accurate demographic annotations.\nHowever, such annotations are usually unavailable in real scenarios. Moreover,\nthese methods are typically designed for a specific demographic group and are\nnot general enough. In this paper, we propose a false positive rate penalty\nloss, which mitigates face recognition bias by increasing the consistency of\ninstance False Positive Rate (FPR). Specifically, we first define the instance\nFPR as the ratio between the number of the non-target similarities above a\nunified threshold and the total number of the non-target similarities. The\nunified threshold is estimated for a given total FPR. Then, an additional\npenalty term, which is in proportion to the ratio of instance FPR overall FPR,\nis introduced into the denominator of the softmax-based loss. The larger the\ninstance FPR, the larger the penalty. By such unequal penalties, the instance\nFPRs are supposed to be consistent. Compared with the previous debiasing\nmethods, our method requires no demographic annotations. Thus, it can mitigate\nthe bias among demographic groups divided by various attributes, and these\nattributes are not needed to be previously predefined during training.\nExtensive experimental results on popular benchmarks demonstrate the\nsuperiority of our method over state-of-the-art competitors. Code and trained\nmodels are available at https://github.com/Tencent/TFace.",
    "published_date": "2021-06-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.05519v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.05498v3",
    "title": "It's COMPASlicated: The Messy Relationship between RAI Datasets and Algorithmic Fairness Benchmarks",
    "authors": [
      "Michelle Bao",
      "Angela Zhou",
      "Samantha Zottola",
      "Brian Brubach",
      "Sarah Desmarais",
      "Aaron Horowitz",
      "Kristian Lum",
      "Suresh Venkatasubramanian"
    ],
    "author_ids": [],
    "abstract": "Risk assessment instrument (RAI) datasets, particularly ProPublica's COMPAS\ndataset, are commonly used in algorithmic fairness papers due to benchmarking\npractices of comparing algorithms on datasets used in prior work. In many\ncases, this data is used as a benchmark to demonstrate good performance without\naccounting for the complexities of criminal justice (CJ) processes. However, we\nshow that pretrial RAI datasets can contain numerous measurement biases and\nerrors, and due to disparities in discretion and deployment, algorithmic\nfairness applied to RAI datasets is limited in making claims about real-world\noutcomes. These reasons make the datasets a poor fit for benchmarking under\nassumptions of ground truth and real-world impact. Furthermore, conventional\npractices of simply replicating previous data experiments may implicitly\ninherit or edify normative positions without explicitly interrogating\nvalue-laden assumptions. Without context of how interdisciplinary fields have\nengaged in CJ research and context of how RAIs operate upstream and downstream,\nalgorithmic fairness practices are misaligned for meaningful contribution in\nthe context of CJ, and would benefit from transparent engagement with normative\nconsiderations and values related to fairness, justice, and equality. These\nfactors prompt questions about whether benchmarks for intrinsically\nsocio-technical systems like the CJ system can exist in a beneficial and\nethical way.",
    "published_date": "2021-06-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.05498v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.05424v4",
    "title": "Fair Disaster Containment via Graph-Cut Problems",
    "authors": [
      "Michael Dinitz",
      "Aravind Srinivasan",
      "Leonidas Tsepenekas",
      "Anil Vullikanti"
    ],
    "author_ids": [],
    "abstract": "Graph cut problems are fundamental in Combinatorial Optimization, and are a\ncentral object of study in both theory and practice. Furthermore, the study of\n\\emph{fairness} in Algorithmic Design and Machine Learning has recently\nreceived significant attention, with many different notions proposed and\nanalyzed for a variety of contexts. In this paper we initiate the study of\nfairness for graph cut problems by giving the first fair definitions for them,\nand subsequently we demonstrate appropriate algorithmic techniques that yield a\nrigorous theoretical analysis. Specifically, we incorporate two different\nnotions of fairness, namely \\emph{demographic} and \\emph{probabilistic\nindividual} fairness, in a particular cut problem that models disaster\ncontainment scenarios. Our results include a variety of approximation\nalgorithms with provable theoretical guarantees.",
    "published_date": "2021-06-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DS",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.05424v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.05423v3",
    "title": "A New Notion of Individually Fair Clustering: $α$-Equitable $k$-Center",
    "authors": [
      "Darshan Chakrabarti",
      "John P. Dickerson",
      "Seyed A. Esmaeili",
      "Aravind Srinivasan",
      "Leonidas Tsepenekas"
    ],
    "author_ids": [],
    "abstract": "Clustering is a fundamental problem in unsupervised machine learning, and\nfair variants of it have recently received significant attention due to its\nsocietal implications. In this work we introduce a novel definition of\nindividual fairness for clustering problems. Specifically, in our model, each\npoint $j$ has a set of other points $\\mathcal{S}_j$ that it perceives as\nsimilar to itself, and it feels that it is fairly treated if the quality of\nservice it receives in the solution is $\\alpha$-close (in a multiplicative\nsense, for a given $\\alpha \\geq 1$) to that of the points in $\\mathcal{S}_j$.\nWe begin our study by answering questions regarding the structure of the\nproblem, namely for what values of $\\alpha$ the problem is well-defined, and\nwhat the behavior of the \\emph{Price of Fairness (PoF)} for it is. For the\nwell-defined region of $\\alpha$, we provide efficient and easily-implementable\napproximation algorithms for the $k$-center objective, which in certain cases\nenjoy bounded-PoF guarantees. We finally complement our analysis by an\nextensive suite of experiments that validates the effectiveness of our\ntheoretical results.",
    "published_date": "2021-06-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.05423v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.05391v1",
    "title": "Fairness-Aware Node Representation Learning",
    "authors": [
      "Öykü Deniz Köse",
      "Yanning Shen"
    ],
    "author_ids": [],
    "abstract": "Node representation learning has demonstrated its effectiveness for various\napplications on graphs. Particularly, recent developments in contrastive\nlearning have led to promising results in unsupervised node representation\nlearning for a number of tasks. Despite the success of graph contrastive\nlearning and consequent growing interest, fairness is largely under-explored in\nthe field. To this end, this study addresses fairness issues in graph\ncontrastive learning with fairness-aware graph augmentation designs, through\nadaptive feature masking and edge deletion. In the study, different fairness\nnotions on graphs are introduced, which serve as guidelines for the proposed\ngraph augmentations. Furthermore, theoretical analysis is provided to\nquantitatively prove that the proposed feature masking approach can reduce\nintrinsic bias. Experimental results on real social networks are presented to\ndemonstrate that the proposed augmentations can enhance fairness in terms of\nstatistical parity and equal opportunity, while providing comparable\nclassification accuracy to state-of-the-art contrastive methods for node\nclassification.",
    "published_date": "2021-06-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.05391v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.05360v2",
    "title": "Proportional Participatory Budgeting with Projects Interaction",
    "authors": [
      "Roy Fairstein",
      "Reshef Meir",
      "Kobi Gal"
    ],
    "author_ids": [],
    "abstract": "Participatory budgeting (PB) is a democratic process for allocating funds to\nprojects based on the votes of community members. PB outcomes are commonly\nevaluated for how they reflect voters preferences (e.g., social welfare) and\nthe extent to which they are fair (e.g., proportionality). Due to practical and\ncomputational reasons, voters are usually asked to report their preferences\nover projects separately, possibly neglecting important dependencies among\nprojects, which causes the outcome to no longer be proportional and achieve\nlower satisfaction. This work is the first to suggest a polynomial-time\naggregation method capable of guaranteeing proportional outcomes under\nsubstitution dependencies. The method is a variant of the Method of Equal\nShares, and we further provide another variation that can guarantee a more\nrelaxed notion of proportionality for any type of dependency, and is FPT rather\nthan polynomial. Through simulations, we demonstrate that these aggregation\nmethods achieve, on average, higher social welfare than their counterparts that\nignore the dependencies.",
    "published_date": "2021-06-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.05360v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.07464v6",
    "title": "Meta-Interpretive Learning as Metarule Specialisation",
    "authors": [
      "Stassa Patsantzis",
      "Stephen H. Muggleton"
    ],
    "author_ids": [],
    "abstract": "In Meta-Interpretive Learning (MIL) the metarules, second-order datalog\nclauses acting as inductive bias, are manually defined by the user. In this\nwork we show that second-order metarules for MIL can be learned by MIL. We\ndefine a generality ordering of metarules by $\\theta$-subsumption and show that\nuser-defined \\emph{sort metarules} are derivable by specialisation of the\nmost-general \\emph{matrix metarules} in a language class; and that these matrix\nmetarules are in turn derivable by specialisation of third-order \\emph{punch\nmetarules} with variables quantified over the set of atoms and for which only\nan upper bound on their number of literals need be user-defined. We show that\nthe cardinality of a metarule language is polynomial in the number of literals\nin punch metarules. We re-frame MIL as metarule specialisation by resolution.\nWe modify the MIL metarule specialisation operator to return new metarules\nrather than first-order clauses and prove the correctness of the new operator.\nWe implement the new operator as TOIL, a sub-system of the MIL system Louise.\nOur experiments show that as user-defined sort metarules are progressively\nreplaced by sort metarules learned by TOIL, Louise's predictive accuracy and\ntraining times are maintained. We conclude that automatically derived metarules\ncan replace user-defined metarules.",
    "published_date": "2021-06-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.LO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.07464v6",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.05127v1",
    "title": "Deep Clustering based Fair Outlier Detection",
    "authors": [
      "Hanyu Song",
      "Peizhao Li",
      "Hongfu Liu"
    ],
    "author_ids": [],
    "abstract": "In this paper, we focus on the fairness issues regarding unsupervised outlier\ndetection. Traditional algorithms, without a specific design for algorithmic\nfairness, could implicitly encode and propagate statistical bias in data and\nraise societal concerns. To correct such unfairness and deliver a fair set of\npotential outlier candidates, we propose Deep Clustering based Fair Outlier\nDetection (DCFOD) that learns a good representation for utility maximization\nwhile enforcing the learnable representation to be subgroup-invariant on the\nsensitive attribute. Considering the coupled and reciprocal nature between\nclustering and outlier detection, we leverage deep clustering to discover the\nintrinsic cluster structure and out-of-structure instances. Meanwhile, an\nadversarial training erases the sensitive pattern for instances for fairness\nadaptation. Technically, we propose an instance-level weighted representation\nlearning strategy to enhance the joint deep clustering and outlier detection,\nwhere the dynamic weight module re-emphasizes contributions of likely-inliers\nwhile mitigating the negative impact from outliers. Demonstrated by experiments\non eight datasets comparing to 17 outlier detection algorithms, our DCFOD\nmethod consistently achieves superior performance on both the outlier detection\nvalidity and two types of fairness notions in outlier detection.",
    "published_date": "2021-06-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.05127v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.05036v1",
    "title": "Towards Defending against Adversarial Examples via Attack-Invariant Features",
    "authors": [
      "Dawei Zhou",
      "Tongliang Liu",
      "Bo Han",
      "Nannan Wang",
      "Chunlei Peng",
      "Xinbo Gao"
    ],
    "author_ids": [],
    "abstract": "Deep neural networks (DNNs) are vulnerable to adversarial noise. Their\nadversarial robustness can be improved by exploiting adversarial examples.\nHowever, given the continuously evolving attacks, models trained on seen types\nof adversarial examples generally cannot generalize well to unseen types of\nadversarial examples. To solve this problem, in this paper, we propose to\nremove adversarial noise by learning generalizable invariant features across\nattacks which maintain semantic classification information. Specifically, we\nintroduce an adversarial feature learning mechanism to disentangle invariant\nfeatures from adversarial noise. A normalization term has been proposed in the\nencoded space of the attack-invariant features to address the bias issue\nbetween the seen and unseen types of attacks. Empirical evaluations demonstrate\nthat our method could provide better protection in comparison to previous\nstate-of-the-art approaches, especially against unseen types of attacks and\nadaptive attacks.",
    "published_date": "2021-06-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.05036v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.05001v2",
    "title": "No Fear of Heterogeneity: Classifier Calibration for Federated Learning with Non-IID Data",
    "authors": [
      "Mi Luo",
      "Fei Chen",
      "Dapeng Hu",
      "Yifan Zhang",
      "Jian Liang",
      "Jiashi Feng"
    ],
    "author_ids": [],
    "abstract": "A central challenge in training classification models in the real-world\nfederated system is learning with non-IID data. To cope with this, most of the\nexisting works involve enforcing regularization in local optimization or\nimproving the model aggregation scheme at the server. Other works also share\npublic datasets or synthesized samples to supplement the training of\nunder-represented classes or introduce a certain level of personalization.\nThough effective, they lack a deep understanding of how the data heterogeneity\naffects each layer of a deep classification model. In this paper, we bridge\nthis gap by performing an experimental analysis of the representations learned\nby different layers. Our observations are surprising: (1) there exists a\ngreater bias in the classifier than other layers, and (2) the classification\nperformance can be significantly improved by post-calibrating the classifier\nafter federated training. Motivated by the above findings, we propose a novel\nand simple algorithm called Classifier Calibration with Virtual Representations\n(CCVR), which adjusts the classifier using virtual representations sampled from\nan approximated gaussian mixture model. Experimental results demonstrate that\nCCVR achieves state-of-the-art performance on popular federated learning\nbenchmarks including CIFAR-10, CIFAR-100, and CINIC-10. We hope that our simple\nyet effective method can shed some light on the future research of federated\nlearning with non-IID data.",
    "published_date": "2021-06-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CV",
      "cs.DC",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.05001v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.04990v2",
    "title": "It Takes Two to Tango: Mixup for Deep Metric Learning",
    "authors": [
      "Shashanka Venkataramanan",
      "Bill Psomas",
      "Ewa Kijak",
      "Laurent Amsaleg",
      "Konstantinos Karantzalos",
      "Yannis Avrithis"
    ],
    "author_ids": [],
    "abstract": "Metric learning involves learning a discriminative representation such that\nembeddings of similar classes are encouraged to be close, while embeddings of\ndissimilar classes are pushed far apart. State-of-the-art methods focus mostly\non sophisticated loss functions or mining strategies. On the one hand, metric\nlearning losses consider two or more examples at a time. On the other hand,\nmodern data augmentation methods for classification consider two or more\nexamples at a time. The combination of the two ideas is under-studied.\n  In this work, we aim to bridge this gap and improve representations using\nmixup, which is a powerful data augmentation approach interpolating two or more\nexamples and corresponding target labels at a time. This task is challenging\nbecause unlike classification, the loss functions used in metric learning are\nnot additive over examples, so the idea of interpolating target labels is not\nstraightforward. To the best of our knowledge, we are the first to investigate\nmixing both examples and target labels for deep metric learning. We develop a\ngeneralized formulation that encompasses existing metric learning loss\nfunctions and modify it to accommodate for mixup, introducing Metric Mix, or\nMetrix. We also introduce a new metric - utilization, to demonstrate that by\nmixing examples during training, we are exploring areas of the embedding space\nbeyond the training classes, thereby improving representations. To validate the\neffect of improved representations, we show that mixing inputs, intermediate\nrepresentations or embeddings along with target labels significantly\noutperforms state-of-the-art metric learning methods on four benchmark deep\nmetric learning datasets.",
    "published_date": "2021-06-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.04990v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.04795v2",
    "title": "Nonasymptotic theory for two-layer neural networks: Beyond the bias-variance trade-off",
    "authors": [
      "Huiyuan Wang",
      "Wei Lin"
    ],
    "author_ids": [],
    "abstract": "Large neural networks have proved remarkably effective in modern deep\nlearning practice, even in the overparametrized regime where the number of\nactive parameters is large relative to the sample size. This contradicts the\nclassical perspective that a machine learning model must trade off bias and\nvariance for optimal generalization. To resolve this conflict, we present a\nnonasymptotic generalization theory for two-layer neural networks with ReLU\nactivation function by incorporating scaled variation regularization.\nInterestingly, the regularizer is equivalent to ridge regression from the angle\nof gradient-based optimization, but plays a similar role to the group lasso in\ncontrolling the model complexity. By exploiting this \"ridge-lasso duality,\" we\nobtain new prediction bounds for all network widths, which reproduce the double\ndescent phenomenon. Moreover, the overparametrized minimum risk is lower than\nits underparametrized counterpart when the signal is strong, and is nearly\nminimax optimal over a suitable class of functions. By contrast, we show that\noverparametrized random feature models suffer from the curse of dimensionality\nand thus are suboptimal.",
    "published_date": "2021-06-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "math.ST",
      "stat.ML",
      "stat.TH",
      "62G08 (Primary) 62J07, 68T07 (Secondary)"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.04795v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.04765v2",
    "title": "Predicting Deep Neural Network Generalization with Perturbation Response Curves",
    "authors": [
      "Yair Schiff",
      "Brian Quanz",
      "Payel Das",
      "Pin-Yu Chen"
    ],
    "author_ids": [],
    "abstract": "The field of Deep Learning is rich with empirical evidence of human-like\nperformance on a variety of prediction tasks. However, despite these successes,\nthe recent Predicting Generalization in Deep Learning (PGDL) NeurIPS 2020\ncompetition suggests that there is a need for more robust and efficient\nmeasures of network generalization. In this work, we propose a new framework\nfor evaluating the generalization capabilities of trained networks. We use\nperturbation response (PR) curves that capture the accuracy change of a given\nnetwork as a function of varying levels of training sample perturbation. From\nthese PR curves, we derive novel statistics that capture generalization\ncapability. Specifically, we introduce two new measures for accurately\npredicting generalization gaps: the Gi-score and Pal-score, which are inspired\nby the Gini coefficient and Palma ratio (measures of income inequality), that\naccurately predict generalization gaps. Using our framework applied to intra\nand inter-class sample mixup, we attain better predictive scores than the\ncurrent state-of-the-art measures on a majority of tasks in the PGDL\ncompetition. In addition, we show that our framework and the proposed\nstatistics can be used to capture to what extent a trained network is invariant\nto a given parametric input transformation, such as rotation or translation.\nTherefore, these generalization gap prediction statistics also provide a useful\nmeans for selecting optimal network architectures and hyperparameters that are\ninvariant to a certain perturbation.",
    "published_date": "2021-06-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.04765v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.04566v1",
    "title": "Data-Efficient Instance Generation from Instance Discrimination",
    "authors": [
      "Ceyuan Yang",
      "Yujun Shen",
      "Yinghao Xu",
      "Bolei Zhou"
    ],
    "author_ids": [],
    "abstract": "Generative Adversarial Networks (GANs) have significantly advanced image\nsynthesis, however, the synthesis quality drops significantly given a limited\namount of training data. To improve the data efficiency of GAN training, prior\nwork typically employs data augmentation to mitigate the overfitting of the\ndiscriminator yet still learn the discriminator with a bi-classification (i.e.,\nreal vs. fake) task. In this work, we propose a data-efficient Instance\nGeneration (InsGen) method based on instance discrimination. Concretely,\nbesides differentiating the real domain from the fake domain, the discriminator\nis required to distinguish every individual image, no matter it comes from the\ntraining set or from the generator. In this way, the discriminator can benefit\nfrom the infinite synthesized samples for training, alleviating the overfitting\nproblem caused by insufficient training data. A noise perturbation strategy is\nfurther introduced to improve its discriminative power. Meanwhile, the learned\ninstance discrimination capability from the discriminator is in turn exploited\nto encourage the generator for diverse generation. Extensive experiments\ndemonstrate the effectiveness of our method on a variety of datasets and\ntraining settings. Noticeably, on the setting of 2K training images from the\nFFHQ dataset, we outperform the state-of-the-art approach with 23.5% FID\nimprovement.",
    "published_date": "2021-06-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.04566v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.04265v2",
    "title": "Towards Social Role-Based Interruptibility Management",
    "authors": [
      "Christoph Anderson",
      "Judith Simone Heinisch",
      "Shohreh Deldari",
      "Flora D. Salim",
      "Sandra Ohly",
      "Klaus David",
      "Veljko Pejovic"
    ],
    "author_ids": [],
    "abstract": "Pervasive and ubiquitous computing facilitates immediate access to\ninformation in the sense of always-on. Information such as news, messages, or\nreminders can significantly enhance our daily routines but are rendered useless\nor disturbing when not being aligned with our intrinsic interruptibility\npreferences. Attention management systems use machine learning to identify\nshort-term opportune moments, so that information delivery leads to fewer\ninterruptions. Humans' intrinsic interruptibility preferences - established for\nand across social roles and life domains - would complement short-term\nattention and interruption management approaches. In this article, we present\nour comprehensive results towards social role-based attention and\ninterruptibility management. Our approach combines on-device sensing and\nmachine learning with theories from social science to form a personalized\ntwo-stage classification model. Finally, we discuss the challenges of the\ncurrent and future AI-driven attention management systems concerning privacy,\nethical issues, and future directions.",
    "published_date": "2021-06-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.04265v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.04008v2",
    "title": "Widening Access to Applied Machine Learning with TinyML",
    "authors": [
      "Vijay Janapa Reddi",
      "Brian Plancher",
      "Susan Kennedy",
      "Laurence Moroney",
      "Pete Warden",
      "Anant Agarwal",
      "Colby Banbury",
      "Massimo Banzi",
      "Matthew Bennett",
      "Benjamin Brown",
      "Sharad Chitlangia",
      "Radhika Ghosal",
      "Sarah Grafman",
      "Rupert Jaeger",
      "Srivatsan Krishnan",
      "Maximilian Lam",
      "Daniel Leiker",
      "Cara Mann",
      "Mark Mazumder",
      "Dominic Pajak",
      "Dhilan Ramaprasad",
      "J. Evan Smith",
      "Matthew Stewart",
      "Dustin Tingley"
    ],
    "author_ids": [],
    "abstract": "Broadening access to both computational and educational resources is critical\nto diffusing machine-learning (ML) innovation. However, today, most ML\nresources and experts are siloed in a few countries and organizations. In this\npaper, we describe our pedagogical approach to increasing access to applied ML\nthrough a massive open online course (MOOC) on Tiny Machine Learning (TinyML).\nWe suggest that TinyML, ML on resource-constrained embedded devices, is an\nattractive means to widen access because TinyML both leverages low-cost and\nglobally accessible hardware, and encourages the development of complete,\nself-contained applications, from data collection to deployment. To this end, a\ncollaboration between academia (Harvard University) and industry (Google)\nproduced a four-part MOOC that provides application-oriented instruction on how\nto develop solutions using TinyML. The series is openly available on the edX\nMOOC platform, has no prerequisites beyond basic programming, and is designed\nfor learners from a global variety of backgrounds. It introduces pupils to\nreal-world applications, ML algorithms, data-set engineering, and the ethical\nconsiderations of these technologies via hands-on programming and deployment of\nTinyML applications in both the cloud and their own microcontrollers. To\nfacilitate continued learning, community building, and collaboration beyond the\ncourses, we launched a standalone website, a forum, a chat, and an optional\ncourse-project competition. We also released the course materials publicly,\nhoping they will inspire the next generation of ML practitioners and educators\nand further broaden access to cutting-edge ML technologies.",
    "published_date": "2021-06-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.04008v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.03948v2",
    "title": "Nonequilibrium Thermodynamics in Measuring Carbon Footprints: Disentangling Structure and Artifact in Input-Output Accounting",
    "authors": [
      "Samuel P. Loomis",
      "Mark Cooper",
      "James P. Crutchfield"
    ],
    "author_ids": [],
    "abstract": "Multiregional input-output (MRIO) tables, in conjunction with Leontief\nanalysis, are widely-used to assess the geographical distribution of carbon\nemissions and the economic activities that cause them. Majorization, a tool\noriginating in economics that has found utility in statistical mechanics, can\nprovide insight into how Leontief analysis links disparities in emissions with\nglobal income inequality. We examine Leontief analysis as a model, drawing out\nsimilarities with modern nonequilibrium statistical mechanics. Paralleling the\nphysical concept of thermo-majorization, we define the concept of\neco-majorization and show it is a sufficient condition to determine the\ndirectionality of embodied emission flows. Surprisingly, relatively small trade\ndeficits and a geographically heterogeneous emissions-per-dollar ratio greatly\nincreases the appearance of eco-majorization, regardless of any further content\nin the MRIO tables used. Our results are bolstered by a statistical analysis of\nnull models of MRIO tables, based on data provided by the Global Trade\nAggregation Project9",
    "published_date": "2021-06-07T00:00:00",
    "year": 2021,
    "categories": [
      "physics.soc-ph",
      "cond-mat.stat-mech",
      "cs.IT",
      "math.IT",
      "quant-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.03948v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.03921v1",
    "title": "Measuring and Improving BERT's Mathematical Abilities by Predicting the Order of Reasoning",
    "authors": [
      "Piotr Piękos",
      "Henryk Michalewski",
      "Mateusz Malinowski"
    ],
    "author_ids": [],
    "abstract": "Imagine you are in a supermarket. You have two bananas in your basket and\nwant to buy four apples. How many fruits do you have in total? This seemingly\nstraightforward question can be challenging for data-driven language models,\neven if trained at scale. However, we would expect such generic language models\nto possess some mathematical abilities in addition to typical linguistic\ncompetence. Towards this goal, we investigate if a commonly used language\nmodel, BERT, possesses such mathematical abilities and, if so, to what degree.\nFor that, we fine-tune BERT on a popular dataset for word math problems,\nAQuA-RAT, and conduct several tests to understand learned representations\nbetter. Since we teach models trained on natural language to do formal\nmathematics, we hypothesize that such models would benefit from training on\nsemi-formal steps that explain how math results are derived. To better\naccommodate such training, we also propose new pretext tasks for learning\nmathematical rules. We call them (Neighbor) Reasoning Order Prediction (ROP or\nNROP). With this new model, we achieve significantly better outcomes than\ndata-driven baselines and even on-par with more tailored models. We also show\nhow to reduce positional bias in such models.",
    "published_date": "2021-06-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "I.2.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.03921v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.03783v2",
    "title": "An Information-theoretic Approach to Distribution Shifts",
    "authors": [
      "Marco Federici",
      "Ryota Tomioka",
      "Patrick Forré"
    ],
    "author_ids": [],
    "abstract": "Safely deploying machine learning models to the real world is often a\nchallenging process. Models trained with data obtained from a specific\ngeographic location tend to fail when queried with data obtained elsewhere,\nagents trained in a simulation can struggle to adapt when deployed in the real\nworld or novel environments, and neural networks that are fit to a subset of\nthe population might carry some selection bias into their decision process. In\nthis work, we describe the problem of data shift from a novel\ninformation-theoretic perspective by (i) identifying and describing the\ndifferent sources of error, (ii) comparing some of the most promising\nobjectives explored in the recent domain generalization, and fair\nclassification literature. From our theoretical analysis and empirical\nevaluation, we conclude that the model selection procedure needs to be guided\nby careful considerations regarding the observed data, the factors used for\ncorrection, and the structure of the data-generating process.",
    "published_date": "2021-06-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.03783v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.03765v2",
    "title": "On Inductive Biases for Heterogeneous Treatment Effect Estimation",
    "authors": [
      "Alicia Curth",
      "Mihaela van der Schaar"
    ],
    "author_ids": [],
    "abstract": "We investigate how to exploit structural similarities of an individual's\npotential outcomes (POs) under different treatments to obtain better estimates\nof conditional average treatment effects in finite samples. Especially when it\nis unknown whether a treatment has an effect at all, it is natural to\nhypothesize that the POs are similar - yet, some existing strategies for\ntreatment effect estimation employ regularization schemes that implicitly\nencourage heterogeneity even when it does not exist and fail to fully make use\nof shared structure. In this paper, we investigate and compare three end-to-end\nlearning strategies to overcome this problem - based on regularization,\nreparametrization and a flexible multi-task architecture - each encoding\ninductive bias favoring shared behavior across POs. To build understanding of\ntheir relative strengths, we implement all strategies using neural networks and\nconduct a wide range of semi-synthetic experiments. We observe that all three\napproaches can lead to substantial improvements upon numerous baselines and\ngain insight into performance differences across various experimental settings.",
    "published_date": "2021-06-07T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.03765v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.03761v4",
    "title": "FairCal: Fairness Calibration for Face Verification",
    "authors": [
      "Tiago Salvador",
      "Stephanie Cairns",
      "Vikram Voleti",
      "Noah Marshall",
      "Adam Oberman"
    ],
    "author_ids": [],
    "abstract": "Despite being widely used, face recognition models suffer from bias: the\nprobability of a false positive (incorrect face match) strongly depends on\nsensitive attributes such as the ethnicity of the face. As a result, these\nmodels can disproportionately and negatively impact minority groups,\nparticularly when used by law enforcement. The majority of bias reduction\nmethods have several drawbacks: they use an end-to-end retraining approach, may\nnot be feasible due to privacy issues, and often reduce accuracy. An\nalternative approach is post-processing methods that build fairer decision\nclassifiers using the features of pre-trained models, thus avoiding the cost of\nretraining. However, they still have drawbacks: they reduce accuracy (AGENDA,\nPASS, FTC), or require retuning for different false positive rates (FSN). In\nthis work, we introduce the Fairness Calibration (FairCal) method, a\npost-training approach that simultaneously: (i) increases model accuracy\n(improving the state-of-the-art), (ii) produces fairly-calibrated\nprobabilities, (iii) significantly reduces the gap in the false positive rates,\n(iv) does not require knowledge of the sensitive attribute, and (v) does not\nrequire retraining, training an additional model, or retuning. We apply it to\nthe task of Face Verification, and obtain state-of-the-art results with all the\nabove advantages.",
    "published_date": "2021-06-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.03761v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.03673v2",
    "title": "Algorithms and Decision-Making in the Public Sector",
    "authors": [
      "Karen Levy",
      "Kyla Chasalow",
      "Sarah Riley"
    ],
    "author_ids": [],
    "abstract": "This article surveys the use of algorithmic systems to support\ndecision-making in the public sector. Governments adopt, procure, and use\nalgorithmic systems to support their functions within several contexts --\nincluding criminal justice, education, and benefits provision -- with important\nconsequences for accountability, privacy, social inequity, and public\nparticipation in decision-making. We explore the social implications of\nmunicipal algorithmic systems across a variety of stages, including problem\nformulation, technology acquisition, deployment, and evaluation. We highlight\nseveral open questions that require further empirical research.",
    "published_date": "2021-06-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.03673v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.03631v1",
    "title": "Unsupervised Representation Disentanglement of Text: An Evaluation on Synthetic Datasets",
    "authors": [
      "Lan Zhang",
      "Victor Prokhorov",
      "Ehsan Shareghi"
    ],
    "author_ids": [],
    "abstract": "To highlight the challenges of achieving representation disentanglement for\ntext domain in an unsupervised setting, in this paper we select a\nrepresentative set of successfully applied models from the image domain. We\nevaluate these models on 6 disentanglement metrics, as well as on downstream\nclassification tasks and homotopy. To facilitate the evaluation, we propose two\nsynthetic datasets with known generative factors. Our experiments highlight the\nexisting gap in the text domain and illustrate that certain elements such as\nrepresentation sparsity (as an inductive bias), or representation coupling with\nthe decoder could impact disentanglement. To the best of our knowledge, our\nwork is the first attempt on the intersection of unsupervised representation\ndisentanglement and text, and provides the experimental framework and datasets\nfor examining future developments in this direction.",
    "published_date": "2021-06-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.03631v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.03614v1",
    "title": "Adversarial Attack and Defense in Deep Ranking",
    "authors": [
      "Mo Zhou",
      "Le Wang",
      "Zhenxing Niu",
      "Qilin Zhang",
      "Nanning Zheng",
      "Gang Hua"
    ],
    "author_ids": [],
    "abstract": "Deep Neural Network classifiers are vulnerable to adversarial attack, where\nan imperceptible perturbation could result in misclassification. However, the\nvulnerability of DNN-based image ranking systems remains under-explored. In\nthis paper, we propose two attacks against deep ranking systems, i.e.,\nCandidate Attack and Query Attack, that can raise or lower the rank of chosen\ncandidates by adversarial perturbations. Specifically, the expected ranking\norder is first represented as a set of inequalities, and then a triplet-like\nobjective function is designed to obtain the optimal perturbation. Conversely,\nan anti-collapse triplet defense is proposed to improve the ranking model\nrobustness against all proposed attacks, where the model learns to prevent the\npositive and negative samples being pulled close to each other by adversarial\nattack. To comprehensively measure the empirical adversarial robustness of a\nranking model with our defense, we propose an empirical robustness score, which\ninvolves a set of representative attacks against ranking models. Our\nadversarial ranking attacks and defenses are evaluated on MNIST, Fashion-MNIST,\nCUB200-2011, CARS196 and Stanford Online Products datasets. Experimental\nresults demonstrate that a typical deep ranking system can be effectively\ncompromised by our attacks. Nevertheless, our defense can significantly improve\nthe ranking system robustness, and simultaneously mitigate a wide range of\nattacks.",
    "published_date": "2021-06-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.03614v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.03569v4",
    "title": "Socially-Aware Self-Supervised Tri-Training for Recommendation",
    "authors": [
      "Junliang Yu",
      "Hongzhi Yin",
      "Min Gao",
      "Xin Xia",
      "Xiangliang Zhang",
      "Nguyen Quoc Viet Hung"
    ],
    "author_ids": [],
    "abstract": "Self-supervised learning (SSL), which can automatically generate ground-truth\nsamples from raw data, holds vast potential to improve recommender systems.\nMost existing SSL-based methods perturb the raw data graph with uniform\nnode/edge dropout to generate new data views and then conduct the\nself-discrimination based contrastive learning over different views to learn\ngeneralizable representations. Under this scheme, only a bijective mapping is\nbuilt between nodes in two different views, which means that the\nself-supervision signals from other nodes are being neglected. Due to the\nwidely observed homophily in recommender systems, we argue that the supervisory\nsignals from other nodes are also highly likely to benefit the representation\nlearning for recommendation. To capture these signals, a general socially-aware\nSSL framework that integrates tri-training is proposed in this paper.\nTechnically, our framework first augments the user data views with the user\nsocial information. And then under the regime of tri-training for multi-view\nencoding, the framework builds three graph encoders (one for recommendation)\nupon the augmented views and iteratively improves each encoder with\nself-supervision signals from other users, generated by the other two encoders.\nSince the tri-training operates on the augmented views of the same data sources\nfor self-supervision signals, we name it self-supervised tri-training.\nExtensive experiments on multiple real-world datasets consistently validate the\neffectiveness of the self-supervised tri-training framework for improving\nrecommendation. The code is released at https://github.com/Coder-Yu/QRec.",
    "published_date": "2021-06-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.03569v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.03521v1",
    "title": "RedditBias: A Real-World Resource for Bias Evaluation and Debiasing of Conversational Language Models",
    "authors": [
      "Soumya Barikeri",
      "Anne Lauscher",
      "Ivan Vulić",
      "Goran Glavaš"
    ],
    "author_ids": [],
    "abstract": "Text representation models are prone to exhibit a range of societal biases,\nreflecting the non-controlled and biased nature of the underlying pretraining\ndata, which consequently leads to severe ethical issues and even bias\namplification. Recent work has predominantly focused on measuring and\nmitigating bias in pretrained language models. Surprisingly, the landscape of\nbias measurements and mitigation resources and methods for conversational\nlanguage models is still very scarce: it is limited to only a few types of\nbias, artificially constructed resources, and completely ignores the impact\nthat debiasing methods may have on the final performance in dialog tasks, e.g.,\nconversational response generation. In this work, we present RedditBias, the\nfirst conversational data set grounded in the actual human conversations from\nReddit, allowing for bias measurement and mitigation across four important bias\ndimensions: gender, race, religion, and queerness. Further, we develop an\nevaluation framework which simultaneously 1) measures bias on the developed\nRedditBias resource, and 2) evaluates model capability in dialog tasks after\nmodel debiasing. We use the evaluation framework to benchmark the widely used\nconversational DialoGPT model along with the adaptations of four debiasing\nmethods. Our results indicate that DialoGPT is biased with respect to religious\ngroups and that some debiasing techniques can remove this bias while preserving\ndownstream task performance.",
    "published_date": "2021-06-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.03521v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.03518v3",
    "title": "Position Bias Mitigation: A Knowledge-Aware Graph Model for Emotion Cause Extraction",
    "authors": [
      "Hanqi Yan",
      "Lin Gui",
      "Gabriele Pergola",
      "Yulan He"
    ],
    "author_ids": [],
    "abstract": "The Emotion Cause Extraction (ECE)} task aims to identify clauses which\ncontain emotion-evoking information for a particular emotion expressed in text.\nWe observe that a widely-used ECE dataset exhibits a bias that the majority of\nannotated cause clauses are either directly before their associated emotion\nclauses or are the emotion clauses themselves. Existing models for ECE tend to\nexplore such relative position information and suffer from the dataset bias. To\ninvestigate the degree of reliance of existing ECE models on clause relative\npositions, we propose a novel strategy to generate adversarial examples in\nwhich the relative position information is no longer the indicative feature of\ncause clauses. We test the performance of existing models on such adversarial\nexamples and observe a significant performance drop. To address the dataset\nbias, we propose a novel graph-based method to explicitly model the emotion\ntriggering paths by leveraging the commonsense knowledge to enhance the\nsemantic dependencies between a candidate clause and an emotion clause.\nExperimental results show that our proposed approach performs on par with the\nexisting state-of-the-art methods on the original ECE dataset, and is more\nrobust against adversarial attacks compared to existing models.",
    "published_date": "2021-06-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.03518v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.03485v4",
    "title": "Redundant representations help generalization in wide neural networks",
    "authors": [
      "Diego Doimo",
      "Aldo Glielmo",
      "Sebastian Goldt",
      "Alessandro Laio"
    ],
    "author_ids": [],
    "abstract": "Deep neural networks (DNNs) defy the classical bias-variance trade-off:\nadding parameters to a DNN that interpolates its training data will typically\nimprove its generalization performance. Explaining the mechanism behind this\n``benign overfitting'' in deep networks remains an outstanding challenge. Here,\nwe study the last hidden layer representations of various state-of-the-art\nconvolutional neural networks and find that if the last hidden representation\nis wide enough, its neurons tend to split into groups that carry identical\ninformation, and differ from each other only by statistically independent\nnoise. The number of such groups increases linearly with the width of the\nlayer, but only if the width is above a critical value. We show that redundant\nneurons appear only when the training process reaches interpolation and the\ntraining error is zero.",
    "published_date": "2021-06-07T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.03485v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.03386v2",
    "title": "Corona Health -- A Study- and Sensor-based Mobile App Platform Exploring Aspects of the COVID-19 Pandemic",
    "authors": [
      "Felix Beierle",
      "Johannes Schobel",
      "Carsten Vogel",
      "Johannes Allgaier",
      "Lena Mulansky",
      "Fabian Haug",
      "Julian Haug",
      "Winfried Schlee",
      "Marc Holfelder",
      "Michael Stach",
      "Marc Schickler",
      "Harald Baumeister",
      "Caroline Cohrdes",
      "Jürgen Deckert",
      "Lorenz Deserno",
      "Johanna-Sophie Edler",
      "Felizitas A. Eichner",
      "Helmut Greger",
      "Grit Hein",
      "Peter Heuschmann",
      "Dennis John",
      "Hans A. Kestler",
      "Dagmar Krefting",
      "Berthold Langguth",
      "Patrick Meybohm",
      "Thomas Probst",
      "Manfred Reichert",
      "Marcel Romanos",
      "Stefan Störk",
      "Yannik Terhorst",
      "Martin Weiß",
      "Rüdiger Pryss"
    ],
    "author_ids": [],
    "abstract": "Physical and mental well-being during the COVID-19 pandemic is typically\nassessed via surveys, which might make it difficult to conduct longitudinal\nstudies and might lead to data suffering from recall bias. Ecological momentary\nassessment (EMA) driven smartphone apps can help alleviate such issues,\nallowing for in situ recordings. Implementing such an app is not trivial,\nnecessitates strict regulatory and legal requirements, and requires short\ndevelopment cycles to appropriately react to abrupt changes in the pandemic.\nBased on an existing app framework, we developed Corona Health, an app that\nserves as a platform for deploying questionnaire-based studies in combination\nwith recordings of mobile sensors. In this paper, we present the technical\ndetails of Corona Health and provide first insights into the collected data.\nThrough collaborative efforts from experts from public health, medicine,\npsychology, and computer science, we released Corona Health publicly on Google\nPlay and the Apple App Store (in July, 2020) in 8 languages and attracted 7,290\ninstallations so far. Currently, five studies related to physical and mental\nwell-being are deployed and 17,241 questionnaires have been filled out. Corona\nHealth proves to be a viable tool for conducting research related to the\nCOVID-19 pandemic and can serve as a blueprint for future EMA-based studies.\nThe data we collected will substantially improve our knowledge on mental and\nphysical health states, traits and trajectories as well as its risk and\nprotective factors over the course of the COVID-19 pandemic and its diverse\nprevention measures.",
    "published_date": "2021-06-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.03386v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.03354v3",
    "title": "AI without networks",
    "authors": [
      "Partha P Mitra",
      "Clément Sire"
    ],
    "author_ids": [],
    "abstract": "Contemporary Artificial Intelligence (AI) stands on two legs: large training\ndata corpora and many-parameter artificial neural networks (ANNs). The data\ncorpora are needed to represent the complexity and heterogeneity of the world.\nThe role of the networks is less transparent due to the obscure dependence of\nthe network parameters and outputs on the training data and inputs. This raises\nproblems, ranging from technical-scientific to legal-ethical. We hypothesize\nthat a transparent approach to machine learning is possible without using\nnetworks at all. By generalizing a parameter-free, statistically consistent\ndata interpolation method, which we analyze theoretically in detail, we develop\na network-free framework for AI incorporating generative modeling. We\ndemonstrate this framework with examples from three different disciplines -\nethology, control theory, and mathematics. Our generative Hilbert framework\napplied to the trajectories of small groups of swimming fish outperformed\nstate-of-the-art traditional mathematical behavioral models and current\nANN-based models. We demonstrate pure data interpolation based control by\nstabilizing an inverted pendulum and a driven logistic map around unstable\nfixed points. Finally, we present a mathematical application by predicting\nzeros of the Riemann Zeta function, achieving comparable performance as a\ntransformer network. We do not suggest that the proposed framework will always\noutperform networks as over-parameterized networks can interpolate. However,\nour framework is theoretically sound, transparent, deterministic, and parameter\nfree: remarkably, it does not require any compute-expensive training, does not\ninvolve optimization, has no model selection, and is easily reproduced and\nported. We also propose an easily computed method of credit assignment based on\nthis framework, to help address ethical-legal challenges raised by generative\nAI.",
    "published_date": "2021-06-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cond-mat.stat-mech",
      "math.FA",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.03354v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.03348v4",
    "title": "ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias",
    "authors": [
      "Yufei Xu",
      "Qiming Zhang",
      "Jing Zhang",
      "Dacheng Tao"
    ],
    "author_ids": [],
    "abstract": "Transformers have shown great potential in various computer vision tasks\nowing to their strong capability in modeling long-range dependency using the\nself-attention mechanism. Nevertheless, vision transformers treat an image as\n1D sequence of visual tokens, lacking an intrinsic inductive bias (IB) in\nmodeling local visual structures and dealing with scale variance.\nAlternatively, they require large-scale training data and longer training\nschedules to learn the IB implicitly. In this paper, we propose a novel Vision\nTransformer Advanced by Exploring intrinsic IB from convolutions, ie, ViTAE.\nTechnically, ViTAE has several spatial pyramid reduction modules to downsample\nand embed the input image into tokens with rich multi-scale context by using\nmultiple convolutions with different dilation rates. In this way, it acquires\nan intrinsic scale invariance IB and is able to learn robust feature\nrepresentation for objects at various scales. Moreover, in each transformer\nlayer, ViTAE has a convolution block in parallel to the multi-head\nself-attention module, whose features are fused and fed into the feed-forward\nnetwork. Consequently, it has the intrinsic locality IB and is able to learn\nlocal features and global dependencies collaboratively. Experiments on ImageNet\nas well as downstream tasks prove the superiority of ViTAE over the baseline\ntransformer and concurrent works. Source code and pretrained models will be\navailable at GitHub.",
    "published_date": "2021-06-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.03348v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.03343v1",
    "title": "Energy Aligning for Biased Models",
    "authors": [
      "Bowen Zhao",
      "Chen Chen",
      "Qi Ju",
      "ShuTao Xia"
    ],
    "author_ids": [],
    "abstract": "Training on class-imbalanced data usually results in biased models that tend\nto predict samples into the majority classes, which is a common and notorious\nproblem. From the perspective of energy-based model, we demonstrate that the\nfree energies of categories are aligned with the label distribution\ntheoretically, thus the energies of different classes are expected to be close\nto each other when aiming for ``balanced'' performance. However, we discover a\nsevere energy-bias phenomenon in the models trained on class-imbalanced\ndataset. To eliminate the bias, we propose a simple and effective method named\nEnergy Aligning by merely adding the calculated shift scalars onto the output\nlogits during inference, which does not require to (i) modify the network\narchitectures, (ii) intervene the standard learning paradigm, (iii) perform\ntwo-stage training. The proposed algorithm is evaluated on two class\nimbalance-related tasks under various settings: class incremental learning and\nlong-tailed recognition. Experimental results show that energy aligning can\neffectively alleviate class imbalance issue and outperform state-of-the-art\nmethods on several benchmarks.",
    "published_date": "2021-06-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.03343v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.03297v1",
    "title": "On the Language Coverage Bias for Neural Machine Translation",
    "authors": [
      "Shuo Wang",
      "Zhaopeng Tu",
      "Zhixing Tan",
      "Shuming Shi",
      "Maosong Sun",
      "Yang Liu"
    ],
    "author_ids": [],
    "abstract": "Language coverage bias, which indicates the content-dependent differences\nbetween sentence pairs originating from the source and target languages, is\nimportant for neural machine translation (NMT) because the target-original\ntraining data is not well exploited in current practice. By carefully designing\nexperiments, we provide comprehensive analyses of the language coverage bias in\nthe training data, and find that using only the source-original data achieves\ncomparable performance with using full training data. Based on these\nobservations, we further propose two simple and effective approaches to\nalleviate the language coverage bias problem through explicitly distinguishing\nbetween the source- and target-original training data, which consistently\nimprove the performance over strong baselines on six WMT20 translation tasks.\nComplementary to the translationese effect, language coverage bias provides\nanother explanation for the performance drop caused by back-translation. We\nalso apply our approach to both back- and forward-translation and find that\nmitigating the language coverage bias can improve the performance of both the\ntwo representative data augmentation methods and their tagged variants.",
    "published_date": "2021-06-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.03297v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.03242v1",
    "title": "Highlighting the Importance of Reducing Research Bias and Carbon Emissions in CNNs",
    "authors": [
      "Ahmed Badar",
      "Arnav Varma",
      "Adrian Staniec",
      "Mahmoud Gamal",
      "Omar Magdy",
      "Haris Iqbal",
      "Elahe Arani",
      "Bahram Zonooz"
    ],
    "author_ids": [],
    "abstract": "Convolutional neural networks (CNNs) have become commonplace in addressing\nmajor challenges in computer vision. Researchers are not only coming up with\nnew CNN architectures but are also researching different techniques to improve\nthe performance of existing architectures. However, there is a tendency to\nover-emphasize performance improvement while neglecting certain important\nvariables such as simplicity, versatility, the fairness of comparisons, and\nenergy efficiency. Overlooking these variables in architectural design and\nevaluation has led to research bias and a significantly negative environmental\nimpact. Furthermore, this can undermine the positive impact of research in\nusing deep learning models to tackle climate change. Here, we perform an\nextensive and fair empirical study of a number of proposed techniques to gauge\nthe utility of each technique for segmentation and classification. Our findings\nrestate the importance of favoring simplicity over complexity in model design\n(Occam's Razor). Furthermore, our results indicate that simple standardized\npractices can lead to a significant reduction in environmental impact with\nlittle drop in performance. We highlight that there is a need to rethink the\ndesign and evaluation of CNNs to alleviate the issue of research bias and\ncarbon emissions.",
    "published_date": "2021-06-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.03242v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.03215v2",
    "title": "PreferenceNet: Encoding Human Preferences in Auction Design with Deep Learning",
    "authors": [
      "Neehar Peri",
      "Michael J. Curry",
      "Samuel Dooley",
      "John P. Dickerson"
    ],
    "author_ids": [],
    "abstract": "The design of optimal auctions is a problem of interest in economics, game\ntheory and computer science. Despite decades of effort, strategyproof,\nrevenue-maximizing auction designs are still not known outside of restricted\nsettings. However, recent methods using deep learning have shown some success\nin approximating optimal auctions, recovering several known solutions and\noutperforming strong baselines when optimal auctions are not known. In addition\nto maximizing revenue, auction mechanisms may also seek to encourage socially\ndesirable constraints such as allocation fairness or diversity. However, these\nphilosophical notions neither have standardization nor do they have widely\naccepted formal definitions. In this paper, we propose PreferenceNet, an\nextension of existing neural-network-based auction mechanisms to encode\nconstraints using (potentially human-provided) exemplars of desirable\nallocations. In addition, we introduce a new metric to evaluate an auction\nallocations' adherence to such socially desirable constraints and demonstrate\nthat our proposed method is competitive with current state-of-the-art\nneural-network based auction designs. We validate our approach through human\nsubject research and show that we are able to effectively capture real human\npreferences. Our code is available at\nhttps://github.com/neeharperi/PreferenceNet",
    "published_date": "2021-06-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.03215v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.03134v4",
    "title": "Pseudo-Riemannian Graph Convolutional Networks",
    "authors": [
      "Bo Xiong",
      "Shichao Zhu",
      "Nico Potyka",
      "Shirui Pan",
      "Chuan Zhou",
      "Steffen Staab"
    ],
    "author_ids": [],
    "abstract": "Graph convolutional networks (GCNs) are powerful frameworks for learning\nembeddings of graph-structured data. GCNs are traditionally studied through the\nlens of Euclidean geometry. Recent works find that non-Euclidean Riemannian\nmanifolds provide specific inductive biases for embedding hierarchical or\nspherical data. However, they cannot align well with data of mixed graph\ntopologies. We consider a larger class of pseudo-Riemannian manifolds that\ngeneralize hyperboloid and sphere. We develop new geodesic tools that allow for\nextending neural network operations into geodesically disconnected\npseudo-Riemannian manifolds. As a consequence, we derive a pseudo-Riemannian\nGCN that models data in pseudo-Riemannian manifolds of constant nonzero\ncurvature in the context of graph neural networks. Our method provides a\ngeometric inductive bias that is sufficiently flexible to model mixed\nheterogeneous topologies like hierarchical graphs with cycles. We demonstrate\nthe representational capabilities of this method by applying it to the tasks of\ngraph reconstruction, node classification and link prediction on a series of\nstandard graphs with mixed topologies. Empirical results demonstrate that our\nmethod outperforms Riemannian counterparts when embedding graphs of complex\ntopologies.",
    "published_date": "2021-06-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.03134v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.03106v2",
    "title": "Uformer: A General U-Shaped Transformer for Image Restoration",
    "authors": [
      "Zhendong Wang",
      "Xiaodong Cun",
      "Jianmin Bao",
      "Wengang Zhou",
      "Jianzhuang Liu",
      "Houqiang Li"
    ],
    "author_ids": [],
    "abstract": "In this paper, we present Uformer, an effective and efficient\nTransformer-based architecture for image restoration, in which we build a\nhierarchical encoder-decoder network using the Transformer block. In Uformer,\nthere are two core designs. First, we introduce a novel locally-enhanced window\n(LeWin) Transformer block, which performs nonoverlapping window-based\nself-attention instead of global self-attention. It significantly reduces the\ncomputational complexity on high resolution feature map while capturing local\ncontext. Second, we propose a learnable multi-scale restoration modulator in\nthe form of a multi-scale spatial bias to adjust features in multiple layers of\nthe Uformer decoder. Our modulator demonstrates superior capability for\nrestoring details for various image restoration tasks while introducing\nmarginal extra parameters and computational cost. Powered by these two designs,\nUformer enjoys a high capability for capturing both local and global\ndependencies for image restoration. To evaluate our approach, extensive\nexperiments are conducted on several image restoration tasks, including image\ndenoising, motion deblurring, defocus deblurring and deraining. Without bells\nand whistles, our Uformer achieves superior or comparable performance compared\nwith the state-of-the-art algorithms. The code and models are available at\nhttps://github.com/ZhendongWang6/Uformer.",
    "published_date": "2021-06-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.03106v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.03076v2",
    "title": "A Convergence Theory for SVGD in the Population Limit under Talagrand's Inequality T1",
    "authors": [
      "Adil Salim",
      "Lukang Sun",
      "Peter Richtárik"
    ],
    "author_ids": [],
    "abstract": "Stein Variational Gradient Descent (SVGD) is an algorithm for sampling from a\ntarget density which is known up to a multiplicative constant. Although SVGD is\na popular algorithm in practice, its theoretical study is limited to a few\nrecent works. We study the convergence of SVGD in the population limit, (i.e.,\nwith an infinite number of particles) to sample from a non-logconcave target\ndistribution satisfying Talagrand's inequality T1. We first establish the\nconvergence of the algorithm. Then, we establish a dimension-dependent\ncomplexity bound in terms of the Kernelized Stein Discrepancy (KSD). Unlike\nexisting works, we do not assume that the KSD is bounded along the trajectory\nof the algorithm. Our approach relies on interpreting SVGD as a gradient\ndescent over a space of probability measures.",
    "published_date": "2021-06-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.03076v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.02970v1",
    "title": "Modeling Coordinated vs. P2P Mining: An Analysis of Inefficiency and Inequality in Proof-of-Work Blockchains",
    "authors": [
      "Mohamed Alzayat",
      "Johnnatan Messias",
      "Balakrishnan Chandrasekaran",
      "Krishna P. Gummadi",
      "Patrick Loiseau"
    ],
    "author_ids": [],
    "abstract": "We study efficiency in a proof-of-work blockchain with non-zero latencies,\nfocusing in particular on the (inequality in) individual miners' efficiencies.\nPrior work attributed differences in miners' efficiencies mostly to attacks,\nbut we pursue a different question: Can inequality in miners' efficiencies be\nexplained by delays, even when all miners are honest? Traditionally, such\nefficiency-related questions were tackled only at the level of the overall\nsystem, and in a peer-to-peer (P2P) setting where miners directly connect to\none another. Despite it being common today for miners to pool compute\ncapacities in a mining pool managed by a centralized coordinator, efficiency in\nsuch a coordinated setting has barely been studied.\n  In this paper, we propose a simple model of a proof-of-work blockchain with\nlatencies for both the P2P and the coordinated settings. We derive a\nclosed-form expression for the efficiency in the coordinated setting with an\narbitrary number of miners and arbitrary latencies, both for the overall system\nand for each individual miner. We leverage this result to show that\ninequalities arise from variability in the delays, but that if all miners are\nequidistant from the coordinator, they have equal efficiency irrespective of\ntheir compute capacities. We then prove that, under a natural consistency\ncondition, the overall system efficiency in the P2P setting is higher than that\nin the coordinated setting. Finally, we perform a simulation-based study to\ndemonstrate that even in the P2P setting delays between miners introduce\ninequalities, and that there is a more complex interplay between delays and\ncompute capacities.",
    "published_date": "2021-06-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.02970v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.02866v2",
    "title": "Conditional Contrastive Learning for Improving Fairness in Self-Supervised Learning",
    "authors": [
      "Martin Q. Ma",
      "Yao-Hung Hubert Tsai",
      "Paul Pu Liang",
      "Han Zhao",
      "Kun Zhang",
      "Ruslan Salakhutdinov",
      "Louis-Philippe Morency"
    ],
    "author_ids": [],
    "abstract": "Contrastive self-supervised learning (SSL) learns an embedding space that\nmaps similar data pairs closer and dissimilar data pairs farther apart. Despite\nits success, one issue has been overlooked: the fairness aspect of\nrepresentations learned using contrastive SSL. Without mitigation, contrastive\nSSL techniques can incorporate sensitive information such as gender or race and\ncause potentially unfair predictions on downstream tasks. In this paper, we\npropose a Conditional Contrastive Learning (CCL) approach to improve the\nfairness of contrastive SSL methods. Our approach samples positive and negative\npairs from distributions conditioning on the sensitive attribute, or\nempirically speaking, sampling positive and negative pairs from the same gender\nor the same race. We show that our approach provably maximizes the conditional\nmutual information between the learned representations of the positive pairs,\nand reduces the effect of the sensitive attribute by taking it as the\nconditional variable. On seven fairness and vision datasets, we empirically\ndemonstrate that the proposed approach achieves state-of-the-art downstream\nperformances compared to unsupervised baselines and significantly improves the\nfairness of contrastive SSL models on multiple fairness metrics.",
    "published_date": "2021-06-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.02866v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.02817v1",
    "title": "ImGAGN:Imbalanced Network Embedding via Generative Adversarial Graph Networks",
    "authors": [
      "Liang Qu",
      "Huaisheng Zhu",
      "Ruiqi Zheng",
      "Yuhui Shi",
      "Hongzhi Yin"
    ],
    "author_ids": [],
    "abstract": "Imbalanced classification on graphs is ubiquitous yet challenging in many\nreal-world applications, such as fraudulent node detection. Recently, graph\nneural networks (GNNs) have shown promising performance on many network\nanalysis tasks. However, most existing GNNs have almost exclusively focused on\nthe balanced networks, and would get unappealing performance on the imbalanced\nnetworks. To bridge this gap, in this paper, we present a generative\nadversarial graph network model, called ImGAGN to address the imbalanced\nclassification problem on graphs. It introduces a novel generator for graph\nstructure data, named GraphGenerator, which can simulate both the minority\nclass nodes' attribute distribution and network topological structure\ndistribution by generating a set of synthetic minority nodes such that the\nnumber of nodes in different classes can be balanced. Then a graph\nconvolutional network (GCN) discriminator is trained to discriminate between\nreal nodes and fake (i.e., generated) nodes, and also between minority nodes\nand majority nodes on the synthetic balanced network. To validate the\neffectiveness of the proposed method, extensive experiments are conducted on\nfour real-world imbalanced network datasets. Experimental results demonstrate\nthat the proposed method ImGAGN outperforms state-of-the-art algorithms for\nsemi-supervised imbalanced node classification task.",
    "published_date": "2021-06-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.02817v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.02715v1",
    "title": "Auditing Source Diversity Bias in Video Search Results Using Virtual Agents",
    "authors": [
      "Aleksandra Urman",
      "Mykola Makhortykh",
      "Roberto Ulloa"
    ],
    "author_ids": [],
    "abstract": "We audit the presence of domain-level source diversity bias in video search\nresults. Using a virtual agent-based approach, we compare outputs of four\nWestern and one non-Western search engines for English and Russian queries. Our\nfindings highlight that source diversity varies substantially depending on the\nlanguage with English queries returning more diverse outputs. We also find\ndisproportionately high presence of a single platform, YouTube, in top search\noutputs for all Western search engines except Google. At the same time, we\nobserve that Youtube's major competitors such as Vimeo or Dailymotion do not\nappear in the sampled Google's video search results. This finding suggests that\nGoogle might be downgrading the results from the main competitors of\nGoogle-owned Youtube and highlights the necessity for further studies focusing\non the presence of own-content bias in Google's search results.",
    "published_date": "2021-06-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.02715v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.02705v1",
    "title": "Understanding and Improving Fairness-Accuracy Trade-offs in Multi-Task Learning",
    "authors": [
      "Yuyan Wang",
      "Xuezhi Wang",
      "Alex Beutel",
      "Flavien Prost",
      "Jilin Chen",
      "Ed H. Chi"
    ],
    "author_ids": [],
    "abstract": "As multi-task models gain popularity in a wider range of machine learning\napplications, it is becoming increasingly important for practitioners to\nunderstand the fairness implications associated with those models. Most\nexisting fairness literature focuses on learning a single task more fairly,\nwhile how ML fairness interacts with multiple tasks in the joint learning\nsetting is largely under-explored. In this paper, we are concerned with how\ngroup fairness (e.g., equal opportunity, equalized odds) as an ML fairness\nconcept plays out in the multi-task scenario. In multi-task learning, several\ntasks are learned jointly to exploit task correlations for a more efficient\ninductive transfer. This presents a multi-dimensional Pareto frontier on (1)\nthe trade-off between group fairness and accuracy with respect to each task, as\nwell as (2) the trade-offs across multiple tasks. We aim to provide a deeper\nunderstanding on how group fairness interacts with accuracy in multi-task\nlearning, and we show that traditional approaches that mainly focus on\noptimizing the Pareto frontier of multi-task accuracy might not perform well on\nfairness goals. We propose a new set of metrics to better capture the\nmulti-dimensional Pareto frontier of fairness-accuracy trade-offs uniquely\npresented in a multi-task learning setting. We further propose a\nMulti-Task-Aware Fairness (MTA-F) approach to improve fairness in multi-task\nlearning. Experiments on several real-world datasets demonstrate the\neffectiveness of our proposed approach.",
    "published_date": "2021-06-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.02705v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.02702v2",
    "title": "Subgroup Fairness in Two-Sided Markets",
    "authors": [
      "Quan Zhou",
      "Jakub Marecek",
      "Robert N. Shorten"
    ],
    "author_ids": [],
    "abstract": "It is well known that two-sided markets are unfair in a number of ways. For\ninstance, female workers at Uber earn less than their male colleagues per mile\ndriven. Similar observations have been made for other minority subgroups in\nother two-sided markets. Here, we suggest a novel market-clearing mechanism for\ntwo-sided markets, which promotes equalisation of the pay per hour worked\nacross multiple subgroups, as well as within each subgroup. In the process, we\nintroduce a novel notion of subgroup fairness (which we call Inter-fairness),\nwhich can be combined with other notions of fairness within each subgroup\n(called Intra-fairness), and the utility for the customers (Customer-Care) in\nthe objective of the market-clearing problem. While the novel non-linear terms\nin the objective complicate market clearing by making the problem non-convex,\nwe show that a certain non-convex augmented Lagrangian relaxation can be\napproximated to any precision in time polynomial in the number of market\nparticipants using semi-definite programming. This makes it possible to\nimplement the market-clearing mechanism efficiently. On the example of\ndriver-ride assignment in an Uber-like system, we demonstrate the efficacy and\nscalability of the approach, and trade-offs between Inter- and Intra-fairness.",
    "published_date": "2021-06-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.GT",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.02702v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.02688v2",
    "title": "Egalitarian Resource Sharing Over Multiple Rounds",
    "authors": [
      "Fu Li",
      "C. Gregory Plaxton",
      "Vaibhav B. Sinha"
    ],
    "author_ids": [],
    "abstract": "It is often beneficial for agents to pool their resources in order to better\naccommodate fluctuations in individual demand. Many multi-round resource\nallocation mechanisms operate in an online manner: in each round, the agents\nspecify their demands for that round, and the mechanism determines a\ncorresponding allocation. In this paper, we focus instead on the offline\nsetting in which the agents specify their demand for each round at the outset.\nWe formulate a specific resource allocation problem in this setting, and design\nand analyze an associated mechanism based on the solution concept of\nlexicographic maximin fairness. We present an efficient implementation of our\nmechanism, and prove that it is envy-free, non-wasteful, resource monotonic,\npopulation monotonic, and group strategyproof. We also prove that our mechanism\nguarantees each agent at least half of the utility that they can obtain by not\nsharing their resources. We complement these positive results by proving that\nno maximin fair mechanism can improve on the aforementioned factor of one-half.",
    "published_date": "2021-06-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.02688v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.02674v2",
    "title": "Differentially Empirical Risk Minimization under the Fairness Lens",
    "authors": [
      "Cuong Tran",
      "My H. Dinh",
      "Ferdinando Fioretto"
    ],
    "author_ids": [],
    "abstract": "Differential Privacy (DP) is an important privacy-enhancing technology for\nprivate machine learning systems. It allows to measure and bound the risk\nassociated with an individual participation in a computation. However, it was\nrecently observed that DP learning systems may exacerbate bias and unfairness\nfor different groups of individuals. This paper builds on these important\nobservations and sheds light on the causes of the disparate impacts arising in\nthe problem of differentially private empirical risk minimization. It focuses\non the accuracy disparity arising among groups of individuals in two\nwell-studied DP learning methods: output perturbation and differentially\nprivate stochastic gradient descent. The paper analyzes which data and model\nproperties are responsible for the disproportionate impacts, why these aspects\nare affecting different groups disproportionately and proposes guidelines to\nmitigate these effects. The proposed approach is evaluated on several datasets\nand settings.",
    "published_date": "2021-06-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.02674v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.02656v2",
    "title": "Approximating Nash Social Welfare under Binary XOS and Binary Subadditive Valuations",
    "authors": [
      "Siddharth Barman",
      "Paritosh Verma"
    ],
    "author_ids": [],
    "abstract": "We study the problem of allocating indivisible goods among agents in a fair\nand economically efficient manner. In this context, the Nash social\nwelfare-defined as the geometric mean of agents' valuations for their assigned\nbundles-stands as a fundamental measure that quantifies the extent of fairness\nof an allocation. Focusing on instances in which the agents' valuations have\nbinary marginals, we develop essentially tight results for (approximately)\nmaximizing Nash social welfare under two of the most general classes of\ncomplement-free valuations, i.e., under binary XOS and binary subadditive\nvaluations.\n  For binary XOS valuations, we develop a polynomial-time algorithm that finds\na constant-factor (specifically $288$) approximation for the optimal Nash\nsocial welfare, in the standard value-oracle model. The allocations computed by\nour algorithm also achieve constant-factor approximation for social welfare and\nthe groupwise maximin share guarantee. These results imply that-in the case of\nbinary XOS valuations-there necessarily exists an allocation that\nsimultaneously satisfies multiple (approximate) fairness and efficiency\ncriteria. We complement the algorithmic result by proving that Nash social\nwelfare maximization is APX-hard under binary XOS valuations.\n  Furthermore, this work establishes an interesting separation between the\nbinary XOS and binary subadditive settings. In particular, we prove that an\nexponential number of value queries are necessarily required to obtain even a\nsub-linear approximation for Nash social welfare under binary subadditive\nvaluations.",
    "published_date": "2021-06-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.02656v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.02604v3",
    "title": "Do Persuasive Designs Make Smartphones More Addictive? -- A Mixed-Methods Study on Chinese University Students",
    "authors": [
      "Xiaowei Chen",
      "Anders Hedman",
      "Verena Distler",
      "Vincent Koenig"
    ],
    "author_ids": [],
    "abstract": "Persuasive designs become prevalent on smartphones, and an increasing number\nof users report having problematic smartphone use behaviours. Persuasive\ndesigns in smartphones might be accountable for the development and\nreinforcement of such problematic use. This paper uses a mixed-methods approach\nto study the relationship between persuasive designs and problematic smartphone\nuse: (1) questionnaires (N=183) to investigate the proportion of participants\nhaving multiple problematic smartphone use behaviours and smartphone designs\nand applications (apps) that they perceived affecting their attitudes and\nbehaviours, and (2) interviews (N=10) to deepen our understanding of users'\nobservations and evaluations of persuasive designs. 25\\% of the participants\nself-reported having multiple problematic smartphone use behaviours, with short\nvideo, social networking, game and learning apps perceived as most attitude and\nbehaviour-affecting. Interviewees identified multiple persuasive designs in\nmost of these apps and stated that persuasive designs prolonged their screen\ntime, reinforced phone-checking habits, and caused distractions. Overall, this\nstudy provides evidence to argue that persuasive designs contribute to\nproblematic smartphone use, potentially making smartphones more addictive. We\nend our study by discussing the ethical implications of persuasive designs that\nbecame salient in our study.",
    "published_date": "2021-06-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.02604v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.02596v1",
    "title": "Understanding and Countering Stereotypes: A Computational Approach to the Stereotype Content Model",
    "authors": [
      "Kathleen C. Fraser",
      "Isar Nejadgholi",
      "Svetlana Kiritchenko"
    ],
    "author_ids": [],
    "abstract": "Stereotypical language expresses widely-held beliefs about different social\ncategories. Many stereotypes are overtly negative, while others may appear\npositive on the surface, but still lead to negative consequences. In this work,\nwe present a computational approach to interpreting stereotypes in text through\nthe Stereotype Content Model (SCM), a comprehensive causal theory from social\npsychology. The SCM proposes that stereotypes can be understood along two\nprimary dimensions: warmth and competence. We present a method for defining\nwarmth and competence axes in semantic embedding space, and show that the four\nquadrants defined by this subspace accurately represent the warmth and\ncompetence concepts, according to annotated lexicons. We then apply our\ncomputational SCM model to textual stereotype data and show that it compares\nfavourably with survey-based studies in the psychological literature.\nFurthermore, we explore various strategies to counter stereotypical beliefs\nwith anti-stereotypes. It is known that countering stereotypes with\nanti-stereotypical examples is one of the most effective ways to reduce biased\nthinking, yet the problem of generating anti-stereotypes has not been\npreviously studied. Thus, a better understanding of how to generate realistic\nand effective anti-stereotypes can contribute to addressing pressing societal\nconcerns of stereotyping, prejudice, and discrimination.",
    "published_date": "2021-06-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.02596v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.02578v1",
    "title": "Alexa, Google, Siri: What are Your Pronouns? Gender and Anthropomorphism in the Design and Perception of Conversational Assistants",
    "authors": [
      "Gavin Abercrombie",
      "Amanda Cercas Curry",
      "Mugdha Pandya",
      "Verena Rieser"
    ],
    "author_ids": [],
    "abstract": "Technology companies have produced varied responses to concerns about the\neffects of the design of their conversational AI systems. Some have claimed\nthat their voice assistants are in fact not gendered or human-like -- despite\ndesign features suggesting the contrary. We compare these claims to user\nperceptions by analysing the pronouns they use when referring to AI assistants.\nWe also examine systems' responses and the extent to which they generate output\nwhich is gendered and anthropomorphic. We find that, while some companies\nappear to be addressing the ethical concerns raised, in some cases, their\nclaims do not seem to hold true. In particular, our results show that system\noutputs are ambiguous as to the humanness of the systems, and that users tend\nto personify and gender them as a result.",
    "published_date": "2021-06-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.02578v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.02553v2",
    "title": "Fair Exploration via Axiomatic Bargaining",
    "authors": [
      "Jackie Baek",
      "Vivek F. Farias"
    ],
    "author_ids": [],
    "abstract": "Exploration is often necessary in online learning to maximize long-term\nreward, but it comes at the cost of short-term 'regret'. We study how this cost\nof exploration is shared across multiple groups. For example, in a clinical\ntrial setting, patients who are assigned a sub-optimal treatment effectively\nincur the cost of exploration. When patients are associated with natural groups\non the basis of, say, race or age, it is natural to ask whether the cost of\nexploration borne by any single group is 'fair'. So motivated, we introduce the\n'grouped' bandit model. We leverage the theory of axiomatic bargaining, and the\nNash bargaining solution in particular, to formalize what might constitute a\nfair division of the cost of exploration across groups. On the one hand, we\nshow that any regret-optimal policy strikingly results in the least fair\noutcome: such policies will perversely leverage the most 'disadvantaged' groups\nwhen they can. More constructively, we derive policies that are optimally fair\nand simultaneously enjoy a small 'price of fairness'. We illustrate the\nrelative merits of our algorithmic framework with a case study on contextual\nbandits for warfarin dosing where we are concerned with the cost of exploration\nacross multiple races and age groups.",
    "published_date": "2021-06-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.02553v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.02498v1",
    "title": "Towards Fairness Certification in Artificial Intelligence",
    "authors": [
      "Tatiana Tommasi",
      "Silvia Bucci",
      "Barbara Caputo",
      "Pietro Asinari"
    ],
    "author_ids": [],
    "abstract": "Thanks to the great progress of machine learning in the last years, several\nArtificial Intelligence (AI) techniques have been increasingly moving from the\ncontrolled research laboratory settings to our everyday life. AI is clearly\nsupportive in many decision-making scenarios, but when it comes to sensitive\nareas such as health care, hiring policies, education, banking or justice, with\nmajor impact on individuals and society, it becomes crucial to establish\nguidelines on how to design, develop, deploy and monitor this technology.\nIndeed the decision rules elaborated by machine learning models are data-driven\nand there are multiple ways in which discriminatory biases can seep into data.\nAlgorithms trained on those data incur the risk of amplifying prejudices and\nsocietal stereotypes by over associating protected attributes such as gender,\nethnicity or disabilities with the prediction task. Starting from the extensive\nexperience of the National Metrology Institute on measurement standards and\ncertification roadmaps, and of Politecnico di Torino on machine learning as\nwell as methods for domain bias evaluation and mastering, we propose a first\njoint effort to define the operational steps needed for AI fairness\ncertification. Specifically we will overview the criteria that should be met by\nan AI system before coming into official service and the conformity assessment\nprocedures useful to monitor its functioning for fair decisions.",
    "published_date": "2021-06-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.02498v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.02422v1",
    "title": "Classification of Audio Segments in Call Center Recordings using Convolutional Recurrent Neural Networks",
    "authors": [
      "Şükrü Ozan"
    ],
    "author_ids": [],
    "abstract": "Detailed statistical analysis of call center recordings is critical in the\ncustomer relationship management point of view. With the recent advances in\nartificial intelligence, many tasks regarding the calculation of call\nstatistics are now performed automatically. This work proposes a neural network\nframework where the aim is to correctly identify audio segments and classify\nthem as either customer or agent sections. Accurately identifying these\nsections gives a fair metric for evaluating agents' performances. We inherited\nthe convolutional recurrent neural network (CRNN) architecture commonly used\nfor such problems as music genre classification. We also tested the same\narchitecture's performance, where the previous class information and the gender\ninformation of speakers are also added to the training data labels. We saw that\nCRNN could generalize the training data and perform well on validation data for\nthis problem with and without the gender information. Moreover, even the\ntraining was performed using Turkish speech samples; the trained network was\nproven to achieve high accuracy for call center recordings in other languages\nlike German and English.",
    "published_date": "2021-06-04T00:00:00",
    "year": 2021,
    "categories": [
      "eess.AS",
      "cs.SD"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.02422v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.02415v1",
    "title": "What is fair? Exploring the artists' perspective on the fairness of music streaming platforms",
    "authors": [
      "Andres Ferraro",
      "Xavier Serra",
      "Christine Bauer"
    ],
    "author_ids": [],
    "abstract": "Music streaming platforms are currently among the main sources of music\nconsumption, and the embedded recommender systems significantly influence what\nthe users consume. There is an increasing interest to ensure that those\nplatforms and systems are fair. Yet, we first need to understand what fairness\nmeans in such a context. Although artists are the main content providers for\nmusic platforms, there is a research gap concerning the artists' perspective.\nTo fill this gap, we conducted interviews with music artists to understand how\nthey are affected by current platforms and what improvements they deem\nnecessary. Using a Qualitative Content Analysis, we identify the aspects that\nthe artists consider relevant for fair platforms. In this paper, we discuss the\nfollowing aspects derived from the interviews: fragmented presentation,\nreaching an audience, transparency, influencing users' listening behavior,\npopularity bias, artists' repertoire size, quotas for local music, gender\nbalance, and new music. For some topics, our findings do not indicate a clear\ndirection about the best way how music platforms should act and function; for\nother topics, though, there is a clear consensus among our interviewees: for\nthese, the artists have a clear idea of the actions that should be taken so\nthat music platforms will be fair also for the artists.",
    "published_date": "2021-06-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.02415v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.02397v6",
    "title": "On Classifying Continuous Constraint Satisfaction Problems",
    "authors": [
      "Tillmann Miltzow",
      "Reinier F. Schmiermann"
    ],
    "author_ids": [],
    "abstract": "A continuous constraint satisfaction problem (CCSP) is a constraint\nsatisfaction problem (CSP) with an interval domain $U \\subset \\mathbb{R}$. We\nengage in a systematic study to classify CCSPs that are complete of the\nExistential Theory of the Reals, i.e., ER-complete. To define this class, we\nfirst consider the problem ETR, which also stands for Existential Theory of the\nReals. In an instance of this problem we are given some sentence of the form\n$\\exists x_1, \\ldots, x_n \\in \\mathbb{R} : \\Phi(x_1, \\ldots, x_n)$, where\n$\\Phi$ is a well-formed quantifier-free formula consisting of the symbols $\\{0,\n1, +, \\cdot, \\geq, >, \\wedge, \\vee, \\neg\\}$, the goal is to check whether this\nsentence is true. Now the class ER is the family of all problems that admit a\npolynomial-time many-one reduction to ETR. It is known that NP $\\subseteq$ ER\n$\\subseteq$ PSPACE.\n  We restrict our attention on CCSPs with addition constraints ($x + y = z$)\nand some other mild technical conditions. Previously, it was shown that\nmultiplication constraints ($x \\cdot y = z$), squaring constraints ($x^2 = y$),\nor inversion constraints ($x\\cdot y = 1$) are sufficient to establish\nER-completeness. We extend this in the strongest possible sense for equality\nconstraints as follows. We show that CCSPs (with addition constraints and some\nother mild technical conditions) that have any one well-behaved curved equality\nconstraint ($f(x,y) = 0$) are ER-complete. We further extend our results to\ninequality constraints. We show that any well-behaved convexly curved and any\nwell-behaved concavely curved inequality constraint ($f(x,y) \\geq 0$ and\n$g(x,y) \\geq 0$) imply ER-completeness on the class of such CCSPs.",
    "published_date": "2021-06-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CC",
      "cs.CG",
      "cs.CL",
      "cs.DM",
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.02397v6",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.02320v4",
    "title": "Few-Shot Segmentation via Cycle-Consistent Transformer",
    "authors": [
      "Gengwei Zhang",
      "Guoliang Kang",
      "Yi Yang",
      "Yunchao Wei"
    ],
    "author_ids": [],
    "abstract": "Few-shot segmentation aims to train a segmentation model that can fast adapt\nto novel classes with few exemplars. The conventional training paradigm is to\nlearn to make predictions on query images conditioned on the features from\nsupport images. Previous methods only utilized the semantic-level prototypes of\nsupport images as conditional information. These methods cannot utilize all\npixel-wise support information for the query predictions, which is however\ncritical for the segmentation task. In this paper, we focus on utilizing\npixel-wise relationships between support and query images to facilitate the\nfew-shot segmentation task. We design a novel Cycle-Consistent TRansformer\n(CyCTR) module to aggregate pixel-wise support features into query ones. CyCTR\nperforms cross-attention between features from different images, i.e. support\nand query images. We observe that there may exist unexpected irrelevant\npixel-level support features. Directly performing cross-attention may aggregate\nthese features from support to query and bias the query features. Thus, we\npropose using a novel cycle-consistent attention mechanism to filter out\npossible harmful support features and encourage query features to attend to the\nmost informative pixels from support images. Experiments on all few-shot\nsegmentation benchmarks demonstrate that our proposed CyCTR leads to remarkable\nimprovement compared to previous state-of-the-art methods. Specifically, on\nPascal-$5^i$ and COCO-$20^i$ datasets, we achieve 67.5% and 45.6% mIoU for\n5-shot segmentation, outperforming previous state-of-the-art methods by 5.6%\nand 7.1% respectively.",
    "published_date": "2021-06-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.02320v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.02288v2",
    "title": "Tackling the Background Bias in Sparse Object Detection via Cropped Windows",
    "authors": [
      "Leon Amadeus Varga",
      "Andreas Zell"
    ],
    "author_ids": [],
    "abstract": "Object detection on Unmanned Aerial Vehicles (UAVs) is still a challenging\ntask. The recordings are mostly sparse and contain only small objects. In this\nwork, we propose a simple tiling method that improves the detection capability\nin the remote sensing case without modifying the model itself. By reducing the\nbackground bias and enabling the usage of higher image resolutions during\ntraining, our method can improve the performance of models substantially. The\nprocedure was validated on three different data sets and outperformed similar\napproaches in performance and speed.",
    "published_date": "2021-06-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.02288v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.02216v1",
    "title": "Fairness-Aware Unsupervised Feature Selection",
    "authors": [
      "Xiaoying Xing",
      "Hongfu Liu",
      "Chen Chen",
      "Jundong Li"
    ],
    "author_ids": [],
    "abstract": "Feature selection is a prevalent data preprocessing paradigm for various\nlearning tasks. Due to the expensive cost of acquiring supervision information,\nunsupervised feature selection sparks great interests recently. However,\nexisting unsupervised feature selection algorithms do not have fairness\nconsiderations and suffer from a high risk of amplifying discrimination by\nselecting features that are over associated with protected attributes such as\ngender, race, and ethnicity. In this paper, we make an initial investigation of\nthe fairness-aware unsupervised feature selection problem and develop a\nprincipled framework, which leverages kernel alignment to find a subset of\nhigh-quality features that can best preserve the information in the original\nfeature space while being minimally correlated with protected attributes.\nSpecifically, different from the mainstream in-processing debiasing methods,\nour proposed framework can be regarded as a model-agnostic debiasing strategy\nthat eliminates biases and discrimination before downstream learning algorithms\nare involved. Experimental results on multiple real-world datasets demonstrate\nthat our framework achieves a good trade-off between utility maximization and\nfairness promotion.",
    "published_date": "2021-06-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.02216v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.02183v1",
    "title": "Towards Equal Gender Representation in the Annotations of Toxic Language Detection",
    "authors": [
      "Elizabeth Excell",
      "Noura Al Moubayed"
    ],
    "author_ids": [],
    "abstract": "Classifiers tend to propagate biases present in the data on which they are\ntrained. Hence, it is important to understand how the demographic identities of\nthe annotators of comments affect the fairness of the resulting model. In this\npaper, we focus on the differences in the ways men and women annotate comments\nfor toxicity, investigating how these differences result in models that amplify\nthe opinions of male annotators. We find that the BERT model as-sociates toxic\ncomments containing offensive words with male annotators, causing the model to\npredict 67.7% of toxic comments as having been annotated by men. We show that\nthis disparity between gender predictions can be mitigated by removing\noffensive words and highly toxic comments from the training data. We then apply\nthe learned associations between gender and language to toxic language\nclassifiers, finding that models trained exclusively on female-annotated data\nperform 1.8% better than those trained solely on male-annotated data and that\ntraining models on data after removing all offensive words reduces bias in the\nmodel by 55.5% while increasing the sensitivity by 0.4%.",
    "published_date": "2021-06-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.02183v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.04757v2",
    "title": "Fair Machine Learning under Limited Demographically Labeled Data",
    "authors": [
      "Mustafa Safa Ozdayi",
      "Murat Kantarcioglu",
      "Rishabh Iyer"
    ],
    "author_ids": [],
    "abstract": "Research has shown that, machine learning models might inherit and propagate\nundesired social biases encoded in the data. To address this problem, fair\ntraining algorithms are developed. However, most algorithms assume we know\ndemographic/sensitive data features such as gender and race. This assumption\nfalls short in scenarios where collecting demographic information is not\nfeasible due to privacy concerns, and data protection policies. A recent line\nof work develops fair training methods that can function without any\ndemographic feature on the data, that are collectively referred as Rawlsian\nmethods. Yet, we show in experiments that, Rawlsian methods tend to exhibit\nrelatively high bias. Given this, we look at the middle ground between the\nprevious approaches, and consider a setting where we know the demographic\nattributes for only a small subset of our data. In such a setting, we design\nfair training algorithms which exhibit both good utility, and low bias. In\nparticular, we show that our techniques can train models to significantly\noutperform Rawlsian approaches even when 0.1% of demographic attributes are\navailable in the training data. Furthermore, our main algorithm can accommodate\nmultiple training objectives easily. We expand our main algorithm to achieve\nrobustness to label noise in addition to fairness in the limited demographics\nsetting to highlight that property as well.",
    "published_date": "2021-06-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.04757v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.02024v6",
    "title": "Time-Efficient Algorithms for Nash-Bargaining-Based Matching Market Models",
    "authors": [
      "Ioannis Panageas",
      "Thorben Tröbst",
      "Vijay V. Vazirani"
    ],
    "author_ids": [],
    "abstract": "In the area of matching-based market design, existing models using cardinal\nutilities suffer from two deficiencies: First, the Hylland-Zeckhauser (HZ)\nmechanism, which has remained a classic in economics for one-sided matching\nmarkets, is intractable; computation of even an approximate equilibrium is\nPPAD-complete. Second, there is an extreme paucity of such models. This led\nHosseini and Vazirani (2022) to define a rich collection of\nNash-bargaining-based models for one-sided and two-sided matching markets, in\nboth Fisher and Arrow-Debreu settings, together with very fast implementations\nusing available solvers and very encouraging experimental results.\n  In this paper, we give fast algorithms with proven running times for the\nmodels introduced by Hosseini and Vazirani, using the techniques of\nmultiplicative weights update (MWU) and conditional gradient descent (CGD).\nAdditionally, we make the following contributions:\n  (1) By Tr\\\"obst and Vazirani (2024), a linear one-sided Nash-bargaining-based\nmatching market satisfies envy-freeness within factor two. We show that the\nother models satisfy approximate equal-share fairness, where the exact factor\ndepends on the utility function being used in the particular model.\n  (2) We define a Nash-bargaining-based model for non-bipartite matching\nmarkets and give fast algorithms for it using conditional gradient descent.",
    "published_date": "2021-06-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT",
      "econ.TH",
      "F.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.02024v6",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.01941v1",
    "title": "Optimizing Rankings for Recommendation in Matching Markets",
    "authors": [
      "Yi Su",
      "Magd Bayoumi",
      "Thorsten Joachims"
    ],
    "author_ids": [],
    "abstract": "Based on the success of recommender systems in e-commerce, there is growing\ninterest in their use in matching markets (e.g., labor). While this holds\npotential for improving market fluidity and fairness, we show in this paper\nthat naively applying existing recommender systems to matching markets is\nsub-optimal. Considering the standard process where candidates apply and then\nget evaluated by employers, we present a new recommendation framework to model\nthis interaction mechanism and propose efficient algorithms for computing\npersonalized rankings in this setting. We show that the optimal rankings need\nto not only account for the potentially divergent preferences of candidates and\nemployers, but they also need to account for capacity constraints. This makes\nconventional ranking systems that merely rank by some local score (e.g.,\none-sided or reciprocal relevance) highly sub-optimal -- not only for an\nindividual user, but also for societal goals (e.g., low unemployment). To\naddress this shortcoming, we propose the first method for jointly optimizing\nthe rankings for all candidates in the market to explicitly maximize social\nwelfare. In addition to the theoretical derivation, we evaluate the method both\non simulated environments and on data from a real-world\nnetworking-recommendation system that we built and fielded at a large computer\nscience conference.",
    "published_date": "2021-06-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.01941v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.01921v2",
    "title": "Sample Selection Bias in Evaluation of Prediction Performance of Causal Models",
    "authors": [
      "James P. Long",
      "Min Jin Ha"
    ],
    "author_ids": [],
    "abstract": "Causal models are notoriously difficult to validate because they make\nuntestable assumptions regarding confounding. New scientific experiments offer\nthe possibility of evaluating causal models using prediction performance.\nPrediction performance measures are typically robust to violations in causal\nassumptions. However, prediction performance does depend on the selection of\ntraining and test sets. Biased training sets can lead to optimistic assessments\nof model performance. In this work, we revisit the prediction performance of\nseveral recently proposed causal models tested on a genetic perturbation data\nset of Kemmeren. We find that sample selection bias is likely a key driver of\nmodel performance. We propose using a less-biased evaluation set for assessing\nprediction performance and compare models on this new set. In this setting, the\ncausal models have similar or worse performance compared to standard\nassociation-based estimators such as Lasso. Finally, we compare the performance\nof causal estimators in simulation studies that reproduce the Kemmeren\nstructure of genetic knockout experiments but without any sample selection\nbias. These results provide an improved understanding of the performance of\nseveral causal models and offer guidance on how future studies should use\nKemmeren.",
    "published_date": "2021-06-03T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.01921v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.01784v2",
    "title": "The Contestation of Tech Ethics: A Sociotechnical Approach to Technology Ethics in Practice",
    "authors": [
      "Ben Green"
    ],
    "author_ids": [],
    "abstract": "This article introduces the special issue \"Technology Ethics in Action:\nCritical and Interdisciplinary Perspectives\". In response to recent\ncontroversies about the harms of digital technology, discourses and practices\nof \"tech ethics\" have proliferated across the tech industry, academia, civil\nsociety, and government. Yet despite the seeming promise of ethics, tech ethics\nin practice suffers from several significant limitations: tech ethics is vague\nand toothless, has a myopic focus on individual engineers and technology\ndesign, and is subsumed into corporate logics and incentives. These limitations\nsuggest that tech ethics enables corporate \"ethics-washing\": embracing the\nlanguage of ethics to defuse criticism and resist government regulation,\nwithout committing to ethical behavior. Given these dynamics, I describe tech\nethics as a terrain of contestation where the central debate is not whether\nethics is desirable, but what \"ethics\" entails and who gets to define it.\nCurrent approaches to tech ethics are poised to enable technologists and\ntechnology companies to label themselves as \"ethical\" without substantively\naltering their practices. Thus, those striving for structural improvements in\ndigital technologies must be mindful of the gap between ethics as a mode of\nnormative inquiry and ethics as a practical endeavor. In order to better\nevaluate the opportunities and limits of tech ethics, I propose a\nsociotechnical approach that analyzes tech ethics in light of who defines it\nand what impacts it generates in practice.",
    "published_date": "2021-06-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.01784v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.01723v1",
    "title": "Risk Minimization from Adaptively Collected Data: Guarantees for Supervised and Policy Learning",
    "authors": [
      "Aurélien Bibaut",
      "Antoine Chambaz",
      "Maria Dimakopoulou",
      "Nathan Kallus",
      "Mark van der Laan"
    ],
    "author_ids": [],
    "abstract": "Empirical risk minimization (ERM) is the workhorse of machine learning,\nwhether for classification and regression or for off-policy policy learning,\nbut its model-agnostic guarantees can fail when we use adaptively collected\ndata, such as the result of running a contextual bandit algorithm. We study a\ngeneric importance sampling weighted ERM algorithm for using adaptively\ncollected data to minimize the average of a loss function over a hypothesis\nclass and provide first-of-their-kind generalization guarantees and fast\nconvergence rates. Our results are based on a new maximal inequality that\ncarefully leverages the importance sampling structure to obtain rates with the\nright dependence on the exploration rate in the data. For regression, we\nprovide fast rates that leverage the strong convexity of squared-error loss.\nFor policy learning, we provide rate-optimal regret guarantees that close an\nopen gap in the existing literature whenever exploration decays to zero, as is\nthe case for bandit-collected data. An empirical investigation validates our\ntheory.",
    "published_date": "2021-06-03T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.ST",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.01723v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.01613v3",
    "title": "NODE-GAM: Neural Generalized Additive Model for Interpretable Deep Learning",
    "authors": [
      "Chun-Hao Chang",
      "Rich Caruana",
      "Anna Goldenberg"
    ],
    "author_ids": [],
    "abstract": "Deployment of machine learning models in real high-risk settings (e.g.\nhealthcare) often depends not only on the model's accuracy but also on its\nfairness, robustness, and interpretability. Generalized Additive Models (GAMs)\nare a class of interpretable models with a long history of use in these\nhigh-risk domains, but they lack desirable features of deep learning such as\ndifferentiability and scalability. In this work, we propose a neural GAM\n(NODE-GAM) and neural GA$^2$M (NODE-GA$^2$M) that scale well and perform better\nthan other GAMs on large datasets, while remaining interpretable compared to\nother ensemble and deep learning models. We demonstrate that our models find\ninteresting patterns in the data. Lastly, we show that we improve model\naccuracy via self-supervised pre-training, an improvement that is not possible\nfor non-differentiable GAMs.",
    "published_date": "2021-06-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.01613v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.01601v1",
    "title": "Men Are Elected, Women Are Married: Events Gender Bias on Wikipedia",
    "authors": [
      "Jiao Sun",
      "Nanyun Peng"
    ],
    "author_ids": [],
    "abstract": "Human activities can be seen as sequences of events, which are crucial to\nunderstanding societies. Disproportional event distribution for different\ndemographic groups can manifest and amplify social stereotypes, and potentially\njeopardize the ability of members in some groups to pursue certain goals. In\nthis paper, we present the first event-centric study of gender biases in a\nWikipedia corpus. To facilitate the study, we curate a corpus of career and\npersonal life descriptions with demographic information consisting of 7,854\nfragments from 10,412 celebrities. Then we detect events with a\nstate-of-the-art event detection model, calibrate the results using\nstrategically generated templates, and extract events that have asymmetric\nassociations with genders. Our study discovers that the Wikipedia pages tend to\nintermingle personal life events with professional events for females but not\nfor males, which calls for the awareness of the Wikipedia community to\nformalize guidelines and train the editors to mind the implicit biases that\ncontributors carry. Our work also lays the foundation for future works on\nquantifying and discovering event biases at the corpus level.",
    "published_date": "2021-06-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.01601v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.01467v1",
    "title": "Domain Adaptation for Facial Expression Classifier via Domain Discrimination and Gradient Reversal",
    "authors": [
      "Kamil Akhmetov"
    ],
    "author_ids": [],
    "abstract": "Bringing empathy to a computerized system could significantly improve the\nquality of human-computer communications, as soon as machines would be able to\nunderstand customer intentions and better serve their needs. According to\ndifferent studies (Literature Review), visual information is one of the most\nimportant channels of human interaction and contains significant behavioral\nsignals, that may be captured from facial expressions. Therefore, it is\nconsistent and natural that the research in the field of Facial Expression\nRecognition (FER) has acquired increased interest over the past decade due to\nhaving diverse application area including health-care, sociology, psychology,\ndriver-safety, virtual reality, cognitive sciences, security, entertainment,\nmarketing, etc. We propose a new architecture for the task of FER and examine\nthe impact of domain discrimination loss regularization on the learning\nprocess. With regard to observations, including both classical training\nconditions and unsupervised domain adaptation scenarios, important aspects of\nthe considered domain adaptation approach integration are traced. The results\nmay serve as a foundation for further research in the field.",
    "published_date": "2021-06-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.01467v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.01465v1",
    "title": "Ethical-Advice Taker: Do Language Models Understand Natural Language Interventions?",
    "authors": [
      "Jieyu Zhao",
      "Daniel Khashabi",
      "Tushar Khot",
      "Ashish Sabharwal",
      "Kai-Wei Chang"
    ],
    "author_ids": [],
    "abstract": "Is it possible to use natural language to intervene in a model's behavior and\nalter its prediction in a desired way? We investigate the effectiveness of\nnatural language interventions for reading-comprehension systems, studying this\nin the context of social stereotypes. Specifically, we propose a new language\nunderstanding task, Linguistic Ethical Interventions (LEI), where the goal is\nto amend a question-answering (QA) model's unethical behavior by communicating\ncontext-specific principles of ethics and equity to it. To this end, we build\nupon recent methods for quantifying a system's social stereotypes, augmenting\nthem with different kinds of ethical interventions and the desired model\nbehavior under such interventions. Our zero-shot evaluation finds that even\ntoday's powerful neural language models are extremely poor ethical-advice\ntakers, that is, they respond surprisingly little to ethical interventions even\nthough these interventions are stated as simple sentences. Few-shot learning\nimproves model behavior but remains far from the desired outcome, especially\nwhen evaluated for various types of generalization. Our new task thus poses a\nnovel language understanding challenge for the community.",
    "published_date": "2021-06-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.01465v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.06054v5",
    "title": "Fair Preprocessing: Towards Understanding Compositional Fairness of Data Transformers in Machine Learning Pipeline",
    "authors": [
      "Sumon Biswas",
      "Hridesh Rajan"
    ],
    "author_ids": [],
    "abstract": "In recent years, many incidents have been reported where machine learning\nmodels exhibited discrimination among people based on race, sex, age, etc.\nResearch has been conducted to measure and mitigate unfairness in machine\nlearning models. For a machine learning task, it is a common practice to build\na pipeline that includes an ordered set of data preprocessing stages followed\nby a classifier. However, most of the research on fairness has considered a\nsingle classifier based prediction task. What are the fairness impacts of the\npreprocessing stages in machine learning pipeline? Furthermore, studies showed\nthat often the root cause of unfairness is ingrained in the data itself, rather\nthan the model. But no research has been conducted to measure the unfairness\ncaused by a specific transformation made in the data preprocessing stage. In\nthis paper, we introduced the causal method of fairness to reason about the\nfairness impact of data preprocessing stages in ML pipeline. We leveraged\nexisting metrics to define the fairness measures of the stages. Then we\nconducted a detailed fairness evaluation of the preprocessing stages in 37\npipelines collected from three different sources. Our results show that certain\ndata transformers are causing the model to exhibit unfairness. We identified a\nnumber of fairness patterns in several categories of data transformers.\nFinally, we showed how the local fairness of a preprocessing stage composes in\nthe global fairness of the pipeline. We used the fairness composition to choose\nappropriate downstream transformer that mitigates unfairness in the machine\nlearning pipeline.",
    "published_date": "2021-06-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "D.2.0; I.2.5"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.06054v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.01410v2",
    "title": "Uncertainty Quantification 360: A Holistic Toolkit for Quantifying and Communicating the Uncertainty of AI",
    "authors": [
      "Soumya Ghosh",
      "Q. Vera Liao",
      "Karthikeyan Natesan Ramamurthy",
      "Jiri Navratil",
      "Prasanna Sattigeri",
      "Kush R. Varshney",
      "Yunfeng Zhang"
    ],
    "author_ids": [],
    "abstract": "In this paper, we describe an open source Python toolkit named Uncertainty\nQuantification 360 (UQ360) for the uncertainty quantification of AI models. The\ngoal of this toolkit is twofold: first, to provide a broad range of\ncapabilities to streamline as well as foster the common practices of\nquantifying, evaluating, improving, and communicating uncertainty in the AI\napplication development lifecycle; second, to encourage further exploration of\nUQ's connections to other pillars of trustworthy AI such as fairness and\ntransparency through the dissemination of latest research and education\nmaterials. Beyond the Python package (\\url{https://github.com/IBM/UQ360}), we\nhave developed an interactive experience (\\url{http://uq360.mybluemix.net}) and\nguidance materials as educational tools to aid researchers and developers in\nproducing and communicating high-quality uncertainties in an effective manner.",
    "published_date": "2021-06-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.01410v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.01105v1",
    "title": "Use of Formal Ethical Reviews in NLP Literature: Historical Trends and Current Practices",
    "authors": [
      "Sebastin Santy",
      "Anku Rani",
      "Monojit Choudhury"
    ],
    "author_ids": [],
    "abstract": "Ethical aspects of research in language technologies have received much\nattention recently. It is a standard practice to get a study involving human\nsubjects reviewed and approved by a professional ethics committee/board of the\ninstitution. How commonly do we see mention of ethical approvals in NLP\nresearch? What types of research or aspects of studies are usually subject to\nsuch reviews? With the rising concerns and discourse around the ethics of NLP,\ndo we also observe a rise in formal ethical reviews of NLP studies? And, if so,\nwould this imply that there is a heightened awareness of ethical issues that\nwas previously lacking? We aim to address these questions by conducting a\ndetailed quantitative and qualitative analysis of the ACL Anthology, as well as\ncomparing the trends in our field to those of other related disciplines, such\nas cognitive science, machine learning, data mining, and systems.",
    "published_date": "2021-06-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.01105v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.01101v2",
    "title": "Learning a Single Neuron with Bias Using Gradient Descent",
    "authors": [
      "Gal Vardi",
      "Gilad Yehudai",
      "Ohad Shamir"
    ],
    "author_ids": [],
    "abstract": "We theoretically study the fundamental problem of learning a single neuron\nwith a bias term ($\\mathbf{x} \\mapsto \\sigma(<\\mathbf{w},\\mathbf{x}> + b)$) in\nthe realizable setting with the ReLU activation, using gradient descent.\nPerhaps surprisingly, we show that this is a significantly different and more\nchallenging problem than the bias-less case (which was the focus of previous\nworks on single neurons), both in terms of the optimization geometry as well as\nthe ability of gradient methods to succeed in some scenarios. We provide a\ndetailed study of this problem, characterizing the critical points of the\nobjective, demonstrating failure cases, and providing positive convergence\nguarantees under different sets of assumptions. To prove our results, we\ndevelop some tools which may be of independent interest, and improve previous\nresults on learning single neurons.",
    "published_date": "2021-06-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.NE",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.01101v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.01070v1",
    "title": "Testing Group Fairness via Optimal Transport Projections",
    "authors": [
      "Nian Si",
      "Karthyek Murthy",
      "Jose Blanchet",
      "Viet Anh Nguyen"
    ],
    "author_ids": [],
    "abstract": "We present a statistical testing framework to detect if a given machine\nlearning classifier fails to satisfy a wide range of group fairness notions.\nThe proposed test is a flexible, interpretable, and statistically rigorous tool\nfor auditing whether exhibited biases are intrinsic to the algorithm or due to\nthe randomness in the data. The statistical challenges, which may arise from\nmultiple impact criteria that define group fairness and which are discontinuous\non model parameters, are conveniently tackled by projecting the empirical\nmeasure onto the set of group-fair probability models using optimal transport.\nThis statistic is efficiently computed using linear programming and its\nasymptotic distribution is explicitly obtained. The proposed framework can also\nbe used to test for testing composite fairness hypotheses and fairness with\nmultiple sensitive attributes. The optimal transport testing formulation\nimproves interpretability by characterizing the minimal covariate perturbations\nthat eliminate the bias observed in the audit.",
    "published_date": "2021-06-02T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.CY",
      "cs.LG",
      "math.ST",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.01070v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.01060v1",
    "title": "John praised Mary because he? Implicit Causality Bias and Its Interaction with Explicit Cues in LMs",
    "authors": [
      "Yova Kementchedjhieva",
      "Mark Anderson",
      "Anders Søgaard"
    ],
    "author_ids": [],
    "abstract": "Some interpersonal verbs can implicitly attribute causality to either their\nsubject or their object and are therefore said to carry an implicit causality\n(IC) bias. Through this bias, causal links can be inferred from a narrative,\naiding language comprehension. We investigate whether pre-trained language\nmodels (PLMs) encode IC bias and use it at inference time. We find that to be\nthe case, albeit to different degrees, for three distinct PLM architectures.\nHowever, causes do not always need to be implicit -- when a cause is explicitly\nstated in a subordinate clause, an incongruent IC bias associated with the verb\nin the main clause leads to a delay in human processing. We hypothesize that\nthe temporary challenge humans face in integrating the two contradicting\nsignals, one from the lexical semantics of the verb, one from the\nsentence-level semantics, would be reflected in higher error rates for models\non tasks dependent on causal links. The results of our study lend support to\nthis hypothesis, suggesting that PLMs tend to prioritize lexical patterns over\nhigher-order signals.",
    "published_date": "2021-06-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.01060v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.01044v1",
    "title": "Examining the Inductive Bias of Neural Language Models with Artificial Languages",
    "authors": [
      "Jennifer C. White",
      "Ryan Cotterell"
    ],
    "author_ids": [],
    "abstract": "Since language models are used to model a wide variety of languages, it is\nnatural to ask whether the neural architectures used for the task have\ninductive biases towards modeling particular types of languages. Investigation\nof these biases has proved complicated due to the many variables that appear in\nthe experimental setup. Languages vary in many typological dimensions, and it\nis difficult to single out one or two to investigate without the others acting\nas confounders. We propose a novel method for investigating the inductive\nbiases of language models using artificial languages. These languages are\nconstructed to allow us to create parallel corpora across languages that differ\nonly in the typological feature being investigated, such as word order. We then\nuse them to train and test language models. This constitutes a fully controlled\ncausal framework, and demonstrates how grammar engineering can serve as a\nuseful tool for analyzing neural models. Using this method, we find that\ncommonly used neural architectures exhibit different inductive biases: LSTMs\ndisplay little preference with respect to word ordering, while transformers\ndisplay a clear preference for some orderings over others. Further, we find\nthat neither the inductive bias of the LSTM nor that of the transformer appears\nto reflect any tendencies that we see in attested natural languages.",
    "published_date": "2021-06-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.01044v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.00841v1",
    "title": "Two Birds With One Stone: Fairness and Welfare via Transfers",
    "authors": [
      "Vishnu V. Narayan",
      "Mashbat Suzuki",
      "Adrian Vetta"
    ],
    "author_ids": [],
    "abstract": "We study the question of dividing a collection of indivisible goods amongst a\nset of agents. The main objective of research in the area is to achieve one of\ntwo goals: fairness or efficiency. On the fairness side, envy-freeness is the\ncentral fairness criterion in economics, but envy-free allocations typically do\nnot exist when the goods are indivisible. A recent line of research shows that\nenvy-freeness can be achieved if a small quantity of a homogeneous divisible\ngood (money) is introduced into the system, or equivalently, if transfer\npayments are allowed between the agents. A natural question to explore, then,\nis whether transfer payments can be used to provide high welfare in addition to\nenvy-freeness, and if so, how much money is needed to be transferred.\n  We show that for general monotone valuations, there always exists an\nallocation with transfers that is envy-free and whose Nash social welfare (NSW)\nis at least an $e^{-1/e}$-fraction of the optimal Nash social welfare.\nAdditionally, when the agents have additive valuations, an envy-free allocation\nwith negligible transfers and whose NSW is within a constant factor of optimal\ncan be found in polynomial time. Consequently, we demonstrate that the\nseemingly incompatible objectives of fairness and high welfare can be achieved\nsimultaneously via transfer payments, even for general valuations, when the\nwelfare objective is NSW. On the other hand, we show that a similar result is\nimpossible for utilitarian social welfare: any envy-freeable allocation that\nachieves a constant fraction of the optimal welfare requires non-negligible\ntransfers. To complement this result we present algorithms that compute an\nenvy-free allocation with a given target welfare and with bounded transfers.",
    "published_date": "2021-06-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.00841v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.00810v1",
    "title": "Some Ethical Issues in the Review Process of Machine Learning Conferences",
    "authors": [
      "Alessio Russo"
    ],
    "author_ids": [],
    "abstract": "Recent successes in the Machine Learning community have led to a steep\nincrease in the number of papers submitted to conferences. This increase made\nmore prominent some of the issues that affect the current review process used\nby these conferences. The review process has several issues that may undermine\nthe nature of scientific research, which is of being fully objective,\napolitical, unbiased and free of misconduct (such as plagiarism, cheating,\nimproper influence, and other improprieties). In this work, we study the\nproblem of reviewers' recruitment, infringements of the double-blind process,\nfraudulent behaviors, biases in numerical ratings, and the appendix phenomenon\n(i.e., the fact that it is becoming more common to publish results in the\nappendix section of a paper). For each of these problems, we provide a short\ndescription and possible solutions. The goal of this work is to raise awareness\nin the Machine Learning community regarding these issues.",
    "published_date": "2021-06-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.00810v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.00772v2",
    "title": "Information Theoretic Measures for Fairness-aware Feature Selection",
    "authors": [
      "Sajad Khodadadian",
      "Mohamed Nafea",
      "AmirEmad Ghassami",
      "Negar Kiyavash"
    ],
    "author_ids": [],
    "abstract": "Machine learning algorithms are increasingly used for consequential decision\nmaking regarding individuals based on their relevant features. Features that\nare relevant for accurate decisions may however lead to either explicit or\nimplicit forms of discrimination against unprivileged groups, such as those of\ncertain race or gender. This happens due to existing biases in the training\ndata, which are often replicated or even exacerbated by the learning algorithm.\nIdentifying and measuring these biases at the data level is a challenging\nproblem due to the interdependence among the features, and the decision\noutcome. In this work, we develop a framework for fairness-aware feature\nselection which takes into account the correlation among the features and the\ndecision outcome, and is based on information theoretic measures for the\naccuracy and discriminatory impacts of features. In particular, we first\npropose information theoretic measures which quantify the impact of different\nsubsets of features on the accuracy and discrimination of the decision\noutcomes. We then deduce the marginal impact of each feature using Shapley\nvalue function; a solution concept in cooperative game theory used to estimate\nmarginal contributions of players in a coalitional game. Finally, we design a\nfairness utility score for each feature (for feature selection) which\nquantifies how this feature influences accurate as well as nondiscriminatory\ndecisions. Our framework depends on the joint statistics of the data rather\nthan a particular classifier design. We examine our proposed framework on real\nand synthetic data to evaluate its performance.",
    "published_date": "2021-06-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.00772v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.00720v3",
    "title": "Fair-Net: A Network Architecture For Reducing Performance Disparity Between Identifiable Sub-Populations",
    "authors": [
      "Arghya Datta",
      "S. Joshua Swamidass"
    ],
    "author_ids": [],
    "abstract": "In real world datasets, particular groups are under-represented, much rarer\nthan others, and machine learning classifiers will often preform worse on\nunder-represented populations. This problem is aggravated across many domains\nwhere datasets are class imbalanced, with a minority class far rarer than the\nmajority class. Naive approaches to handle under-representation and class\nimbalance include training sub-population specific classifiers that handle\nclass imbalance or training a global classifier that overlooks sub-population\ndisparities and aims to achieve high overall accuracy by handling class\nimbalance. In this study, we find that these approaches are vulnerable in class\nimbalanced datasets with minority sub-populations. We introduced Fair-Net, a\nbranched multitask neural network architecture that improves both\nclassification accuracy and probability calibration across identifiable\nsub-populations in class imbalanced datasets. Fair-Nets is a straightforward\nextension to the output layer and error function of a network, so can be\nincorporated in far more complex architectures. Empirical studies with three\nreal world benchmark datasets demonstrate that Fair-Net improves classification\nand calibration performance, substantially reducing performance disparity\nbetween gender and racial sub-populations.",
    "published_date": "2021-06-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.00720v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.00604v1",
    "title": "Optimal Stopping with Behaviorally Biased Agents: The Role of Loss Aversion and Changing Reference Points",
    "authors": [
      "Jon Kleinberg",
      "Robert Kleinberg",
      "Sigal Oren"
    ],
    "author_ids": [],
    "abstract": "People are often reluctant to sell a house, or shares of stock, below the\nprice at which they originally bought it. While this is generally not\nconsistent with rational utility maximization, it does reflect two strong\nempirical regularities that are central to the behavioral science of human\ndecision-making: a tendency to evaluate outcomes relative to a reference point\ndetermined by context (in this case the original purchase price), and the\nphenomenon of loss aversion in which people are particularly prone to avoid\noutcomes below the reference point. Here we explore the implications of\nreference points and loss aversion in optimal stopping problems, where people\nevaluate a sequence of options in one pass, either accepting the option and\nstopping the search or giving up on the option forever. The best option seen so\nfar sets a reference point that shifts as the search progresses, and a biased\ndecision-maker's utility incurs an additional penalty when they accept a later\noption that is below this reference point.\n  We formulate and study a behaviorally well-motivated version of the optimal\nstopping problem that incorporates these notions of reference dependence and\nloss aversion. We obtain tight bounds on the performance of a biased agent in\nthis model relative to the best option obtainable in retrospect (a type of\nprophet inequality for biased agents), as well as tight bounds on the ratio\nbetween the performance of a biased agent and the performance of a rational\none. We further establish basic monotonicity results, and show an exponential\ngap between the performance of a biased agent in a stopping problem with\nrespect to a worst-case versus a random order. As part of this, we establish\nfundamental differences between optimal stopping problems for rational versus\nbiased agents, and these differences inform our analysis.",
    "published_date": "2021-06-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT",
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.00604v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.00600v2",
    "title": "Fair Clustering Using Antidote Data",
    "authors": [
      "Anshuman Chhabra",
      "Adish Singla",
      "Prasant Mohapatra"
    ],
    "author_ids": [],
    "abstract": "Clustering algorithms are widely utilized for many modern data science\napplications. This motivates the need to make outputs of clustering algorithms\nfair. Traditionally, new fair algorithmic variants to clustering algorithms are\ndeveloped for specific notions of fairness. However, depending on the\napplication context, different definitions of fairness might need to be\nemployed. As a result, new algorithms and analysis need to be proposed for each\ncombination of clustering algorithm and fairness definition. Additionally, each\nnew algorithm would need to be reimplemented for deployment in a real-world\nsystem. Hence, we propose an alternate approach to group-level fairness in\ncenter-based clustering inspired by research on data poisoning attacks. We seek\nto augment the original dataset with a small number of data points, called\nantidote data. When clustering is undertaken on this new dataset, the output is\nfair, for the chosen clustering algorithm and fairness definition. We formulate\nthis as a general bi-level optimization problem which can accommodate any\ncenter-based clustering algorithms and fairness notions. We then categorize\napproaches for solving this bi-level optimization for two different problem\nsettings. Extensive experiments on different clustering algorithms and fairness\nnotions show that our algorithms can achieve desired levels of fairness on many\nreal-world datasets with a very small percentage of antidote data added. We\nalso find that our algorithms achieve lower fairness costs and competitive\nclustering performance compared to other state-of-the-art fair clustering\nalgorithms.",
    "published_date": "2021-06-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.00600v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.00467v4",
    "title": "A Clarification of the Nuances in the Fairness Metrics Landscape",
    "authors": [
      "Alessandro Castelnovo",
      "Riccardo Crupi",
      "Greta Greco",
      "Daniele Regoli",
      "Ilaria Giuseppina Penco",
      "Andrea Claudio Cosentini"
    ],
    "author_ids": [],
    "abstract": "In recent years, the problem of addressing fairness in Machine Learning (ML)\nand automatic decision-making has attracted a lot of attention in the\nscientific communities dealing with Artificial Intelligence. A plethora of\ndifferent definitions of fairness in ML have been proposed, that consider\ndifferent notions of what is a \"fair decision\" in situations impacting\nindividuals in the population. The precise differences, implications and\n\"orthogonality\" between these notions have not yet been fully analyzed in the\nliterature. In this work, we try to make some order out of this zoo of\ndefinitions.",
    "published_date": "2021-06-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.00467v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.06049v1",
    "title": "FiSH: Fair Spatial Hotspots",
    "authors": [
      "Deepak P",
      "Sowmya S Sundaram"
    ],
    "author_ids": [],
    "abstract": "Pervasiveness of tracking devices and enhanced availability of spatially\nlocated data has deepened interest in using them for various policy\ninterventions, through computational data analysis tasks such as spatial hot\nspot detection. In this paper, we consider, for the first time to our best\nknowledge, fairness in detecting spatial hot spots. We motivate the need for\nensuring fairness through statistical parity over the collective population\ncovered across chosen hot spots. We then characterize the task of identifying a\ndiverse set of solutions in the noteworthiness-fairness trade-off spectrum, to\nempower the user to choose a trade-off justified by the policy domain. Being a\nnovel task formulation, we also develop a suite of evaluation metrics for fair\nhot spots, motivated by the need to evaluate pertinent aspects of the task. We\nillustrate the computational infeasibility of identifying fair hot spots using\nnaive and/or direct approaches and devise a method, codenamed {\\it FiSH}, for\nefficiently identifying high-quality, fair and diverse sets of spatial hot\nspots. FiSH traverses the tree-structured search space using heuristics that\nguide it towards identifying effective and fair sets of spatial hot spots.\nThrough an extensive empirical analysis over a real-world dataset from the\ndomain of human development, we illustrate that FiSH generates high-quality\nsolutions at fast response times.",
    "published_date": "2021-06-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.06049v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.00369v1",
    "title": "Rate-Splitting Multiple Access in Cache-Aided Cloud-Radio Access Networks",
    "authors": [
      "Robert-Jeron Reifert",
      "Alaa Alameer Ahmad",
      "Yijie Mao",
      "Aydin Sezgin",
      "Bruno Clerckx"
    ],
    "author_ids": [],
    "abstract": "Rate-splitting multiple access (RSMA) has been recognized as a promising\nphysical layer strategy for 6G. Motivated by ever increasing popularity of\ncache-enabled content delivery in wireless communications, this paper proposes\nan innovative multigroup multicast transmission scheme based on RSMA for\ncache-aided cloud-radio access networks (C-RAN). Our proposed scheme not only\nexploits the properties of content-centric communications and local caching at\nthe base stations (BSs), but also incorporates RSMA to better manage\ninterference in multigroup multicast transmission with statistical channel\nstate information (CSI) known at the central processor (CP) and the BSs. At the\nRSMA-enabled cloud CP, the message of each multicast group is split into a\nprivate and a common part with the former private part being decoded by all\nusers in the respective group and the latter common part being decoded by\nmultiple users from other multicast groups. Common message decoding is done for\nthe purpose of mitigating the interference. In this work, we jointly optimize\nthe clustering of BSs and the precoding with the aim of maximizing the minimum\nrate among all multicast groups to guarantee fairness serving all groups. The\nproblem is a mixed-integer non-linear stochastic program (MINLSP), which is\nsolved by a practical algorithm we proposed including a heuristic clustering\nalgorithm for assigning a set of BSs to serve each user followed by an\nefficient iterative algorithm that combines the sample average approximation\n(SAA) and weighted minimum mean square error (WMMSE) to solve the stochastic\nnon-convex sub-problem of precoder design. Numerical results show the explicit\nmax-min rate gain of our proposed transmission scheme compared to the\nstate-of-the-art trivial interference processing methods. Therefore, we\nconclude that RSMA is a promising technique for cache-aided C-RAN.",
    "published_date": "2021-06-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.00369v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.00326v1",
    "title": "AI-Ethics by Design. Evaluating Public Perception on the Importance of Ethical Design Principles of AI",
    "authors": [
      "Kimon Kieslich",
      "Birte Keller",
      "Christopher Starke"
    ],
    "author_ids": [],
    "abstract": "Despite the immense societal importance of ethically designing artificial\nintelligence (AI), little research on the public perceptions of ethical AI\nprinciples exists. This becomes even more striking when considering that\nethical AI development has the aim to be human-centric and of benefit for the\nwhole society. In this study, we investigate how ethical principles\n(explainability, fairness, security, accountability, accuracy, privacy, machine\nautonomy) are weighted in comparison to each other. This is especially\nimportant, since simultaneously considering ethical principles is not only\ncostly, but sometimes even impossible, as developers must make specific\ntrade-off decisions. In this paper, we give first answers on the relative\nimportance of ethical principles given a specific use case - the use of AI in\ntax fraud detection. The results of a large conjoint survey (n=1099) suggest\nthat, by and large, German respondents found the ethical principles equally\nimportant. However, subsequent cluster analysis shows that different preference\nmodels for ethically designed systems exist among the German population. These\nclusters substantially differ not only in the preferred attributes, but also in\nthe importance level of the attributes themselves. We further describe how\nthese groups are constituted in terms of sociodemographics as well as opinions\non AI. Societal implications as well as design challenges are discussed.",
    "published_date": "2021-06-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.00326v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.00306v4",
    "title": "Understanding peacefulness through the world news",
    "authors": [
      "Vasiliki Voukelatou",
      "Ioanna Miliou",
      "Fosca Giannotti",
      "Luca Pappalardo"
    ],
    "author_ids": [],
    "abstract": "Peacefulness is a principal dimension of well-being and is the way out of\ninequity and violence. Thus, its measurement has drawn the attention of\nresearchers, policymakers, and peacekeepers. During the last years, novel\ndigital data streams have drastically changed the research in this field. The\ncurrent study exploits information extracted from a new digital database called\nGlobal Data on Events, Location, and Tone (GDELT) to capture peacefulness\nthrough the Global Peace Index (GPI). Applying predictive machine learning\nmodels, we demonstrate that news media attention from GDELT can be used as a\nproxy for measuring GPI at a monthly level. Additionally, we use explainable AI\ntechniques to obtain the most important variables that drive the predictions.\nThis analysis highlights each country's profile and provides explanations for\nthe predictions, and particularly for the errors and the events that drive\nthese errors. We believe that digital data exploited by researchers,\npolicymakers, and peacekeepers, with data science tools as powerful as machine\nlearning, could contribute to maximizing the societal benefits and minimizing\nthe risks to peacefulness.",
    "published_date": "2021-06-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.00306v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.00199v2",
    "title": "Agent mental models and Bayesian rules as a tool to create opinion dynamics models",
    "authors": [
      "Andre C. R. Martins"
    ],
    "author_ids": [],
    "abstract": "Traditional models of opinion dynamics provide a simple approach to\nunderstanding human behavior in basic social scenarios. However, when it comes\nto issues such as polarization and extremism, we require a more nuanced\nunderstanding of human biases and cognitive tendencies. In this paper, we\npropose an approach to modeling opinion dynamics by integrating mental models\nand assumptions of individuals agents using Bayesian-inspired methods. By\nexploring the relationship between human rationality and Bayesian theory, we\ndemonstrate the efficacy of these methods in describing how opinions evolve.\nOur analysis leverages the Continuous Opinions and Discrete Actions (CODA)\nmodel, applying Bayesian-inspired rules to account for key human behaviors such\nas confirmation bias, motivated reasoning, and our reluctance to change\nopinions. Through this, we obtain update rules that offer deeper insights into\nthe dynamics of extreme opinions. Our work sheds light on the role of human\nbiases in shaping opinion dynamics and highlights the potential of\nBayesian-inspired modeling to provide more accurate predictions of real-world\nscenarios.\n  Keywords: Opinion dynamics, Bayesian methods, Cognition, CODA, Agent-based\nmodels",
    "published_date": "2021-06-01T00:00:00",
    "year": 2021,
    "categories": [
      "physics.soc-ph",
      "cs.MA",
      "nlin.AO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.00199v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.00181v1",
    "title": "Gender Bias Hidden Behind Chinese Word Embeddings: The Case of Chinese Adjectives",
    "authors": [
      "Meichun Jiao",
      "Ziyang Luo"
    ],
    "author_ids": [],
    "abstract": "Gender bias in word embeddings gradually becomes a vivid research field in\nrecent years. Most studies in this field aim at measurement and debiasing\nmethods with English as the target language. This paper investigates gender\nbias in static word embeddings from a unique perspective, Chinese adjectives.\nBy training word representations with different models, the gender bias behind\nthe vectors of adjectives is assessed. Through a comparison between the\nproduced results and a human-scored data set, we demonstrate how gender bias\nencoded in word embeddings differentiates from people's attitudes.",
    "published_date": "2021-06-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.00181v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.00169v1",
    "title": "Gender Bias Amplification During Speed-Quality Optimization in Neural Machine Translation",
    "authors": [
      "Adithya Renduchintala",
      "Denise Diaz",
      "Kenneth Heafield",
      "Xian Li",
      "Mona Diab"
    ],
    "author_ids": [],
    "abstract": "Is bias amplified when neural machine translation (NMT) models are optimized\nfor speed and evaluated on generic test sets using BLEU? We investigate\narchitectures and techniques commonly used to speed up decoding in\nTransformer-based models, such as greedy search, quantization, average\nattention networks (AANs) and shallow decoder models and show their effect on\ngendered noun translation. We construct a new gender bias test set, SimpleGEN,\nbased on gendered noun phrases in which there is a single, unambiguous, correct\nanswer. While we find minimal overall BLEU degradation as we apply speed\noptimizations, we observe that gendered noun translation performance degrades\nat a much faster rate.",
    "published_date": "2021-06-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.00169v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.00131v1",
    "title": "Clustering-friendly Representation Learning via Instance Discrimination and Feature Decorrelation",
    "authors": [
      "Yaling Tao",
      "Kentaro Takagi",
      "Kouta Nakata"
    ],
    "author_ids": [],
    "abstract": "Clustering is one of the most fundamental tasks in machine learning.\nRecently, deep clustering has become a major trend in clustering techniques.\nRepresentation learning often plays an important role in the effectiveness of\ndeep clustering, and thus can be a principal cause of performance degradation.\nIn this paper, we propose a clustering-friendly representation learning method\nusing instance discrimination and feature decorrelation. Our\ndeep-learning-based representation learning method is motivated by the\nproperties of classical spectral clustering. Instance discrimination learns\nsimilarities among data and feature decorrelation removes redundant correlation\namong features. We utilize an instance discrimination method in which learning\nindividual instance classes leads to learning similarity among instances.\nThrough detailed experiments and examination, we show that the approach can be\nadapted to learning a latent space for clustering. We design novel\nsoftmax-formulated decorrelation constraints for learning. In evaluations of\nimage clustering using CIFAR-10 and ImageNet-10, our method achieves accuracy\nof 81.5% and 95.4%, respectively. We also show that the softmax-formulated\nconstraints are compatible with various neural networks.",
    "published_date": "2021-05-31T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.00131v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.15182v1",
    "title": "Model Mis-specification and Algorithmic Bias",
    "authors": [
      "Runshan Fu",
      "Yangfan Liang",
      "Peter Zhang"
    ],
    "author_ids": [],
    "abstract": "Machine learning algorithms are increasingly used to inform critical\ndecisions. There is a growing concern about bias, that algorithms may produce\nuneven outcomes for individuals in different demographic groups. In this work,\nwe measure bias as the difference between mean prediction errors across groups.\nWe show that even with unbiased input data, when a model is mis-specified: (1)\npopulation-level mean prediction error can still be negligible, but group-level\nmean prediction errors can be large; (2) such errors are not equal across\ngroups; and (3) the difference between errors, i.e., bias, can take the\nworst-case realization. That is, when there are two groups of the same size,\nmean prediction errors for these two groups have the same magnitude but\nopposite signs. In closed form, we show such errors and bias are functions of\nthe first and second moments of the joint distribution of features (for linear\nand probit regressions). We also conduct numerical experiments to show similar\nresults in more general settings. Our work provides a first step for decoupling\nthe impact of different causes of bias.",
    "published_date": "2021-05-31T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.15182v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.15164v4",
    "title": "DISSECT: Disentangled Simultaneous Explanations via Concept Traversals",
    "authors": [
      "Asma Ghandeharioun",
      "Been Kim",
      "Chun-Liang Li",
      "Brendan Jou",
      "Brian Eoff",
      "Rosalind W. Picard"
    ],
    "author_ids": [],
    "abstract": "Explaining deep learning model inferences is a promising venue for scientific\nunderstanding, improving safety, uncovering hidden biases, evaluating fairness,\nand beyond, as argued by many scholars. One of the principal benefits of\ncounterfactual explanations is allowing users to explore \"what-if\" scenarios\nthrough what does not and cannot exist in the data, a quality that many other\nforms of explanation such as heatmaps and influence functions are inherently\nincapable of doing. However, most previous work on generative explainability\ncannot disentangle important concepts effectively, produces unrealistic\nexamples, or fails to retain relevant information. We propose a novel approach,\nDISSECT, that jointly trains a generator, a discriminator, and a concept\ndisentangler to overcome such challenges using little supervision. DISSECT\ngenerates Concept Traversals (CTs), defined as a sequence of generated examples\nwith increasing degrees of concepts that influence a classifier's decision. By\ntraining a generative model from a classifier's signal, DISSECT offers a way to\ndiscover a classifier's inherent \"notion\" of distinct concepts automatically\nrather than rely on user-predefined concepts. We show that DISSECT produces CTs\nthat (1) disentangle several concepts, (2) are influential to a classifier's\ndecision and are coupled to its reasoning due to joint training (3), are\nrealistic, (4) preserve relevant information, and (5) are stable across similar\ninputs. We validate DISSECT on several challenging synthetic and realistic\ndatasets where previous methods fall short of satisfying desirable criteria for\ninterpretability and show that it performs consistently well and better than\nexisting methods. Finally, we present experiments showing applications of\nDISSECT for detecting potential biases of a classifier and identifying spurious\nartifacts that impact predictions.",
    "published_date": "2021-05-31T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.15164v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.15064v1",
    "title": "Using Pareto Simulated Annealing to Address Algorithmic Bias in Machine Learning",
    "authors": [
      "William Blanzeisky",
      "Pádraig Cunningham"
    ],
    "author_ids": [],
    "abstract": "Algorithmic Bias can be due to bias in the training data or issues with the\nalgorithm itself. These algorithmic issues typically relate to problems with\nmodel capacity and regularisation. This underestimation bias may arise because\nthe model has been optimised for good generalisation accuracy without any\nexplicit consideration of bias or fairness. In a sense, we should not be\nsurprised that a model might be biased when it hasn't been \"asked\" not to be.\nIn this paper, we consider including bias (underestimation) as an additional\ncriterion in model training. We present a multi-objective optimisation strategy\nusing Pareto Simulated Annealing that optimise for both balanced accuracy and\nunderestimation. We demonstrate the effectiveness of this strategy on one\nsynthetic and two real-world datasets.",
    "published_date": "2021-05-31T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.15064v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.15032v1",
    "title": "Truthful Mechanisms for Two-Sided Markets via Prophet Inequalities",
    "authors": [
      "Alexander Braun",
      "Thomas Kesselheim"
    ],
    "author_ids": [],
    "abstract": "We design novel mechanisms for welfare-maximization in two-sided markets.\nThat is, there are buyers willing to purchase items and sellers holding items\ninitially, both acting rationally and strategically in order to maximize\nutility. Our mechanisms are designed based on a powerful correspondence between\ntwo-sided markets and prophet inequalities. They satisfy individual\nrationality, dominant-strategy incentive compatibility, budget-balance\nconstraints and give constant-factor approximations to the optimal social\nwelfare.\n  We improve previous results in several settings: Our main focus is on matroid\ndouble auctions, where the set of buyers who obtain an item needs to be\nindependent in a matroid. We construct two mechanisms, the first being a\n$1/3$-approximation of the optimal social welfare satisfying strong\nbudget-balance and requiring the agents to trade in a customized order, the\nsecond being a $1/2$-approximation, weakly budget-balanced and able to deal\nwith online arrival determined by an adversary. In addition, we construct\nconstant-factor approximations in two-sided markets when buyers need to fulfill\na knapsack constraint. Also, in combinatorial double auctions, where buyers\nhave valuation functions over item bundles instead of being interested in only\none item, using similar techniques, we design a mechanism which is a\n$1/2$-approximation of the optimal social welfare, strongly budget-balanced and\ncan deal with online arrival of agents in an adversarial order.",
    "published_date": "2021-05-31T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT",
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.15032v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.14986v1",
    "title": "Feasibility Assessment of Multitasking in MRI Neuroimaging Analysis: Tissue Segmentation, Cross-Modality Conversion and Bias correction",
    "authors": [
      "Mohammad Eslami",
      "Solale Tabarestani",
      "Malek Adjouadi"
    ],
    "author_ids": [],
    "abstract": "Neuroimaging is essential in brain studies for the diagnosis and\nidentification of disease, structure, and function of the brain in its healthy\nand disease states. Literature shows that there are advantages of multitasking\nwith some deep learning (DL) schemes in challenging neuroimaging applications.\nThis study examines the feasibility of using multitasking in three different\napplications, including tissue segmentation, cross-modality conversion, and\nbias-field correction. These applications reflect five different scenarios in\nwhich multitasking is explored and 280 training and testing sessions conducted\nfor empirical evaluations. Two well-known networks, U-Net as a well-known\nconvolutional neural network architecture, and a closed architecture based on\nthe conditional generative adversarial network are implemented. Different\nmetrics such as the normalized cross-correlation coefficient and Dice scores\nare used for comparison of methods and results of the different experiments.\nStatistical analysis is also provided by paired t-test. The present study\nexplores the pros and cons of these methods and their practical impacts on\nmultitasking in different implementation scenarios. This investigation shows\nthat bias correction and cross-modality conversion applications are\nsignificantly easier than the segmentation application, and having multitasking\nwith segmentation is not reasonable if one of them is identified as the main\ntarget application. However, when the main application is the segmentation of\ntissues, multitasking with cross-modality conversion is beneficial, especially\nfor the U-net architecture.",
    "published_date": "2021-05-31T00:00:00",
    "year": 2021,
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG",
      "physics.med-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.14986v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.14890v1",
    "title": "Rawlsian Fair Adaptation of Deep Learning Classifiers",
    "authors": [
      "Kulin Shah",
      "Pooja Gupta",
      "Amit Deshpande",
      "Chiranjib Bhattacharyya"
    ],
    "author_ids": [],
    "abstract": "Group-fairness in classification aims for equality of a predictive utility\nacross different sensitive sub-populations, e.g., race or gender. Equality or\nnear-equality constraints in group-fairness often worsen not only the aggregate\nutility but also the utility for the least advantaged sub-population. In this\npaper, we apply the principles of Pareto-efficiency and least-difference to the\nutility being accuracy, as an illustrative example, and arrive at the Rawls\nclassifier that minimizes the error rate on the worst-off sensitive\nsub-population. Our mathematical characterization shows that the Rawls\nclassifier uniformly applies a threshold to an ideal score of features, in the\nspirit of fair equality of opportunity. In practice, such a score or a feature\nrepresentation is often computed by a black-box model that has been useful but\nunfair. Our second contribution is practical Rawlsian fair adaptation of any\ngiven black-box deep learning model, without changing the score or feature\nrepresentation it computes. Given any score function or feature representation\nand only its second-order statistics on the sensitive sub-populations, we seek\na threshold classifier on the given score or a linear threshold classifier on\nthe given feature representation that achieves the Rawls error rate restricted\nto this hypothesis class. Our technical contribution is to formulate the above\nproblems using ambiguous chance constraints, and to provide efficient\nalgorithms for Rawlsian fair adaptation, along with provable upper bounds on\nthe Rawls error rate. Our empirical results show significant improvement over\nstate-of-the-art group-fair algorithms, even without retraining for fairness.",
    "published_date": "2021-05-31T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.14890v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.14874v2",
    "title": "BiasRV: Uncovering Biased Sentiment Predictions at Runtime",
    "authors": [
      "Zhou Yang",
      "Muhammad Hilmi Asyrofi",
      "David Lo"
    ],
    "author_ids": [],
    "abstract": "Sentiment analysis (SA) systems, though widely applied in many domains, have\nbeen demonstrated to produce biased results. Some research works have been done\nin automatically generating test cases to reveal unfairness in SA systems, but\nthe community still lacks tools that can monitor and uncover biased predictions\nat runtime. This paper fills this gap by proposing BiasRV, the first tool to\nraise an alarm when a deployed SA system makes a biased prediction on a given\ninput text. To implement this feature, BiasRV dynamically extracts a template\nfrom an input text and from the template generates gender-discriminatory\nmutants (semantically-equivalent texts that only differ in gender information).\nBased on popular metrics used to evaluate the overall fairness of an SA system,\nwe define distributional fairness property for an individual prediction of an\nSA system. This property specifies a requirement that for one piece of text,\nmutants from different gender classes should be treated similarly as a whole.\nVerifying the distributional fairness property causes much overhead to the\nrunning system. To run more efficiently, BiasRV adopts a two-step heuristic:\n(1) sampling several mutants from each gender and checking if the system\npredicts them as of the same sentiment, (2) checking distributional fairness\nonly when sampled mutants have conflicting results. Experiments show that\ncompared to directly checking the distributional fairness property for each\ninput text, our two-step heuristic can decrease overhead used for analyzing\nmutants by 73.81% while only resulting in 6.7% of biased predictions being\nmissed. Besides, BiasRV can be used conveniently without knowing the\nimplementation of SA systems. Future researchers can easily extend BiasRV to\ndetect more types of bias, e.g. race and occupation.",
    "published_date": "2021-05-31T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.14874v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.14844v2",
    "title": "Demographic Fairness in Biometric Systems: What do the Experts say?",
    "authors": [
      "Christian Rathgeb",
      "Pawel Drozdowski",
      "Naser Damer",
      "Dinusha C. Frings",
      "Christoph Busch"
    ],
    "author_ids": [],
    "abstract": "Algorithmic decision systems have frequently been labelled as \"biased\",\n\"racist\", \"sexist\", or \"unfair\" by numerous media outlets, organisations, and\nresearchers. There is an ongoing debate whether such assessments are justified\nand whether citizens and policymakers should be concerned. These and other\nrelated matters have recently become a hot topic in the context of biometric\ntechnologies, which are ubiquitous in personal, commercial, and governmental\napplications. Biometrics represent an essential component of many surveillance,\naccess control, and operational identity management systems, thus directly or\nindirectly affecting billions of people all around the world. In order to\nprovide a forum for experts in the field, the European Association for\nBiometrics organised an event series with \"demographic fairness in biometric\nsystems\" as an overarching theme. The events featured presentations by\ninternational experts from academic, industry, and governmental organisations\nand facilitated interactions and discussions between the experts and the\naudience. Further consultation of experts was undertaken by means of a\nquestionnaire. This work summarises opinions of experts and findings of said\nevents on the topic of demographic fairness in biometric systems including\nseveral important aspects such as the developments of evaluation metrics and\nstandards as well as related issues, e.g. the need for transparency and\nexplainability in biometric systems or legal and ethical issues.",
    "published_date": "2021-05-31T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.14844v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.11039v1",
    "title": "Institutionalising Ethics in AI through Broader Impact Requirements",
    "authors": [
      "Carina Prunkl",
      "Carolyn Ashurst",
      "Markus Anderljung",
      "Helena Webb",
      "Jan Leike",
      "Allan Dafoe"
    ],
    "author_ids": [],
    "abstract": "Turning principles into practice is one of the most pressing challenges of\nartificial intelligence (AI) governance. In this article, we reflect on a novel\ngovernance initiative by one of the world's largest AI conferences. In 2020,\nthe Conference on Neural Information Processing Systems (NeurIPS) introduced a\nrequirement for submitting authors to include a statement on the broader\nsocietal impacts of their research. Drawing insights from similar governance\ninitiatives, including institutional review boards (IRBs) and impact\nrequirements for funding applications, we investigate the risks, challenges and\npotential benefits of such an initiative. Among the challenges, we list a lack\nof recognised best practice and procedural transparency, researcher opportunity\ncosts, institutional and social pressures, cognitive biases, and the inherently\ndifficult nature of the task. The potential benefits, on the other hand,\ninclude improved anticipation and identification of impacts, better\ncommunication with policy and governance experts, and a general strengthening\nof the norms around responsible research. To maximise the chance of success, we\nrecommend measures to increase transparency, improve guidance, create\nincentives to engage earnestly with the process, and facilitate public\ndeliberation on the requirement's merits and future. Perhaps the most important\ncontribution from this analysis are the insights we can gain regarding\neffective community-based governance and the role and responsibility of the AI\nresearch community more broadly.",
    "published_date": "2021-05-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.11039v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.14423v2",
    "title": "Enumerating Fair Packages for Group Recommendations",
    "authors": [
      "Ryoma Sato"
    ],
    "author_ids": [],
    "abstract": "Package-to-group recommender systems recommend a set of unified items to a\ngroup of people. Different from conventional settings, it is not easy to\nmeasure the utility of group recommendations because it involves more than one\nuser. In particular, fairness is crucial in group recommendations. Even if some\nmembers in a group are substantially satisfied with a recommendation, it is\nundesirable if other members are ignored to increase the total utility. Many\nmethods for evaluating and applying the fairness of group recommendations have\nbeen proposed in the literature. However, all these methods maximize the score\nand output only one package. This is in contrast to conventional recommender\nsystems, which output several (e.g., top-$K$) candidates. This can be\nproblematic because a group can be dissatisfied with the recommended package\nowing to some unobserved reasons, even if the score is high. To address this\nissue, we propose a method to enumerate fair packages efficiently. Our method\nfurthermore supports filtering queries, such as top-$K$ and intersection, to\nselect favorite packages when the list is long. We confirm that our algorithm\nscales to large datasets and can balance several aspects of the utility of the\npackages.",
    "published_date": "2021-05-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.14423v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2107.07334v1",
    "title": "Tournesol: A quest for a large, secure and trustworthy database of reliable human judgments",
    "authors": [
      "Lê-Nguyên Hoang",
      "Louis Faucon",
      "Aidan Jungo",
      "Sergei Volodin",
      "Dalia Papuc",
      "Orfeas Liossatos",
      "Ben Crulis",
      "Mariame Tighanimine",
      "Isabela Constantin",
      "Anastasiia Kucherenko",
      "Alexandre Maurer",
      "Felix Grimberg",
      "Vlad Nitu",
      "Chris Vossen",
      "Sébastien Rouault",
      "El-Mahdi El-Mhamdi"
    ],
    "author_ids": [],
    "abstract": "Today's large-scale algorithms have become immensely influential, as they\nrecommend and moderate the content that billions of humans are exposed to on a\ndaily basis. They are the de-facto regulators of our societies' information\ndiet, from shaping opinions on public health to organizing groups for social\nmovements. This creates serious concerns, but also great opportunities to\npromote quality information. Addressing the concerns and seizing the\nopportunities is a challenging, enormous and fabulous endeavor, as intuitively\nappealing ideas often come with unwanted {\\it side effects}, and as it requires\nus to think about what we deeply prefer.\n  Understanding how today's large-scale algorithms are built is critical to\ndetermine what interventions will be most effective. Given that these\nalgorithms rely heavily on {\\it machine learning}, we make the following key\nobservation: \\emph{any algorithm trained on uncontrolled data must not be\ntrusted}. Indeed, a malicious entity could take control over the data, poison\nit with dangerously manipulative fabricated inputs, and thereby make the\ntrained algorithm extremely unsafe. We thus argue that the first step towards\nsafe and ethical large-scale algorithms must be the collection of a large,\nsecure and trustworthy dataset of reliable human judgments.\n  To achieve this, we introduce \\emph{Tournesol}, an open source platform\navailable at \\url{https://tournesol.app}. Tournesol aims to collect a large\ndatabase of human judgments on what algorithms ought to widely recommend (and\nwhat they ought to stop widely recommending). We outline the structure of the\nTournesol database, the key features of the Tournesol platform and the main\nhurdles that must be overcome to make it a successful project. Most\nimportantly, we argue that, if successful, Tournesol may then serve as the\nessential foundation for any safe and ethical large-scale algorithm.",
    "published_date": "2021-05-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC",
      "cs.CR",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.07334v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.14311v1",
    "title": "Synthesizing Invariant Barrier Certificates via Difference-of-Convex Programming",
    "authors": [
      "Qiuye Wang",
      "Mingshuai Chen",
      "Bai Xue",
      "Naijun Zhan",
      "Joost-Pieter Katoen"
    ],
    "author_ids": [],
    "abstract": "A barrier certificate often serves as an inductive invariant that isolates an\nunsafe region from the reachable set of states, and hence is widely used in\nproving safety of hybrid systems possibly over the infinite time horizon. We\npresent a novel condition on barrier certificates, termed the invariant\nbarrier-certificate condition, that witnesses unbounded-time safety of\ndifferential dynamical systems. The proposed condition is by far the least\nconservative one on barrier certificates, and can be shown as the weakest\npossible one to attain inductive invariance. We show that discharging the\ninvariant barrier-certificate condition -- thereby synthesizing invariant\nbarrier certificates -- can be encoded as solving an optimization problem\nsubject to bilinear matrix inequalities (BMIs). We further propose a synthesis\nalgorithm based on difference-of-convex programming, which approaches a local\noptimum of the BMI problem via solving a series of convex optimization\nproblems. This algorithm is incorporated in a branch-and-bound framework that\nsearches for the global optimum in a divide-and-conquer fashion. We present a\nweak completeness result of our method, in the sense that a barrier certificate\nis guaranteed to be found (under some mild assumptions) whenever there exists\nan inductive invariant (in the form of a given template) that suffices to\ncertify safety of the system. Experimental results on benchmark examples\ndemonstrate the effectiveness and efficiency of our approach.",
    "published_date": "2021-05-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.14311v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.14262v1",
    "title": "The Privacy Paradox and Optimal Bias-Variance Trade-offs in Data Acquisition",
    "authors": [
      "Guocheng Liao",
      "Yu Su",
      "Juba Ziani",
      "Adam Wierman",
      "Jianwei Huang"
    ],
    "author_ids": [],
    "abstract": "While users claim to be concerned about privacy, often they do little to\nprotect their privacy in their online actions. One prominent explanation for\nthis \"privacy paradox\" is that when an individual shares her data, it is not\njust her privacy that is compromised; the privacy of other individuals with\ncorrelated data is also compromised. This information leakage encourages\noversharing of data and significantly impacts the incentives of individuals in\nonline platforms. In this paper, we study the design of mechanisms for data\nacquisition in settings with information leakage and verifiable data. We design\nan incentive compatible mechanism that optimizes the worst-case trade-off\nbetween bias and variance of the estimation subject to a budget constraint,\nwhere the worst-case is over the unknown correlation between costs and data.\nAdditionally, we characterize the structure of the optimal mechanism in closed\nform and study monotonicity and non-monotonicity properties of the marketplace.",
    "published_date": "2021-05-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.14262v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.14241v1",
    "title": "Demoting the Lead Bias in News Summarization via Alternating Adversarial Learning",
    "authors": [
      "Linzi Xing",
      "Wen Xiao",
      "Giuseppe Carenini"
    ],
    "author_ids": [],
    "abstract": "In news articles the lead bias is a common phenomenon that usually dominates\nthe learning signals for neural extractive summarizers, severely limiting their\nperformance on data with different or even no bias. In this paper, we introduce\na novel technique to demote lead bias and make the summarizer focus more on the\ncontent semantics. Experiments on two news corpora with different degrees of\nlead bias show that our method can effectively demote the model's learned lead\nbias and improve its generality on out-of-distribution data, with little to no\nperformance loss on in-distribution data.",
    "published_date": "2021-05-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.14241v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.14210v1",
    "title": "Exploiting Position Bias for Robust Aspect Sentiment Classification",
    "authors": [
      "Fang Ma",
      "Chen Zhang",
      "Dawei Song"
    ],
    "author_ids": [],
    "abstract": "Aspect sentiment classification (ASC) aims at determining sentiments\nexpressed towards different aspects in a sentence. While state-of-the-art ASC\nmodels have achieved remarkable performance, they are recently shown to suffer\nfrom the issue of robustness. Particularly in two common scenarios: when\ndomains of test and training data are different (out-of-domain scenario) or\ntest data is adversarially perturbed (adversarial scenario), ASC models may\nattend to irrelevant words and neglect opinion expressions that truly describe\ndiverse aspects. To tackle the challenge, in this paper, we hypothesize that\nposition bias (i.e., the words closer to a concerning aspect would carry a\nhigher degree of importance) is crucial for building more robust ASC models by\nreducing the probability of mis-attending. Accordingly, we propose two\nmechanisms for capturing position bias, namely position-biased weight and\nposition-biased dropout, which can be flexibly injected into existing models to\nenhance representations for classification. Experiments conducted on\nout-of-domain and adversarial datasets demonstrate that our proposed approaches\nlargely improve the robustness and effectiveness of current models.",
    "published_date": "2021-05-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.14210v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.14172v2",
    "title": "A Stochastic Alternating Balance $k$-Means Algorithm for Fair Clustering",
    "authors": [
      "Suyun Liu",
      "Luis Nunes Vicente"
    ],
    "author_ids": [],
    "abstract": "In the application of data clustering to human-centric decision-making\nsystems, such as loan applications and advertisement recommendations, the\nclustering outcome might discriminate against people across different\ndemographic groups, leading to unfairness. A natural conflict occurs between\nthe cost of clustering (in terms of distance to cluster centers) and the\nbalance representation of all demographic groups across the clusters, leading\nto a bi-objective optimization problem that is nonconvex and nonsmooth. To\ndetermine the complete trade-off between these two competing goals, we design a\nnovel stochastic alternating balance fair $k$-means (SAfairKM) algorithm, which\nconsists of alternating classical mini-batch $k$-means updates and group swap\nupdates. The number of $k$-means updates and the number of swap updates\nessentially parameterize the weight put on optimizing each objective function.\nOur numerical experiments show that the proposed SAfairKM algorithm is robust\nand computationally efficient in constructing well-spread and high-quality\nPareto fronts both on synthetic and real datasets.",
    "published_date": "2021-05-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.14172v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.14150v4",
    "title": "Annotation Inconsistency and Entity Bias in MultiWOZ",
    "authors": [
      "Kun Qian",
      "Ahmad Beirami",
      "Zhouhan Lin",
      "Ankita De",
      "Alborz Geramifard",
      "Zhou Yu",
      "Chinnadhurai Sankar"
    ],
    "author_ids": [],
    "abstract": "MultiWOZ is one of the most popular multi-domain task-oriented dialog\ndatasets, containing 10K+ annotated dialogs covering eight domains. It has been\nwidely accepted as a benchmark for various dialog tasks, e.g., dialog state\ntracking (DST), natural language generation (NLG), and end-to-end (E2E) dialog\nmodeling. In this work, we identify an overlooked issue with dialog state\nannotation inconsistencies in the dataset, where a slot type is tagged\ninconsistently across similar dialogs leading to confusion for DST modeling. We\npropose an automated correction for this issue, which is present in a whopping\n70% of the dialogs. Additionally, we notice that there is significant entity\nbias in the dataset (e.g., \"cambridge\" appears in 50% of the destination cities\nin the train domain). The entity bias can potentially lead to named entity\nmemorization in generative models, which may go unnoticed as the test set\nsuffers from a similar entity bias as well. We release a new test set with all\nentities replaced with unseen entities. Finally, we benchmark joint goal\naccuracy (JGA) of the state-of-the-art DST baselines on these modified versions\nof the data. Our experiments show that the annotation inconsistency corrections\nlead to 7-10% improvement in JGA. On the other hand, we observe a 29% drop in\nJGA when models are evaluated on the new test set with unseen entities.",
    "published_date": "2021-05-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.14150v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.14149v1",
    "title": "Log2NS: Enhancing Deep Learning Based Analysis of Logs With Formal to Prevent Survivorship Bias",
    "authors": [
      "Charanraj Thimmisetty",
      "Praveen Tiwari",
      "Didac Gil de la Iglesia",
      "Nandini Ramanan",
      "Marjorie Sayer",
      "Viswesh Ananthakrishnan",
      "Claudionor Nunes Coelho Jr"
    ],
    "author_ids": [],
    "abstract": "Analysis of large observational data sets generated by a reactive system is a\ncommon challenge in debugging system failures and determining their root cause.\nOne of the major problems is that these observational data suffer from\nsurvivorship bias. Examples include analyzing traffic logs from networks, and\nsimulation logs from circuit design. In such applications, users want to detect\nnon-spurious correlations from observational data and obtain actionable\ninsights about them. In this paper, we introduce log to Neuro-symbolic\n(Log2NS), a framework that combines probabilistic analysis from machine\nlearning (ML) techniques on observational data with certainties derived from\nsymbolic reasoning on an underlying formal model. We apply the proposed\nframework to network traffic debugging by employing the following steps. To\ndetect patterns in network logs, we first generate global embedding vector\nrepresentations of entities such as IP addresses, ports, and applications.\nNext, we represent large log flow entries as clusters that make it easier for\nthe user to visualize and detect interesting scenarios that will be further\nanalyzed. To generalize these patterns, Log2NS provides an ability to query\nfrom static logs and correlation engines for positive instances, as well as\nformal reasoning for negative and unseen instances. By combining the strengths\nof deep learning and symbolic methods, Log2NS provides a very powerful\nreasoning and debugging tool for log-based data. Empirical evaluations on a\nreal internal data set demonstrate the capabilities of Log2NS.",
    "published_date": "2021-05-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.14149v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.14146v1",
    "title": "Deep Fair Discriminative Clustering",
    "authors": [
      "Hongjing Zhang",
      "Ian Davidson"
    ],
    "author_ids": [],
    "abstract": "Deep clustering has the potential to learn a strong representation and hence\nbetter clustering performance compared to traditional clustering methods such\nas $k$-means and spectral clustering. However, this strong representation\nlearning ability may make the clustering unfair by discovering surrogates for\nprotected information which we empirically show in our experiments. In this\nwork, we study a general notion of group-level fairness for both binary and\nmulti-state protected status variables (PSVs). We begin by formulating the\ngroup-level fairness problem as an integer linear programming formulation whose\ntotally unimodular constraint matrix means it can be efficiently solved via\nlinear programming. We then show how to inject this solver into a\ndiscriminative deep clustering backbone and hence propose a refinement learning\nalgorithm to combine the clustering goal with the fairness objective to learn\nfair clusters adaptively. Experimental results on real-world datasets\ndemonstrate that our model consistently outperforms state-of-the-art fair\nclustering algorithms. Our framework shows promising results for novel\nclustering tasks including flexible fairness constraints, multi-state PSVs and\npredictive clustering.",
    "published_date": "2021-05-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.14146v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.14117v4",
    "title": "About Explicit Variance Minimization: Training Neural Networks for Medical Imaging With Limited Data Annotations",
    "authors": [
      "Dmitrii Shubin",
      "Danny Eytan",
      "Sebastian D. Goodfellow"
    ],
    "author_ids": [],
    "abstract": "Self-supervised learning methods for computer vision have demonstrated the\neffectiveness of pre-training feature representations, resulting in\nwell-generalizing Deep Neural Networks, even if the annotated data are limited.\nHowever, representation learning techniques require a significant amount of\ntime for model training, with most of the time spent on precise hyper-parameter\noptimization and selection of augmentation techniques. We hypothesized that if\nthe annotated dataset has enough morphological diversity to capture the\ndiversity of the general population, as is common in medical imaging due to\nconserved similarities of tissue morphology, the variance error of the trained\nmodel is the dominant component of the Bias-Variance Trade-off. Therefore, we\nproposed the Variance Aware Training (VAT) method that exploits this data\nproperty by introducing the variance error into the model loss function,\nthereby, explicitly regularizing the model. Additionally, we provided a\ntheoretical formulation and proof of the proposed method to aid interpreting\nthe approach. Our method requires selecting only one hyper-parameter and\nmatching or improving the performance of state-of-the-art self-supervised\nmethods while achieving an order of magnitude reduction in the GPU training\ntime. We validated VAT on three medical imaging datasets from diverse domains\nand for various learning objectives. These included a Magnetic Resonance\nImaging (MRI) dataset for the heart semantic segmentation (MICCAI 2017 ACDC\nchallenge), fundus photography dataset for ordinary regression of diabetic\nretinopathy progression (Kaggle 2019 APTOS Blindness Detection challenge), and\nclassification of histopathologic scans of lymph node sections (PatchCamelyon\ndataset). Our code is available at\nhttps://github.com/DmitriiShubin/Variance-Aware-Training.",
    "published_date": "2021-05-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "68T07 (Primary) 68T45 (Secondary)"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.14117v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.14077v1",
    "title": "On the Bias Against Inductive Biases",
    "authors": [
      "George Cazenavette",
      "Simon Lucey"
    ],
    "author_ids": [],
    "abstract": "Borrowing from the transformer models that revolutionized the field of\nnatural language processing, self-supervised feature learning for visual tasks\nhas also seen state-of-the-art success using these extremely deep, isotropic\nnetworks. However, the typical AI researcher does not have the resources to\nevaluate, let alone train, a model with several billion parameters and\nquadratic self-attention activations. To facilitate further research, it is\nnecessary to understand the features of these huge transformer models that can\nbe adequately studied by the typical researcher. One interesting characteristic\nof these transformer models is that they remove most of the inductive biases\npresent in classical convolutional networks. In this work, we analyze the\neffect of these and more inductive biases on small to moderately-sized\nisotropic networks used for unsupervised visual feature learning and show that\ntheir removal is not always ideal.",
    "published_date": "2021-05-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.14077v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.14069v2",
    "title": "The Evaluation of Rating Systems in Team-based Battle Royale Games",
    "authors": [
      "Arman Dehpanah",
      "Muheeb Faizan Ghori",
      "Jonathan Gemmell",
      "Bamshad Mobasher"
    ],
    "author_ids": [],
    "abstract": "Online competitive games have become a mainstream entertainment platform. To\ncreate a fair and exciting experience, these games use rating systems to match\nplayers with similar skills. While there has been an increasing amount of\nresearch on improving the performance of these systems, less attention has been\npaid to how their performance is evaluated. In this paper, we explore the\nutility of several metrics for evaluating three popular rating systems on a\nreal-world dataset of over 25,000 team battle royale matches. Our results\nsuggest considerable differences in their evaluation patterns. Some metrics\nwere highly impacted by the inclusion of new players. Many could not capture\nthe real differences between certain groups of players. Among all metrics\nstudied, normalized discounted cumulative gain (NDCG) demonstrated more\nreliable performance and more flexibility. It alleviated most of the challenges\nfaced by the other metrics while adding the freedom to adjust the focus of the\nevaluations on different groups of players.",
    "published_date": "2021-05-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.PF"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.14069v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.14058v1",
    "title": "Symmetry-driven graph neural networks",
    "authors": [
      "Francesco Farina",
      "Emma Slade"
    ],
    "author_ids": [],
    "abstract": "Exploiting symmetries and invariance in data is a powerful, yet not fully\nexploited, way to achieve better generalisation with more efficiency. In this\npaper, we introduce two graph network architectures that are equivariant to\nseveral types of transformations affecting the node coordinates. First, we\nbuild equivariance to any transformation in the coordinate embeddings that\npreserves the distance between neighbouring nodes, allowing for equivariance to\nthe Euclidean group. Then, we introduce angle attributes to build equivariance\nto any angle preserving transformation - thus, to the conformal group. Thanks\nto their equivariance properties, the proposed models can be vastly more data\nefficient with respect to classical graph architectures, intrinsically equipped\nwith a better inductive bias and better at generalising. We demonstrate these\ncapabilities on a synthetic dataset composed of $n$-dimensional geometric\nobjects. Additionally, we provide examples of their limitations when (the\nright) symmetries are not present in the data.",
    "published_date": "2021-05-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.14058v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.13835v1",
    "title": "Kernel-based methods for Solving Time-Dependent Advection-Diffusion Equations on Manifolds",
    "authors": [
      "Qile Yan",
      "Shixiao Willing Jiang",
      "John Harlim"
    ],
    "author_ids": [],
    "abstract": "In this paper, we extend the class of kernel methods, the so-called diffusion\nmaps (DM) and ghost point diffusion maps (GPDM), to solve the time-dependent\nadvection-diffusion PDE on unknown smooth manifolds without and with\nboundaries. The core idea is to directly approximate the spatial components of\nthe differential operator on the manifold with a local integral operator and\ncombine it with the standard implicit time difference scheme. When the manifold\nhas a boundary, a simplified version of the GPDM approach is used to overcome\nthe bias of the integral approximation near the boundary. The Monte-Carlo\ndiscretization of the integral operator over the point cloud data gives rise to\na mesh-free formulation that is natural for randomly distributed points, even\nwhen the manifold is embedded in high-dimensional ambient space. Here, we\nestablish the convergence of the proposed solver on appropriate topologies,\ndepending on the distribution of point cloud data and boundary type. We provide\nnumerical results to validate the convergence results on various examples that\ninvolve simple geometry and an unknown manifold. Additionally, we also found\npositive results in solving the one-dimensional viscous Burger's equation where\nGPDM is adopted with a pseudo-spectral Galerkin framework to approximate\nnonlinear advection term.",
    "published_date": "2021-05-28T00:00:00",
    "year": 2021,
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.13835v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.13782v1",
    "title": "How to Split: the Effect of Word Segmentation on Gender Bias in Speech Translation",
    "authors": [
      "Marco Gaido",
      "Beatrice Savoldi",
      "Luisa Bentivogli",
      "Matteo Negri",
      "Marco Turchi"
    ],
    "author_ids": [],
    "abstract": "Having recognized gender bias as a major issue affecting current translation\ntechnologies, researchers have primarily attempted to mitigate it by working on\nthe data front. However, whether algorithmic aspects concur to exacerbate\nunwanted outputs remains so far under-investigated. In this work, we bring the\nanalysis on gender bias in automatic translation onto a seemingly neutral yet\ncritical component: word segmentation. Can segmenting methods influence the\nability to translate gender? Do certain segmentation approaches penalize the\nrepresentation of feminine linguistic markings? We address these questions by\ncomparing 5 existing segmentation strategies on the target side of speech\ntranslation systems. Our results on two language pairs (English-Italian/French)\nshow that state-of-the-art sub-word splitting (BPE) comes at the cost of higher\ngender bias. In light of this finding, we propose a combined approach that\npreserves BPE overall translation quality, while leveraging the higher ability\nof character-based segmentation to properly translate gender.",
    "published_date": "2021-05-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.13782v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.13774v1",
    "title": "Mapping urban socioeconomic inequalities in developing countries through Facebook advertising data",
    "authors": [
      "Serena Giurgola",
      "Simone Piaggesi",
      "Márton Karsai",
      "Yelena Mejova",
      "André Panisson",
      "Michele Tizzoni"
    ],
    "author_ids": [],
    "abstract": "Ending poverty in all its forms everywhere is the number one Sustainable\nDevelopment Goal of the UN 2030 Agenda. To monitor the progress towards such an\nambitious target, reliable, up-to-date and fine-grained measurements of\nsocioeconomic indicators are necessary. When it comes to socioeconomic\ndevelopment, novel digital traces can provide a complementary data source to\novercome the limits of traditional data collection methods, which are often not\nregularly updated and lack adequate spatial resolution. In this study, we\ncollect publicly available and anonymous advertising audience estimates from\nFacebook to predict socioeconomic conditions of urban residents, at a fine\nspatial granularity, in four large urban areas: Atlanta (USA), Bogot\\'a\n(Colombia), Santiago (Chile), and Casablanca (Morocco). We find that behavioral\nattributes inferred from the Facebook marketing platform can accurately map the\nsocioeconomic status of residential areas within cities, and that predictive\nperformance is comparable in both high and low-resource settings. We also show\nthat training a model on attributes of adult Facebook users, aged more than 25,\nleads to a more accurate mapping of socioeconomic conditions in all cities. Our\nwork provides additional evidence of the value of social advertising media data\nto measure human development.",
    "published_date": "2021-05-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.13774v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.13728v2",
    "title": "An Explanatory Query-Based Framework for Exploring Academic Expertise",
    "authors": [
      "Oana Cocarascu",
      "Andrew McLean",
      "Paul French",
      "Francesca Toni"
    ],
    "author_ids": [],
    "abstract": "The success of research institutions heavily relies upon identifying the\nright researchers \"for the job\": researchers may need to identify appropriate\ncollaborators, often from across disciplines; students may need to identify\nsuitable supervisors for projects of their interest; administrators may need to\nmatch funding opportunities with relevant researchers, and so on. Usually,\nfinding potential collaborators in institutions is a time-consuming manual\nsearch task prone to bias. In this paper, we propose a novel query-based\nframework for searching, scoring, and exploring research expertise\nautomatically, based upon processing abstracts of academic publications. Given\nuser queries in natural language, our framework finds researchers with relevant\nexpertise, making use of domain-specific knowledge bases and word embeddings.\nIt also generates explanations for its recommendations. We evaluate our\nframework with an institutional repository of papers from a leading university,\nusing, as baselines, artificial neural networks and transformer-based models\nfor a multilabel classification task to identify authors of publication\nabstracts. We also assess the cross-domain effectiveness of our framework with\na (separate) research funding repository for the same institution. We show that\nour simple method is effective in identifying matches, while satisfying\ndesirable properties and being efficient.",
    "published_date": "2021-05-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.13728v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.13703v1",
    "title": "SPFA: SFA on Multiple Persistent Faults",
    "authors": [
      "Susanne Engels",
      "Falk Schellenberg",
      "Christof Paar"
    ],
    "author_ids": [],
    "abstract": "For classical fault analysis, a transient fault is required to be injected\nduring runtime, e.g., only at a specific round. Instead, Persistent Fault\nAnalysis (PFA) introduces a powerful class of fault attacks that allows for a\nfault to be present throughout the whole execution. One limitation of original\nPFA as introduced by Zhang et al. at CHES'18 is that the faulty values need to\nbe known to the adversary. While this was addressed at a follow-up work at\nCHES'20, the solution is only applicable to a single faulty value. Instead, we\nuse the potency of Statistical Fault Analysis (SFA) in the persistent fault\nsetting, presenting Statistical Persistent Fault Analysis (SPFA) as a more\ngeneral approach of PFA. As a result, any or even a multitude of unknown faults\nthat cause an exploitable bias in the targeted round can be used to recover the\ncipher's secret key. Indeed, the undesired faults in the other rounds that\noccur due the persistent nature of the attack converge to a uniform\ndistribution as required by SFA. We verify the effectiveness of our attack\nagainst LED and AES.",
    "published_date": "2021-05-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.13703v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.13609v3",
    "title": "A nearly Blackwell-optimal policy gradient method",
    "authors": [
      "Vektor Dewanto",
      "Marcus Gallagher"
    ],
    "author_ids": [],
    "abstract": "For continuing environments, reinforcement learning (RL) methods commonly\nmaximize the discounted reward criterion with discount factor close to 1 in\norder to approximate the average reward (the gain). However, such a criterion\nonly considers the long-run steady-state performance, ignoring the transient\nbehaviour in transient states. In this work, we develop a policy gradient\nmethod that optimizes the gain, then the bias (which indicates the transient\nperformance and is important to capably select from policies with equal gain).\nWe derive expressions that enable sampling for the gradient of the bias and its\npreconditioning Fisher matrix. We further devise an algorithm that solves the\ngain-then-bias (bi-level) optimization. Its key ingredient is an RL-specific\nlogarithmic barrier function. Experimental results provide insights into the\nfundamental mechanisms of our proposal.",
    "published_date": "2021-05-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.13609v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.13515v1",
    "title": "Vaccine Credential Technology Principles",
    "authors": [
      "Divya Siddarth",
      "Vi Hart",
      "Bethan Cantrell",
      "Kristina Yasuda",
      "Josh Mandel",
      "Karen Easterbrook"
    ],
    "author_ids": [],
    "abstract": "The historically rapid development of effective COVID-19 vaccines has\npolicymakers facing evergreen public health questions regarding vaccination\nrecords and verification. Governments and institutions around the world are\nalready taking action on digital vaccine certificates, including guidance and\nrecommendations from the European Commission, the WHO, and the Biden\nAdministration. These could be encouraging efforts: an effective system for\nvaccine certificates could potentially be part of a safe return to work,\ntravel, and daily life, and a secure technological implementation could improve\non existing systems to prioritize privacy, streamline access, and build for\nnecessary interoperability across countries and contexts. However, vaccine\ncredentials are not without potential harms, and, particularly given major\ninequities in vaccine access and rollout, there are valid concerns that they\nmay be used in ineffective or exclusionary ways that exacerbate inequality,\nallow for discrimination, violate privacy, and assume consent. While the\npresent moment calls for urgency, we must also acknowledge that choices made in\nthe vaccine credentialing rollout for COVID-19 are likely to have long-term\nimplications, and must be made with care. In this paper, we outline potential\nimplementation and ethical concerns that may arise from tech-enabled vaccine\ncredentialing programs now and in the future, and discuss the technological\ntradeoffs implicated in these concerns. We suggest a set of principles that, if\nadopted, may mitigate these concerns, forestall preventable harms, and point\nthe way forward; the paper is structured as a deep dive into each of these\nprinciples.",
    "published_date": "2021-05-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.13515v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2106.04411v2",
    "title": "Fair Feature Distillation for Visual Recognition",
    "authors": [
      "Sangwon Jung",
      "Donggyu Lee",
      "Taeeon Park",
      "Taesup Moon"
    ],
    "author_ids": [],
    "abstract": "Fairness is becoming an increasingly crucial issue for computer vision,\nespecially in the human-related decision systems. However, achieving\nalgorithmic fairness, which makes a model produce indiscriminative outcomes\nagainst protected groups, is still an unresolved problem. In this paper, we\ndevise a systematic approach which reduces algorithmic biases via feature\ndistillation for visual recognition tasks, dubbed as MMD-based Fair\nDistillation (MFD). While the distillation technique has been widely used in\ngeneral to improve the prediction accuracy, to the best of our knowledge, there\nhas been no explicit work that also tries to improve fairness via distillation.\nFurthermore, We give a theoretical justification of our MFD on the effect of\nknowledge distillation and fairness. Throughout the extensive experiments, we\nshow our MFD significantly mitigates the bias against specific minorities\nwithout any loss of the accuracy on both synthetic and real-world face\ndatasets.",
    "published_date": "2021-05-27T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.04411v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.13093v1",
    "title": "Towards Understanding Knowledge Distillation",
    "authors": [
      "Mary Phuong",
      "Christoph H. Lampert"
    ],
    "author_ids": [],
    "abstract": "Knowledge distillation, i.e., one classifier being trained on the outputs of\nanother classifier, is an empirically very successful technique for knowledge\ntransfer between classifiers. It has even been observed that classifiers learn\nmuch faster and more reliably if trained with the outputs of another classifier\nas soft labels, instead of from ground truth data. So far, however, there is no\nsatisfactory theoretical explanation of this phenomenon. In this work, we\nprovide the first insights into the working mechanisms of distillation by\nstudying the special case of linear and deep linear classifiers. Specifically,\nwe prove a generalization bound that establishes fast convergence of the\nexpected risk of a distillation-trained linear classifier. From the bound and\nits proof we extract three key factors that determine the success of\ndistillation: * data geometry -- geometric properties of the data distribution,\nin particular class separation, has a direct influence on the convergence speed\nof the risk; * optimization bias -- gradient descent optimization finds a very\nfavorable minimum of the distillation objective; and * strong monotonicity --\nthe expected risk of the student classifier always decreases when the size of\nthe training set grows.",
    "published_date": "2021-05-27T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.13093v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.13010v5",
    "title": "An error analysis of generative adversarial networks for learning distributions",
    "authors": [
      "Jian Huang",
      "Yuling Jiao",
      "Zhen Li",
      "Shiao Liu",
      "Yang Wang",
      "Yunfei Yang"
    ],
    "author_ids": [],
    "abstract": "This paper studies how well generative adversarial networks (GANs) learn\nprobability distributions from finite samples. Our main results establish the\nconvergence rates of GANs under a collection of integral probability metrics\ndefined through H\\\"older classes, including the Wasserstein distance as a\nspecial case. We also show that GANs are able to adaptively learn data\ndistributions with low-dimensional structures or have H\\\"older densities, when\nthe network architectures are chosen properly. In particular, for distributions\nconcentrated around a low-dimensional set, we show that the learning rates of\nGANs do not depend on the high ambient dimension, but on the lower intrinsic\ndimension. Our analysis is based on a new oracle inequality decomposing the\nestimation error into the generator and discriminator approximation error and\nthe statistical error, which may be of independent interest.",
    "published_date": "2021-05-27T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.13010v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.12982v1",
    "title": "Sampling from the Gibbs Distribution in Congestion Games",
    "authors": [
      "Pieter Kleer"
    ],
    "author_ids": [],
    "abstract": "Logit dynamics is a form of randomized game dynamics where players have a\nbias towards strategic deviations that give a higher improvement in cost. It is\nused extensively in practice. In congestion (or potential) games, the dynamics\nconverges to the so-called Gibbs distribution over the set of all strategy\nprofiles, when interpreted as a Markov chain. In general, logit dynamics might\nconverge slowly to the Gibbs distribution, but beyond that, not much is known\nabout their algorithmic aspects, nor that of the Gibbs distribution. In this\nwork, we are interested in the following two questions for congestion games: i)\nIs there an efficient algorithm for sampling from the Gibbs distribution? ii)\nIf yes, do there also exist natural randomized dynamics that converges quickly\nto the Gibbs distribution?\n  We first study these questions in extension parallel congestion games, a\nwell-studied special case of symmetric network congestion games. As our main\nresult, we show that there is a simple variation on the logit dynamics (in\nwhich we in addition are allowed to randomly interchange the strategies of two\nplayers) that converges quickly to the Gibbs distribution in such games. This\nanswers both questions above affirmatively. We also address the first question\nfor the class of so-called capacitated $k$-uniform congestion games.\n  To prove our results, we rely on the recent breakthrough work of Anari, Liu,\nOveis-Gharan and Vinzant (2019) concerning the approximate sampling of the base\nof a matroid according to strongly log-concave probability distribution.",
    "published_date": "2021-05-27T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT",
      "cs.DM",
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.12982v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.12980v2",
    "title": "Investigating label suggestions for opinion mining in German Covid-19 social media",
    "authors": [
      "Tilman Beck",
      "Ji-Ung Lee",
      "Christina Viehmann",
      "Marcus Maurer",
      "Oliver Quiring",
      "Iryna Gurevych"
    ],
    "author_ids": [],
    "abstract": "This work investigates the use of interactively updated label suggestions to\nimprove upon the efficiency of gathering annotations on the task of opinion\nmining in German Covid-19 social media data. We develop guidelines to conduct a\ncontrolled annotation study with social science students and find that\nsuggestions from a model trained on a small, expert-annotated dataset already\nlead to a substantial improvement - in terms of inter-annotator agreement(+.14\nFleiss' $\\kappa$) and annotation quality - compared to students that do not\nreceive any label suggestions. We further find that label suggestions from\ninteractively trained models do not lead to an improvement over suggestions\nfrom a static model. Nonetheless, our analysis of suggestion bias shows that\nannotators remain capable of reflecting upon the suggested label in general.\nFinally, we confirm the quality of the annotated data in transfer learning\nexperiments between different annotator groups. To facilitate further research\nin opinion mining on social media data, we release our collected data\nconsisting of 200 expert and 2,785 student annotations.",
    "published_date": "2021-05-27T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.12980v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.12856v2",
    "title": "Beyond Algorithmic Bias: A Socio-Computational Interrogation of the Google Search by Image Algorithm",
    "authors": [
      "Orestis Papakyriakopoulos",
      "Arwa Michelle Mboya"
    ],
    "author_ids": [],
    "abstract": "We perform a socio-computational interrogation of the google search by image\nalgorithm, a main component of the google search engine. We audit the algorithm\nby presenting it with more than 40 thousands faces of all ages and more than\nfour races and collecting and analyzing the assigned labels with the\nappropriate statistical tools. We find that the algorithm reproduces white male\npatriarchal structures, often simplifying, stereotyping and discriminating\nfemales and non-white individuals, while providing more diverse and positive\ndescriptions of white men. By drawing from Bourdieu's theory of cultural\nreproduction, we link these results to the attitudes of the algorithm's\ndesigners, owners, and the dataset the algorithm was trained on. We further\nunderpin the problematic nature of the algorithm by using the ethnographic\npractice of studying-up: We show how the algorithm places individuals at the\ntop of the tech industry within the socio-cultural reality that they shaped,\nmany times creating biased representations of them. We claim that the use of\nsocial-theoretic frameworks such as the above are able to contribute to\nimproved algorithmic accountability, algorithmic impact assessment and provide\nadditional and more critical depth in algorithmic bias and auditing studies.\nBased on the analysis, we discuss the scientific and design implications and\nprovide suggestions for alternative ways to design just socioalgorithmic\nsystems.",
    "published_date": "2021-05-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.12856v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.12821v1",
    "title": "On the Achievable Max-Min User Rates in Multi-Carrier Centralized NOMA-VLC Networks",
    "authors": [
      "Omar Maraqa",
      "Umair F. Siddiqi",
      "Saad Al-Ahmadi",
      "Sadiq M. Sait"
    ],
    "author_ids": [],
    "abstract": "Visible light communications (VLC) is gaining interest as one of the enablers\nof short-distance, high-data-rate applications, in future beyond 5G networks.\nMoreover, non-orthogonal multiple-access (NOMA)-enabled schemes have recently\nemerged as a promising multiple-access scheme for these networks that would\nallow realization of the target spectral efficiency and user fairness\nrequirements. The integration of NOMA in the widely adopted orthogonal\nfrequency-division multiplexing (OFDM)-based VLC networks would require an\noptimal resource allocation for the pair or the cluster of users sharing the\nsame subcarrier(s). In this paper, the max-min rate of a multi-cell indoor\ncentralized VLC network is maximized through optimizing user pairing,\nsubcarrier allocation, and power allocation. The joint complex optimization\nproblem is tackled using a low-complexity solution. At first, the user pairing\nis assumed to follow the divide-and-next-largest-difference user-pairing\nalgorithm (D-NLUPA) that can ensure fairness among the different clusters.\nThen, subcarrier allocation and power allocation are solved iteratively through\nboth the Simulated Annealing (SA) meta-heuristic algorithm and the bisection\nmethod. The obtained results quantify the achievable max-min user rates for the\ndifferent relevant variants of NOMA-enabled schemes and shed new light on both\nthe performance and design of multi-user multi-carrier NOMA-enabled centralized\nVLC networks.",
    "published_date": "2021-05-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IT",
      "eess.SP",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.12821v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.12754v1",
    "title": "Computer Vision and Conflicting Values: Describing People with Automated Alt Text",
    "authors": [
      "Margot Hanley",
      "Solon Barocas",
      "Karen Levy",
      "Shiri Azenkot",
      "Helen Nissenbaum"
    ],
    "author_ids": [],
    "abstract": "Scholars have recently drawn attention to a range of controversial issues\nposed by the use of computer vision for automatically generating descriptions\nof people in images. Despite these concerns, automated image description has\nbecome an important tool to ensure equitable access to information for blind\nand low vision people. In this paper, we investigate the ethical dilemmas faced\nby companies that have adopted the use of computer vision for producing alt\ntext: textual descriptions of images for blind and low vision people, We use\nFacebook's automatic alt text tool as our primary case study. First, we analyze\nthe policies that Facebook has adopted with respect to identity categories,\nsuch as race, gender, age, etc., and the company's decisions about whether to\npresent these terms in alt text. We then describe an alternative -- and manual\n-- approach practiced in the museum community, focusing on how museums\ndetermine what to include in alt text descriptions of cultural artifacts. We\ncompare these policies, using notable points of contrast to develop an analytic\nframework that characterizes the particular apprehensions behind these policy\nchoices. We conclude by considering two strategies that seem to sidestep some\nof these concerns, finding that there are no easy ways to avoid the normative\ndilemmas posed by the use of computer vision to automate alt text.",
    "published_date": "2021-05-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.12754v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.12697v6",
    "title": "Structural Causal Models Reveal Confounder Bias in Linear Program Modelling",
    "authors": [
      "Matej Zečević",
      "Devendra Singh Dhami",
      "Kristian Kersting"
    ],
    "author_ids": [],
    "abstract": "The recent years have been marked by extended research on adversarial\nattacks, especially on deep neural networks. With this work we intend on posing\nand investigating the question of whether the phenomenon might be more general\nin nature, that is, adversarial-style attacks outside classical classification\ntasks. Specifically, we investigate optimization problems as they constitute a\nfundamental part of modern AI research. To this end, we consider the base class\nof optimizers namely Linear Programs (LPs). On our initial attempt of a na\\\"ive\nmapping between the formalism of adversarial examples and LPs, we quickly\nidentify the key ingredients missing for making sense of a reasonable notion of\nadversarial examples for LPs. Intriguingly, the formalism of Pearl's notion to\ncausality allows for the right description of adversarial like examples for\nLPs. Characteristically, we show the direct influence of the Structural Causal\nModel (SCM) onto the subsequent LP optimization, which ultimately exposes a\nnotion of confounding in LPs (inherited by said SCM) that allows for\nadversarial-style attacks. We provide both the general proof formally alongside\nexistential proofs of such intriguing LP-parameterizations based on SCM for\nthree combinatorial problems, namely Linear Assignment, Shortest Path and a\nreal world problem of energy systems.",
    "published_date": "2021-05-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.12697v6",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.12479v2",
    "title": "Pattern Detection in the Activation Space for Identifying Synthesized Content",
    "authors": [
      "Celia Cintas",
      "Skyler Speakman",
      "Girmaw Abebe Tadesse",
      "Victor Akinwande",
      "Edward McFowland III",
      "Komminist Weldemariam"
    ],
    "author_ids": [],
    "abstract": "Generative Adversarial Networks (GANs) have recently achieved unprecedented\nsuccess in photo-realistic image synthesis from low-dimensional random noise.\nThe ability to synthesize high-quality content at a large scale brings\npotential risks as the generated samples may lead to misinformation that can\ncreate severe social, political, health, and business hazards. We propose\nSubsetGAN to identify generated content by detecting a subset of anomalous\nnode-activations in the inner layers of pre-trained neural networks. These\nnodes, as a group, maximize a non-parametric measure of divergence away from\nthe expected distribution of activations created from real data. This enable us\nto identify synthesised images without prior knowledge of their distribution.\nSubsetGAN efficiently scores subsets of nodes and returns the group of nodes\nwithin the pre-trained classifier that contributed to the maximum score. The\nclassifier can be a general fake classifier trained over samples from multiple\nsources or the discriminator network from different GANs. Our approach shows\nconsistently higher detection power than existing detection methods across\nseveral state-of-the-art GANs (PGGAN, StarGAN, and CycleGAN) and over different\nproportions of generated content.",
    "published_date": "2021-05-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.CR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.12479v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.12239v3",
    "title": "Finite sample guarantees for quantile estimation: An application to detector threshold tuning",
    "authors": [
      "David Umsonst",
      "Justin Ruths",
      "Henrik Sandberg"
    ],
    "author_ids": [],
    "abstract": "In threshold-based anomaly detection, we want to tune the threshold of a\ndetector to achieve an acceptable false alarm rate. However, tuning the\nthreshold is often a non-trivial task due to unknown detector output\ndistributions. A detector threshold that provides an acceptable false alarm\nrate is equivalent to a specific quantile of the detector output distribution.\nTherefore, we use quantile estimators based on order statistics to estimate the\ndetector threshold. The estimation of quantiles from sample data has a more\nthan a century long tradition and we provide three different distribution-free\nfinite sample guarantees for a class of quantile estimators. The first is based\non the Dworetzky-Kiefer-Wolfowitz inequality, the second utilizes the\nVysochanskij-Petunin inequality, and the third is based on exact confidence\nintervals for a beta distribution. These guarantees are then compared and used\nin the detector threshold tuning problem. We use both simulated data as well as\ndata obtained from an experimental setup with the Temperature Control Lab to\nvalidate the guarantees provided.",
    "published_date": "2021-05-25T00:00:00",
    "year": 2021,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.12239v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.12195v3",
    "title": "Bias in Machine Learning Software: Why? How? What to do?",
    "authors": [
      "Joymallya Chakraborty",
      "Suvodeep Majumder",
      "Tim Menzies"
    ],
    "author_ids": [],
    "abstract": "Increasingly, software is making autonomous decisions in case of criminal\nsentencing, approving credit cards, hiring employees, and so on. Some of these\ndecisions show bias and adversely affect certain social groups (e.g. those\ndefined by sex, race, age, marital status). Many prior works on bias mitigation\ntake the following form: change the data or learners in multiple ways, then see\nif any of that improves fairness. Perhaps a better approach is to postulate\nroot causes of bias and then applying some resolution strategy. This paper\npostulates that the root causes of bias are the prior decisions that affect-\n(a) what data was selected and (b) the labels assigned to those examples. Our\nFair-SMOTE algorithm removes biased labels; and rebalances internal\ndistributions such that based on sensitive attribute, examples are equal in\nboth positive and negative classes. On testing, it was seen that this method\nwas just as effective at reducing bias as prior approaches. Further, models\ngenerated via Fair-SMOTE achieve higher performance (measured in terms of\nrecall and F1) than other state-of-the-art fairness improvement algorithms. To\nthe best of our knowledge, measured in terms of number of analyzed learners and\ndatasets, this study is one of the largest studies on bias mitigation yet\npresented in the literature.",
    "published_date": "2021-05-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.12195v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.11999v1",
    "title": "Throughput-Fairness Tradeoffs in Mobility Platforms",
    "authors": [
      "Arjun Balasingam",
      "Karthik Gopalakrishnan",
      "Radhika Mittal",
      "Venkat Arun",
      "Ahmed Saeed",
      "Mohammad Alizadeh",
      "Hamsa Balakrishnan",
      "Hari Balakrishnan"
    ],
    "author_ids": [],
    "abstract": "This paper studies the problem of allocating tasks from different customers\nto vehicles in mobility platforms, which are used for applications like food\nand package delivery, ridesharing, and mobile sensing. A mobility platform\nshould allocate tasks to vehicles and schedule them in order to optimize both\nthroughput and fairness across customers. However, existing approaches to\nscheduling tasks in mobility platforms ignore fairness.\n  We introduce Mobius, a system that uses guided optimization to achieve both\nhigh throughput and fairness across customers. Mobius supports spatiotemporally\ndiverse and dynamic customer demands. It provides a principled method to\nnavigate inherent tradeoffs between fairness and throughput caused by shared\nmobility. Our evaluation demonstrates these properties, along with the\nversatility and scalability of Mobius, using traces gathered from ridesharing\nand aerial sensing applications. Our ridesharing case study shows that Mobius\ncan schedule more than 16,000 tasks across 40 customers and 200 vehicles in an\nonline manner.",
    "published_date": "2021-05-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.MA",
      "cs.NI",
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.11999v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.11874v1",
    "title": "Few-Shot Learning with Part Discovery and Augmentation from Unlabeled Images",
    "authors": [
      "Wentao Chen",
      "Chenyang Si",
      "Wei Wang",
      "Liang Wang",
      "Zilei Wang",
      "Tieniu Tan"
    ],
    "author_ids": [],
    "abstract": "Few-shot learning is a challenging task since only few instances are given\nfor recognizing an unseen class. One way to alleviate this problem is to\nacquire a strong inductive bias via meta-learning on similar tasks. In this\npaper, we show that such inductive bias can be learned from a flat collection\nof unlabeled images, and instantiated as transferable representations among\nseen and unseen classes. Specifically, we propose a novel part-based\nself-supervised representation learning scheme to learn transferable\nrepresentations by maximizing the similarity of an image to its discriminative\npart. To mitigate the overfitting in few-shot classification caused by data\nscarcity, we further propose a part augmentation strategy by retrieving extra\nimages from a base dataset. We conduct systematic studies on miniImageNet and\ntieredImageNet benchmarks. Remarkably, our method yields impressive results,\noutperforming the previous best unsupervised methods by 7.74% and 9.24% under\n5-way 1-shot and 5-way 5-shot settings, which are comparable with\nstate-of-the-art supervised methods.",
    "published_date": "2021-05-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.11874v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.11828v1",
    "title": "Bridging the Gap Between Explainable AI and Uncertainty Quantification to Enhance Trustability",
    "authors": [
      "Dominik Seuß"
    ],
    "author_ids": [],
    "abstract": "After the tremendous advances of deep learning and other AI methods, more\nattention is flowing into other properties of modern approaches, such as\ninterpretability, fairness, etc. combined in frameworks like Responsible AI.\nTwo research directions, namely Explainable AI and Uncertainty Quantification\nare becoming more and more important, but have been so far never combined and\njointly explored. In this paper, I show how both research areas provide\npotential for combination, why more research should be done in this direction\nand how this would lead to an increase in trustability in AI systems.",
    "published_date": "2021-05-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.11828v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.11802v2",
    "title": "Bias-Robust Bayesian Optimization via Dueling Bandits",
    "authors": [
      "Johannes Kirschner",
      "Andreas Krause"
    ],
    "author_ids": [],
    "abstract": "We consider Bayesian optimization in settings where observations can be\nadversarially biased, for example by an uncontrolled hidden confounder. Our\nfirst contribution is a reduction of the confounded setting to the dueling\nbandit model. Then we propose a novel approach for dueling bandits based on\ninformation-directed sampling (IDS). Thereby, we obtain the first efficient\nkernelized algorithm for dueling bandits that comes with cumulative regret\nguarantees. Our analysis further generalizes a previously proposed\nsemi-parametric linear bandit model to non-linear reward functions, and\nuncovers interesting links to doubly-robust estimation.",
    "published_date": "2021-05-25T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.11802v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.11765v1",
    "title": "Deep learning-based bias transfer for overcoming laboratory differences of microscopic images",
    "authors": [
      "Ann-Katrin Thebille",
      "Esther Dietrich",
      "Martin Klaus",
      "Lukas Gernhold",
      "Maximilian Lennartz",
      "Christoph Kuppe",
      "Rafael Kramann",
      "Tobias B. Huber",
      "Guido Sauter",
      "Victor G. Puelles",
      "Marina Zimmermann",
      "Stefan Bonn"
    ],
    "author_ids": [],
    "abstract": "The automated analysis of medical images is currently limited by technical\nand biological noise and bias. The same source tissue can be represented by\nvastly different images if the image acquisition or processing protocols vary.\nFor an image analysis pipeline, it is crucial to compensate such biases to\navoid misinterpretations. Here, we evaluate, compare, and improve existing\ngenerative model architectures to overcome domain shifts for immunofluorescence\n(IF) and Hematoxylin and Eosin (H&E) stained microscopy images. To determine\nthe performance of the generative models, the original and transformed images\nwere segmented or classified by deep neural networks that were trained only on\nimages of the target bias. In the scope of our analysis, U-Net cycleGANs\ntrained with an additional identity and an MS-SSIM-based loss and Fixed-Point\nGANs trained with an additional structure loss led to the best results for the\nIF and H&E stained samples, respectively. Adapting the bias of the samples\nsignificantly improved the pixel-level segmentation for human kidney glomeruli\nand podocytes and improved the classification accuracy for human prostate\nbiopsies by up to 14%.",
    "published_date": "2021-05-25T00:00:00",
    "year": 2021,
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.11765v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.11570v1",
    "title": "Robust Fairness-aware Learning Under Sample Selection Bias",
    "authors": [
      "Wei Du",
      "Xintao Wu"
    ],
    "author_ids": [],
    "abstract": "The underlying assumption of many machine learning algorithms is that the\ntraining data and test data are drawn from the same distributions. However, the\nassumption is often violated in real world due to the sample selection bias\nbetween the training and test data. Previous research works focus on reweighing\nbiased training data to match the test data and then building classification\nmodels on the reweighed training data. However, how to achieve fairness in the\nbuilt classification models is under-explored. In this paper, we propose a\nframework for robust and fair learning under sample selection bias. Our\nframework adopts the reweighing estimation approach for bias correction and the\nminimax robust estimation approach for achieving robustness on prediction\naccuracy. Moreover, during the minimax optimization, the fairness is achieved\nunder the worst case, which guarantees the model's fairness on test data. We\nfurther develop two algorithms to handle sample selection bias when test data\nis both available and unavailable. We conduct experiments on two real-world\ndatasets and the experimental results demonstrate its effectiveness in terms of\nboth utility and fairness metrics.",
    "published_date": "2021-05-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.11570v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.11569v1",
    "title": "Dynamics of Public Opinion Evolution with Asymmetric Cognitive Bias",
    "authors": [
      "Yanbing Mao",
      "Naira Hovakimyan",
      "Tarek Abdelzaher"
    ],
    "author_ids": [],
    "abstract": "In this paper, we propose a pubic opinion model with incorporation of\nasymmetric cognitive bias: confirmation bias and negativity bias. We then\ninvestigate the generic modeling guidance of capturing asymmetric confirmation\nbias and negativity bias. A numerical examples is provided to demonstrate the\ncorrectness of asymmetric cognitive bias model.",
    "published_date": "2021-05-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.11569v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.11531v1",
    "title": "Informative Bayesian model selection for RR Lyrae star classifiers",
    "authors": [
      "F. Pérez-Galarce",
      "K. Pichara",
      "P. Huijse",
      "M. Catelan",
      "D. Mery"
    ],
    "author_ids": [],
    "abstract": "Machine learning has achieved an important role in the automatic\nclassification of variable stars, and several classifiers have been proposed\nover the last decade. These classifiers have achieved impressive performance in\nseveral astronomical catalogues. However, some scientific articles have also\nshown that the training data therein contain multiple sources of bias. Hence,\nthe performance of those classifiers on objects not belonging to the training\ndata is uncertain, potentially resulting in the selection of incorrect models.\nBesides, it gives rise to the deployment of misleading classifiers. An example\nof the latter is the creation of open-source labelled catalogues with biased\npredictions. In this paper, we develop a method based on an informative\nmarginal likelihood to evaluate variable star classifiers. We collect\ndeterministic rules that are based on physical descriptors of RR Lyrae stars,\nand then, to mitigate the biases, we introduce those rules into the marginal\nlikelihood estimation. We perform experiments with a set of Bayesian Logistic\nRegressions, which are trained to classify RR Lyraes, and we found that our\nmethod outperforms traditional non-informative cross-validation strategies,\neven when penalized models are assessed. Our methodology provides a more\nrigorous alternative to assess machine learning models using astronomical\nknowledge. From this approach, applications to other classes of variable stars\nand algorithmic improvements can be developed.",
    "published_date": "2021-05-24T00:00:00",
    "year": 2021,
    "categories": [
      "astro-ph.IM",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.11531v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.11519v3",
    "title": "The advent and fall of a vocabulary learning bias from communicative efficiency",
    "authors": [
      "David Carrera-Casado",
      "Ramon Ferrer-i-Cancho"
    ],
    "author_ids": [],
    "abstract": "Biosemiosis is a process of choice-making between simultaneously alternative\noptions. It is well-known that, when sufficiently young children encounter a\nnew word, they tend to interpret it as pointing to a meaning that does not have\na word yet in their lexicon rather than to a meaning that already has a word\nattached. In previous research, the strategy was shown to be optimal from an\ninformation theoretic standpoint. In that framework, interpretation is\nhypothesized to be driven by the minimization of a cost function: the option of\nleast communication cost is chosen. However, the information theoretic model\nemployed in that research neither explains the weakening of that vocabulary\nlearning bias in older children or polylinguals nor reproduces Zipf's\nmeaning-frequency law, namely the non-linear relationship between the number of\nmeanings of a word and its frequency. Here we consider a generalization of the\nmodel that is channeled to reproduce that law. The analysis of the new model\nreveals regions of the phase space where the bias disappears consistently with\nthe weakening or loss of the bias in older children or polylinguals. The model\nis abstract enough to support future research on other levels of life that are\nrelevant to biosemiotics. In the deep learning era, the model is a transparent\nlow-dimensional tool for future experimental research and illustrates the\npredictive power of a theoretical framework originally designed to shed light\non the origins of Zipf's rank-frequency law.",
    "published_date": "2021-05-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.IT",
      "math.IT",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.11519v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.11479v4",
    "title": "A Flawed Dataset for Symbolic Equation Verification",
    "authors": [
      "Ernest Davis"
    ],
    "author_ids": [],
    "abstract": "Arabshahi, Singh, and Anandkumar (2018) propose a method for creating a\ndataset of symbolic mathematical equations for the tasks of symbolic equation\nverification and equation completion. Unfortunately, a dataset constructed\nusing the method they propose will suffer from two serious flaws. First, the\nclass of true equations that the procedure can generate will be very limited.\nSecond, because true and false equations are generated in completely different\nways, there are likely to be artifactual features that allow easy\ndiscrimination.\n  Moreover, over the class of equations they consider, there is an extremely\nsimple probabilistic procedure that solves the problem of equation verification\nwith extremely high reliability. The usefulness of this problem in general as a\ntestbed for AI systems is therefore doubtful.",
    "published_date": "2021-05-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.SC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.11479v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.11348v1",
    "title": "PROPm Allocations of Indivisible Goods to Multiple Agents",
    "authors": [
      "Artem Baklanov",
      "Pranav Garimidi",
      "Vasilis Gkatzelis",
      "Daniel Schoepflin"
    ],
    "author_ids": [],
    "abstract": "We study the classic problem of fairly allocating a set of indivisible goods\namong a group of agents, and focus on the notion of approximate proportionality\nknown as PROPm. Prior work showed that there exists an allocation that\nsatisfies this notion of fairness for instances involving up to five agents,\nbut fell short of proving that this is true in general. We extend this result\nto show that a PROPm allocation is guaranteed to exist for all instances,\nindependent of the number of agents or goods. Our proof is constructive,\nproviding an algorithm that computes such an allocation and, unlike prior work,\nthe running time of this algorithm is polynomial in both the number of agents\nand the number of goods.",
    "published_date": "2021-05-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.11348v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.11313v2",
    "title": "Verification of Dissipativity and Evaluation of Storage Function in Economic Nonlinear MPC using Q-Learning",
    "authors": [
      "Arash Bahari Kordabad",
      "Sebastien Gros"
    ],
    "author_ids": [],
    "abstract": "In the Economic Nonlinear Model Predictive (ENMPC) context, closed-loop\nstability relates to the existence of a storage function satisfying a\ndissipation inequality. Finding the storage function is in general -- for\nnonlinear dynamics and cost -- challenging, and has attracted attentions\nrecently. Q-Learning is a well-known Reinforcement Learning (RL) techniques\nthat attempts to capture action-value functions based on the state-input\ntransitions and stage cost of the system. In this paper, we present the use of\nthe Q-Learning approach to obtain the storage function and verify the\ndissipativity for discrete-time systems subject to state-input constraints. We\nshow that undiscounted Q-learning is able to capture the storage function for\ndissipative problems when the parameterization is rich enough. The efficiency\nof the proposed method will be illustrated in the different case studies.",
    "published_date": "2021-05-24T00:00:00",
    "year": 2021,
    "categories": [
      "eess.SY",
      "cs.SY",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.11313v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.11160v3",
    "title": "Out-of-Distribution Detection in Dermatology using Input Perturbation and Subset Scanning",
    "authors": [
      "Hannah Kim",
      "Girmaw Abebe Tadesse",
      "Celia Cintas",
      "Skyler Speakman",
      "Kush Varshney"
    ],
    "author_ids": [],
    "abstract": "Recent advances in deep learning have led to breakthroughs in the development\nof automated skin disease classification. As we observe an increasing interest\nin these models in the dermatology space, it is crucial to address aspects such\nas the robustness towards input data distribution shifts. Current skin disease\nmodels could make incorrect inferences for test samples from different hardware\ndevices and clinical settings or unknown disease samples, which are\nout-of-distribution (OOD) from the training samples. To this end, we propose a\nsimple yet effective approach that detect these OOD samples prior to making any\ndecision. The detection is performed via scanning in the latent space\nrepresentation (e.g., activations of the inner layers of any pre-trained skin\ndisease classifier). The input samples could also perturbed to maximise\ndivergence of OOD samples. We validate our ODD detection approach in two use\ncases: 1) identify samples collected from different protocols, and 2) detect\nsamples from unknown disease classes. Additionally, we evaluate the performance\nof the proposed approach and compare it with other state-of-the-art methods.\nFurthermore, data-driven dermatology applications may deepen the disparity in\nclinical care across racial and ethnic groups since most datasets are reported\nto suffer from bias in skin tone distribution. Therefore, we also evaluate the\nfairness of these OOD detection methods across different skin tones. Our\nexperiments resulted in competitive performance across multiple datasets in\ndetecting OOD samples, which could be used (in the future) to design more\neffective transfer learning techniques prior to inferring on these samples.",
    "published_date": "2021-05-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.11160v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.11136v2",
    "title": "Using Adversarial Attacks to Reveal the Statistical Bias in Machine Reading Comprehension Models",
    "authors": [
      "Jieyu Lin",
      "Jiajie Zou",
      "Nai Ding"
    ],
    "author_ids": [],
    "abstract": "Pre-trained language models have achieved human-level performance on many\nMachine Reading Comprehension (MRC) tasks, but it remains unclear whether these\nmodels truly understand language or answer questions by exploiting statistical\nbiases in datasets. Here, we demonstrate a simple yet effective method to\nattack MRC models and reveal the statistical biases in these models. We apply\nthe method to the RACE dataset, for which the answer to each MRC question is\nselected from 4 options. It is found that several pre-trained language models,\nincluding BERT, ALBERT, and RoBERTa, show consistent preference to some\noptions, even when these options are irrelevant to the question. When\ninterfered by these irrelevant options, the performance of MRC models can be\nreduced from human-level performance to the chance-level performance. Human\nreaders, however, are not clearly affected by these irrelevant options.\nFinally, we propose an augmented training method that can greatly reduce\nmodels' statistical biases.",
    "published_date": "2021-05-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.11136v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.11113v1",
    "title": "Dynamic Class Queue for Large Scale Face Recognition In the Wild",
    "authors": [
      "Bi Li",
      "Teng Xi",
      "Gang Zhang",
      "Haocheng Feng",
      "Junyu Han",
      "Jingtuo Liu",
      "Errui Ding",
      "Wenyu Liu"
    ],
    "author_ids": [],
    "abstract": "Learning discriminative representation using large-scale face datasets in the\nwild is crucial for real-world applications, yet it remains challenging. The\ndifficulties lie in many aspects and this work focus on computing resource\nconstraint and long-tailed class distribution. Recently, classification-based\nrepresentation learning with deep neural networks and well-designed losses have\ndemonstrated good recognition performance. However, the computing and memory\ncost linearly scales up to the number of identities (classes) in the training\nset, and the learning process suffers from unbalanced classes. In this work, we\npropose a dynamic class queue (DCQ) to tackle these two problems. Specifically,\nfor each iteration during training, a subset of classes for recognition are\ndynamically selected and their class weights are dynamically generated\non-the-fly which are stored in a queue. Since only a subset of classes is\nselected for each iteration, the computing requirement is reduced. By using a\nsingle server without model parallel, we empirically verify in large-scale\ndatasets that 10% of classes are sufficient to achieve similar performance as\nusing all classes. Moreover, the class weights are dynamically generated in a\nfew-shot manner and therefore suitable for tail classes with only a few\ninstances. We show clear improvement over a strong baseline in the largest\npublic dataset Megaface Challenge2 (MF2) which has 672K identities and over 88%\nof them have less than 10 instances. Code is available at\nhttps://github.com/bilylee/DCQ",
    "published_date": "2021-05-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.11113v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.11069v2",
    "title": "InfoFair: Information-Theoretic Intersectional Fairness",
    "authors": [
      "Jian Kang",
      "Tiankai Xie",
      "Xintao Wu",
      "Ross Maciejewski",
      "Hanghang Tong"
    ],
    "author_ids": [],
    "abstract": "Algorithmic fairness is becoming increasingly important in data mining and\nmachine learning. Among others, a foundational notation is group fairness. The\nvast majority of the existing works on group fairness, with a few exceptions,\nprimarily focus on debiasing with respect to a single sensitive attribute,\ndespite the fact that the co-existence of multiple sensitive attributes (e.g.,\ngender, race, marital status, etc.) in the real-world is commonplace. As such,\nmethods that can ensure a fair learning outcome with respect to all sensitive\nattributes of concern simultaneously need to be developed. In this paper, we\nstudy the problem of information-theoretic intersectional fairness (InfoFair),\nwhere statistical parity, a representative group fairness measure, is\nguaranteed among demographic groups formed by multiple sensitive attributes of\ninterest. We formulate it as a mutual information minimization problem and\npropose a generic end-to-end algorithmic framework to solve it. The key idea is\nto leverage a variational representation of mutual information, which considers\nthe variational distribution between learning outcomes and sensitive\nattributes, as well as the density ratio between the variational and the\noriginal distributions. Our proposed framework is generalizable to many\ndifferent settings, including other statistical notions of fairness, and could\nhandle any type of learning task equipped with a gradient-based optimizer.\nEmpirical evaluations in the fair classification task on three real-world\ndatasets demonstrate that our proposed framework can effectively debias the\nclassification results with minimal impact to the classification accuracy.",
    "published_date": "2021-05-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.IT",
      "math.IT",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.11069v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.15121v1",
    "title": "Pregnancy loss and unethical algorithms: Ethical issues in targeted advertising",
    "authors": [
      "Fatemeh Golpayegani"
    ],
    "author_ids": [],
    "abstract": "In this paper, the ethical issues and the importance of ethical algorithm\ndesign for target ads were briefly discussed.",
    "published_date": "2021-05-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.15121v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.10990v1",
    "title": "Wisdom for the Crowd: Discoursive Power in Annotation Instructions for Computer Vision",
    "authors": [
      "Milagros Miceli",
      "Julian Posada"
    ],
    "author_ids": [],
    "abstract": "Developers of computer vision algorithms outsource some of the labor involved\nin annotating training data through business process outsourcing companies and\ncrowdsourcing platforms. Many data annotators are situated in the Global South\nand are considered independent contractors. This paper focuses on the\nexperiences of Argentinian and Venezuelan annotation workers. Through\nqualitative methods, we explore the discourses encoded in the task instructions\nthat these workers follow to annotate computer vision datasets. Our preliminary\nfindings indicate that annotation instructions reflect worldviews imposed on\nworkers and, through their labor, on datasets. Moreover, we observe that\nfor-profit goals drive task instructions and that managers and algorithms make\nsure annotations are done according to requesters' commands. This configuration\npresents a form of commodified labor that perpetuates power asymmetries while\nreinforcing social inequalities and is compelled to reproduce them into\ndatasets and, subsequently, in computer vision systems.",
    "published_date": "2021-05-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.10990v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.10915v1",
    "title": "GOALS: Gradient-Only Approximations for Line Searches Towards Robust and Consistent Training of Deep Neural Networks",
    "authors": [
      "Younghwan Chae",
      "Daniel N. Wilke",
      "Dominic Kafka"
    ],
    "author_ids": [],
    "abstract": "Mini-batch sub-sampling (MBSS) is favored in deep neural network training to\nreduce the computational cost. Still, it introduces an inherent sampling error,\nmaking the selection of appropriate learning rates challenging. The sampling\nerrors can manifest either as a bias or variances in a line search. Dynamic\nMBSS re-samples a mini-batch at every function evaluation. Hence, dynamic MBSS\nresults in point-wise discontinuous loss functions with smaller bias but larger\nvariance than static sampled loss functions. However, dynamic MBSS has the\nadvantage of having larger data throughput during training but requires the\ncomplexity regarding discontinuities to be resolved. This study extends the\ngradient-only surrogate (GOS), a line search method using quadratic\napproximation models built with only directional derivative information, for\ndynamic MBSS loss functions. We propose a gradient-only approximation line\nsearch (GOALS) with strong convergence characteristics with defined optimality\ncriterion. We investigate GOALS's performance by applying it on various\noptimizers that include SGD, RMSprop and Adam on ResNet-18 and EfficientNetB0.\nWe also compare GOALS's against the other existing learning rate methods. We\nquantify both the best performing and most robust algorithms. For the latter,\nwe introduce a relative robust criterion that allows us to quantify the\ndifference between an algorithm and the best performing algorithm for a given\nproblem. The results show that training a model with the recommended learning\nrate for a class of search directions helps to reduce the model errors in\nmultimodal cases.",
    "published_date": "2021-05-23T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG",
      "93E35, 65K05, 90C15, 49M05, 90C20, 90C26",
      "I.2.6; I.2.8"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.10915v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.10686v1",
    "title": "Towards Automatic Recognition of Pure & Mixed Stones using Intraoperative Endoscopic Digital Images",
    "authors": [
      "Vincent Estrade",
      "Michel Daudon",
      "Emmanuel Richard",
      "Jean-Christophe Bernhard",
      "Franck Bladou",
      "Gregoire Robert",
      "Baudouin Denis de Senneville"
    ],
    "author_ids": [],
    "abstract": "Objective: To assess automatic computer-aided in-situ recognition of\nmorphological features of pure and mixed urinary stones using intraoperative\ndigital endoscopic images acquired in a clinical setting. Materials and\nmethods: In this single-centre study, an experienced urologist intraoperatively\nand prospectively examined the surface and section of all kidney stones\nencountered. Calcium oxalate monohydrate (COM/Ia), dihydrate (COD/IIb) and uric\nacid (UA/IIIb) morphological criteria were collected and classified to generate\nannotated datasets. A deep convolutional neural network (CNN) was trained to\npredict the composition of both pure and mixed stones. To explain the\npredictions of the deep neural network model, coarse localisation heat-maps\nwere plotted to pinpoint key areas identified by the network. Results: This\nstudy included 347 and 236 observations of stone surface and stone section,\nrespectively. A highest sensitivity of 98 % was obtained for the type \"pure\nIIIb/UA\" using surface images. The most frequently encountered morphology was\nthat of the type \"pure Ia/COM\"; it was correctly predicted in 91 % and 94 % of\ncases using surface and section images, respectively. Of the mixed type\n\"Ia/COM+IIb/COD\", Ia/COM was predicted in 84 % of cases using surface images,\nIIb/COD in 70 % of cases, and both in 65 % of cases. Concerning mixed\nIa/COM+IIIb/UA stones, Ia/COM was predicted in 91 % of cases using section\nimages, IIIb/UA in 69 % of cases, and both in 74 % of cases. Conclusions: This\npreliminary study demonstrates that deep convolutional neural networks are\npromising to identify kidney stone composition from endoscopic images acquired\nintraoperatively. Both pure and mixed stone composition could be discriminated.\nCollected in a clinical setting, surface and section images analysed by deep\nCNN provide valuable information about stone morphology for computer-aided\ndiagnosis.",
    "published_date": "2021-05-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.10686v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.10648v1",
    "title": "Deconfounded Recommendation for Alleviating Bias Amplification",
    "authors": [
      "Wenjie Wang",
      "Fuli Feng",
      "Xiangnan He",
      "Xiang Wang",
      "Tat-Seng Chua"
    ],
    "author_ids": [],
    "abstract": "Recommender systems usually amplify the biases in the data. The model learned\nfrom historical interactions with imbalanced item distribution will amplify the\nimbalance by over-recommending items from the major groups. Addressing this\nissue is essential for a healthy ecosystem of recommendation in the long run.\nExisting works apply bias control to the ranking targets (e.g., calibration,\nfairness, and diversity), but ignore the true reason for bias amplification and\ntrade-off the recommendation accuracy.\n  In this work, we scrutinize the cause-effect factors for bias amplification,\nidentifying the main reason lies in the confounder effect of imbalanced item\ndistribution on user representation and prediction score. The existence of such\nconfounder pushes us to go beyond merely modeling the conditional probability\nand embrace the causal modeling for recommendation. Towards this end, we\npropose a Deconfounded Recommender System (DecRS), which models the causal\neffect of user representation on the prediction score. The key to eliminating\nthe impact of the confounder lies in backdoor adjustment, which is however\ndifficult to do due to the infinite sample space of the confounder. For this\nchallenge, we contribute an approximation operator for backdoor adjustment\nwhich can be easily plugged into most recommender models. Lastly, we devise an\ninference strategy to dynamically regulate backdoor adjustment according to\nuser status. We instantiate DecRS on two representative models FM and NFM, and\nconduct extensive experiments over two benchmarks to validate the superiority\nof our proposed DecRS.",
    "published_date": "2021-05-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.10648v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.10644v1",
    "title": "Semi-Supervised Few-Shot Classification with Deep Invertible Hybrid Models",
    "authors": [
      "Yusuke Ohtsubo",
      "Tetsu Matsukawa",
      "Einoshin Suzuki"
    ],
    "author_ids": [],
    "abstract": "In this paper, we propose a deep invertible hybrid model which integrates\ndiscriminative and generative learning at a latent space level for\nsemi-supervised few-shot classification. Various tasks for classifying new\nspecies from image data can be modeled as a semi-supervised few-shot\nclassification, which assumes a labeled and unlabeled training examples and a\nsmall support set of the target classes. Predicting target classes with a few\nsupport examples per class makes the learning task difficult for existing\nsemi-supervised classification methods, including selftraining, which\niteratively estimates class labels of unlabeled training examples to learn a\nclassifier for the training classes. To exploit unlabeled training examples\neffectively, we adopt as the objective function the composite likelihood, which\nintegrates discriminative and generative learning and suits better with deep\nneural networks than the parameter coupling prior, the other popular integrated\nlearning approach. In our proposed model, the discriminative and generative\nmodels are respectively Prototypical Networks, which have shown excellent\nperformance in various kinds of few-shot learning, and Normalizing Flow a deep\ninvertible model which returns the exact marginal likelihood unlike the other\nthree major methods, i.e., VAE, GAN, and autoregressive model. Our main\noriginality lies in our integration of these components at a latent space\nlevel, which is effective in preventing overfitting. Experiments using\nmini-ImageNet and VGG-Face datasets show that our method outperforms\nselftraining based Prototypical Networks.",
    "published_date": "2021-05-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.10644v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.10639v1",
    "title": "Simultaneous Distributed Estimation and Attack Detection/Isolation in Social Networks: Structural Observability, Kronecker-Product Network, and Chi-Square Detector",
    "authors": [
      "Mohammadreza Doostmohammadian",
      "Themistoklis Charalambous",
      "Miadreza Shafie-khah",
      "Nader Meskin",
      "Usman A. Khan"
    ],
    "author_ids": [],
    "abstract": "This paper considers distributed estimation of linear systems when the state\nobservations are corrupted with Gaussian noise of unbounded support and under\npossible random adversarial attacks. We consider sensors equipped with single\ntime-scale estimators and local chi-square ($\\chi^2$) detectors to\nsimultaneously opserve the states, share information, fuse the\nnoise/attack-corrupted data locally, and detect possible anomalies in their own\nobservations. While this scheme is applicable to a wide variety of systems\nassociated with full-rank (invertible) matrices, we discuss it within the\ncontext of distributed inference in social networks. The proposed technique\noutperforms existing results in the sense that: (i) we consider Gaussian noise\nwith no simplifying upper-bound assumption on the support; (ii) all existing\n$\\chi^2$-based techniques are centralized while our proposed technique is\ndistributed, where the sensors \\textit{locally} detect attacks, with no central\ncoordinator, using specific probabilistic thresholds; and (iii) no\nlocal-observability assumption at a sensor is made, which makes our method\nfeasible for large-scale social networks. Moreover, we consider a Linear Matrix\nInequalities (LMI) approach to design block-diagonal gain (estimator) matrices\nunder appropriate constraints for isolating the attacks.",
    "published_date": "2021-05-22T00:00:00",
    "year": 2021,
    "categories": [
      "eess.SY",
      "cs.MA",
      "cs.SI",
      "cs.SY",
      "math.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.10639v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.10503v1",
    "title": "Enhanced Fairness and Scalability of Power Control Schemes in Multi-Cell Massive MIMO",
    "authors": [
      "Amin Ghazanfari",
      "Hei Victor Cheng",
      "Emil Björnson",
      "Erik G. Larsson"
    ],
    "author_ids": [],
    "abstract": "This paper studies the transmit power optimization in multi-cell massive\nmultiple-input multiple-output (MIMO) systems. Network-wide max-min fairness\n(NW-MMF) and network-wide proportional fairness (NW-PF) are two well-known\npower control schemes in the literature. The NW-MMF focus on maximizing the\nfairness among users at the cost of penalizing users with good channel\nconditions. On the other hand, the NW-PF focuses on maximizing the sum SE,\nthereby ignoring fairness, but gives some extra attention to the weakest users.\nHowever, both of these schemes suffer from a scalability issue which means that\nfor large networks, it is highly probable that one user has a very poor channel\ncondition, pushing the spectral efficiency (SE) of all users towards zero. To\novercome the scalability issue of NW-MMF and NW-PF, we propose a novel power\ncontrol scheme that is provably scalable. This scheme maximizes the geometric\nmean (GM) of the per-cell max-min SE. To solve this new optimization problem,\nwe prove that it can be rewritten in a convex optimization form and then solved\nusing standard tools. The simulation results highlight the benefits of our\nmodel which is balancing between NW-PF and NW-MMF.",
    "published_date": "2021-05-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.10503v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.10205v1",
    "title": "Rational Dynamic Price Model for Demand Response Programs in Modern Distribution Systems",
    "authors": [
      "Rayees A. Thokar",
      "Nikhil Gupta",
      "K. R. Niazi",
      "Anil Swarnkar",
      "Nand K. Meena"
    ],
    "author_ids": [],
    "abstract": "Demand response (DR) refers to change in electricity consumption pattern of\ncustomers during on-peak hours in lieu of financial gains to reduce stress on\ndistribution systems. Existing dynamic price models have not provided adequate\nsuccess to price-based demand response (PBDR) programs. It happened as these\nmodels have raised typical socio-economic problems pertaining to cross-subsidy,\nfree-riders, social inequity, assured profit of utilities, financial gains and\ncomfort of customers, etc. This paper presents a new dynamic price model for\nPBDR in distribution systems which aims to overcome some of the above mentioned\nproblems of the existing price models. The main aim of the developed price\nmodel is to overcome the problems of cross-subsidy and free-riders of the\nexisting price models for widespread acceptance, deployment and efficient\nutilization of PBDR programs in contemporary distribution systems. Proposed\nprice model generates demand-linked price signal that imposes different price\nsignals to different customers during on-peak hours and remains static\notherwise. This makes proposed model a class apart from other existing models.\nThe novelty of the proposed model lies in the fact that the financial benefits\nand penalties pertaining to DR are self-adjusted among customers while\npreserving social equity and profit of the utility. Such an ideology has not\nbeen yet addressed in the literature. Detailed investigation of application\nresults on a standard test bench reveals that the proposed model equally cares\nregarding the interests of both customers and utility. For economic assessment,\na comparison of the proposed price model with the existing pricing models is\nalso performed.",
    "published_date": "2021-05-21T00:00:00",
    "year": 2021,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.10205v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.10196v1",
    "title": "Multimodal Remote Sensing Benchmark Datasets for Land Cover Classification with A Shared and Specific Feature Learning Model",
    "authors": [
      "Danfeng Hong",
      "Jingliang Hu",
      "Jing Yao",
      "Jocelyn Chanussot",
      "Xiao Xiang Zhu"
    ],
    "author_ids": [],
    "abstract": "As remote sensing (RS) data obtained from different sensors become available\nlargely and openly, multimodal data processing and analysis techniques have\nbeen garnering increasing interest in the RS and geoscience community. However,\ndue to the gap between different modalities in terms of imaging sensors,\nresolutions, and contents, embedding their complementary information into a\nconsistent, compact, accurate, and discriminative representation, to a great\nextent, remains challenging. To this end, we propose a shared and specific\nfeature learning (S2FL) model. S2FL is capable of decomposing multimodal RS\ndata into modality-shared and modality-specific components, enabling the\ninformation blending of multi-modalities more effectively, particularly for\nheterogeneous data sources. Moreover, to better assess multimodal baselines and\nthe newly-proposed S2FL model, three multimodal RS benchmark datasets, i.e.,\nHouston2013 -- hyperspectral and multispectral data, Berlin -- hyperspectral\nand synthetic aperture radar (SAR) data, Augsburg -- hyperspectral, SAR, and\ndigital surface model (DSM) data, are released and used for land cover\nclassification. Extensive experiments conducted on the three datasets\ndemonstrate the superiority and advancement of our S2FL model in the task of\nland cover classification in comparison with previously-proposed\nstate-of-the-art baselines. Furthermore, the baseline codes and datasets used\nin this paper will be made available freely at\nhttps://github.com/danfenghong/ISPRS_S2FL.",
    "published_date": "2021-05-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.10196v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.10123v3",
    "title": "Backdoor Attacks on Self-Supervised Learning",
    "authors": [
      "Aniruddha Saha",
      "Ajinkya Tejankar",
      "Soroush Abbasi Koohpayegani",
      "Hamed Pirsiavash"
    ],
    "author_ids": [],
    "abstract": "Large-scale unlabeled data has spurred recent progress in self-supervised\nlearning methods that learn rich visual representations. State-of-the-art\nself-supervised methods for learning representations from images (e.g., MoCo,\nBYOL, MSF) use an inductive bias that random augmentations (e.g., random crops)\nof an image should produce similar embeddings. We show that such methods are\nvulnerable to backdoor attacks - where an attacker poisons a small part of the\nunlabeled data by adding a trigger (image patch chosen by the attacker) to the\nimages. The model performance is good on clean test images, but the attacker\ncan manipulate the decision of the model by showing the trigger at test time.\nBackdoor attacks have been studied extensively in supervised learning and to\nthe best of our knowledge, we are the first to study them for self-supervised\nlearning. Backdoor attacks are more practical in self-supervised learning,\nsince the use of large unlabeled data makes data inspection to remove poisons\nprohibitive. We show that in our targeted attack, the attacker can produce many\nfalse positives for the target category by using the trigger at test time. We\nalso propose a defense method based on knowledge distillation that succeeds in\nneutralizing the attack. Our code is available here:\nhttps://github.com/UMBCvision/SSL-Backdoor .",
    "published_date": "2021-05-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.10123v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.10101v2",
    "title": "Anomaly Detection of Adversarial Examples using Class-conditional Generative Adversarial Networks",
    "authors": [
      "Hang Wang",
      "David J. Miller",
      "George Kesidis"
    ],
    "author_ids": [],
    "abstract": "Deep Neural Networks (DNNs) have been shown vulnerable to Test-Time Evasion\nattacks (TTEs, or adversarial examples), which, by making small changes to the\ninput, alter the DNN's decision. We propose an unsupervised attack detector on\nDNN classifiers based on class-conditional Generative Adversarial Networks\n(GANs). We model the distribution of clean data conditioned on the predicted\nclass label by an Auxiliary Classifier GAN (AC-GAN). Given a test sample and\nits predicted class, three detection statistics are calculated based on the\nAC-GAN Generator and Discriminator. Experiments on image classification\ndatasets under various TTE attacks show that our method outperforms previous\ndetection methods. We also investigate the effectiveness of anomaly detection\nusing different DNN layers (input features or internal-layer features) and\ndemonstrate, as one might expect, that anomalies are harder to detect using\nfeatures closer to the DNN's output layer.",
    "published_date": "2021-05-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.10101v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.10064v2",
    "title": "Fair and Efficient Resource Allocation with Partial Information",
    "authors": [
      "Daniel Halpern",
      "Nisarg Shah"
    ],
    "author_ids": [],
    "abstract": "We study the fundamental problem of allocating indivisible goods to agents\nwith additive preferences. We consider eliciting from each agent only a ranking\nof her $k$ most preferred goods instead of her full cardinal valuations. We\ncharacterize the value of $k$ needed to achieve envy-freeness up to one good\nand approximate maximin share guarantee, two widely studied fairness notions.\nWe also analyze the multiplicative loss in social welfare incurred due to the\nlack of full information with and without the fairness requirements.",
    "published_date": "2021-05-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.10064v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.09989v1",
    "title": "Multi-group Agnostic PAC Learnability",
    "authors": [
      "Guy N Rothblum",
      "Gal Yona"
    ],
    "author_ids": [],
    "abstract": "An agnostic PAC learning algorithm finds a predictor that is competitive with\nthe best predictor in a benchmark hypothesis class, where competitiveness is\nmeasured with respect to a given loss function. However, its predictions might\nbe quite sub-optimal for structured subgroups of individuals, such as protected\ndemographic groups. Motivated by such fairness concerns, we study \"multi-group\nagnostic PAC learnability\": fixing a measure of loss, a benchmark class $\\H$\nand a (potentially) rich collection of subgroups $\\G$, the objective is to\nlearn a single predictor such that the loss experienced by every group $g \\in\n\\G$ is not much larger than the best possible loss for this group within $\\H$.\nUnder natural conditions, we provide a characterization of the loss functions\nfor which such a predictor is guaranteed to exist. For any such loss function\nwe construct a learning algorithm whose sample complexity is logarithmic in the\nsize of the collection $\\G$. Our results unify and extend previous positive and\nnegative results from the multi-group fairness literature, which applied for\nspecific loss functions.",
    "published_date": "2021-05-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.09989v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.09985v1",
    "title": "Measuring Model Fairness under Noisy Covariates: A Theoretical Perspective",
    "authors": [
      "Flavien Prost",
      "Pranjal Awasthi",
      "Nick Blumm",
      "Aditee Kumthekar",
      "Trevor Potter",
      "Li Wei",
      "Xuezhi Wang",
      "Ed H. Chi",
      "Jilin Chen",
      "Alex Beutel"
    ],
    "author_ids": [],
    "abstract": "In this work we study the problem of measuring the fairness of a machine\nlearning model under noisy information. Focusing on group fairness metrics, we\ninvestigate the particular but common situation when the evaluation requires\ncontrolling for the confounding effect of covariate variables. In a practical\nsetting, we might not be able to jointly observe the covariate and group\ninformation, and a standard workaround is to then use proxies for one or more\nof these variables. Prior works have demonstrated the challenges with using a\nproxy for sensitive attributes, and strong independence assumptions are needed\nto provide guarantees on the accuracy of the noisy estimates. In contrast, in\nthis work we study using a proxy for the covariate variable and present a\ntheoretical analysis that aims to characterize weaker conditions under which\naccurate fairness evaluation is possible.\n  Furthermore, our theory identifies potential sources of errors and decouples\nthem into two interpretable parts $\\gamma$ and $\\epsilon$. The first part\n$\\gamma$ depends solely on the performance of the proxy such as precision and\nrecall, whereas the second part $\\epsilon$ captures correlations between all\nthe variables of interest. We show that in many scenarios the error in the\nestimates is dominated by $\\gamma$ via a linear dependence, whereas the\ndependence on the correlations $\\epsilon$ only constitutes a lower order term.\nAs a result we expand the understanding of scenarios where measuring model\nfairness via proxies can be an effective approach. Finally, we compare, via\nsimulations, the theoretical upper-bounds to the distribution of simulated\nestimation errors and show that assuming some structure on the data, even weak,\nis key to significantly improve both theoretical guarantees and empirical\nresults.",
    "published_date": "2021-05-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.09985v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.09966v1",
    "title": "On planetary systems as ordered sequences",
    "authors": [
      "Emily Sandford",
      "David Kipping",
      "Michael Collins"
    ],
    "author_ids": [],
    "abstract": "A planetary system consists of a host star and one or more planets, arranged\ninto a particular configuration. Here, we consider what information belongs to\nthe configuration, or ordering, of 4286 Kepler planets in their 3277 planetary\nsystems. First, we train a neural network model to predict the radius and\nperiod of a planet based on the properties of its host star and the radii and\nperiod of its neighbors. The mean absolute error of the predictions of the\ntrained model is a factor of 2.1 better than the MAE of the predictions of a\nnaive model which draws randomly from dynamically allowable periods and radii.\nSecond, we adapt a model used for unsupervised part-of-speech tagging in\ncomputational linguistics to investigate whether planets or planetary systems\nfall into natural categories with physically interpretable \"grammatical rules.\"\nThe model identifies two robust groups of planetary systems: (1) compact\nmulti-planet systems and (2) systems around giant stars ($\\log{g} \\lesssim\n4.0$), although the latter group is strongly sculpted by the selection bias of\nthe transit method. These results reinforce the idea that planetary systems are\nnot random sequences -- instead, as a population, they contain predictable\npatterns that can provide insight into the formation and evolution of planetary\nsystems.",
    "published_date": "2021-05-20T00:00:00",
    "year": 2021,
    "categories": [
      "astro-ph.EP",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.09966v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.09934v2",
    "title": "Probing the Effect of Selection Bias on Generalization: A Thought Experiment",
    "authors": [
      "John K. Tsotsos",
      "Jun Luo"
    ],
    "author_ids": [],
    "abstract": "Learned systems in the domain of visual recognition and cognition impress in\npart because even though they are trained with datasets many orders of\nmagnitude smaller than the full population of possible images, they exhibit\nsufficient generalization to be applicable to new and previously unseen data.\nSince training data sets typically represent small sampling of a domain, the\npossibility of bias in their composition is very real. But what are the limits\nof generalization given such bias, and up to what point might it be sufficient\nfor a real problem task? Although many have examined issues regarding\ngeneralization, this question may require examining the data itself. Here, we\nfocus on the characteristics of the training data that may play a role. Other\ndisciplines have grappled with these problems, most interestingly epidemiology,\nwhere experimental bias is a critical concern. The range and nature of data\nbiases seen clinically are really quite relatable to learned vision systems.\nOne obvious way to deal with bias is to ensure a large enough training set, but\nthis might be infeasible for many domains. Another approach might be to perform\na statistical analysis of the actual training set, to determine if all aspects\nof the domain are fairly captured. This too is difficult, in part because the\nfull set of variables might not be known, or perhaps not even knowable. Here,\nwe try a different approach in the tradition of the Thought Experiment, whose\nmost famous instance may be Schr\\\"odinger's Cat. There are many types of bias\nas will be seen, but we focus only on one, selection bias. The point of the\nthought experiment is not to demonstrate problems with all learned systems.\nRather, this might be a simple theoretical tool to probe into bias during data\ncollection to highlight deficiencies that might then deserve extra attention\neither in data collection or system development.",
    "published_date": "2021-05-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.2.10; I.4.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.09934v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.09926v2",
    "title": "Diversity, Fairness, and Sustainability in Population Protocols",
    "authors": [
      "Nan Kang",
      "Frederik Mallmann-Trenn",
      "Nicolás Rivera"
    ],
    "author_ids": [],
    "abstract": "Over the years, population protocols with the goal of reaching consensus have\nbeen studied in great depth. However, many systems in the real-world do not\nresult in all agents eventually reaching consensus, but rather in the opposite:\nthey converge to a state of rich diversity. Consider for example task\nallocation in ants. If eventually all ants perform the same task, then the\ncolony will perish (lack of food, no brood care, etc.). Then, it is vital for\nthe survival of the colony to have a diverse set of tasks and enough ants\nworking on each task. What complicates matters is that ants need to switch\ntasks periodically to adjust the needs of the colony; e.g., when too many\nforagers fell victim to other ant colonies. Moreover, all tasks are equally\nimportant and maybe they need to keep certain proportions in the distribution\nof the task. How can ants keep a healthy and balanced allocation of tasks?\n  To answer this question, we propose a simple population protocol for $n$\nagents on a complete graph and an arbitrary initial distribution of $k$ colours\n(tasks). We assume that each colour $i$ has an associated weight (importance)\n$w_i \\geq 1$. By denoting $w$ as the sum of the weights of different colours,\nwe show that the protocol converges in $O(w^2 n \\log n)$ rounds to a\nconfiguration where the number of agents supporting each colour $i$ is\nconcentrated on the fair share $w_in/w$ and will stay concentrated for a large\nnumber of rounds, w.h.p.\n  Our protocol has many interesting properties: agents do not need to know\nother colours and weights in the system, and our protocol requires very little\nmemory per agent. Furthermore, the protocol guarantees fairness meaning that\nover a long period each agent has each colour roughly a number of times\nproportional to the weight of the colour. Finally, our protocol also fulfils\nsustainability meaning that no colour ever vanishes.",
    "published_date": "2021-05-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.09926v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.09829v3",
    "title": "Personalized Counterfactual Fairness in Recommendation",
    "authors": [
      "Yunqi Li",
      "Hanxiong Chen",
      "Shuyuan Xu",
      "Yingqiang Ge",
      "Yongfeng Zhang"
    ],
    "author_ids": [],
    "abstract": "Recommender systems are gaining increasing and critical impacts on human and\nsociety since a growing number of users use them for information seeking and\ndecision making. Therefore, it is crucial to address the potential unfairness\nproblems in recommendations. Just like users have personalized preferences on\nitems, users' demands for fairness are also personalized in many scenarios.\nTherefore, it is important to provide personalized fair recommendations for\nusers to satisfy their personalized fairness demands. Besides, previous works\non fair recommendation mainly focus on association-based fairness. However, it\nis important to advance from associative fairness notions to causal fairness\nnotions for assessing fairness more properly in recommender systems. Based on\nthe above considerations, this paper focuses on achieving personalized\ncounterfactual fairness for users in recommender systems. To this end, we\nintroduce a framework for achieving counterfactually fair recommendations\nthrough adversary learning by generating feature-independent user embeddings\nfor recommendation. The framework allows recommender systems to achieve\npersonalized fairness for users while also covering non-personalized\nsituations. Experiments on two real-world datasets with shallow and deep\nrecommendation algorithms show that our method can generate fairer\nrecommendations for users with a desirable recommendation performance.",
    "published_date": "2021-05-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.09829v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.11910v1",
    "title": "MBIC -- A Media Bias Annotation Dataset Including Annotator Characteristics",
    "authors": [
      "T. Spinde",
      "L. Rudnitckaia",
      "K. Sinha",
      "F. Hamborg",
      "B. Gipp",
      "K. Donnay"
    ],
    "author_ids": [],
    "abstract": "Many people consider news articles to be a reliable source of information on\ncurrent events. However, due to the range of factors influencing news agencies,\nsuch coverage may not always be impartial. Media bias, or slanted news\ncoverage, can have a substantial impact on public perception of events, and,\naccordingly, can potentially alter the beliefs and views of the public. The\nmain data gap in current research on media bias detection is a robust,\nrepresentative, and diverse dataset containing annotations of biased words and\nsentences. In particular, existing datasets do not control for the individual\nbackground of annotators, which may affect their assessment and, thus,\nrepresents critical information for contextualizing their annotations. In this\nposter, we present a matrix-based methodology to crowdsource such data using a\nself-developed annotation platform. We also present MBIC (Media Bias Including\nCharacteristics) - the first sample of 1,700 statements representing various\nmedia bias instances. The statements were reviewed by ten annotators each and\ncontain labels for media bias identification both on the word and sentence\nlevel. MBIC is the first available dataset about media bias reporting detailed\ninformation on annotator characteristics and their individual background. The\ncurrent dataset already significantly extends existing data in this domain\nproviding unique and more reliable insights into the perception of bias. In\nfuture, we will further extend it both with respect to the number of articles\nand annotators per article.",
    "published_date": "2021-05-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.11910v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.09720v2",
    "title": "Covid-19 Detection from Chest X-ray and Patient Metadata using Graph Convolutional Neural Networks",
    "authors": [
      "Thosini Bamunu Mudiyanselage",
      "Nipuna Senanayake",
      "Chunyan Ji",
      "Yi Pan",
      "Yanqing Zhang"
    ],
    "author_ids": [],
    "abstract": "The novel corona virus (Covid-19) has introduced significant challenges due\nto its rapid spreading nature through respiratory transmission. As a result,\nthere is a huge demand for Artificial Intelligence (AI) based quick disease\ndiagnosis methods as an alternative to high demand tests such as Polymerase\nChain Reaction (PCR). Chest X-ray (CXR) Image analysis is such cost-effective\nradiography technique due to resource availability and quick screening. But, a\nsufficient and systematic data collection that is required by complex deep\nleaning (DL) models is more difficult and hence there are recent efforts that\nutilize transfer learning to address this issue. Still these transfer learnt\nmodels suffer from lack of generalization and increased bias to the training\ndataset resulting poor performance for unseen data. Limited correlation of the\ntransferred features from the pre-trained model to a specific medical imaging\ndomain like X-ray and overfitting on fewer data can be reasons for this\ncircumstance. In this work, we propose a novel Graph Convolution Neural Network\n(GCN) that is capable of identifying bio-markers of Covid-19 pneumonia from CXR\nimages and meta information about patients. The proposed method exploits\nimportant relational knowledge between data instances and their features using\ngraph representation and applies convolution to learn the graph data which is\nnot possible with conventional convolution on Euclidean domain. The results of\nextensive experiments of proposed model on binary (Covid vs normal) and three\nclass (Covid, normal, other pneumonia) classification problems outperform\ndifferent benchmark transfer learnt models, hence overcoming the aforementioned\ndrawbacks.",
    "published_date": "2021-05-20T00:00:00",
    "year": 2021,
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.09720v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.09705v1",
    "title": "Low-complexity Multicast Beamforming for Multi-stream Multi-group Communications",
    "authors": [
      "Hamidreza Bakhshzad Mahmoodi",
      "Bikshapathi Gouda",
      "MohammadJavad Salehi",
      "Antti Tolli"
    ],
    "author_ids": [],
    "abstract": "In this paper, assuming multi-antenna transmitter and receivers, we consider\nmulticast beamformer design for the weighted max-min-fairness (WMMF) problem in\na multi-stream multi-group communication setup. Unlike the single-stream\nscenario, the WMMF objective in this setup is not equivalent to maximizing the\nminimum weighted SINR due to the summation over the rates of multiple streams.\nTherefore, the non-convex problem at hand is first approximated with a convex\none and then solved using Karush-Kuhn-Tucker (KKT) conditions. Then, a\npractically appealing closed-form solution is derived, as a function of dual\nvariables, for both transmit and receive beamformers. Finally, we use an\niterative solution based on the sub-gradient method to solve for the mutually\ncoupled and interdependent dual variables. The proposed solution does not rely\non generic solvers and does not require any bisection loop for finding the\nachievable rate of various streams. As a result, it significantly outperforms\nthe state-of-art in terms of computational cost and convergence speed.",
    "published_date": "2021-05-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.09705v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.09672v1",
    "title": "Newsalyze: Enabling News Consumers to Understand Media Bias",
    "authors": [
      "Felix Hamborg",
      "Anastasia Zhukova",
      "Karsten Donnay",
      "Bela Gipp"
    ],
    "author_ids": [],
    "abstract": "News is a central source of information for individuals to inform themselves\non current topics. Knowing a news article's slant and authenticity is of\ncrucial importance in times of \"fake news,\" news bots, and centralization of\nmedia ownership. We introduce Newsalyze, a bias-aware news reader focusing on a\nsubtle, yet powerful form of media bias, named bias by word choice and labeling\n(WCL). WCL bias can alter the assessment of entities reported in the news,\ne.g., \"freedom fighters\" vs. \"terrorists.\" At the core of the analysis is a\nneural model that uses a news-adapted BERT language model to determine\ntarget-dependent sentiment, a high-level effect of WCL bias. While the analysis\ncurrently focuses on only this form of bias, the visualizations already reveal\npatterns of bias when contrasting articles (overview) and in-text instances of\nbias (article view).",
    "published_date": "2021-05-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.09672v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.09640v1",
    "title": "Enabling News Consumers to View and Understand Biased News Coverage: A Study on the Perception and Visualization of Media Bias",
    "authors": [
      "Timo Spinde",
      "Felix Hamborg",
      "Karsten Donnay",
      "Angelica Becerra",
      "Bela Gipp"
    ],
    "author_ids": [],
    "abstract": "Traditional media outlets are known to report political news in a biased way,\npotentially affecting the political beliefs of the audience and even altering\ntheir voting behaviors. Many researchers focus on automatically detecting and\nidentifying media bias in the news, but only very few studies exist that\nsystematically analyze how theses biases can be best visualized and\ncommunicated. We create three manually annotated datasets and test varying\nvisualization strategies. The results show no strong effects of becoming aware\nof the bias of the treatment groups compared to the control group, although a\nvisualization of hand-annotated bias communicated bias instances more\neffectively than a framing visualization. Showing participants an overview\npage, which opposes different viewpoints on the same topic, does not yield\ndifferences in respondents' bias perception. Using a multilevel model, we find\nthat perceived journalist bias is significantly related to perceived political\nextremeness and impartiality of the article.",
    "published_date": "2021-05-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.09640v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.10356v1",
    "title": "AI Certification: Advancing Ethical Practice by Reducing Information Asymmetries",
    "authors": [
      "Peter Cihon",
      "Moritz J. Kleinaltenkamp",
      "Jonas Schuett",
      "Seth D. Baum"
    ],
    "author_ids": [],
    "abstract": "As artificial intelligence (AI) systems are increasingly deployed, principles\nfor ethical AI are also proliferating. Certification offers a method to both\nincentivize adoption of these principles and substantiate that they have been\nimplemented in practice. This paper draws from management literature on\ncertification and reviews current AI certification programs and proposals.\nSuccessful programs rely on both emerging technical methods and specific design\nconsiderations. In order to avoid two common failures of certification, program\ndesigns should ensure that the symbol of the certification is substantially\nimplemented in practice and that the program achieves its stated goals. The\nreview indicates that the field currently focuses on self-certification and\nthird-party certification of systems, individuals, and organizations - to the\nexclusion of process management certifications. Additionally, the paper\nconsiders prospects for future AI certification programs. Ongoing changes in AI\ntechnology suggest that AI certification regimes should be designed to\nemphasize governance criteria of enduring value, such as ethics training for AI\ndevelopers, and to adjust technical criteria as the technology changes.\nOverall, certification can play a valuable mix in the portfolio of AI\ngovernance tools.",
    "published_date": "2021-05-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.10356v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.09567v1",
    "title": "Unified Dual-view Cognitive Model for Interpretable Claim Verification",
    "authors": [
      "Lianwei Wu",
      "Yuan Rao",
      "Yuqian Lan",
      "Ling Sun",
      "Zhaoyin Qi"
    ],
    "author_ids": [],
    "abstract": "Recent studies constructing direct interactions between the claim and each\nsingle user response (a comment or a relevant article) to capture evidence have\nshown remarkable success in interpretable claim verification. Owing to\ndifferent single responses convey different cognition of individual users\n(i.e., audiences), the captured evidence belongs to the perspective of\nindividual cognition. However, individuals' cognition of social things is not\nalways able to truly reflect the objective. There may be one-sided or biased\nsemantics in their opinions on a claim. The captured evidence correspondingly\ncontains some unobjective and biased evidence fragments, deteriorating task\nperformance. In this paper, we propose a Dual-view model based on the views of\nCollective and Individual Cognition (CICD) for interpretable claim\nverification. From the view of the collective cognition, we not only capture\nthe word-level semantics based on individual users, but also focus on\nsentence-level semantics (i.e., the overall responses) among all users and\nadjust the proportion between them to generate global evidence. From the view\nof individual cognition, we select the top-$k$ articles with high degree of\ndifference and interact with the claim to explore the local key evidence\nfragments. To weaken the bias of individual cognition-view evidence, we devise\ninconsistent loss to suppress the divergence between global and local evidence\nfor strengthening the consistent shared evidence between the both. Experiments\non three benchmark datasets confirm that CICD achieves state-of-the-art\nperformance.",
    "published_date": "2021-05-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.09567v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.09522v1",
    "title": "Matchings with Group Fairness Constraints: Online and Offline Algorithms",
    "authors": [
      "Govind S. Sankar",
      "Anand Louis",
      "Meghana Nasre",
      "Prajakta Nimbhorkar"
    ],
    "author_ids": [],
    "abstract": "We consider the problem of assigning items to platforms in the presence of\ngroup fairness constraints. In the input, each item belongs to certain\ncategories, called classes in this paper. Each platform specifies the group\nfairness constraints through an upper bound on the number of items it can serve\nfrom each class. Additionally, each platform also has an upper bound on the\ntotal number of items it can serve. The goal is to assign items to platforms so\nas to maximize the number of items assigned while satisfying the upper bounds\nof each class. In some cases, there is a revenue associated with matching an\nitem to a platform, then the goal is to maximize the revenue generated.\n  This problem models several important real-world problems like ad-auctions,\nscheduling, resource allocations, school choice etc.We also show an interesting\nconnection to computing a generalized maximum independent set on hypergraphs\nand ranking items under group fairness constraints.\n  We show that if the classes are arbitrary, then the problem is NP-hard and\nhas a strong inapproximability. We consider the problem in both online and\noffline settings under natural restrictions on the classes. Under these\nrestrictions, the problem continues to remain NP-hard but admits approximation\nalgorithms with small approximation factors. We also implement some of the\nalgorithms. Our experiments show that the algorithms work well in practice both\nin terms of efficiency and the number of items that get assigned to some\nplatform.",
    "published_date": "2021-05-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.09522v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.09491v1",
    "title": "Generalized Few-Shot Object Detection without Forgetting",
    "authors": [
      "Zhibo Fan",
      "Yuchen Ma",
      "Zeming Li",
      "Jian Sun"
    ],
    "author_ids": [],
    "abstract": "Recently few-shot object detection is widely adopted to deal with\ndata-limited situations. While most previous works merely focus on the\nperformance on few-shot categories, we claim that detecting all classes is\ncrucial as test samples may contain any instances in realistic applications,\nwhich requires the few-shot detector to learn new concepts without forgetting.\nThrough analysis on transfer learning based methods, some neglected but\nbeneficial properties are utilized to design a simple yet effective few-shot\ndetector, Retentive R-CNN. It consists of Bias-Balanced RPN to debias the\npretrained RPN and Re-detector to find few-shot class objects without\nforgetting previous knowledge. Extensive experiments on few-shot detection\nbenchmarks show that Retentive R-CNN significantly outperforms state-of-the-art\nmethods on overall performance among all settings as it can achieve competitive\nresults on few-shot classes and does not degrade the base class performance at\nall. Our approach has demonstrated that the long desired never-forgetting\nlearner is available in object detection.",
    "published_date": "2021-05-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.09491v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.09428v1",
    "title": "Explainable Health Risk Predictor with Transformer-based Medicare Claim Encoder",
    "authors": [
      "Chuhong Lahlou",
      "Ancil Crayton",
      "Caroline Trier",
      "Evan Willett"
    ],
    "author_ids": [],
    "abstract": "In 2019, The Centers for Medicare and Medicaid Services (CMS) launched an\nArtificial Intelligence (AI) Health Outcomes Challenge seeking solutions to\npredict risk in value-based care for incorporation into CMS Innovation Center\npayment and service delivery models. Recently, modern language models have\nplayed key roles in a number of health related tasks. This paper presents, to\nthe best of our knowledge, the first application of these models to patient\nreadmission prediction. To facilitate this, we create a dataset of 1.2 million\nmedical history samples derived from the Limited Dataset (LDS) issued by CMS.\nMoreover, we propose a comprehensive modeling solution centered on a deep\nlearning framework for this data. To demonstrate the framework, we train an\nattention-based Transformer to learn Medicare semantics in support of\nperforming downstream prediction tasks thereby achieving 0.91 AUC and 0.91\nrecall on readmission classification. We also introduce a novel data\npre-processing pipeline and discuss pertinent deployment considerations\nsurrounding model explainability and bias.",
    "published_date": "2021-05-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.09428v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.09383v1",
    "title": "Guaranteeing Maximin Shares: Some Agents Left Behind",
    "authors": [
      "Hadi Hosseini",
      "Andrew Searns"
    ],
    "author_ids": [],
    "abstract": "The maximin share (MMS) guarantee is a desirable fairness notion for\nallocating indivisible goods. While MMS allocations do not always exist,\nseveral approximation techniques have been developed to ensure that all agents\nreceive a fraction of their maximin share. We focus on an alternative\napproximation notion, based on the population of agents, that seeks to\nguarantee MMS for a fraction of agents. We show that no optimal approximation\nalgorithm can satisfy more than a constant number of agents, and discuss the\nexistence and computation of MMS for all but one agent and its relation to\napproximate MMS guarantees. We then prove the existence of allocations that\nguarantee MMS for $\\frac{2}{3}$ of agents, and devise a polynomial time\nalgorithm that achieves this bound for up to nine agents. A key implication of\nour result is the existence of allocations that guarantee\n$\\text{MMS}^{\\lceil{3n/2}\\rceil}$, i.e., the value that agents receive by\npartitioning the goods into $\\lceil{\\frac{3}{2}n}\\rceil$ bundles, improving the\nbest known guarantee of $\\text{MMS}^{2n-2}$. Finally, we provide empirical\nexperiments using synthetic data.",
    "published_date": "2021-05-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT",
      "cs.AI",
      "econ.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.09383v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.09060v1",
    "title": "The State of AI Ethics Report (Volume 4)",
    "authors": [
      "Abhishek Gupta",
      "Alexandrine Royer",
      "Connor Wright",
      "Victoria Heath",
      "Muriam Fancy",
      "Marianna Bergamaschi Ganapini",
      "Shannon Egan",
      "Masa Sweidan",
      "Mo Akif",
      "Renjie Butalid"
    ],
    "author_ids": [],
    "abstract": "The 4th edition of the Montreal AI Ethics Institute's The State of AI Ethics\ncaptures the most relevant developments in the field of AI Ethics since January\n2021. This report aims to help anyone, from machine learning experts to human\nrights activists and policymakers, quickly digest and understand the\never-changing developments in the field. Through research and article\nsummaries, as well as expert commentary, this report distills the research and\nreporting surrounding various domains related to the ethics of AI, with a\nparticular focus on four key themes: Ethical AI, Fairness & Justice, Humans &\nTech, and Privacy.\n  In addition, The State of AI Ethics includes exclusive content written by\nworld-class AI Ethics experts from universities, research institutes,\nconsulting firms, and governments. Opening the report is a long-form piece by\nEdward Higgs (Professor of History, University of Essex) titled \"AI and the\nFace: A Historian's View.\" In it, Higgs examines the unscientific history of\nfacial analysis and how AI might be repeating some of those mistakes at scale.\nThe report also features chapter introductions by Alexa Hagerty\n(Anthropologist, University of Cambridge), Marianna Ganapini (Faculty Director,\nMontreal AI Ethics Institute), Deborah G. Johnson (Emeritus Professor,\nEngineering and Society, University of Virginia), and Soraj Hongladarom\n(Professor of Philosophy and Director, Center for Science, Technology and\nSociety, Chulalongkorn University in Bangkok).\n  This report should be used not only as a point of reference and insight on\nthe latest thinking in the field of AI Ethics, but should also be used as a\ntool for introspection as we aim to foster a more nuanced conversation\nregarding the impacts of AI on the world.",
    "published_date": "2021-05-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.09060v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.09059v1",
    "title": "The State of AI Ethics Report (January 2021)",
    "authors": [
      "Abhishek Gupta",
      "Alexandrine Royer",
      "Connor Wright",
      "Falaah Arif Khan",
      "Victoria Heath",
      "Erick Galinkin",
      "Ryan Khurana",
      "Marianna Bergamaschi Ganapini",
      "Muriam Fancy",
      "Masa Sweidan",
      "Mo Akif",
      "Renjie Butalid"
    ],
    "author_ids": [],
    "abstract": "The 3rd edition of the Montreal AI Ethics Institute's The State of AI Ethics\ncaptures the most relevant developments in AI Ethics since October 2020. It\naims to help anyone, from machine learning experts to human rights activists\nand policymakers, quickly digest and understand the field's ever-changing\ndevelopments. Through research and article summaries, as well as expert\ncommentary, this report distills the research and reporting surrounding various\ndomains related to the ethics of AI, including: algorithmic injustice,\ndiscrimination, ethical AI, labor impacts, misinformation, privacy, risk and\nsecurity, social media, and more.\n  In addition, The State of AI Ethics includes exclusive content written by\nworld-class AI Ethics experts from universities, research institutes,\nconsulting firms, and governments. Unique to this report is \"The Abuse and\nMisogynoir Playbook,\" written by Dr. Katlyn Tuner (Research Scientist, Space\nEnabled Research Group, MIT), Dr. Danielle Wood (Assistant Professor, Program\nin Media Arts and Sciences; Assistant Professor, Aeronautics and Astronautics;\nLead, Space Enabled Research Group, MIT) and Dr. Catherine D'Ignazio (Assistant\nProfessor, Urban Science and Planning; Director, Data + Feminism Lab, MIT). The\npiece (and accompanying infographic), is a deep-dive into the historical and\nsystematic silencing, erasure, and revision of Black women's contributions to\nknowledge and scholarship in the United Stations, and globally. Exposing and\ncountering this Playbook has become increasingly important following the firing\nof AI Ethics expert Dr. Timnit Gebru (and several of her supporters) at Google.\n  This report should be used not only as a point of reference and insight on\nthe latest thinking in the field of AI Ethics, but should also be used as a\ntool for introspection as we aim to foster a more nuanced conversation\nregarding the impacts of AI on the world.",
    "published_date": "2021-05-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.09059v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.09047v1",
    "title": "Obstructing Classification via Projection",
    "authors": [
      "Pantea Haghighatkhah",
      "Wouter Meulemans",
      "Bettina Speckman",
      "Jérôme Urhausen",
      "Kevin Verbeek"
    ],
    "author_ids": [],
    "abstract": "Machine learning and data mining techniques are effective tools to classify\nlarge amounts of data. But they tend to preserve any inherent bias in the data,\nfor example, with regards to gender or race. Removing such bias from data or\nthe learned representations is quite challenging. In this paper we study a\ngeometric problem which models a possible approach for bias removal. Our input\nis a set of points P in Euclidean space R^d and each point is labeled with k\nbinary-valued properties. A priori we assume that it is \"easy\" to classify the\ndata according to each property. Our goal is to obstruct the classification\naccording to one property by a suitable projection to a lower-dimensional\nEuclidean space R^m (m < d), while classification according to all other\nproperties remains easy.\n  What it means for classification to be easy depends on the classification\nmodel used. We first consider classification by linear separability as employed\nby support vector machines. We use Kirchberger's Theorem to show that, under\ncertain conditions, a simple projection to R^(d-1) suffices to eliminate the\nlinear separability of one of the properties whilst maintaining the linear\nseparability of the other properties. We also study the problem of maximizing\nthe linear \"inseparability\" of the chosen property. Second, we consider more\ncomplex forms of separability and prove a connection between the number of\nprojections required to obstruct classification and the Helly-type properties\nof such separabilities.",
    "published_date": "2021-05-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CG",
      "cs.LG",
      "68-XX"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.09047v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.08896v1",
    "title": "Designing a Pseudo-Random Bit Generator with a Novel 5D-Hyperchaotic System",
    "authors": [
      "Ngoc T. Nguyen",
      "Toan Q. Bui",
      "Ghyslain Gagnon",
      "Pascal Giard",
      "Georges Kaddoum"
    ],
    "author_ids": [],
    "abstract": "Dynamic and non-linear systems are emerging as potential candidates for\nrandom bit generation. In this context, chaotic systems, which are both dynamic\nand stochastic, are particularly suitable. This paper introduces a new\ncontinuous chaotic system along with its corresponding implementation, which\ntargets field-programmable gate array (FPGA). This chaotic system has five\ndimensions, which exhibit complex chaotic dynamics, thus enabling the\nutilization of chaotic signals in cryptography. A mathematical analysis is\npresented to demonstrate the dynamic characteristics of the proposed\nhyperchaotic system. A novel digital implementation of the proposed system is\npresented. Moreover, a data scrambling circuit is implemented to eliminate the\nbias effect and increase the randomness of the bitstream generated from the\nchaotic signals. We show that the proposed random bit generator has high\nrandomness. The generated bits successfully pass well-known statistical\nrandomness test-suites, i.e., NIST SP800-22, Diehard, and TestU01. The\nready-to-use random bit generator is deployed on a Xilinx Zynq-7000 SoC ZC702\nEvaluation Kit. Experimental results show that the proposed random bit\ngenerator can achieve a maximum throughput of 6.78 Gbps, which is over 3.6\ntimes greater than state-of-the-art designs while requiring under 4% of the\nresources available on the targeted FPGA.",
    "published_date": "2021-05-19T00:00:00",
    "year": 2021,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.08896v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.08867v1",
    "title": "AI and Ethics -- Operationalising Responsible AI",
    "authors": [
      "Liming Zhu",
      "Xiwei Xu",
      "Qinghua Lu",
      "Guido Governatori",
      "Jon Whittle"
    ],
    "author_ids": [],
    "abstract": "In the last few years, AI continues demonstrating its positive impact on\nsociety while sometimes with ethically questionable consequences. Building and\nmaintaining public trust in AI has been identified as the key to successful and\nsustainable innovation. This chapter discusses the challenges related to\noperationalizing ethical AI principles and presents an integrated view that\ncovers high-level ethical AI principles, the general notion of\ntrust/trustworthiness, and product/process support in the context of\nresponsible AI, which helps improve both trust and trustworthiness of AI for a\nwider set of stakeholders.",
    "published_date": "2021-05-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.08867v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.08847v2",
    "title": "Beyond \"Fairness:\" Structural (In)justice Lenses on AI for Education",
    "authors": [
      "Michael Madaio",
      "Su Lin Blodgett",
      "Elijah Mayfield",
      "Ezekiel Dixon-Román"
    ],
    "author_ids": [],
    "abstract": "Educational technologies, and the systems of schooling in which they are\ndeployed, enact particular ideologies about what is important to know and how\nlearners should learn. As artificial intelligence technologies -- in education\nand beyond -- may contribute to inequitable outcomes for marginalized\ncommunities, various approaches have been developed to evaluate and mitigate\nthe harmful impacts of AI. However, we argue in this paper that the dominant\nparadigm of evaluating fairness on the basis of performance disparities in AI\nmodels is inadequate for confronting the systemic inequities that educational\nAI systems (re)produce. We draw on a lens of structural injustice informed by\ncritical theory and Black feminist scholarship to critically interrogate\nseveral widely-studied and widely-adopted categories of educational AI and\nexplore how they are bound up in and reproduce historical legacies of\nstructural injustice and inequity, regardless of the parity of their models'\nperformance. We close with alternative visions for a more equitable future for\neducational AI.",
    "published_date": "2021-05-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "K.3; K.4; I.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.08847v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.08814v1",
    "title": "A Generalized Framework for Measuring Pedestrian Accessibility around the World Using Open Data",
    "authors": [
      "Shiqin Liu",
      "Carl Higgs",
      "Jonathan Arundel",
      "Geoff Boeing",
      "Nicholas Cerdera",
      "David Moctezuma",
      "Ester Cerin",
      "Deepti Adlakha",
      "Melanie Lowe",
      "Billie Giles-Corti"
    ],
    "author_ids": [],
    "abstract": "Pedestrian accessibility is an important factor in urban transport and land\nuse policy and critical for creating healthy, sustainable cities. Developing\nand evaluating indicators measuring inequalities in pedestrian accessibility\ncan help planners and policymakers benchmark and monitor the progress of city\nplanning interventions. However, measuring and assessing indicators of urban\ndesign and transport features at high resolution worldwide to enable city\ncomparisons is challenging due to limited availability of official, high\nquality, and comparable spatial data, as well as spatial analysis tools\noffering customizable frameworks for indicator construction and analysis. To\naddress these challenges, this study develops an open source software framework\nto construct pedestrian accessibility indicators for cities using open and\nconsistent data. It presents a generalized method to consistently measure\npedestrian accessibility at high resolution and spatially aggregated scale, to\nallow for both within- and between-city analyses. The open source and open data\nmethods developed in this study can be extended to other cities worldwide to\nsupport local planning and policymaking. The software is made publicly\navailable for reuse in an open repository.",
    "published_date": "2021-05-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "econ.GN",
      "physics.soc-ph",
      "q-fin.EC",
      "stat.CO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.08814v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.08667v2",
    "title": "Image Cropping on Twitter: Fairness Metrics, their Limitations, and the Importance of Representation, Design, and Agency",
    "authors": [
      "Kyra Yee",
      "Uthaipon Tantipongpipat",
      "Shubhanshu Mishra"
    ],
    "author_ids": [],
    "abstract": "Twitter uses machine learning to crop images, where crops are centered around\nthe part predicted to be the most salient. In fall 2020, Twitter users raised\nconcerns that the automated image cropping system on Twitter favored\nlight-skinned over dark-skinned individuals, as well as concerns that the\nsystem favored cropping woman's bodies instead of their heads. In order to\naddress these concerns, we conduct an extensive analysis using formalized group\nfairness metrics. We find systematic disparities in cropping and identify\ncontributing factors, including the fact that the cropping based on the single\nmost salient point can amplify the disparities because of an effect we term\nargmax bias. However, we demonstrate that formalized fairness metrics and\nquantitative analysis on their own are insufficient for capturing the risk of\nrepresentational harm in automatic cropping. We suggest the removal of\nsaliency-based cropping in favor of a solution that better preserves user\nagency. For developing a new solution that sufficiently address concerns\nrelated to representational harm, our critique motivates a combination of\nquantitative and qualitative methods that include human-centered design.",
    "published_date": "2021-05-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.CV",
      "cs.HC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.08667v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.13817v3",
    "title": "Achieving Fairness with a Simple Ridge Penalty",
    "authors": [
      "Marco Scutari",
      "Francesca Panero",
      "Manuel Proissl"
    ],
    "author_ids": [],
    "abstract": "In this paper we present a general framework for estimating regression models\nsubject to a user-defined level of fairness. We enforce fairness as a model\nselection step in which we choose the value of a ridge penalty to control the\neffect of sensitive attributes. We then estimate the parameters of the model\nconditional on the chosen penalty value. Our proposal is mathematically simple,\nwith a solution that is partly in closed form, and produces estimates of the\nregression coefficients that are intuitive to interpret as a function of the\nlevel of fairness. Furthermore, it is easily extended to generalised linear\nmodels, kernelised regression models and other penalties; and it can\naccommodate multiple definitions of fairness.\n  We compare our approach with the regression model from Komiyama et al.\n(2018), which implements a provably-optimal linear regression model; and with\nthe fair models from Zafar et al. (2019). We evaluate these approaches\nempirically on six different data sets, and we find that our proposal provides\nbetter goodness of fit and better predictive accuracy for the same level of\nfairness. In addition, we highlight a source of bias in the original\nexperimental evaluation in Komiyama et al. (2018).",
    "published_date": "2021-05-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ME"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.13817v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.08493v2",
    "title": "Identifying Undercompensated Groups Defined By Multiple Attributes in Risk Adjustment",
    "authors": [
      "Anna Zink",
      "Sherri Rose"
    ],
    "author_ids": [],
    "abstract": "Risk adjustment in health care aims to redistribute payments to insurers\nbased on costs. However, risk adjustment formulas are known to underestimate\ncosts for some groups of patients. This undercompensation makes these groups\nunprofitable to insurers and creates incentives for insurers to discriminate.\nWe develop a machine learning method for \"group importance\" to identify\nunprofitable groups defined by multiple attributes, improving on the arbitrary\nnature of existing evaluations. This procedure was designed to evaluate the\nrisk adjustment formulas used in the U.S. health insurance Marketplaces as well\nas Medicare. We find that a number of previously unidentified groups with\nmultiple chronic conditions are undercompensated in the Marketplaces risk\nadjustment formula, while groups without chronic conditions tend to be\novercompensated in the Marketplaces. The magnitude of undercompensation when\ndefining groups with multiple attributes is larger than with single attributes.\nNo complex groups were found to be consistently under- or overcompensated in\nthe Medicare risk adjustment formula. Our work provides policy makers with new\ninformation on potential targets of discrimination in the health care system\nand a path towards more equitable health coverage.",
    "published_date": "2021-05-18T00:00:00",
    "year": 2021,
    "categories": [
      "stat.AP",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.08493v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.08475v1",
    "title": "AI and Shared Prosperity",
    "authors": [
      "Katya Klinova",
      "Anton Korinek"
    ],
    "author_ids": [],
    "abstract": "Future advances in AI that automate away human labor may have stark\nimplications for labor markets and inequality. This paper proposes a framework\nto analyze the effects of specific types of AI systems on the labor market,\nbased on how much labor demand they will create versus displace, while taking\ninto account that productivity gains also make society wealthier and thereby\ncontribute to additional labor demand. This analysis enables ethically-minded\ncompanies creating or deploying AI systems as well as researchers and\npolicymakers to take into account the effects of their actions on labor markets\nand inequality, and therefore to steer progress in AI in a direction that\nadvances shared prosperity and an inclusive economic future for all of\nhumanity.",
    "published_date": "2021-05-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.CY",
      "econ.GN",
      "q-fin.EC",
      "J.4; K.4.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.08475v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.08390v1",
    "title": "3D Displays: Their Evolution, Inherent Challenges & Future Perspectives",
    "authors": [
      "Xingyu Pan",
      "Xuanhui Xu",
      "Soumyabrata Dev",
      "Abraham G Campbell"
    ],
    "author_ids": [],
    "abstract": "The popularity of 3D displays has risen drastically over the past few decades\nbut these displays are still merely a novelty compared to their true potential.\nThe development has mostly focused on Head Mounted Displays (HMD) development\nfor Virtual Reality and in general ignored non-HMD 3D displays. This is due to\nthe inherent difficulty in the creation of these displays and their\nimpracticability in general use due to cost, performance, and lack of\nmeaningful use cases. In fairness to the hardware manufacturers who have made\nstriking innovations in this field, there has been a dereliction of duty of\nsoftware developers and researchers in terms of developing software to best\nutilize these displays.\n  This paper will seek to identify what areas of future software development\ncould mitigate this dereliction. To achieve this goal, the paper will first\nexamine the current state of the art and perform a comparative analysis on\ndifferent types of 3D displays, from this analysis a clear researcher gap\nexists in terms of software development for Light field displays which are the\ncurrent state of the art of non-HMD-based 3D displays.\n  The paper will then outline six distinct areas where the context-awareness\nconcept will allow for non-HMD-based 3D displays in particular light field\ndisplays that can not only compete but surpass their HMD-based brethren for\nmany specific use cases.",
    "published_date": "2021-05-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.08390v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.08268v1",
    "title": "Permutation Invariant Policy Optimization for Mean-Field Multi-Agent Reinforcement Learning: A Principled Approach",
    "authors": [
      "Yan Li",
      "Lingxiao Wang",
      "Jiachen Yang",
      "Ethan Wang",
      "Zhaoran Wang",
      "Tuo Zhao",
      "Hongyuan Zha"
    ],
    "author_ids": [],
    "abstract": "Multi-agent reinforcement learning (MARL) becomes more challenging in the\npresence of more agents, as the capacity of the joint state and action spaces\ngrows exponentially in the number of agents. To address such a challenge of\nscale, we identify a class of cooperative MARL problems with permutation\ninvariance, and formulate it as a mean-field Markov decision processes (MDP).\nTo exploit the permutation invariance therein, we propose the mean-field\nproximal policy optimization (MF-PPO) algorithm, at the core of which is a\npermutation-invariant actor-critic neural architecture. We prove that MF-PPO\nattains the globally optimal policy at a sublinear rate of convergence.\nMoreover, its sample complexity is independent of the number of agents. We\nvalidate the theoretical advantages of MF-PPO with numerical experiments in the\nmulti-agent particle environment (MPE). In particular, we show that the\ninductive bias introduced by the permutation-invariant neural architecture\nenables MF-PPO to outperform existing competitors with a smaller number of\nmodel parameters, which is the key to its generalization performance.",
    "published_date": "2021-05-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.08268v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.08165v1",
    "title": "Social Behavior and Mental Health: A Snapshot Survey under COVID-19 Pandemic",
    "authors": [
      "Sahraoui Dhelim",
      "Liming Luke Chen",
      "Huansheng Ning",
      "Sajal K Das",
      "Chris Nugent",
      "Devin Burns",
      "Gerard Leavey",
      "Dirk Pesch",
      "Eleanor Bantry-White"
    ],
    "author_ids": [],
    "abstract": "Online social media provides a channel for monitoring people's social\nbehaviors and their mental distress. Due to the restrictions imposed by\nCOVID-19 people are increasingly using online social networks to express their\nfeelings. Consequently, there is a significant amount of diverse user-generated\nsocial media content. However, COVID-19 pandemic has changed the way we live,\nstudy, socialize and recreate and this has affected our well-being and mental\nhealth problems. There are growing researches that leverage online social media\nanalysis to detect and assess user's mental status. In this paper, we survey\nthe literature of social media analysis for mental disorders detection, with a\nspecial focus on the studies conducted in the context of COVID-19 during\n2020-2021. Firstly, we classify the surveyed studies in terms of feature\nextraction types, varying from language usage patterns to aesthetic preferences\nand online behaviors. Secondly, we explore detection methods used for mental\ndisorders detection including machine learning and deep learning detection\nmethods. Finally, we discuss the challenges of mental disorder detection using\nsocial media data, including the privacy and ethical concerns, as well as the\ntechnical challenges of scaling and deploying such systems at large scales, and\ndiscuss the learnt lessons over the last few years.",
    "published_date": "2021-05-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI",
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.08165v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.07775v2",
    "title": "Be Causal: De-biasing Social Network Confounding in Recommendation",
    "authors": [
      "Qian Li",
      "Xiangmeng Wang",
      "Guandong Xu"
    ],
    "author_ids": [],
    "abstract": "In recommendation systems, the existence of the missing-not-at-random (MNAR)\nproblem results in the selection bias issue, degrading the recommendation\nperformance ultimately. A common practice to address MNAR is to treat missing\nentries from the so-called \"exposure\" perspective, i.e., modeling how an item\nis exposed (provided) to a user. Most of the existing approaches use heuristic\nmodels or re-weighting strategy on observed ratings to mimic the\nmissing-at-random setting. However, little research has been done to reveal how\nthe ratings are missing from a causal perspective. To bridge the gap, we\npropose an unbiased and robust method called DENC (De-bias Network Confounding\nin Recommendation) inspired by confounder analysis in causal inference. In\ngeneral, DENC provides a causal analysis on MNAR from both the inherent factors\n(e.g., latent user or item factors) and auxiliary network's perspective.\nParticularly, the proposed exposure model in DENC can control the social\nnetwork confounder meanwhile preserves the observed exposure information. We\nalso develop a deconfounding model through the balanced representation learning\nto retain the primary user and item features, which enables DENC generalize\nwell on the rating prediction. Extensive experiments on three datasets validate\nthat our proposed model outperforms the state-of-the-art baselines.",
    "published_date": "2021-05-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.07775v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.07703v4",
    "title": "The effect of algorithmic bias and network structure on coexistence, consensus, and polarization of opinions",
    "authors": [
      "Antonio F. Peralta",
      "Matteo Neri",
      "János Kertész",
      "Gerardo Iñiguez"
    ],
    "author_ids": [],
    "abstract": "Individuals of modern societies share ideas and participate in collective\nprocesses within a pervasive, variable, and mostly hidden ecosystem of content\nfiltering technologies that determine what information we see online. Despite\nthe impact of these algorithms on daily life and society, little is known about\ntheir effect on information transfer and opinion formation. It is thus unclear\nto what extent algorithmic bias has a harmful influence on collective\ndecision-making, such as a tendency to polarize debate. Here we introduce a\ngeneral theoretical framework to systematically link models of opinion\ndynamics, social network structure, and content filtering. We showcase the\nflexibility of our framework by exploring a family of binary-state opinion\ndynamics models where information exchange lies in a spectrum from pairwise to\ngroup interactions. All models show an opinion polarization regime driven by\nalgorithmic bias and modular network structure. The role of content filtering\nis, however, surprisingly nuanced; for pairwise interactions it leads to\npolarization, while for group interactions it promotes coexistence of opinions.\nThis allows us to pinpoint which social interactions are robust against\nalgorithmic bias, and which ones are susceptible to bias-enhanced opinion\npolarization. Our framework gives theoretical ground for the development of\nheuristics to tackle harmful effects of online bias, such as information\nbottlenecks, echo chambers, and opinion radicalization.",
    "published_date": "2021-05-17T00:00:00",
    "year": 2021,
    "categories": [
      "physics.soc-ph",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.07703v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.07637v2",
    "title": "Class-Incremental Few-Shot Object Detection",
    "authors": [
      "Pengyang Li",
      "Yanan Li",
      "Han Cui",
      "Donghui Wang"
    ],
    "author_ids": [],
    "abstract": "Conventional detection networks usually need abundant labeled training\nsamples, while humans can learn new concepts incrementally with just a few\nexamples. This paper focuses on a more challenging but realistic\nclass-incremental few-shot object detection problem (iFSD). It aims to\nincrementally transfer the model for novel objects from only a few annotated\nsamples without catastrophically forgetting the previously learned ones. To\ntackle this problem, we propose a novel method LEAST, which can transfer with\nLess forgetting, fEwer training resources, And Stronger Transfer capability.\nSpecifically, we first present the transfer strategy to reduce unnecessary\nweight adaptation and improve the transfer capability for iFSD. On this basis,\nwe then integrate the knowledge distillation technique using a less\nresource-consuming approach to alleviate forgetting and propose a novel\nclustering-based exemplar selection process to preserve more discriminative\nfeatures previously learned. Being a generic and effective method, LEAST can\nlargely improve the iFSD performance on various benchmarks.",
    "published_date": "2021-05-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.07637v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.07513v2",
    "title": "Decision Making with Differential Privacy under a Fairness Lens",
    "authors": [
      "Ferdinando Fioretto",
      "Cuong Tran",
      "Pascal Van Hentenryck"
    ],
    "author_ids": [],
    "abstract": "Agencies, such as the U.S. Census Bureau, release data sets and statistics\nabout groups of individuals that are used as input to a number of critical\ndecision processes. To conform to privacy and confidentiality requirements,\nthese agencies are often required to release privacy-preserving versions of the\ndata. This paper studies the release of differentially private data sets and\nanalyzes their impact on some critical resource allocation tasks under a\nfairness perspective. {The paper shows that, when the decisions take as input\ndifferentially private data}, the noise added to achieve privacy\ndisproportionately impacts some groups over others. The paper analyzes the\nreasons for these disproportionate impacts and proposes guidelines to mitigate\nthese effects. The proposed approaches are evaluated on critical decision\nproblems that use differentially private census data.",
    "published_date": "2021-05-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.07513v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.07401v3",
    "title": "Dissipativity and Integral Quadratic Constraints, Tailored computational robustness tests for complex interconnections",
    "authors": [
      "Carsten Scherer"
    ],
    "author_ids": [],
    "abstract": "A central notion in systems theory is dissipativity, which has been\nintroduced by Jan Willems with the explicit goal of arriving at a fundamental\nunderstanding of the stability properties of feedback interconnections. In\nrobust control, the framework of integral quadratic constraints (IQCs) builds\non the seminal contributions of Yakubovich and Zames in the 1960's. It provides\na technique for analyzing the stability of an interconnection of some linear\nsystem in feedback with a whole class of systems, also refereed to as\nuncertainty.\n  In this paper we survey the key ideas of exploiting dissipativity and\nintegral quadratic constraints for the computational analysis of robust\nstability and performance properties of uncertain interconnections in terms of\nlinear matrix inequalities. In particular for dynamic supply rates, the paper\nrevolves around the notion of finite-horizon integral quadratic constraints\nwith a terminal cost. We reveal that this provides a seamless link between the\ngeneral IQC theorem and dissipativity theory that has been established only\nrather recently.",
    "published_date": "2021-05-16T00:00:00",
    "year": 2021,
    "categories": [
      "math.OC",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.07401v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.07346v1",
    "title": "Understanding the Effect of Bias in Deep Anomaly Detection",
    "authors": [
      "Ziyu Ye",
      "Yuxin Chen",
      "Haitao Zheng"
    ],
    "author_ids": [],
    "abstract": "Anomaly detection presents a unique challenge in machine learning, due to the\nscarcity of labeled anomaly data. Recent work attempts to mitigate such\nproblems by augmenting training of deep anomaly detection models with\nadditional labeled anomaly samples. However, the labeled data often does not\nalign with the target distribution and introduces harmful bias to the trained\nmodel. In this paper, we aim to understand the effect of a biased anomaly set\non anomaly detection. Concretely, we view anomaly detection as a supervised\nlearning task where the objective is to optimize the recall at a given false\npositive rate. We formally study the relative scoring bias of an anomaly\ndetector, defined as the difference in performance with respect to a baseline\nanomaly detector. We establish the first finite sample rates for estimating the\nrelative scoring bias for deep anomaly detection, and empirically validate our\ntheoretical results on both synthetic and real-world datasets. We also provide\nan extensive empirical study on how a biased training anomaly set affects the\nanomaly score function and therefore the detection performance on different\nanomaly classes. Our study demonstrates scenarios in which the biased anomaly\nset can be useful or problematic, and provides a solid benchmark for future\nresearch.",
    "published_date": "2021-05-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.07346v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.07244v1",
    "title": "Fairly Private Through Group Tagging and Relation Impact",
    "authors": [
      "Poushali Sengupta",
      "Subhankar Mishra"
    ],
    "author_ids": [],
    "abstract": "Privacy and Fairness both are very important nowadays. For most of the cases\nin the online service providing system, users have to share their personal\ninformation with the organizations. In return, the clients not only demand a\nhigh privacy guarantee to their sensitive data but also expected to be treated\nfairly irrespective of their age, gender, religion, race, skin color, or other\nsensitive protected attributes. Our work introduces a novel architecture that\nis balanced among the privacy-utility-fairness trade-off. The proposed\nmechanism applies Group Tagging Method and Fairly Iterative Shuffling (FIS)\nthat amplifies privacy through random shuffling and prevents linkage attack.\nThe algorithm introduces a fair classification problem by Relation Impact based\non Equalized Minimal FPR-FNR among the protected tagged group. For the count\nreport generation, the aggregator uses TF-IDF to add noise for providing\nlongitudinal Differential Privacy guarantee. Lastly, the mechanism boosts the\nutility through risk minimization function and obtain the optimal\nprivacy-utility budget of the system. In our work, we have done a case study on\ngender equality in the admission system and helps to obtain a satisfying result\nwhich implies that the proposed architecture achieves the group fairness and\noptimal privacy-utility trade-off for both the numerical and decision making\nQueries.",
    "published_date": "2021-05-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.07244v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.07168v1",
    "title": "Cohort Shapley value for algorithmic fairness",
    "authors": [
      "Masayoshi Mase",
      "Art B. Owen",
      "Benjamin B. Seiler"
    ],
    "author_ids": [],
    "abstract": "Cohort Shapley value is a model-free method of variable importance grounded\nin game theory that does not use any unobserved and potentially impossible\nfeature combinations. We use it to evaluate algorithmic fairness, using the\nwell known COMPAS recidivism data as our example. This approach allows one to\nidentify for each individual in a data set the extent to which they were\nadversely or beneficially affected by their value of a protected attribute such\nas their race. The method can do this even if race was not one of the original\npredictors and even if it does not have access to a proprietary algorithm that\nhas made the predictions. The grounding in game theory lets us define aggregate\nvariable importance for a data set consistently with its per subject\ndefinitions. We can investigate variable importance for multiple quantities of\ninterest in the fairness literature including false positive predictions.",
    "published_date": "2021-05-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "econ.EM",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.07168v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.07144v3",
    "title": "A Cognitive Regularizer for Language Modeling",
    "authors": [
      "Jason Wei",
      "Clara Meister",
      "Ryan Cotterell"
    ],
    "author_ids": [],
    "abstract": "The uniform information density (UID) hypothesis, which posits that speakers\nbehaving optimally tend to distribute information uniformly across a linguistic\nsignal, has gained traction in psycholinguistics as an explanation for certain\nsyntactic, morphological, and prosodic choices. In this work, we explore\nwhether the UID hypothesis can be operationalized as an inductive bias for\nstatistical language modeling. Specifically, we augment the canonical MLE\nobjective for training language models with a regularizer that encodes UID. In\nexperiments on ten languages spanning five language families, we find that\nusing UID regularization consistently improves perplexity in language models,\nhaving a larger effect when training data is limited. Moreover, via an analysis\nof generated sequences, we find that UID-regularized language models have other\ndesirable properties, e.g., they generate text that is more lexically diverse.\nOur results not only suggest that UID is a reasonable inductive bias for\nlanguage modeling, but also provide an alternative validation of the UID\nhypothesis using modern-day NLP tools.",
    "published_date": "2021-05-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.07144v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.07020v1",
    "title": "Urban Analytics: History, Trajectory, and Critique",
    "authors": [
      "Geoff Boeing",
      "Michael Batty",
      "Shan Jiang",
      "Lisa Schweitzer"
    ],
    "author_ids": [],
    "abstract": "Urban analytics combines spatial analysis, statistics, computer science, and\nurban planning to understand and shape city futures. While it promises better\npolicymaking insights, concerns exist around its epistemological scope and\nimpacts on privacy, ethics, and social control. This chapter reflects on the\nhistory and trajectory of urban analytics as a scholarly and professional\ndiscipline. In particular, it considers the direction in which this field is\ngoing and whether it improves our collective and individual welfare. It first\nintroduces early theories, models, and deductive methods from which the field\noriginated before shifting toward induction. It then explores urban network\nanalytics that enrich traditional representations of spatial interaction and\nstructure. Next it discusses urban applications of spatiotemporal big data and\nmachine learning. Finally, it argues that privacy and ethical concerns are too\noften ignored as ubiquitous monitoring and analytics can empower social\nrepression. It concludes with a call for a more critical urban analytics that\nrecognizes its epistemological limits, emphasizes human dignity, and learns\nfrom and supports marginalized communities.",
    "published_date": "2021-05-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.07020v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.06929v1",
    "title": "On Measuring the Diversity of Organizational Networks",
    "authors": [
      "Zeinab S. Jalali",
      "Krishnaram Kenthapadi",
      "Sucheta Soundarajan"
    ],
    "author_ids": [],
    "abstract": "The interaction patterns of employees in social and professional networks\nplay an important role in the success of employees and organizations as a\nwhole. However, in many fields there is a severe under-representation of\nminority groups; moreover, minority individuals may be segregated from the rest\nof the network or isolated from one another. While the problem of increasing\nthe representation of minority groups in various fields has been well-studied,\ndiver- sification in terms of numbers alone may not be sufficient: social\nrelationships should also be considered. In this work, we consider the problem\nof assigning a set of employment candidates to positions in a social network so\nthat diversity and overall fitness are maximized, and propose Fair Employee\nAssignment (FairEA), a novel algorithm for finding such a matching. The output\nfrom FairEA can be used as a benchmark by organizations wishing to evaluate\ntheir hiring and assignment practices. On real and synthetic networks, we\ndemonstrate that FairEA does well at finding high-fitness, high-diversity\nmatchings.",
    "published_date": "2021-05-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.06929v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.05244v1",
    "title": "Coopetition methodology for resource sharing in distributed OFDM-based cognitive radio networks",
    "authors": [
      "Marcin Parzy",
      "Hanna Bogucka"
    ],
    "author_ids": [],
    "abstract": "In this paper, we present a distributed resource allocation mechanism in\ncognitive radio networks, based on a new coopeti-tion methodology, which\ncombines advantages of nodes competition and cooperation. We postulate that\nthis new method allows for fully distributed resource management between\ncognitive radio devices. The presented framework is generic, however, we\nconsider it for the application in OFDMA networks. Coopetition takes the best\nfrom cooperative and competitive problem formulation and provides the\nopportunity to control the balance between fairness and spectral efficiency\n(SE) of resource allocation. Simulation results confirm that coopetition allows\nfor efficient resource utilization, and may be used practically in wireless\ncognitive networks.",
    "published_date": "2021-05-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.NI",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.05244v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.06791v2",
    "title": "Agree to Disagree: When Deep Learning Models With Identical Architectures Produce Distinct Explanations",
    "authors": [
      "Matthew Watson",
      "Bashar Awwad Shiekh Hasan",
      "Noura Al Moubayed"
    ],
    "author_ids": [],
    "abstract": "Deep Learning of neural networks has progressively become more prominent in\nhealthcare with models reaching, or even surpassing, expert accuracy levels.\nHowever, these success stories are tainted by concerning reports on the lack of\nmodel transparency and bias against some medical conditions or patients'\nsub-groups. Explainable methods are considered the gateway to alleviate many of\nthese concerns. In this study we demonstrate that the generated explanations\nare volatile to changes in model training that are perpendicular to the\nclassification task and model structure. This raises further questions about\ntrust in deep learning models for healthcare. Mainly, whether the models\ncapture underlying causal links in the data or just rely on spurious\ncorrelations that are made visible via explanation methods. We demonstrate that\nthe output of explainability methods on deep neural networks can vary\nsignificantly by changes of hyper-parameters, such as the random seed or how\nthe training set is shuffled. We introduce a measure of explanation consistency\nwhich we use to highlight the identified problems on the MIMIC-CXR dataset. We\nfind explanations of identical models but with different training setups have a\nlow consistency: $\\approx$ 33% on average. On the contrary, kernel methods are\nrobust against any orthogonal changes, with explanation consistency at 94%. We\nconclude that current trends in model explanation are not sufficient to\nmitigate the risks of deploying models in real life healthcare applications.",
    "published_date": "2021-05-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "I.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.06791v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.06756v1",
    "title": "Long Short-term Memory RNN",
    "authors": [
      "Christian Bakke Vennerød",
      "Adrian Kjærran",
      "Erling Stray Bugge"
    ],
    "author_ids": [],
    "abstract": "This paper is based on a machine learning project at the Norwegian University\nof Science and Technology, fall 2020. The project was initiated with a\nliterature review on the latest developments within time-series forecasting\nmethods in the scientific community over the past five years. The paper\nsummarizes the essential aspects of this research. Furthermore, in this paper,\nwe introduce an LSTM cell's architecture, and explain how different components\ngo together to alter the cell's memory and predict the output. Also, the paper\nprovides the necessary formulas and foundations to calculate a forward\niteration through an LSTM. Then, the paper refers to some practical\napplications and research that emphasize the strength and weaknesses of LSTMs,\nshown within the time-series domain and the natural language processing (NLP)\ndomain. Finally, alternative statistical methods for time series predictions\nare highlighted, where the paper outline ARIMA and exponential smoothing.\nNevertheless, as LSTMs can be viewed as a complex architecture, the paper\nassumes that the reader has some knowledge of essential machine learning\naspects, such as the multi-layer perceptron, activation functions, overfitting,\nbackpropagation, bias, over- and underfitting, and more.",
    "published_date": "2021-05-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.06756v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.06677v1",
    "title": "XAI Handbook: Towards a Unified Framework for Explainable AI",
    "authors": [
      "Sebastian Palacio",
      "Adriano Lucieri",
      "Mohsin Munir",
      "Jörn Hees",
      "Sheraz Ahmed",
      "Andreas Dengel"
    ],
    "author_ids": [],
    "abstract": "The field of explainable AI (XAI) has quickly become a thriving and prolific\ncommunity. However, a silent, recurrent and acknowledged issue in this area is\nthe lack of consensus regarding its terminology. In particular, each new\ncontribution seems to rely on its own (and often intuitive) version of terms\nlike \"explanation\" and \"interpretation\". Such disarray encumbers the\nconsolidation of advances in the field towards the fulfillment of scientific\nand regulatory demands e.g., when comparing methods or establishing their\ncompliance with respect to biases and fairness constraints. We propose a\ntheoretical framework that not only provides concrete definitions for these\nterms, but it also outlines all steps necessary to produce explanations and\ninterpretations. The framework also allows for existing contributions to be\nre-contextualized such that their scope can be measured, thus making them\ncomparable to other methods. We show that this framework is compliant with\ndesiderata on explanations, on interpretability and on evaluation metrics. We\npresent a use-case showing how the framework can be used to compare LIME, SHAP\nand MDNet, establishing their advantages and shortcomings. Finally, we discuss\nrelevant trends in XAI as well as recommendations for future work, all from the\nstandpoint of our framework.",
    "published_date": "2021-05-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.HC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.06677v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.06625v2",
    "title": "Biometrics: Trust, but Verify",
    "authors": [
      "Anil K. Jain",
      "Debayan Deb",
      "Joshua J. Engelsma"
    ],
    "author_ids": [],
    "abstract": "Over the past two decades, biometric recognition has exploded into a plethora\nof different applications around the globe. This proliferation can be\nattributed to the high levels of authentication accuracy and user convenience\nthat biometric recognition systems afford end-users. However, in-spite of the\nsuccess of biometric recognition systems, there are a number of outstanding\nproblems and concerns pertaining to the various sub-modules of biometric\nrecognition systems that create an element of mistrust in their use - both by\nthe scientific community and also the public at large. Some of these problems\ninclude: i) questions related to system recognition performance, ii) security\n(spoof attacks, adversarial attacks, template reconstruction attacks and\ndemographic information leakage), iii) uncertainty over the bias and fairness\nof the systems to all users, iv) explainability of the seemingly black-box\ndecisions made by most recognition systems, and v) concerns over data\ncentralization and user privacy. In this paper, we provide an overview of each\nof the aforementioned open-ended challenges. We survey work that has been\nconducted to address each of these concerns and highlight the issues requiring\nfurther attention. Finally, we provide insights into how the biometric\ncommunity can address core biometric recognition systems design issues to\nbetter instill trust, fairness, and security for all.",
    "published_date": "2021-05-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.06625v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.06604v1",
    "title": "Towards Equity and Algorithmic Fairness in Student Grade Prediction",
    "authors": [
      "Weijie Jiang",
      "Zachary A. Pardos"
    ],
    "author_ids": [],
    "abstract": "Equity of educational outcome and fairness of AI with respect to race have\nbeen topics of increasing importance in education. In this work, we address\nboth with empirical evaluations of grade prediction in higher education, an\nimportant task to improve curriculum design, plan interventions for academic\nsupport, and offer course guidance to students. With fairness as the aim, we\ntrial several strategies for both label and instance balancing to attempt to\nminimize differences in algorithm performance with respect to race. We find\nthat an adversarial learning approach, combined with grade label balancing,\nachieved by far the fairest results. With equity of educational outcome as the\naim, we trial strategies for boosting predictive performance on historically\nunderserved groups and find success in sampling those groups in inverse\nproportion to their historic outcomes. With AI-infused technology supports\nincreasingly prevalent on campuses, our methodologies fill a need for\nframeworks to consider performance trade-offs with respect to sensitive student\nattributes and allow institutions to instrument their AI resources in ways that\nare attentive to equity and fairness.",
    "published_date": "2021-05-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.06604v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.09293v1",
    "title": "Lessons Learned Addressing Dataset Bias in Model-Based Candidate Generation at Twitter",
    "authors": [
      "Alim Virani",
      "Jay Baxter",
      "Dan Shiebler",
      "Philip Gautier",
      "Shivam Verma",
      "Yan Xia",
      "Apoorv Sharma",
      "Sumit Binnani",
      "Linlin Chen",
      "Chenguang Yu"
    ],
    "author_ids": [],
    "abstract": "Traditionally, heuristic methods are used to generate candidates for large\nscale recommender systems. Model-based candidate generation promises multiple\npotential advantages, primarily that we can explicitly optimize the same\nobjective as the downstream ranking model. However, large scale model-based\ncandidate generation approaches suffer from dataset bias problems caused by the\ninfeasibility of obtaining representative data on very irrelevant candidates.\nPopular techniques to correct dataset bias, such as inverse propensity scoring,\ndo not work well in the context of candidate generation. We first explore the\ndynamics of the dataset bias problem and then demonstrate how to use random\nsampling techniques to mitigate it. Finally, in a novel application of\nfine-tuning, we show performance gains when applying our candidate generation\nsystem to Twitter's home timeline.",
    "published_date": "2021-05-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.09293v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.06558v1",
    "title": "Bias, Fairness, and Accountability with AI and ML Algorithms",
    "authors": [
      "Nengfeng Zhou",
      "Zach Zhang",
      "Vijayan N. Nair",
      "Harsh Singhal",
      "Jie Chen",
      "Agus Sudjianto"
    ],
    "author_ids": [],
    "abstract": "The advent of AI and ML algorithms has led to opportunities as well as\nchallenges. In this paper, we provide an overview of bias and fairness issues\nthat arise with the use of ML algorithms. We describe the types and sources of\ndata bias, and discuss the nature of algorithmic unfairness. This is followed\nby a review of fairness metrics in the literature, discussion of their\nlimitations, and a description of de-biasing (or mitigation) techniques in the\nmodel life cycle.",
    "published_date": "2021-05-13T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG",
      "00-02"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.06558v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.06442v1",
    "title": "An Empirical Comparison of Bias Reduction Methods on Real-World Problems in High-Stakes Policy Settings",
    "authors": [
      "Hemank Lamba",
      "Kit T. Rodolfa",
      "Rayid Ghani"
    ],
    "author_ids": [],
    "abstract": "Applications of machine learning (ML) to high-stakes policy settings -- such\nas education, criminal justice, healthcare, and social service delivery -- have\ngrown rapidly in recent years, sparking important conversations about how to\nensure fair outcomes from these systems. The machine learning research\ncommunity has responded to this challenge with a wide array of proposed\nfairness-enhancing strategies for ML models, but despite the large number of\nmethods that have been developed, little empirical work exists evaluating these\nmethods in real-world settings. Here, we seek to fill this research gap by\ninvestigating the performance of several methods that operate at different\npoints in the ML pipeline across four real-world public policy and social good\nproblems. Across these problems, we find a wide degree of variability and\ninconsistency in the ability of many of these methods to improve model\nfairness, but post-processing by choosing group-specific score thresholds\nconsistently removes disparities, with important implications for both the ML\nresearch community and practitioners deploying machine learning to inform\nconsequential policy decisions.",
    "published_date": "2021-05-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.06442v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.06351v2",
    "title": "Convergence and Implicit Bias of Gradient Flow on Overparametrized Linear Networks",
    "authors": [
      "Hancheng Min",
      "Salma Tarmoun",
      "Rene Vidal",
      "Enrique Mallada"
    ],
    "author_ids": [],
    "abstract": "Neural networks trained via gradient descent with random initialization and\nwithout any regularization enjoy good generalization performance in practice\ndespite being highly overparametrized. A promising direction to explain this\nphenomenon is to study how initialization and overparametrization affect\nconvergence and implicit bias of training algorithms. In this paper, we present\na novel analysis of single-hidden-layer linear networks trained under gradient\nflow, which connects initialization, optimization, and overparametrization.\nFirstly, we show that the squared loss converges exponentially to its optimum\nat a rate that depends on the level of imbalance and the margin of the\ninitialization. Secondly, we show that proper initialization constrains the\ndynamics of the network parameters to lie within an invariant set. In turn,\nminimizing the loss over this set leads to the min-norm solution. Finally, we\nshow that large hidden layer width, together with (properly scaled) random\ninitialization, ensures proximity to such an invariant set during training,\nallowing us to derive a novel non-asymptotic upper-bound on the distance\nbetween the trained network and the min-norm solution.",
    "published_date": "2021-05-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.06351v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.06345v1",
    "title": "Addressing Fairness, Bias and Class Imbalance in Machine Learning: the FBI-loss",
    "authors": [
      "Elisa Ferrari",
      "Davide Bacciu"
    ],
    "author_ids": [],
    "abstract": "Resilience to class imbalance and confounding biases, together with the\nassurance of fairness guarantees are highly desirable properties of autonomous\ndecision-making systems with real-life impact. Many different targeted\nsolutions have been proposed to address separately these three problems,\nhowever a unifying perspective seems to be missing. With this work, we provide\na general formalization, showing that they are different expressions of\nunbalance. Following this intuition, we formulate a unified loss correction to\naddress issues related to Fairness, Biases and Imbalances (FBI-loss). The\ncorrection capabilities of the proposed approach are assessed on three\nreal-world benchmarks, each associated to one of the issues under\nconsideration, and on a family of synthetic data in order to better investigate\nthe effectiveness of our loss on tasks with different complexities. The\nempirical results highlight that the flexible formulation of the FBI-loss leads\nalso to competitive performances with respect to literature solutions\nspecialised for the single problems.",
    "published_date": "2021-05-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.06345v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.06323v1",
    "title": "Bootstrapping User and Item Representations for One-Class Collaborative Filtering",
    "authors": [
      "Dongha Lee",
      "SeongKu Kang",
      "Hyunjun Ju",
      "Chanyoung Park",
      "Hwanjo Yu"
    ],
    "author_ids": [],
    "abstract": "The goal of one-class collaborative filtering (OCCF) is to identify the\nuser-item pairs that are positively-related but have not been interacted yet,\nwhere only a small portion of positive user-item interactions (e.g., users'\nimplicit feedback) are observed. For discriminative modeling between positive\nand negative interactions, most previous work relied on negative sampling to\nsome extent, which refers to considering unobserved user-item pairs as\nnegative, as actual negative ones are unknown. However, the negative sampling\nscheme has critical limitations because it may choose \"positive but unobserved\"\npairs as negative. This paper proposes a novel OCCF framework, named as BUIR,\nwhich does not require negative sampling. To make the representations of\npositively-related users and items similar to each other while avoiding a\ncollapsed solution, BUIR adopts two distinct encoder networks that learn from\neach other; the first encoder is trained to predict the output of the second\nencoder as its target, while the second encoder provides the consistent targets\nby slowly approximating the first encoder. In addition, BUIR effectively\nalleviates the data sparsity issue of OCCF, by applying stochastic data\naugmentation to encoder inputs. Based on the neighborhood information of users\nand items, BUIR randomly generates the augmented views of each positive\ninteraction each time it encodes, then further trains the model by this\nself-supervision. Our extensive experiments demonstrate that BUIR consistently\nand significantly outperforms all baseline methods by a large margin especially\nfor much sparse datasets in which any assumptions about negative interactions\nare less valid.",
    "published_date": "2021-05-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.06323v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.06067v1",
    "title": "Causal Intervention for Leveraging Popularity Bias in Recommendation",
    "authors": [
      "Yang Zhang",
      "Fuli Feng",
      "Xiangnan He",
      "Tianxin Wei",
      "Chonggang Song",
      "Guohui Ling",
      "Yongdong Zhang"
    ],
    "author_ids": [],
    "abstract": "Recommender system usually faces popularity bias issues: from the data\nperspective, items exhibit uneven (long-tail) distribution on the interaction\nfrequency; from the method perspective, collaborative filtering methods are\nprone to amplify the bias by over-recommending popular items. It is undoubtedly\ncritical to consider popularity bias in recommender systems, and existing work\nmainly eliminates the bias effect. However, we argue that not all biases in the\ndata are bad -- some items demonstrate higher popularity because of their\nbetter intrinsic quality. Blindly pursuing unbiased learning may remove the\nbeneficial patterns in the data, degrading the recommendation accuracy and user\nsatisfaction.\n  This work studies an unexplored problem in recommendation -- how to leverage\npopularity bias to improve the recommendation accuracy. The key lies in two\naspects: how to remove the bad impact of popularity bias during training, and\nhow to inject the desired popularity bias in the inference stage that generates\ntop-K recommendations. This questions the causal mechanism of the\nrecommendation generation process. Along this line, we find that item\npopularity plays the role of confounder between the exposed items and the\nobserved interactions, causing the bad effect of bias amplification. To achieve\nour goal, we propose a new training and inference paradigm for recommendation\nnamed Popularity-bias Deconfounding and Adjusting (PDA). It removes the\nconfounding popularity bias in model training and adjusts the recommendation\nscore with desired popularity bias via causal intervention. We demonstrate the\nnew paradigm on latent factor model and perform extensive experiments on three\nreal-world datasets. Empirical studies validate that the deconfounded training\nis helpful to discover user real interests and the inference adjustment with\npopularity bias could further improve the recommendation accuracy.",
    "published_date": "2021-05-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.06067v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.05956v3",
    "title": "2022 Roadmap on Neuromorphic Computing and Engineering",
    "authors": [
      "Dennis V. Christensen",
      "Regina Dittmann",
      "Bernabé Linares-Barranco",
      "Abu Sebastian",
      "Manuel Le Gallo",
      "Andrea Redaelli",
      "Stefan Slesazeck",
      "Thomas Mikolajick",
      "Sabina Spiga",
      "Stephan Menzel",
      "Ilia Valov",
      "Gianluca Milano",
      "Carlo Ricciardi",
      "Shi-Jun Liang",
      "Feng Miao",
      "Mario Lanza",
      "Tyler J. Quill",
      "Scott T. Keene",
      "Alberto Salleo",
      "Julie Grollier",
      "Danijela Marković",
      "Alice Mizrahi",
      "Peng Yao",
      "J. Joshua Yang",
      "Giacomo Indiveri",
      "John Paul Strachan",
      "Suman Datta",
      "Elisa Vianello",
      "Alexandre Valentian",
      "Johannes Feldmann",
      "Xuan Li",
      "Wolfram H. P. Pernice",
      "Harish Bhaskaran",
      "Steve Furber",
      "Emre Neftci",
      "Franz Scherr",
      "Wolfgang Maass",
      "Srikanth Ramaswamy",
      "Jonathan Tapson",
      "Priyadarshini Panda",
      "Youngeun Kim",
      "Gouhei Tanaka",
      "Simon Thorpe",
      "Chiara Bartolozzi",
      "Thomas A. Cleland",
      "Christoph Posch",
      "Shih-Chii Liu",
      "Gabriella Panuccio",
      "Mufti Mahmud",
      "Arnab Neelim Mazumder",
      "Morteza Hosseini",
      "Tinoosh Mohsenin",
      "Elisa Donati",
      "Silvia Tolu",
      "Roberto Galeazzi",
      "Martin Ejsing Christensen",
      "Sune Holm",
      "Daniele Ielmini",
      "N. Pryds"
    ],
    "author_ids": [],
    "abstract": "Modern computation based on the von Neumann architecture is today a mature\ncutting-edge science. In the Von Neumann architecture, processing and memory\nunits are implemented as separate blocks interchanging data intensively and\ncontinuously. This data transfer is responsible for a large part of the power\nconsumption. The next generation computer technology is expected to solve\nproblems at the exascale with 1018 calculations each second. Even though these\nfuture computers will be incredibly powerful, if they are based on von Neumann\ntype architectures, they will consume between 20 and 30 megawatts of power and\nwill not have intrinsic physically built-in capabilities to learn or deal with\ncomplex data as our brain does. These needs can be addressed by neuromorphic\ncomputing systems which are inspired by the biological concepts of the human\nbrain. This new generation of computers has the potential to be used for the\nstorage and processing of large amounts of digital information with much lower\npower consumption than conventional processors. Among their potential future\napplications, an important niche is moving the control from data centers to\nedge devices.\n  The aim of this Roadmap is to present a snapshot of the present state of\nneuromorphic technology and provide an opinion on the challenges and\nopportunities that the future holds in the major areas of neuromorphic\ntechnology, namely materials, devices, neuromorphic circuits, neuromorphic\nalgorithms, applications, and ethics. The Roadmap is a collection of\nperspectives where leading researchers in the neuromorphic community provide\ntheir own view about the current state and the future challenges. We hope that\nthis Roadmap will be a useful resource to readers outside this field, for those\nwho are just entering the field, and for those who are well established in the\nneuromorphic community. https://doi.org/10.1088/2634-4386/ac4a83",
    "published_date": "2021-05-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.ET",
      "cond-mat.dis-nn",
      "cond-mat.mtrl-sci"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.05956v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.05950v1",
    "title": "Identifying Biased Users in Online Social Networks to Enhance the Accuracy of Sentiment Analysis: A User Behavior-Based Approach",
    "authors": [
      "Amin Mahmoudi"
    ],
    "author_ids": [],
    "abstract": "The development of an automatic way to extract user opinions about products,\nmovies, and foods from online social network (OSN) interactions is among the\nmain interests of sentiment analysis and opinion mining studies. Existing\napproaches in the sentiment analysis domain mostly do not discriminate the\nsentences of different types of users, even though some users are always\nnegative and some are always positive. Thus, finding a way to identify these\ntwo types of user is significant because their attitudes can change the\nanalysis of user reviews of businesses and products. Due to the complexity of\nnatural language processing, pure text mining methods may lead to\nmisunderstandings about the exact nature of the sentiments expressed in review\ntext. In this study, we propose a neural network classifier to predict the\npresence of biased users on the basis of users' psychological behaviors. The\nidentification of the psychological behaviors of users allows us to find overly\npositive and overly negative users and to categorize these users' attitudes\nregardless of the content of their review texts. The experiment result\nindicates that the biased users can be predicted based on user behavior at an\naccuracy rate of 89%, 67% and 81% for three different datasets.",
    "published_date": "2021-05-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.05950v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.05779v4",
    "title": "Fairness in Information Access Systems",
    "authors": [
      "Michael D. Ekstrand",
      "Anubrata Das",
      "Robin Burke",
      "Fernando Diaz"
    ],
    "author_ids": [],
    "abstract": "Recommendation, information retrieval, and other information access systems\npose unique challenges for investigating and applying the fairness and\nnon-discrimination concepts that have been developed for studying other machine\nlearning systems. While fair information access shares many commonalities with\nfair classification, the multistakeholder nature of information access\napplications, the rank-based problem setting, the centrality of personalization\nin many cases, and the role of user response complicate the problem of\nidentifying precisely what types and operationalizations of fairness may be\nrelevant, let alone measuring or promoting them.\n  In this monograph, we present a taxonomy of the various dimensions of fair\ninformation access and survey the literature to date on this new and\nrapidly-growing topic. We preface this with brief introductions to information\naccess and algorithmic fairness, to facilitate use of this work by scholars\nwith experience in one (or neither) of these fields who wish to learn about\ntheir intersection. We conclude with several open problems in fair information\naccess, along with some suggestions for how to approach research in this space.",
    "published_date": "2021-05-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "H.3.3; K.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.05779v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.05736v1",
    "title": "Disentangling Sampling and Labeling Bias for Learning in Large-Output Spaces",
    "authors": [
      "Ankit Singh Rawat",
      "Aditya Krishna Menon",
      "Wittawat Jitkrittum",
      "Sadeep Jayasumana",
      "Felix X. Yu",
      "Sashank Reddi",
      "Sanjiv Kumar"
    ],
    "author_ids": [],
    "abstract": "Negative sampling schemes enable efficient training given a large number of\nclasses, by offering a means to approximate a computationally expensive loss\nfunction that takes all labels into account. In this paper, we present a new\nconnection between these schemes and loss modification techniques for\ncountering label imbalance. We show that different negative sampling schemes\nimplicitly trade-off performance on dominant versus rare labels. Further, we\nprovide a unified means to explicitly tackle both sampling bias, arising from\nworking with a subset of all labels, and labeling bias, which is inherent to\nthe data due to label imbalance. We empirically verify our findings on\nlong-tail classification and retrieval benchmarks.",
    "published_date": "2021-05-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.05736v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.05612v3",
    "title": "Evading the Simplicity Bias: Training a Diverse Set of Models Discovers Solutions with Superior OOD Generalization",
    "authors": [
      "Damien Teney",
      "Ehsan Abbasnejad",
      "Simon Lucey",
      "Anton van den Hengel"
    ],
    "author_ids": [],
    "abstract": "Neural networks trained with SGD were recently shown to rely preferentially\non linearly-predictive features and can ignore complex, equally-predictive\nones. This simplicity bias can explain their lack of robustness out of\ndistribution (OOD). The more complex the task to learn, the more likely it is\nthat statistical artifacts (i.e. selection biases, spurious correlations) are\nsimpler than the mechanisms to learn. We demonstrate that the simplicity bias\ncan be mitigated and OOD generalization improved. We train a set of similar\nmodels to fit the data in different ways using a penalty on the alignment of\ntheir input gradients. We show theoretically and empirically that this induces\nthe learning of more complex predictive patterns. OOD generalization\nfundamentally requires information beyond i.i.d. examples, such as multiple\ntraining environments, counterfactual examples, or other side information. Our\napproach shows that we can defer this requirement to an independent model\nselection stage. We obtain SOTA results in visual recognition on biased data\nand generalization across visual domains. The method - the first to evade the\nsimplicity bias - highlights the need for a better understanding and control of\ninductive biases in deep learning.",
    "published_date": "2021-05-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.05612v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.05595v1",
    "title": "An Introduction to Algorithmic Fairness",
    "authors": [
      "Hilde J. P. Weerts"
    ],
    "author_ids": [],
    "abstract": "In recent years, there has been an increasing awareness of both the public\nand scientific community that algorithmic systems can reproduce, amplify, or\neven introduce unfairness in our societies. These lecture notes provide an\nintroduction to some of the core concepts in algorithmic fairness research. We\nlist different types of fairness-related harms, explain two main notions of\nalgorithmic fairness, and map the biases that underlie these harms upon the\nmachine learning development process.",
    "published_date": "2021-05-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.05595v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.05553v8",
    "title": "Principal Components Bias in Over-parameterized Linear Models, and its Manifestation in Deep Neural Networks",
    "authors": [
      "Guy Hacohen",
      "Daphna Weinshall"
    ],
    "author_ids": [],
    "abstract": "Recent work suggests that convolutional neural networks of different\narchitectures learn to classify images in the same order. To understand this\nphenomenon, we revisit the over-parametrized deep linear network model. Our\nanalysis reveals that, when the hidden layers are wide enough, the convergence\nrate of this model's parameters is exponentially faster along the directions of\nthe larger principal components of the data, at a rate governed by the\ncorresponding singular values. We term this convergence pattern the Principal\nComponents bias (PC-bias). Empirically, we show how the PC-bias streamlines the\norder of learning of both linear and non-linear networks, more prominently at\nearlier stages of learning. We then compare our results to the simplicity bias,\nshowing that both biases can be seen independently, and affect the order of\nlearning in different ways. Finally, we discuss how the PC-bias may explain\nsome benefits of early stopping and its connection to PCA, and why deep\nnetworks converge more slowly with random labels.",
    "published_date": "2021-05-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.05553v8",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.05541v1",
    "title": "Evaluating Gender Bias in Natural Language Inference",
    "authors": [
      "Shanya Sharma",
      "Manan Dey",
      "Koustuv Sinha"
    ],
    "author_ids": [],
    "abstract": "Gender-bias stereotypes have recently raised significant ethical concerns in\nnatural language processing. However, progress in detection and evaluation of\ngender bias in natural language understanding through inference is limited and\nrequires further investigation. In this work, we propose an evaluation\nmethodology to measure these biases by constructing a challenge task that\ninvolves pairing gender-neutral premises against a gender-specific hypothesis.\nWe use our challenge task to investigate state-of-the-art NLI models on the\npresence of gender stereotypes using occupations. Our findings suggest that\nthree models (BERT, RoBERTa, BART) trained on MNLI and SNLI datasets are\nsignificantly prone to gender-induced prediction errors. We also find that\ndebiasing techniques such as augmenting the training dataset to ensure a\ngender-balanced dataset can help reduce such bias in certain cases.",
    "published_date": "2021-05-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.LG",
      "68T50",
      "I.2.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.05541v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.05308v3",
    "title": "Sequential Fair Allocation: Achieving the Optimal Envy-Efficiency Tradeoff Curve",
    "authors": [
      "Sean R. Sinclair",
      "Gauri Jain",
      "Siddhartha Banerjee",
      "Christina Lee Yu"
    ],
    "author_ids": [],
    "abstract": "We consider the problem of dividing limited resources to individuals arriving\nover $T$ rounds. Each round has a random number of individuals arrive, and\nindividuals can be characterized by their type (i.e. preferences over the\ndifferent resources). A standard notion of 'fairness' in this setting is that\nan allocation simultaneously satisfy envy-freeness and efficiency. The former\nis an individual guarantee, requiring that each agent prefers their own\nallocation over the allocation of any other; in contrast, efficiency is a\nglobal property, requiring that the allocations clear the available resources.\nFor divisible resources, when the number of individuals of each type are known\nupfront, the above desiderata are simultaneously achievable for a large class\nof utility functions. However, in an online setting when the number of\nindividuals of each type are only revealed round by round, no policy can\nguarantee these desiderata simultaneously, and hence the best one can do is to\ntry and allocate so as to approximately satisfy the two properties.\n  We show that in the online setting, the two desired properties (envy-freeness\nand efficiency) are in direct contention, in that any algorithm achieving\nadditive counterfactual envy-freeness up to a factor of $L_T$ necessarily\nsuffers a efficiency loss of at least $1 / L_T$. We complement this uncertainty\nprinciple with a simple algorithm, HopeGuardrail, which allocates resources\nbased on an adaptive threshold policy and is able to achieve any\nfairness-efficiency point on this frontier. In simulation results, our\nalgorithm provides allocations close to the optimal fair solution in hindsight,\nmotivating its use in practical applications as the algorithm is able to adapt\nto any desired fairness efficiency trade-off.",
    "published_date": "2021-05-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT",
      "cs.SY",
      "eess.SY",
      "math.OC",
      "91B32"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.05308v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.05066v1",
    "title": "ChaLearn LAP Large Scale Signer Independent Isolated Sign Language Recognition Challenge: Design, Results and Future Research",
    "authors": [
      "Ozge Mercanoglu Sincan",
      "Julio C. S. Jacques Junior",
      "Sergio Escalera",
      "Hacer Yalim Keles"
    ],
    "author_ids": [],
    "abstract": "The performances of Sign Language Recognition (SLR) systems have improved\nconsiderably in recent years. However, several open challenges still need to be\nsolved to allow SLR to be useful in practice. The research in the field is in\nits infancy in regards to the robustness of the models to a large diversity of\nsigns and signers, and to fairness of the models to performers from different\ndemographics. This work summarises the ChaLearn LAP Large Scale Signer\nIndependent Isolated SLR Challenge, organised at CVPR 2021 with the goal of\novercoming some of the aforementioned challenges. We analyse and discuss the\nchallenge design, top winning solutions and suggestions for future research.\nThe challenge attracted 132 participants in the RGB track and 59 in the\nRGB+Depth track, receiving more than 1.5K submissions in total. Participants\nwere evaluated using a new large-scale multi-modal Turkish Sign Language\n(AUTSL) dataset, consisting of 226 sign labels and 36,302 isolated sign video\nsamples performed by 43 different signers. Winning teams achieved more than 96%\nrecognition rate, and their approaches benefited from pose/hand/face\nestimation, transfer learning, external data, fusion/ensemble of modalities and\ndifferent strategies to model spatio-temporal information. However, methods\nstill fail to distinguish among very similar signs, in particular those sharing\nsimilar hand trajectories.",
    "published_date": "2021-05-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.05066v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.05020v1",
    "title": "Towards transparency in NLP shared tasks",
    "authors": [
      "Carla Parra Escartín",
      "Teresa Lynn",
      "Joss Moorkens",
      "Jane Dunne"
    ],
    "author_ids": [],
    "abstract": "This article reports on a survey carried out across the Natural Language\nProcessing (NLP) community. The survey aimed to capture the opinions of the\nresearch community on issues surrounding shared tasks, with respect to both\nparticipation and organisation. Amongst the 175 responses received, both\npositive and negative observations were made. We carried out and report on an\nextensive analysis of these responses, which leads us to propose a Shared Task\nOrganisation Checklist that could support future participants and organisers.\nThe proposed Checklist is flexible enough to accommodate the wide diversity of\nshared tasks in our field and its goal is not to be prescriptive, but rather to\nserve as a tool that encourages shared task organisers to foreground ethical\nbehaviour, beginning with the common issues that the 175 respondents deemed\nimportant. Its usage would not only serve as an instrument to reflect on\nimportant aspects of shared tasks, but would also promote increased\ntransparency around them.",
    "published_date": "2021-05-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.05020v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.04967v2",
    "title": "Open Set Domain Recognition via Attention-Based GCN and Semantic Matching Optimization",
    "authors": [
      "Xinxing He",
      "Yuan Yuan",
      "Zhiyu Jiang"
    ],
    "author_ids": [],
    "abstract": "Open set domain recognition has got the attention in recent years. The task\naims to specifically classify each sample in the practical unlabeled target\ndomain, which consists of all known classes in the manually labeled source\ndomain and target-specific unknown categories. The absence of annotated\ntraining data or auxiliary attribute information for unknown categories makes\nthis task especially difficult. Moreover, exiting domain discrepancy in label\nspace and data distribution further distracts the knowledge transferred from\nknown classes to unknown classes. To address these issues, this work presents\nan end-to-end model based on attention-based GCN and semantic matching\noptimization, which first employs the attention mechanism to enable the central\nnode to learn more discriminating representations from its neighbors in the\nknowledge graph. Moreover, a coarse-to-fine semantic matching optimization\napproach is proposed to progressively bridge the domain gap. Experimental\nresults validate that the proposed model not only has superiority on\nrecognizing the images of known and unknown classes, but also can adapt to\nvarious openness of the target domain.",
    "published_date": "2021-05-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.04967v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.04760v2",
    "title": "Unpacking the Expressed Consequences of AI Research in Broader Impact Statements",
    "authors": [
      "Priyanka Nanayakkara",
      "Jessica Hullman",
      "Nicholas Diakopoulos"
    ],
    "author_ids": [],
    "abstract": "The computer science research community and the broader public have become\nincreasingly aware of negative consequences of algorithmic systems. In\nresponse, the top-tier Neural Information Processing Systems (NeurIPS)\nconference for machine learning and artificial intelligence research required\nthat authors include a statement of broader impact to reflect on potential\npositive and negative consequences of their work. We present the results of a\nqualitative thematic analysis of a sample of statements written for the 2020\nconference. The themes we identify broadly fall into categories related to how\nconsequences are expressed (e.g., valence, specificity, uncertainty), areas of\nimpacts expressed (e.g., bias, the environment, labor, privacy), and\nresearchers' recommendations for mitigating negative consequences in the\nfuture. In light of our results, we offer perspectives on how the broader\nimpact statement can be implemented in future iterations to better align with\npotential goals.",
    "published_date": "2021-05-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.04760v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.04693v1",
    "title": "Emergence of Structural Bias in Differential Evolution",
    "authors": [
      "Bas van Stein",
      "Fabio Caraffini",
      "Anna V. Kononova"
    ],
    "author_ids": [],
    "abstract": "Heuristic optimisation algorithms are in high demand due to the overwhelming\namount of complex optimisation problems that need to be solved. The complexity\nof these problems is well beyond the boundaries of applicability of exact\noptimisation algorithms and therefore require modern heuristics to find\nfeasible solutions quickly. These heuristics and their effects are almost\nalways evaluated and explained by particular problem instances. In previous\nworks, it has been shown that many such algorithms show structural bias, by\neither being attracted to a certain region of the search space or by\nconsistently avoiding regions of the search space, on a special test function\ndesigned to ensure uniform 'exploration' of the domain. In this paper, we\nanalyse the emergence of such structural bias for Differential Evolution (DE)\nconfigurations and, specifically, the effect of different mutation, crossover\nand correction strategies. We also analyse the emergence of the structural bias\nduring the run-time of each algorithm. We conclude with recommendations of\nwhich configurations should be avoided in order to run DE unbiased.",
    "published_date": "2021-05-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.NE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.04693v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.04655v4",
    "title": "Causal Inference in medicine and in health policy, a summary",
    "authors": [
      "Wenhao Zhang",
      "Ramin Ramezani",
      "Arash Naeim"
    ],
    "author_ids": [],
    "abstract": "A data science task can be deemed as making sense of the data or testing a\nhypothesis about it. The conclusions inferred from data can greatly guide us to\nmake informative decisions. Big data has enabled us to carry out countless\nprediction tasks in conjunction with machine learning, such as identifying high\nrisk patients suffering from a certain disease and taking preventable measures.\nHowever, healthcare practitioners are not content with mere predictions - they\nare also interested in the cause-effect relation between input features and\nclinical outcomes. Understanding such relations will help doctors treat\npatients and reduce the risk effectively. Causality is typically identified by\nrandomized controlled trials. Often such trials are not feasible when\nscientists and researchers turn to observational studies and attempt to draw\ninferences. However, observational studies may also be affected by selection\nand/or confounding biases that can result in wrong causal conclusions. In this\nchapter, we will try to highlight some of the drawbacks that may arise in\ntraditional machine learning and statistical approaches to analyze the\nobservational data, particularly in the healthcare data analytics domain. We\nwill discuss causal inference and ways to discover the cause-effect from\nobservational studies in healthcare domain. Moreover, we will demonstrate the\napplications of causal inference in tackling some common machine learning\nissues such as missing data and model transportability. Finally, we will\ndiscuss the possibility of integrating reinforcement learning with causality as\na way to counter confounding bias.",
    "published_date": "2021-05-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.04655v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.04534v1",
    "title": "Improving Fairness of AI Systems with Lossless De-biasing",
    "authors": [
      "Yan Zhou",
      "Murat Kantarcioglu",
      "Chris Clifton"
    ],
    "author_ids": [],
    "abstract": "In today's society, AI systems are increasingly used to make critical\ndecisions such as credit scoring and patient triage. However, great convenience\nbrought by AI systems comes with troubling prevalence of bias against\nunderrepresented groups. Mitigating bias in AI systems to increase overall\nfairness has emerged as an important challenge. Existing studies on mitigating\nbias in AI systems focus on eliminating sensitive demographic information\nembedded in data. Given the temporal and contextual complexity of\nconceptualizing fairness, lossy treatment of demographic information may\ncontribute to an unnecessary trade-off between accuracy and fairness,\nespecially when demographic attributes and class labels are correlated. In this\npaper, we present an information-lossless de-biasing technique that targets the\nscarcity of data in the disadvantaged group. Unlike the existing work, we\ndemonstrate, both theoretically and empirically, that oversampling\nunderrepresented groups can not only mitigate algorithmic bias in AI systems\nthat consistently predict a favorable outcome for a certain group, but improve\noverall accuracy by mitigating class imbalance within data that leads to a bias\ntowards the majority class. We demonstrate the effectiveness of our technique\non real datasets using a variety of fairness metrics.",
    "published_date": "2021-05-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.04534v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.04480v1",
    "title": "Is there Anisotropy in Structural Bias?",
    "authors": [
      "Diederick Vermetten",
      "Anna V. Kononova",
      "Fabio Caraffini",
      "Hao Wang",
      "Thomas Bäck"
    ],
    "author_ids": [],
    "abstract": "Structural Bias (SB) is an important type of algorithmic deficiency within\niterative optimisation heuristics. However, methods for detecting structural\nbias have not yet fully matured, and recent studies have uncovered many\ninteresting questions. One of these is the question of how structural bias can\nbe related to anisotropy. Intuitively, an algorithm that is not isotropic would\nbe considered structurally biased. However, there have been cases where\nalgorithms appear to only show SB in some dimensions. As such, we investigate\nwhether these algorithms actually exhibit anisotropy, and how this impacts the\ndetection of SB. We find that anisotropy is very rare, and even in cases where\nit is present, there are clear tests for SB which do not rely on any\nassumptions of isotropy, so we can safely expand the suite of SB tests to\nencompass these kinds of deficiencies not found by the original tests.\n  We propose several additional testing procedures for SB detection and aim to\nmotivate further research into the creation of a robust portfolio of tests.\nThis is crucial since no single test will be able to work effectively with all\ntypes of SB we identify.",
    "published_date": "2021-05-10T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ME",
      "cs.NE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.04480v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.04452v1",
    "title": "Who Gets What, According to Whom? An Analysis of Fairness Perceptions in Service Allocation",
    "authors": [
      "Jacqueline Hannan",
      "Huei-Yen Winnie Chen",
      "Kenneth Joseph"
    ],
    "author_ids": [],
    "abstract": "Algorithmic fairness research has traditionally been linked to the\ndisciplines of philosophy, ethics, and economics, where notions of fairness are\nprescriptive and seek objectivity. Increasingly, however, scholars are turning\nto the study of what different people perceive to be fair, and how these\nperceptions can or should help to shape the design of machine learning,\nparticularly in the policy realm. The present work experimentally explores five\nnovel research questions at the intersection of the \"Who,\" \"What,\" and \"How\" of\nfairness perceptions. Specifically, we present the results of a multi-factor\nconjoint analysis study that quantifies the effects of the specific context in\nwhich a question is asked, the framing of the given question, and who is\nanswering it. Our results broadly suggest that the \"Who\" and \"What,\" at least,\nmatter in ways that are 1) not easily explained by any one theoretical\nperspective, 2) have critical implications for how perceptions of fairness\nshould be measured and/or integrated into algorithmic decision-making systems.",
    "published_date": "2021-05-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.04452v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.04273v1",
    "title": "Loss-Aversively Fair Classification",
    "authors": [
      "Junaid Ali",
      "Muhammad Bilal Zafar",
      "Adish Singla",
      "Krishna P. Gummadi"
    ],
    "author_ids": [],
    "abstract": "The use of algorithmic (learning-based) decision making in scenarios that\naffect human lives has motivated a number of recent studies to investigate such\ndecision making systems for potential unfairness, such as discrimination\nagainst subjects based on their sensitive features like gender or race.\nHowever, when judging the fairness of a newly designed decision making system,\nthese studies have overlooked an important influence on people's perceptions of\nfairness, which is how the new algorithm changes the status quo, i.e.,\ndecisions of the existing decision making system. Motivated by extensive\nliterature in behavioral economics and behavioral psychology (prospect theory),\nwe propose a notion of fair updates that we refer to as loss-averse updates.\nLoss-averse updates constrain the updates to yield improved (more beneficial)\noutcomes to subjects compared to the status quo. We propose tractable proxy\nmeasures that would allow this notion to be incorporated in the training of a\nvariety of linear and non-linear classifiers. We show how our proxy measures\ncan be combined with existing measures for training nondiscriminatory\nclassifiers. Our evaluation using synthetic and real-world datasets\ndemonstrates that the proposed proxy measures are effective for their desired\ntasks.",
    "published_date": "2021-05-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.04273v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.04249v1",
    "title": "Accounting for Model Uncertainty in Algorithmic Discrimination",
    "authors": [
      "Junaid Ali",
      "Preethi Lahoti",
      "Krishna P. Gummadi"
    ],
    "author_ids": [],
    "abstract": "Traditional approaches to ensure group fairness in algorithmic decision\nmaking aim to equalize ``total'' error rates for different subgroups in the\npopulation. In contrast, we argue that the fairness approaches should instead\nfocus only on equalizing errors arising due to model uncertainty (a.k.a\nepistemic uncertainty), caused due to lack of knowledge about the best model or\ndue to lack of data. In other words, our proposal calls for ignoring the errors\nthat occur due to uncertainty inherent in the data, i.e., aleatoric\nuncertainty. We draw a connection between predictive multiplicity and model\nuncertainty and argue that the techniques from predictive multiplicity could be\nused to identify errors made due to model uncertainty. We propose scalable\nconvex proxies to come up with classifiers that exhibit predictive multiplicity\nand empirically show that our methods are comparable in performance and up to\nfour orders of magnitude faster than the current state-of-the-art. We further\npropose methods to achieve our goal of equalizing group error rates arising due\nto model uncertainty in algorithmic decision making and demonstrate the\neffectiveness of these methods using synthetic and real-world datasets.",
    "published_date": "2021-05-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.04249v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.04144v1",
    "title": "Transitioning from Real to Synthetic data: Quantifying the bias in model",
    "authors": [
      "Aman Gupta",
      "Deepak Bhatt",
      "Anubha Pandey"
    ],
    "author_ids": [],
    "abstract": "With the advent of generative modeling techniques, synthetic data and its use\nhas penetrated across various domains from unstructured data such as image,\ntext to structured dataset modeling healthcare outcome, risk decisioning in\nfinancial domain, and many more. It overcomes various challenges such as\nlimited training data, class imbalance, restricted access to dataset owing to\nprivacy issues. To ensure the trained model used for automated decisioning\npurposes makes a fair decision there exist prior work to quantify and mitigate\nthose issues. This study aims to establish a trade-off between bias and\nfairness in the models trained using synthetic data. Variants of synthetic data\ngeneration techniques were studied to understand bias amplification including\ndifferentially private generation schemes. Through experiments on a tabular\ndataset, we demonstrate there exist a varying levels of bias impact on models\ntrained using synthetic data. Techniques generating less correlated feature\nperforms well as evident through fairness metrics with 94\\%, 82\\%, and 88\\%\nrelative drop in DPD (demographic parity difference), EoD (equality of odds)\nand EoP (equality of opportunity) respectively, and 24\\% relative improvement\nin DRP (demographic parity ratio) with respect to the real dataset. We believe\nthe outcome of our research study will help data science practitioners\nunderstand the bias in the use of synthetic data.",
    "published_date": "2021-05-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.04144v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.04077v1",
    "title": "Dynamic Multichannel Access via Multi-agent Reinforcement Learning: Throughput and Fairness Guarantees",
    "authors": [
      "Muhammad Sohaib",
      "Jongjin Jeong",
      "Sang-Woon Jeon"
    ],
    "author_ids": [],
    "abstract": "We consider a multichannel random access system in which each user accesses a\nsingle channel at each time slot to communicate with an access point (AP).\nUsers arrive to the system at random and be activated for a certain period of\ntime slots and then disappear from the system. Under such dynamic network\nenvironment, we propose a distributed multichannel access protocol based on\nmulti-agent reinforcement learning (RL) to improve both throughput and fairness\nbetween active users. Unlike the previous approaches adjusting channel access\nprobabilities at each time slot, the proposed RL algorithm deterministically\nselects a set of channel access policies for several consecutive time slots. To\neffectively reduce the complexity of the proposed RL algorithm, we adopt a\nbranching dueling Q-network architecture and propose an efficient training\nmethodology for producing proper Q-values over time-varying user sets. We\nperform extensive simulations on realistic traffic environments and demonstrate\nthat the proposed online learning improves both throughput and fairness\ncompared to the conventional RL approaches and centralized scheduling policies.",
    "published_date": "2021-05-10T00:00:00",
    "year": 2021,
    "categories": [
      "eess.SP",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.04077v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.04054v3",
    "title": "Societal Biases in Language Generation: Progress and Challenges",
    "authors": [
      "Emily Sheng",
      "Kai-Wei Chang",
      "Premkumar Natarajan",
      "Nanyun Peng"
    ],
    "author_ids": [],
    "abstract": "Technology for language generation has advanced rapidly, spurred by\nadvancements in pre-training large models on massive amounts of data and the\nneed for intelligent agents to communicate in a natural manner. While\ntechniques can effectively generate fluent text, they can also produce\nundesirable societal biases that can have a disproportionately negative impact\non marginalized populations. Language generation presents unique challenges for\nbiases in terms of direct user interaction and the structure of decoding\ntechniques. To better understand these challenges, we present a survey on\nsocietal biases in language generation, focusing on how data and techniques\ncontribute to biases and progress towards reducing biases. Motivated by a lack\nof studies on biases from decoding techniques, we also conduct experiments to\nquantify the effects of these techniques. By further discussing general trends\nand open challenges, we call to attention promising directions for research and\nthe importance of fairness and inclusivity considerations for language\ngeneration applications.",
    "published_date": "2021-05-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.04054v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.05041v1",
    "title": "English Accent Accuracy Analysis in a State-of-the-Art Automatic Speech Recognition System",
    "authors": [
      "Guillermo Cámbara",
      "Alex Peiró-Lilja",
      "Mireia Farrús",
      "Jordi Luque"
    ],
    "author_ids": [],
    "abstract": "Nowadays, research in speech technologies has gotten a lot out thanks to\nrecently created public domain corpora that contain thousands of recording\nhours. These large amounts of data are very helpful for training the new\ncomplex models based on deep learning technologies. However, the lack of\ndialectal diversity in a corpus is known to cause performance biases in speech\nsystems, mainly for underrepresented dialects. In this work, we propose to\nevaluate a state-of-the-art automatic speech recognition (ASR) deep\nlearning-based model, using unseen data from a corpus with a wide variety of\nlabeled English accents from different countries around the world. The model\nhas been trained with 44.5K hours of English speech from an open access corpus\ncalled Multilingual LibriSpeech, showing remarkable results in popular\nbenchmarks. We test the accuracy of such ASR against samples extracted from\nanother public corpus that is continuously growing, the Common Voice dataset.\nThen, we present graphically the accuracy in terms of Word Error Rate of each\nof the different English included accents, showing that there is indeed an\naccuracy bias in terms of accentual variety, favoring the accents most\nprevalent in the training corpus.",
    "published_date": "2021-05-09T00:00:00",
    "year": 2021,
    "categories": [
      "eess.AS",
      "cs.CL",
      "cs.LG",
      "cs.SD"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.05041v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.09742v1",
    "title": "Robustness of end-to-end Automatic Speech Recognition Models -- A Case Study using Mozilla DeepSpeech",
    "authors": [
      "Aashish Agarwal",
      "Torsten Zesch"
    ],
    "author_ids": [],
    "abstract": "When evaluating the performance of automatic speech recognition models,\nusually word error rate within a certain dataset is used. Special care must be\ntaken in understanding the dataset in order to report realistic performance\nnumbers. We argue that many performance numbers reported probably underestimate\nthe expected error rate. We conduct experiments controlling for selection bias,\ngender as well as overlap (between training and test data) in content, voices,\nand recording conditions. We find that content overlap has the biggest impact,\nbut other factors like gender also play a role.",
    "published_date": "2021-05-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.09742v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.03714v2",
    "title": "Consistency of Constrained Spectral Clustering under Graph Induced Fair Planted Partitions",
    "authors": [
      "Shubham Gupta",
      "Ambedkar Dukkipati"
    ],
    "author_ids": [],
    "abstract": "Spectral clustering is popular among practitioners and theoreticians alike.\nWhile performance guarantees for spectral clustering are well understood,\nrecent studies have focused on enforcing ``fairness'' in clusters, requiring\nthem to be ``balanced'' with respect to a categorical sensitive node attribute\n(e.g. the race distribution in clusters must match the race distribution in the\npopulation). In this paper, we consider a setting where sensitive attributes\nindirectly manifest in an auxiliary \\textit{representation graph} rather than\nbeing directly observed. This graph specifies node pairs that can represent\neach other with respect to sensitive attributes and is observed in addition to\nthe usual \\textit{similarity graph}. Our goal is to find clusters in the\nsimilarity graph while respecting a new individual-level fairness constraint\nencoded by the representation graph. We develop variants of unnormalized and\nnormalized spectral clustering for this task and analyze their performance\nunder a \\emph{fair} planted partition model induced by the representation\ngraph. This model uses both the cluster membership of the nodes and the\nstructure of the representation graph to generate random similarity graphs. To\nthe best of our knowledge, these are the first consistency results for\nconstrained spectral clustering under an individual-level fairness constraint.\nNumerical results corroborate our theoretical findings.",
    "published_date": "2021-05-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.03714v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.03693v3",
    "title": "Discrepancy and Sparsity",
    "authors": [
      "Mario Grobler",
      "Yiting Jiang",
      "Patrice Ossona de Mendez",
      "Sebastian Siebertz",
      "Alexandre Vigny"
    ],
    "author_ids": [],
    "abstract": "We study the connections between the notions of combinatorial discrepancy and\ngraph degeneracy. In particular, we prove that the maximum discrepancy over all\nsubgraphs $H$ of a graph $G$ of the neighborhood set system of $H$ is\nsandwiched between $\\Omega(\\log\\mathrm{deg}(G))$ and\n$\\mathcal{O}(\\mathrm{deg}(G))$, where $\\mathrm{deg}(G)$ denotes the degeneracy\nof $G$. We extend this result to inequalities relating weak coloring numbers\nand discrepancy of graph powers and deduce a new characterization of bounded\nexpansion classes.\n  Then, we switch to a model theoretical point of view, introduce pointer\nstructures, and study their relations to graph classes with bounded expansion.\nWe deduce that a monotone class of graphs has bounded expansion if and only if\nall the set systems definable in this class have bounded hereditary\ndiscrepancy.\n  Using known bounds on the VC-density of set systems definable in nowhere\ndense classes we also give a characterization of nowhere dense classes in terms\nof discrepancy. As consequences of our results, we obtain a corollary on the\ndiscrepancy of neighborhood set systems of edge colored graphs, a\npolynomial-time algorithm to compute $\\varepsilon$-approximations of size\n$\\mathcal{O}(1/\\varepsilon)$ for set systems definable in bounded expansion\nclasses, an application to clique coloring, and even the non-existence of a\nquantifier elimination scheme for nowhere dense classes.",
    "published_date": "2021-05-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DM",
      "cs.LO",
      "math.CO",
      "math.LO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.03693v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.03636v1",
    "title": "RISe of Flight: RIS-Empowered UAV Communications for Robust and Reliable Air-to-Ground Networks",
    "authors": [
      "Placido Mursia",
      "Francesco Devoti",
      "Vincenzo Sciancalepore",
      "Xavier Costa-Pérez"
    ],
    "author_ids": [],
    "abstract": "Next generation mobile networks need to expand towards uncharted territories\nin order to enable the digital transformation of society. In this context,\naerial devices such as unmanned aerial vehicles (UAVs) are expected to address\nthis gap in hard-to-reach locations. However, limited battery-life is an\nobstacle for the successful spread of such solutions. Reconfigurable\nintelligent surfaces (RISs) represent a promising solution addressing this\nchallenge since on-board passive and lightweight controllable devices can\nefficiently reflect the signal propagation from the ground BSs towards specific\ntarget areas. In this paper, we focus on air-to-ground networks where UAVs\nequipped with RIS can fly over selected areas to provide connectivity. In\nparticular, we study how to optimally compensate flight effects and propose\nRiFe as well as its practical implementation Fair-RiFe that automatically\nconfigure RIS parameters accounting for undesired UAV oscillations due to\nadverse atmospheric conditions. Our results show that both algorithms provide\nrobustness and reliability while outperforming state-of-the-art solutions in\nthe multiple conditions studied.",
    "published_date": "2021-05-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.03636v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.03632v2",
    "title": "CASIA-Face-Africa: A Large-scale African Face Image Database",
    "authors": [
      "Jawad Muhammad",
      "Yunlong Wang",
      "Caiyong Wang",
      "Kunbo Zhang",
      "Zhenan Sun"
    ],
    "author_ids": [],
    "abstract": "Face recognition is a popular and well-studied area with wide applications in\nour society. However, racial bias had been proven to be inherent in most State\nOf The Art (SOTA) face recognition systems. Many investigative studies on face\nrecognition algorithms have reported higher false positive rates of African\nsubjects cohorts than the other cohorts. Lack of large-scale African face image\ndatabases in public domain is one of the main restrictions in studying the\nracial bias problem of face recognition. To this end, we collect a face image\ndatabase namely CASIA-Face-Africa which contains 38,546 images of 1,183 African\nsubjects. Multi-spectral cameras are utilized to capture the face images under\nvarious illumination settings. Demographic attributes and facial expressions of\nthe subjects are also carefully recorded. For landmark detection, each face\nimage in the database is manually labeled with 68 facial keypoints. A group of\nevaluation protocols are constructed according to different applications,\ntasks, partitions and scenarios. The performances of SOTA face recognition\nalgorithms without re-training are reported as baselines. The proposed database\nalong with its face landmark annotations, evaluation protocols and preliminary\nresults form a good benchmark to study the essential aspects of face biometrics\nfor African subjects, especially face image preprocessing, face feature\nanalysis and matching, facial expression recognition, sex/age estimation,\nethnic classification, face image generation, etc. The database can be\ndownloaded from our http://www.cripacsir.cn/dataset/",
    "published_date": "2021-05-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.03632v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.03568v1",
    "title": "ChaRRNets: Channel Robust Representation Networks for RF Fingerprinting",
    "authors": [
      "Carter N. Brown",
      "Enrico Mattei",
      "Andrew Draganov"
    ],
    "author_ids": [],
    "abstract": "We present complex-valued Convolutional Neural Networks (CNNs) for RF\nfingerprinting that go beyond translation invariance and appropriately account\nfor the inductive bias with respect to multipath propagation channels, a\nphenomenon that is specific to the fields of wireless signal processing and\ncommunications. We focus on the problem of fingerprinting wireless IoT devices\nin-the-wild using Deep Learning (DL) techniques. Under these real-world\nconditions, the multipath environments represented in the train and test sets\nwill be different. These differences are due to the physics governing the\npropagation of wireless signals, as well as the limitations of practical data\ncollection campaigns. Our approach follows a group-theoretic framework,\nleverages prior work on DL on manifold-valued data, and extends this prior work\nto the wireless signal processing domain. We introduce the Lie group of\ntransformations that a signal experiences under the multipath propagation model\nand define operations that are equivariant and invariant to the frequency\nresponse of a Finite Impulse Response (FIR) filter to build a ChaRRNet. We\npresent results using synthetic and real-world datasets, and we benchmark\nagainst a strong baseline model, that show the efficacy of our approach. Our\nresults provide evidence of the benefits of incorporating appropriate wireless\ndomain biases into DL models. We hope to spur new work in the area of robust RF\nmachine learning, as the 5G revolution increases demand for enhanced security\nmechanisms.",
    "published_date": "2021-05-08T00:00:00",
    "year": 2021,
    "categories": [
      "eess.SP",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.03568v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.04408v1",
    "title": "The Challenges and Opportunities of Human-Centered AI for Trustworthy Robots and Autonomous Systems",
    "authors": [
      "Hongmei He",
      "John Gray",
      "Angelo Cangelosi",
      "Qinggang Meng",
      "T. Martin McGinnity",
      "Jörn Mehnen"
    ],
    "author_ids": [],
    "abstract": "The trustworthiness of Robots and Autonomous Systems (RAS) has gained a\nprominent position on many research agendas towards fully autonomous systems.\nThis research systematically explores, for the first time, the key facets of\nhuman-centered AI (HAI) for trustworthy RAS. In this article, five key\nproperties of a trustworthy RAS initially have been identified. RAS must be (i)\nsafe in any uncertain and dynamic surrounding environments; (ii) secure, thus\nprotecting itself from any cyber-threats; (iii) healthy with fault tolerance;\n(iv) trusted and easy to use to allow effective human-machine interaction\n(HMI), and (v) compliant with the law and ethical expectations. Then, the\nchallenges in implementing trustworthy autonomous system are analytically\nreviewed, in respects of the five key properties, and the roles of AI\ntechnologies have been explored to ensure the trustiness of RAS with respects\nto safety, security, health and HMI, while reflecting the requirements of\nethics in the design of RAS. While applications of RAS have mainly focused on\nperformance and productivity, the risks posed by advanced AI in RAS have not\nreceived sufficient scientific attention. Hence, a new acceptance model of RAS\nis provided, as a framework for requirements to human-centered AI and for\nimplementing trustworthy RAS by design. This approach promotes human-level\nintelligence to augment human's capacity. while focusing on contributions to\nhumanity.",
    "published_date": "2021-05-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.RO",
      "cs.AI",
      "68T05, 68T40",
      "I.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.04408v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.03192v1",
    "title": "An interdisciplinary conceptual study of Artificial Intelligence (AI) for helping benefit-risk assessment practices: Towards a comprehensive qualification matrix of AI programs and devices (pre-print 2020)",
    "authors": [
      "Gauthier Chassang",
      "Mogens Thomsen",
      "Pierre Rumeau",
      "Florence Sèdes",
      "Alejandra Delfin"
    ],
    "author_ids": [],
    "abstract": "This paper proposes a comprehensive analysis of existing concepts coming from\ndifferent disciplines tackling the notion of intelligence, namely psychology\nand engineering, and from disciplines aiming to regulate AI innovations, namely\nAI ethics and law. The aim is to identify shared notions or discrepancies to\nconsider for qualifying AI systems. Relevant concepts are integrated into a\nmatrix intended to help defining more precisely when and how computing tools\n(programs or devices) may be qualified as AI while highlighting critical\nfeatures to serve a specific technical, ethical and legal assessment of\nchallenges in AI development. Some adaptations of existing notions of AI\ncharacteristics are proposed. The matrix is a risk-based conceptual model\ndesigned to allow an empirical, flexible and scalable qualification of AI\ntechnologies in the perspective of benefit-risk assessment practices,\ntechnological monitoring and regulatory compliance: it offers a structured\nreflection tool for stakeholders in AI development that are engaged in\nresponsible research and innovation.Pre-print version (achieved on May 2020)",
    "published_date": "2021-05-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.03192v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.03153v2",
    "title": "Pairwise Fairness for Ordinal Regression",
    "authors": [
      "Matthäus Kleindessner",
      "Samira Samadi",
      "Muhammad Bilal Zafar",
      "Krishnaram Kenthapadi",
      "Chris Russell"
    ],
    "author_ids": [],
    "abstract": "We initiate the study of fairness for ordinal regression. We adapt two\nfairness notions previously considered in fair ranking and propose a strategy\nfor training a predictor that is approximately fair according to either notion.\nOur predictor has the form of a threshold model, composed of a scoring function\nand a set of thresholds, and our strategy is based on a reduction to fair\nbinary classification for learning the scoring function and local search for\nchoosing the thresholds. We provide generalization guarantees on the error and\nfairness violation of our predictor, and we illustrate the effectiveness of our\napproach in extensive experiments.",
    "published_date": "2021-05-07T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.03153v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.02951v3",
    "title": "Multi-FR: A Multi-objective Optimization Framework for Multi-stakeholder Fairness-aware Recommendation",
    "authors": [
      "Haolun Wu",
      "Chen Ma",
      "Bhaskar Mitra",
      "Fernando Diaz",
      "Xue Liu"
    ],
    "author_ids": [],
    "abstract": "Nowadays, most online services are hosted on multi-stakeholder marketplaces,\nwhere consumers and producers may have different objectives. Conventional\nrecommendation systems, however, mainly focus on maximizing consumers'\nsatisfaction by recommending the most relevant items to each individual. This\nmay result in unfair exposure of items, thus jeopardizing producer benefits.\nAdditionally, they do not care whether consumers from diverse demographic\ngroups are equally satisfied. To address these limitations, we propose a\nmulti-objective optimization framework for fairness-aware recommendation,\nMulti-FR, that adaptively balances accuracy and fairness for various\nstakeholders with Pareto optimality guarantee. We first propose four fairness\nconstraints on consumers and producers. In order to train the whole framework\nin an end-to-end way, we utilize the smooth rank and stochastic ranking policy\nto make these fairness criteria differentiable and friendly to\nback-propagation. Then, we adopt the multiple gradient descent algorithm to\ngenerate a Pareto set of solutions, from which the most appropriate one is\nselected by the Least Misery Strategy. The experimental results demonstrate\nthat Multi-FR largely improves recommendation fairness on multiple stakeholders\nover the state-of-the-art approaches while maintaining almost the same\nrecommendation accuracy. The training efficiency study confirms our model's\nability to simultaneously optimize different fairness constraints for many\nstakeholders efficiently.",
    "published_date": "2021-05-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.02951v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.02949v1",
    "title": "The Dynamics of Faculty Hiring Networks",
    "authors": [
      "Eun Lee",
      "Aaron Clauset",
      "Daniel B. Larremore"
    ],
    "author_ids": [],
    "abstract": "Faculty hiring networks-who hires whose graduates as faculty-exhibit steep\nhierarchies, which can reinforce both social and epistemic inequalities in\nacademia. Understanding the mechanisms driving these patterns would inform\nefforts to diversify the academy and shed new light on the role of hiring in\nshaping which scientific discoveries are made. Here, we investigate the degree\nto which structural mechanisms can explain hierarchy and other network\ncharacteristics observed in empirical faculty hiring networks. We study a\nfamily of adaptive rewiring network models, which reinforce institutional\nprestige within the hierarchy in five distinct ways. Each mechanism determines\nthe probability that a new hire comes from a particular institution according\nto that institution's prestige score, which is inferred from the hiring\nnetwork's existing structure. We find that structural inequalities and\ncentrality patterns in real hiring networks are best reproduced by a mechanism\nof global placement power, in which a new hire is drawn from a particular\ninstitution in proportion to the number of previously drawn hires anywhere. On\nthe other hand, network measures of biased visibility are better recapitulated\nby a mechanism of local placement power, in which a new hire is drawn from a\nparticular institution in proportion to the number of its previous hires\nalready present at the hiring institution. These contrasting results suggest\nthat the underlying structural mechanism reinforcing hierarchies in faculty\nhiring networks is a mixture of global and local preference for institutional\nprestige. Under these dynamics, we show that each institution's position in the\nhierarchy is remarkably stable, due to a dynamic competition that\noverwhelmingly favors more prestigious institutions.",
    "published_date": "2021-05-06T00:00:00",
    "year": 2021,
    "categories": [
      "physics.soc-ph",
      "cs.CY",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.02949v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.02936v1",
    "title": "Exact Acceleration of K-Means++ and K-Means$\\|$",
    "authors": [
      "Edward Raff"
    ],
    "author_ids": [],
    "abstract": "K-Means++ and its distributed variant K-Means$\\|$ have become de facto tools\nfor selecting the initial seeds of K-means. While alternatives have been\ndeveloped, the effectiveness, ease of implementation, and theoretical grounding\nof the K-means++ and $\\|$ methods have made them difficult to \"best\" from a\nholistic perspective. By considering the limited opportunities within seed\nselection to perform pruning, we develop specialized triangle inequality\npruning strategies and a dynamic priority queue to show the first acceleration\nof K-Means++ and K-Means$\\|$ that is faster in run-time while being\nalgorithmicly equivalent. For both algorithms we are able to reduce distance\ncomputations by over $500\\times$. For K-means++ this results in up to a\n17$\\times$ speedup in run-time and a $551\\times$ speedup for K-means$\\|$. We\nachieve this with simple, but carefully chosen, modifications to known\ntechniques which makes it easy to integrate our approach into existing\nimplementations of these algorithms.",
    "published_date": "2021-05-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MS",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.02936v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.02851v1",
    "title": "Algorithmic Ethics: Formalization and Verification of Autonomous Vehicle Obligations",
    "authors": [
      "Colin Shea-Blymyer",
      "Houssam Abbas"
    ],
    "author_ids": [],
    "abstract": "We develop a formal framework for automatic reasoning about the obligations\nof autonomous cyber-physical systems, including their social and ethical\nobligations. Obligations, permissions and prohibitions are distinct from a\nsystem's mission, and are a necessary part of specifying advanced, adaptive\nAI-equipped systems. They need a dedicated deontic logic of obligations to\nformalize them. Most existing deontic logics lack corresponding algorithms and\nsystem models that permit automatic verification. We demonstrate how a\nparticular deontic logic, Dominance Act Utilitarianism (DAU), is a suitable\nstarting point for formalizing the obligations of autonomous systems like\nself-driving cars. We demonstrate its usefulness by formalizing a subset of\nResponsibility-Sensitive Safety (RSS) in DAU; RSS is an industrial proposal for\nhow self-driving cars should and should not behave in traffic. We show that\ncertain logical consequences of RSS are undesirable, indicating a need to\nfurther refine the proposal. We also demonstrate how obligations can change\nover time, which is necessary for long-term autonomy. We then demonstrate a\nmodel-checking algorithm for DAU formulas on weighted transition systems, and\nillustrate it by model-checking obligations of a self-driving car controller\nfrom the literature.",
    "published_date": "2021-05-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.02851v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.02778v1",
    "title": "The Authors Matter: Understanding and Mitigating Implicit Bias in Deep Text Classification",
    "authors": [
      "Haochen Liu",
      "Wei Jin",
      "Hamid Karimi",
      "Zitao Liu",
      "Jiliang Tang"
    ],
    "author_ids": [],
    "abstract": "It is evident that deep text classification models trained on human data\ncould be biased. In particular, they produce biased outcomes for texts that\nexplicitly include identity terms of certain demographic groups. We refer to\nthis type of bias as explicit bias, which has been extensively studied.\nHowever, deep text classification models can also produce biased outcomes for\ntexts written by authors of certain demographic groups. We refer to such bias\nas implicit bias of which we still have a rather limited understanding. In this\npaper, we first demonstrate that implicit bias exists in different text\nclassification tasks for different demographic groups. Then, we build a\nlearning-based interpretation method to deepen our knowledge of implicit bias.\nSpecifically, we verify that classifiers learn to make predictions based on\nlanguage features that are related to the demographic attributes of the\nauthors. Next, we propose a framework Debiased-TC to train deep text\nclassifiers to make predictions on the right features and consequently mitigate\nimplicit bias. We conduct extensive experiments on three real-world datasets.\nThe results show that the text classification models trained under our proposed\nframework outperform traditional models significantly in terms of fairness, and\nalso slightly in terms of classification performance.",
    "published_date": "2021-05-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.02778v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.02751v3",
    "title": "On the Ethical Limits of Natural Language Processing on Legal Text",
    "authors": [
      "Dimitrios Tsarapatsanis",
      "Nikolaos Aletras"
    ],
    "author_ids": [],
    "abstract": "Natural language processing (NLP) methods for analyzing legal text offer\nlegal scholars and practitioners a range of tools allowing to empirically\nanalyze law on a large scale. However, researchers seem to struggle when it\ncomes to identifying ethical limits to using NLP systems for acquiring genuine\ninsights both about the law and the systems' predictive capacity. In this paper\nwe set out a number of ways in which to think systematically about such issues.\nWe place emphasis on three crucial normative parameters which have, to the best\nof our knowledge, been underestimated by current debates: (a) the importance of\nacademic freedom, (b) the existence of a wide diversity of legal and ethical\nnorms domestically but even more so internationally and (c) the threat of\nmoralism in research related to computational law. For each of these three\nparameters we provide specific recommendations for the legal NLP community. Our\ndiscussion is structured around the study of a real-life scenario that has\nprompted recent debate in the legal NLP research community.",
    "published_date": "2021-05-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.02751v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.02738v2",
    "title": "Digital Voodoo Dolls",
    "authors": [
      "Marija Slavkovik",
      "Clemens Stachl",
      "Caroline Pitman",
      "Jonathan Askonas"
    ],
    "author_ids": [],
    "abstract": "An institution, be it a body of government, commercial enterprise, or a\nservice, cannot interact directly with a person. Instead, a model is created to\nrepresent us. We argue the existence of a new high-fidelity type of person\nmodel which we call a digital voodoo doll. We conceptualize it and compare its\nfeatures with existing models of persons. Digital voodoo dolls are\ndistinguished by existing completely beyond the influence and control of the\nperson they represent. We discuss the ethical issues that such a lack of\naccountability creates and argue how these concerns can be mitigated.",
    "published_date": "2021-05-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.02738v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.02725v2",
    "title": "CrossWalk: Fairness-enhanced Node Representation Learning",
    "authors": [
      "Ahmad Khajehnejad",
      "Moein Khajehnejad",
      "Mahmoudreza Babaei",
      "Krishna P. Gummadi",
      "Adrian Weller",
      "Baharan Mirzasoleiman"
    ],
    "author_ids": [],
    "abstract": "The potential for machine learning systems to amplify social inequities and\nunfairness is receiving increasing popular and academic attention. Much recent\nwork has focused on developing algorithmic tools to assess and mitigate such\nunfairness. However, there is little work on enhancing fairness in graph\nalgorithms. Here, we develop a simple, effective and general method, CrossWalk,\nthat enhances fairness of various graph algorithms, including influence\nmaximization, link prediction and node classification, applied to node\nembeddings. CrossWalk is applicable to any random walk based node\nrepresentation learning algorithm, such as DeepWalk and Node2Vec. The key idea\nis to bias random walks to cross group boundaries, by upweighting edges which\n(1) are closer to the groups' peripheries or (2) connect different groups in\nthe network. CrossWalk pulls nodes that are near groups' peripheries towards\ntheir neighbors from other groups in the embedding space, while preserving the\nnecessary structural properties of the graph. Extensive experiments show the\neffectiveness of our algorithm to enhance fairness in various graph algorithms,\nincluding influence maximization, link prediction and node classification in\nsynthetic and real networks, with only a very small decrease in performance.",
    "published_date": "2021-05-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.SI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.02725v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.02637v1",
    "title": "Dataset Bias in the Natural Sciences: A Case Study in Chemical Reaction Prediction and Synthesis Design",
    "authors": [
      "Ryan-Rhys Griffiths",
      "Philippe Schwaller",
      "Alpha A. Lee"
    ],
    "author_ids": [],
    "abstract": "Datasets in the Natural Sciences are often curated with the goal of aiding\nscientific understanding and hence may not always be in a form that facilitates\nthe application of machine learning. In this paper, we identify three trends\nwithin the fields of chemical reaction prediction and synthesis design that\nrequire a change in direction. First, the manner in which reaction datasets are\nsplit into reactants and reagents encourages testing models in an\nunrealistically generous manner. Second, we highlight the prevalence of\nmislabelled data, and suggest that the focus should be on outlier removal\nrather than data fitting only. Lastly, we discuss the problem of reagent\nprediction, in addition to reactant prediction, in order to solve the full\nsynthesis design problem, highlighting the mismatch between what machine\nlearning solves and what a lab chemist would need. Our critiques are also\nrelevant to the burgeoning field of using machine learning to accelerate\nprogress in experimental Natural Sciences, where datasets are often split in a\nbiased way, are highly noisy, and contextual variables that are not evident\nfrom the data strongly influence the outcome of experiments.",
    "published_date": "2021-05-06T00:00:00",
    "year": 2021,
    "categories": [
      "physics.chem-ph",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.02637v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.02590v3",
    "title": "Reliability Testing for Natural Language Processing Systems",
    "authors": [
      "Samson Tan",
      "Shafiq Joty",
      "Kathy Baxter",
      "Araz Taeihagh",
      "Gregory A. Bennett",
      "Min-Yen Kan"
    ],
    "author_ids": [],
    "abstract": "Questions of fairness, robustness, and transparency are paramount to address\nbefore deploying NLP systems. Central to these concerns is the question of\nreliability: Can NLP systems reliably treat different demographics fairly and\nfunction correctly in diverse and noisy environments? To address this, we argue\nfor the need for reliability testing and contextualize it among existing work\non improving accountability. We show how adversarial attacks can be reframed\nfor this goal, via a framework for developing reliability tests. We argue that\nreliability testing -- with an emphasis on interdisciplinary collaboration --\nwill enable rigorous and targeted testing, and aid in the enactment and\nenforcement of industry standards.",
    "published_date": "2021-05-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.NE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.02590v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.02481v1",
    "title": "MAFER: a Multi-resolution Approach to Facial Expression Recognition",
    "authors": [
      "Fabio Valerio Massoli",
      "Donato Cafarelli",
      "Claudio Gennaro",
      "Giuseppe Amato",
      "Fabrizio Falchi"
    ],
    "author_ids": [],
    "abstract": "Emotions play a central role in the social life of every human being, and\ntheir study, which represents a multidisciplinary subject, embraces a great\nvariety of research fields. Especially concerning the latter, the analysis of\nfacial expressions represents a very active research area due to its relevance\nto human-computer interaction applications. In such a context, Facial\nExpression Recognition (FER) is the task of recognizing expressions on human\nfaces. Typically, face images are acquired by cameras that have, by nature,\ndifferent characteristics, such as the output resolution. It has been already\nshown in the literature that Deep Learning models applied to face recognition\nexperience a degradation in their performance when tested against\nmulti-resolution scenarios. Since the FER task involves analyzing face images\nthat can be acquired with heterogeneous sources, thus involving images with\ndifferent quality, it is plausible to expect that resolution plays an important\nrole in such a case too. Stemming from such a hypothesis, we prove the benefits\nof multi-resolution training for models tasked with recognizing facial\nexpressions. Hence, we propose a two-step learning procedure, named MAFER, to\ntrain DCNNs to empower them to generate robust predictions across a wide range\nof resolutions. A relevant feature of MAFER is that it is task-agnostic, i.e.,\nit can be used complementarily to other objective-related techniques. To assess\nthe effectiveness of the proposed approach, we performed an extensive\nexperimental campaign on publicly available datasets: \\fer{}, \\raf{}, and\n\\oulu{}. For a multi-resolution context, we observe that with our approach,\nlearning models improve upon the current SotA while reporting comparable\nresults in fix-resolution contexts. Finally, we analyze the performance of our\nmodels and observe the higher discrimination power of deep features generated\nfrom them.",
    "published_date": "2021-05-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "68T07"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.02481v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.02439v1",
    "title": "Weakly Supervised Action Selection Learning in Video",
    "authors": [
      "Junwei Ma",
      "Satya Krishna Gorti",
      "Maksims Volkovs",
      "Guangwei Yu"
    ],
    "author_ids": [],
    "abstract": "Localizing actions in video is a core task in computer vision. The weakly\nsupervised temporal localization problem investigates whether this task can be\nadequately solved with only video-level labels, significantly reducing the\namount of expensive and error-prone annotation that is required. A common\napproach is to train a frame-level classifier where frames with the highest\nclass probability are selected to make a video-level prediction. Frame level\nactivations are then used for localization. However, the absence of frame-level\nannotations cause the classifier to impart class bias on every frame. To\naddress this, we propose the Action Selection Learning (ASL) approach to\ncapture the general concept of action, a property we refer to as \"actionness\".\nUnder ASL, the model is trained with a novel class-agnostic task to predict\nwhich frames will be selected by the classifier. Empirically, we show that ASL\noutperforms leading baselines on two popular benchmarks THUMOS-14 and\nActivityNet-1.2, with 10.3% and 5.7% relative improvement respectively. We\nfurther analyze the properties of ASL and demonstrate the importance of\nactionness. Full code for this work is available here:\nhttps://github.com/layer6ai-labs/ASL.",
    "published_date": "2021-05-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.02439v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.02414v1",
    "title": "Person Retrieval in Surveillance Using Textual Query: A Review",
    "authors": [
      "Hiren Galiyawala",
      "Mehul S Raval"
    ],
    "author_ids": [],
    "abstract": "Recent advancement of research in biometrics, computer vision, and natural\nlanguage processing has discovered opportunities for person retrieval from\nsurveillance videos using textual query. The prime objective of a surveillance\nsystem is to locate a person using a description, e.g., a short woman with a\npink t-shirt and white skirt carrying a black purse. She has brown hair. Such a\ndescription contains attributes like gender, height, type of clothing, colour\nof clothing, hair colour, and accessories. Such attributes are formally known\nas soft biometrics. They help bridge the semantic gap between a human\ndescription and a machine as a textual query contains the person's soft\nbiometric attributes. It is also not feasible to manually search through huge\nvolumes of surveillance footage to retrieve a specific person. Hence, automatic\nperson retrieval using vision and language-based algorithms is becoming\npopular. In comparison to other state-of-the-art reviews, the contribution of\nthe paper is as follows: 1. Recommends most discriminative soft biometrics for\nspecifiic challenging conditions. 2. Integrates benchmark datasets and\nretrieval methods for objective performance evaluation. 3. A complete snapshot\nof techniques based on features, classifiers, number of soft biometric\nattributes, type of the deep neural networks, and performance measures. 4. The\ncomprehensive coverage of person retrieval from handcrafted features based\nmethods to end-to-end approaches based on natural language description.",
    "published_date": "2021-05-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.02414v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.02407v1",
    "title": "Reconfiguring Diversity and Inclusion for AI Ethics",
    "authors": [
      "Nicole Chi",
      "Emma Lurie",
      "Deirdre K. Mulligan"
    ],
    "author_ids": [],
    "abstract": "Activists, journalists, and scholars have long raised critical questions\nabout the relationship between diversity, representation, and structural\nexclusions in data-intensive tools and services. We build on work mapping the\nemergent landscape of corporate AI ethics to center one outcome of these\nconversations: the incorporation of diversity and inclusion in corporate AI\nethics activities. Using interpretive document analysis and analytic tools from\nthe values in design field, we examine how diversity and inclusion work is\narticulated in public-facing AI ethics documentation produced by three\ncompanies that create application and services layer AI infrastructure: Google,\nMicrosoft, and Salesforce.\n  We find that as these documents make diversity and inclusion more tractable\nto engineers and technical clients, they reveal a drift away from civil rights\njustifications that resonates with the managerialization of diversity by\ncorporations in the mid-1980s. The focus on technical artifacts, such as\ndiverse and inclusive datasets, and the replacement of equity with fairness\nmake ethical work more actionable for everyday practitioners. Yet, they appear\ndivorced from broader DEI initiatives and other subject matter experts that\ncould provide needed context to nuanced decisions around how to operationalize\nthese values. Finally, diversity and inclusion, as configured by engineering\nlogic, positions firms not as ethics owners but as ethics allocators; while\nthese companies claim expertise on AI ethics, the responsibility of defining\nwho diversity and inclusion are meant to protect and where it is relevant is\npushed downstream to their customers.",
    "published_date": "2021-05-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.02407v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.02340v1",
    "title": "DeepSMOTE: Fusing Deep Learning and SMOTE for Imbalanced Data",
    "authors": [
      "Damien Dablain",
      "Bartosz Krawczyk",
      "Nitesh V. Chawla"
    ],
    "author_ids": [],
    "abstract": "Despite over two decades of progress, imbalanced data is still considered a\nsignificant challenge for contemporary machine learning models. Modern advances\nin deep learning have magnified the importance of the imbalanced data problem.\nThe two main approaches to address this issue are based on loss function\nmodifications and instance resampling. Instance sampling is typically based on\nGenerative Adversarial Networks (GANs), which may suffer from mode collapse.\nTherefore, there is a need for an oversampling method that is specifically\ntailored to deep learning models, can work on raw images while preserving their\nproperties, and is capable of generating high quality, artificial images that\ncan enhance minority classes and balance the training set. We propose DeepSMOTE\n- a novel oversampling algorithm for deep learning models. It is simple, yet\neffective in its design. It consists of three major components: (i) an\nencoder/decoder framework; (ii) SMOTE-based oversampling; and (iii) a dedicated\nloss function that is enhanced with a penalty term. An important advantage of\nDeepSMOTE over GAN-based oversampling is that DeepSMOTE does not require a\ndiscriminator, and it generates high-quality artificial images that are both\ninformation-rich and suitable for visual inspection. DeepSMOTE code is publicly\navailable at: https://github.com/dd1github/DeepSMOTE",
    "published_date": "2021-05-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.02340v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.02317v1",
    "title": "A Step Toward More Inclusive People Annotations for Fairness",
    "authors": [
      "Candice Schumann",
      "Susanna Ricco",
      "Utsav Prabhu",
      "Vittorio Ferrari",
      "Caroline Pantofaru"
    ],
    "author_ids": [],
    "abstract": "The Open Images Dataset contains approximately 9 million images and is a\nwidely accepted dataset for computer vision research. As is common practice for\nlarge datasets, the annotations are not exhaustive, with bounding boxes and\nattribute labels for only a subset of the classes in each image. In this paper,\nwe present a new set of annotations on a subset of the Open Images dataset\ncalled the MIAP (More Inclusive Annotations for People) subset, containing\nbounding boxes and attributes for all of the people visible in those images.\nThe attributes and labeling methodology for the MIAP subset were designed to\nenable research into model fairness. In addition, we analyze the original\nannotation methodology for the person class and its subclasses, discussing the\nresulting patterns in order to inform future annotation efforts. By considering\nboth the original and exhaustive annotation sets, researchers can also now\nstudy how systematic patterns in training annotations affect modeling.",
    "published_date": "2021-05-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.02317v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.02117v1",
    "title": "Ethics and Governance of Artificial Intelligence: Evidence from a Survey of Machine Learning Researchers",
    "authors": [
      "Baobao Zhang",
      "Markus Anderljung",
      "Lauren Kahn",
      "Noemi Dreksler",
      "Michael C. Horowitz",
      "Allan Dafoe"
    ],
    "author_ids": [],
    "abstract": "Machine learning (ML) and artificial intelligence (AI) researchers play an\nimportant role in the ethics and governance of AI, including taking action\nagainst what they perceive to be unethical uses of AI (Belfield, 2020; Van\nNoorden, 2020). Nevertheless, this influential group's attitudes are not well\nunderstood, which undermines our ability to discern consensuses or\ndisagreements between AI/ML researchers. To examine these researchers' views,\nwe conducted a survey of those who published in the top AI/ML conferences (N =\n524). We compare these results with those from a 2016 survey of AI/ML\nresearchers (Grace, Salvatier, Dafoe, Zhang, & Evans, 2018) and a 2018 survey\nof the US public (Zhang & Dafoe, 2020). We find that AI/ML researchers place\nhigh levels of trust in international organizations and scientific\norganizations to shape the development and use of AI in the public interest;\nmoderate trust in most Western tech companies; and low trust in national\nmilitaries, Chinese tech companies, and Facebook. While the respondents were\noverwhelmingly opposed to AI/ML researchers working on lethal autonomous\nweapons, they are less opposed to researchers working on other military\napplications of AI, particularly logistics algorithms. A strong majority of\nrespondents think that AI safety research should be prioritized and that ML\ninstitutions should conduct pre-publication review to assess potential harms.\nBeing closer to the technology itself, AI/ML re-searchers are well placed to\nhighlight new risks and develop technical solutions, so this novel attempt to\nmeasure their attitudes has broad relevance. The findings should help to\nimprove how researchers, private sector executives, and policymakers think\nabout regulations, governance frameworks, guiding principles, and national and\ninternational governance strategies for AI.",
    "published_date": "2021-05-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "K.7.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.02117v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.02091v2",
    "title": "When Fair Ranking Meets Uncertain Inference",
    "authors": [
      "Avijit Ghosh",
      "Ritam Dutt",
      "Christo Wilson"
    ],
    "author_ids": [],
    "abstract": "Existing fair ranking systems, especially those designed to be\ndemographically fair, assume that accurate demographic information about\nindividuals is available to the ranking algorithm. In practice, however, this\nassumption may not hold -- in real-world contexts like ranking job applicants\nor credit seekers, social and legal barriers may prevent algorithm operators\nfrom collecting peoples' demographic information. In these cases, algorithm\noperators may attempt to infer peoples' demographics and then supply these\ninferences as inputs to the ranking algorithm.\n  In this study, we investigate how uncertainty and errors in demographic\ninference impact the fairness offered by fair ranking algorithms. Using\nsimulations and three case studies with real datasets, we show how demographic\ninferences drawn from real systems can lead to unfair rankings. Our results\nsuggest that developers should not use inferred demographic data as input to\nfair ranking algorithms, unless the inferences are extremely accurate.",
    "published_date": "2021-05-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.02091v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.01959v1",
    "title": "Attack-agnostic Adversarial Detection on Medical Data Using Explainable Machine Learning",
    "authors": [
      "Matthew Watson",
      "Noura Al Moubayed"
    ],
    "author_ids": [],
    "abstract": "Explainable machine learning has become increasingly prevalent, especially in\nhealthcare where explainable models are vital for ethical and trusted automated\ndecision making. Work on the susceptibility of deep learning models to\nadversarial attacks has shown the ease of designing samples to mislead a model\ninto making incorrect predictions. In this work, we propose a model agnostic\nexplainability-based method for the accurate detection of adversarial samples\non two datasets with different complexity and properties: Electronic Health\nRecord (EHR) and chest X-ray (CXR) data. On the MIMIC-III and Henan-Renmin EHR\ndatasets, we report a detection accuracy of 77% against the Longitudinal\nAdversarial Attack. On the MIMIC-CXR dataset, we achieve an accuracy of 88%;\nsignificantly improving on the state of the art of adversarial detection in\nboth datasets by over 10% in all settings. We propose an anomaly detection\nbased method using explainability techniques to detect adversarial samples\nwhich is able to generalise to different attack methods without a need for\nretraining.",
    "published_date": "2021-05-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CR",
      "I.2; I.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.01959v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.01801v1",
    "title": "Fair and Truthful Mechanism with Limited Subsidy",
    "authors": [
      "Hiromichi Goko",
      "Ayumi Igarashi",
      "Yasushi Kawase",
      "Kazuhisa Makino",
      "Hanna Sumita",
      "Akihisa Tamura",
      "Yu Yokoi",
      "Makoto Yokoo"
    ],
    "author_ids": [],
    "abstract": "The notion of \\emph{envy-freeness} is a natural and intuitive fairness\nrequirement in resource allocation. With indivisible goods, such fair\nallocations are unfortunately not guaranteed to exist. Classical works have\navoided this issue by introducing an additional divisible resource, i.e.,\nmoney, to subsidize envious agents. In this paper, we aim to design a truthful\nallocation mechanism of indivisible goods to achieve both fairness and\nefficiency criteria with a limited amount of subsidy. Following the work of\nHalpern and Shah, our central question is as follows: to what extent do we need\nto rely on the power of money to accomplish these objectives? For general\nvaluations, the impossibility theorem of combinatorial auction translates to\nour setting: even if an arbitrarily large amount of money is available for use,\nno mechanism can achieve truthfulness, envy-freeness, and utilitarian\noptimality simultaneously when agents have general monotone submodular\nvaluations. By contrast, we show that, when agents have matroidal valuations,\nthere is a truthful allocation mechanism that achieves envy-freeness and\nutilitarian optimality by subsidizing each agent with at most $1$, the maximum\nmarginal contribution of each item for each agent. The design of the mechanism\nrests crucially on the underlying matroidal M-convexity of the Lorenz\ndominating allocations.",
    "published_date": "2021-05-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.01801v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.01774v2",
    "title": "Envisioning Communities: A Participatory Approach Towards AI for Social Good",
    "authors": [
      "Elizabeth Bondi",
      "Lily Xu",
      "Diana Acosta-Navas",
      "Jackson A. Killian"
    ],
    "author_ids": [],
    "abstract": "Research in artificial intelligence (AI) for social good presupposes some\ndefinition of social good, but potential definitions have been seldom suggested\nand never agreed upon. The normative question of what AI for social good\nresearch should be \"for\" is not thoughtfully elaborated, or is frequently\naddressed with a utilitarian outlook that prioritizes the needs of the majority\nover those who have been historically marginalized, brushing aside realities of\ninjustice and inequity. We argue that AI for social good ought to be assessed\nby the communities that the AI system will impact, using as a guide the\ncapabilities approach, a framework to measure the ability of different policies\nto improve human welfare equity. Furthermore, we lay out how AI research has\nthe potential to catalyze social progress by expanding and equalizing\ncapabilities. We show how the capabilities approach aligns with a participatory\napproach for the design and implementation of AI for social good research in a\nframework we introduce called PACT, in which community members affected should\nbe brought in as partners and their input prioritized throughout the project.\nWe conclude by providing an incomplete set of guiding questions for carrying\nout such participatory AI research in a way that elicits and respects a\ncommunity's own definition of social good.",
    "published_date": "2021-05-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.01774v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.02693v1",
    "title": "Uncertainty-aware INVASE: Enhanced Breast Cancer Diagnosis Feature Selection",
    "authors": [
      "Jia-Xing Zhong",
      "Hongbo Zhang"
    ],
    "author_ids": [],
    "abstract": "In this paper, we present an uncertainty-aware INVASE to quantify predictive\nconfidence of healthcare problem. By introducing learnable Gaussian\ndistributions, we lever-age their variances to measure the degree of\nuncertainty. Based on the vanilla INVASE, two additional modules are proposed,\ni.e., an uncertainty quantification module in the predictor, and a reward\nshaping module in the selector. We conduct extensive experiments on UCI-WDBC\ndataset. Notably, our method eliminates almost all predictive bias with only\nabout 20% queries, while the uncertainty-agnostic counterpart requires nearly\n100% queries. The open-source implementation with a detailed tutorial is\navailable at\nhttps://github.com/jx-zhong-for-academic-purpose/Uncertainty-aware-INVASE/blob/main/tutorialinvase%2B.ipynb.",
    "published_date": "2021-05-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.02693v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.01609v2",
    "title": "Almost Envy-Freeness for Groups: Improved Bounds via Discrepancy Theory",
    "authors": [
      "Pasin Manurangsi",
      "Warut Suksompong"
    ],
    "author_ids": [],
    "abstract": "We study the allocation of indivisible goods among groups of agents using\nwell-known fairness notions such as envy-freeness and proportionality. While\nthese notions cannot always be satisfied, we provide several bounds on the\noptimal relaxations that can be guaranteed. For instance, our bounds imply that\nwhen the number of groups is constant and the $n$ agents are divided into\ngroups arbitrarily, there exists an allocation that is envy-free up to\n$\\Theta(\\sqrt{n})$ goods, and this bound is tight. Moreover, we show that while\nsuch an allocation can be found efficiently, it is NP-hard to compute an\nallocation that is envy-free up to $o(\\sqrt{n})$ goods even when a fully\nenvy-free allocation exists. Our proofs make extensive use of tools from\ndiscrepancy theory.",
    "published_date": "2021-05-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT",
      "cs.DM",
      "math.CO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.01609v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.01567v1",
    "title": "Towards Error Measures which Influence a Learners Inductive Bias to the Ground Truth",
    "authors": [
      "A. I. Parkes",
      "A. J. Sobey",
      "D. A. Hudson"
    ],
    "author_ids": [],
    "abstract": "Artificial intelligence is applied in a range of sectors, and is relied upon\nfor decisions requiring a high level of trust. For regression methods, trust is\nincreased if they approximate the true input-output relationships and perform\naccurately outside the bounds of the training data. But often performance\noff-test-set is poor, especially when data is sparse. This is because the\nconditional average, which in many scenarios is a good approximation of the\n`ground truth', is only modelled with conventional Minkowski-r error measures\nwhen the data set adheres to restrictive assumptions, with many real data sets\nviolating these. To combat this there are several methods that use prior\nknowledge to approximate the `ground truth'. However, prior knowledge is not\nalways available, and this paper investigates how error measures affect the\nability for a regression method to model the `ground truth' in these scenarios.\nCurrent error measures are shown to create an unhelpful bias and a new error\nmeasure is derived which does not exhibit this behaviour. This is tested on 36\nrepresentative data sets with different characteristics, showing that it is\nmore consistent in determining the `ground truth' and in giving improved\npredictions in regions beyond the range of the training data.",
    "published_date": "2021-05-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.01567v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.01468v1",
    "title": "Sentiment and Emotion Classification of Epidemic Related Bilingual data from Social Media",
    "authors": [
      "Muhammad Zain Ali",
      "Kashif Javed",
      "Ehsan ul Haq",
      "Anoshka Tariq"
    ],
    "author_ids": [],
    "abstract": "In recent years, sentiment analysis and emotion classification are two of the\nmost abundantly used techniques in the field of Natural Language Processing\n(NLP). Although sentiment analysis and emotion classification are used commonly\nin applications such as analyzing customer reviews, the popularity of\ncandidates contesting in elections, and comments about various sporting events;\nhowever, in this study, we have examined their application for epidemic\noutbreak detection. Early outbreak detection is the key to deal with epidemics\neffectively, however, the traditional ways of outbreak detection are\ntime-consuming which inhibits prompt response from the respective departments.\nSocial media platforms such as Twitter, Facebook, Instagram, etc. allow the\nusers to express their thoughts related to different aspects of life, and\ntherefore, serve as a substantial source of information in such situations. The\nproposed study exploits the bilingual (Urdu and English) data from Twitter and\nNEWS websites related to the dengue epidemic in Pakistan, and sentiment\nanalysis and emotion classification are performed to acquire deep insights from\nthe data set for gaining a fair idea related to an epidemic outbreak. Machine\nlearning and deep learning algorithms have been used to train and implement the\nmodels for the execution of both tasks. The comparative performance of each\nmodel has been evaluated using accuracy, precision, recall, and f1-measure.",
    "published_date": "2021-05-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.01468v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.01441v2",
    "title": "Distributive Justice and Fairness Metrics in Automated Decision-making: How Much Overlap Is There?",
    "authors": [
      "Matthias Kuppler",
      "Christoph Kern",
      "Ruben L. Bach",
      "Frauke Kreuter"
    ],
    "author_ids": [],
    "abstract": "The advent of powerful prediction algorithms led to increased automation of\nhigh-stake decisions regarding the allocation of scarce resources such as\ngovernment spending and welfare support. This automation bears the risk of\nperpetuating unwanted discrimination against vulnerable and historically\ndisadvantaged groups. Research on algorithmic discrimination in computer\nscience and other disciplines developed a plethora of fairness metrics to\ndetect and correct discriminatory algorithms. Drawing on robust sociological\nand philosophical discourse on distributive justice, we identify the\nlimitations and problematic implications of prominent fairness metrics. We show\nthat metrics implementing equality of opportunity only apply when resource\nallocations are based on deservingness, but fail when allocations should\nreflect concerns about egalitarianism, sufficiency, and priority. We argue that\nby cleanly distinguishing between prediction tasks and decision tasks, research\non fair machine learning could take better advantage of the rich literature on\ndistributive justice.",
    "published_date": "2021-05-04T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.01441v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.01400v1",
    "title": "Coin Flipping of \\emph{Any} Constant Bias Implies One-Way Functions",
    "authors": [
      "Itay Berman",
      "Iftach Haitner",
      "Aris Tentes"
    ],
    "author_ids": [],
    "abstract": "We show that the existence of a coin-flipping protocol safe against\n\\emph{any} non-trivial constant bias (\\eg $.499$) implies the existence of\none-way functions. This improves upon a recent result of Haitner and Omri [FOCS\n'11], who proved this implication for protocols with bias $\\frac{\\sqrt2 -1}2 -\no(1) \\approx .207$. Unlike the result of Haitner and Omri, our result also\nholds for \\emph{weak} coin-flipping protocols.",
    "published_date": "2021-05-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.01400v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.01241v3",
    "title": "End-to-end One-shot Human Parsing",
    "authors": [
      "Haoyu He",
      "Bohan Zhuang",
      "Jing Zhang",
      "Jianfei Cai",
      "Dacheng Tao"
    ],
    "author_ids": [],
    "abstract": "Previous human parsing methods are limited to parsing humans into pre-defined\nclasses, which is inflexible for practical fashion applications that often have\nnew fashion item classes. In this paper, we define a novel one-shot human\nparsing (OSHP) task that requires parsing humans into an open set of classes\ndefined by any test example. During training, only base classes are exposed,\nwhich only overlap with part of the test-time classes. To address three main\nchallenges in OSHP, i.e., small sizes, testing bias, and similar parts, we\ndevise an End-to-end One-shot human Parsing Network (EOP-Net). Firstly, an\nend-to-end human parsing framework is proposed to parse the query image into\nboth coarse-grained and fine-grained human classes, which embeds rich semantic\ninformation that is shared across different granularities to identify the\nsmall-sized human classes. Then, we gradually smooth the training-time static\nprototypes to get robust class representations. Moreover, we employ a dynamic\nobjective to encourage the network's enhancing features' representational\ncapability in the early training phase while improving features'\ntransferability in the late training phase. Therefore, our method can quickly\nadapt to the novel classes and mitigate the testing bias issue. In addition, we\nadd a contrastive loss at the prototype level to enforce inter-class distances,\nthereby discriminating the similar parts. For comprehensive evaluations on the\nnew task, we tailor three existing popular human parsing benchmarks to the OSHP\ntask. Experiments demonstrate that EOP-Net outperforms representative one-shot\nsegmentation models by large margins and serves as a strong baseline for\nfurther research. The source code is available at\nhttps://github.com/Charleshhy/One-shot-Human-Parsing.",
    "published_date": "2021-05-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.01241v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.01198v2",
    "title": "Weighted Least Squares Twin Support Vector Machine with Fuzzy Rough Set Theory for Imbalanced Data Classification",
    "authors": [
      "Maysam Behmanesh",
      "Peyman Adibi",
      "Hossein Karshenas"
    ],
    "author_ids": [],
    "abstract": "Support vector machines (SVMs) are powerful supervised learning tools\ndeveloped to solve classification problems. However, SVMs are likely to perform\npoorly in the classification of imbalanced data. The rough set theory presents\na mathematical tool for inference in nondeterministic cases that provides\nmethods for removing irrelevant information from data. In this work, we propose\nan approach that efficiently used fuzzy rough set theory in weighted least\nsquares twin support vector machine called FRLSTSVM for classification of\nimbalanced data. The first innovation is introducing a new fuzzy rough\nset-based under-sampling strategy to make the classifier robust in terms of the\nimbalanced data. For constructing the two proximal hyperplanes in FRLSTSVM,\ndata points from the minority class remain unchanged while a subset of data\npoints in the majority class are selected using a new method. In this model, we\nembed the weight biases in the LSTSVM formulations to overcome the bias\nphenomenon in the original twin SVM for the classification of imbalanced data.\nIn order to determine these weights in this formulation, we introduce a new\nstrategy that uses fuzzy rough set theory as the second innovation.\nExperimental results on the famous imbalanced datasets, compared to the related\ntraditional SVM-based methods, demonstrate the superiority of the proposed\nFRLSTSVM model in the imbalanced data classification.",
    "published_date": "2021-05-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.01198v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.01149v1",
    "title": "Near-Optimal Cayley Expanders for Abelian Groups",
    "authors": [
      "Akhil Jalan",
      "Dana Moshkovitz"
    ],
    "author_ids": [],
    "abstract": "We give an efficient deterministic algorithm that outputs an expanding\ngenerating set for any finite abelian group. The size of the generating set is\nclose to the randomized construction of Alon and Roichman (1994), improving\nupon various deterministic constructions in both the dependence on the\ndimension and the spectral gap. By obtaining optimal dependence on the\ndimension we resolve a conjecture of Azar, Motwani, and Naor (1998) in the\naffirmative. Our technique is an extension of the bias amplification technique\nof Ta-Shma (2017), who used random walks on expanders to obtain expanding\ngenerating sets over the additive group of n-bit strings. As a consequence, we\nobtain (i) randomness-efficient constructions of almost k-wise independent\nvariables, (ii) a faster deterministic algorithm for the Remote Point Problem,\n(iii) randomness-efficient low-degree tests, and (iv) randomness-efficient\nverification of matrix multiplication.",
    "published_date": "2021-05-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DS",
      "cs.CC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.01149v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.01031v1",
    "title": "Algorithms are not neutral: Bias in collaborative filtering",
    "authors": [
      "Catherine Stinson"
    ],
    "author_ids": [],
    "abstract": "Discussions of algorithmic bias tend to focus on examples where either the\ndata or the people building the algorithms are biased. This gives the\nimpression that clean data and good intentions could eliminate bias. The\nneutrality of the algorithms themselves is defended by prominent Artificial\nIntelligence researchers. However, algorithms are not neutral. In addition to\nbiased data and biased algorithm makers, AI algorithms themselves can be\nbiased. This is illustrated with the example of collaborative filtering, which\nis known to suffer from popularity, and homogenizing biases. Iterative\ninformation filtering algorithms in general create a selection bias in the\ncourse of learning from user responses to documents that the algorithm\nrecommended. These are not merely biases in the statistical sense; these\nstatistical biases can cause discriminatory outcomes. Data points on the\nmargins of distributions of human data tend to correspond to marginalized\npeople. Popularity and homogenizing biases have the effect of further\nmarginalizing the already marginal. This source of bias warrants serious\nattention given the ubiquity of algorithmic decision-making.",
    "published_date": "2021-05-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.01031v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.00962v2",
    "title": "From Fairness to Full Security in Multiparty Computation",
    "authors": [
      "Ran Cohen",
      "Iftach Haitner",
      "Eran Omri",
      "Lior Rotem"
    ],
    "author_ids": [],
    "abstract": "In the setting of secure multiparty computation (MPC), a set of mutually\ndistrusting parties wish to jointly compute a function, while guaranteeing the\nprivacy of their inputs and the correctness of the output. An MPC protocol is\ncalled \\emph{fully secure} if no adversary can prevent the honest parties from\nobtaining their outputs. A protocol is called \\emph{fair} if an adversary can\nprematurely abort the computation, however, only before learning any new\ninformation.\n  We present highly efficient transformations from fair computations to fully\nsecure computations, assuming the fraction of honest parties is constant (e.g.,\n$1\\%$ of the parties are honest). Compared to previous transformations that\nrequire linear invocations (in the number of parties) of the fair computation,\nour transformations require super-logarithmic, and sometimes even\nsuper-constant, such invocations. The main idea is to delegate the computation\nto chosen random committees that invoke the fair computation. Apart from the\nbenefit of uplifting security, the reduction in the number of parties is also\nuseful, since only committee members are required to work, whereas the\nremaining parties simply \"listen\" to the computation over a broadcast channel.",
    "published_date": "2021-05-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.00962v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.00908v3",
    "title": "Impact of Gender Debiased Word Embeddings in Language Modeling",
    "authors": [
      "Christine Basta",
      "Marta R. Costa-jussà"
    ],
    "author_ids": [],
    "abstract": "Gender, race and social biases have recently been detected as evident\nexamples of unfairness in applications of Natural Language Processing. A key\npath towards fairness is to understand, analyse and interpret our data and\nalgorithms. Recent studies have shown that the human-generated data used in\ntraining is an apparent factor of getting biases. In addition, current\nalgorithms have also been proven to amplify biases from data.\n  To further address these concerns, in this paper, we study how an\nstate-of-the-art recurrent neural language model behaves when trained on data,\nwhich under-represents females, using pre-trained standard and debiased word\nembeddings. Results show that language models inherit higher bias when trained\non unbalanced data when using pre-trained embeddings, in comparison with using\nembeddings trained within the task. Moreover, results show that, on the same\ndata, language models inherit lower bias when using debiased pre-trained\nemdeddings, compared to using standard pre-trained embeddings.",
    "published_date": "2021-05-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.00908v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.00855v2",
    "title": "Computationally Efficient Optimization of Plackett-Luce Ranking Models for Relevance and Fairness",
    "authors": [
      "Harrie Oosterhuis"
    ],
    "author_ids": [],
    "abstract": "Recent work has proposed stochastic Plackett-Luce (PL) ranking models as a\nrobust choice for optimizing relevance and fairness metrics. Unlike their\ndeterministic counterparts that require heuristic optimization algorithms, PL\nmodels are fully differentiable. Theoretically, they can be used to optimize\nranking metrics via stochastic gradient descent. However, in practice, the\ncomputation of the gradient is infeasible because it requires one to iterate\nover all possible permutations of items. Consequently, actual applications rely\non approximating the gradient via sampling techniques. In this paper, we\nintroduce a novel algorithm: PL-Rank, that estimates the gradient of a PL\nranking model w.r.t. both relevance and fairness metrics. Unlike existing\napproaches that are based on policy gradients, PL-Rank makes use of the\nspecific structure of PL models and ranking metrics. Our experimental analysis\nshows that PL-Rank has a greater sample-efficiency and is computationally less\ncostly than existing policy gradients, resulting in faster convergence at\nhigher performance. PL-Rank further enables the industry to apply PL models for\nmore relevant and fairer real-world ranking systems.",
    "published_date": "2021-05-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.00855v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.00850v2",
    "title": "An Almost-Optimally Fair Three-Party Coin-Flipping Protocol",
    "authors": [
      "Iftach Haitner",
      "Eliad Tsfadia"
    ],
    "author_ids": [],
    "abstract": "In a multiparty fair coin-flipping protocol, the parties output a common\n(close to) unbiased bit, even when some corrupted parties try to bias the\noutput. Cleve [STOC 1986] has shown that in the case of dishonest majority\n(i.e., at least half of the parties can be corrupted), in any $m$-round\ncoin-flipping protocol the corrupted parties can bias the honest parties'\ncommon output bit by $\\Omega(\\frac1{m})$. For more than two decades the best\nknown coin-flipping protocols against dishonest majority had bias\n$\\Theta(\\frac{\\ell}{\\sqrt{m}})$, where $\\ell$ is the number of corrupted\nparties. This was changed by a recent breakthrough result of Moran et al. [TCC\n2009], who constructed an $m$-round, two-party coin-flipping protocol with\noptimal bias $\\Theta(\\frac1{m})$. In a subsequent work, Beimel et al. [Crypto\n2010] extended this result to the multiparty case in which less than $\\frac23$\nof the parties can be corrupted. Still for the case of $\\frac23$ (or more)\ncorrupted parties, the best known protocol had bias\n$\\Theta(\\frac{\\ell}{\\sqrt{m}})$. In particular, this was the state of affairs\nfor the natural three-party case.\n  We make a step towards eliminating the above gap, presenting an $m$-round,\nthree-party coin-flipping protocol, with bias $\\frac{O(\\log^3 m)}m$. Our\napproach (which we also apply for the two-party case) does not follow the\n\"threshold round\" paradigm used in the work of Moran et al. and Beimel et al.,\nbut rather is a variation of the majority protocol of Cleve, used to obtain the\naforementioned $\\Theta(\\frac{\\ell}{\\sqrt{m}})$-bias protocol.",
    "published_date": "2021-05-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.00850v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.00756v1",
    "title": "The Matter of Chance: Auditing Web Search Results Related to the 2020 U.S. Presidential Primary Elections Across Six Search Engines",
    "authors": [
      "Aleksandra Urman",
      "Mykola Makhortykh",
      "Roberto Ulloa"
    ],
    "author_ids": [],
    "abstract": "We examine how six search engines filter and rank information in relation to\nthe queries on the U.S. 2020 presidential primary elections under the default -\nthat is nonpersonalized - conditions. For that, we utilize an algorithmic\nauditing methodology that uses virtual agents to conduct large-scale analysis\nof algorithmic information curation in a controlled environment. Specifically,\nwe look at the text search results for \"us elections\", \"donald trump\", \"joe\nbiden\" and \"bernie sanders\" queries on Google, Baidu, Bing, DuckDuckGo, Yahoo,\nand Yandex, during the 2020 primaries. Our findings indicate substantial\ndifferences in the search results between search engines and multiple\ndiscrepancies within the results generated for different agents using the same\nsearch engine. It highlights that whether users see certain information is\ndecided by chance due to the inherent randomization of search results. We also\nfind that some search engines prioritize different categories of information\nsources with respect to specific candidates. These observations demonstrate\nthat algorithmic curation of political information can create information\ninequalities between the search engine users even under nonpersonalized\nconditions. Such inequalities are particularly troubling considering that\nsearch results are highly trusted by the public and can shift the opinions of\nundecided voters as demonstrated by previous research.",
    "published_date": "2021-05-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.00756v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.00741v1",
    "title": "MLCheck- Property-Driven Testing of Machine Learning Models",
    "authors": [
      "Arnab Sharma",
      "Caglar Demir",
      "Axel-Cyrille Ngonga Ngomo",
      "Heike Wehrheim"
    ],
    "author_ids": [],
    "abstract": "In recent years, we observe an increasing amount of software with machine\nlearning components being deployed. This poses the question of quality\nassurance for such components: how can we validate whether specified\nrequirements are fulfilled by a machine learned software? Current testing and\nverification approaches either focus on a single requirement (e.g., fairness)\nor specialize on a single type of machine learning model (e.g., neural\nnetworks). In this paper, we propose property-driven testing of machine\nlearning models. Our approach MLCheck encompasses (1) a language for property\nspecification, and (2) a technique for systematic test case generation. The\nspecification language is comparable to property-based testing languages. Test\ncase generation employs advanced verification technology for a systematic,\nproperty-dependent construction of test suites, without additional\nuser-supplied generator functions. We evaluate MLCheck using requirements and\ndata sets from three different application areas (software discrimination,\nlearning on knowledge graphs and security). Our evaluation shows that despite\nits generality MLCheck can even outperform specialised testing approaches while\nhaving a comparable runtime.",
    "published_date": "2021-05-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.00741v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.00674v1",
    "title": "Bias in Knowledge Graphs -- an Empirical Study with Movie Recommendation and Different Language Editions of DBpedia",
    "authors": [
      "Michael Matthias Voit",
      "Heiko Paulheim"
    ],
    "author_ids": [],
    "abstract": "Public knowledge graphs such as DBpedia and Wikidata have been recognized as\ninteresting sources of background knowledge to build content-based recommender\nsystems. They can be used to add information about the items to be recommended\nand links between those. While quite a few approaches for exploiting knowledge\ngraphs have been proposed, most of them aim at optimizing the recommendation\nstrategy while using a fixed knowledge graph. In this paper, we take a\ndifferent approach, i.e., we fix the recommendation strategy and observe\nchanges when using different underlying knowledge graphs. Particularly, we use\ndifferent language editions of DBpedia. We show that the usage of different\nknowledge graphs does not only lead to differently biased recommender systems,\nbut also to recommender systems that differ in performance for particular\nfields of recommendations.",
    "published_date": "2021-05-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.00674v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.00667v1",
    "title": "Explaining how your AI system is fair",
    "authors": [
      "Boris Ruf",
      "Marcin Detyniecki"
    ],
    "author_ids": [],
    "abstract": "To implement fair machine learning in a sustainable way, choosing the right\nfairness objective is key. Since fairness is a concept of justice which comes\nin various, sometimes conflicting definitions, this is not a trivial task\nthough. The most appropriate fairness definition for an artificial intelligence\n(AI) system is a matter of ethical standards and legal requirements, and the\nright choice depends on the particular use case and its context. In this\nposition paper, we propose to use a decision tree as means to explain and\njustify the implemented kind of fairness to the end users. Such a structure\nwould first of all support AI practitioners in mapping ethical principles to\nfairness definitions for a concrete application and therefore make the\nselection a straightforward and transparent process. However, this approach\nwould also help document the reasoning behind the decision making. Due to the\ngeneral complexity of the topic of fairness in AI, we argue that specifying\n\"fairness\" for a given use case is the best way forward to maintain confidence\nin AI systems. In this case, this could be achieved by sharing the reasons and\nprinciples expressed during the decision making process with the broader\naudience.",
    "published_date": "2021-05-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.00667v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.00581v2",
    "title": "Robust Sample Weighting to Facilitate Individualized Treatment Rule Learning for a Target Population",
    "authors": [
      "Rui Chen",
      "Jared D. Huling",
      "Guanhua Chen",
      "Menggang Yu"
    ],
    "author_ids": [],
    "abstract": "Learning individualized treatment rules (ITRs) is an important topic in\nprecision medicine. Current literature mainly focuses on deriving ITRs from a\nsingle source population. We consider the observational data setting when the\nsource population differs from a target population of interest. Compared with\ncausal generalization for the average treatment effect which is a scalar\nquantity, ITR generalization poses new challenges due to the need to model and\ngeneralize the rules based on a prespecified class of functions which may not\ncontain the unrestricted true optimal ITR. The aim of this paper is to develop\na weighting framework to mitigate the impact of such misspecification and thus\nfacilitate the generalizability of optimal ITRs from a source population to a\ntarget population. Our method seeks covariate balance over a non-parametric\nfunction class characterized by a reproducing kernel Hilbert space and can\nimprove many ITR learning methods that rely on weights. We show that the\nproposed method encompasses importance weights and overlap weights as two\nextreme cases, allowing for a better bias-variance trade-off in between.\nNumerical examples demonstrate that the use of our weighting method can greatly\nimprove ITR estimation for the target population compared with other weighting\nmethods.",
    "published_date": "2021-05-03T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.ME"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.00581v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.00558v1",
    "title": "An Examination of Fairness of AI Models for Deepfake Detection",
    "authors": [
      "Loc Trinh",
      "Yan Liu"
    ],
    "author_ids": [],
    "abstract": "Recent studies have demonstrated that deep learning models can discriminate\nbased on protected classes like race and gender. In this work, we evaluate bias\npresent in deepfake datasets and detection models across protected subgroups.\nUsing facial datasets balanced by race and gender, we examine three popular\ndeepfake detectors and find large disparities in predictive performances across\nraces, with up to 10.7% difference in error rate between subgroups. A closer\nlook reveals that the widely used FaceForensics++ dataset is overwhelmingly\ncomposed of Caucasian subjects, with the majority being female Caucasians. Our\ninvestigation of the racial distribution of deepfakes reveals that the methods\nused to create deepfakes as positive training signals tend to produce\n\"irregular\" faces - when a person's face is swapped onto another person of a\ndifferent race or gender. This causes detectors to learn spurious correlations\nbetween the foreground faces and fakeness. Moreover, when detectors are trained\nwith the Blended Image (BI) dataset from Face X-Rays, we find that those\ndetectors develop systematic discrimination towards certain racial subgroups,\nprimarily female Asians.",
    "published_date": "2021-05-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.00558v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.00455v2",
    "title": "Synthesized Difference in Differences",
    "authors": [
      "Eric V. Strobl",
      "Thomas A. Lasko"
    ],
    "author_ids": [],
    "abstract": "We consider estimating the conditional average treatment effect for everyone\nby eliminating confounding and selection bias. Unfortunately, randomized\nclinical trials (RCTs) eliminate confounding but impose strict exclusion\ncriteria that prevent sampling of the entire clinical population. Observational\ndatasets are more inclusive but suffer from confounding. We therefore analyze\nRCT and observational data simultaneously in order to extract the strengths of\neach. Our solution builds upon Difference in Differences (DD), an algorithm\nthat eliminates confounding from observational data by comparing outcomes\nbefore and after treatment administration. DD requires a parallel slopes\nassumption that may not apply in practice when confounding shifts across time.\nWe instead propose Synthesized Difference in Differences (SDD) that infers the\ncorrect (possibly non-parallel) slopes by linearly adjusting a conditional\nversion of DD using additional RCT data. The algorithm achieves state of the\nart performance across multiple synthetic and real datasets even when the RCT\nexcludes the majority of patients.",
    "published_date": "2021-05-02T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.ME"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.00455v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.02963v3",
    "title": "Attention-augmented Spatio-Temporal Segmentation for Land Cover Mapping",
    "authors": [
      "Rahul Ghosh",
      "Praveen Ravirathinam",
      "Xiaowei Jia",
      "Chenxi Lin",
      "Zhenong Jin",
      "Vipin Kumar"
    ],
    "author_ids": [],
    "abstract": "The availability of massive earth observing satellite data provide huge\nopportunities for land use and land cover mapping. However, such mapping effort\nis challenging due to the existence of various land cover classes, noisy data,\nand the lack of proper labels. Also, each land cover class typically has its\nown unique temporal pattern and can be identified only during certain periods.\nIn this article, we introduce a novel architecture that incorporates the UNet\nstructure with Bidirectional LSTM and Attention mechanism to jointly exploit\nthe spatial and temporal nature of satellite data and to better identify the\nunique temporal patterns of each land cover. We evaluate this method for\nmapping crops in multiple regions over the world. We compare our method with\nother state-of-the-art methods both quantitatively and qualitatively on two\nreal-world datasets which involve multiple land cover classes. We also\nvisualise the attention weights to study its effectiveness in mitigating noise\nand identifying discriminative time period.",
    "published_date": "2021-05-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.02963v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.00292v1",
    "title": "Non-asymptotic Excess Risk Bounds for Classification with Deep Convolutional Neural Networks",
    "authors": [
      "Guohao Shen",
      "Yuling Jiao",
      "Yuanyuan Lin",
      "Jian Huang"
    ],
    "author_ids": [],
    "abstract": "In this paper, we consider the problem of binary classification with a class\nof general deep convolutional neural networks, which includes fully-connected\nneural networks and fully convolutional neural networks as special cases. We\nestablish non-asymptotic excess risk bounds for a class of convex surrogate\nlosses and target functions with different modulus of continuity. An important\nfeature of our results is that we clearly define the prefactors of the risk\nbounds in terms of the input data dimension and other model parameters and show\nthat they depend polynomially on the dimensionality in some important models.\nWe also show that the classification methods with CNNs can circumvent the curse\nof dimensionality if the input data is supported on an approximate\nlow-dimensional manifold. To establish these results, we derive an upper bound\nfor the covering number for the class of general convolutional neural networks\nwith a bias term in each convolutional layer, and derive new results on the\napproximation power of CNNs for any uniformly-continuous target functions.\nThese results provide further insights into the complexity and the\napproximation power of general convolutional neural networks, which are of\nindependent interest and may have other applications. Finally, we apply our\ngeneral results to analyze the non-asymptotic excess risk bounds for four\nwidely used methods with different loss functions using CNNs, including the\nleast squares, the logistic, the exponential and the SVM hinge losses.",
    "published_date": "2021-05-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "math.ST",
      "stat.TH",
      "68T07, 62G05"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.00292v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.00060v1",
    "title": "Ethical Implementation of Artificial Intelligence to Select Embryos in In Vitro Fertilization",
    "authors": [
      "Michael Anis Mihdi Afnan",
      "Cynthia Rudin",
      "Vincent Conitzer",
      "Julian Savulescu",
      "Abhishek Mishra",
      "Yanhe Liu",
      "Masoud Afnan"
    ],
    "author_ids": [],
    "abstract": "AI has the potential to revolutionize many areas of healthcare. Radiology,\ndermatology, and ophthalmology are some of the areas most likely to be impacted\nin the near future, and they have received significant attention from the\nbroader research community. But AI techniques are now also starting to be used\nin in vitro fertilization (IVF), in particular for selecting which embryos to\ntransfer to the woman. The contribution of AI to IVF is potentially\nsignificant, but must be done carefully and transparently, as the ethical\nissues are significant, in part because this field involves creating new\npeople. We first give a brief introduction to IVF and review the use of AI for\nembryo selection. We discuss concerns with the interpretation of the reported\nresults from scientific and practical perspectives. We then consider the\nbroader ethical issues involved. We discuss in detail the problems that result\nfrom the use of black-box methods in this context and advocate strongly for the\nuse of interpretable models. Importantly, there have been no published trials\nof clinical effectiveness, a problem in both the AI and IVF communities, and we\ntherefore argue that clinical implementation at this point would be premature.\nFinally, we discuss ways for the broader AI community to become involved to\nensure scientifically sound and ethically responsible development of AI in IVF.",
    "published_date": "2021-04-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.00060v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.07844v1",
    "title": "Does \"AI\" stand for augmenting inequality in the era of covid-19 healthcare?",
    "authors": [
      "David Leslie",
      "Anjali Mazumder",
      "Aidan Peppin",
      "Maria Wolters",
      "Alexa Hagerty"
    ],
    "author_ids": [],
    "abstract": "Among the most damaging characteristics of the covid-19 pandemic has been its\ndisproportionate effect on disadvantaged communities. As the outbreak has\nspread globally, factors such as systemic racism, marginalisation, and\nstructural inequality have created path dependencies that have led to poor\nhealth outcomes. These social determinants of infectious disease and\nvulnerability to disaster have converged to affect already disadvantaged\ncommunities with higher levels of economic instability, disease exposure,\ninfection severity, and death. Artificial intelligence (AI) technologies are an\nimportant part of the health informatics toolkit used to fight contagious\ndisease. AI is well known, however, to be susceptible to algorithmic biases\nthat can entrench and augment existing inequality. Uncritically deploying AI in\nthe fight against covid-19 thus risks amplifying the pandemic's adverse effects\non vulnerable groups, exacerbating health inequity. In this paper, we claim\nthat AI systems can introduce or reflect bias and discrimination in three ways:\nin patterns of health discrimination that become entrenched in datasets, in\ndata representativeness, and in human choices made during the design,\ndevelopment, and deployment of these systems. We highlight how the use of AI\ntechnologies threaten to exacerbate the disparate effect of covid-19 on\nmarginalised, under-represented, and vulnerable groups, particularly black,\nAsian, and other minoritised ethnic people, older populations, and those of\nlower socioeconomic status. We conclude that, to mitigate the compounding\neffects of AI on inequalities associated with covid-19, decision makers,\ntechnology developers, and health officials must account for the potential\nbiases and inequities at all stages of the AI process.",
    "published_date": "2021-04-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.07844v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.15058v1",
    "title": "Perpetual Voting: The Axiomatic Lens",
    "authors": [
      "Martin Lackner",
      "Jan Maly"
    ],
    "author_ids": [],
    "abstract": "Perpetual voting was recently introduced as a framework for long-term\ncollective decision making. In this framework, we consider a sequence of\nsubsequent approval-based elections and try to achieve a fair overall outcome.\nTo achieve fairness over time, perpetual voting rules take the history of\nprevious decisions into account and identify voters that were dissatisfied with\nprevious decisions. In this paper, we look at perpetual voting rules from an\naxiomatic perspective and study two main questions. First, we ask how simple\nsuch rules can be while still meeting basic desiderata. For two simple but\nnatural classes, we fully characterize the axiomatic possibilities. Second, we\nask how proportionality can be formalized in perpetual voting. We study\nproportionality on simple profiles that are equivalent to the apportionment\nsetting and show that lower and upper quota axioms can be used to distinguish\n(and sometimes characterize) perpetual voting rules. Furthermore, we show a\nsurprising connection between a perpetual rule called Perpetual Consensus and\nFrege's apportionment method.",
    "published_date": "2021-04-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.15058v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.14938v2",
    "title": "Super Odometry: IMU-centric LiDAR-Visual-Inertial Estimator for Challenging Environments",
    "authors": [
      "Shibo Zhao",
      "Hengrui Zhang",
      "Peng Wang",
      "Lucas Nogueira",
      "Sebastian Scherer"
    ],
    "author_ids": [],
    "abstract": "We propose Super Odometry, a high-precision multi-modal sensor fusion\nframework, providing a simple but effective way to fuse multiple sensors such\nas LiDAR, camera, and IMU sensors and achieve robust state estimation in\nperceptually-degraded environments. Different from traditional sensor-fusion\nmethods, Super Odometry employs an IMU-centric data processing pipeline, which\ncombines the advantages of loosely coupled methods with tightly coupled methods\nand recovers motion in a coarse-to-fine manner. The proposed framework is\ncomposed of three parts: IMU odometry, visual-inertial odometry, and\nlaser-inertial odometry. The visual-inertial odometry and laser-inertial\nodometry provide the pose prior to constrain the IMU bias and receive the\nmotion prediction from IMU odometry. To ensure high performance in real-time,\nwe apply a dynamic octree that only consumes 10 % of the running time compared\nwith a static KD-tree. The proposed system was deployed on drones and ground\nrobots, as part of Team Explorer's effort to the DARPA Subterranean Challenge\nwhere the team won $1^{st}$ and $2^{nd}$ place in the Tunnel and Urban\nCircuits, respectively.",
    "published_date": "2021-04-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.14938v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.00002v1",
    "title": "Ethics-Based Auditing to Develop Trustworthy AI",
    "authors": [
      "Jakob Mokander",
      "Luciano Floridi"
    ],
    "author_ids": [],
    "abstract": "A series of recent developments points towards auditing as a promising\nmechanism to bridge the gap between principles and practice in AI ethics.\nBuilding on ongoing discussions concerning ethics-based auditing, we offer\nthree contributions. First, we argue that ethics-based auditing can improve the\nquality of decision making, increase user satisfaction, unlock growth\npotential, enable law-making, and relieve human suffering. Second, we highlight\ncurrent best practices to support the design and implementation of ethics-based\nauditing: To be feasible and effective, ethics-based auditing should take the\nform of a continuous and constructive process, approach ethical alignment from\na system perspective, and be aligned with public policies and incentives for\nethically desirable behaviour. Third, we identify and discuss the constraints\nassociated with ethics-based auditing. Only by understanding and accounting for\nthese constraints can ethics-based auditing facilitate ethical alignment of AI,\nwhile enabling society to reap the full economic and social benefits of\nautomation.",
    "published_date": "2021-04-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.00002v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.14832v1",
    "title": "A string averaging method based on strictly quasi-nonexpansive operators with generalized relaxation",
    "authors": [
      "Touraj Nikazad",
      "Mahdi Mirzapour"
    ],
    "author_ids": [],
    "abstract": "We study a fixed point iterative method based on generalized relaxation of\nstrictly quasi-nonexpansive operators. The iterative method is assembled by\naveraging of strings, and each string is composed of finitely many strictly\nquasi-nonexpansive operators. To evaluate the study, we examine a wide class of\niterative methods for solving linear systems of equations (inequalities) and\nthe subgradient projection method for solving nonlinear convex feasibility\nproblems. The mathematical analysis is complemented by some experiments in\nimage reconstruction from projections and classical examples, which illustrate\nthe performance using generalized relaxation.",
    "published_date": "2021-04-30T00:00:00",
    "year": 2021,
    "categories": [
      "math.OC",
      "cs.NA",
      "math.NA",
      "90C25, 47J25, 49M20"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.14832v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.14795v1",
    "title": "Mitigating Political Bias in Language Models Through Reinforced Calibration",
    "authors": [
      "Ruibo Liu",
      "Chenyan Jia",
      "Jason Wei",
      "Guangxuan Xu",
      "Lili Wang",
      "Soroush Vosoughi"
    ],
    "author_ids": [],
    "abstract": "Current large-scale language models can be politically biased as a result of\nthe data they are trained on, potentially causing serious problems when they\nare deployed in real-world settings. In this paper, we describe metrics for\nmeasuring political bias in GPT-2 generation and propose a reinforcement\nlearning (RL) framework for mitigating political biases in generated text. By\nusing rewards from word embeddings or a classifier, our RL framework guides\ndebiased generation without having access to the training data or requiring the\nmodel to be retrained. In empirical experiments on three attributes sensitive\nto political bias (gender, location, and topic), our methods reduced bias\naccording to both our metrics and human evaluation, while maintaining\nreadability and semantic coherence.",
    "published_date": "2021-04-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.14795v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.14746v1",
    "title": "Center Prediction Loss for Re-identification",
    "authors": [
      "Lu Yang",
      "Yunlong Wang",
      "Lingqiao Liu",
      "Peng Wang",
      "Lu Chi",
      "Zehuan Yuan",
      "Changhu Wang",
      "Yanning Zhang"
    ],
    "author_ids": [],
    "abstract": "The training loss function that enforces certain training sample distribution\npatterns plays a critical role in building a re-identification (ReID) system.\nBesides the basic requirement of discrimination, i.e., the features\ncorresponding to different identities should not be mixed, additional\nintra-class distribution constraints, such as features from the same identities\nshould be close to their centers, have been adopted to construct losses.\nDespite the advances of various new loss functions, it is still challenging to\nstrike the balance between the need of reducing the intra-class variation and\nallowing certain distribution freedom. In this paper, we propose a new loss\nbased on center predictivity, that is, a sample must be positioned in a\nlocation of the feature space such that from it we can roughly predict the\nlocation of the center of same-class samples. The prediction error is then\nregarded as a loss called Center Prediction Loss (CPL). We show that, without\nintroducing additional hyper-parameters, this new loss leads to a more flexible\nintra-class distribution constraint while ensuring the between-class samples\nare well-separated. Extensive experiments on various real-world ReID datasets\nshow that the proposed loss can achieve superior performance and can also be\ncomplementary to existing losses.",
    "published_date": "2021-04-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.14746v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.14700v2",
    "title": "The Zero Resource Speech Challenge 2021: Spoken language modelling",
    "authors": [
      "Ewan Dunbar",
      "Mathieu Bernard",
      "Nicolas Hamilakis",
      "Tu Anh Nguyen",
      "Maureen de Seyssel",
      "Patricia Rozé",
      "Morgane Rivière",
      "Eugene Kharitonov",
      "Emmanuel Dupoux"
    ],
    "author_ids": [],
    "abstract": "We present the Zero Resource Speech Challenge 2021, which asks participants\nto learn a language model directly from audio, without any text or labels. The\nchallenge is based on the Libri-light dataset, which provides up to 60k hours\nof audio from English audio books without any associated text. We provide a\npipeline baseline system consisting on an encoder based on contrastive\npredictive coding (CPC), a quantizer ($k$-means) and a standard language model\n(BERT or LSTM). The metrics evaluate the learned representations at the\nacoustic (ABX discrimination), lexical (spot-the-word), syntactic\n(acceptability judgment) and semantic levels (similarity judgment). We present\nan overview of the eight submitted systems from four groups and discuss the\nmain results.",
    "published_date": "2021-04-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.14700v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.14662v3",
    "title": "Dynamic Population Games: A Tractable Intersection of Mean-Field Games and Population Games",
    "authors": [
      "Ezzat Elokda",
      "Saverio Bolognani",
      "Andrea Censi",
      "Florian Dörfler",
      "Emilio Frazzoli"
    ],
    "author_ids": [],
    "abstract": "In many real-world large-scale decision problems, self-interested agents have\nindividual dynamics and optimize their own long-term payoffs. Important\nexamples include the competitive access to shared resources (e.g., roads,\nenergy, or bandwidth) but also non-engineering domains like epidemic\npropagation and control. These problems are natural to model as mean-field\ngames. Existing mathematical formulations of mean field games have had limited\napplicability in practice, since they require solving non-standard\ninitial-terminal-value problems that are tractable only in limited special\ncases. In this letter, we propose a novel formulation, along with computational\ntools, for a practically relevant class of Dynamic Population Games (DPGs),\nwhich correspond to discrete-time, finite-state-and-action, stationary\nmean-field games. Our main contribution is a mathematical reduction of\nStationary Nash Equilibria (SNE) in DPGs to standard Nash Equilibria (NE) in\nstatic population games. This reduction is leveraged to guarantee the existence\nof a SNE, develop an evolutionary dynamics-based SNE computation algorithm, and\nderive simple conditions that guarantee stability and uniqueness of the SNE. We\nprovide two examples of applications: fair resource allocation with\nheterogeneous agents and control of epidemic propagation. Open source software\nfor SNE computation: https://gitlab.ethz.ch/elokdae/dynamic-population-games",
    "published_date": "2021-04-29T00:00:00",
    "year": 2021,
    "categories": [
      "math.OC",
      "cs.GT",
      "cs.SY",
      "econ.TH",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.14662v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2105.07841v1",
    "title": "Post-war Civil War Propaganda Techniques and Media Spins in Nigeria and Journalism Practice",
    "authors": [
      "Bolu John Folayan",
      "Olumide Samuel Ogunjobi",
      "Prosper Zannu",
      "Taiwo Ajibolu Balofin"
    ],
    "author_ids": [],
    "abstract": "In public relations and political communication, a spin is a form of\npropaganda achieved through knowingly presenting a biased interpretation of an\nevent or issues. It is also the act of presenting narratives to influence\npublic opinion about events, people or and ideas. In war time, various forms of\nspins are employed by antagonists to push their brigades to victory and wear\nout the opponents. During the Nigerian civil war, quite a number of these spins\nwere dominant for example GOWON (Go On With One Nigeria); On Aburi We Stand, O\nLe Ku Ija Ore. Post-war years presented different spins and fifty years after\nthe war, different spins continue to push emerging narratives (e.g.\nmarginalization, restructuring. This paper investigates and analyzes the\ndifferent propaganda techniques and spins in the narratives of the Nigerian\ncivil in the past five years through a content analysis of three national\nnewspapers: The Nigerian Tribune, Daily Trust and Sun Newspapers. Findings\nconfirm that propaganda and spins are not limited to war time, but are actively\ndeployed in peace time. This development places additional challenge on\njournalists to uphold the canons of balance, truth and fairness in reporting\nsensitive national issues.",
    "published_date": "2021-04-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.MM",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.07841v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.14556v3",
    "title": "Discover the Unknown Biased Attribute of an Image Classifier",
    "authors": [
      "Zhiheng Li",
      "Chenliang Xu"
    ],
    "author_ids": [],
    "abstract": "Recent works find that AI algorithms learn biases from data. Therefore, it is\nurgent and vital to identify biases in AI algorithms. However, the previous\nbias identification pipeline overly relies on human experts to conjecture\npotential biases (e.g., gender), which may neglect other underlying biases not\nrealized by humans. To help human experts better find the AI algorithms'\nbiases, we study a new problem in this work -- for a classifier that predicts a\ntarget attribute of the input image, discover its unknown biased attribute.\n  To solve this challenging problem, we use a hyperplane in the generative\nmodel's latent space to represent an image attribute; thus, the original\nproblem is transformed to optimizing the hyperplane's normal vector and offset.\nWe propose a novel total-variation loss within this framework as the objective\nfunction and a new orthogonalization penalty as a constraint. The latter\nprevents trivial solutions in which the discovered biased attribute is\nidentical with the target or one of the known-biased attributes. Extensive\nexperiments on both disentanglement datasets and real-world datasets show that\nour method can discover biased attributes and achieve better disentanglement\nw.r.t. target attributes. Furthermore, the qualitative results show that our\nmethod can discover unnoticeable biased attributes for various object and scene\nclassifiers, proving our method's generalizability for detecting biased\nattributes in diverse domains of images. The code is available at\nhttps://git.io/J3kMh.",
    "published_date": "2021-04-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.14556v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.14537v4",
    "title": "Towards Fair Classifiers Without Sensitive Attributes: Exploring Biases in Related Features",
    "authors": [
      "Tianxiang Zhao",
      "Enyan Dai",
      "Kai Shu",
      "Suhang Wang"
    ],
    "author_ids": [],
    "abstract": "Despite the rapid development and great success of machine learning models,\nextensive studies have exposed their disadvantage of inheriting latent\ndiscrimination and societal bias from the training data. This phenomenon\nhinders their adoption on high-stake applications. Thus, many efforts have been\ntaken for developing fair machine learning models. Most of them require that\nsensitive attributes are available during training to learn fair models.\nHowever, in many real-world applications, it is usually infeasible to obtain\nthe sensitive attributes due to privacy or legal issues, which challenges\nexisting fair-ensuring strategies. Though the sensitive attribute of each data\nsample is unknown, we observe that there are usually some non-sensitive\nfeatures in the training data that are highly correlated with sensitive\nattributes, which can be used to alleviate the bias. Therefore, in this paper,\nwe study a novel problem of exploring features that are highly correlated with\nsensitive attributes for learning fair and accurate classifiers. We\ntheoretically show that by minimizing the correlation between these related\nfeatures and model prediction, we can learn a fair classifier. Based on this\nmotivation, we propose a novel framework which simultaneously uses these\nrelated features for accurate prediction and enforces fairness. In addition,\nthe model can dynamically adjust the regularization weight of each related\nfeature to balance its contribution on model classification and fairness.\nExperimental results on real-world datasets demonstrate the effectiveness of\nthe proposed model for learning fair models with high classification accuracy.",
    "published_date": "2021-04-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.14537v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.14536v1",
    "title": "State-level Racially Motivated Hate Crimes Contrast Public Opinion on the #StopAsianHate and #StopAAPIHate Movement",
    "authors": [
      "Hanjia Lyu",
      "Yangxin Fan",
      "Ziyu Xiong",
      "Mayya Komisarchik",
      "Jiebo Luo"
    ],
    "author_ids": [],
    "abstract": "#StopAsianHate and #StopAAPIHate are two of the most commonly used hashtags\nthat represent the current movement to end hate crimes against the Asian\nAmerican and Pacific Islander community. We conduct a social media study of\npublic opinion on the #StopAsianHate and #StopAAPIHate movement based on 46,058\nTwitter users across 30 states in the United States ranging from March 18 to\nApril 11, 2021. The movement attracts more participation from women, younger\nadults, Asian and Black communities. 51.56% of the Twitter users show direct\nsupport, 18.38% are news about anti-Asian hate crimes, while 5.43% show a\nnegative attitude towards the movement. Public opinion varies across user\ncharacteristics. Furthermore, among the states with most racial bias motivated\nhate crimes, the negative attitude towards the #StopAsianHate and #StopAAPIHate\nmovement is the weakest. To our best knowledge, this is the first large-scale\nsocial media-based study to understand public opinion on the #StopAsianHate and\n#StopAAPIHate movement. We hope our study can provide insights and promote\nresearch on anti-Asian hate crimes, and ultimately help address such a serious\nsocietal issue for the common benefits of all communities.",
    "published_date": "2021-04-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.14536v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.14527v5",
    "title": "Online certification of preference-based fairness for personalized recommender systems",
    "authors": [
      "Virginie Do",
      "Sam Corbett-Davies",
      "Jamal Atif",
      "Nicolas Usunier"
    ],
    "author_ids": [],
    "abstract": "Recommender systems are facing scrutiny because of their growing impact on\nthe opportunities we have access to. Current audits for fairness are limited to\ncoarse-grained parity assessments at the level of sensitive groups. We propose\nto audit for envy-freeness, a more granular criterion aligned with individual\npreferences: every user should prefer their recommendations to those of other\nusers. Since auditing for envy requires to estimate the preferences of users\nbeyond their existing recommendations, we cast the audit as a new pure\nexploration problem in multi-armed bandits. We propose a sample-efficient\nalgorithm with theoretical guarantees that it does not deteriorate user\nexperience. We also study the trade-offs achieved on real-world recommendation\ndatasets.",
    "published_date": "2021-04-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.14527v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.14504v2",
    "title": "An Axiomatic Theory of Provably-Fair Welfare-Centric Machine Learning",
    "authors": [
      "Cyrus Cousins"
    ],
    "author_ids": [],
    "abstract": "We address an inherent difficulty in welfare-theoretic fair machine learning\nby proposing an equivalently axiomatically-justified alternative and studying\nthe resulting computational and statistical learning questions. Welfare metrics\nquantify overall wellbeing across a population of one or more groups, and\nwelfare-based objectives and constraints have recently been proposed to\nincentivize fair machine learning methods to produce satisfactory solutions\nthat consider the diverse needs of multiple groups. Unfortunately, many\nmachine-learning problems are more naturally cast as loss minimization tasks,\nrather than utility maximization, which complicates direct application of\nwelfare-centric methods to fair machine learning. In this work, we define a\ncomplementary measure, termed malfare, measuring overall societal harm (rather\nthan wellbeing), with axiomatic justification via the standard axioms of\ncardinal welfare. We then cast fair machine learning as malfare minimization\nover the risk values (expected losses) of each group. Surprisingly, the axioms\nof cardinal welfare (malfare) dictate that this is not equivalent to simply\ndefining utility as negative loss. Building upon these concepts, we define\nfair-PAC (FPAC) learning, where an FPAC learner is an algorithm that learns an\n$\\varepsilon$-$\\delta$ malfare-optimal model with bounded sample complexity,\nfor any data distribution, and for any (axiomatically justified) malfare\nconcept. Finally, we show broad conditions under which, with appropriate\nmodifications, standard PAC-learners may be converted to FPAC learners. This\nplaces FPAC learning on firm theoretical ground, as it yields statistical and\ncomputational efficiency guarantees for many well-studied machine-learning\nmodels, and is also practically relevant, as it democratizes fair ML by\nproviding concrete training algorithms and rigorous generalization guarantees\nfor these models",
    "published_date": "2021-04-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.14504v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.14492v1",
    "title": "Questioning causality on sex, gender and COVID-19, and identifying bias in large-scale data-driven analyses: the Bias Priority Recommendations and Bias Catalog for Pandemics",
    "authors": [
      "Natalia Díaz-Rodríguez",
      "Rūta Binkytė-Sadauskienė",
      "Wafae Bakkali",
      "Sannidhi Bookseller",
      "Paola Tubaro",
      "Andrius Bacevicius",
      "Raja Chatila"
    ],
    "author_ids": [],
    "abstract": "The COVID-19 pandemic has spurred a large amount of observational studies\nreporting linkages between the risk of developing severe COVID-19 or dying from\nit, and sex and gender. By reviewing a large body of related literature and\nconducting a fine grained analysis based on sex-disaggregated data of 61\ncountries spanning 5 continents, we discover several confounding factors that\ncould possibly explain the supposed male vulnerability to COVID-19. We thus\nhighlight the challenge of making causal claims based on available data, given\nthe lack of statistical significance and potential existence of biases.\nInformed by our findings on potential variables acting as confounders, we\ncontribute a broad overview on the issues bias, explainability and fairness\nentail in data-driven analyses. Thus, we outline a set of discriminatory policy\nconsequences that could, based on such results, lead to unintended\ndiscrimination. To raise awareness on the dimensionality of such foreseen\nimpacts, we have compiled an encyclopedia-like reference guide, the Bias\nCatalog for Pandemics (BCP), to provide definitions and emphasize realistic\nexamples of bias in general, and within the COVID-19 pandemic context. These\nare categorized within a division of bias families and a 2-level priority\nscale, together with preventive steps. In addition, we facilitate the Bias\nPriority Recommendations on how to best use and apply this catalog, and provide\nguidelines in order to address real world research questions. The objective is\nto anticipate and avoid disparate impact and discrimination, by considering\ncausality, explainability, bias and techniques to mitigate the latter. With\nthese, we hope to 1) contribute to designing and conducting fair and equitable\ndata-driven studies and research; and 2) interpret and draw meaningful and\nactionable conclusions from these.",
    "published_date": "2021-04-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.IR",
      "K.4.1; K.4.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.14492v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.14385v2",
    "title": "Cross-Domain Few-Shot Classification via Adversarial Task Augmentation",
    "authors": [
      "Haoqing Wang",
      "Zhi-Hong Deng"
    ],
    "author_ids": [],
    "abstract": "Few-shot classification aims to recognize unseen classes with few labeled\nsamples from each class. Many meta-learning models for few-shot classification\nelaborately design various task-shared inductive bias (meta-knowledge) to solve\nsuch tasks, and achieve impressive performance. However, when there exists the\ndomain shift between the training tasks and the test tasks, the obtained\ninductive bias fails to generalize across domains, which degrades the\nperformance of the meta-learning models. In this work, we aim to improve the\nrobustness of the inductive bias through task augmentation. Concretely, we\nconsider the worst-case problem around the source task distribution, and\npropose the adversarial task augmentation method which can generate the\ninductive bias-adaptive 'challenging' tasks. Our method can be used as a simple\nplug-and-play module for various meta-learning models, and improve their\ncross-domain generalization capability. We conduct extensive experiments under\nthe cross-domain setting, using nine few-shot classification datasets:\nmini-ImageNet, CUB, Cars, Places, Plantae, CropDiseases, EuroSAT, ISIC and\nChestX. Experimental results show that our method can effectively improve the\nfew-shot classification performance of the meta-learning models under domain\nshift, and outperforms the existing works. Our code is available at\nhttps://github.com/Haoqing-Wang/CDFSL-ATA.",
    "published_date": "2021-04-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.14385v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.14372v1",
    "title": "A neural anisotropic view of underspecification in deep learning",
    "authors": [
      "Guillermo Ortiz-Jimenez",
      "Itamar Franco Salazar-Reque",
      "Apostolos Modas",
      "Seyed-Mohsen Moosavi-Dezfooli",
      "Pascal Frossard"
    ],
    "author_ids": [],
    "abstract": "The underspecification of most machine learning pipelines means that we\ncannot rely solely on validation performance to assess the robustness of deep\nlearning systems to naturally occurring distribution shifts. Instead, making\nsure that a neural network can generalize across a large number of different\nsituations requires to understand the specific way in which it solves a task.\nIn this work, we propose to study this problem from a geometric perspective\nwith the aim to understand two key characteristics of neural network solutions\nin underspecified settings: how is the geometry of the learned function related\nto the data representation? And, are deep networks always biased towards\nsimpler solutions, as conjectured in recent literature? We show that the way\nneural networks handle the underspecification of these problems is highly\ndependent on the data representation, affecting both the geometry and the\ncomplexity of the learned predictors. Our results highlight that understanding\nthe architectural inductive bias in deep learning is fundamental to address the\nfairness, robustness, and generalization of these systems.",
    "published_date": "2021-04-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.14372v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.14371v1",
    "title": "Generalized Linear Models with Structured Sparsity Estimators",
    "authors": [
      "Mehmet Caner"
    ],
    "author_ids": [],
    "abstract": "In this paper, we introduce structured sparsity estimators in Generalized\nLinear Models. Structured sparsity estimators in the least squares loss are\nintroduced by Stucky and van de Geer (2018) recently for fixed design and\nnormal errors. We extend their results to debiased structured sparsity\nestimators with Generalized Linear Model based loss. Structured sparsity\nestimation means penalized loss functions with a possible sparsity structure\nused in the chosen norm. These include weighted group lasso, lasso and norms\ngenerated from convex cones. The significant difficulty is that it is not clear\nhow to prove two oracle inequalities. The first one is for the initial\npenalized Generalized Linear Model estimator. Since it is not clear how a\nparticular feasible-weighted nodewise regression may fit in an oracle\ninequality for penalized Generalized Linear Model, we need a second oracle\ninequality to get oracle bounds for the approximate inverse for the sample\nestimate of second-order partial derivative of Generalized Linear Model.\n  Our contributions are fivefold: 1. We generalize the existing oracle\ninequality results in penalized Generalized Linear Models by proving the\nunderlying conditions rather than assuming them. One of the key issues is the\nproof of a sample one-point margin condition and its use in an oracle\ninequality. 2. Our results cover even non sub-Gaussian errors and regressors.\n3. We provide a feasible weighted nodewise regression proof which generalizes\nthe results in the literature from a simple l_1 norm usage to norms generated\nfrom convex cones. 4. We realize that norms used in feasible nodewise\nregression proofs should be weaker or equal to the norms in penalized\nGeneralized Linear Model loss. 5. We can debias the first step estimator via\ngetting an approximate inverse of the singular-sample second order partial\nderivative of Generalized Linear Model loss.",
    "published_date": "2021-04-29T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG",
      "econ.EM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.14371v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.14347v2",
    "title": "Picking Sequences and Monotonicity in Weighted Fair Division",
    "authors": [
      "Mithun Chakraborty",
      "Ulrike Schmidt-Kraepelin",
      "Warut Suksompong"
    ],
    "author_ids": [],
    "abstract": "We study the problem of fairly allocating indivisible items to agents with\ndifferent entitlements, which captures, for example, the distribution of\nministries among political parties in a coalition government. Our focus is on\npicking sequences derived from common apportionment methods, including five\ntraditional divisor methods and the quota method. We paint a complete picture\nof these methods in relation to known envy-freeness and proportionality\nrelaxations for indivisible items as well as monotonicity properties with\nrespect to the resource, population, and weights. In addition, we provide\ncharacterizations of picking sequences satisfying each of the fairness notions,\nand show that the well-studied maximum Nash welfare solution fails resource-\nand population-monotonicity even in the unweighted setting. Our results serve\nas an argument in favor of using picking sequences in weighted fair division\nproblems.",
    "published_date": "2021-04-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT",
      "econ.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.14347v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.14226v1",
    "title": "Assuming Just Enough Fairness to make Session Types Complete for Lock-freedom",
    "authors": [
      "Rob van Glabbeek",
      "Peter Höfner",
      "Ross Horne"
    ],
    "author_ids": [],
    "abstract": "We investigate how different fairness assumptions affect results concerning\nlock-freedom, a typical liveness property targeted by session type systems. We\nfix a minimal session calculus and systematically take into account all known\nfairness assumptions, thereby identifying precisely three interesting and\nsemantically distinct notions of lock-freedom, all of which having a sound\nsession type system. We then show that, by using a general merge operator in an\notherwise standard approach to global session types, we obtain a session type\nsystem complete for the strongest amongst those notions of lock-freedom, which\nassumes only justness of execution paths, a minimal fairness assumption for\nconcurrent systems.",
    "published_date": "2021-04-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LO",
      "F.3.1; F.4.1; F.1.2; F.3.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.14226v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.14210v2",
    "title": "FairDrop: Biased Edge Dropout for Enhancing Fairness in Graph Representation Learning",
    "authors": [
      "Indro Spinelli",
      "Simone Scardapane",
      "Amir Hussain",
      "Aurelio Uncini"
    ],
    "author_ids": [],
    "abstract": "Graph representation learning has become a ubiquitous component in many\nscenarios, ranging from social network analysis to energy forecasting in smart\ngrids. In several applications, ensuring the fairness of the node (or graph)\nrepresentations with respect to some protected attributes is crucial for their\ncorrect deployment. Yet, fairness in graph deep learning remains\nunder-explored, with few solutions available. In particular, the tendency of\nsimilar nodes to cluster on several real-world graphs (i.e., homophily) can\ndramatically worsen the fairness of these procedures. In this paper, we propose\na novel biased edge dropout algorithm (FairDrop) to counter-act homophily and\nimprove fairness in graph representation learning. FairDrop can be plugged in\neasily on many existing algorithms, is efficient, adaptable, and can be\ncombined with other fairness-inducing solutions. After describing the general\nalgorithm, we demonstrate its application on two benchmark tasks, specifically,\nas a random walk model for producing node embeddings, and to a graph\nconvolutional network for link prediction. We prove that the proposed algorithm\ncan successfully improve the fairness of all models up to a small or negligible\ndrop in accuracy, and compares favourably with existing state-of-the-art\nsolutions. In an ablation study, we demonstrate that our algorithm can flexibly\ninterpolate between biasing towards fairness and an unbiased edge dropout.\nFurthermore, to better evaluate the gains, we propose a new dyadic group\ndefinition to measure the bias of a link prediction task when paired with\ngroup-based fairness metrics. In particular, we extend the metric used to\nmeasure the bias in the node embeddings to take into account the graph\nstructure.",
    "published_date": "2021-04-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.14210v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.14976v2",
    "title": "Comparing Power Processing System Approaches in Second-Use Battery Energy Buffering for Electric Vehicle Charging",
    "authors": [
      "Xiaofan Cui",
      "Alireza Ramyar",
      "Jason Siegel",
      "Peyman Mohtat",
      "Anna Stefanopoulou",
      "Al-Thaddeus Avestruz"
    ],
    "author_ids": [],
    "abstract": "The heterogeneity in pack voltages and capacity of aged packs limits the\nperformance and economic viability of second-use battery energy storage systems\n(2-BESS) due to issues of reliability and available energy. Overcoming these\nlimitations could enable extended use of batteries and improve the\nenvironmental impacts of electric vehicles by reducing the number of batteries\nproduced. This paper compares Lite-Sparse Hierarchical Partial Power Processing\n(LS-HiPPP), a new method for power processing in 2-BESS, to conventional power\nprocessing architectures using a stochastic EV charging plaza model. This\nmethod for performance evaluation allows a fair comparison among power\nprocessing architectures for 2-BESS. Results show that LS-HiPPP increases the\nbattery energy utilization to 94% as compared to 78% for conventional partial\npower processing (C-PPP) and 23% for full power processing. These results were\nobtained with 25% heterogeneity in individual battery capacities and 20% power\nprocessing within the 2-BESS. Derating and captured value are two derived\nperformance metrics for comparing LS-HiPPP and C-PPP in this work. The derating\nfor LS-HiPPP is 84.3% in comparison to 63.1% for C-PPP. The captured value for\nLS-HiPPP is 79.8% versus 51% for C-PPP.",
    "published_date": "2021-04-29T00:00:00",
    "year": 2021,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.14976v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.14067v2",
    "title": "Improving Fairness in Speaker Recognition",
    "authors": [
      "Gianni Fenu",
      "Giacomo Medda",
      "Mirko Marras",
      "Giacomo Meloni"
    ],
    "author_ids": [],
    "abstract": "The human voice conveys unique characteristics of an individual, making voice\nbiometrics a key technology for verifying identities in various industries.\nDespite the impressive progress of speaker recognition systems in terms of\naccuracy, a number of ethical and legal concerns has been raised, specifically\nrelating to the fairness of such systems. In this paper, we aim to explore the\ndisparity in performance achieved by state-of-the-art deep speaker recognition\nsystems, when different groups of individuals characterized by a common\nsensitive attribute (e.g., gender) are considered. In order to mitigate the\nunfairness we uncovered by means of an exploratory study, we investigate\nwhether balancing the representation of the different groups of individuals in\nthe training set can lead to a more equal treatment of these demographic\ngroups. Experiments on two state-of-the-art neural architectures and a\nlarge-scale public dataset show that models trained with\ndemographically-balanced training sets exhibit a fairer behavior on different\ngroups, while still being accurate. Our study is expected to provide a solid\nbasis for instilling beyond-accuracy objectives (e.g., fairness) in speaker\nrecognition.",
    "published_date": "2021-04-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.14067v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.14014v1",
    "title": "Algorithmic Factors Influencing Bias in Machine Learning",
    "authors": [
      "William Blanzeisky",
      "Pádraig Cunningham"
    ],
    "author_ids": [],
    "abstract": "It is fair to say that many of the prominent examples of bias in Machine\nLearning (ML) arise from bias that is there in the training data. In fact, some\nwould argue that supervised ML algorithms cannot be biased, they reflect the\ndata on which they are trained. In this paper we demonstrate how ML algorithms\ncan misrepresent the training data through underestimation. We show how\nirreducible error, regularization and feature and class imbalance can\ncontribute to this underestimation. The paper concludes with a demonstration of\nhow the careful management of synthetic counterfactuals can ameliorate the\nimpact of this underestimation bias.",
    "published_date": "2021-04-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.14014v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.13640v2",
    "title": "Societal Biases in Retrieved Contents: Measurement Framework and Adversarial Mitigation for BERT Rankers",
    "authors": [
      "Navid Rekabsaz",
      "Simone Kopeinik",
      "Markus Schedl"
    ],
    "author_ids": [],
    "abstract": "Societal biases resonate in the retrieved contents of information retrieval\n(IR) systems, resulting in reinforcing existing stereotypes. Approaching this\nissue requires established measures of fairness in respect to the\nrepresentation of various social groups in retrieval results, as well as\nmethods to mitigate such biases, particularly in the light of the advances in\ndeep ranking models. In this work, we first provide a novel framework to\nmeasure the fairness in the retrieved text contents of ranking models.\nIntroducing a ranker-agnostic measurement, the framework also enables the\ndisentanglement of the effect on fairness of collection from that of rankers.\nTo mitigate these biases, we propose AdvBert, a ranking model achieved by\nadapting adversarial bias mitigation for IR, which jointly learns to predict\nrelevance and remove protected attributes. We conduct experiments on two\npassage retrieval collections (MSMARCO Passage Re-ranking and TREC Deep\nLearning 2019 Passage Re-ranking), which we extend by fairness annotations of a\nselected subset of queries regarding gender attributes. Our results on the\nMSMARCO benchmark show that, (1) all ranking models are less fair in comparison\nwith ranker-agnostic baselines, and (2) the fairness of Bert rankers\nsignificantly improves when using the proposed AdvBert models. Lastly, we\ninvestigate the trade-off between fairness and utility, showing that we can\nmaintain the significant improvements in fairness without any significant loss\nin utility.",
    "published_date": "2021-04-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.13640v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.13582v1",
    "title": "[Re] Don't Judge an Object by Its Context: Learning to Overcome Contextual Bias",
    "authors": [
      "Sunnie S. Y. Kim",
      "Sharon Zhang",
      "Nicole Meister",
      "Olga Russakovsky"
    ],
    "author_ids": [],
    "abstract": "Singh et al. (2020) point out the dangers of contextual bias in visual\nrecognition datasets. They propose two methods, CAM-based and feature-split,\nthat better recognize an object or attribute in the absence of its typical\ncontext while maintaining competitive within-context accuracy. To verify their\nperformance, we attempted to reproduce all 12 tables in the original paper,\nincluding those in the appendix. We also conducted additional experiments to\nbetter understand the proposed methods, including increasing the regularization\nin CAM-based and removing the weighted loss in feature-split. As the original\ncode was not made available, we implemented the entire pipeline from scratch in\nPyTorch 1.7.0. Our implementation is based on the paper and email exchanges\nwith the authors. We found that both proposed methods in the original paper\nhelp mitigate contextual bias, although for some methods, we could not\ncompletely replicate the quantitative results in the paper even after\ncompleting an extensive hyperparameter search. For example, on COCO-Stuff,\nDeepFashion, and UnRel, our feature-split model achieved an increase in\naccuracy on out-of-context images over the standard baseline, whereas on AwA,\nwe saw a drop in performance. For the proposed CAM-based method, we were able\nto reproduce the original paper's results to within 0.5$\\%$ mAP. Our\nimplementation can be found at\nhttps://github.com/princetonvisualai/ContextualBias.",
    "published_date": "2021-04-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.13582v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.13490v1",
    "title": "Leveraging Community and Author Context to Explain the Performance and Bias of Text-Based Deception Detection Models",
    "authors": [
      "Galen Weld",
      "Ellyn Ayton",
      "Tim Althoff",
      "Maria Glenski"
    ],
    "author_ids": [],
    "abstract": "Deceptive news posts shared in online communities can be detected with NLP\nmodels, and much recent research has focused on the development of such models.\nIn this work, we use characteristics of online communities and authors -- the\ncontext of how and where content is posted -- to explain the performance of a\nneural network deception detection model and identify sub-populations who are\ndisproportionately affected by model accuracy or failure. We examine who is\nposting the content, and where the content is posted to. We find that while\nauthor characteristics are better predictors of deceptive content than\ncommunity characteristics, both characteristics are strongly correlated with\nmodel performance. Traditional performance metrics such as F1 score may fail to\ncapture poor model performance on isolated sub-populations such as specific\nauthors, and as such, more nuanced evaluation of deception detection models is\ncritical.",
    "published_date": "2021-04-27T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.13490v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.13312v3",
    "title": "Multi-fairness under class-imbalance",
    "authors": [
      "Arjun Roy",
      "Vasileios Iosifidis",
      "Eirini Ntoutsi"
    ],
    "author_ids": [],
    "abstract": "Recent studies showed that datasets used in fairness-aware machine learning\nfor multiple protected attributes (referred to as multi-discrimination\nhereafter) are often imbalanced. The class-imbalance problem is more severe for\nthe often underrepresented protected group (e.g. female, non-white, etc.) in\nthe critical minority class. Still, existing methods focus only on the overall\nerror-discrimination trade-off, ignoring the imbalance problem, thus amplifying\nthe prevalent bias in the minority classes. Therefore, solutions are needed to\nsolve the combined problem of multi-discrimination and class-imbalance. To this\nend, we introduce a new fairness measure, Multi-Max Mistreatment (MMM), which\nconsiders both (multi-attribute) protected group and class membership of\ninstances to measure discrimination. To solve the combined problem, we propose\na boosting approach that incorporates MMM-costs in the distribution update and\npost-training selects the optimal trade-off among accurate, balanced, and fair\nsolutions. The experimental results show the superiority of our approach\nagainst state-of-the-art methods in producing the best balanced performance\nacross groups and classes and the best accuracy for the protected groups in the\nminority class.",
    "published_date": "2021-04-27T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.13312v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.13247v2",
    "title": "IATos: AI-powered pre-screening tool for COVID-19 from cough audio samples",
    "authors": [
      "D. Trejo Pizzo",
      "S. Esteban"
    ],
    "author_ids": [],
    "abstract": "OBJECTIVE: Our objective is to evaluate the possibility of using cough audio\nrecordings (spontaneous or simulated) to detect sound patterns in people who\nare diagnosed with COVID-19. The research question that led our work was: what\nis the sensitivity and specificity of a machine learning based COVID-19 cough\nclassifier, using RT-PCR tests as gold standard?\n  SETTING: The audio samples that were collected for this study belong to\nindividuals who were swabbed in the City of Buenos Aires in 20 public and 1\nprivate facilities where RT-PCR studies were carried out on patients suspected\nof COVID, and 14 out-of-hospital isolation units for patients with confirmed\nCOVID mild cases. The audios were collected through the Buenos Aires city\ngovernment WhatsApp chatbot that was specifically designed to address citizen\ninquiries related to the coronavirus pandemic (COVID-19).\n  PARTICIPANTS: The data collected corresponds to 2821 individuals who were\nswabbed in the City of Buenos Aires, between August 11 and December 2, 2020.\nIndividuals were divided into 1409 that tested positive for COVID-19 and 1412\nthat tested negative. From this sample group, 52.6% of the individuals were\nfemale and 47.4% were male. 2.5% were between the age of 0 and 20 , 61.1%\nbetween the age of 21 and 40 , 30.3% between the age of 41 and 60 and 6.1% were\nover 61 years of age.\n  RESULTS: Using the dataset of 2821 individuals our results showed that the\nneural network classifier was able to discriminate between the COVID-19\npositive and the healthy coughs with an accuracy of 86%. This accuracy obtained\nduring the training process was later tested and confirmed with a second\ndataset corresponding to 492 individuals.",
    "published_date": "2021-04-27T00:00:00",
    "year": 2021,
    "categories": [
      "eess.AS",
      "cs.SD"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.13247v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.13083v1",
    "title": "Using Radio Archives for Low-Resource Speech Recognition: Towards an Intelligent Virtual Assistant for Illiterate Users",
    "authors": [
      "Moussa Doumbouya",
      "Lisa Einstein",
      "Chris Piech"
    ],
    "author_ids": [],
    "abstract": "For many of the 700 million illiterate people around the world, speech\nrecognition technology could provide a bridge to valuable information and\nservices. Yet, those most in need of this technology are often the most\nunderserved by it. In many countries, illiterate people tend to speak only\nlow-resource languages, for which the datasets necessary for speech technology\ndevelopment are scarce. In this paper, we investigate the effectiveness of\nunsupervised speech representation learning on noisy radio broadcasting\narchives, which are abundant even in low-resource languages. We make three core\ncontributions. First, we release two datasets to the research community. The\nfirst, West African Radio Corpus, contains 142 hours of audio in more than 10\nlanguages with a labeled validation subset. The second, West African Virtual\nAssistant Speech Recognition Corpus, consists of 10K labeled audio clips in\nfour languages. Next, we share West African wav2vec, a speech encoder trained\non the noisy radio corpus, and compare it with the baseline Facebook speech\nencoder trained on six times more data of higher quality. We show that West\nAfrican wav2vec performs similarly to the baseline on a multilingual speech\nrecognition task, and significantly outperforms the baseline on a West African\nlanguage identification task. Finally, we share the first-ever speech\nrecognition models for Maninka, Pular and Susu, languages spoken by a combined\n10 million people in over seven countries, including six where the majority of\nthe adult population is illiterate. Our contributions offer a path forward for\nethical AI research to serve the needs of those most disadvantaged by the\ndigital divide.",
    "published_date": "2021-04-27T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.13083v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.12920v1",
    "title": "Equity and Artificial Intelligence in Education: Will \"AIEd\" Amplify or Alleviate Inequities in Education?",
    "authors": [
      "Kenneth Holstein",
      "Shayan Doroudi"
    ],
    "author_ids": [],
    "abstract": "The development of educational AI (AIEd) systems has often been motivated by\ntheir potential to promote educational equity and reduce achievement gaps\nacross different groups of learners -- for example, by scaling up the benefits\nof one-on-one human tutoring to a broader audience, or by filling gaps in\nexisting educational services. Given these noble intentions, why might AIEd\nsystems have inequitable impacts in practice? In this chapter, we discuss four\nlenses that can be used to examine how and why AIEd systems risk amplifying\nexisting inequities. Building from these lenses, we then outline possible paths\ntowards more equitable futures for AIEd, while highlighting debates surrounding\neach proposal. In doing so, we hope to provoke new conversations around the\ndesign of equitable AIEd, and to push ongoing conversations in the field\nforward.",
    "published_date": "2021-04-27T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.12920v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.12909v6",
    "title": "Algorithm as Experiment: Machine Learning, Market Design, and Policy Eligibility Rules",
    "authors": [
      "Yusuke Narita",
      "Kohei Yata"
    ],
    "author_ids": [],
    "abstract": "Algorithms make a growing portion of policy and business decisions. We\ndevelop a treatment-effect estimator using algorithmic decisions as instruments\nfor a class of stochastic and deterministic algorithms. Our estimator is\nconsistent and asymptotically normal for well-defined causal effects. A special\ncase of our setup is multidimensional regression discontinuity designs with\ncomplex boundaries. We apply our estimator to evaluate the Coronavirus Aid,\nRelief, and Economic Security Act, which allocated many billions of dollars\nworth of relief funding to hospitals via an algorithmic rule. The funding is\nshown to have little effect on COVID-19-related hospital activities. Naive\nestimates exhibit selection bias.",
    "published_date": "2021-04-26T00:00:00",
    "year": 2021,
    "categories": [
      "econ.EM",
      "cs.LG",
      "stat.ME",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.12909v6",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.12731v1",
    "title": "We Haven't Gone Paperless Yet: Why the Printing Press Can Help Us Understand Data and AI",
    "authors": [
      "Julian Posada",
      "Nicholas Weller",
      "Wendy H. Wong"
    ],
    "author_ids": [],
    "abstract": "How should we understand the social and political effects of the datafication\nof human life? This paper argues that the effects of data should be understood\nas a constitutive shift in social and political relations. We explore how\ndatafication, or quantification of human and non-human factors into binary\ncode, affects the identity of individuals and groups. This fundamental shift\ngoes beyond economic and ethical concerns, which has been the focus of other\nefforts to explore the effects of datafication and AI. We highlight that\ntechnologies such as datafication and AI (and previously, the printing press)\nboth disrupted extant power arrangements, leading to decentralization, and\ntriggered a recentralization of power by new actors better adapted to\nleveraging the new technology. We use the analogy of the printing press to\nprovide a framework for understanding constitutive change. The printing press\nexample gives us more clarity on 1) what can happen when the medium of\ncommunication drastically alters how information is communicated and stored; 2)\nthe shift in power from state to private actors; and 3) the tension of\nsimultaneously connecting individuals while driving them towards narrower\ncommunities through algorithmic analyses of data.",
    "published_date": "2021-04-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.12731v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.12608v1",
    "title": "Generalized ADMM in Distributed Learning via Variational Inequality",
    "authors": [
      "Saeedeh Parsaeefard",
      "Alberto Leon Garcia"
    ],
    "author_ids": [],
    "abstract": "Due to the explosion in size and complexity of modern data sets and privacy\nconcerns of data holders, it is increasingly important to be able to solve\nmachine learning problems in distributed manners. The Alternating Direction\nMethod of Multipliers (ADMM) through the concept of consensus variables is a\npractical algorithm in this context where its diverse variations and its\nperformance have been studied in different application areas. In this paper, we\nstudy the effect of the local data sets of users in the distributed learning of\nADMM. Our aim is to deploy variational inequality (VI) to attain an unified\nview of ADMM variations. Through the simulation results, we demonstrate how\nmore general definitions of consensus parameters and introducing the uncertain\nparameters in distribute approach can help to get the better results in\nlearning processes.",
    "published_date": "2021-04-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.NI",
      "cs.DC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.12608v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.12380v2",
    "title": "International Migration in Academia and Citation Performance: An Analysis of German-Affiliated Researchers by Gender and Discipline Using Scopus Publications 1996-2020",
    "authors": [
      "Xinyi Zhao",
      "Samin Aref",
      "Emilio Zagheni",
      "Guy Stecklov"
    ],
    "author_ids": [],
    "abstract": "Germany has become a major country of immigration, as well as a research\npowerhouse in Europe. As Germany spends a higher fraction of its GDP on\nresearch and development than most countries with advanced economies, there is\nan expectation that Germany should be able to attract and retain international\nscholars who have high citation performance. Using an exhaustive set of over\neight million Scopus publications, we analyze the trends in international\nmigration to and from Germany among published researchers over the past 24\nyears. We assess changes in institutional affiliations for over one million\nresearchers who have published with a German affiliation address at some point\nduring the 1996-2020 period. We show that while Germany has been highly\nintegrated into the global movement of researchers, with particularly strong\nties to the US, the UK, and Switzerland, the country has been sending more\npublished researchers abroad than it has attracted. While the balance has been\nlargely negative over time, analyses disaggregated by gender, citation\nperformance, and field of research show that compositional differences in\nmigrant flows may help to alleviate persistent gender inequalities in selected\nfields.",
    "published_date": "2021-04-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DL",
      "cs.CY",
      "K.4; J.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.12380v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.12279v1",
    "title": "Computational Group Selection",
    "authors": [
      "Nripsuta Saxena"
    ],
    "author_ids": [],
    "abstract": "Humans spend a significant part of their lives being a part of groups. In\nthis document we propose research directions that would make it possible to\ncomputationally form productive groups. We bring to light several issues that\nneed to be addressed in the pursuit of this goal (not amplifying existing\nbiases and inequality, for example), as well as multiple avenues to study that\nwould help achieve us this task efficiently.",
    "published_date": "2021-04-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.12279v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.12278v2",
    "title": "Causal Learning for Socially Responsible AI",
    "authors": [
      "Lu Cheng",
      "Ahmadreza Mosallanezhad",
      "Paras Sheth",
      "Huan Liu"
    ],
    "author_ids": [],
    "abstract": "There have been increasing concerns about Artificial Intelligence (AI) due to\nits unfathomable potential power. To make AI address ethical challenges and\nshun undesirable outcomes, researchers proposed to develop socially responsible\nAI (SRAI). One of these approaches is causal learning (CL). We survey\nstate-of-the-art methods of CL for SRAI. We begin by examining the seven CL\ntools to enhance the social responsibility of AI, then review how existing\nworks have succeeded using these tools to tackle issues in developing SRAI such\nas fairness. The goal of this survey is to bring forefront the potentials and\npromises of CL for SRAI.",
    "published_date": "2021-04-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.12278v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.12259v1",
    "title": "User Preference-aware Fake News Detection",
    "authors": [
      "Yingtong Dou",
      "Kai Shu",
      "Congying Xia",
      "Philip S. Yu",
      "Lichao Sun"
    ],
    "author_ids": [],
    "abstract": "Disinformation and fake news have posed detrimental effects on individuals\nand society in recent years, attracting broad attention to fake news detection.\nThe majority of existing fake news detection algorithms focus on mining news\ncontent and/or the surrounding exogenous context for discovering deceptive\nsignals; while the endogenous preference of a user when he/she decides to\nspread a piece of fake news or not is ignored. The confirmation bias theory has\nindicated that a user is more likely to spread a piece of fake news when it\nconfirms his/her existing beliefs/preferences. Users' historical, social\nengagements such as posts provide rich information about users' preferences\ntoward news and have great potential to advance fake news detection. However,\nthe work on exploring user preference for fake news detection is somewhat\nlimited. Therefore, in this paper, we study the novel problem of exploiting\nuser preference for fake news detection. We propose a new framework, UPFD,\nwhich simultaneously captures various signals from user preferences by joint\ncontent and graph modeling. Experimental results on real-world datasets\ndemonstrate the effectiveness of the proposed framework. We release our code\nand data as a benchmark for GNN-based fake news detection:\nhttps://github.com/safe-graph/GNN-FakeNews.",
    "published_date": "2021-04-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.12259v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.12231v1",
    "title": "Model-based metrics: Sample-efficient estimates of predictive model subpopulation performance",
    "authors": [
      "Andrew C. Miller",
      "Leon A. Gatys",
      "Joseph Futoma",
      "Emily B. Fox"
    ],
    "author_ids": [],
    "abstract": "Machine learning models $-$ now commonly developed to screen, diagnose, or\npredict health conditions $-$ are evaluated with a variety of performance\nmetrics. An important first step in assessing the practical utility of a model\nis to evaluate its average performance over an entire population of interest.\nIn many settings, it is also critical that the model makes good predictions\nwithin predefined subpopulations. For instance, showing that a model is fair or\nequitable requires evaluating the model's performance in different demographic\nsubgroups. However, subpopulation performance metrics are typically computed\nusing only data from that subgroup, resulting in higher variance estimates for\nsmaller groups. We devise a procedure to measure subpopulation performance that\ncan be more sample-efficient than the typical subsample estimates. We propose\nusing an evaluation model $-$ a model that describes the conditional\ndistribution of the predictive model score $-$ to form model-based metric (MBM)\nestimates. Our procedure incorporates model checking and validation, and we\npropose a computationally efficient approximation of the traditional\nnonparametric bootstrap to form confidence intervals. We evaluate MBMs on two\nmain tasks: a semi-synthetic setting where ground truth metrics are available\nand a real-world hospital readmission prediction task. We find that MBMs\nconsistently produce more accurate and lower variance estimates of model\nperformance for small subpopulations.",
    "published_date": "2021-04-25T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.AP",
      "stat.ME"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.12231v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.12116v2",
    "title": "Fair-Capacitated Clustering",
    "authors": [
      "Tai Le Quy",
      "Arjun Roy",
      "Gunnar Friege",
      "Eirini Ntoutsi"
    ],
    "author_ids": [],
    "abstract": "Traditionally, clustering algorithms focus on partitioning the data into\ngroups of similar instances. The similarity objective, however, is not\nsufficient in applications where a fair-representation of the groups in terms\nof protected attributes like gender or race, is required for each cluster.\nMoreover, in many applications, to make the clusters useful for the end-user, a\nbalanced cardinality among the clusters is required. Our motivation comes from\nthe education domain where studies indicate that students might learn better in\ndiverse student groups and of course groups of similar cardinality are more\npractical e.g., for group assignments. To this end, we introduce the\nfair-capacitated clustering problem that partitions the data into clusters of\nsimilar instances while ensuring cluster fairness and balancing cluster\ncardinalities. We propose a two-step solution to the problem: i) we rely on\nfairlets to generate minimal sets that satisfy the fair constraint and ii) we\npropose two approaches, namely hierarchical clustering and partitioning-based\nclustering, to obtain the fair-capacitated clustering. The hierarchical\napproach embeds the additional cardinality requirements during the merging step\nwhile the partitioning-based one alters the assignment step using a knapsack\nproblem formulation to satisfy the additional requirements. Our experiments on\nfour educational datasets show that our approaches deliver well-balanced\nclusters in terms of both fairness and cardinality while maintaining a good\nclustering quality.",
    "published_date": "2021-04-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.12116v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.12037v1",
    "title": "Precarity: Modeling the Long Term Effects of Compounded Decisions on Individual Instability",
    "authors": [
      "Pegah Nokhiz",
      "Aravinda Kanchana Ruwanpathirana",
      "Neal Patwari",
      "Suresh Venkatasubramanian"
    ],
    "author_ids": [],
    "abstract": "When it comes to studying the impacts of decision making, the research has\nbeen largely focused on examining the fairness of the decisions, the long-term\neffects of the decision pipelines, and utility-based perspectives considering\nboth the decision-maker and the individuals. However, there has hardly been any\nfocus on precarity which is the term that encapsulates the instability in\npeople's lives. That is, a negative outcome can overspread to other decisions\nand measures of well-being. Studying precarity necessitates a shift in focus -\nfrom the point of view of the decision-maker to the perspective of the decision\nsubject. This centering of the subject is an important direction that unlocks\nthe importance of parting with aggregate measures to examine the long-term\neffects of decision making. To address this issue, in this paper, we propose a\nmodeling framework that simulates the effects of compounded decision-making on\nprecarity over time. Through our simulations, we are able to show the\nheterogeneity of precarity by the non-uniform ruinous aftereffects of negative\ndecisions on different income classes of the underlying population and how\npolicy interventions can help mitigate such effects.",
    "published_date": "2021-04-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.12037v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.11883v4",
    "title": "Carrying out CNN Channel Pruning in a White Box",
    "authors": [
      "Yuxin Zhang",
      "Mingbao Lin",
      "Chia-Wen Lin",
      "Jie Chen",
      "Feiyue Huang",
      "Yongjian Wu",
      "Yonghong Tian",
      "Rongrong Ji"
    ],
    "author_ids": [],
    "abstract": "Channel Pruning has been long studied to compress CNNs, which significantly\nreduces the overall computation. Prior works implement channel pruning in an\nunexplainable manner, which tends to reduce the final classification errors\nwhile failing to consider the internal influence of each channel. In this\npaper, we conduct channel pruning in a white box. Through deep visualization of\nfeature maps activated by different channels, we observe that different\nchannels have a varying contribution to different categories in image\nclassification. Inspired by this, we choose to preserve channels contributing\nto most categories. Specifically, to model the contribution of each channel to\ndifferentiating categories, we develop a class-wise mask for each channel,\nimplemented in a dynamic training manner w.r.t. the input image's category. On\nthe basis of the learned class-wise mask, we perform a global voting mechanism\nto remove channels with less category discrimination. Lastly, a fine-tuning\nprocess is conducted to recover the performance of the pruned model. To our\nbest knowledge, it is the first time that CNN interpretability theory is\nconsidered to guide channel pruning. Extensive experiments on representative\nimage classification tasks demonstrate the superiority of our White-Box over\nmany state-of-the-arts. For instance, on CIFAR-10, it reduces 65.23% FLOPs with\neven 0.62% accuracy improvement for ResNet-110. On ILSVRC-2012, White-Box\nachieves a 45.6% FLOPs reduction with only a small loss of 0.83% in the top-1\naccuracy for ResNet-50.",
    "published_date": "2021-04-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.11883v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.11797v1",
    "title": "Ensembles of GANs for synthetic training data generation",
    "authors": [
      "Gabriel Eilertsen",
      "Apostolia Tsirikoglou",
      "Claes Lundström",
      "Jonas Unger"
    ],
    "author_ids": [],
    "abstract": "Insufficient training data is a major bottleneck for most deep learning\npractices, not least in medical imaging where data is difficult to collect and\npublicly available datasets are scarce due to ethics and privacy. This work\ninvestigates the use of synthetic images, created by generative adversarial\nnetworks (GANs), as the only source of training data. We demonstrate that for\nthis application, it is of great importance to make use of multiple GANs to\nimprove the diversity of the generated data, i.e. to sufficiently cover the\ndata distribution. While a single GAN can generate seemingly diverse image\ncontent, training on this data in most cases lead to severe over-fitting. We\ntest the impact of ensembled GANs on synthetic 2D data as well as common image\ndatasets (SVHN and CIFAR-10), and using both DCGANs and progressively growing\nGANs. As a specific use case, we focus on synthesizing digital pathology\npatches to provide anonymized training data.",
    "published_date": "2021-04-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.11797v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.11612v1",
    "title": "Understanding who uses Reddit: Profiling individuals with a self-reported bipolar disorder diagnosis",
    "authors": [
      "Glorianna Jagfeld",
      "Fiona Lobban",
      "Paul Rayson",
      "Steven H. Jones"
    ],
    "author_ids": [],
    "abstract": "Recently, research on mental health conditions using public online data,\nincluding Reddit, has surged in NLP and health research but has not reported\nuser characteristics, which are important to judge generalisability of\nfindings. This paper shows how existing NLP methods can yield information on\nclinical, demographic, and identity characteristics of almost 20K Reddit users\nwho self-report a bipolar disorder diagnosis. This population consists of\nslightly more feminine- than masculine-gendered mainly young or middle-aged\nUS-based adults who often report additional mental health diagnoses, which is\ncompared with general Reddit statistics and epidemiological studies.\nAdditionally, this paper carefully evaluates all methods and discusses ethical\nissues.",
    "published_date": "2021-04-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.11612v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.11538v1",
    "title": "A Multi-Agent Model for Polarization under Confirmation Bias in Social Networks",
    "authors": [
      "Mário S. Alvim",
      "Bernardo Amorim",
      "Sophia Knight",
      "Santiago Quintero",
      "Frank Valencia"
    ],
    "author_ids": [],
    "abstract": "We describe a model for polarization in multi-agent systems based on Esteban\nand Ray's standard measure of polarization from economics. Agents evolve by\nupdating their beliefs (opinions) based on an underlying influence graph, as in\nthe standard DeGroot model for social learning, but under a confirmation bias;\ni.e., a discounting of opinions of agents with dissimilar views. We show that\neven under this bias polarization eventually vanishes (converges to zero) if\nthe influence graph is strongly-connected. If the influence graph is a regular\nsymmetric circulation, we determine the unique belief value to which all agents\nconverge. Our more insightful result establishes that, under some natural\nassumptions, if polarization does not eventually vanish then either there is a\ndisconnected subgroup of agents, or some agent influences others more than she\nis influenced. We also show that polarization does not necessarily vanish in\nweakly-connected graphs under confirmation bias. We illustrate our model with a\nseries of case studies and simulations, and show how it relates to the classic\nDeGroot model for social learning.",
    "published_date": "2021-04-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.11538v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.11436v1",
    "title": "Learning from Ambiguous Labels for Lung Nodule Malignancy Prediction",
    "authors": [
      "Zehui Liao",
      "Yutong Xie",
      "Shishuai Hu",
      "Yong Xia"
    ],
    "author_ids": [],
    "abstract": "Lung nodule malignancy prediction is an essential step in the early diagnosis\nof lung cancer. Besides the difficulties commonly discussed, the challenges of\nthis task also come from the ambiguous labels provided by annotators, since\ndeep learning models may learn, even amplify, the bias embedded in them. In\nthis paper, we propose a multi-view \"divide-and-rule\" (MV-DAR) model to learn\nfrom both reliable and ambiguous annotations for lung nodule malignancy\nprediction. According to the consistency and reliability of their annotations,\nwe divide nodules into three sets: a consistent and reliable set (CR-Set), an\ninconsistent set (IC-Set), and a low reliable set (LR-Set). The nodule in\nIC-Set is annotated by multiple radiologists inconsistently, and the nodule in\nLR-Set is annotated by only one radiologist. The proposed MV-DAR contains three\nDAR submodels to characterize a lung nodule from three orthographic views. Each\nDAR consists of a prediction network (Prd-Net), a counterfactual network\n(CF-Net), and a low reliable network (LR-Net), learning on CR-Set, IC-Set, and\nLR-Set, respectively. The image representation ability learned by CF-Net and\nLR-Net is then transferred to Prd-Net by negative-attention module (NA-Module)\nand consistent-attention module (CA-Module), aiming to boost the prediction\nability of Prd-Net. The MV-DAR model has been evaluated on the LIDC-IDRI\ndataset and LUNGx dataset. Our results indicate not only the effectiveness of\nthe proposed MV-DAR model in learning from ambiguous labels but also its\nsuperiority over present noisy label-learning models in lung nodule malignancy\nprediction.",
    "published_date": "2021-04-23T00:00:00",
    "year": 2021,
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.11436v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.11092v1",
    "title": "Survey on Modeling Intensity Function of Hawkes Process Using Neural Models",
    "authors": [
      "Jayesh Malaviya"
    ],
    "author_ids": [],
    "abstract": "The event sequence of many diverse systems is represented as a sequence of\ndiscrete events in a continuous space. Examples of such an event sequence are\nearthquake aftershock events, financial transactions, e-commerce transactions,\nsocial network activity of a user, and the user's web search pattern. Finding\nsuch an intricate pattern helps discover which event will occur in the future\nand when it will occur. A Hawkes process is a mathematical tool used for\nmodeling such time series discrete events. Traditionally, the Hawkes process\nuses a critical component for modeling data as an intensity function with a\nparameterized kernel function. The Hawkes process's intensity function involves\ntwo components: the background intensity and the effect of events' history.\nHowever, such parameterized assumption can not capture future event\ncharacteristics using past events data precisely due to bias in modeling kernel\nfunction. This paper explores the recent advancement using novel deep\nlearning-based methods to model kernel function to remove such parametrized\nkernel function. In the end, we will give potential future research directions\nto improve modeling using the Hawkes process.",
    "published_date": "2021-04-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.11092v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.10916v1",
    "title": "Automated Tackle Injury Risk Assessment in Contact-Based Sports -- A Rugby Union Example",
    "authors": [
      "Zubair Martin",
      "Amir Patel",
      "Sharief Hendricks"
    ],
    "author_ids": [],
    "abstract": "Video analysis in tackle-collision based sports is highly subjective and\nexposed to bias, which is inherent in human observation, especially under time\nconstraints. This limitation of match analysis in tackle-collision based sports\ncan be seen as an opportunity for computer vision applications. Objectively\ntracking, detecting and recognising an athlete's movements and actions during\nmatch play from a distance using video, along with our improved understanding\nof injury aetiology and skill execution will enhance our understanding how\ninjury occurs, assist match day injury management, reduce referee subjectivity.\nIn this paper, we present a system of objectively evaluating in-game tackle\nrisk in rugby union matches. First, a ball detection model is trained using the\nYou Only Look Once (YOLO) framework, these detections are then tracked by a\nKalman Filter (KF). Following this, a separate YOLO model is used to detect\npersons/players within a tackle segment and then the ball-carrier and tackler\nare identified. Subsequently, we utilize OpenPose to determine the pose of\nball-carrier and tackle, the relative pose of these is then used to evaluate\nthe risk of the tackle. We tested the system on a diverse collection of rugby\ntackles and achieved an evaluation accuracy of 62.50%. These results will\nenable referees in tackle-contact based sports to make more subjective\ndecisions, ultimately making these sports safer.",
    "published_date": "2021-04-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.10916v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.10857v3",
    "title": "Attribute-Modulated Generative Meta Learning for Zero-Shot Classification",
    "authors": [
      "Yun Li",
      "Zhe Liu",
      "Lina Yao",
      "Xiaojun Chang"
    ],
    "author_ids": [],
    "abstract": "Zero-shot learning (ZSL) aims to transfer knowledge from seen classes to\nsemantically related unseen classes, which are absent during training. The\npromising strategies for ZSL are to synthesize visual features of unseen\nclasses conditioned on semantic side information and to incorporate\nmeta-learning to eliminate the model's inherent bias towards seen classes.\nWhile existing meta generative approaches pursue a common model shared across\ntask distributions, we aim to construct a generative network adaptive to task\ncharacteristics. To this end, we propose an Attribute-Modulated generAtive\nmeta-model for Zero-shot learning (AMAZ). Our model consists of an\nattribute-aware modulation network, an attribute-augmented generative network,\nand an attribute-weighted classifier. Given unseen classes, the modulation\nnetwork adaptively modulates the generator by applying task-specific\ntransformations so that the generative network can adapt to highly diverse\ntasks. The weighted classifier utilizes the data quality to enhance the\ntraining procedure, further improving the model performance. Our empirical\nevaluations on four widely-used benchmarks show that AMAZ outperforms\nstate-of-the-art methods by 3.8% and 3.1% in ZSL and generalized ZSL settings,\nrespectively, demonstrating the superiority of our method. Our experiments on a\nzero-shot image retrieval task show AMAZ's ability to synthesize instances that\nportray real visual characteristics.",
    "published_date": "2021-04-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.10857v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.10751v5",
    "title": "Rule Generation for Classification: Scalability, Interpretability, and Fairness",
    "authors": [
      "Tabea E. Röber",
      "Adia C. Lumadjeng",
      "M. Hakan Akyüz",
      "Ş. İlker Birbil"
    ],
    "author_ids": [],
    "abstract": "We introduce a new rule-based optimization method for classification with\nconstraints. The proposed method leverages column generation for linear\nprogramming, and hence, is scalable to large datasets. The resulting pricing\nsubproblem is shown to be NP-Hard. We recourse to a decision tree-based\nheuristic and solve a proxy pricing subproblem for acceleration. The method\nreturns a set of rules along with their optimal weights indicating the\nimportance of each rule for learning. We address interpretability and fairness\nby assigning cost coefficients to the rules and introducing additional\nconstraints. In particular, we focus on local interpretability and generalize a\nseparation criterion in fairness to multiple sensitive attributes and classes.\nWe test the performance of the proposed methodology on a collection of datasets\nand present a case study to elaborate on its different aspects. The proposed\nrule-based learning method exhibits a good compromise between local\ninterpretability and fairness on the one side, and accuracy on the other side.",
    "published_date": "2021-04-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.10751v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.10747v2",
    "title": "Accented Speech Recognition: A Survey",
    "authors": [
      "Arthur Hinsvark",
      "Natalie Delworth",
      "Miguel Del Rio",
      "Quinten McNamara",
      "Joshua Dong",
      "Ryan Westerman",
      "Michelle Huang",
      "Joseph Palakapilly",
      "Jennifer Drexler",
      "Ilya Pirkin",
      "Nishchal Bhandari",
      "Miguel Jette"
    ],
    "author_ids": [],
    "abstract": "Automatic Speech Recognition (ASR) systems generalize poorly on accented\nspeech. The phonetic and linguistic variability of accents present hard\nchallenges for ASR systems today in both data collection and modeling\nstrategies. The resulting bias in ASR performance across accents comes at a\ncost to both users and providers of ASR.\n  We present a survey of current promising approaches to accented speech\nrecognition and highlight the key challenges in the space. Approaches mostly\nfocus on single model generalization and accent feature engineering. Among the\nchallenges, lack of a standard benchmark makes research and comparison\nespecially difficult.",
    "published_date": "2021-04-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.10747v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.10705v1",
    "title": "Multi-Class Micro-CT Image Segmentation Using Sparse Regularized Deep Networks",
    "authors": [
      "Amirsaeed Yazdani",
      "Yung-Chen Sun",
      "Nicholas B. Stephens",
      "Timothy Ryan",
      "Vishal Monga"
    ],
    "author_ids": [],
    "abstract": "It is common in anthropology and paleontology to address questions about\nextant and extinct species through the quantification of osteological features\nobservable in micro-computed tomographic (micro-CT) scans. In cases where\nremains were buried, the grey values present in these scans may be classified\nas belonging to air, dirt, or bone. While various intensity-based methods have\nbeen proposed to segment scans into these classes, it is often the case that\nintensity values for dirt and bone are nearly indistinguishable. In these\ninstances, scientists resort to laborious manual segmentation, which does not\nscale well in practice when a large number of scans are to be analyzed. Here we\npresent a new domain-enriched network for three-class image segmentation, which\nutilizes the domain knowledge of experts familiar with manually segmenting bone\nand dirt structures. More precisely, our novel structure consists of two\ncomponents: 1) a representation network trained on special samples based on\nnewly designed custom loss terms, which extracts discriminative bone and dirt\nfeatures, 2) and a segmentation network that leverages these extracted\ndiscriminative features. These two parts are jointly trained in order to\noptimize the segmentation performance. A comparison of our network to that of\nthe current state-of-the-art U-NETs demonstrates the benefits of our proposal,\nparticularly when the number of labeled training images are limited, which is\ninvariably the case for micro-CT segmentation.",
    "published_date": "2021-04-21T00:00:00",
    "year": 2021,
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.10705v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.10671v1",
    "title": "User-oriented Fairness in Recommendation",
    "authors": [
      "Yunqi Li",
      "Hanxiong Chen",
      "Zuohui Fu",
      "Yingqiang Ge",
      "Yongfeng Zhang"
    ],
    "author_ids": [],
    "abstract": "As a highly data-driven application, recommender systems could be affected by\ndata bias, resulting in unfair results for different data groups, which could\nbe a reason that affects the system performance. Therefore, it is important to\nidentify and solve the unfairness issues in recommendation scenarios. In this\npaper, we address the unfairness problem in recommender systems from the user\nperspective. We group users into advantaged and disadvantaged groups according\nto their level of activity, and conduct experiments to show that current\nrecommender systems will behave unfairly between two groups of users.\nSpecifically, the advantaged users (active) who only account for a small\nproportion in data enjoy much higher recommendation quality than those\ndisadvantaged users (inactive). Such bias can also affect the overall\nperformance since the disadvantaged users are the majority. To solve this\nproblem, we provide a re-ranking approach to mitigate this unfairness problem\nby adding constraints over evaluation metrics. The experiments we conducted on\nseveral real-world datasets with various recommendation algorithms show that\nour approach can not only improve group fairness of users in recommender\nsystems, but also achieve better overall recommendation performance.",
    "published_date": "2021-04-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.10671v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.10217v1",
    "title": "Bias-Aware Loss for Training Image and Speech Quality Prediction Models from Multiple Datasets",
    "authors": [
      "Gabriel Mittag",
      "Saman Zadtootaghaj",
      "Thilo Michael",
      "Babak Naderi",
      "Sebastian Möller"
    ],
    "author_ids": [],
    "abstract": "The ground truth used for training image, video, or speech quality prediction\nmodels is based on the Mean Opinion Scores (MOS) obtained from subjective\nexperiments. Usually, it is necessary to conduct multiple experiments, mostly\nwith different test participants, to obtain enough data to train quality models\nbased on machine learning. Each of these experiments is subject to an\nexperiment-specific bias, where the rating of the same file may be\nsubstantially different in two experiments (e.g. depending on the overall\nquality distribution). These different ratings for the same distortion levels\nconfuse neural networks during training and lead to lower performance. To\novercome this problem, we propose a bias-aware loss function that estimates\neach dataset's biases during training with a linear function and considers it\nwhile optimising the network weights. We prove the efficiency of the proposed\nmethod by training and validating quality prediction models on synthetic and\nsubjective image and speech quality datasets.",
    "published_date": "2021-04-20T00:00:00",
    "year": 2021,
    "categories": [
      "eess.AS",
      "cs.LG",
      "cs.SD",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.10217v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.10134v2",
    "title": "Egalitarian and Congestion Aware Truthful Airport Slot Allocation Mechanism",
    "authors": [
      "Aasheesh Dixit",
      "Garima Shakya",
      "Suresh Kumar Jakhar",
      "Swaprava Nath"
    ],
    "author_ids": [],
    "abstract": "We propose a mechanism to allocate slots fairly at congested airports. This\nmechanism: (a) ensures that the slots are allocated according to the true\nvaluations of airlines, (b) provides fair opportunities to the flights\nconnecting remote cities to large airports, and (c) controls the number of\nflights in each slot to minimize congestion. The mechanism draws inspiration\nfrom economic theory. It allocates the slots based on an affine maximizer\nallocation rule and charges payments to the airlines such that they are\nincentivized to reveal their true valuations. The allocation also optimizes the\noccupancy of every slot to keep them as uncongested as possible. The\nformulation solves an optimal integral solution in strongly polynomial time. We\nconduct experiments on the data collected from two major airports in India. We\nalso compare our results with existing allocations and also with the\nallocations based on the International Air Transport Association (IATA)\nguidelines. The computational results show that the social utility generated\nusing our mechanism is 20-30% higher than IATA and current allocations.",
    "published_date": "2021-04-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.10134v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.10097v1",
    "title": "Beyond Fair Pay: Ethical Implications of NLP Crowdsourcing",
    "authors": [
      "Boaz Shmueli",
      "Jan Fell",
      "Soumya Ray",
      "Lun-Wei Ku"
    ],
    "author_ids": [],
    "abstract": "The use of crowdworkers in NLP research is growing rapidly, in tandem with\nthe exponential increase in research production in machine learning and AI.\nEthical discussion regarding the use of crowdworkers within the NLP research\ncommunity is typically confined in scope to issues related to labor conditions\nsuch as fair pay. We draw attention to the lack of ethical considerations\nrelated to the various tasks performed by workers, including labeling,\nevaluation, and production. We find that the Final Rule, the common ethical\nframework used by researchers, did not anticipate the use of online\ncrowdsourcing platforms for data collection, resulting in gaps between the\nspirit and practice of human-subjects ethics in NLP research. We enumerate\ncommon scenarios where crowdworkers performing NLP tasks are at risk of harm.\nWe thus recommend that researchers evaluate these risks by considering the\nthree ethical principles set up by the Belmont Report. We also clarify some\ncommon misconceptions regarding the Institutional Review Board (IRB)\napplication. We hope this paper will serve to reopen the discussion within our\ncommunity regarding the ethical use of crowdworkers.",
    "published_date": "2021-04-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.10097v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.10093v2",
    "title": "Class-Incremental Learning with Generative Classifiers",
    "authors": [
      "Gido M. van de Ven",
      "Zhe Li",
      "Andreas S. Tolias"
    ],
    "author_ids": [],
    "abstract": "Incrementally training deep neural networks to recognize new classes is a\nchallenging problem. Most existing class-incremental learning methods store\ndata or use generative replay, both of which have drawbacks, while\n'rehearsal-free' alternatives such as parameter regularization or\nbias-correction methods do not consistently achieve high performance. Here, we\nput forward a new strategy for class-incremental learning: generative\nclassification. Rather than directly learning the conditional distribution\np(y|x), our proposal is to learn the joint distribution p(x,y), factorized as\np(x|y)p(y), and to perform classification using Bayes' rule. As a\nproof-of-principle, here we implement this strategy by training a variational\nautoencoder for each class to be learned and by using importance sampling to\nestimate the likelihoods p(x|y). This simple approach performs very well on a\ndiverse set of continual learning benchmarks, outperforming generative replay\nand other existing baselines that do not store data.",
    "published_date": "2021-04-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.10093v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.10079v1",
    "title": "Development of an accessible 10-year Digital CArdioVAscular (DiCAVA) risk assessment: a UK Biobank study",
    "authors": [
      "Nikola Dolezalova",
      "Angus B. Reed",
      "Alex Despotovic",
      "Bernard Dillon Obika",
      "Davide Morelli",
      "Mert Aral",
      "David Plans"
    ],
    "author_ids": [],
    "abstract": "Background: Cardiovascular diseases (CVDs) are among the leading causes of\ndeath worldwide. Predictive scores providing personalised risk of developing\nCVD are increasingly used in clinical practice. Most scores, however, utilise a\nhomogenous set of features and require the presence of a physician.\n  Objective: The aim was to develop a new risk model (DiCAVA) using statistical\nand machine learning techniques that could be applied in a remote setting. A\nsecondary goal was to identify new patient-centric variables that could be\nincorporated into CVD risk assessments.\n  Methods: Across 466,052 participants, Cox proportional hazards (CPH) and\nDeepSurv models were trained using 608 variables derived from the UK Biobank to\ninvestigate the 10-year risk of developing a CVD. Data-driven feature selection\nreduced the number of features to 47, after which reduced models were trained.\nBoth models were compared to the Framingham score.\n  Results: The reduced CPH model achieved a c-index of 0.7443, whereas DeepSurv\nachieved a c-index of 0.7446. Both CPH and DeepSurv were superior in\ndetermining the CVD risk compared to Framingham score. Minimal difference was\nobserved when cholesterol and blood pressure were excluded from the models\n(CPH: 0.741, DeepSurv: 0.739). The models show very good calibration and\ndiscrimination on the test data.\n  Conclusion: We developed a cardiovascular risk model that has very good\npredictive capacity and encompasses new variables. The score could be\nincorporated into clinical practice and utilised in a remote setting, without\nthe need of including cholesterol. Future studies will focus on external\nvalidation across heterogeneous samples.",
    "published_date": "2021-04-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.10079v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.09918v1",
    "title": "CrossATNet - A Novel Cross-Attention Based Framework for Sketch-Based Image Retrieval",
    "authors": [
      "Ushasi Chaudhuri",
      "Biplab Banerjee",
      "Avik Bhattacharya",
      "Mihai Datcu"
    ],
    "author_ids": [],
    "abstract": "We propose a novel framework for cross-modal zero-shot learning (ZSL) in the\ncontext of sketch-based image retrieval (SBIR). Conventionally, the SBIR schema\nmainly considers simultaneous mappings among the two image views and the\nsemantic side information. Therefore, it is desirable to consider fine-grained\nclasses mainly in the sketch domain using highly discriminative and\nsemantically rich feature space. However, the existing deep generative\nmodeling-based SBIR approaches majorly focus on bridging the gaps between the\nseen and unseen classes by generating pseudo-unseen-class samples. Besides,\nviolating the ZSL protocol by not utilizing any unseen-class information during\ntraining, such techniques do not pay explicit attention to modeling the\ndiscriminative nature of the shared space. Also, we note that learning a\nunified feature space for both the multi-view visual data is a tedious task\nconsidering the significant domain difference between sketches and color\nimages. In this respect, as a remedy, we introduce a novel framework for\nzero-shot SBIR. While we define a cross-modal triplet loss to ensure the\ndiscriminative nature of the shared space, an innovative cross-modal attention\nlearning strategy is also proposed to guide feature extraction from the image\ndomain exploiting information from the respective sketch counterpart. In order\nto preserve the semantic consistency of the shared space, we consider a graph\nCNN-based module that propagates the semantic class topology to the shared\nspace. To ensure an improved response time during inference, we further explore\nthe possibility of representing the shared space in terms of hash codes.\nExperimental results obtained on the benchmark TU-Berlin and the Sketchy\ndatasets confirm the superiority of CrossATNet in yielding state-of-the-art\nresults.",
    "published_date": "2021-04-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.09918v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.09789v1",
    "title": "Does enhanced shape bias improve neural network robustness to common corruptions?",
    "authors": [
      "Chaithanya Kumar Mummadi",
      "Ranjitha Subramaniam",
      "Robin Hutmacher",
      "Julien Vitay",
      "Volker Fischer",
      "Jan Hendrik Metzen"
    ],
    "author_ids": [],
    "abstract": "Convolutional neural networks (CNNs) learn to extract representations of\ncomplex features, such as object shapes and textures to solve image recognition\ntasks. Recent work indicates that CNNs trained on ImageNet are biased towards\nfeatures that encode textures and that these alone are sufficient to generalize\nto unseen test data from the same distribution as the training data but often\nfail to generalize to out-of-distribution data. It has been shown that\naugmenting the training data with different image styles decreases this texture\nbias in favor of increased shape bias while at the same time improving\nrobustness to common corruptions, such as noise and blur. Commonly, this is\ninterpreted as shape bias increasing corruption robustness. However, this\nrelationship is only hypothesized. We perform a systematic study of different\nways of composing inputs based on natural images, explicit edge information,\nand stylization. While stylization is essential for achieving high corruption\nrobustness, we do not find a clear correlation between shape bias and\nrobustness. We conclude that the data augmentation caused by style-variation\naccounts for the improved corruption robustness and increased shape bias is\nonly a byproduct.",
    "published_date": "2021-04-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.09789v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.09684v2",
    "title": "Suppressing simulation bias using multi-modal data",
    "authors": [
      "Bogdan Kustowski",
      "Jim A. Gaffney",
      "Brian K. Spears",
      "Gemma J. Anderson",
      "Rushil Anirudh",
      "Peer-Timo Bremer",
      "Jayaraman J. Thiagarajan",
      "Michael K. G. Kruse",
      "Ryan C. Nora"
    ],
    "author_ids": [],
    "abstract": "Many problems in science and engineering require making predictions based on\nfew observations. To build a robust predictive model, these sparse data may\nneed to be augmented with simulated data, especially when the design space is\nmulti-dimensional. Simulations, however, often suffer from an inherent bias.\nEstimation of this bias may be poorly constrained not only because of data\nsparsity, but also because traditional predictive models fit only one type of\nobserved outputs, such as scalars or images, instead of all available output\ndata modalities, which might have been acquired and simulated at great cost. To\nbreak this limitation and open up the path for multi-modal calibration, we\npropose to combine a novel, transfer learning technique for suppressing the\nbias with recent developments in deep learning, which allow building predictive\nmodels with multi-modal outputs. First, we train an initial neural network\nmodel on simulated data to learn important correlations between different\noutput modalities and between simulation inputs and outputs. Then, the model is\npartially retrained, or transfer learned, to fit the experiments; a method that\nhas never been implemented in this type of architecture. Using fewer than 10\ninertial confinement fusion experiments for training, transfer learning\nsystematically improves the simulation predictions while a simple output\ncalibration, which we design as a baseline, makes the predictions worse. We\nalso offer extensive cross-validation with real and carefully designed\nsynthetic data. The method described in this paper can be applied to a wide\nrange of problems that require transferring knowledge from simulations to the\ndomain of experiments.",
    "published_date": "2021-04-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.09684v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.09415v1",
    "title": "Cross-Domain Adaptive Clustering for Semi-Supervised Domain Adaptation",
    "authors": [
      "Jichang Li",
      "Guanbin Li",
      "Yemin Shi",
      "Yizhou Yu"
    ],
    "author_ids": [],
    "abstract": "In semi-supervised domain adaptation, a few labeled samples per class in the\ntarget domain guide features of the remaining target samples to aggregate\naround them. However, the trained model cannot produce a highly discriminative\nfeature representation for the target domain because the training data is\ndominated by labeled samples from the source domain. This could lead to\ndisconnection between the labeled and unlabeled target samples as well as\nmisalignment between unlabeled target samples and the source domain. In this\npaper, we propose a novel approach called Cross-domain Adaptive Clustering to\naddress this problem. To achieve both inter-domain and intra-domain adaptation,\nwe first introduce an adversarial adaptive clustering loss to group features of\nunlabeled target data into clusters and perform cluster-wise feature alignment\nacross the source and target domains. We further apply pseudo labeling to\nunlabeled samples in the target domain and retain pseudo-labels with high\nconfidence. Pseudo labeling expands the number of ``labeled\" samples in each\nclass in the target domain, and thus produces a more robust and powerful\ncluster core for each class to facilitate adversarial learning. Extensive\nexperiments on benchmark datasets, including DomainNet, Office-Home and Office,\ndemonstrate that our proposed approach achieves the state-of-the-art\nperformance in semi-supervised domain adaptation.",
    "published_date": "2021-04-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.09415v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.09402v1",
    "title": "Agent-Centric Representations for Multi-Agent Reinforcement Learning",
    "authors": [
      "Wenling Shang",
      "Lasse Espeholt",
      "Anton Raichuk",
      "Tim Salimans"
    ],
    "author_ids": [],
    "abstract": "Object-centric representations have recently enabled significant progress in\ntackling relational reasoning tasks. By building a strong object-centric\ninductive bias into neural architectures, recent efforts have improved\ngeneralization and data efficiency of machine learning algorithms for these\nproblems. One problem class involving relational reasoning that still remains\nunder-explored is multi-agent reinforcement learning (MARL). Here we\ninvestigate whether object-centric representations are also beneficial in the\nfully cooperative MARL setting. Specifically, we study two ways of\nincorporating an agent-centric inductive bias into our RL algorithm: 1.\nIntroducing an agent-centric attention module with explicit connections across\nagents 2. Adding an agent-centric unsupervised predictive objective (i.e. not\nusing action labels), to be used as an auxiliary loss for MARL, or as the basis\nof a pre-training step. We evaluate these approaches on the Google Research\nFootball environment as well as DeepMind Lab 2D. Empirically, agent-centric\nrepresentation learning leads to the emergence of more complex cooperation\nstrategies between agents as well as enhanced sample efficiency and\ngeneralization.",
    "published_date": "2021-04-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.09402v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.09066v1",
    "title": "IIITT@LT-EDI-EACL2021-Hope Speech Detection: There is always Hope in Transformers",
    "authors": [
      "Karthik Puranik",
      "Adeep Hande",
      "Ruba Priyadharshini",
      "Sajeetha Thavareesan",
      "Bharathi Raja Chakravarthi"
    ],
    "author_ids": [],
    "abstract": "In a world filled with serious challenges like climate change, religious and\npolitical conflicts, global pandemics, terrorism, and racial discrimination, an\ninternet full of hate speech, abusive and offensive content is the last thing\nwe desire for. In this paper, we work to identify and promote positive and\nsupportive content on these platforms. We work with several transformer-based\nmodels to classify social media comments as hope speech or not-hope speech in\nEnglish, Malayalam and Tamil languages. This paper portrays our work for the\nShared Task on Hope Speech Detection for Equality, Diversity, and Inclusion at\nLT-EDI 2021- EACL 2021.",
    "published_date": "2021-04-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.09066v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.09052v2",
    "title": "Metadata Normalization",
    "authors": [
      "Mandy Lu",
      "Qingyu Zhao",
      "Jiequan Zhang",
      "Kilian M. Pohl",
      "Li Fei-Fei",
      "Juan Carlos Niebles",
      "Ehsan Adeli"
    ],
    "author_ids": [],
    "abstract": "Batch Normalization (BN) and its variants have delivered tremendous success\nin combating the covariate shift induced by the training step of deep learning\nmethods. While these techniques normalize feature distributions by\nstandardizing with batch statistics, they do not correct the influence on\nfeatures from extraneous variables or multiple distributions. Such extra\nvariables, referred to as metadata here, may create bias or confounding effects\n(e.g., race when classifying gender from face images). We introduce the\nMetadata Normalization (MDN) layer, a new batch-level operation which can be\nused end-to-end within the training framework, to correct the influence of\nmetadata on feature distributions. MDN adopts a regression analysis technique\ntraditionally used for preprocessing to remove (regress out) the metadata\neffects on model features during training. We utilize a metric based on\ndistance correlation to quantify the distribution bias from the metadata and\ndemonstrate that our method successfully removes metadata effects on four\ndiverse settings: one synthetic, one 2D image, one video, and one 3D medical\nimage dataset.",
    "published_date": "2021-04-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.09052v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.09024v1",
    "title": "TFROM: A Two-sided Fairness-Aware Recommendation Model for Both Customers and Providers",
    "authors": [
      "Yao Wu",
      "Jian Cao",
      "Guandong Xu",
      "Yudong Tan"
    ],
    "author_ids": [],
    "abstract": "At present, most research on the fairness of recommender systems is conducted\neither from the perspective of customers or from the perspective of product (or\nservice) providers. However, such a practice ignores the fact that when\nfairness is guaranteed to one side, the fairness and rights of the other side\nare likely to reduce. In this paper, we consider recommendation scenarios from\nthe perspective of two sides (customers and providers). From the perspective of\nproviders, we consider the fairness of the providers' exposure in recommender\nsystem. For customers, we consider the fairness of the reduced quality of\nrecommendation results due to the introduction of fairness measures. We\ntheoretically analyzed the relationship between recommendation quality,\ncustomers fairness, and provider fairness, and design a two-sided\nfairness-aware recommendation model (TFROM) for both customers and providers.\nSpecifically, we design two versions of TFROM for offline and online\nrecommendation. The effectiveness of the model is verified on three real-world\ndata sets. The experimental results show that TFROM provides better two-sided\nfairness while still maintaining a higher level of personalization than the\nbaseline algorithms.",
    "published_date": "2021-04-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.09024v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.09002v2",
    "title": "On the Complexity of Inverse Mixed Integer Linear Optimization",
    "authors": [
      "Aykut Bulut",
      "Ted K. Ralphs"
    ],
    "author_ids": [],
    "abstract": "Inverse optimization is the problem of determining the values of missing\ninput parameters for an associated forward problem that are closest to given\nestimates and that will make a given target vector optimal. This study is\nconcerned with the relationship of a particular inverse mixed integer linear\noptimization problem (MILP) to both the forward problem and the separation\nproblem associated with its feasible region. We show that a decision version of\nthe inverse MILP in which a primal bound is verified is coNP-complete, whereas\nprimal bound verification for the associated forward problem is NP-complete,\nand that the optimal value verification problems for both the inverse problem\nand the associated forward problem are complete for the complexity class D^P.\nWe also describe a cutting-plane algorithm for solving inverse MILPs that\nillustrates the close relationship between the separation problem for the\nconvex hull of solutions to a given MILP and the associated inverse problem.\nThe inverse problem is shown to be equivalent to the separation problem for the\nradial cone defined by all inequalities that are both valid for the convex hull\nof solutions to the forward problem and binding at the target vector. Thus, the\ninverse, forward, and separation problems can be said to be equivalent.",
    "published_date": "2021-04-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CC",
      "math.OC",
      "90C11, 68Q25"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.09002v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.08820v2",
    "title": "Fair Coin Flipping: Tighter Analysis and the Many-Party Case",
    "authors": [
      "Niv Buchbinder",
      "Iftach Haitner",
      "Nissan Levi",
      "Eliad Tsfadia"
    ],
    "author_ids": [],
    "abstract": "In a multi-party fair coin-flipping protocol, the parties output a common\n(close to) unbiased bit, even when some adversarial parties try to bias the\noutput. In this work we focus on the case of an arbitrary number of corrupted\nparties. Cleve [STOC 1986] has shown that in any such $m$-round coin-flipping\nprotocol, the corrupted parties can bias the honest parties' common output bit\nby $\\Theta(1/m)$. For more than two decades, the best known coin-flipping\nprotocol was the one of Awerbuch et al. [Manuscript 1985], who presented a\n$t$-party, $m$-round protocol with bias $\\Theta(t/\\sqrt{m})$. This was changed\nby the breakthrough result of Moran et al. [TCC 2009], who constructed an\n$m$-round, two-party coin-flipping protocol with optimal bias $\\Theta(1/m)$.\nHaitner and Tsfadia [STOC 2014] constructed an $m$-round, three-party\ncoin-flipping protocol with bias $O(\\log^3m / m)$. Still for the case of more\nthan three parties, the best known protocol remained the\n$\\Theta(t/\\sqrt{m})$-bias protocol of Awerbuch et al.\n  We make a step towards eliminating the above gap, presenting a $t$-party,\n$m$-round coin-flipping protocol, with bias $O(\\frac{t^4 \\cdot 2^t \\cdot\n\\sqrt{\\log m}}{m^{1/2+1/\\left(2^{t-1}-2\\right)}})$ for any $t\\le \\tfrac12\n\\log\\log m$. This improves upon the $\\Theta(t/\\sqrt{m})$-bias protocol of\nAwerbuch et al., and in particular, for $t\\in O(1)$ it is an $1/m^{\\frac12 +\n\\Theta(1)}$-bias protocol. For the three-party case, it is an $O(\\sqrt{\\log\nm}/m)$-bias protocol, improving over the $O(\\log^3m / m)$-bias protocol of\nHaitner and Tsfadia.\n  Our protocol generalizes that of Haitner and Tsfadia, by presenting an\nappropriate recovery protocol for the remaining parties to interact in, in the\ncase that some parties abort or are caught cheating. We prove the fairness of\nthe new protocol by presenting a new paradigm for analyzing fairness of\ncoin-flipping protocols.",
    "published_date": "2021-04-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.08820v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.08769v1",
    "title": "Fair Representation Learning for Heterogeneous Information Networks",
    "authors": [
      "Ziqian Zeng",
      "Rashidul Islam",
      "Kamrun Naher Keya",
      "James Foulds",
      "Yangqiu Song",
      "Shimei Pan"
    ],
    "author_ids": [],
    "abstract": "Recently, much attention has been paid to the societal impact of AI,\nespecially concerns regarding its fairness. A growing body of research has\nidentified unfair AI systems and proposed methods to debias them, yet many\nchallenges remain. Representation learning for Heterogeneous Information\nNetworks (HINs), a fundamental building block used in complex network mining,\nhas socially consequential applications such as automated career counseling,\nbut there have been few attempts to ensure that it will not encode or amplify\nharmful biases, e.g. sexism in the job market. To address this gap, in this\npaper we propose a comprehensive set of de-biasing methods for fair HINs\nrepresentation learning, including sampling-based, projection-based, and graph\nneural networks (GNNs)-based techniques. We systematically study the behavior\nof these algorithms, especially their capability in balancing the trade-off\nbetween fairness and prediction accuracy. We evaluate the performance of the\nproposed methods in an automated career counseling application where we\nmitigate gender bias in career recommendation. Based on the evaluation results\non two datasets, we identify the most effective fair HINs representation\nlearning techniques under different conditions.",
    "published_date": "2021-04-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.08769v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.08739v1",
    "title": "Continuity-Discrimination Convolutional Neural Network for Visual Object Tracking",
    "authors": [
      "Shen Li",
      "Bingpeng Ma",
      "Hong Chang",
      "Shiguang Shan",
      "Xilin Chen"
    ],
    "author_ids": [],
    "abstract": "This paper proposes a novel model, named Continuity-Discrimination\nConvolutional Neural Network (CD-CNN), for visual object tracking. Existing\nstate-of-the-art tracking methods do not deal with temporal relationship in\nvideo sequences, which leads to imperfect feature representations. To address\nthis problem, CD-CNN models temporal appearance continuity based on the idea of\ntemporal slowness. Mathematically, we prove that, by introducing temporal\nappearance continuity into tracking, the upper bound of target appearance\nrepresentation error can be sufficiently small with high probability. Further,\nin order to alleviate inaccurate target localization and drifting, we propose a\nnovel notion, object-centroid, to characterize not only objectness but also the\nrelative position of the target within a given patch. Both temporal appearance\ncontinuity and object-centroid are jointly learned during offline training and\nthen transferred for online tracking. We evaluate our tracker through extensive\nexperiments on two challenging benchmarks and show its competitive tracking\nperformance compared with state-of-the-art trackers.",
    "published_date": "2021-04-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.08739v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.08717v4",
    "title": "Do We Really Need Dice? The Hidden Region-Size Biases of Segmentation Losses",
    "authors": [
      "Bingyuan Liu",
      "Jose Dolz",
      "Adrian Galdran",
      "Riadh Kobbi",
      "Ismail Ben Ayed"
    ],
    "author_ids": [],
    "abstract": "Most segmentation losses are arguably variants of the Cross-Entropy (CE) or\nDice losses. On the surface, these two categories of losses seem unrelated, and\nthere is no clear consensus as to which category is a better choice, with\nvarying performances for each across different benchmarks and applications.\nFurthermore, it is widely argued within the medical-imaging community that Dice\nand CE are complementary, which has motivated the use of compound CE-Dice\nlosses. In this work, we provide a theoretical analysis, which shows that CE\nand Dice share a much deeper connection than previously thought. First, we show\nthat, from a constrained-optimization perspective, they both decompose into two\ncomponents, i.e., a similar ground-truth matching term, which pushes the\npredicted foreground regions towards the ground-truth, and a region-size\npenalty term imposing different biases on the size of the predicted regions.\nThen, we provide bound relationships and an information-theoretic analysis,\nwhich uncover hidden region-size biases: Dice has an intrinsic bias towards\nspecific extremely imbalanced solutions, whereas CE implicitly encourages the\nground-truth region proportions. Our theoretical results explain the wide\nexperimental evidence in the medical-imaging literature, whereby Dice losses\nbring improvements for imbalanced segmentation. Based on our theoretical\nanalysis, we propose a principled and simple solution, which enables to control\nexplicitly the region-size bias. The proposed method integrates CE with\nexplicit terms based on L1 or the KL divergence, which encourage segmenting\nregion proportions to match target class proportions, thereby mitigating class\nimbalance but without losing generality. Comprehensive experiments and ablation\nstudies over different losses and applications validate our theoretical\nanalysis, as well as the effectiveness of explicit and simple region-size\nterms.",
    "published_date": "2021-04-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.08717v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.08702v1",
    "title": "Reconsidering CO2 emissions from Computer Vision",
    "authors": [
      "Andre Fu",
      "Mahdi S. Hosseini",
      "Konstantinos N. Plataniotis"
    ],
    "author_ids": [],
    "abstract": "Climate change is a pressing issue that is currently affecting and will\naffect every part of our lives. It's becoming incredibly vital we, as a\nsociety, address the climate crisis as a universal effort, including those in\nthe Computer Vision (CV) community. In this work, we analyze the total cost of\nCO2 emissions by breaking it into (1) the architecture creation cost and (2)\nthe life-time evaluation cost. We show that over time, these costs are\nnon-negligible and are having a direct impact on our future. Importantly, we\nconduct an ethical analysis of how the CV-community is unintentionally\noverlooking its own ethical AI principles by emitting this level of CO2. To\naddress these concerns, we propose adding \"enforcement\" as a pillar of ethical\nAI and provide some recommendations for how architecture designers and broader\nCV community can curb the climate crisis.",
    "published_date": "2021-04-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.08702v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.08666v2",
    "title": "Worst of Both Worlds: Biases Compound in Pre-trained Vision-and-Language Models",
    "authors": [
      "Tejas Srinivasan",
      "Yonatan Bisk"
    ],
    "author_ids": [],
    "abstract": "Numerous works have analyzed biases in vision and pre-trained language models\nindividually - however, less attention has been paid to how these biases\ninteract in multimodal settings. This work extends text-based bias analysis\nmethods to investigate multimodal language models, and analyzes intra- and\ninter-modality associations and biases learned by these models. Specifically,\nwe demonstrate that VL-BERT (Su et al., 2020) exhibits gender biases, often\npreferring to reinforce a stereotype over faithfully describing the visual\nscene. We demonstrate these findings on a controlled case-study and extend them\nfor a larger set of stereotypically gendered entities.",
    "published_date": "2021-04-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.08666v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.08646v3",
    "title": "Competency Problems: On Finding and Removing Artifacts in Language Data",
    "authors": [
      "Matt Gardner",
      "William Merrill",
      "Jesse Dodge",
      "Matthew E. Peters",
      "Alexis Ross",
      "Sameer Singh",
      "Noah A. Smith"
    ],
    "author_ids": [],
    "abstract": "Much recent work in NLP has documented dataset artifacts, bias, and spurious\ncorrelations between input features and output labels. However, how to tell\nwhich features have \"spurious\" instead of legitimate correlations is typically\nleft unspecified. In this work we argue that for complex language understanding\ntasks, all simple feature correlations are spurious, and we formalize this\nnotion into a class of problems which we call competency problems. For example,\nthe word \"amazing\" on its own should not give information about a sentiment\nlabel independent of the context in which it appears, which could include\nnegation, metaphor, sarcasm, etc. We theoretically analyze the difficulty of\ncreating data for competency problems when human bias is taken into account,\nshowing that realistic datasets will increasingly deviate from competency\nproblems as dataset size increases. This analysis gives us a simple statistical\ntest for dataset artifacts, which we use to show more subtle biases than were\ndescribed in prior work, including demonstrating that models are\ninappropriately affected by these less extreme biases. Our theoretical\ntreatment of this problem also allows us to analyze proposed solutions, such as\nmaking local edits to dataset instances, and to give recommendations for future\ndata collection and model design efforts that target competency problems.",
    "published_date": "2021-04-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.08646v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.08636v1",
    "title": "Avoiding the bullies: The resilience of cooperation among unequals",
    "authors": [
      "Michael Foley",
      "Rory Smead",
      "Patrick Forber",
      "Christoph Riedl"
    ],
    "author_ids": [],
    "abstract": "Can egalitarian norms or conventions survive the presence of dominant\nindividuals who are ensured of victory in conflicts? We investigate the\ninteraction of power asymmetry and partner choice in games of conflict over a\ncontested resource. We introduce three models to study the emergence and\nresilience of cooperation among unequals when interaction is random, when\nindividuals can choose their partners, and where power asymmetries dynamically\ndepend on accumulated payoffs. We find that the ability to avoid bullies with\nhigher competitive ability afforded by partner choice mostly restores\ncooperative conventions and that the competitive hierarchy never forms. Partner\nchoice counteracts the hyper dominance of bullies who are isolated in the\nnetwork and eliminates the need for others to coordinate in a coalition. When\ncompetitive ability dynamically depends on cumulative payoffs, complex cycles\nof coupled network-strategy-rank changes emerge. Effective collaborators gain\npopularity (and thus power), adopt aggressive behavior, get isolated, and\nultimately lose power. Neither the network nor behavior converge to a stable\nequilibrium. Despite the instability of power dynamics, the cooperative\nconvention in the population remains stable overall and long-term inequality is\ncompletely eliminated. The interaction between partner choice and dynamic power\nasymmetry is crucial for these results: without partner choice, bullies cannot\nbe isolated, and without dynamic power asymmetry, bullies do not lose their\npower even when isolated. We analytically identify a single critical point that\nmarks a phase transition in all three iterations of our models. This critical\npoint is where the first individual breaks from the convention and cycles start\nto emerge.",
    "published_date": "2021-04-17T00:00:00",
    "year": 2021,
    "categories": [
      "physics.soc-ph",
      "cs.GT",
      "econ.TH",
      "nlin.AO",
      "q-bio.PE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.08636v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.08590v1",
    "title": "H$^2-$ Korn's Inequality and the Nonconforming Elements for The Strain Gradient Elastic Model",
    "authors": [
      "Hongliang Li",
      "Pingbing Ming",
      "Huiyu Wang"
    ],
    "author_ids": [],
    "abstract": "We establish a new H2 Korn's inequality and its discrete analog, which\ngreatly simplify the construction of nonconforming elements for a linear strain\ngradient elastic model. The Specht triangle [41] and the NZT tetrahedron [45]\nare analyzed as two typical representatives for robust nonconforming elements\nin the sense that the rate of convergence is independent of the small material\nparameter. We construct new regularized interpolation estimate and the\nenriching operator for both elements, and prove the error estimates under\nminimal smoothness assumption on the solution. Numerical results are consistent\nwith the theoretical prediction.",
    "published_date": "2021-04-17T00:00:00",
    "year": 2021,
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.08590v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.08433v2",
    "title": "Are Word Embedding Methods Stable and Should We Care About It?",
    "authors": [
      "Angana Borah",
      "Manash Pratim Barman",
      "Amit Awekar"
    ],
    "author_ids": [],
    "abstract": "A representation learning method is considered stable if it consistently\ngenerates similar representation of the given data across multiple runs. Word\nEmbedding Methods (WEMs) are a class of representation learning methods that\ngenerate dense vector representation for each word in the given text data. The\ncentral idea of this paper is to explore the stability measurement of WEMs\nusing intrinsic evaluation based on word similarity. We experiment with three\npopular WEMs: Word2Vec, GloVe, and fastText. For stability measurement, we\ninvestigate the effect of five parameters involved in training these models. We\nperform experiments using four real-world datasets from different domains:\nWikipedia, News, Song lyrics, and European parliament proceedings. We also\nobserve the effect of WEM stability on three downstream tasks: Clustering, POS\ntagging, and Fairness evaluation. Our experiments indicate that amongst the\nthree WEMs, fastText is the most stable, followed by GloVe and Word2Vec.",
    "published_date": "2021-04-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.08433v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.08266v1",
    "title": "Unified Analysis of Discontinuous Galerkin Methods for Frictional Contact Problem with normal compliance",
    "authors": [
      "Kamana Porwal",
      "Tanvi"
    ],
    "author_ids": [],
    "abstract": "In this article, a reliable and efficient a posteriori error estimator of\nresidual type is derived for a class of discontinuous Galerkin methods for the\nfrictional contact problem with reduced normal compliance which is modeled as a\nquasi-variational inequality. We further derive a priori error estimates in the\nenergy norm under the minimal regularity assumption on the exact solution. The\nconvergence behavior of error over uniform mesh and the performance of error\nestimator are illustrated by the numerical results.",
    "published_date": "2021-04-16T00:00:00",
    "year": 2021,
    "categories": [
      "math.NA",
      "cs.NA",
      "math.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.08266v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.08205v1",
    "title": "Thinking Through and Writing About Research Ethics Beyond \"Broader Impact\"",
    "authors": [
      "Kate Sim",
      "Andrew Brown",
      "Amelia Hassoun"
    ],
    "author_ids": [],
    "abstract": "In March 2021, we held the first instalment of the tutorial on thinking\nthrough and writing about research ethics beyond 'Broader Impact' in\nconjunction with the ACM Conference on Fairness, Accountability, and\nTransparency (FAccT '21). The goal of this tutorial was to offer a conceptual\nand practical starting point for engineers and social scientists interested in\nthinking more expansively, holistically, and critically about research ethics.\nThis report provides an outline of the tutorial, and contains our 'lifecourse\nchecklist'. This was presented as part of the tutorial, and provides a\npractical starting point for researchers when thinking about research ethics\nbefore a project's start. We provide this to the research community, with the\nhope that researchers use it when considering the ethics of their research.",
    "published_date": "2021-04-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.08205v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.07780v2",
    "title": "Impact of gender on the formation and outcome of mentoring relationships in academic research",
    "authors": [
      "Leah P. Schwartz",
      "Jean Liénard",
      "Stephen V. David"
    ],
    "author_ids": [],
    "abstract": "Despite increasing representation in graduate training programs, a\ndisproportionate number of women leave academic research before obtaining an\nindependent position. To understand factors underlying this trend, we analyzed\na multidisciplinary database of Ph.D. and postdoctoral mentoring relationships\ncovering the years 2000-2020, focusing on data from the life sciences. Student\nand mentor gender are both associated with differences in rates of student's\ncontinuation to independent mentor positions of their own. Although trainees of\nwomen mentors are less likely to take on independent positions than trainees of\nmen mentors, this effect is reduced substantially after controlling for several\nmeasurements of mentor status. Thus the effect of mentor gender can be\nexplained at least partially by gender disparities in social and financial\nresources available to mentors. Because trainees and mentors tend to be of the\nsame gender, this association between mentor gender and academic continuation\ndisproportionately impacts women trainees. On average, gender homophily in\ngraduate training is unrelated to mentor status. A notable exception to this\ntrend is the special case of scientists having been granted an outstanding\ndistinction, evidenced by membership in the National Academy of Sciences, being\na grantee of the Howard Hughes Medical Institute, or having been awarded the\nNobel Prize. This group of mentors trains men graduate students at higher rates\nthan their most successful colleagues. These results suggest that, in addition\nto other factors that limit career choices for women trainees, gender\ninequities in mentors' access to resources and prestige contribute to women's\nattrition from independent research positions.",
    "published_date": "2021-04-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.07780v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.07767v2",
    "title": "Exploring Visual Engagement Signals for Representation Learning",
    "authors": [
      "Menglin Jia",
      "Zuxuan Wu",
      "Austin Reiter",
      "Claire Cardie",
      "Serge Belongie",
      "Ser-Nam Lim"
    ],
    "author_ids": [],
    "abstract": "Visual engagement in social media platforms comprises interactions with photo\nposts including comments, shares, and likes. In this paper, we leverage such\nvisual engagement clues as supervisory signals for representation learning.\nHowever, learning from engagement signals is non-trivial as it is not clear how\nto bridge the gap between low-level visual information and high-level social\ninteractions. We present VisE, a weakly supervised learning approach, which\nmaps social images to pseudo labels derived by clustered engagement signals. We\nthen study how models trained in this way benefit subjective downstream\ncomputer vision tasks such as emotion recognition or political bias detection.\nThrough extensive studies, we empirically demonstrate the effectiveness of VisE\nacross a diverse set of classification tasks beyond the scope of conventional\nrecognition.",
    "published_date": "2021-04-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.07767v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.07750v2",
    "title": "Joint Attention for Multi-Agent Coordination and Social Learning",
    "authors": [
      "Dennis Lee",
      "Natasha Jaques",
      "Chase Kew",
      "Jiaxing Wu",
      "Douglas Eck",
      "Dale Schuurmans",
      "Aleksandra Faust"
    ],
    "author_ids": [],
    "abstract": "Joint attention - the ability to purposefully coordinate attention with\nanother agent, and mutually attend to the same thing -- is a critical component\nof human social cognition. In this paper, we ask whether joint attention can be\nuseful as a mechanism for improving multi-agent coordination and social\nlearning. We first develop deep reinforcement learning (RL) agents with a\nrecurrent visual attention architecture. We then train agents to minimize the\ndifference between the attention weights that they apply to the environment at\neach timestep, and the attention of other agents. Our results show that this\njoint attention incentive improves agents' ability to solve difficult\ncoordination tasks, by reducing the exponential cost of exploring the joint\nmulti-agent action space. Joint attention leads to higher performance than a\ncompetitive centralized critic baseline across multiple environments. Further,\nwe show that joint attention enhances agents' ability to learn from experts\npresent in their environment, even when completing hard exploration tasks that\ndo not require coordination. Taken together, these findings suggest that joint\nattention may be a useful inductive bias for multi-agent learning.",
    "published_date": "2021-04-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.07750v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.07571v2",
    "title": "Toward Deconfounding the Influence of Entity Demographics for Question Answering Accuracy",
    "authors": [
      "Maharshi Gor",
      "Kellie Webster",
      "Jordan Boyd-Graber"
    ],
    "author_ids": [],
    "abstract": "The goal of question answering (QA) is to answer any question. However, major\nQA datasets have skewed distributions over gender, profession, and nationality.\nDespite that skew, model accuracy analysis reveals little evidence that\naccuracy is lower for people based on gender or nationality; instead, there is\nmore variation on professions (question topic). But QA's lack of representation\ncould itself hide evidence of bias, necessitating QA datasets that better\nrepresent global diversity.",
    "published_date": "2021-04-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.07571v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.07505v2",
    "title": "Quantifying Gender Bias Towards Politicians in Cross-Lingual Language Models",
    "authors": [
      "Karolina Stańczak",
      "Sagnik Ray Choudhury",
      "Tiago Pimentel",
      "Ryan Cotterell",
      "Isabelle Augenstein"
    ],
    "author_ids": [],
    "abstract": "Recent research has demonstrated that large pre-trained language models\nreflect societal biases expressed in natural language. The present paper\nintroduces a simple method for probing language models to conduct a\nmultilingual study of gender bias towards politicians. We quantify the usage of\nadjectives and verbs generated by language models surrounding the names of\npoliticians as a function of their gender. To this end, we curate a dataset of\n250k politicians worldwide, including their names and gender. Our study is\nconducted in seven languages across six different language modeling\narchitectures. The results demonstrate that pre-trained language models' stance\ntowards politicians varies strongly across analyzed languages. We find that\nwhile some words such as dead, and designated are associated with both male and\nfemale politicians, a few specific words such as beautiful and divorced are\npredominantly associated with female politicians. Finally, and contrary to\nprevious findings, our study suggests that larger language models do not tend\nto be significantly more gender-biased than smaller ones.",
    "published_date": "2021-04-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.07505v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.07496v1",
    "title": "Unmasking the Mask -- Evaluating Social Biases in Masked Language Models",
    "authors": [
      "Masahiro Kaneko",
      "Danushka Bollegala"
    ],
    "author_ids": [],
    "abstract": "Masked Language Models (MLMs) have shown superior performances in numerous\ndownstream NLP tasks when used as text encoders. Unfortunately, MLMs also\ndemonstrate significantly worrying levels of social biases. We show that the\npreviously proposed evaluation metrics for quantifying the social biases in\nMLMs are problematic due to following reasons: (1) prediction accuracy of the\nmasked tokens itself tend to be low in some MLMs, which raises questions\nregarding the reliability of the evaluation metrics that use the (pseudo)\nlikelihood of the predicted tokens, and (2) the correlation between the\nprediction accuracy of the mask and the performance in downstream NLP tasks is\nnot taken into consideration, and (3) high frequency words in the training data\nare masked more often, introducing noise due to this selection bias in the test\ncases. To overcome the above-mentioned disfluencies, we propose All Unmasked\nLikelihood (AUL), a bias evaluation measure that predicts all tokens in a test\ncase given the MLM embedding of the unmasked input. We find that AUL accurately\ndetects different types of biases in MLMs. We also propose AUL with attention\nweights (AULA) to evaluate tokens based on their importance in a sentence.\nHowever, unlike AUL and AULA, previously proposed bias evaluation measures for\nMLMs systematically overestimate the measured biases, and are heavily\ninfluenced by the unmasked tokens in the context.",
    "published_date": "2021-04-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.07496v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.07429v2",
    "title": "First the worst: Finding better gender translations during beam search",
    "authors": [
      "Danielle Saunders",
      "Rosie Sallis",
      "Bill Byrne"
    ],
    "author_ids": [],
    "abstract": "Neural machine translation inference procedures like beam search generate the\nmost likely output under the model. This can exacerbate any demographic biases\nexhibited by the model. We focus on gender bias resulting from systematic\nerrors in grammatical gender translation, which can lead to human referents\nbeing misrepresented or misgendered.\n  Most approaches to this problem adjust the training data or the model. By\ncontrast, we experiment with simply adjusting the inference procedure. We\nexperiment with reranking nbest lists using gender features obtained\nautomatically from the source sentence, and applying gender constraints while\ndecoding to improve nbest list gender diversity. We find that a combination of\nthese techniques allows large gains in WinoMT accuracy without requiring\nadditional bilingual data or an additional NMT model.",
    "published_date": "2021-04-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.07429v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.09306v1",
    "title": "Ethical Challenges in the Human-Robot Interaction Field",
    "authors": [
      "Julia Rosén",
      "Jessica Lindblom",
      "Erik Billing",
      "Maurice Lamb"
    ],
    "author_ids": [],
    "abstract": "The aim of this extended abstract is to introduce five examples of ethical\nissues in HRI that could have potential ethical implications, particularly on\nHRI participants. We consider these examples important to discuss in order to\nreach a consensus on how to handle them. Due to space limitations, this list is\nfar from exhaustive and we hope that it can lead to a wider discussion that\nstimulates HRI researchers to think ethically. Previous work has shown a trend\nof underreporting ethical conduct in the HRI field; in this extended abstract\nwe consider some of the ethical issues that could arise in HRI research.",
    "published_date": "2021-04-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.09306v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.07360v1",
    "title": "DebiasedRec: Bias-aware User Modeling and Click Prediction for Personalized News Recommendation",
    "authors": [
      "Jingwei Yi",
      "Fangzhao Wu",
      "Chuhan Wu",
      "Qifei Li",
      "Guangzhong Sun",
      "Xing Xie"
    ],
    "author_ids": [],
    "abstract": "News recommendation is critical for personalized news access. Existing news\nrecommendation methods usually infer users' personal interest based on their\nhistorical clicked news, and train the news recommendation models by predicting\nfuture news clicks. A core assumption behind these methods is that news click\nbehaviors can indicate user interest. However, in practical scenarios, beyond\nthe relevance between user interest and news content, the news click behaviors\nmay also be affected by other factors, such as the bias of news presentation in\nthe online platform. For example, news with higher positions and larger sizes\nare usually more likely to be clicked. The bias of clicked news may bring\nnoises to user interest modeling and model training, which may hurt the\nperformance of the news recommendation model.\n  In this paper, we propose a bias-aware personalized news recommendation\nmethod named DebiasRec, which can handle the bias information for more accurate\nuser interest inference and model training. The core of our method includes a\nbias representation module, a bias-aware user modeling module, and a bias-aware\nclick prediction module. The bias representation module is used to model\ndifferent kinds of news bias and their interactions to capture their joint\neffect on click behaviors. The bias-aware user modeling module aims to infer\nusers' debiased interest from the clicked news articles by using their bias\ninformation to calibrate the interest model. The bias-aware click prediction\nmodule is used to train a debiased news recommendation model from the biased\nclick behaviors, where the click score is decomposed into a preference score\nindicating user's interest in the news content and a news bias score inferred\nfrom its different bias features. Experiments on two real-world datasets show\nthat our method can effectively improve the performance of news recommendation.",
    "published_date": "2021-04-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.07360v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.07099v5",
    "title": "Synchronization of Identical Boundary-Actuated Semilinear Infinite-Dimensional Systems",
    "authors": [
      "Francesco Ferrante",
      "Giacomo Casadei",
      "Christophe Prieur"
    ],
    "author_ids": [],
    "abstract": "This paper deals with synchronization of a class of infinite-dimensional\nsystems. The considered network is described by a collection of semilinear\nLipschitz boundary-actuated infinite-dimensional dynamics. For undirected\nconnected graphs, sufficient conditions for asymptotic synchronization are\nestablished. We show that the proposed conditions when applied to systems of\nhyperbolic semilinear conservation laws can be recast into a set of matrix\ninequalities. For this class of systems, sufficient conditions in the form of\nlinear matrix inequalities for the design of synchronizing policies are\nprovided.",
    "published_date": "2021-04-14T00:00:00",
    "year": 2021,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.07099v5",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.07004v1",
    "title": "Do Neural Network Weights account for Classes Centers?",
    "authors": [
      "Ioannis Kansizoglou",
      "Loukas Bampis",
      "Antonios Gasteratos"
    ],
    "author_ids": [],
    "abstract": "The exploitation of Deep Neural Networks (DNNs) as descriptors in feature\nlearning challenges enjoys apparent popularity over the past few years. The\nabove tendency focuses on the development of effective loss functions that\nensure both high feature discrimination among different classes, as well as low\ngeodesic distance between the feature vectors of a given class. The vast\nmajority of the contemporary works rely their formulation on an empirical\nassumption about the feature space of a network's last hidden layer, claiming\nthat the weight vector of a class accounts for its geometrical center in the\nstudied space. The paper at hand follows a theoretical approach and indicates\nthat the aforementioned hypothesis is not exclusively met. This fact raises\nstability issues regarding the training procedure of a DNN, as shown in our\nexperimental study. Consequently, a specific symmetry is proposed and studied\nboth analytically and empirically that satisfies the above assumption,\naddressing the established convergence issues.",
    "published_date": "2021-04-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CV",
      "I.5.1; I.2.10; G.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.07004v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.06973v1",
    "title": "[RE] Double-Hard Debias: Tailoring Word Embeddings for Gender Bias Mitigation",
    "authors": [
      "Haswanth Aekula",
      "Sugam Garg",
      "Animesh Gupta"
    ],
    "author_ids": [],
    "abstract": "Despite widespread use in natural language processing (NLP) tasks, word\nembeddings have been criticized for inheriting unintended gender bias from\ntraining corpora. programmer is more closely associated with man and homemaker\nis more closely associated with woman. Such gender bias has also been shown to\npropagate in downstream tasks.",
    "published_date": "2021-04-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.06973v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.06879v1",
    "title": "Can Active Learning Preemptively Mitigate Fairness Issues?",
    "authors": [
      "Frédéric Branchaud-Charron",
      "Parmida Atighehchian",
      "Pau Rodríguez",
      "Grace Abuhamad",
      "Alexandre Lacoste"
    ],
    "author_ids": [],
    "abstract": "Dataset bias is one of the prevailing causes of unfairness in machine\nlearning. Addressing fairness at the data collection and dataset preparation\nstages therefore becomes an essential part of training fairer algorithms. In\nparticular, active learning (AL) algorithms show promise for the task by\ndrawing importance to the most informative training samples. However, the\neffect and interaction between existing AL algorithms and algorithmic fairness\nremain under-explored. In this paper, we study whether models trained with\nuncertainty-based AL heuristics such as BALD are fairer in their decisions with\nrespect to a protected class than those trained with identically independently\ndistributed (i.i.d.) sampling. We found a significant improvement on predictive\nparity when using BALD, while also improving accuracy compared to i.i.d.\nsampling. We also explore the interaction of algorithmic fairness methods such\nas gradient reversal (GRAD) and BALD. We found that, while addressing different\nfairness issues, their interaction further improves the results on most\nbenchmarks and metrics we explored.",
    "published_date": "2021-04-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.06879v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.06788v1",
    "title": "Neural Architecture Search of Deep Priors: Towards Continual Learning without Catastrophic Interference",
    "authors": [
      "Martin Mundt",
      "Iuliia Pliushch",
      "Visvanathan Ramesh"
    ],
    "author_ids": [],
    "abstract": "In this paper we analyze the classification performance of neural network\nstructures without parametric inference. Making use of neural architecture\nsearch, we empirically demonstrate that it is possible to find random weight\narchitectures, a deep prior, that enables a linear classification to perform on\npar with fully trained deep counterparts. Through ablation experiments, we\nexclude the possibility of winning a weight initialization lottery and confirm\nthat suitable deep priors do not require additional inference. In an extension\nto continual learning, we investigate the possibility of catastrophic\ninterference free incremental learning. Under the assumption of classes\noriginating from the same data distribution, a deep prior found on only a\nsubset of classes is shown to allow discrimination of further classes through\ntraining of a simple linear classifier.",
    "published_date": "2021-04-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.06788v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.06737v1",
    "title": "Natural-Language Multi-Agent Simulations of Argumentative Opinion Dynamics",
    "authors": [
      "Gregor Betz"
    ],
    "author_ids": [],
    "abstract": "This paper develops a natural-language agent-based model of argumentation\n(ABMA). Its artificial deliberative agents (ADAs) are constructed with the help\nof so-called neural language models recently developed in AI and computational\nlinguistics. ADAs are equipped with a minimalist belief system and may generate\nand submit novel contributions to a conversation. The natural-language ABMA\nallows us to simulate collective deliberation in English, i.e. with arguments,\nreasons, and claims themselves -- rather than with their mathematical\nrepresentations (as in formal models). This paper uses the natural-language\nABMA to test the robustness of formal reason-balancing models of argumentation\n[Maes & Flache 2013, Singer et al. 2019]: First of all, as long as ADAs remain\npassive, confirmation bias and homophily updating trigger polarization, which\nis consistent with results from formal models. However, once ADAs start to\nactively generate new contributions, the evolution of a conservation is\ndominated by properties of the agents *as authors*. This suggests that the\ncreation of new arguments, reasons, and claims critically affects a\nconversation and is of pivotal importance for understanding the dynamics of\ncollective deliberation. The paper closes by pointing out further fruitful\napplications of the model and challenges for future research.",
    "published_date": "2021-04-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.MA",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.06737v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.12553v3",
    "title": "Avoiding bias when inferring race using name-based approaches",
    "authors": [
      "Diego Kozlowski",
      "Dakota S. Murray",
      "Alexis Bell",
      "Will Hulsey",
      "Vincent Larivière",
      "Thema Monroe-White",
      "Cassidy R. Sugimoto"
    ],
    "author_ids": [],
    "abstract": "Racial disparity in academia is a widely acknowledged problem. The\nquantitative understanding of racial based systemic inequalities is an\nimportant step towards a more equitable research system. However, because of\nthe lack of robust information on authors' race, few large scale analyses have\nbeen performed on this topic. Algorithmic approaches offer one solution, using\nknown information about authors, such as their names, to infer their perceived\nrace. As with any other algorithm, the process of racial inference can generate\nbiases if it is not carefully considered. The goal of this article is to assess\nthe extent to which algorithmic bias is introduced using different approaches\nfor name based racial inference. We use information from the U.S. Census and\nmortgage applications to infer the race of U.S. affiliated authors in the Web\nof Science. We estimate the effects of using given and family names, thresholds\nor continuous distributions, and imputation. Our results demonstrate that the\nvalidity of name based inference varies by race/ethnicity and that threshold\napproaches underestimate Black authors and overestimate White authors. We\nconclude with recommendations to avoid potential biases. This article lays the\nfoundation for more systematic and less biased investigations into racial\ndisparities in science.",
    "published_date": "2021-04-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.IR",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.12553v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.06483v1",
    "title": "Can a Transformer Pass the Wug Test? Tuning Copying Bias in Neural Morphological Inflection Models",
    "authors": [
      "Ling Liu",
      "Mans Hulden"
    ],
    "author_ids": [],
    "abstract": "Deep learning sequence models have been successfully applied to the task of\nmorphological inflection. The results of the SIGMORPHON shared tasks in the\npast several years indicate that such models can perform well, but only if the\ntraining data cover a good amount of different lemmata, or if the lemmata that\nare inflected at test time have also been seen in training, as has indeed been\nlargely the case in these tasks. Surprisingly, standard models such as the\nTransformer almost completely fail at generalizing inflection patterns when\nasked to inflect previously unseen lemmata -- i.e. under \"wug test\"-like\ncircumstances. While established data augmentation techniques can be employed\nto alleviate this shortcoming by introducing a copying bias through\nhallucinating synthetic new word forms using the alphabet in the language at\nhand, we show that, to be more effective, the hallucination process needs to\npay attention to substrings of syllable-like length rather than individual\ncharacters or stems. We report a significant performance improvement with our\nsubstring-based hallucination model over previous data hallucination methods\nwhen training and test data do not overlap in their lemmata.",
    "published_date": "2021-04-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.06483v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.06474v2",
    "title": "On the Interpretability and Significance of Bias Metrics in Texts: a PMI-based Approach",
    "authors": [
      "Francisco Valentini",
      "Germán Rosati",
      "Damián Blasi",
      "Diego Fernandez Slezak",
      "Edgar Altszyler"
    ],
    "author_ids": [],
    "abstract": "In recent years, word embeddings have been widely used to measure biases in\ntexts. Even if they have proven to be effective in detecting a wide variety of\nbiases, metrics based on word embeddings lack transparency and\ninterpretability. We analyze an alternative PMI-based metric to quantify biases\nin texts. It can be expressed as a function of conditional probabilities, which\nprovides a simple interpretation in terms of word co-occurrences. We also prove\nthat it can be approximated by an odds ratio, which allows estimating\nconfidence intervals and statistical significance of textual biases. This\napproach produces similar results to metrics based on word embeddings when\ncapturing gender gaps of the real world embedded in large corpora.",
    "published_date": "2021-04-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.06474v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.06338v1",
    "title": "On the Impact of Random Seeds on the Fairness of Clinical Classifiers",
    "authors": [
      "Silvio Amir",
      "Jan-Willem van de Meent",
      "Byron C. Wallace"
    ],
    "author_ids": [],
    "abstract": "Recent work has shown that fine-tuning large networks is surprisingly\nsensitive to changes in random seed(s). We explore the implications of this\nphenomenon for model fairness across demographic groups in clinical prediction\ntasks over electronic health records (EHR) in MIMIC-III -- the standard dataset\nin clinical NLP research. Apparent subgroup performance varies substantially\nfor seeds that yield similar overall performance, although there is no evidence\nof a trade-off between overall and subgroup performance. However, we also find\nthat the small sample sizes inherent to looking at intersections of minority\ngroups and somewhat rare conditions limit our ability to accurately estimate\ndisparities. Further, we find that jointly optimizing for high overall\nperformance and low disparities does not yield statistically significant\nimprovements. Our results suggest that fairness work using MIMIC-III should\ncarefully account for variations in apparent differences that may arise from\nstochasticity and small sample sizes.",
    "published_date": "2021-04-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.06338v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.06182v1",
    "title": "Understanding Transformers for Bot Detection in Twitter",
    "authors": [
      "Andres Garcia-Silva",
      "Cristian Berrio",
      "Jose Manuel Gomez-Perez"
    ],
    "author_ids": [],
    "abstract": "In this paper we shed light on the impact of fine-tuning over social media\ndata in the internal representations of neural language models. We focus on bot\ndetection in Twitter, a key task to mitigate and counteract the automatic\nspreading of disinformation and bias in social media. We investigate the use of\npre-trained language models to tackle the detection of tweets generated by a\nbot or a human account based exclusively on its content. Unlike the general\ntrend in benchmarks like GLUE, where BERT generally outperforms generative\ntransformers like GPT and GPT-2 for most classification tasks on regular text,\nwe observe that fine-tuning generative transformers on a bot detection task\nproduces higher accuracies. We analyze the architectural components of each\ntransformer and study the effect of fine-tuning on their hidden states and\noutput representations. Among our findings, we show that part of the\nsyntactical information and distributional properties captured by BERT during\npre-training is lost upon fine-tuning while the generative pre-training\napproach manage to preserve these properties.",
    "published_date": "2021-04-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.06182v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.06095v4",
    "title": "Relevance-Aware Anomalous Users Detection in Social Network via Graph Neural Network",
    "authors": [
      "Yangyang Li",
      "Yipeng Ji",
      "Shaoning Li",
      "Shulong He",
      "Yinhao Cao",
      "Xiong Li",
      "Jun Shi",
      "Yangchao Yang",
      "Yifeng Liu"
    ],
    "author_ids": [],
    "abstract": "Anomalous users detection in social network is an imperative task for\nsecurity problems. Motivated by the great power of Graph Neural Networks(GNNs),\nmany current researches adopt GNN-based detectors to reveal the anomalous\nusers. However, the increasing scale of social activities, explosive growth of\nusers and manifold technical disguise render the user detection a difficult\ntask. In this paper, we propose an innovate Relevance-aware Anomalous Users\nDetection model (RAU-GNN) to obtain a fine-grained detection result. RAU-GNN\nfirst extracts multiple relations of all types of users in social network,\nincluding both benign and anomalous users, and accordingly constructs the\nmultiple user relation graph. Secondly, we employ relevance-aware GNN framework\nto learn the hidden features of users, and discriminate the anomalous users\nafter discriminating. Concretely, by integrating Graph Convolution Network(GCN)\nand Graph Attention Network(GAT), we design a GCN-based relation fusion layer\nto aggregate initial information from different relations, and a GAT-based\nembedding layer to obtain the high-level embeddings. Lastly, we feed the\nlearned representations to the following GNN layer in order to consolidate the\nnode embedding by aggregating the final users' embeddings. We conduct extensive\nexperiment on real-world datasets. The experimental results show that our\napproach can achieve high accuracy for anomalous users detection.",
    "published_date": "2021-04-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.06095v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.06040v1",
    "title": "Conclusive Local Interpretation Rules for Random Forests",
    "authors": [
      "Ioannis Mollas",
      "Nick Bassiliades",
      "Grigorios Tsoumakas"
    ],
    "author_ids": [],
    "abstract": "In critical situations involving discrimination, gender inequality, economic\ndamage, and even the possibility of casualties, machine learning models must be\nable to provide clear interpretations for their decisions. Otherwise, their\nobscure decision-making processes can lead to socioethical issues as they\ninterfere with people's lives. In the aforementioned sectors, random forest\nalgorithms strive, thus their ability to explain themselves is an obvious\nrequirement. In this paper, we present LionForests, which relies on a\npreliminary work of ours. LionForests is a random forest-specific\ninterpretation technique, which provides rules as explanations. It is\napplicable from binary classification tasks to multi-class classification and\nregression tasks, and it is supported by a stable theoretical background.\nExperimentation, including sensitivity analysis and comparison with\nstate-of-the-art techniques, is also performed to demonstrate the efficacy of\nour contribution. Finally, we highlight a unique property of LionForests,\ncalled conclusiveness, that provides interpretation validity and distinguishes\nit from previous techniques.",
    "published_date": "2021-04-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.0; I.2.6"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.06040v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.06001v3",
    "title": "Gender Bias in Machine Translation",
    "authors": [
      "Beatrice Savoldi",
      "Marco Gaido",
      "Luisa Bentivogli",
      "Matteo Negri",
      "Marco Turchi"
    ],
    "author_ids": [],
    "abstract": "Machine translation (MT) technology has facilitated our daily tasks by\nproviding accessible shortcuts for gathering, elaborating and communicating\ninformation. However, it can suffer from biases that harm users and society at\nlarge. As a relatively new field of inquiry, gender bias in MT still lacks\ninternal cohesion, which advocates for a unified framework to ease future\nresearch. To this end, we: i) critically review current conceptualizations of\nbias in light of theoretical insights from related disciplines, ii) summarize\nprevious analyses aimed at assessing gender bias in MT, iii) discuss the\nmitigating strategies proposed so far, and iv) point toward potential\ndirections for future work.",
    "published_date": "2021-04-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.06001v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.05994v2",
    "title": "Fairness in Rankings and Recommendations: An Overview",
    "authors": [
      "Evaggelia Pitoura",
      "Kostas Stefanidis",
      "Georgia Koutrika"
    ],
    "author_ids": [],
    "abstract": "We increasingly depend on a variety of data-driven algorithmic systems to\nassist us in many aspects of life. Search engines and recommender systems\namongst others are used as sources of information and to help us in making all\nsort of decisions from selecting restaurants and books, to choosing friends and\ncareers. This has given rise to important concerns regarding the fairness of\nsuch systems. In this work, we aim at presenting a toolkit of definitions,\nmodels and methods used for ensuring fairness in rankings and recommendations.\nOur objectives are three-fold: (a) to provide a solid framework on a novel,\nquickly evolving, and impactful domain, (b) to present related methods and put\nthem into perspective, and (c) to highlight open challenges and research paths\nfor future work.",
    "published_date": "2021-04-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DB"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.05994v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.05928v1",
    "title": "Semantic maps and metrics for science Semantic maps and metrics for science using deep transformer encoders",
    "authors": [
      "Brendan Chambers",
      "James Evans"
    ],
    "author_ids": [],
    "abstract": "The growing deluge of scientific publications demands text analysis tools\nthat can help scientists and policy-makers navigate, forecast and beneficially\nguide scientific research. Recent advances in natural language understanding\ndriven by deep transformer networks offer new possibilities for mapping\nscience. Because the same surface text can take on multiple and sometimes\ncontradictory specialized senses across distinct research communities,\nsensitivity to context is critical for infometric applications. Transformer\nembedding models such as BERT capture shades of association and connotation\nthat vary across the different linguistic contexts of any particular word or\nspan of text. Here we report a procedure for encoding scientific documents with\nthese tools, measuring their improvement over static word embeddings in a\nnearest-neighbor retrieval task. We find discriminability of contextual\nrepresentations is strongly influenced by choice of pooling strategy for\nsummarizing the high-dimensional network activations. Importantly, we note that\nfundamentals such as domain-matched training data are more important than\nstate-of-the-art NLP tools. Yet state-of-the-art models did offer significant\ngains. The best approach we investigated combined domain-matched pretraining,\nsound pooling, and state-of-the-art deep transformer network encoders. Finally,\nwith the goal of leveraging contextual representations from deep encoders, we\npresent a range of measurements for understanding and forecasting research\ncommunities in science.",
    "published_date": "2021-04-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.05928v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.05694v1",
    "title": "On the Inductive Bias of Masked Language Modeling: From Statistical to Syntactic Dependencies",
    "authors": [
      "Tianyi Zhang",
      "Tatsunori Hashimoto"
    ],
    "author_ids": [],
    "abstract": "We study how masking and predicting tokens in an unsupervised fashion can\ngive rise to linguistic structures and downstream performance gains. Recent\ntheories have suggested that pretrained language models acquire useful\ninductive biases through masks that implicitly act as cloze reductions for\ndownstream tasks. While appealing, we show that the success of the random\nmasking strategy used in practice cannot be explained by such cloze-like masks\nalone. We construct cloze-like masks using task-specific lexicons for three\ndifferent classification datasets and show that the majority of pretrained\nperformance gains come from generic masks that are not associated with the\nlexicon. To explain the empirical success of these generic masks, we\ndemonstrate a correspondence between the Masked Language Model (MLM) objective\nand existing methods for learning statistical dependencies in graphical models.\nUsing this, we derive a method for extracting these learned statistical\ndependencies in MLMs and show that these dependencies encode useful inductive\nbiases in the form of syntactic structures. In an unsupervised parsing\nevaluation, simply forming a minimum spanning tree on the implied statistical\ndependence structure outperforms a classic method for unsupervised parsing\n(58.74 vs. 55.91 UUAS).",
    "published_date": "2021-04-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.05694v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.05658v1",
    "title": "Towards Algorithmic Transparency: A Diversity Perspective",
    "authors": [
      "Fausto Giunchiglia",
      "Jahna Otterbacher",
      "Styliani Kleanthous",
      "Khuyagbaatar Batsuren",
      "Veronika Bogin",
      "Tsvi Kuflik",
      "Avital Shulner Tal"
    ],
    "author_ids": [],
    "abstract": "As the role of algorithmic systems and processes increases in society, so\ndoes the risk of bias, which can result in discrimination against individuals\nand social groups. Research on algorithmic bias has exploded in recent years,\nhighlighting both the problems of bias, and the potential solutions, in terms\nof algorithmic transparency (AT). Transparency is important for facilitating\nfairness management as well as explainability in algorithms; however, the\nconcept of diversity, and its relationship to bias and transparency, has been\nlargely left out of the discussion. We reflect on the relationship between\ndiversity and bias, arguing that diversity drives the need for transparency.\nUsing a perspective-taking lens, which takes diversity as a given, we propose a\nconceptual framework to characterize the problem and solution spaces of AT, to\naid its application in algorithmic systems. Example cases from three research\ndomains are described using our framework.",
    "published_date": "2021-04-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.05658v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.05634v4",
    "title": "The Undecidability of Conditional Affine Information Inequalities and Conditional Independence Implication with a Binary Constraint",
    "authors": [
      "Cheuk Ting Li"
    ],
    "author_ids": [],
    "abstract": "We establish the undecidability of conditional affine information\ninequalities, the undecidability of the conditional independence implication\nproblem with a constraint that one random variable is binary, and the\nundecidability of the problem of deciding whether the intersection of the\nentropic region and a given affine subspace is empty. This is a step towards\nthe conjecture on the undecidability of conditional independence implication.\nThe undecidability is proved via a reduction from the periodic tiling problem\n(a variant of the domino problem). Hence, one can construct examples of the\naforementioned problems that are independent of ZFC (assuming ZFC is\nconsistent).",
    "published_date": "2021-04-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IT",
      "math.IT",
      "math.PR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.05634v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.05467v2",
    "title": "Understanding Prediction Discrepancies in Machine Learning Classifiers",
    "authors": [
      "Xavier Renard",
      "Thibault Laugel",
      "Marcin Detyniecki"
    ],
    "author_ids": [],
    "abstract": "A multitude of classifiers can be trained on the same data to achieve similar\nperformances during test time, while having learned significantly different\nclassification patterns. This phenomenon, which we call prediction\ndiscrepancies, is often associated with the blind selection of one model\ninstead of another with similar performances. When making a choice, the machine\nlearning practitioner has no understanding on the differences between models,\ntheir limits, where they agree and where they don't. But his/her choice will\nresult in concrete consequences for instances to be classified in the\ndiscrepancy zone, since the final decision will be based on the selected\nclassification pattern. Besides the arbitrary nature of the result, a bad\nchoice could have further negative consequences such as loss of opportunity or\nlack of fairness. This paper proposes to address this question by analyzing the\nprediction discrepancies in a pool of best-performing models trained on the\nsame data. A model-agnostic algorithm, DIG, is proposed to capture and explain\ndiscrepancies locally, to enable the practitioner to make the best educated\ndecision when selecting a model by anticipating its potential undesired\nconsequences. All the code to reproduce the experiments is available.",
    "published_date": "2021-04-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.05467v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.05409v1",
    "title": "Breaking Community Boundary: Comparing Academic and Social Communication Preferences regarding Global Pandemics",
    "authors": [
      "Qingqing Zhou",
      "Chengzhi Zhang"
    ],
    "author_ids": [],
    "abstract": "The global spread of COVID-19 has caused pandemics to be widely discussed.\nThis is evident in the large number of scientific articles and the amount of\nuser-generated content on social media. This paper aims to compare academic\ncommunication and social communication about the pandemic from the perspective\nof communication preference differences. It aims to provide information for the\nongoing research on global pandemics, thereby eliminating knowledge barriers\nand information inequalities between the academic and the social communities.\nFirst, we collected the full text and the metadata of pandemic-related articles\nand Twitter data mentioning the articles. Second, we extracted and analyzed the\ntopics and sentiment tendencies of the articles and related tweets. Finally, we\nconducted pandemic-related differential analysis on the academic community and\nthe social community. We mined the resulting data to generate pandemic\ncommunication preferences (e.g., information needs, attitude tendencies) of\nresearchers and the public, respectively. The research results from 50,338\narticles and 927,266 corresponding tweets mentioning the articles revealed\ncommunication differences about global pandemics between the academic and the\nsocial communities regarding the consistency of research recognition and the\npreferences for particular research topics. The analysis of large-scale\npandemic-related tweets also confirmed the communication preference differences\nbetween the two communities.",
    "published_date": "2021-04-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DL",
      "cs.CL",
      "H.3.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.05409v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.05232v1",
    "title": "Double Perturbation: On the Robustness of Robustness and Counterfactual Bias Evaluation",
    "authors": [
      "Chong Zhang",
      "Jieyu Zhao",
      "Huan Zhang",
      "Kai-Wei Chang",
      "Cho-Jui Hsieh"
    ],
    "author_ids": [],
    "abstract": "Robustness and counterfactual bias are usually evaluated on a test dataset.\nHowever, are these evaluations robust? If the test dataset is perturbed\nslightly, will the evaluation results keep the same? In this paper, we propose\na \"double perturbation\" framework to uncover model weaknesses beyond the test\ndataset. The framework first perturbs the test dataset to construct abundant\nnatural sentences similar to the test data, and then diagnoses the prediction\nchange regarding a single-word substitution. We apply this framework to study\ntwo perturbation-based approaches that are used to analyze models' robustness\nand counterfactual bias in English. (1) For robustness, we focus on synonym\nsubstitutions and identify vulnerable examples where prediction can be altered.\nOur proposed attack attains high success rates (96.0%-99.8%) in finding\nvulnerable examples on both original and robustly trained CNNs and\nTransformers. (2) For counterfactual bias, we focus on substituting demographic\ntokens (e.g., gender, race) and measure the shift of the expected prediction\namong constructed sentences. Our method is able to reveal the hidden model\nbiases not directly shown in the test dataset. Our code is available at\nhttps://github.com/chong-z/nlp-second-order-attack.",
    "published_date": "2021-04-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.05232v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.05188v1",
    "title": "Accelerating science with human versus alien artificial intelligences",
    "authors": [
      "Jamshid Sourati",
      "James Evans"
    ],
    "author_ids": [],
    "abstract": "Data-driven artificial intelligence models fed with published scientific\nfindings have been used to create powerful prediction engines for scientific\nand technological advance, such as the discovery of novel materials with\ndesired properties and the targeted invention of new therapies and vaccines.\nThese AI approaches typically ignore the distribution of human prediction\nengines -- scientists and inventor -- who continuously alter the landscape of\ndiscovery and invention. As a result, AI hypotheses are designed to substitute\nfor human experts, failing to complement them for punctuated collective\nadvance. Here we show that incorporating the distribution of human expertise\ninto self-supervised models by training on inferences cognitively available to\nexperts dramatically improves AI prediction of future human discoveries and\ninventions. Including expert-awareness into models that propose (a) valuable\nenergy-relevant materials increases the precision of materials predictions by\n~100%, (b) repurposing thousands of drugs to treat new diseases increases\nprecision by 43%, and (c) COVID-19 vaccine candidates examined in clinical\ntrials by 260%. These models succeed by predicting human predictions and the\nscientists who will make them. By tuning AI to avoid the crowd, however, it\ngenerates scientifically promising \"alien\" hypotheses unlikely to be imagined\nor pursued without intervention, not only accelerating but punctuating\nscientific advance. By identifying and correcting for collective human bias,\nthese models also suggest opportunities to improve human prediction by\nreformulating science education for discovery.",
    "published_date": "2021-04-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cond-mat.mtrl-sci",
      "q-bio.BM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.05188v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.05030v1",
    "title": "Social Virtual Reality: Ethical Considerations and Future Directions for An Emerging Research Space",
    "authors": [
      "Divine Maloney",
      "Guo Freeman",
      "Andrew Robb"
    ],
    "author_ids": [],
    "abstract": "The boom of commercial social virtual reality (VR) platforms in recent years\nhas signaled the growth and wide-spread adoption of consumer VR. Social VR\nplatforms draw aspects from traditional 2D virtual worlds where users engage in\nvarious immersive experiences, interactive activities, and choices in\navatar-based representation. However, social VR also demonstrates specific\nnuances that extend traditional 2D virtual worlds and other online social\nspaces, such as full/partial body tracked avatars, experiencing mundane\neveryday activities in a new way (e.g., sleeping), and an immersive means to\nexplore new and complex identities. The growing popularity has signaled\ninterest and investment from top technology companies who each have their own\nsocial VR platforms. Thus far, social VR has become an emerging research space,\nmainly focusing on design strategies, communication and interaction modalities,\nnuanced activities, self-presentation, harassment, privacy, and\nself-disclosure. These recent works suggest that many questions still remain in\nsocial VR scholarship regarding how to ethically conduct research on these\nsites and which research areas require additional attention. Therefore, in this\npaper, we provide an overview of modern Social VR, critically review current\nscholarship in the area, raise ethical considerations for conducting research\non these sites, and highlight unexplored areas.",
    "published_date": "2021-04-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.05030v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.04991v1",
    "title": "Integrating Information Theory and Adversarial Learning for Cross-modal Retrieval",
    "authors": [
      "Wei Chen",
      "Yu Liu",
      "Erwin M. Bakker",
      "Michael S. Lew"
    ],
    "author_ids": [],
    "abstract": "Accurately matching visual and textual data in cross-modal retrieval has been\nwidely studied in the multimedia community. To address these challenges posited\nby the heterogeneity gap and the semantic gap, we propose integrating Shannon\ninformation theory and adversarial learning. In terms of the heterogeneity gap,\nwe integrate modality classification and information entropy maximization\nadversarially. For this purpose, a modality classifier (as a discriminator) is\nbuilt to distinguish the text and image modalities according to their different\nstatistical properties. This discriminator uses its output probabilities to\ncompute Shannon information entropy, which measures the uncertainty of the\nmodality classification it performs. Moreover, feature encoders (as a\ngenerator) project uni-modal features into a commonly shared space and attempt\nto fool the discriminator by maximizing its output information entropy. Thus,\nmaximizing information entropy gradually reduces the distribution discrepancy\nof cross-modal features, thereby achieving a domain confusion state where the\ndiscriminator cannot classify two modalities confidently. To reduce the\nsemantic gap, Kullback-Leibler (KL) divergence and bi-directional triplet loss\nare used to associate the intra- and inter-modality similarity between features\nin the shared space. Furthermore, a regularization term based on KL-divergence\nwith temperature scaling is used to calibrate the biased label classifier\ncaused by the data imbalance issue. Extensive experiments with four deep models\non four benchmarks are conducted to demonstrate the effectiveness of the\nproposed approach.",
    "published_date": "2021-04-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.04991v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.04980v1",
    "title": "Zero-Shot Learning on 3D Point Cloud Objects and Beyond",
    "authors": [
      "Ali Cheraghian",
      "Shafinn Rahman",
      "Townim F. Chowdhury",
      "Dylan Campbell",
      "Lars Petersson"
    ],
    "author_ids": [],
    "abstract": "Zero-shot learning, the task of learning to recognize new classes not seen\nduring training, has received considerable attention in the case of 2D image\nclassification. However, despite the increasing ubiquity of 3D sensors, the\ncorresponding 3D point cloud classification problem has not been meaningfully\nexplored and introduces new challenges. In this paper, we identify some of the\nchallenges and apply 2D Zero-Shot Learning (ZSL) methods in the 3D domain to\nanalyze the performance of existing models. Then, we propose a novel approach\nto address the issues specific to 3D ZSL. We first present an inductive ZSL\nprocess and then extend it to the transductive ZSL and Generalized ZSL (GZSL)\nsettings for 3D point cloud classification. To this end, a novel loss function\nis developed that simultaneously aligns seen semantics with point cloud\nfeatures and takes advantage of unlabeled test data to address some known\nissues (e.g., the problems of domain adaptation, hubness, and data bias). While\ndesigned for the particularities of 3D point cloud classification, the method\nis shown to also be applicable to the more common use-case of 2D image\nclassification. An extensive set of experiments is carried out, establishing\nstate-of-the-art for ZSL and GZSL on synthetic (ModelNet40, ModelNet10, McGill)\nand real (ScanObjectNN) 3D point cloud datasets.",
    "published_date": "2021-04-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.04980v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.04885v1",
    "title": "Description of Structural Biases and Associated Data in Sensor-Rich Environments",
    "authors": [
      "Massinissa Hamidi",
      "Aomar Osmani"
    ],
    "author_ids": [],
    "abstract": "In this article, we study activity recognition in the context of sensor-rich\nenvironments. We address, in particular, the problem of inductive biases and\ntheir impact on the data collection process. To be effective and robust,\nactivity recognition systems must take these biases into account at all levels\nand model them as hyperparameters by which they can be controlled. Whether it\nis a bias related to sensor measurement, transmission protocol, sensor\ndeployment topology, heterogeneity, dynamicity, or stochastic effects, it is\nimportant to understand their substantial impact on the quality of activity\nrecognition models. This study highlights the need to separate the different\ntypes of biases arising in real situations so that machine learning models,\ne.g., adapt to the dynamicity of these environments, resist to sensor failures,\nand follow the evolution of the sensors topology. We propose a metamodeling\nprocess in which the sensor data is structured in layers. The lower layers\nencode the various biases linked to transformations, transmissions, and\ntopology of data. The upper layers encode biases related to the data itself.\nThis way, it becomes easier to model hyperparameters and follow changes in the\ndata acquisition infrastructure. We illustrate our approach on the SHL dataset\nwhich provides motion sensor data for a list of human activities collected\nunder real conditions. The trade-offs exposed and the broader implications of\nour approach are discussed with alternative techniques to encode and\nincorporate knowledge into activity recognition models.",
    "published_date": "2021-04-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "eess.SP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.04885v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.04881v1",
    "title": "Adaptive Learning on the Grids for Elliptic Hemivariational Inequalities",
    "authors": [
      "Jianguo Huang",
      "Chunmei Wang",
      "Haoqin Wang"
    ],
    "author_ids": [],
    "abstract": "This paper introduces a deep learning method for solving an elliptic\nhemivariational inequality (HVI). In this method, an expectation minimization\nproblem is first formulated based on the variational principle of underlying\nHVI, which is solved by stochastic optimization algorithms using three\ndifferent training strategies for updating network parameters. The method is\napplied to solve two practical problems in contact mechanics, one of which is a\nfrictional bilateral contact problem and the other of which is a frictionless\nnormal compliance contact problem. Numerical results show that the deep\nlearning method is efficient in solving HVIs and the adaptive mesh-free\nmultigrid algorithm can provide the most accurate solution among the three\nlearning methods.",
    "published_date": "2021-04-11T00:00:00",
    "year": 2021,
    "categories": [
      "math.NA",
      "cs.NA",
      "65N30"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.04881v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.04848v2",
    "title": "Autoequivariant Network Search via Group Decomposition",
    "authors": [
      "Sourya Basu",
      "Akshayaa Magesh",
      "Harshit Yadav",
      "Lav R. Varshney"
    ],
    "author_ids": [],
    "abstract": "Recent works show that group equivariance as an inductive bias improves\nneural network performance for both classification and generation. However,\ndesigning group-equivariant neural networks is challenging when the group of\ninterest is large and is unknown. Moreover, inducing equivariance can\nsignificantly reduce the number of independent parameters in a network with\nfixed feature size, affecting its overall performance. We address these\nproblems by proving a new group-theoretic result in the context of equivariant\nneural networks that shows that a network is equivariant to a large group if\nand only if it is equivariant to smaller groups from which it is constructed.\nUsing this result, we design a novel fast group equivariant construction\nalgorithm, and a deep Q-learning-based search algorithm in a reduced search\nspace, yielding what we call autoequivariant networks (AENs). AENs find the\nright balance between equivariance and network size when tested on new\nbenchmark datasets, G-MNIST and G-Fashion-MNIST, obtained via group\ntransformations on MNIST and Fashion-MNIST respectively that we release.\nExtending these results to group convolutional neural networks, where we\noptimize between equivariances, augmentations, and network sizes, we find group\nequivariance to be the most dominating factor in all high-performing GCNNs on\nseveral datasets like CIFAR10, SVHN, RotMNIST, ASL, EMNIST, and KMNIST.",
    "published_date": "2021-04-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.04848v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.04665v1",
    "title": "On Universal Black-Box Domain Adaptation",
    "authors": [
      "Bin Deng",
      "Yabin Zhang",
      "Hui Tang",
      "Changxing Ding",
      "Kui Jia"
    ],
    "author_ids": [],
    "abstract": "In this paper, we study an arguably least restrictive setting of domain\nadaptation in a sense of practical deployment, where only the interface of\nsource model is available to the target domain, and where the label-space\nrelations between the two domains are allowed to be different and unknown. We\nterm such a setting as Universal Black-Box Domain Adaptation (UB$^2$DA). The\ngreat promise that UB$^2$DA makes, however, brings significant learning\nchallenges, since domain adaptation can only rely on the predictions of\nunlabeled target data in a partially overlapped label space, by accessing the\ninterface of source model. To tackle the challenges, we first note that the\nlearning task can be converted as two subtasks of in-class\\footnote{In this\npaper we use in-class (out-class) to describe the classes observed (not\nobserved) in the source black-box model.} discrimination and out-class\ndetection, which can be respectively learned by model distillation and entropy\nseparation. We propose to unify them into a self-training framework,\nregularized by consistency of predictions in local neighborhoods of target\nsamples. Our framework is simple, robust, and easy to be optimized. Experiments\non domain adaptation benchmarks show its efficacy. Notably, by accessing the\ninterface of source model only, our framework outperforms existing methods of\nuniversal domain adaptation that make use of source data and/or source models,\nwith a newly proposed (and arguably more reasonable) metric of H-score, and\nperforms on par with them with the metric of averaged class accuracy.",
    "published_date": "2021-04-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.04665v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.12547v1",
    "title": "A Framework for Ethical AI at the United Nations",
    "authors": [
      "Lambert Hogenhout"
    ],
    "author_ids": [],
    "abstract": "This paper aims to provide an overview of the ethical concerns in artificial\nintelligence (AI) and the framework that is needed to mitigate those risks, and\nto suggest a practical path to ensure the development and use of AI at the\nUnited Nations (UN) aligns with our ethical values. The overview discusses how\nAI is an increasingly powerful tool with potential for good, albeit one with a\nhigh risk of negative side-effects that go against fundamental human rights and\nUN values. It explains the need for ethical principles for AI aligned with\nprinciples for data governance, as data and AI are tightly interwoven. It\nexplores different ethical frameworks that exist and tools such as assessment\nlists. It recommends that the UN develop a framework consisting of ethical\nprinciples, architectural standards, assessment methods, tools and\nmethodologies, and a policy to govern the implementation and adherence to this\nframework, accompanied by an education program for staff.",
    "published_date": "2021-04-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.12547v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.04633v1",
    "title": "Automated Meta-Analysis: A Causal Learning Perspective",
    "authors": [
      "Lu Cheng",
      "Dmitriy A. Katz-Rogozhnikov",
      "Kush R. Varshney",
      "Ioana Baldini"
    ],
    "author_ids": [],
    "abstract": "Meta-analysis is a systematic approach for understanding a phenomenon by\nanalyzing the results of many previously published experimental studies. It is\ncentral to deriving conclusions about the summary effect of treatments and\ninterventions in medicine, poverty alleviation, and other applications with\nsocial impact. Unfortunately, meta-analysis involves great human effort,\nrendering a process that is extremely inefficient and vulnerable to human bias.\nTo overcome these issues, we work toward automating meta-analysis with a focus\non controlling for risks of bias. In particular, we first extract information\nfrom scientific publications written in natural language. From a novel causal\nlearning perspective, we then propose to frame automated meta-analysis -- based\non the input of the first step -- as a multiple-causal-inference problem where\nthe summary effect is obtained through intervention. Built upon existing\nefforts for automating the initial steps of meta-analysis, the proposed\napproach achieves the goal of automated meta-analysis and largely reduces the\nhuman effort involved. Evaluations on synthetic and semi-synthetic datasets\nshow that this approach can yield promising results.",
    "published_date": "2021-04-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.04633v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.04502v1",
    "title": "Auditing for Discrimination in Algorithms Delivering Job Ads",
    "authors": [
      "Basileal Imana",
      "Aleksandra Korolova",
      "John Heidemann"
    ],
    "author_ids": [],
    "abstract": "Ad platforms such as Facebook, Google and LinkedIn promise value for\nadvertisers through their targeted advertising. However, multiple studies have\nshown that ad delivery on such platforms can be skewed by gender or race due to\nhidden algorithmic optimization by the platforms, even when not requested by\nthe advertisers. Building on prior work measuring skew in ad delivery, we\ndevelop a new methodology for black-box auditing of algorithms for\ndiscrimination in the delivery of job advertisements. Our first contribution is\nto identify the distinction between skew in ad delivery due to protected\ncategories such as gender or race, from skew due to differences in\nqualification among people in the targeted audience. This distinction is\nimportant in U.S. law, where ads may be targeted based on qualifications, but\nnot on protected categories. Second, we develop an auditing methodology that\ndistinguishes between skew explainable by differences in qualifications from\nother factors, such as the ad platform's optimization for engagement or\ntraining its algorithms on biased data. Our method controls for job\nqualification by comparing ad delivery of two concurrent ads for similar jobs,\nbut for a pair of companies with different de facto gender distributions of\nemployees. We describe the careful statistical tests that establish evidence of\nnon-qualification skew in the results. Third, we apply our proposed methodology\nto two prominent targeted advertising platforms for job ads: Facebook and\nLinkedIn. We confirm skew by gender in ad delivery on Facebook, and show that\nit cannot be justified by differences in qualifications. We fail to find skew\nin ad delivery on LinkedIn. Finally, we suggest improvements to ad platform\npractices that could make external auditing of their algorithms in the public\ninterest more feasible and accurate.",
    "published_date": "2021-04-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.04502v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.04470v1",
    "title": "Did they answer? Subjective acts and intents in conversational discourse",
    "authors": [
      "Elisa Ferracane",
      "Greg Durrett",
      "Junyi Jessy Li",
      "Katrin Erk"
    ],
    "author_ids": [],
    "abstract": "Discourse signals are often implicit, leaving it up to the interpreter to\ndraw the required inferences. At the same time, discourse is embedded in a\nsocial context, meaning that interpreters apply their own assumptions and\nbeliefs when resolving these inferences, leading to multiple, valid\ninterpretations. However, current discourse data and frameworks ignore the\nsocial aspect, expecting only a single ground truth. We present the first\ndiscourse dataset with multiple and subjective interpretations of English\nconversation in the form of perceived conversation acts and intents. We\ncarefully analyze our dataset and create computational models to (1) confirm\nour hypothesis that taking into account the bias of the interpreters leads to\nbetter predictions of the interpretations, (2) and show disagreements are\nnuanced and require a deeper understanding of the different contextual factors.\nWe share our dataset and code at http://github.com/elisaF/subjective_discourse.",
    "published_date": "2021-04-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.04470v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.04353v1",
    "title": "Implementing Fair Regression In The Real World",
    "authors": [
      "Boris Ruf",
      "Marcin Detyniecki"
    ],
    "author_ids": [],
    "abstract": "Most fair regression algorithms mitigate bias towards sensitive sub\npopulations and therefore improve fairness at group level. In this paper, we\ninvestigate the impact of such implementation of fair regression on the\nindividual. More precisely, we assess the evolution of continuous predictions\nfrom an unconstrained to a fair algorithm by comparing results from baseline\nalgorithms with fair regression algorithms for the same data points. Based on\nour findings, we propose a set of post-processing algorithms to improve the\nutility of the existing fair regression approaches.",
    "published_date": "2021-04-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.04353v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.04162v2",
    "title": "eGAN: Unsupervised approach to class imbalance using transfer learning",
    "authors": [
      "Ademola Okerinde",
      "Lior Shamir",
      "William Hsu",
      "Tom Theis",
      "Nasik Nafi"
    ],
    "author_ids": [],
    "abstract": "Class imbalance is an inherent problem in many machine learning\nclassification tasks. This often leads to trained models that are unusable for\nany practical purpose. In this study we explore an unsupervised approach to\naddress these imbalances by leveraging transfer learning from pre-trained image\nclassification models to encoder-based Generative Adversarial Network (eGAN).\nTo the best of our knowledge, this is the first work to tackle this problem\nusing GAN without needing to augment with synthesized fake images.\n  In the proposed approach we use the discriminator network to output a\nnegative or positive score. We classify as minority, test samples with negative\nscores and as majority those with positive scores. Our approach eliminates\nepistemic uncertainty in model predictions, as the P(minority) + P(majority)\nneed not sum up to 1. The impact of transfer learning and combinations of\ndifferent pre-trained image classification models at the generator and\ndiscriminator is also explored. Best result of 0.69 F1-score was obtained on\nCIFAR-10 classification task with imbalance ratio of 1:2500.\n  Our approach also provides a mechanism of thresholding the specificity or\nsensitivity of our machine learning system. Keywords: Class imbalance, Transfer\nLearning, GAN, nash equilibrium",
    "published_date": "2021-04-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.04162v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.04116v1",
    "title": "It's All About The Cards: Sharing on Social Media Probably Encouraged HTML Metadata Growth",
    "authors": [
      "Shawn M. Jones",
      "Valentina Neblitt-Jones",
      "Michele C. Weigle",
      "Martin Klein",
      "Michael L. Nelson"
    ],
    "author_ids": [],
    "abstract": "In a perfect world, all articles consistently contain sufficient metadata to\ndescribe the resource. We know this is not the reality, so we are motivated to\ninvestigate the evolution of the metadata that is present when authors and\npublishers supply their own. Because applying metadata takes time, we recognize\nthat each news article author has a limited metadata budget with which to spend\ntheir time and effort. How are they spending this budget? What are the top\nmetadata categories in use? How did they grow over time? What purpose do they\nserve? We also recognize that not all metadata fields are used equally. What is\nthe growth of individual fields over time? Which fields experienced the fastest\nadoption? In this paper, we review 227,726 HTML news articles from 29 outlets\ncaptured by the Internet Archive between 1998 and 2016. Upon reviewing the\nmetadata fields in each article, we discovered that 2010 began a metadata\nrenaissance as publishers embraced metadata for improved search engine ranking,\nsearch engine tracking, social media tracking, and social media sharing. When\nanalyzing individual fields, we find that one application of metadata stands\nout above all others: social cards -- the cards generated by platforms like\nTwitter when one shares a URL. Once a metadata standard was established for\ncards in 2010, its fields were adopted by 20% of articles in the first year and\nreached more than 95% adoption by 2016. This rate of adoption surpasses efforts\nlike Schema.org and Dublin Core by a fair margin. When confronted with these\nresults on how news publishers spend their metadata budget, we must conclude\nthat it is all about the cards.",
    "published_date": "2021-04-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DL",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.04116v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.04050v1",
    "title": "Flavored Tacotron: Conditional Learning for Prosodic-linguistic Features",
    "authors": [
      "Mahsa Elyasi",
      "Gaurav Bharaj"
    ],
    "author_ids": [],
    "abstract": "Neural sequence-to-sequence text-to-speech synthesis (TTS), such as\nTacotron-2, transforms text into high-quality speech. However, generating\nspeech with natural prosody still remains a challenge. Yasuda et. al. show that\nunlike natural speech, Tacotron-2's encoder doesn't fully represent prosodic\nfeatures (e.g. syllable stress in English) from characters, and result in flat\nfundamental frequency variations.\n  In this work, we propose a novel carefully designed strategy for conditioning\nTacotron-2 on two fundamental prosodic features in English -- stress syllable\nand pitch accent, that help achieve more natural prosody. To this end, we use\nof a classifier to learn these features in an end-to-end fashion, and apply\nfeature conditioning at three parts of Tacotron-2's Text-To-Mel Spectrogram:\npre-encoder, post-encoder, and intra-decoder. Further, we show that jointly\nconditioned features at pre-encoder and intra-decoder stages result in\nprosodically natural synthesized speech (vs. Tacotron-2), and allows the model\nto produce speech with more accurate pitch accent and stress patterns.\n  Quantitative evaluations show that our formulation achieves higher\nfundamental frequency contour correlation, and lower Mel Cepstral Distortion\nmeasure between synthesized and natural speech. And subjective evaluation shows\nthat the proposed method's Mean Opinion Score of 4.14 fairs higher than\nbaseline Tacotron-2, 3.91, when compared against natural speech (LJSpeech\ncorpus), 4.28.",
    "published_date": "2021-04-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SD",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.04050v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.04046v4",
    "title": "Semi-Supervised Learning of Classifiers from a Statistical Perspective: A Brief Review",
    "authors": [
      "Daniel Ahfock",
      "Geoffrey J. McLachlan"
    ],
    "author_ids": [],
    "abstract": "There has been increasing attention to semi-supervised learning (SSL)\napproaches in machine learning to forming a classifier in situations where the\ntraining data for a classifier consists of a limited number of classified\nobservations but a much larger number of unclassified observations. This is\nbecause the procurement of classified data can be quite costly due to high\nacquisition costs and subsequent financial, time, and ethical issues that can\narise in attempts to provide the true class labels for the unclassified data\nthat have been acquired. We provide here a review of statistical SSL approaches\nto this problem, focussing on the recent result that a classifier formed from a\npartially classified sample can actually have smaller expected error rate than\nthat if the sample were completely classified.",
    "published_date": "2021-04-08T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.04046v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.03759v1",
    "title": "Phoneme-based Distribution Regularization for Speech Enhancement",
    "authors": [
      "Yajing Liu",
      "Xiulian Peng",
      "Zhiwei Xiong",
      "Yan Lu"
    ],
    "author_ids": [],
    "abstract": "Existing speech enhancement methods mainly separate speech from noises at the\nsignal level or in the time-frequency domain. They seldom pay attention to the\nsemantic information of a corrupted signal. In this paper, we aim to bridge\nthis gap by extracting phoneme identities to help speech enhancement.\nSpecifically, we propose a phoneme-based distribution regularization (PbDr) for\nspeech enhancement, which incorporates frame-wise phoneme information into\nspeech enhancement network in a conditional manner. As different phonemes\nalways lead to different feature distributions in frequency, we propose to\nlearn a parameter pair, i.e. scale and bias, through a phoneme classification\nvector to modulate the speech enhancement network. The modulation parameter\npair includes not only frame-wise but also frequency-wise conditions, which\neffectively map features to phoneme-related distributions. In this way, we\nexplicitly regularize speech enhancement features by recognition vectors.\nExperiments on public datasets demonstrate that the proposed PbDr module can\nnot only boost the perceptual quality for speech enhancement but also the\nrecognition accuracy of an ASR system on the enhanced speech. This PbDr module\ncould be readily incorporated into other speech enhancement networks as well.",
    "published_date": "2021-04-08T00:00:00",
    "year": 2021,
    "categories": [
      "eess.AS",
      "cs.SD"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.03759v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.03741v1",
    "title": "Voluntary safety commitments provide an escape from over-regulation in AI development",
    "authors": [
      "The Anh Han",
      "Tom Lenaerts",
      "Francisco C. Santos",
      "Luis Moniz Pereira"
    ],
    "author_ids": [],
    "abstract": "With the introduction of Artificial Intelligence (AI) and related\ntechnologies in our daily lives, fear and anxiety about their misuse as well as\nthe hidden biases in their creation have led to a demand for regulation to\naddress such issues. Yet blindly regulating an innovation process that is not\nwell understood, may stifle this process and reduce benefits that society may\ngain from the generated technology, even under the best intentions. In this\npaper, starting from a baseline model that captures the fundamental dynamics of\na race for domain supremacy using AI technology, we demonstrate how socially\nunwanted outcomes may be produced when sanctioning is applied unconditionally\nto risk-taking, i.e. potentially unsafe, behaviours. As an alternative to\nresolve the detrimental effect of over-regulation, we propose a voluntary\ncommitment approach wherein technologists have the freedom of choice between\nindependently pursuing their course of actions or establishing binding\nagreements to act safely, with sanctioning of those that do not abide to what\nthey pledged. Overall, this work reveals for the first time how voluntary\ncommitments, with sanctions either by peers or an institution, leads to\nsocially beneficial outcomes in all scenarios envisageable in a short-term race\ntowards domain supremacy through AI technology. These results are directly\nrelevant for the design of governance and regulatory policies that aim to\nensure an ethical and responsible AI technology development process.",
    "published_date": "2021-04-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.MA",
      "nlin.AO",
      "nlin.CD"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.03741v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2105.15098v1",
    "title": "Zero-bias Deep Learning Enabled Quick and Reliable Abnormality Detection in IoT",
    "authors": [
      "Yongxin Liu",
      "Jian Wang",
      "Jianqiang Li",
      "Shuteng Niu",
      "Houbing Song"
    ],
    "author_ids": [],
    "abstract": "Abnormality detection is essential to the performance of safety-critical and\nlatency-constrained systems. However, as systems are becoming increasingly\ncomplicated with a large quantity of heterogeneous data, conventional\nstatistical change point detection methods are becoming less effective and\nefficient. Although Deep Learning (DL) and Deep Neural Networks (DNNs) are\nincreasingly employed to handle heterogeneous data, they still lack theoretic\nassurable performance and explainability. This paper integrates zero-bias DNN\nand Quickest Event Detection algorithms to provide a holistic framework for\nquick and reliable detection of both abnormalities and time-dependent abnormal\nevents in the Internet of Things (IoT). We first use the zero-bias dense layer\nto increase the explainability of DNN. We provide a solution to convert\nzero-bias DNN classifiers into performance assured binary abnormality\ndetectors. Using the converted abnormality detector, we then present a\nsequential quickest detection scheme that provides the theoretically assured\nlowest abnormal event detection delay under false alarm constraints. Finally,\nwe demonstrate the effectiveness of the framework using both massive signal\nrecords from real-world aviation communication systems and simulated data. Code\nand data of our work is available at\n\\url{https://github.com/pcwhy/AbnormalityDetectionInZbDNN}",
    "published_date": "2021-04-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.NI",
      "cs.LG",
      "cs.PF"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.15098v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.03469v2",
    "title": "Gi and Pal Scores: Deep Neural Network Generalization Statistics",
    "authors": [
      "Yair Schiff",
      "Brian Quanz",
      "Payel Das",
      "Pin-Yu Chen"
    ],
    "author_ids": [],
    "abstract": "The field of Deep Learning is rich with empirical evidence of human-like\nperformance on a variety of regression, classification, and control tasks.\nHowever, despite these successes, the field lacks strong theoretical error\nbounds and consistent measures of network generalization and learned\ninvariances. In this work, we introduce two new measures, the Gi-score and\nPal-score, that capture a deep neural network's generalization capabilities.\nInspired by the Gini coefficient and Palma ratio, measures of income\ninequality, our statistics are robust measures of a network's invariance to\nperturbations that accurately predict generalization gaps, i.e., the difference\nbetween accuracy on training and test sets.",
    "published_date": "2021-04-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "I.2.6; G.3; I.5.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.03469v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.03428v2",
    "title": "Generating Multi-type Temporal Sequences to Mitigate Class-imbalanced Problem",
    "authors": [
      "Lun Jiang",
      "Nima Salehi Sadghiani",
      "Zhuo Tao",
      "Andrew Cohen"
    ],
    "author_ids": [],
    "abstract": "From the ad network standpoint, a user's activity is a multi-type sequence of\ntemporal events consisting of event types and time intervals. Understanding\nuser patterns in ad networks has received increasing attention from the machine\nlearning community. Particularly, the problems of fraud detection, Conversion\nRate (CVR), and Click-Through Rate (CTR) prediction are of interest. However,\nthe class imbalance between major and minor classes in these tasks can bias a\nmachine learning model leading to poor performance. This study proposes using\ntwo multi-type (continuous and discrete) training approaches for GANs to deal\nwith the limitations of traditional GANs in passing the gradient updates for\ndiscrete tokens. First, we used the Reinforcement Learning (RL)-based training\napproach and then, an approximation of the multinomial distribution\nparameterized in terms of the softmax function (Gumble-Softmax). Our extensive\nexperiments based on synthetic data have shown the trained generator can\ngenerate sequences with desired properties measured by multiple criteria.",
    "published_date": "2021-04-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.03428v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.03298v2",
    "title": "Minimax Estimation of Linear Functions of Eigenvectors in the Face of Small Eigen-Gaps",
    "authors": [
      "Gen Li",
      "Changxiao Cai",
      "H. Vincent Poor",
      "Yuxin Chen"
    ],
    "author_ids": [],
    "abstract": "Eigenvector perturbation analysis plays a vital role in various data science\napplications. A large body of prior works, however, focused on establishing\n$\\ell_{2}$ eigenvector perturbation bounds, which are often highly inadequate\nin addressing tasks that rely on fine-grained behavior of an eigenvector. This\npaper makes progress on this by studying the perturbation of linear functions\nof an unknown eigenvector. Focusing on two fundamental problems -- matrix\ndenoising and principal component analysis -- in the presence of Gaussian\nnoise, we develop a suite of statistical theory that characterizes the\nperturbation of arbitrary linear functions of an unknown eigenvector. In order\nto mitigate a non-negligible bias issue inherent to the natural ``plug-in''\nestimator, we develop de-biased estimators that (1) achieve minimax lower\nbounds for a family of scenarios (modulo some logarithmic factor), and (2) can\nbe computed in a data-driven manner without sample splitting. Noteworthily, the\nproposed estimators are nearly minimax optimal even when the associated\neigen-gap is {\\em substantially smaller} than what is required in prior\nstatistical theory.",
    "published_date": "2021-04-07T00:00:00",
    "year": 2021,
    "categories": [
      "math.ST",
      "cs.IT",
      "cs.LG",
      "math.IT",
      "stat.ML",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.03298v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.03066v2",
    "title": "Distributional Robustness Loss for Long-tail Learning",
    "authors": [
      "Dvir Samuel",
      "Gal Chechik"
    ],
    "author_ids": [],
    "abstract": "Real-world data is often unbalanced and long-tailed, but deep models struggle\nto recognize rare classes in the presence of frequent classes. To address\nunbalanced data, most studies try balancing the data, the loss, or the\nclassifier to reduce classification bias towards head classes. Far less\nattention has been given to the latent representations learned with unbalanced\ndata. We show that the feature extractor part of deep networks suffers greatly\nfrom this bias. We propose a new loss based on robustness theory, which\nencourages the model to learn high-quality representations for both head and\ntail classes. While the general form of the robustness loss may be hard to\ncompute, we further derive an easy-to-compute upper bound that can be minimized\nefficiently. This procedure reduces representation bias towards head classes in\nthe feature space and achieves new SOTA results on CIFAR100-LT, ImageNet-LT,\nand iNaturalist long-tail benchmarks. We find that training with robustness\nincreases recognition accuracy of tail classes while largely maintaining the\naccuracy of head classes. The new robustness loss can be combined with various\nclassifier balancing techniques and can be applied to representations at\nseveral layers of the deep model.",
    "published_date": "2021-04-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.03066v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.03026v1",
    "title": "How to Write a Bias Statement: Recommendations for Submissions to the Workshop on Gender Bias in NLP",
    "authors": [
      "Christian Hardmeier",
      "Marta R. Costa-jussà",
      "Kellie Webster",
      "Will Radford",
      "Su Lin Blodgett"
    ],
    "author_ids": [],
    "abstract": "At the Workshop on Gender Bias in NLP (GeBNLP), we'd like to encourage\nauthors to give explicit consideration to the wider aspects of bias and its\nsocial implications. For the 2020 edition of the workshop, we therefore\nrequested that all authors include an explicit bias statement in their work to\nclarify how their work relates to the social context in which NLP systems are\nused.\n  The programme committee of the workshops included a number of reviewers with\na background in the humanities and social sciences, in addition to NLP experts\ndoing the bulk of the reviewing. Each paper was assigned one of those\nreviewers, and they were asked to pay specific attention to the provided bias\nstatements in their reviews. This initiative was well received by the authors\nwho submitted papers to the workshop, several of whom said they received useful\nsuggestions and literature hints from the bias reviewers. We are therefore\nplanning to keep this feature of the review process in future editions of the\nworkshop.",
    "published_date": "2021-04-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.03026v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.03010v1",
    "title": "Ethical User Interfaces: Exploring the Effects of Dark Patterns on Facebook",
    "authors": [
      "Thomas Mildner",
      "Gian-Luca Savino"
    ],
    "author_ids": [],
    "abstract": "Many researchers have been concerned with whether social media has a negative\nimpact on the well-being of their audience. With the popularity of social\nnetworking sites (SNS) steadily increasing, psychological and social sciences\nhave shown great interest in their effects and consequences on humans. In this\nwork, we investigate Facebook using the tools of HCI to find connections\nbetween interface features and the concerns raised by these domains. Using an\nempirical design analysis, we identify interface interferences impacting users'\nonline privacy. Through a subsequent survey (n=116), we find usage behaviour\nchanges due to increased privacy concerns and report individual cases of\naddiction and mental health issues. These observations are the results of a\nrapidly changing SNS creating a gap of understanding between users'\ninteractions with the platform and future consequences. We explore how HCI can\nhelp close this gap and work towards more ethical user interfaces in the\nfuture.",
    "published_date": "2021-04-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.03010v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.03007v1",
    "title": "Representative & Fair Synthetic Data",
    "authors": [
      "Paul Tiwald",
      "Alexandra Ebert",
      "Daniel T. Soukup"
    ],
    "author_ids": [],
    "abstract": "Algorithms learn rules and associations based on the training data that they\nare exposed to. Yet, the very same data that teaches machines to understand and\npredict the world, contains societal and historic biases, resulting in biased\nalgorithms with the risk of further amplifying these once put into use for\ndecision support. Synthetic data, on the other hand, emerges with the promise\nto provide an unlimited amount of representative, realistic training samples,\nthat can be shared further without disclosing the privacy of individual\nsubjects. We present a framework to incorporate fairness constraints into the\nself-supervised learning process, that allows to then simulate an unlimited\namount of representative as well as fair synthetic data. This framework\nprovides a handle to govern and control for privacy as well as for bias within\nAI at its very source: the training data. We demonstrate the proposed approach\nby amending an existing generative model architecture and generating a\nrepresentative as well as fair version of the UCI Adult census data set. While\nthe relationships between attributes are faithfully retained, the gender and\nracial biases inherent in the original data are controlled for. This is further\nvalidated by comparing propensity scores of downstream predictive models that\nare trained on the original data versus the fair synthetic data. We consider\nrepresentative & fair synthetic data a promising future building block to teach\nalgorithms not on historic worlds, but rather on the worlds that we strive to\nlive in.",
    "published_date": "2021-04-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.03007v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.02988v3",
    "title": "Optimal Algorithms for Differentially Private Stochastic Monotone Variational Inequalities and Saddle-Point Problems",
    "authors": [
      "Digvijay Boob",
      "Cristóbal Guzmán"
    ],
    "author_ids": [],
    "abstract": "In this work, we conduct the first systematic study of stochastic variational\ninequality (SVI) and stochastic saddle point (SSP) problems under the\nconstraint of differential privacy (DP). We propose two algorithms: Noisy\nStochastic Extragradient (NSEG) and Noisy Inexact Stochastic Proximal Point\n(NISPP). We show that a stochastic approximation variant of these algorithms\nattains risk bounds vanishing as a function of the dataset size, with respect\nto the strong gap function; and a sampling with replacement variant achieves\noptimal risk bounds with respect to a weak gap function. We also show lower\nbounds of the same order on weak gap function. Hence, our algorithms are\noptimal. Key to our analysis is the investigation of algorithmic stability\nbounds, both of which are new even in the nonprivate case. The dependence of\nthe running time of the sampling with replacement algorithms, with respect to\nthe dataset size $n$, is $n^2$ for NSEG and $\\tilde{O}(n^{3/2})$ for NISPP.",
    "published_date": "2021-04-07T00:00:00",
    "year": 2021,
    "categories": [
      "math.OC",
      "cs.LG",
      "stat.ML",
      "65K10, 49M37, 68T09"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.02988v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.02939v3",
    "title": "OpenGAN: Open-Set Recognition via Open Data Generation",
    "authors": [
      "Shu Kong",
      "Deva Ramanan"
    ],
    "author_ids": [],
    "abstract": "Real-world machine learning systems need to analyze test data that may differ\nfrom training data. In K-way classification, this is crisply formulated as\nopen-set recognition, core to which is the ability to discriminate open-set\ndata outside the K closed-set classes. Two conceptually elegant ideas for\nopen-set discrimination are: 1) discriminatively learning an open-vs-closed\nbinary discriminator by exploiting some outlier data as the open-set, and 2)\nunsupervised learning the closed-set data distribution with a GAN, using its\ndiscriminator as the open-set likelihood function. However, the former\ngeneralizes poorly to diverse open test data due to overfitting to the training\noutliers, which are unlikely to exhaustively span the open-world. The latter\ndoes not work well, presumably due to the instable training of GANs. Motivated\nby the above, we propose OpenGAN, which addresses the limitation of each\napproach by combining them with several technical insights. First, we show that\na carefully selected GAN-discriminator on some real outlier data already\nachieves the state-of-the-art. Second, we augment the available set of real\nopen training examples with adversarially synthesized \"fake\" data. Third and\nmost importantly, we build the discriminator over the features computed by the\nclosed-world K-way networks. This allows OpenGAN to be implemented via a\nlightweight discriminator head built on top of an existing K-way network.\nExtensive experiments show that OpenGAN significantly outperforms prior\nopen-set methods.",
    "published_date": "2021-04-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.02939v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.02821v2",
    "title": "Towards Measuring Fairness in AI: the Casual Conversations Dataset",
    "authors": [
      "Caner Hazirbas",
      "Joanna Bitton",
      "Brian Dolhansky",
      "Jacqueline Pan",
      "Albert Gordo",
      "Cristian Canton Ferrer"
    ],
    "author_ids": [],
    "abstract": "This paper introduces a novel dataset to help researchers evaluate their\ncomputer vision and audio models for accuracy across a diverse set of age,\ngenders, apparent skin tones and ambient lighting conditions. Our dataset is\ncomposed of 3,011 subjects and contains over 45,000 videos, with an average of\n15 videos per person. The videos were recorded in multiple U.S. states with a\ndiverse set of adults in various age, gender and apparent skin tone groups. A\nkey feature is that each subject agreed to participate for their likenesses to\nbe used. Additionally, our age and gender annotations are provided by the\nsubjects themselves. A group of trained annotators labeled the subjects'\napparent skin tone using the Fitzpatrick skin type scale. Moreover, annotations\nfor videos recorded in low ambient lighting are also provided. As an\napplication to measure robustness of predictions across certain attributes, we\nprovide a comprehensive study on the top five winners of the DeepFake Detection\nChallenge (DFDC). Experimental evaluation shows that the winning models are\nless performant on some specific groups of people, such as subjects with darker\nskin tones and thus may not generalize to all people. In addition, we also\nevaluate the state-of-the-art apparent age and gender classification methods.\nOur experiments provides a thorough analysis on these models in terms of fair\ntreatment of people from various backgrounds.",
    "published_date": "2021-04-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.02821v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.02808v2",
    "title": "Robust Control Barrier-Value Functions for Safety-Critical Control",
    "authors": [
      "Jason J. Choi",
      "Donggun Lee",
      "Koushil Sreenath",
      "Claire J. Tomlin",
      "Sylvia L. Herbert"
    ],
    "author_ids": [],
    "abstract": "This paper works towards unifying two popular approaches in the safety\ncontrol community: Hamilton-Jacobi (HJ) reachability and Control Barrier\nFunctions (CBFs). HJ Reachability has methods for direct construction of value\nfunctions that provide safety guarantees and safe controllers, however the\nonline implementation can be overly conservative and/or rely on chattering\nbang-bang control. The CBF community has methods for safe-guarding controllers\nin the form of point-wise optimization using quadratic programs (CBF-QP), where\nthe CBF-based safety certificate is used as a constraint. However, finding a\nvalid CBF for a general dynamical system is challenging. This paper unifies\nthese two methods by introducing a new reachability formulation inspired by the\nstructure of CBFs to construct a Control Barrier-Value Function (CBVF). We\nverify that CBVF is a viscosity solution to a novel Hamilton-Jacobi-Isaacs\nVariational Inequality and preserves the same safety guarantee as the original\nreachability formulation. Finally, inspired by the CBF-QP, we propose a\nQP-based online control synthesis for systems affine in control and\ndisturbance, whose solution is always the CBVF's optimal control signal robust\nto bounded disturbance. We demonstrate the benefit of using the CBVFs for\ndouble-integrator and Dubins car systems by comparing it to previous methods.",
    "published_date": "2021-04-06T00:00:00",
    "year": 2021,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.02808v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.02797v1",
    "title": "VERB: Visualizing and Interpreting Bias Mitigation Techniques for Word Representations",
    "authors": [
      "Archit Rathore",
      "Sunipa Dev",
      "Jeff M. Phillips",
      "Vivek Srikumar",
      "Yan Zheng",
      "Chin-Chia Michael Yeh",
      "Junpeng Wang",
      "Wei Zhang",
      "Bei Wang"
    ],
    "author_ids": [],
    "abstract": "Word vector embeddings have been shown to contain and amplify biases in data\nthey are extracted from. Consequently, many techniques have been proposed to\nidentify, mitigate, and attenuate these biases in word representations. In this\npaper, we utilize interactive visualization to increase the interpretability\nand accessibility of a collection of state-of-the-art debiasing techniques. To\naid this, we present Visualization of Embedding Representations for deBiasing\nsystem (\"VERB\"), an open-source web-based visualization tool that helps the\nusers gain a technical understanding and visual intuition of the inner workings\nof debiasing techniques, with a focus on their geometric properties. In\nparticular, VERB offers easy-to-follow use cases in exploring the effects of\nthese debiasing techniques on the geometry of high-dimensional word vectors. To\nhelp understand how various debiasing techniques change the underlying\ngeometry, VERB decomposes each technique into interpretable sequences of\nprimitive transformations and highlights their effect on the word vectors using\ndimensionality reduction and interactive visual exploration. VERB is designed\nto target natural language processing (NLP) practitioners who are designing\ndecision-making systems on top of word embeddings, and also researchers working\nwith fairness and ethics of machine learning systems in NLP. It can also serve\nas a visual medium for education, which helps an NLP novice to understand and\nmitigate biases in word embeddings.",
    "published_date": "2021-04-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.02797v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.02776v1",
    "title": "Misbehavior Detection in Wi-Fi/LTE Coexistence over Unlicensed Bands",
    "authors": [
      "Islam Samy",
      "Xiao Han",
      "Loukas Lazos",
      "Ming Li",
      "Yong Xiao",
      "Marwan Krunz"
    ],
    "author_ids": [],
    "abstract": "We consider the problem of fair coexistence between LTE and Wi-Fi systems in\nthe unlicensed 5 GHz U-NII bands. We focus on the misbehavior opportunities due\nto the heterogeneity in channel access mechanism and the lack of a common\ncontrol plane. We define selfish misbehavior strategies for the LTE that yield\nan unfair share of the spectrum resources. Such strategies are based on\nmanipulating the operational parameters of the LTE-LAA standard, namely the\nbackoff mechanism, the traffic class parameters, the clear channel access (CCA)\nthreshold, and others. Prior methods for detecting misbehavior in homogeneous\nsettings are not applicable in a spectrum sharing scenario because the devices\nof one system cannot decode the transmissions of another. We develop implicit\nsensing techniques that can accurately estimate the operational parameters of\nLTE transmissions under various topological scenarios and {\\em without\ndecoding.} These techniques apply correlation-based signal detection to infer\nthe required information. Our techniques are validated through experiments on a\nUSRP testbed. We further apply a statistical inference framework for\ndetermining deviations of the LTE behavior from the coexistence etiquette. By\ncharacterizing the detection and false alarm probabilities, we show that our\nframework yields high detection accuracy at a very low false alarm rate.\nAlthough our methods focus on detecting misbehavior of the LTE system, they can\nbe generalized to other coexistence scenarios.",
    "published_date": "2021-04-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.02776v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.02752v1",
    "title": "Multiscale Governance",
    "authors": [
      "David Pastor-Escuredo",
      "Philip Treleaven"
    ],
    "author_ids": [],
    "abstract": "Future societal systems will be characterized by heterogeneous human\nbehaviors and also collective action. The interaction between local systems and\nglobal systems will be complex. Humandemics will propagate because of the\npathways that connect the different systems and several invariant behaviors and\npatterns that have emerged globally. On the contrary, infodemics of\nmisinformation can be a risk as it has occurred in the COVID-19 pandemic. The\nemerging fragility or robustness of the system will depend on how this complex\nnetwork of systems is governed. Future societal systems will not be only\nmultiscale in terms of the social dimension, but also in the temporality.\nNecessary and proper prevention and response systems based on complexity, ethic\nand multi-scale governance will be required. Real-time response systems are the\nbasis for resilience to be the foundation of robust societies. A top-down\napproach led by Governmental organs for managing humandemics is not sufficient\nand may be only effective if policies are very restrictive and their efficacy\ndepends not only in the measures implemented but also on the dynamics of the\npolicies and the population perception and compliance. This top-down approach\nis even weaker if there is not national and international coordination.\nCoordinating top-down agencies with bottom-up constructs will be the design\nprinciple. Multi-scale governance integrates decision-making processes with\nsignaling, sensing and leadership mechanisms to drive thriving societal systems\nwith real-time sensitivity.",
    "published_date": "2021-04-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.CC",
      "econ.GN",
      "q-fin.EC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.02752v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.02640v3",
    "title": "A non-asymptotic approach for model selection via penalization in high-dimensional mixture of experts models",
    "authors": [
      "TrungTin Nguyen",
      "Hien Duy Nguyen",
      "Faicel Chamroukhi",
      "Florence Forbes"
    ],
    "author_ids": [],
    "abstract": "Mixture of experts (MoE) are a popular class of statistical and machine\nlearning models that have gained attention over the years due to their\nflexibility and efficiency. In this work, we consider Gaussian-gated localized\nMoE (GLoME) and block-diagonal covariance localized MoE (BLoME) regression\nmodels to present nonlinear relationships in heterogeneous data with potential\nhidden graph-structured interactions between high-dimensional predictors. These\nmodels pose difficult statistical estimation and model selection questions,\nboth from a computational and theoretical perspective. This paper is devoted to\nthe study of the problem of model selection among a collection of GLoME or\nBLoME models characterized by the number of mixture components, the complexity\nof Gaussian mean experts, and the hidden block-diagonal structures of the\ncovariance matrices, in a penalized maximum likelihood estimation framework. In\nparticular, we establish non-asymptotic risk bounds that take the form of weak\noracle inequalities, provided that lower bounds for the penalties hold. The\ngood empirical behavior of our models is then demonstrated on synthetic and\nreal datasets.",
    "published_date": "2021-04-06T00:00:00",
    "year": 2021,
    "categories": [
      "math.ST",
      "cs.AI",
      "cs.LG",
      "stat.ME",
      "stat.ML",
      "stat.TH",
      "62E17 (Primary) 62H30, 62H12 (Secondary)"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.02640v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.02592v1",
    "title": "Global Software Engineering in the Age of GitHub and Zoom",
    "authors": [
      "James Herbsleb"
    ],
    "author_ids": [],
    "abstract": "Much has changed since the inaugural ICGSE conference in 2006. Tools have\nimproved, awareness of cultural differences is widespread, and developments\nsuch as the foregrounding of open source have all enhanced our ability to work\nacross geographic divides. But the pervasive and profound impact of software in\nthe world -- especially for societal scale systems such as social media --\nforces new and deeply challenging responsibilities on both developers and\nacademics. We must find better ways of incorporating ethics into our\ndevelopment practices and pay far more attention to harmful unintended\nconsequences as deployed systems interact with and often disrupt crucial social\nsystems.",
    "published_date": "2021-04-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SE",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.02592v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.02580v1",
    "title": "Future of work: ethics",
    "authors": [
      "David Pastor-Escuredo"
    ],
    "author_ids": [],
    "abstract": "Work must be reshaped in the upcoming new era characterized by new challenges\nand the presence of new technologies and computational tools. Over-automation\nseems to be the driver of the digitalization process. Substitution is the\nparadigm leading Artificial Intelligence and robotics development against human\ncognition. Digital technology should be designed to enhance human skills and\nmake more productive use of human cognition and capacities. Digital technology\nis characterized also by scalability because of its easy and inexpensive\ndeployment. Thus, automation can lead to the absence of jobs and scalable\nnegative impact in human development and the performance of business. A look at\ndigitalization from the lens of Sustainable Development Goals can tell us how\ndigitalization impact in different sectors and areas considering society as a\ncomplex interconnected system. Here, reflections on how AI and Data impact\nfuture of work and sustainable development are provided grounded on an ethical\ncore that comprises human-level principles and also systemic principles.",
    "published_date": "2021-04-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "econ.GN",
      "q-fin.EC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.02580v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.02532v3",
    "title": "End-To-End Bias Mitigation: Removing Gender Bias in Deep Learning",
    "authors": [
      "Tal Feldman",
      "Ashley Peake"
    ],
    "author_ids": [],
    "abstract": "Machine Learning models have been deployed across many different aspects of\nsociety, often in situations that affect social welfare. Although these models\noffer streamlined solutions to large problems, they may contain biases and\ntreat groups or individuals unfairly based on protected attributes such as\ngender. In this paper, we introduce several examples of machine learning gender\nbias in practice followed by formalizations of fairness. We provide a survey of\nfairness research by detailing influential pre-processing, in-processing, and\npost-processing bias mitigation algorithms. We then propose an end-to-end bias\nmitigation framework, which employs a fusion of pre-, in-, and post-processing\nmethods to leverage the strengths of each individual technique. We test this\nmethod, along with the standard techniques we review, on a deep neural network\nto analyze bias mitigation in a deep learning setting. We find that our\nend-to-end bias mitigation framework outperforms the baselines with respect to\nseveral fairness metrics, suggesting its promise as a method for improving\nfairness. As society increasingly relies on artificial intelligence to help in\ndecision-making, addressing gender biases present in deep learning models is\nimperative. To provide readers with the tools to assess the fairness of machine\nlearning models and mitigate the biases present in them, we discuss multiple\nopen source packages for fairness in AI.",
    "published_date": "2021-04-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.02532v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.02425v2",
    "title": "The Duo of Artificial Intelligence and Big Data for Industry 4.0: Review of Applications, Techniques, Challenges, and Future Research Directions",
    "authors": [
      "Senthil Kumar Jagatheesaperumal",
      "Mohamed Rahouti",
      "Kashif Ahmad",
      "Ala Al-Fuqaha",
      "Mohsen Guizani"
    ],
    "author_ids": [],
    "abstract": "The increasing need for economic, safe, and sustainable smart manufacturing\ncombined with novel technological enablers, has paved the way for Artificial\nIntelligence (AI) and Big Data in support of smart manufacturing. This implies\na substantial integration of AI, Industrial Internet of Things (IIoT),\nRobotics, Big data, Blockchain, 5G communications, in support of smart\nmanufacturing and the dynamical processes in modern industries. In this paper,\nwe provide a comprehensive overview of different aspects of AI and Big Data in\nIndustry 4.0 with a particular focus on key applications, techniques, the\nconcepts involved, key enabling technologies, challenges, and research\nperspective towards deployment of Industry 5.0. In detail, we highlight and\nanalyze how the duo of AI and Big Data is helping in different applications of\nIndustry 4.0. We also highlight key challenges in a successful deployment of AI\nand Big Data methods in smart industries with a particular emphasis on\ndata-related issues, such as availability, bias, auditing, management,\ninterpretability, communication, and different adversarial attacks and security\nissues. In a nutshell, we have explored the significance of AI and Big data\ntowards Industry 4.0 applications through panoramic reviews and discussions. We\nbelieve, this work will provide a baseline for future research in the domain.",
    "published_date": "2021-04-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.02425v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.02414v2",
    "title": "On Adaptive Fairness in Software Systems",
    "authors": [
      "Ali Farahani",
      "Liliana Pasquale",
      "Amel Bennaceur",
      "Thomas Welsh",
      "Bashar Nuseibeh"
    ],
    "author_ids": [],
    "abstract": "Software systems are increasingly making decisions on behalf of humans,\nraising concerns about the fairness of such decisions. Such concerns are\nusually attributed to flaws in algorithmic design or biased data, but we argue\nthat they are often the result of a lack of explicit specification of fairness\nrequirements. However, such requirements are challenging to elicit, a problem\nexacerbated by increasingly dynamic environments in which software systems\noperate, as well as stakeholders' changing needs. Therefore, capturing all\nfairness requirements during the production of software is challenging, and is\ninsufficient for addressing software changes post deployment. In this paper, we\npropose adaptive fairness as a means for maintaining the satisfaction of\nchanging fairness requirements. We demonstrate how to combine\nrequirements-driven and resource-driven adaptation in order to address\nvariabilities in both fairness requirements and their associated resources.\nUsing models for fairness requirements, resources, and their relations, we show\nhow the approach can be used to provide systems owners and end-users with\ncapabilities that reflect adaptive fairness behaviours at runtime. We\ndemonstrate our approach using an example drawn from shopping experiences of\ncitizens. We conclude with a discussion of open research challenges in the\nengineering of adaptive fairness in human-facing software systems.",
    "published_date": "2021-04-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.02414v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.02413v1",
    "title": "Bias Correction in Deterministic Policy Gradient Using Robust MPC",
    "authors": [
      "Arash Bahari Kordabad",
      "Hossein Nejatbakhsh Esfahani",
      "Sebastien Gros"
    ],
    "author_ids": [],
    "abstract": "In this paper, we discuss the deterministic policy gradient using the\nActor-Critic methods based on the linear compatible advantage function\napproximator, where the input spaces are continuous. When the policy is\nrestricted by hard constraints, the exploration may not be Centred or Isotropic\n(non-CI). As a result, the policy gradient estimation can be biased. We focus\non constrained policies based on Model Predictive Control (MPC) schemes and to\naddress the bias issue, we propose an approximate Robust MPC approach\naccounting for the exploration. The RMPC-based policy ensures that a Centered\nand Isotropic (CI) exploration is approximately feasible. A posterior\nprojection is used to ensure its exact feasibility, we formally prove that this\napproach does not bias the gradient estimation.",
    "published_date": "2021-04-06T00:00:00",
    "year": 2021,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.02413v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.02290v1",
    "title": "Contrastive Syn-to-Real Generalization",
    "authors": [
      "Wuyang Chen",
      "Zhiding Yu",
      "Shalini De Mello",
      "Sifei Liu",
      "Jose M. Alvarez",
      "Zhangyang Wang",
      "Anima Anandkumar"
    ],
    "author_ids": [],
    "abstract": "Training on synthetic data can be beneficial for label or data-scarce\nscenarios. However, synthetically trained models often suffer from poor\ngeneralization in real domains due to domain gaps. In this work, we make a key\nobservation that the diversity of the learned feature embeddings plays an\nimportant role in the generalization performance. To this end, we propose\ncontrastive synthetic-to-real generalization (CSG), a novel framework that\nleverages the pre-trained ImageNet knowledge to prevent overfitting to the\nsynthetic domain, while promoting the diversity of feature embeddings as an\ninductive bias to improve generalization. In addition, we enhance the proposed\nCSG framework with attentional pooling (A-pool) to let the model focus on\nsemantically important regions and further improve its generalization. We\ndemonstrate the effectiveness of CSG on various synthetic training tasks,\nexhibiting state-of-the-art performance on zero-shot domain generalization.",
    "published_date": "2021-04-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.02290v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.12544v1",
    "title": "The Myth of Complete AI-Fairness",
    "authors": [
      "Virginia Dignum"
    ],
    "author_ids": [],
    "abstract": "The idea of fairness and justice has long and deep roots in Western\ncivilization, and is strongly linked to ethics. It is therefore not strange\nthat it is core to the current discussion about the ethics of development and\nuse of AI systems. In this short paper, I wish to further motivate my position\nin this matter: ``I will never be completely fair. Nothing ever is. The point\nis not complete fairness, but the need to establish metrics and thresholds for\nfairness that ensure trust in AI systems\".",
    "published_date": "2021-04-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.12544v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.02145v3",
    "title": "What Will it Take to Fix Benchmarking in Natural Language Understanding?",
    "authors": [
      "Samuel R. Bowman",
      "George E. Dahl"
    ],
    "author_ids": [],
    "abstract": "Evaluation for many natural language understanding (NLU) tasks is broken:\nUnreliable and biased systems score so highly on standard benchmarks that there\nis little room for researchers who develop better systems to demonstrate their\nimprovements. The recent trend to abandon IID benchmarks in favor of\nadversarially-constructed, out-of-distribution test sets ensures that current\nmodels will perform poorly, but ultimately only obscures the abilities that we\nwant our benchmarks to measure. In this position paper, we lay out four\ncriteria that we argue NLU benchmarks should meet. We argue most current\nbenchmarks fail at these criteria, and that adversarial data collection does\nnot meaningfully address the causes of these failures. Instead, restoring a\nhealthy evaluation ecosystem will require significant progress in the design of\nbenchmark datasets, the reliability with which they are annotated, their size,\nand the ways they handle social bias.",
    "published_date": "2021-04-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.02145v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.02050v2",
    "title": "Prophet Inequalities for Matching with a Single Sample",
    "authors": [
      "Paul Dütting",
      "Federico Fusco",
      "Philip Lazos",
      "Stefano Leonardi",
      "Rebecca Reiffenhäuser"
    ],
    "author_ids": [],
    "abstract": "We consider the prophet inequality problem for (not necessarily bipartite)\nmatching problems with independent edge values, under both edge arrivals and\nvertex arrivals. We show constant-factor prophet inequalities for the case\nwhere the online algorithm has only limited access to the value distributions\nthrough samples. First, we give a $16$-approximate prophet inequality for\nmatching in general graphs under edge arrivals that uses only a single sample\nfrom each value distribution as prior information. Then, for bipartite matching\nand (one-sided) vertex arrivals, we show an improved bound of $8$ that also\nuses just a single sample from each distribution. Finally, we show how to turn\nour $16$-approximate single-sample prophet inequality into a truthful\nsingle-sample mechanism for online bipartite matching with vertex arrivals.",
    "published_date": "2021-04-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DS",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.02050v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.01955v1",
    "title": "Automating Transfer Credit Assessment in Student Mobility -- A Natural Language Processing-based Approach",
    "authors": [
      "Dhivya Chandrasekaran",
      "Vijay Mago"
    ],
    "author_ids": [],
    "abstract": "Student mobility or academic mobility involves students moving between\ninstitutions during their post-secondary education, and one of the challenging\ntasks in this process is to assess the transfer credits to be offered to the\nincoming student. In general, this process involves domain experts comparing\nthe learning outcomes of the courses, to decide on offering transfer credits to\nthe incoming students. This manual implementation is not only labor-intensive\nbut also influenced by undue bias and administrative complexity. The proposed\nresearch article focuses on identifying a model that exploits the advancements\nin the field of Natural Language Processing (NLP) to effectively automate this\nprocess. Given the unique structure, domain specificity, and complexity of\nlearning outcomes (LOs), a need for designing a tailor-made model arises. The\nproposed model uses a clustering-inspired methodology based on knowledge-based\nsemantic similarity measures to assess the taxonomic similarity of LOs and a\ntransformer-based semantic similarity model to assess the semantic similarity\nof the LOs. The similarity between LOs is further aggregated to form course to\ncourse similarity. Due to the lack of quality benchmark datasets, a new\nbenchmark dataset containing seven course-to-course similarity measures is\nproposed. Understanding the inherent need for flexibility in the\ndecision-making process the aggregation part of the model offers tunable\nparameters to accommodate different scenarios. While providing an efficient\nmodel to assess the similarity between courses with existing resources, this\nresearch work steers future research attempts to apply NLP in the field of\narticulation in an ideal direction by highlighting the persisting research\ngaps.",
    "published_date": "2021-04-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.01955v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.01852v1",
    "title": "Tokenising behaviour change: optimising blockchain technology for sustainable transport interventions",
    "authors": [
      "Iain Barclay",
      "Michael Cooper",
      "Alun Preece",
      "Omer Rana",
      "Ian Taylor"
    ],
    "author_ids": [],
    "abstract": "Transport makes an impact across SDGs, encompassing climate change, health,\ninequality and sustainability. It is also an area in which individuals are able\nto make decisions which have potential to collectively contribute to\nsignificant and wide-ranging benefits. Governments and authorities need\ncitizens to make changes towards adopting sustainable transport behaviours and\nbehaviour change interventions are being used as tools to foster changes in\ntravel choices, towards more sustainable modes. Blockchain technology has the\npotential to bring new levels of scale to transport behaviour change\ninterventions, but a rigorous approach to token design is required. This paper\nuses a survey of research projects and use cases to analyse current\napplications of blockchain technology in transport behaviour change\ninterventions, and identifies barriers and limitations to achieving targeted\nchange at scale. The paper draws upon these findings to outline a research\nagenda that brings a focus on correlating specific Behaviour Change Techniques\n(BCTs) to token design, and defines processes for standardising token designs\nin behaviour change tools. The paper further outlines architecture and\noperational considerations for blockchain-based platforms in behaviour change\ninterventions, such that design choices do not compromise opportunities or\nwider environmental goals.",
    "published_date": "2021-04-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.01852v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.01832v1",
    "title": "Task-Independent Knowledge Makes for Transferable Representations for Generalized Zero-Shot Learning",
    "authors": [
      "Chaoqun Wang",
      "Xuejin Chen",
      "Shaobo Min",
      "Xiaoyan Sun",
      "Houqiang Li"
    ],
    "author_ids": [],
    "abstract": "Generalized Zero-Shot Learning (GZSL) targets recognizing new categories by\nlearning transferable image representations. Existing methods find that, by\naligning image representations with corresponding semantic labels, the\nsemantic-aligned representations can be transferred to unseen categories.\nHowever, supervised by only seen category labels, the learned semantic\nknowledge is highly task-specific, which makes image representations biased\ntowards seen categories. In this paper, we propose a novel Dual-Contrastive\nEmbedding Network (DCEN) that simultaneously learns task-specific and\ntask-independent knowledge via semantic alignment and instance discrimination.\nFirst, DCEN leverages task labels to cluster representations of the same\nsemantic category by cross-modal contrastive learning and exploring\nsemantic-visual complementarity. Besides task-specific knowledge, DCEN then\nintroduces task-independent knowledge by attracting representations of\ndifferent views of the same image and repelling representations of different\nimages. Compared to high-level seen category supervision, this instance\ndiscrimination supervision encourages DCEN to capture low-level visual\nknowledge, which is less biased toward seen categories and alleviates the\nrepresentation bias. Consequently, the task-specific and task-independent\nknowledge jointly make for transferable representations of DCEN, which obtains\naveraged 4.1% improvement on four public benchmarks.",
    "published_date": "2021-04-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.01832v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.01781v1",
    "title": "Reducing Racial Bias in Facial Age Prediction using Unsupervised Domain Adaptation in Regression",
    "authors": [
      "Apoorva Gokhale",
      "Astuti Sharma",
      "Kaustav Datta",
      "Savyasachi"
    ],
    "author_ids": [],
    "abstract": "We propose an approach for unsupervised domain adaptation for the task of\nestimating someone's age from a given face image. In order to avoid the\npropagation of racial bias in most publicly available face image datasets into\nthe inefficacy of models trained on them, we perform domain adaptation to\nmotivate the predictor to learn features that are invariant to ethnicity,\nenhancing the generalization performance across faces of people from different\nethnic backgrounds. Exploiting the ordinality of age, we also impose ranking\nconstraints on the prediction of the model and design our model such that it\ntakes as input a pair of images, and outputs both the relative age difference\nand the rank of the first identity with respect to the other in terms of their\nages. Furthermore, we implement Multi-Dimensional Scaling to retrieve absolute\nages from the predicted age differences from as few as two labeled images from\nthe domain to be adapted to. We experiment with a publicly available dataset\nwith age labels, dividing it into subsets based on the ethnicity labels, and\nevaluating the performance of our approach on the data from an ethnicity\ndifferent from the one that the model is trained on. Additionally, we impose a\nconstraint to preserve the sanity of the predictions with respect to relative\nand absolute ages, and another to ensure the smoothness of the predictions with\nrespect to the input. We experiment extensively and compare various domain\nadaptation approaches for the task of regression.",
    "published_date": "2021-04-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.01781v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.01634v1",
    "title": "Pareto Efficient Fairness in Supervised Learning: From Extraction to Tracing",
    "authors": [
      "Mohammad Mahdi Kamani",
      "Rana Forsati",
      "James Z. Wang",
      "Mehrdad Mahdavi"
    ],
    "author_ids": [],
    "abstract": "As algorithmic decision-making systems are becoming more pervasive, it is\ncrucial to ensure such systems do not become mechanisms of unfair\ndiscrimination on the basis of gender, race, ethnicity, religion, etc.\nMoreover, due to the inherent trade-off between fairness measures and accuracy,\nit is desirable to learn fairness-enhanced models without significantly\ncompromising the accuracy. In this paper, we propose Pareto efficient Fairness\n(PEF) as a suitable fairness notion for supervised learning, that can ensure\nthe optimal trade-off between overall loss and other fairness criteria. The\nproposed PEF notion is definition-agnostic, meaning that any well-defined\nnotion of fairness can be reduced to the PEF notion. To efficiently find a PEF\nclassifier, we cast the fairness-enhanced classification as a bilevel\noptimization problem and propose a gradient-based method that can guarantee the\nsolution belongs to the Pareto frontier with provable guarantees for convex and\nnon-convex objectives. We also generalize the proposed algorithmic solution to\nextract and trace arbitrary solutions from the Pareto frontier for a given\npreference over accuracy and fairness measures. This approach is generic and\ncan be generalized to any multicriteria optimization problem to trace points on\nthe Pareto frontier curve, which is interesting by its own right. We\nempirically demonstrate the effectiveness of the PEF solution and the extracted\nPareto frontier on real-world datasets compared to state-of-the-art methods.",
    "published_date": "2021-04-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.01634v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.01290v1",
    "title": "Measuring Linguistic Diversity During COVID-19",
    "authors": [
      "Jonathan Dunn",
      "Tom Coupe",
      "Benjamin Adams"
    ],
    "author_ids": [],
    "abstract": "Computational measures of linguistic diversity help us understand the\nlinguistic landscape using digital language data. The contribution of this\npaper is to calibrate measures of linguistic diversity using restrictions on\ninternational travel resulting from the COVID-19 pandemic. Previous work has\nmapped the distribution of languages using geo-referenced social media and web\ndata. The goal, however, has been to describe these corpora themselves rather\nthan to make inferences about underlying populations. This paper shows that a\ndifference-in-differences method based on the Herfindahl-Hirschman Index can\nidentify the bias in digital corpora that is introduced by non-local\npopulations. These methods tell us where significant changes have taken place\nand whether this leads to increased or decreased diversity. This is an\nimportant step in aligning digital corpora like social media with the\nreal-world populations that have produced them.",
    "published_date": "2021-04-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.01290v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.01173v1",
    "title": "Branch-and-cut algorithms for the covering salesman problem",
    "authors": [
      "Lucas Porto Maziero",
      "Fábio Luiz Usberti",
      "Celso Cavellucci"
    ],
    "author_ids": [],
    "abstract": "The Covering Salesman Problem (CSP) is a generalization of the Traveling\nSalesman Problem in which the tour is not required to visit all vertices, as\nlong as all vertices are covered by the tour. The objective of CSP is to find a\nminimum length Hamiltonian cycle over a subset of vertices that covers an\nundirected graph. In this paper, valid inequalities from the generalized\ntraveling salesman problem are applied to the CSP in addition to new valid\ninequalities that explore distinct aspects of the problem. A branch-and-cut\nframework assembles exact and heuristic separation routines for integer and\nfractional CSP solutions. Computational experiments show that the proposed\nframework outperformed methodologies from literature with respect to optimality\ngaps. Moreover, optimal solutions were proven for several previously unsolved\ninstances.",
    "published_date": "2021-04-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DS",
      "cs.CC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.01173v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.01131v2",
    "title": "Mining Trends of COVID-19 Vaccine Beliefs on Twitter with Lexical Embeddings",
    "authors": [
      "Harshita Chopra",
      "Aniket Vashishtha",
      "Ridam Pal",
      "Ashima",
      "Ananya Tyagi",
      "Tavpritesh Sethi"
    ],
    "author_ids": [],
    "abstract": "Social media plays a pivotal role in disseminating news globally and acts as\na platform for people to express their opinions on various topics. A wide\nvariety of views accompanies COVID-19 vaccination drives across the globe,\noften colored by emotions, which change along with rising cases, approval of\nvaccines, and multiple factors discussed online. This study aims at analyzing\nthe temporal evolution of different Emotion categories: Hesitation, Rage,\nSorrow, Anticipation, Faith, and Contentment with Influencing Factors: Vaccine\nRollout, Misinformation, Health Effects, and Inequities as lexical categories\ncreated from Tweets belonging to five countries with vital vaccine roll-out\nprograms, namely, India, United States of America, Brazil, United Kingdom, and\nAustralia. We extracted a corpus of nearly 1.8 million Twitter posts related to\nCOVID-19 vaccination. Using cosine distance from selected seed words, we\nexpanded the vocabulary of each category and tracked the longitudinal change in\ntheir strength from June 2020 to April 2021. We used community detection\nalgorithms to find modules in positive correlation networks. Our findings\nsuggest that tweets expressing hesitancy towards vaccines contain the highest\nmentions of health-related effects in all countries. Our results indicated that\nthe patterns of hesitancy were variable across geographies and can help us\nlearn targeted interventions. We also observed a significant change in the\nlinear trends of categories like hesitation and contentment before and after\napproval of vaccines. Negative emotions like rage and sorrow gained the highest\nimportance in the alluvial diagram. They formed a significant module with all\nthe influencing factors in April 2021, when India observed the second wave of\nCOVID-19 cases. The relationship between Emotions and Influencing Factors was\nfound to be variable across the countries.",
    "published_date": "2021-04-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.01131v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.01065v2",
    "title": "Fairness and Communication-Based Semantics for Session-Typed Languages",
    "authors": [
      "Ryan Kavanagh"
    ],
    "author_ids": [],
    "abstract": "We give communication-based semantics and reasoning techniques for Polarized\nSILL, a rich session-typed programming language with general recursion. Its\nfeatures include channel and code transmission, synchronous and asynchronous\ncommunication, and functional programming. Our contributions are distinguished\nby their faithfulness to the process abstraction, i.e., to the premise that\ncommunication is the only observable phenomenon of processes. We give the first\nobserved communication semantics that supports general recursion and code\ntransmission. Observed communication semantics define the meaning of processes\nin terms of their observed communications. We use this observational semantics\nto define experiments on processes, and we give a communication-based testing\nequivalences framework for defining observational simulations and equivalences\non processes. This framework captures several natural equivalences, and we show\nthat one of these coincides with barbed congruence, the canonical notion of\nprocess equivalence.\n  Polarized SILL is defined using a substructural operational semantics based\non multiset rewriting. To ensure that our contributions are well-defined in the\npresence of non-termination, we introduce fairness for multiset rewriting\nsystems. We construct a fair scheduler, we give sufficient conditions for\ntraces to be fair, and we study the effects of permutation on fair traces.",
    "published_date": "2021-04-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.PL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.01065v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.01061v3",
    "title": "Information Geometry and Classical Cramér-Rao Type Inequalities",
    "authors": [
      "Kumar Vijay Mishra",
      "M. Ashok Kumar"
    ],
    "author_ids": [],
    "abstract": "We examine the role of information geometry in the context of classical\nCram\\'er-Rao (CR) type inequalities. In particular, we focus on Eguchi's theory\nof obtaining dualistic geometric structures from a divergence function and then\napplying Amari-Nagoaka's theory to obtain a CR type inequality. The classical\ndeterministic CR inequality is derived from Kullback-Leibler (KL)-divergence.\nWe show that this framework could be generalized to other CR type inequalities\nthrough four examples: $\\alpha$-version of CR inequality, generalized CR\ninequality, Bayesian CR inequality, and Bayesian $\\alpha$-CR inequality. These\nare obtained from, respectively, $I_\\alpha$-divergence (or relative\n$\\alpha$-entropy), generalized Csisz\\'ar divergence, Bayesian KL divergence,\nand Bayesian $I_\\alpha$-divergence.",
    "published_date": "2021-04-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IT",
      "cs.LG",
      "eess.SP",
      "math.DG",
      "math.IT",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.01061v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.00959v1",
    "title": "Fairness in Network-Friendly Recommendations",
    "authors": [
      "Theodoros Giannakas",
      "Pavlos Sermpezis",
      "Anastasios Giovanidis",
      "Thrasyvoulos Spyropoulos",
      "George Arvanitakis"
    ],
    "author_ids": [],
    "abstract": "As mobile traffic is dominated by content services (e.g., video), which\ntypically use recommendation systems, the paradigm of network-friendly\nrecommendations (NFR) has been proposed recently to boost the network\nperformance by promoting content that can be efficiently delivered (e.g.,\ncached at the edge). NFR increase the network performance, however, at the cost\nof being unfair towards certain contents when compared to the standard\nrecommendations. This unfairness is a side effect of NFR that has not been\nstudied in literature. Nevertheless, retaining fairness among contents is a key\noperational requirement for content providers. This paper is the first to study\nthe fairness in NFR, and design fair-NFR. Specifically, we use a set of metrics\nthat capture different notions of fairness, and study the unfairness created by\nexisting NFR schemes. Our analysis reveals that NFR can be significantly\nunfair. We identify an inherent trade-off between the network gains achieved by\nNFR and the resulting unfairness, and derive bounds for this trade-off. We show\nthat existing NFR schemes frequently operate far from the bounds, i.e., there\nis room for improvement. To this end, we formulate the design of Fair-NFR\n(i.e., NFR with fairness guarantees compared to the baseline recommendations)\nas a linear optimization problem. Our results show that the Fair-NFR can\nachieve high network gains (similar to non-fair-NFR) with little unfairness.",
    "published_date": "2021-04-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.NI",
      "cs.MM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.00959v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.00941v1",
    "title": "Multi-Class Data Description for Out-of-distribution Detection",
    "authors": [
      "Dongha Lee",
      "Sehun Yu",
      "Hwanjo Yu"
    ],
    "author_ids": [],
    "abstract": "The capability of reliably detecting out-of-distribution samples is one of\nthe key factors in deploying a good classifier, as the test distribution always\ndoes not match with the training distribution in most real-world applications.\nIn this work, we present a deep multi-class data description, termed as\nDeep-MCDD, which is effective to detect out-of-distribution (OOD) samples as\nwell as classify in-distribution (ID) samples. Unlike the softmax classifier\nthat only focuses on the linear decision boundary partitioning its latent space\ninto multiple regions, our Deep-MCDD aims to find a spherical decision boundary\nfor each class which determines whether a test sample belongs to the class or\nnot. By integrating the concept of Gaussian discriminant analysis into deep\nneural networks, we propose a deep learning objective to learn\nclass-conditional distributions that are explicitly modeled as separable\nGaussian distributions. Thereby, we can define the confidence score by the\ndistance of a test sample from each class-conditional distribution, and utilize\nit for identifying OOD samples. Our empirical evaluation on multi-class tabular\nand image datasets demonstrates that Deep-MCDD achieves the best performances\nin distinguishing OOD samples while showing the classification accuracy as high\nas the other competitors.",
    "published_date": "2021-04-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.00941v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.00885v1",
    "title": "Adaptive Class Suppression Loss for Long-Tail Object Detection",
    "authors": [
      "Tong Wang",
      "Yousong Zhu",
      "Chaoyang Zhao",
      "Wei Zeng",
      "Jinqiao Wang",
      "Ming Tang"
    ],
    "author_ids": [],
    "abstract": "To address the problem of long-tail distribution for the large vocabulary\nobject detection task, existing methods usually divide the whole categories\ninto several groups and treat each group with different strategies. These\nmethods bring the following two problems. One is the training inconsistency\nbetween adjacent categories of similar sizes, and the other is that the learned\nmodel is lack of discrimination for tail categories which are semantically\nsimilar to some of the head categories. In this paper, we devise a novel\nAdaptive Class Suppression Loss (ACSL) to effectively tackle the above problems\nand improve the detection performance of tail categories. Specifically, we\nintroduce a statistic-free perspective to analyze the long-tail distribution,\nbreaking the limitation of manual grouping. According to this perspective, our\nACSL adjusts the suppression gradients for each sample of each class\nadaptively, ensuring the training consistency and boosting the discrimination\nfor rare categories. Extensive experiments on long-tail datasets LVIS and Open\nImages show that the our ACSL achieves 5.18% and 5.2% improvements with\nResNet50-FPN, and sets a new state of the art. Code and models are available at\nhttps://github.com/CASIA-IVA-Lab/ACSL.",
    "published_date": "2021-04-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.00885v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.00606v2",
    "title": "Model Selection's Disparate Impact in Real-World Deep Learning Applications",
    "authors": [
      "Jessica Zosa Forde",
      "A. Feder Cooper",
      "Kweku Kwegyir-Aggrey",
      "Chris De Sa",
      "Michael Littman"
    ],
    "author_ids": [],
    "abstract": "Algorithmic fairness has emphasized the role of biased data in automated\ndecision outcomes. Recently, there has been a shift in attention to sources of\nbias that implicate fairness in other stages in the ML pipeline. We contend\nthat one source of such bias, human preferences in model selection, remains\nunder-explored in terms of its role in disparate impact across demographic\ngroups. Using a deep learning model trained on real-world medical imaging data,\nwe verify our claim empirically and argue that choice of metric for model\ncomparison, especially those that do not take variability into account, can\nsignificantly bias model selection outcomes.",
    "published_date": "2021-04-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.00606v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.00552v2",
    "title": "Using Graph Theory to Derive Inequalities for the Bell Numbers",
    "authors": [
      "Alain Hertz",
      "Anaelle Hertz",
      "Hadrien Mélot"
    ],
    "author_ids": [],
    "abstract": "The Bell numbers count the number of different ways to partition a set of $n$\nelements while the graphical Bell numbers count the number of non-equivalent\npartitions of the vertex set of a graph into stable sets. This relation between\ngraph theory and integer sequences has motivated us to study properties on the\naverage number of colors in the non-equivalent colorings of a graph to discover\nnew non trivial inequalities for the Bell numbers. Example are given to\nillustrate our approach.",
    "published_date": "2021-04-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DM",
      "math.CO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.00552v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.00507v2",
    "title": "fairmodels: A Flexible Tool For Bias Detection, Visualization, And Mitigation",
    "authors": [
      "Jakub Wiśniewski",
      "Przemysław Biecek"
    ],
    "author_ids": [],
    "abstract": "Machine learning decision systems are getting omnipresent in our lives. From\ndating apps to rating loan seekers, algorithms affect both our well-being and\nfuture. Typically, however, these systems are not infallible. Moreover, complex\npredictive models are really eager to learn social biases present in historical\ndata that can lead to increasing discrimination. If we want to create models\nresponsibly then we need tools for in-depth validation of models also from the\nperspective of potential discrimination. This article introduces an R package\nfairmodels that helps to validate fairness and eliminate bias in classification\nmodels in an easy and flexible fashion. The fairmodels package offers a\nmodel-agnostic approach to bias detection, visualization and mitigation. The\nimplemented set of functions and fairness metrics enables model fairness\nvalidation from different perspectives. The package includes a series of\nmethods for bias mitigation that aim to diminish the discrimination in the\nmodel. The package is designed not only to examine a single model, but also to\nfacilitate comparisons between multiple models.",
    "published_date": "2021-04-01T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG",
      "cs.MS",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.00507v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.00466v1",
    "title": "Improving Calibration for Long-Tailed Recognition",
    "authors": [
      "Zhisheng Zhong",
      "Jiequan Cui",
      "Shu Liu",
      "Jiaya Jia"
    ],
    "author_ids": [],
    "abstract": "Deep neural networks may perform poorly when training datasets are heavily\nclass-imbalanced. Recently, two-stage methods decouple representation learning\nand classifier learning to improve performance. But there is still the vital\nissue of miscalibration. To address it, we design two methods to improve\ncalibration and performance in such scenarios. Motivated by the fact that\npredicted probability distributions of classes are highly related to the\nnumbers of class instances, we propose label-aware smoothing to deal with\ndifferent degrees of over-confidence for classes and improve classifier\nlearning. For dataset bias between these two stages due to different samplers,\nwe further propose shifted batch normalization in the decoupling framework. Our\nproposed methods set new records on multiple popular long-tailed recognition\nbenchmark datasets, including CIFAR-10-LT, CIFAR-100-LT, ImageNet-LT,\nPlaces-LT, and iNaturalist 2018. Code will be available at\nhttps://github.com/Jia-Research-Lab/MiSLAS.",
    "published_date": "2021-04-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.00466v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.00336v1",
    "title": "Mitigating Media Bias through Neutral Article Generation",
    "authors": [
      "Nayeon Lee",
      "Yejin Bang",
      "Andrea Madotto",
      "Pascale Fung"
    ],
    "author_ids": [],
    "abstract": "Media bias can lead to increased political polarization, and thus, the need\nfor automatic mitigation methods is growing. Existing mitigation work displays\narticles from multiple news outlets to provide diverse news coverage, but\nwithout neutralizing the bias inherent in each of the displayed articles.\nTherefore, we propose a new task, a single neutralized article generation out\nof multiple biased articles, to facilitate more efficient access to balanced\nand unbiased information. In this paper, we compile a new dataset NeuWS, define\nan automatic evaluation metric, and provide baselines and multiple analyses to\nserve as a solid starting point for the proposed task. Lastly, we obtain a\nhuman evaluation to demonstrate the alignment between our metric and human\njudgment.",
    "published_date": "2021-04-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.00336v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.00331v1",
    "title": "On the Convergence Time of Federated Learning Over Wireless Networks Under Imperfect CSI",
    "authors": [
      "Francesco Pase",
      "Marco Giordani",
      "Michele Zorzi"
    ],
    "author_ids": [],
    "abstract": "Federated learning (FL) has recently emerged as an attractive decentralized\nsolution for wireless networks to collaboratively train a shared model while\nkeeping data localized. As a general approach, existing FL methods tend to\nassume perfect knowledge of the Channel State Information (CSI) during the\ntraining phase, which may not be easy to acquire in case of fast fading\nchannels. Moreover, literature analyses either consider a fixed number of\nclients participating in the training of the federated model, or simply assume\nthat all clients operate at the maximum achievable rate to transmit model data.\nIn this paper, we fill these gaps by proposing a training process that takes\nchannel statistics as a bias to minimize the convergence time under imperfect\nCSI. Numerical experiments demonstrate that it is possible to reduce the\ntraining time by neglecting model updates from clients that cannot sustain a\nminimum predefined transmission rate. We also examine the trade-off between\nnumber of clients involved in the training process and model accuracy as a\nfunction of different fading regimes.",
    "published_date": "2021-04-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.00331v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.00170v4",
    "title": "Are Bias Mitigation Techniques for Deep Learning Effective?",
    "authors": [
      "Robik Shrestha",
      "Kushal Kafle",
      "Christopher Kanan"
    ],
    "author_ids": [],
    "abstract": "A critical problem in deep learning is that systems learn inappropriate\nbiases, resulting in their inability to perform well on minority groups. This\nhas led to the creation of multiple algorithms that endeavor to mitigate bias.\nHowever, it is not clear how effective these methods are. This is because study\nprotocols differ among papers, systems are tested on datasets that fail to test\nmany forms of bias, and systems have access to hidden knowledge or are tuned\nspecifically to the test set. To address this, we introduce an improved\nevaluation protocol, sensible metrics, and a new dataset, which enables us to\nask and answer critical questions about bias mitigation algorithms. We evaluate\nseven state-of-the-art algorithms using the same network architecture and\nhyperparameter selection policy across three benchmark datasets. We introduce a\nnew dataset called Biased MNIST that enables assessment of robustness to\nmultiple bias sources. We use Biased MNIST and a visual question answering\n(VQA) benchmark to assess robustness to hidden biases. Rather than only tuning\nto the test set distribution, we study robustness across different tuning\ndistributions, which is critical because for many applications the test\ndistribution may not be known during development. We find that algorithms\nexploit hidden biases, are unable to scale to multiple forms of bias, and are\nhighly sensitive to the choice of tuning set. Based on our findings, we implore\nthe community to adopt more rigorous assessment of future bias mitigation\nmethods. All data, code, and results are publicly available at:\nhttps://github.com/erobic/bias-mitigators.",
    "published_date": "2021-04-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.00170v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.00137v3",
    "title": "Achieving Transparency Report Privacy in Linear Time",
    "authors": [
      "Chien-Lun Chen",
      "Leana Golubchik",
      "Ranjan Pal"
    ],
    "author_ids": [],
    "abstract": "An accountable algorithmic transparency report (ATR) should ideally\ninvestigate the (a) transparency of the underlying algorithm, and (b) fairness\nof the algorithmic decisions, and at the same time preserve data subjects'\nprivacy. However, a provably formal study of the impact to data subjects'\nprivacy caused by the utility of releasing an ATR (that investigates\ntransparency and fairness), is yet to be addressed in the literature. The\nfar-fetched benefit of such a study lies in the methodical characterization of\nprivacy-utility trade-offs for release of ATRs in public, and their\nconsequential application-specific impact on the dimensions of society,\npolitics, and economics. In this paper, we first investigate and demonstrate\npotential privacy hazards brought on by the deployment of transparency and\nfairness measures in released ATRs. To preserve data subjects' privacy, we then\npropose a linear-time optimal-privacy scheme, built upon standard linear\nfractional programming (LFP) theory, for announcing ATRs, subject to\nconstraints controlling the tolerance of privacy perturbation on the utility of\ntransparency schemes. Subsequently, we quantify the privacy-utility trade-offs\ninduced by our scheme, and analyze the impact of privacy perturbation on\nfairness measures in ATRs. To the best of our knowledge, this is the first\nanalytical work that simultaneously addresses trade-offs between the triad of\nprivacy, utility, and fairness, applicable to algorithmic transparency reports.",
    "published_date": "2021-03-31T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.00137v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.00124v2",
    "title": "Misinformation detection in Luganda-English code-mixed social media text",
    "authors": [
      "Peter Nabende",
      "David Kabiito",
      "Claire Babirye",
      "Hewitt Tusiime",
      "Joyce Nakatumba-Nabende"
    ],
    "author_ids": [],
    "abstract": "The increasing occurrence, forms, and negative effects of misinformation on\nsocial media platforms has necessitated more misinformation detection tools.\nCurrently, work is being done addressing COVID-19 misinformation however, there\nare no misinformation detection tools for any of the 40 distinct indigenous\nUgandan languages. This paper addresses this gap by presenting basic language\nresources and a misinformation detection data set based on code-mixed\nLuganda-English messages sourced from the Facebook and Twitter social media\nplatforms. Several machine learning methods are applied on the misinformation\ndetection data set to develop classification models for detecting whether a\ncode-mixed Luganda-English message contains misinformation or not. A 10-fold\ncross validation evaluation of the classification methods in an experimental\nmisinformation detection task shows that a Discriminative Multinomial Naive\nBayes (DMNB) method achieves the highest accuracy and F-measure of 78.19% and\n77.90% respectively. Also, Support Vector Machine and Bagging ensemble\nclassification models achieve comparable results. These results are promising\nsince the machine learning models are based on n-gram features from only the\nmisinformation detection dataset.",
    "published_date": "2021-03-31T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.00124v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.00098v4",
    "title": "Balancing Fairness and Efficiency in Traffic Routing via Interpolated Traffic Assignment",
    "authors": [
      "Devansh Jalota",
      "Kiril Solovey",
      "Matthew Tsao",
      "Stephen Zoepf",
      "Marco Pavone"
    ],
    "author_ids": [],
    "abstract": "System optimum (SO) routing, wherein the total travel time of all users is\nminimized, is a holy grail for transportation authorities. However, SO routing\nmay discriminate against users who incur much larger travel times than others\nto achieve high system efficiency, i.e., low total travel times. To address the\ninherent unfairness of SO routing, we study the ${\\beta}$-fair SO problem whose\ngoal is to minimize the total travel time while guaranteeing a ${\\beta\\geq 1}$\nlevel of unfairness, which specifies the maximum possible ratio between the\ntravel times of different users with shared origins and destinations.\n  To obtain feasible solutions to the ${\\beta}$-fair SO problem while achieving\nhigh system efficiency, we develop a new convex program, the Interpolated\nTraffic Assignment Problem (I-TAP), which interpolates between a\nfairness-promoting and an efficiency-promoting traffic-assignment objective. We\nevaluate the efficacy of I-TAP through theoretical bounds on the total system\ntravel time and level of unfairness in terms of its interpolation parameter, as\nwell as present a numerical comparison between I-TAP and a state-of-the-art\nalgorithm on a range of transportation networks. The numerical results indicate\nthat our approach is faster by several orders of magnitude as compared to the\nbenchmark algorithm, while achieving higher system efficiency for all desirable\nlevels of unfairness. We further leverage the structure of I-TAP to develop two\npricing mechanisms to collectively enforce the I-TAP solution in the presence\nof selfish homogeneous and heterogeneous users, respectively, that\nindependently choose routes to minimize their own travel costs. We mention that\nthis is the first study of pricing in the context of fair routing for general\nroad networks (as opposed to, e.g., parallel road networks).",
    "published_date": "2021-03-31T00:00:00",
    "year": 2021,
    "categories": [
      "eess.SY",
      "cs.CY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.00098v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.00096v1",
    "title": "Taking Stock of the Present and Future of Smart Technologies for Older Adults and Caregivers",
    "authors": [
      "Christina N. Harrington",
      "Ben Jelen",
      "Amanda Lazar",
      "Aqueasha Martin-Hammond",
      "Alisha Pradhan",
      "Blaine Reeder",
      "Katie Siek"
    ],
    "author_ids": [],
    "abstract": "Technology has the opportunity to assist older adults as they age in place,\ncoordinate caregiving resources, and meet unmet needs through access to\nresources. Currently, older adults use consumer technologies to support\neveryday life, however these technologies are not always accessible or as\nuseful as they can be. Indeed, industry has attempted to create smart home\ntechnologies with older adults as a target user group, however these solutions\nare often more focused on the technical aspects and are short lived. In this\npaper, we advocate for older adults being involved in the design process - from\ninitial ideation to product development to deployment. We encourage federally\nfunded researchers and industry to create compensated, diverse older adult\nadvisory boards to address stereotypes about aging while ensuring their needs\nare considered.\n  We envision artificial intelligence systems that augment resources instead of\nreplacing them - especially in under-resourced communities. Older adults rely\non their caregiver networks and community organizations for social, emotional,\nand physical support; thus, AI should be used to coordinate resources better\nand lower the burden of connecting with these resources. Although\nsociotechnical smart systems can help identify needs of older adults, the lack\nof affordable research infrastructure and translation of findings into consumer\ntechnology perpetuates inequities in designing for diverse older adults. In\naddition, there is a disconnect between the creation of smart sensing systems\nand creating understandable, actionable data for older adults and caregivers to\nutilize. We ultimately advocate for a well-coordinated research effort across\nthe United States that connects older adults, caregivers, community\norganizations, and researchers together to catalyze innovative and practical\nresearch for all stakeholders.",
    "published_date": "2021-03-31T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.00096v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.17191v2",
    "title": "Modeling Users and Online Communities for Abuse Detection: A Position on Ethics and Explainability",
    "authors": [
      "Pushkar Mishra",
      "Helen Yannakoudakis",
      "Ekaterina Shutova"
    ],
    "author_ids": [],
    "abstract": "Abuse on the Internet is an important societal problem of our time. Millions\nof Internet users face harassment, racism, personal attacks, and other types of\nabuse across various platforms. The psychological effects of abuse on\nindividuals can be profound and lasting. Consequently, over the past few years,\nthere has been a substantial research effort towards automated abusive language\ndetection in the field of NLP. In this position paper, we discuss the role that\nmodeling of users and online communities plays in abuse detection.\nSpecifically, we review and analyze the state of the art methods that leverage\nuser or community information to enhance the understanding and detection of\nabusive language. We then explore the ethical challenges of incorporating user\nand community information, laying out considerations to guide future research.\nFinally, we address the topic of explainability in abusive language detection,\nproposing properties that an explainable method should aim to exhibit. We\ndescribe how user and community information can facilitate the realization of\nthese properties and discuss the effective operationalization of explainability\nin view of the properties.",
    "published_date": "2021-03-31T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.17191v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.17171v4",
    "title": "Spectral decoupling allows training transferable neural networks in medical imaging",
    "authors": [
      "Joona Pohjonen",
      "Carolin Stürenberg",
      "Antti Rannikko",
      "Tuomas Mirtti",
      "Esa Pitkänen"
    ],
    "author_ids": [],
    "abstract": "Many current neural networks for medical imaging generalise poorly to data\nunseen during training. Such behaviour can be caused by networks overfitting\neasy-to-learn, or statistically dominant, features while disregarding other\npotentially informative features. For example, indistinguishable differences in\nthe sharpness of the images from two different scanners can degrade the\nperformance of the network significantly. All neural networks intended for\nclinical practice need to be robust to variation in data caused by differences\nin imaging equipment, sample preparation and patient populations.\n  To address these challenges, we evaluate the utility of spectral decoupling\nas an implicit bias mitigation method. Spectral decoupling encourages the\nneural network to learn more features by simply regularising the networks'\nunnormalised prediction scores with an L2 penalty, thus having no added\ncomputational costs.\n  We show that spectral decoupling allows training neural networks on datasets\nwith strong spurious correlations and increases networks' robustness for data\ndistribution shifts. To validate our findings, we train networks with and\nwithout spectral decoupling to detect prostate cancer tissue slides and\nCOVID-19 in chest radiographs. Networks trained with spectral decoupling\nachieve up to 9.5 percent point higher performance on external datasets.\n  Our results show that spectral decoupling helps with generalisation issues\nassociated with neural networks, and can be used to complement or replace\ncomputationally expensive explicit bias mitigation methods, such as stain\nnormalization in histological images. We recommend using spectral decoupling as\nan implicit bias mitigation method in any neural network intended for clinical\nuse.",
    "published_date": "2021-03-31T00:00:00",
    "year": 2021,
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.17171v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.16971v1",
    "title": "A Transactive Energy Market Framework Considering Network Constraints and Fairness",
    "authors": [
      "Fredmar Asarias",
      "Michael Angelo Pedrasa"
    ],
    "author_ids": [],
    "abstract": "The continuous penetration of distributed energy resources (DER) in the\nelectric power grid is driving a new paradigm shift towards transactive energy\nsystem (TES), an active and more sustainable system characterized by\ndistributed generation and energy exchanges among consumers and producers in\nthe network. This transition, however, comes with challenges such as dealing\nwith the nonlinear and non-convex power flows of the system, determining an\noptimal transaction price to maximize overall system welfare, and ensuring\nfairness for all participants. In this paper, we propose a three-stage\ntransactive energy framework that aims to address these challenges. In the\nfirst stage, the cost without trading is calculated which will serve as the\nreference in the profit maximization problem in the next stage. DER dispatch,\npower flows and initial transaction payments/incentives of the participants\nwill then be determined in the second stage. A benefit allocation algorithm is\napplied in the third control stage to determine the optimal transaction price\nand final payments/incentives that will ensure fairness for trading\nparticipants. The proposed framework was tested in an IEEE 33-bus system and\nresults show that fair benefits are given for all participants during trading\nand the system operates within the network and economic constraints.",
    "published_date": "2021-03-31T00:00:00",
    "year": 2021,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.16971v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.16953v2",
    "title": "Mitigating Bias in Algorithmic Systems -- A Fish-Eye View",
    "authors": [
      "Kalia Orphanou",
      "Jahna Otterbacher",
      "Styliani Kleanthous",
      "Khuyagbaatar Batsuren",
      "Fausto Giunchiglia",
      "Veronika Bogina",
      "Avital Shulner Tal",
      "AlanHartman",
      "Tsvi Kuflik"
    ],
    "author_ids": [],
    "abstract": "Mitigating bias in algorithmic systems is a critical issue drawing attention\nacross communities within the information and computer sciences. Given the\ncomplexity of the problem and the involvement of multiple stakeholders --\nincluding developers, end-users, and third parties -- there is a need to\nunderstand the landscape of the sources of bias, and the solutions being\nproposed to address them, from a broad, cross-domain perspective. This survey\nprovides a \"fish-eye view,\" examining approaches across four areas of research.\nThe literature describes three steps toward a comprehensive treatment -- bias\ndetection, fairness management and explainability management -- and underscores\nthe need to work from within the system as well as from the perspective of\nstakeholders in the broader context.",
    "published_date": "2021-03-31T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "K.4.0; A.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.16953v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.16910v1",
    "title": "Trusted Artificial Intelligence: Towards Certification of Machine Learning Applications",
    "authors": [
      "Philip Matthias Winter",
      "Sebastian Eder",
      "Johannes Weissenböck",
      "Christoph Schwald",
      "Thomas Doms",
      "Tom Vogt",
      "Sepp Hochreiter",
      "Bernhard Nessler"
    ],
    "author_ids": [],
    "abstract": "Artificial Intelligence is one of the fastest growing technologies of the\n21st century and accompanies us in our daily lives when interacting with\ntechnical applications. However, reliance on such technical systems is crucial\nfor their widespread applicability and acceptance. The societal tools to\nexpress reliance are usually formalized by lawful regulations, i.e., standards,\nnorms, accreditations, and certificates. Therefore, the T\\\"UV AUSTRIA Group in\ncooperation with the Institute for Machine Learning at the Johannes Kepler\nUniversity Linz, proposes a certification process and an audit catalog for\nMachine Learning applications. We are convinced that our approach can serve as\nthe foundation for the certification of applications that use Machine Learning\nand Deep Learning, the techniques that drive the current revolution in\nArtificial Intelligence. While certain high-risk areas, such as fully\nautonomous robots in workspaces shared with humans, are still some time away\nfrom certification, we aim to cover low-risk applications with our\ncertification procedure. Our holistic approach attempts to analyze Machine\nLearning applications from multiple perspectives to evaluate and verify the\naspects of secure software development, functional requirements, data quality,\ndata protection, and ethics. Inspired by existing work, we introduce four\ncriticality levels to map the criticality of a Machine Learning application\nregarding the impact of its decisions on people, environment, and\norganizations. Currently, the audit catalog can be applied to low-risk\napplications within the scope of supervised learning as commonly encountered in\nindustry. Guided by field experience, scientific developments, and market\ndemands, the audit catalog will be extended and modified accordingly.",
    "published_date": "2021-03-31T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.CY",
      "cs.LG",
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.16910v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.16879v1",
    "title": "Optimal class assignment problem: a case study at Gunma University",
    "authors": [
      "Akifumi Kira",
      "Kiyohito Nagano",
      "Manabu Sugiyama",
      "Naoyuki Kamiyama"
    ],
    "author_ids": [],
    "abstract": "In this study, we consider the real-world problem of assigning students to\nclasses, where each student has a preference list, ranking a subset of classes\nin order of preference. Though we use existing approaches to include the daily\nclass assignment of Gunma University, new concepts and adjustments are required\nto find improved results depending on real instances in the field. Thus, we\npropose minimax-rank constrained maximum-utility matchings and a compromise\nbetween maximum-utility matchings and fair matchings, where a matching is said\nto be fair if it lexicographically minimizes the number of students assigned to\nclasses not included in their choices, the number of students assigned to their\nlast choices, and so on. In addition, we also observe the potential\ninefficiency of the student proposing deferred acceptance mechanism with single\ntie-breaking, which a hot topic in the literature on the school choice problem.",
    "published_date": "2021-03-31T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT",
      "econ.TH",
      "math.OC",
      "90C27, 91A80"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.16879v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.16848v2",
    "title": "Embracing Uncertainty: Decoupling and De-bias for Robust Temporal Grounding",
    "authors": [
      "Hao Zhou",
      "Chongyang Zhang",
      "Yan Luo",
      "Yanjun Chen",
      "Chuanping Hu"
    ],
    "author_ids": [],
    "abstract": "Temporal grounding aims to localize temporal boundaries within untrimmed\nvideos by language queries, but it faces the challenge of two types of\ninevitable human uncertainties: query uncertainty and label uncertainty. The\ntwo uncertainties stem from human subjectivity, leading to limited\ngeneralization ability of temporal grounding. In this work, we propose a novel\nDeNet (Decoupling and De-bias) to embrace human uncertainty: Decoupling - We\nexplicitly disentangle each query into a relation feature and a modified\nfeature. The relation feature, which is mainly based on skeleton-like words\n(including nouns and verbs), aims to extract basic and consistent information\nin the presence of query uncertainty. Meanwhile, modified feature assigned with\nstyle-like words (including adjectives, adverbs, etc) represents the subjective\ninformation, and thus brings personalized predictions; De-bias - We propose a\nde-bias mechanism to generate diverse predictions, aim to alleviate the bias\ncaused by single-style annotations in the presence of label uncertainty.\nMoreover, we put forward new multi-label metrics to diversify the performance\nevaluation. Extensive experiments show that our approach is more effective and\nrobust than state-of-the-arts on Charades-STA and ActivityNet Captions\ndatasets.",
    "published_date": "2021-03-31T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.16848v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.16814v1",
    "title": "Novel Outage-Aware NOMA Protocol for Secrecy Fairness Maximization Among Untrusted Users",
    "authors": [
      "Sapna Thapar",
      "Deepak Mishra",
      "Ravikant Saini"
    ],
    "author_ids": [],
    "abstract": "Observing the significance of spectrally-efficient secure non-orthogonal\nmultiple access (NOMA), this paper proposes a novel quality of service (QoS)\naware secure NOMA protocol that maximizes secrecy fairness among untrusted\nusers. Considering a base station (BS) and two users, a novel decoding order is\ndesigned that provides security to both users. With the objective of ensuring\nsecrecy fairness between users, while satisfying their QoS requirements under\nBS transmit power budget constraint, we explore the problem of minimizing the\nmaximum secrecy outage probability (SOP). Closed-form expression of pair outage\nprobability (POP) and optimal power allocation (PA) minimizing POP are\nobtained. To analyze secrecy performance, analytical expressions of SOP for\nboth users are derived, and individual SOP minimization problems are solved\nusing concept of generalized-convexity. High signal-to-noise ratio\napproximation of SOP and asymptotically optimized solution minimizing this\napproximation is also found. Furthermore, a global-optimal solution from\nsecrecy fairness standpoint is obtained at low computational complexity, and\ntight approximation is derived to get analytical insights. Numerical results\npresent useful insights on globally optimized PA which ensure secrecy fairness\nand provide performance gain of about 55.12%, 69.30%, and 19.11%, respectively,\ncompared to fixed PA and individual users' optimal PAs. Finally, a tradeoff\nbetween secrecy fairness performance and QoS demands is presented.",
    "published_date": "2021-03-31T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IT",
      "eess.SP",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.16814v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.16788v1",
    "title": "DER: Dynamically Expandable Representation for Class Incremental Learning",
    "authors": [
      "Shipeng Yan",
      "Jiangwei Xie",
      "Xuming He"
    ],
    "author_ids": [],
    "abstract": "We address the problem of class incremental learning, which is a core step\ntowards achieving adaptive vision intelligence. In particular, we consider the\ntask setting of incremental learning with limited memory and aim to achieve\nbetter stability-plasticity trade-off. To this end, we propose a novel\ntwo-stage learning approach that utilizes a dynamically expandable\nrepresentation for more effective incremental concept modeling. Specifically,\nat each incremental step, we freeze the previously learned representation and\naugment it with additional feature dimensions from a new learnable feature\nextractor. This enables us to integrate new visual concepts with retaining\nlearned knowledge. We dynamically expand the representation according to the\ncomplexity of novel concepts by introducing a channel-level mask-based pruning\nstrategy. Moreover, we introduce an auxiliary loss to encourage the model to\nlearn diverse and discriminate features for novel concepts. We conduct\nextensive experiments on the three class incremental learning benchmarks and\nour method consistently outperforms other methods with a large margin.",
    "published_date": "2021-03-31T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.16788v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.16785v1",
    "title": "Individually Fair Gradient Boosting",
    "authors": [
      "Alexander Vargo",
      "Fan Zhang",
      "Mikhail Yurochkin",
      "Yuekai Sun"
    ],
    "author_ids": [],
    "abstract": "We consider the task of enforcing individual fairness in gradient boosting.\nGradient boosting is a popular method for machine learning from tabular data,\nwhich arise often in applications where algorithmic fairness is a concern. At a\nhigh level, our approach is a functional gradient descent on a\n(distributionally) robust loss function that encodes our intuition of\nalgorithmic fairness for the ML task at hand. Unlike prior approaches to\nindividual fairness that only work with smooth ML models, our approach also\nworks with non-smooth models such as decision trees. We show that our algorithm\nconverges globally and generalizes. We also demonstrate the efficacy of our\nalgorithm on three ML problems susceptible to algorithmic bias.",
    "published_date": "2021-03-31T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.16785v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.04072v1",
    "title": "Towards a New Participatory Approach for Designing Artificial Intelligence and Data-Driven Technologies",
    "authors": [
      "Soaad Hossain",
      "Syed Ishtiaque Ahmed"
    ],
    "author_ids": [],
    "abstract": "With there being many technical and ethical issues with artificial\nintelligence (AI) that involve marginalized communities, there is a growing\ninterest for design methods used with marginalized people that may be\ntransferable to the design of AI technologies. Participatory design (PD) is a\ndesign method that is often used with marginalized communities for the design\nof social development, policy, IT and other matters and solutions. However,\nthere are issues with the current PD, raising concerns when it is applied to\nthe design of technologies, including AI technologies. This paper argues for\nthe use of PD for the design of AI technologies, and introduces and proposes a\nnew PD, which we call agile participatory design, that not only can could be\nused for the design of AI and data-driven technologies, but also overcomes\nissues surrounding current PD and its use in the design of such technologies.",
    "published_date": "2021-03-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "68Txx",
      "H.1.2; H.5; I.2; K.4; K.6.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.04072v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.16714v1",
    "title": "Statistical inference for individual fairness",
    "authors": [
      "Subha Maity",
      "Songkai Xue",
      "Mikhail Yurochkin",
      "Yuekai Sun"
    ],
    "author_ids": [],
    "abstract": "As we rely on machine learning (ML) models to make more consequential\ndecisions, the issue of ML models perpetuating or even exacerbating undesirable\nhistorical biases (e.g., gender and racial biases) has come to the fore of the\npublic's attention. In this paper, we focus on the problem of detecting\nviolations of individual fairness in ML models. We formalize the problem as\nmeasuring the susceptibility of ML models against a form of adversarial attack\nand develop a suite of inference tools for the adversarial cost function. The\ntools allow auditors to assess the individual fairness of ML models in a\nstatistically-principled way: form confidence intervals for the worst-case\nperformance differential between similar individuals and test hypotheses of\nmodel fairness with (asymptotic) non-coverage/Type I error rate control. We\ndemonstrate the utility of our tools in a real-world case study.",
    "published_date": "2021-03-30T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.16714v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.16685v1",
    "title": "Deep Learning in current Neuroimaging: a multivariate approach with power and type I error control but arguable generalization ability",
    "authors": [
      "Carmen Jiménez-Mesa",
      "Javier Ramírez",
      "John Suckling",
      "Jonathan Vöglein",
      "Johannes Levin",
      "Juan Manuel Górriz",
      "Alzheimer's Disease Neuroimaging Initiative ADNI",
      "Dominantly Inherited Alzheimer Network DIAN"
    ],
    "author_ids": [],
    "abstract": "Discriminative analysis in neuroimaging by means of deep/machine learning\ntechniques is usually tested with validation techniques, whereas the associated\nstatistical significance remains largely under-developed due to their\ncomputational complexity. In this work, a non-parametric framework is proposed\nthat estimates the statistical significance of classifications using deep\nlearning architectures. In particular, a combination of autoencoders (AE) and\nsupport vector machines (SVM) is applied to: (i) a one-condition, within-group\ndesigns often of normal controls (NC) and; (ii) a two-condition, between-group\ndesigns which contrast, for example, Alzheimer's disease (AD) patients with NC\n(the extension to multi-class analyses is also included). A random-effects\ninference based on a label permutation test is proposed in both studies using\ncross-validation (CV) and resubstitution with upper bound correction (RUB) as\nvalidation methods. This allows both false positives and classifier overfitting\nto be detected as well as estimating the statistical power of the test. Several\nexperiments were carried out using the Alzheimer's Disease Neuroimaging\nInitiative (ADNI) dataset, the Dominantly Inherited Alzheimer Network (DIAN)\ndataset, and a MCI prediction dataset. We found in the permutation test that CV\nand RUB methods offer a false positive rate close to the significance level and\nan acceptable statistical power (although lower using cross-validation). A\nlarge separation between training and test accuracies using CV was observed,\nespecially in one-condition designs. This implies a low generalization ability\nas the model fitted in training is not informative with respect to the test\nset. We propose as solution by applying RUB, whereby similar results are\nobtained to those of the CV test set, but considering the whole set and with a\nlower computational cost per iteration.",
    "published_date": "2021-03-30T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG",
      "eess.IV",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.16685v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.16463v1",
    "title": "Secrecy Fairness Aware NOMA for Untrusted Users",
    "authors": [
      "Sapna Thapar",
      "Deepak Mishra",
      "Ravikant Saini"
    ],
    "author_ids": [],
    "abstract": "Spectrally-efficient secure non-orthogonal multiple access (NOMA) has\nrecently attained a substantial research interest for fifth generation\ndevelopment. This work explores crucial security issue in NOMA which is stemmed\nfrom utilizing the decoding concept of successive interference cancellation.\nConsidering untrusted users, we design a novel secure NOMA transmission\nprotocol to maximize secrecy fairness among users. A new decoding order for two\nusers' NOMA is proposed that provides positive secrecy rate to both users.\nObserving the objective of maximizing secrecy fairness between users under\ngiven power budget constraint, the problem is formulated as minimizing the\nmaximum secrecy outage probability (SOP) between users. In particular,\nclosed-form expressions of SOP for both users are derived to analyze secrecy\nperformance. SOP minimization problems are solved using pseudoconvexity\nconcept, and optimized power allocation (PA) for each user is obtained.\nAsymptotic expressions of SOPs, and optimal PAs minimizing these approximations\nare obtained to get deeper insights. Further, globally-optimized power control\nsolution from secrecy fairness perspective is obtained at a low computational\ncomplexity and, asymptotic approximation is obtained to gain analytical\ninsights. Numerical results validate the correctness of analysis, and present\ninsights on optimal solutions. Finally, we present insights on global-optimal\nPA by which fairness is ensured and gains of about 55.12%, 69.30%, and 19.11%,\nrespectively are achieved, compared to fixed PA and individual users' optimal\nPAs.",
    "published_date": "2021-03-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IT",
      "eess.SP",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.16463v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.16129v1",
    "title": "Self-Guided and Cross-Guided Learning for Few-Shot Segmentation",
    "authors": [
      "Bingfeng Zhang",
      "Jimin Xiao",
      "Terry Qin"
    ],
    "author_ids": [],
    "abstract": "Few-shot segmentation has been attracting a lot of attention due to its\neffectiveness to segment unseen object classes with a few annotated samples.\nMost existing approaches use masked Global Average Pooling (GAP) to encode an\nannotated support image to a feature vector to facilitate query image\nsegmentation. However, this pipeline unavoidably loses some discriminative\ninformation due to the average operation. In this paper, we propose a simple\nbut effective self-guided learning approach, where the lost critical\ninformation is mined. Specifically, through making an initial prediction for\nthe annotated support image, the covered and uncovered foreground regions are\nencoded to the primary and auxiliary support vectors using masked GAP,\nrespectively. By aggregating both primary and auxiliary support vectors, better\nsegmentation performances are obtained on query images. Enlightened by our\nself-guided module for 1-shot segmentation, we propose a cross-guided module\nfor multiple shot segmentation, where the final mask is fused using predictions\nfrom multiple annotated samples with high-quality support vectors contributing\nmore and vice versa. This module improves the final prediction in the inference\nstage without re-training. Extensive experiments show that our approach\nachieves new state-of-the-art performances on both PASCAL-5i and COCO-20i\ndatasets.",
    "published_date": "2021-03-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.16129v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.16033v1",
    "title": "FaiR-IoT: Fairness-aware Human-in-the-Loop Reinforcement Learning for Harnessing Human Variability in Personalized IoT",
    "authors": [
      "Salma Elmalaki"
    ],
    "author_ids": [],
    "abstract": "Thanks to the rapid growth in wearable technologies, monitoring complex human\ncontext becomes feasible, paving the way to develop human-in-the-loop IoT\nsystems that naturally evolve to adapt to the human and environment state\nautonomously. Nevertheless, a central challenge in designing such personalized\nIoT applications arises from human variability. Such variability stems from the\nfact that different humans exhibit different behaviors when interacting with\nIoT applications (intra-human variability), the same human may change the\nbehavior over time when interacting with the same IoT application (inter-human\nvariability), and human behavior may be affected by the behaviors of other\npeople in the same environment (multi-human variability). To that end, we\npropose FaiR-IoT, a general reinforcement learning-based framework for adaptive\nand fairness-aware human-in-the-loop IoT applications. In FaiR-IoT, three\nlevels of reinforcement learning agents interact to continuously learn human\npreferences and maximize the system's performance and fairness while taking\ninto account the intra-, inter-, and multi-human variability. We validate the\nproposed framework on two applications, namely (i) Human-in-the-Loop Automotive\nAdvanced Driver Assistance Systems and (ii) Human-in-the-Loop Smart House.\nResults obtained on these two applications validate the generality of FaiR-IoT\nand its ability to provide a personalized experience while enhancing the\nsystem's performance by 40%-60% compared to non-personalized systems and\nenhancing the fairness of the multi-human systems by 1.5 orders of magnitude.",
    "published_date": "2021-03-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.16033v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.16002v1",
    "title": "AGQA: A Benchmark for Compositional Spatio-Temporal Reasoning",
    "authors": [
      "Madeleine Grunde-McLaughlin",
      "Ranjay Krishna",
      "Maneesh Agrawala"
    ],
    "author_ids": [],
    "abstract": "Visual events are a composition of temporal actions involving actors\nspatially interacting with objects. When developing computer vision models that\ncan reason about compositional spatio-temporal events, we need benchmarks that\ncan analyze progress and uncover shortcomings. Existing video question\nanswering benchmarks are useful, but they often conflate multiple sources of\nerror into one accuracy metric and have strong biases that models can exploit,\nmaking it difficult to pinpoint model weaknesses. We present Action Genome\nQuestion Answering (AGQA), a new benchmark for compositional spatio-temporal\nreasoning. AGQA contains $192M$ unbalanced question answer pairs for $9.6K$\nvideos. We also provide a balanced subset of $3.9M$ question answer pairs, $3$\norders of magnitude larger than existing benchmarks, that minimizes bias by\nbalancing the answer distributions and types of question structures. Although\nhuman evaluators marked $86.02\\%$ of our question-answer pairs as correct, the\nbest model achieves only $47.74\\%$ accuracy. In addition, AGQA introduces\nmultiple training/test splits to test for various reasoning abilities,\nincluding generalization to novel compositions, to indirect references, and to\nmore compositional steps. Using AGQA, we evaluate modern visual reasoning\nsystems, demonstrating that the best models barely perform better than\nnon-visual baselines exploiting linguistic biases and that none of the existing\nmodels generalize to novel compositions unseen during training.",
    "published_date": "2021-03-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.16002v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.15916v1",
    "title": "Robust Audio-Visual Instance Discrimination",
    "authors": [
      "Pedro Morgado",
      "Ishan Misra",
      "Nuno Vasconcelos"
    ],
    "author_ids": [],
    "abstract": "We present a self-supervised learning method to learn audio and video\nrepresentations. Prior work uses the natural correspondence between audio and\nvideo to define a standard cross-modal instance discrimination task, where a\nmodel is trained to match representations from the two modalities. However, the\nstandard approach introduces two sources of training noise. First, audio-visual\ncorrespondences often produce faulty positives since the audio and video\nsignals can be uninformative of each other. To limit the detrimental impact of\nfaulty positives, we optimize a weighted contrastive learning loss, which\ndown-weighs their contribution to the overall loss. Second, since\nself-supervised contrastive learning relies on random sampling of negative\ninstances, instances that are semantically similar to the base instance can be\nused as faulty negatives. To alleviate the impact of faulty negatives, we\npropose to optimize an instance discrimination loss with a soft target\ndistribution that estimates relationships between instances. We validate our\ncontributions through extensive experiments on action recognition tasks and\nshow that they address the problems of audio-visual instance discrimination and\nimprove transfer learning performance.",
    "published_date": "2021-03-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.15916v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.15746v1",
    "title": "Towards An Ethics-Audit Bot",
    "authors": [
      "Siani Pearson",
      "Martin Lloyd",
      "Vivek Nallur"
    ],
    "author_ids": [],
    "abstract": "In this paper we focus on artificial intelligence (AI) for governance, not\ngovernance for AI, and on just one aspect of governance, namely ethics audit.\nDifferent kinds of ethical audit bots are possible, but who makes the choices\nand what are the implications? In this paper, we do not provide\nethical/philosophical solutions, but rather focus on the technical aspects of\nwhat an AI-based solution for validating the ethical soundness of a target\nsystem would be like. We propose a system that is able to conduct an ethical\naudit of a target system, given certain socio-technical conditions. To be more\nspecific, we propose the creation of a bot that is able to support\norganisations in ensuring that their software development lifecycles contain\nprocesses that meet certain ethical standards.",
    "published_date": "2021-03-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "68T37",
      "I.2.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.15746v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.15739v1",
    "title": "Automation: An Essential Component Of Ethical AI?",
    "authors": [
      "Vivek Nallur",
      "Martin Lloyd",
      "Siani Pearson"
    ],
    "author_ids": [],
    "abstract": "Ethics is sometimes considered to be too abstract to be meaningfully\nimplemented in artificial intelligence (AI). In this paper, we reflect on other\naspects of computing that were previously considered to be very abstract. Yet,\nthese are now accepted as being done very well by computers. These tasks have\nranged from multiple aspects of software engineering to mathematics to\nconversation in natural language with humans. This was done by automating the\nsimplest possible step and then building on it to perform more complex tasks.\nWe wonder if ethical AI might be similarly achieved and advocate the process of\nautomation as key step in making AI take ethical decisions. The key\ncontribution of this paper is to reflect on how automation was introduced into\ndomains previously considered too abstract for computers.",
    "published_date": "2021-03-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "68T01",
      "I.2.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.15739v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.15622v1",
    "title": "Graph Classification by Mixture of Diverse Experts",
    "authors": [
      "Fenyu Hu",
      "Liping Wang",
      "Shu Wu",
      "Liang Wang",
      "Tieniu Tan"
    ],
    "author_ids": [],
    "abstract": "Graph classification is a challenging research problem in many applications\nacross a broad range of domains. In these applications, it is very common that\nclass distribution is imbalanced. Recently, Graph Neural Network (GNN) models\nhave achieved superior performance on various real-world datasets. Despite\ntheir success, most of current GNN models largely overlook the important\nsetting of imbalanced class distribution, which typically results in prediction\nbias towards majority classes. To alleviate the prediction bias, we propose to\nleverage semantic structure of dataset based on the distribution of node\nembedding. Specifically, we present GraphDIVE, a general framework leveraging\nmixture of diverse experts (i.e., graph classifiers) for imbalanced graph\nclassification. With a divide-and-conquer principle, GraphDIVE employs a gating\nnetwork to partition an imbalanced graph dataset into several subsets. Then\neach expert network is trained based on its corresponding subset. Experiments\non real-world imbalanced graph datasets demonstrate the effectiveness of\nGraphDIVE.",
    "published_date": "2021-03-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.15622v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.15620v1",
    "title": "Asymptotically Optimal Massey-Like Inequality on Guessing Entropy With Application to Side-Channel Attack Evaluations",
    "authors": [
      "Andrei Tănăsescu",
      "Marios O. Choudary",
      "Olivier Rioul",
      "Pantelimon George Popescu"
    ],
    "author_ids": [],
    "abstract": "A Massey-like inequality is any useful lower bound on guessing entropy in\nterms of the computationally scalable Shannon entropy. The asymptotically\noptimal Massey-like inequality is determined and further refined for\nfinite-support distributions. The impact of these results are highlighted for\nside-channel attack evaluation where guessing entropy is a key metric. In this\ncontext, the obtained bounds are compared to the state of the art.",
    "published_date": "2021-03-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IT",
      "cs.CR",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.15620v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.15237v2",
    "title": "Should College Dropout Prediction Models Include Protected Attributes?",
    "authors": [
      "Renzhe Yu",
      "Hansol Lee",
      "René F. Kizilcec"
    ],
    "author_ids": [],
    "abstract": "Early identification of college dropouts can provide tremendous value for\nimproving student success and institutional effectiveness, and predictive\nanalytics are increasingly used for this purpose. However, ethical concerns\nhave emerged about whether including protected attributes in the prediction\nmodels discriminates against underrepresented student groups and exacerbates\nexisting inequities. We examine this issue in the context of a large U.S.\nresearch university with both residential and fully online degree-seeking\nstudents. Based on comprehensive institutional records for this entire student\npopulation across multiple years, we build machine learning models to predict\nstudent dropout after one academic year of study, and compare the overall\nperformance and fairness of model predictions with or without four protected\nattributes (gender, URM, first-generation student, and high financial need). We\nfind that including protected attributes does not impact the overall prediction\nperformance and it only marginally improves algorithmic fairness of\npredictions. While these findings suggest that including protected attributes\nis preferred, our analysis also offers guidance on how to evaluate the impact\nin a local context, where institutional stakeholders seek to leverage\npredictive analytics to support student success.",
    "published_date": "2021-03-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.15237v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.15163v3",
    "title": "Countering Racial Bias in Computer Graphics Research",
    "authors": [
      "Theodore Kim",
      "Holly Rushmeier",
      "Julie Dorsey",
      "Derek Nowrouzezahrai",
      "Raqi Syed",
      "Wojciech Jarosz",
      "A. M. Darke"
    ],
    "author_ids": [],
    "abstract": "Current computer graphics research practices contain racial biases that have\nresulted in investigations into \"skin\" and \"hair\" that focus on the hegemonic\nvisual features of Europeans and East Asians. To broaden our research horizons\nto encompass all of humanity, we propose a variety of improvements to\nquantitative measures and qualitative practices, and pose novel, open research\nproblems.",
    "published_date": "2021-03-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.15163v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.15122v2",
    "title": "Quantifying Bias in Automatic Speech Recognition",
    "authors": [
      "Siyuan Feng",
      "Olya Kudina",
      "Bence Mark Halpern",
      "Odette Scharenborg"
    ],
    "author_ids": [],
    "abstract": "Automatic speech recognition (ASR) systems promise to deliver objective\ninterpretation of human speech. Practice and recent evidence suggests that the\nstate-of-the-art (SotA) ASRs struggle with the large variation in speech due to\ne.g., gender, age, speech impairment, race, and accents. Many factors can cause\nthe bias of an ASR system. Our overarching goal is to uncover bias in ASR\nsystems to work towards proactive bias mitigation in ASR. This paper is a first\nstep towards this goal and systematically quantifies the bias of a Dutch SotA\nASR system against gender, age, regional accents and non-native accents. Word\nerror rates are compared, and an in-depth phoneme-level error analysis is\nconducted to understand where bias is occurring. We primarily focus on bias due\nto articulation differences in the dataset. Based on our findings, we suggest\nbias mitigation strategies for ASR development.",
    "published_date": "2021-03-28T00:00:00",
    "year": 2021,
    "categories": [
      "eess.AS",
      "cs.CL",
      "cs.SD"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.15122v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.04069v2",
    "title": "A survey on extremism analysis using Natural Language Processing",
    "authors": [
      "Javier Torregrosa",
      "Gema Bello-Orgaz",
      "Eugenio Martinez-Camara",
      "Javier Del Ser",
      "David Camacho"
    ],
    "author_ids": [],
    "abstract": "Extremism research has grown as an open problem for several countries during\nrecent years, especially due to the apparition of movements such as jihadism.\nThis and other extremist groups have taken advantage of different approaches,\nsuch as the use of Social Media, to spread their ideology, promote their acts\nand recruit followers. Natural Language Processing (NLP) represents a way of\ndetecting this type of content, and several authors make use of it to describe\nand discriminate the discourse held by this groups, with the final objective of\ndetecting and preventing its spread. This survey aims to review the\ncontributions of NLP to the field of extremism research, providing the reader\nwith a comprehensive picture of the state of the art of this research area. The\ncontent includes a description and comparison of the frequently used NLP\ntechniques, how they were applied, the insights they provided, the most\nfrequently used NLP software tools and the availability of datasets and data\nsources for research. Finally, research questions are approached and answered\nwith highlights from the review, while future trends, challenges and directions\nderived from these highlights are suggested.",
    "published_date": "2021-03-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.04069v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.15072v1",
    "title": "A Survey on Ethical Hacking: Issues and Challenges",
    "authors": [
      "Jean-Paul A. Yaacoub",
      "Hassan N. Noura",
      "Ola Salman",
      "Ali Chehab"
    ],
    "author_ids": [],
    "abstract": "Security attacks are growing in an exponential manner and their impact on\nexisting systems is seriously high and can lead to dangerous consequences.\nHowever, in order to reduce the effect of these attacks, penetration tests are\nhighly required, and can be considered as a suitable solution for this task.\nTherefore, the main focus of this paper is to explain the technical and\nnon-technical steps of penetration tests. The objective of penetration tests is\nto make existing systems and their corresponding data more secure, efficient\nand resilient. In other terms, pen testing is a simulated attack with the goal\nof identifying any exploitable vulnerability or/and a security gap. In fact,\nany identified exploitable vulnerability will be used to conduct attacks on\nsystems, devices, or personnel. This growing problem should be solved and\nmitigated to reach better resistance against these attacks. Moreover, the\nadvantages and limitations of penetration tests are also listed. The main issue\nof penetration tests that it is efficient to detect known vulnerabilities.\nTherefore, in order to resist unknown vulnerabilities, a new kind of modern\npenetration tests is required, in addition to reinforcing the use of shadows\nhoneypots. This can also be done by reinforcing the anomaly detection of\nintrusion detection/prevention system. In fact, security is increased by\ndesigning an efficient cooperation between the different security elements and\npenetration tests.",
    "published_date": "2021-03-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.15072v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.14869v1",
    "title": "Instance segmentation with the number of clusters incorporated in embedding learning",
    "authors": [
      "Jianfeng Cao",
      "Hong Yan"
    ],
    "author_ids": [],
    "abstract": "Semantic and instance segmentation algorithms are two general yet distinct\nimage segmentation solutions powered by Convolution Neural Network. While\nsemantic segmentation benefits extensively from the end-to-end training\nstrategy, instance segmentation is frequently framed as a multi-stage task,\nsupported by learning-based discrimination and post-process clustering.\nIndependent optimizations on substages instigate the accumulation of\nsegmentation errors. In this work, we propose to embed prior clustering\ninformation into an embedding learning framework FCRNet, stimulating the\none-stage instance segmentation. FCRNet relieves the complexity of post process\nby incorporating the number of clustering groups into the embedding space. The\nsuperior performance of FCRNet is verified and compared with other methods on\nthe nucleus dataset BBBC006.",
    "published_date": "2021-03-27T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.14869v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.14713v2",
    "title": "Do the Rich Get Richer? Fairness Analysis for Blockchain Incentives",
    "authors": [
      "Yuming Huang",
      "Jing Tang",
      "Qianhao Cong",
      "Andrew Lim",
      "Jianliang Xu"
    ],
    "author_ids": [],
    "abstract": "Proof-of-Work (PoW) is the most widely adopted incentive model in current\nblockchain systems, which unfortunately is energy inefficient. Proof-of-Stake\n(PoS) is then proposed to tackle the energy issue. The rich-get-richer concern\nof PoS has been heavily debated in the blockchain community. The debate is\ncentered around the argument that whether rich miners possessing more stakes\nwill obtain higher staking rewards and further increase their potential income\nin the future. In this paper, we define two types of fairness, i.e.,\nexpectational fairness and robust fairness, that are useful for answering this\nquestion. In particular, expectational fairness illustrates that the expected\nincome of a miner is proportional to her initial investment, indicating that\nthe expected return on investment is a constant. To better capture the\nuncertainty of mining outcomes, robust fairness is proposed to characterize\nwhether the return on investment concentrates to a constant with high\nprobability as time evolves. Our analysis shows that the classical PoW\nmechanism can always preserve both types of fairness as long as the mining game\nruns for a sufficiently long time. Furthermore, we observe that current PoS\nblockchains implement various incentive models and discuss three\nrepresentatives, namely ML-PoS, SL-PoS and C-PoS. We find that (i) ML-PoS\n(e.g., Qtum and Blackcoin) preserves expectational fairness but may not achieve\nrobust fairness, (ii) SL-PoS (e.g., NXT) does not protect any type of fairness,\nand (iii) C-PoS (e.g., Ethereum 2.0) outperforms ML-PoS in terms of robust\nfairness while still maintaining expectational fairness. Finally, massive\nexperiments on real blockchain systems and extensive numerical simulations are\nperformed to validate our analysis.",
    "published_date": "2021-03-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CR",
      "cs.DB"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.14713v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.14548v1",
    "title": "Deep Unsupervised Learning for Generalized Assignment Problems: A Case-Study of User-Association in Wireless Networks",
    "authors": [
      "Arjun Kaushik",
      "Mehrazin Alizadeh",
      "Omer Waqar",
      "Hina Tabassum"
    ],
    "author_ids": [],
    "abstract": "There exists many resource allocation problems in the field of wireless\ncommunications which can be formulated as the generalized assignment problems\n(GAP). GAP is a generic form of linear sum assignment problem (LSAP) and is\nmore challenging to solve owing to the presence of both equality and inequality\nconstraints. We propose a novel deep unsupervised learning (DUL) approach to\nsolve GAP in a time-efficient manner. More specifically, we propose a new\napproach that facilitates to train a deep neural network (DNN) using a\ncustomized loss function. This customized loss function constitutes the\nobjective function and penalty terms corresponding to both equality and\ninequality constraints. Furthermore, we propose to employ a Softmax activation\nfunction at the output of DNN along with tensor splitting which simplifies the\ncustomized loss function and guarantees to meet the equality constraint. As a\ncase-study, we consider a typical user-association problem in a wireless\nnetwork, formulate it as GAP, and consequently solve it using our proposed DUL\napproach. Numerical results demonstrate that the proposed DUL approach provides\nnear-optimal results with significantly lower time-complexity.",
    "published_date": "2021-03-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "eess.SP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.14548v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.14518v3",
    "title": "A survey of numerical methods for hemivariational inequalities with applications to Contact Mechanics",
    "authors": [
      "Anna Ochal",
      "Michal Jureczka",
      "Piotr Bartman"
    ],
    "author_ids": [],
    "abstract": "In this paper we present an abstract nonsmooth optimization problem for which\nwe recall existence and uniqueness results. We show a numerical scheme to\napproximate its solution. The theory is later applied to a sample static\ncontact problem describing an elastic body in frictional contact with a\nfoundation. This problem leads to a hemivariational inequality which we solve\nnumerically. Finally, we compare three computational methods of solving contact\nmechanical problems: direct optimization method, augmented Lagrangian method\nand primal-dual active set strategy.",
    "published_date": "2021-03-26T00:00:00",
    "year": 2021,
    "categories": [
      "math.OC",
      "cs.NA",
      "math.NA",
      "35Q74, 49J40, 65K10, 65M60, 74S05, 74M15, 74M10, 74G15"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.14518v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.14278v2",
    "title": "Boundary Control of Traffic Congestion Modeled as a Non-stationary Stochastic Process",
    "authors": [
      "Xun Liu",
      "Hossein Rastgoftar"
    ],
    "author_ids": [],
    "abstract": "In this paper, we introduce a new conservation-based approach to model\ntraffic dynamics, and apply the model predictive control (MPC) approach to\ncontrol the boundary traffic inflow and outflow, so that the traffic congestion\nis reduced. We establish an interface between the Simulation of Urban Mobility\n(SUMO) software and MATLAB to define a network of interconnected roads (NOIR)\nas a directed graph, and present traffic congestion management as a network\ncontrol problem. By formally specifying the traffic feasibility conditions, and\nusing the linear temporal logic, we present the proposed MPC-based boundary\ncontrol problem as a quadratic programming with linear equality and inequality\nconstraints. The success of the proposed traffic boundary control is\ndemonstrated by simulation of traffic congestion control in Center City\nPhiladelphia.",
    "published_date": "2021-03-26T00:00:00",
    "year": 2021,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.14278v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.14152v2",
    "title": "Residual Energy-Based Models for End-to-End Speech Recognition",
    "authors": [
      "Qiujia Li",
      "Yu Zhang",
      "Bo Li",
      "Liangliang Cao",
      "Philip C. Woodland"
    ],
    "author_ids": [],
    "abstract": "End-to-end models with auto-regressive decoders have shown impressive results\nfor automatic speech recognition (ASR). These models formulate the\nsequence-level probability as a product of the conditional probabilities of all\nindividual tokens given their histories. However, the performance of locally\nnormalised models can be sub-optimal because of factors such as exposure bias.\nConsequently, the model distribution differs from the underlying data\ndistribution. In this paper, the residual energy-based model (R-EBM) is\nproposed to complement the auto-regressive ASR model to close the gap between\nthe two distributions. Meanwhile, R-EBMs can also be regarded as\nutterance-level confidence estimators, which may benefit many downstream tasks.\nExperiments on a 100hr LibriSpeech dataset show that R-EBMs can reduce the word\nerror rates (WERs) by 8.2%/6.7% while improving areas under precision-recall\ncurves of confidence scores by 12.6%/28.4% on test-clean/test-other sets.\nFurthermore, on a state-of-the-art model using self-supervised learning\n(wav2vec 2.0), R-EBMs still significantly improves both the WER and confidence\nestimation performance.",
    "published_date": "2021-03-25T00:00:00",
    "year": 2021,
    "categories": [
      "eess.AS",
      "cs.CL",
      "cs.LG",
      "cs.SD"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.14152v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.14066v3",
    "title": "Beyond permutation equivariance in graph networks",
    "authors": [
      "Emma Slade",
      "Francesco Farina"
    ],
    "author_ids": [],
    "abstract": "In this draft paper, we introduce a novel architecture for graph networks\nwhich is equivariant to the Euclidean group in $n$-dimensions. The model is\ndesigned to work with graph networks in their general form and can be shown to\ninclude particular variants as special cases. Thanks to its equivariance\nproperties, we expect the proposed model to be more data efficient with respect\nto classical graph architectures and also intrinsically equipped with a better\ninductive bias. We defer investigating this matter to future work.",
    "published_date": "2021-03-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.14066v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.14051v1",
    "title": "Tilted Cross Entropy (TCE): Promoting Fairness in Semantic Segmentation",
    "authors": [
      "Attila Szabo",
      "Hadi Jamali-Rad",
      "Siva-Datta Mannava"
    ],
    "author_ids": [],
    "abstract": "Traditional empirical risk minimization (ERM) for semantic segmentation can\ndisproportionately advantage or disadvantage certain target classes in favor of\nan (unfair but) improved overall performance. Inspired by the recently\nintroduced tilted ERM (TERM), we propose tilted cross-entropy (TCE) loss and\nadapt it to the semantic segmentation setting to minimize performance disparity\namong target classes and promote fairness. Through quantitative and qualitative\nperformance analyses, we demonstrate that the proposed Stochastic TCE for\nsemantic segmentation can efficiently improve the low-performing classes of\nCityscapes and ADE20k datasets trained with multi-class cross-entropy (MCCE),\nand also results in improved overall fairness.",
    "published_date": "2021-03-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.14051v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.14000v3",
    "title": "Fairness in Ranking: A Survey",
    "authors": [
      "Meike Zehlike",
      "Ke Yang",
      "Julia Stoyanovich"
    ],
    "author_ids": [],
    "abstract": "In the past few years, there has been much work on incorporating fairness\nrequirements into algorithmic rankers, with contributions coming from the data\nmanagement, algorithms, information retrieval, and recommender systems\ncommunities. In this survey we give a systematic overview of this work,\noffering a broad perspective that connects formalizations and algorithmic\napproaches across subfields. An important contribution of our work is in\ndeveloping a common narrative around the value frameworks that motivate\nspecific fairness-enhancing interventions in ranking. This allows us to unify\nthe presentation of mitigation objectives and of algorithmic techniques to help\nmeet those objectives or identify trade-offs. In this survey, we describe four\nclassification frameworks for fairness-enhancing interventions, along which we\nrelate the technical methods surveyed in this paper, discuss evaluation\ndatasets, and present technical work on fairness in score-based ranking. Then,\nwe present methods that incorporate fairness in supervised learning, and also\ngive representative examples of recent work on fairness in recommendation and\nmatchmaking systems. We also discuss evaluation frameworks for fair score-based\nranking and fair learning-to-rank, and draw a set of recommendations for the\nevaluation of fair ranking methods.",
    "published_date": "2021-03-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "cs.DB",
      "I.2.6; I.2.8; H.3.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.14000v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.14456v1",
    "title": "Authorship ethics: an overview of research on the state of practice",
    "authors": [
      "Nasir Mehmood Minhas"
    ],
    "author_ids": [],
    "abstract": "Authorship ethics is a central topic of discussion in research ethics fora.\nThere are various guidelines for authorship (i.e., naming and order). It is not\neasy to decide the authorship in the presence of varying authorship guidelines.\nThis paper gives an overview of research on authorship practices and issues. It\npresents a review of 16 empirical research papers published between 2014 --\n2020. The objective is to learn how various research disciplines handle\nauthorship. What are the authorship practices in various research disciplines,\nand what are the issues associated with these practices?",
    "published_date": "2021-03-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.14456v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.13954v1",
    "title": "Developing Apps for Researching the COVID-19 Pandemic with the TrackYourHealth Platform",
    "authors": [
      "Carsten Vogel",
      "Rüdiger Pryss",
      "Johannes Schobel",
      "Winfried Schlee",
      "Felix Beierle"
    ],
    "author_ids": [],
    "abstract": "Through lockdowns and other severe changes to daily life, almost everyone is\naffected by the COVID-19 pandemic. Scientists and medical doctors are - among\nothers - mainly interested in researching, monitoring, and improving physical\nand mental health of the general population. Mobile health apps (mHealth), and\napps conducting ecological momentary assessments (EMA) respectively, can help\nin this context. However, developing such mobile applications poses many\nchallenges like costly software development efforts, strict privacy rules,\ncompliance with ethical guidelines, local laws, and regulations. In this paper,\nwe present TrackYourHealth (TYH), a highly configurable, generic, and modular\nmobile data collection and EMA platform, which enabled us to develop and\nrelease two mobile multi-platform applications related to COVID-19 in just a\nfew weeks. We present TYH and highlight specific challenges researchers and\ndevelopers of similar apps may also face, especially when developing apps\nrelated to the medical field.",
    "published_date": "2021-03-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SE",
      "cs.CY",
      "D.2.13; J.3; J.4; H.4.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.13954v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.13868v1",
    "title": "Equality before the Law: Legal Judgment Consistency Analysis for Fairness",
    "authors": [
      "Yuzhong Wang",
      "Chaojun Xiao",
      "Shirong Ma",
      "Haoxi Zhong",
      "Cunchao Tu",
      "Tianyang Zhang",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "author_ids": [],
    "abstract": "In a legal system, judgment consistency is regarded as one of the most\nimportant manifestations of fairness. However, due to the complexity of factual\nelements that impact sentencing in real-world scenarios, few works have been\ndone on quantitatively measuring judgment consistency towards real-world data.\nIn this paper, we propose an evaluation metric for judgment inconsistency,\nLegal Inconsistency Coefficient (LInCo), which aims to evaluate inconsistency\nbetween data groups divided by specific features (e.g., gender, region, race).\nWe propose to simulate judges from different groups with legal judgment\nprediction (LJP) models and measure the judicial inconsistency with the\ndisagreement of the judgment results given by LJP models trained on different\ngroups. Experimental results on the synthetic data verify the effectiveness of\nLInCo. We further employ LInCo to explore the inconsistency in real cases and\ncome to the following observations: (1) Both regional and gender inconsistency\nexist in the legal system, but gender inconsistency is much less than regional\ninconsistency; (2) The level of regional inconsistency varies little across\ndifferent time periods; (3) In general, judicial inconsistency is negatively\ncorrelated with the severity of the criminal charges. Besides, we use LInCo to\nevaluate the performance of several de-bias methods, such as adversarial\nlearning, and find that these mechanisms can effectively help LJP models to\navoid suffering from data bias.",
    "published_date": "2021-03-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.13868v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.13850v1",
    "title": "Autism Spectrum Disorder Screening Using Discriminative Brain Sub-Networks: An Entropic Approach",
    "authors": [
      "Mohammad Amin",
      "Farshad Safaei"
    ],
    "author_ids": [],
    "abstract": "Autism is one of the most important neurological disorders which leads to\nproblems in a person's social interactions. Improvement of brain imaging\ntechnologies and techniques help us to build brain structural and functional\nnetworks. Finding networks topology pattern in each of the groups (autism and\nhealthy control) can aid us to achieve an autism disorder screening model. In\nthe present study, we have utilized the genetic algorithm to extract a\ndiscriminative sub-network that represents differences between two groups\nbetter. In the fitness evaluation phase, for each sub-network, a machine\nlearning model was trained using various entropy features of the sub-network\nand its performance was measured. Proper model performance implies extracting a\ngood discriminative sub-network. Network entropies can be used as network\ntopological descriptors. The evaluation results indicate the acceptable\nperformance of the proposed screening method based on extracted discriminative\nsub-networks and the machine learning models succeeded in obtaining a maximum\naccuracy of 73.1% in structural networks of the UCLA dataset, 82.2% in\nfunctional networks of the UCLA dataset, and 66.1% in functional networks of\nABIDE datasets.",
    "published_date": "2021-03-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.13850v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.13680v1",
    "title": "Decentralized Coordination Between Economic Dispatch and Demand Response in Multi-Energy Systems",
    "authors": [
      "Zishun Liu",
      "Shanying Zhu",
      "Jinming Xu",
      "Cailian Chen"
    ],
    "author_ids": [],
    "abstract": "In this paper, we investigate the problem of coordination between economic\ndispatch (ED) and demand response (DR) in multi-energy systems (MESs), aiming\nto improve the economic utility and reduce the waste of energy in MESs. Since\nmultiple energy sources are coupled through energy hubs (EHs), the\nsupply-demand constraints are nonconvex. To deal with this issue, we propose a\nlinearization method to transform the coordination problem to a convex social\nwelfare optimization one. Then a decentralized algorithm based on parallel\nAlternating Direction Method of Multipliers (ADMM) and dynamic average tracking\nprotocol is developed, where each agent could only make decisions based on\ninformation from their neighbors. Moreover, by using variational inequality and\nLyapunov-based techniques, we show that our algorithm could always converge to\nthe global optimal solution. Finally, a case study on the modified IEEE 14-bus\nnetwork verifies the feasibility and effectiveness of our algorithm.",
    "published_date": "2021-03-25T00:00:00",
    "year": 2021,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.13680v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.13553v2",
    "title": "The Role of Differentiation in Tolling of Traffic Networks with Mixed Autonomy",
    "authors": [
      "Daniel A. Lazar",
      "Ramtin Pedarsani"
    ],
    "author_ids": [],
    "abstract": "With autonomous vehicles now sharing roads with human drivers, the era of\nmixed autonomy brings new challenges in dealing with congestion. One cause of\ncongestion is when vehicle users choose their routes selfishly to minimize\ntheir personal travel delay rather than a global travel delay, and prior works\naddress this phenomenon using tolling to influence routing choices, but do not\naddress the setting of mixed autonomy. Tolls may be differentiated, meaning\ndifferent users of a road experience different tolls, or they may be anonymous;\nthe latter is desirable to allay concerns of fairness and privacy, as well as\nlogistical challenges. In this work we examine the role of differentiation in\ntraffic networks with mixed autonomy. Specifically, we first establish\ndifferentiated tolls which completely eliminate inefficiency due to selfish\nrouting. We then show the fundamental limitations of anonymous tolls in our\nsetting, and we provide anonymous tolls with mild performance guarantees. We\nshow that in parallel networks, an infinitesimal differentiation in tolls is\nenough to guarantee optimality, and finally we establish a lower bound on the\ninefficiency of variable marginal cost tolling in the mixed autonomy setting.",
    "published_date": "2021-03-25T00:00:00",
    "year": 2021,
    "categories": [
      "math.OC",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.13553v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.13509v3",
    "title": "A Variational Inequality Approach to Bayesian Regression Games",
    "authors": [
      "Wenshuo Guo",
      "Michael I. Jordan",
      "Tianyi Lin"
    ],
    "author_ids": [],
    "abstract": "Bayesian regression games are a special class of two-player general-sum\nBayesian games in which the learner is partially informed about the adversary's\nobjective through a Bayesian prior. This formulation captures the uncertainty\nin regard to the adversary, and is useful in problems where the learner and\nadversary may have conflicting, but not necessarily perfectly antagonistic\nobjectives. Although the Bayesian approach is a more general alternative to the\nstandard minimax formulation, the applications of Bayesian regression games\nhave been limited due to computational difficulties, and the existence and\nuniqueness of a Bayesian equilibrium are only known for quadratic cost\nfunctions. First, we prove the existence and uniqueness of a Bayesian\nequilibrium for a class of convex and smooth Bayesian games by regarding it as\na solution of an infinite-dimensional variational inequality (VI) in Hilbert\nspace. We consider two special cases in which the infinite-dimensional VI\nreduces to a high-dimensional VI or a nonconvex stochastic optimization, and\nprovide two simple algorithms of solving them with strong convergence\nguarantees. Numerical results on real datasets demonstrate the promise of this\napproach.",
    "published_date": "2021-03-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.13509v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.15298v1",
    "title": "US Fatal Police Shooting Analysis and Prediction",
    "authors": [
      "Yuan Wang",
      "Yangxin Fan"
    ],
    "author_ids": [],
    "abstract": "We believe that \"all men are created equal\". With the rise of the police\nshootings reported by media, more people in the U.S. think that police use\nexcessive force during law enforcement, especially to a specific group of\npeople. We want to apply multidimensional statistical analysis to reveal more\nfacts than the monotone mainstream media. Our paper has three parts. First, we\nproposed a new method to quantify fatal police shooting news reporting\ndeviation of mainstream media, which includes CNN, FOX, ABC, and NBC. Second,\nwe analyzed the most comprehensive US fatal police shooting dataset from\nWashington Post. We used FP-growth to reveal the frequent patterns and DBSCAN\nclustering to find fatal shooting hotspots. We brought multi-attributes (social\neconomics, demographics, political tendency, education, gun ownership rate,\npolice training hours, etc.) to reveal connections under the iceberg. We found\nthat the police shooting rate of a state depends on many variables. The top\nfour most relevant attributes were state joined year, state land area, gun\nownership rate, and violent crime rate. Third, we proposed four regression\nmodels to predict police shooting rates at the state level. The best model\nKstar could predict the fatal police shooting rate with about 88.53%\ncorrelation coefficient. We also proposed classification models, including\nGradient Boosting Machine, Multi-class Classifier, Logistic Regression, and\nNaive Bayes Classifier, to predict the race of fatal police shooting victims.\nOur classification models show no significant evidence to conclude that racial\ndiscrimination happened during fatal police shootings recorded by the WP\ndataset.",
    "published_date": "2021-03-24T00:00:00",
    "year": 2021,
    "categories": [
      "physics.soc-ph",
      "cs.LG",
      "cs.SI",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.15298v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.13455v1",
    "title": "Matched sample selection with GANs for mitigating attribute confounding",
    "authors": [
      "Chandan Singh",
      "Guha Balakrishnan",
      "Pietro Perona"
    ],
    "author_ids": [],
    "abstract": "Measuring biases of vision systems with respect to protected attributes like\ngender and age is critical as these systems gain widespread use in society.\nHowever, significant correlations between attributes in benchmark datasets make\nit difficult to separate algorithmic bias from dataset bias. To mitigate such\nattribute confounding during bias analysis, we propose a matching approach that\nselects a subset of images from the full dataset with balanced attribute\ndistributions across protected attributes. Our matching approach first projects\nreal images onto a generative adversarial network (GAN)'s latent space in a\nmanner that preserves semantic attributes. It then finds image matches in this\nlatent space across a chosen protected attribute, yielding a dataset where\nsemantic and perceptual attributes are balanced across the protected attribute.\nWe validate projection and matching strategies with qualitative, quantitative,\nand human annotation experiments. We demonstrate our work in the context of\ngender bias in multiple open-source facial-recognition classifiers and find\nthat bias persists after removing key confounders via matching. Code and\ndocumentation to reproduce the results here and apply the methods to new data\nis available at https://github.com/csinva/matching-with-gans .",
    "published_date": "2021-03-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.13455v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.13287v1",
    "title": "Human Factors in Security Research: Lessons Learned from 2008-2018",
    "authors": [
      "Mannat Kaur",
      "Michel van Eeten",
      "Marijn Janssen",
      "Kevin Borgolte",
      "Tobias Fiebig"
    ],
    "author_ids": [],
    "abstract": "Instead of only considering technology, computer security research now\nstrives to also take into account the human factor by studying regular users\nand, to a lesser extent, experts like operators and developers of systems. We\nfocus our analysis on the research on the crucial population of experts, whose\nhuman errors can impact many systems at once, and compare it to research on\nregular users. To understand how far we advanced in the area of human factors,\nhow the field can further mature, and to provide a point of reference for\nresearchers new to this field, we analyzed the past decade of human factors\nresearch in security and privacy, identifying 557 relevant publications. Of\nthese, we found 48 publications focused on expert users and analyzed all in\ndepth. For additional insights, we compare them to a stratified sample of 48\nend-user studies.\n  In this paper we investigate:\n  (i) The perspective on human factors, and how we can learn from safety\nscience (ii) How and who are the participants recruited, and how this -- as we\nfind -- creates a western-centric perspective (iii) Research objectives, and\nhow to align these with the chosen research methods (iv) How theories can be\nused to increase rigor in the communities scientific work, including\nlimitations to the use of Grounded Theory, which is often incompletely applied\n(v) How researchers handle ethical implications, and what we can do to account\nfor them more consistently\n  Although our literature review has limitations, new insights were revealed\nand avenues for further research identified.",
    "published_date": "2021-03-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.CR",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.13287v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.13145v1",
    "title": "Unsupervised collaborative learning using privileged information",
    "authors": [
      "Yohan Foucade",
      "Younès Bennani"
    ],
    "author_ids": [],
    "abstract": "In the collaborative clustering framework, the hope is that by combining\nseveral clustering solutions, each one with its own bias and imperfections, one\nwill get a better overall solution. The goal is that each local computation,\nquite possibly applied to distinct data sets, benefits from the work done by\nthe other collaborators. This article is dedicated to collaborative clustering\nbased on the Learning Using Privileged Information paradigm. Local algorithms\nweight incoming information at the level of each observation, depending on the\nconfidence level of the classification of that observation. A comparison\nbetween our algorithm and state of the art implementations shows improvement of\nthe collaboration process using the proposed approach.",
    "published_date": "2021-03-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.13145v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.13089v2",
    "title": "Single-Sample Prophet Inequalities Revisited",
    "authors": [
      "Constantine Caramanis",
      "Matthew Faw",
      "Orestis Papadigenopoulos",
      "Emmanouil Pountourakis"
    ],
    "author_ids": [],
    "abstract": "The study of the prophet inequality problem in the limited information regime\nwas initiated by Azar et al. [SODA'14] in the pursuit of prior-independent\nposted-price mechanisms. As they show, $O(1)$-competitive policies are\nachievable using only a single sample from the distribution of each agent. A\nnotable portion of their results relies on reducing the design of single-sample\nprophet inequalities (SSPIs) to that of order-oblivious secretary (OOS)\npolicies. The above reduction comes at the cost of not fully utilizing the\navailable samples. However, to date, this is essentially the only method for\nproving SSPIs for many combinatorial sets. Very recently, Rubinstein et al.\n[ITCS'20] give a surprisingly simple algorithm which achieves the optimal\ncompetitive ratio for the single-choice SSPI problem $-$ a result which is\nunobtainable going through the reduction to secretary problems. Motivated by\nthis discrepancy, we study the competitiveness of simple SSPI policies\ndirectly, without appealing to results from OOS literature. In this direction,\nwe first develop a framework for analyzing policies against a greedy-like\nprophet solution. Using this framework, we obtain the first SSPI for general\n(non-bipartite) matching environments, as well as improved competitive ratios\nfor transversal and truncated partition matroids. Second, motivated by the\nobservation that many OOS policies for matroids decompose the problem into\nindependent rank-$1$ instances, we provide a meta-theorem which applies to any\nmatroid satisfying this partition property. Leveraging the recent results by\nRubinstein et al., we obtain improved competitive guarantees (most by a factor\nof $2$) for a number of matroids captured by the reduction of Azar et al.\nFinally, we discuss applications of our SSPIs to the design of mechanisms for\nmulti-dimensional limited information settings with improved revenue and\nwelfare guarantees.",
    "published_date": "2021-03-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT",
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.13089v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.12838v4",
    "title": "Repairing Pronouns in Translation with BERT-Based Post-Editing",
    "authors": [
      "Reid Pryzant"
    ],
    "author_ids": [],
    "abstract": "Pronouns are important determinants of a text's meaning but difficult to\ntranslate. This is because pronoun choice can depend on entities described in\nprevious sentences, and in some languages pronouns may be dropped when the\nreferent is inferrable from the context. These issues can lead Neural Machine\nTranslation (NMT) systems to make critical errors on pronouns that impair\nintelligibility and even reinforce gender bias. We investigate the severity of\nthis pronoun issue, showing that (1) in some domains, pronoun choice can\naccount for more than half of a NMT systems' errors, and (2) pronouns have a\ndisproportionately large impact on perceived translation quality. We then\ninvestigate a possible solution: fine-tuning BERT on a pronoun prediction task\nusing chunks of source-side sentences, then using the resulting classifier to\nrepair the translations of an existing NMT model. We offer an initial case\nstudy of this approach for the Japanese-English language pair, observing that a\nsmall number of translations are significantly improved according to human\nevaluators.",
    "published_date": "2021-03-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.12838v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.12828v2",
    "title": "Learning to Optimize: A Primer and A Benchmark",
    "authors": [
      "Tianlong Chen",
      "Xiaohan Chen",
      "Wuyang Chen",
      "Howard Heaton",
      "Jialin Liu",
      "Zhangyang Wang",
      "Wotao Yin"
    ],
    "author_ids": [],
    "abstract": "Learning to optimize (L2O) is an emerging approach that leverages machine\nlearning to develop optimization methods, aiming at reducing the laborious\niterations of hand engineering. It automates the design of an optimization\nmethod based on its performance on a set of training problems. This data-driven\nprocedure generates methods that can efficiently solve problems similar to\nthose in the training. In sharp contrast, the typical and traditional designs\nof optimization methods are theory-driven, so they obtain performance\nguarantees over the classes of problems specified by the theory. The difference\nmakes L2O suitable for repeatedly solving a certain type of optimization\nproblems over a specific distribution of data, while it typically fails on\nout-of-distribution problems. The practicality of L2O depends on the type of\ntarget optimization, the chosen architecture of the method to learn, and the\ntraining procedure. This new paradigm has motivated a community of researchers\nto explore L2O and report their findings.\n  This article is poised to be the first comprehensive survey and benchmark of\nL2O for continuous optimization. We set up taxonomies, categorize existing\nworks and research directions, present insights, and identify open challenges.\nWe also benchmarked many existing L2O approaches on a few but representative\noptimization problems. For reproducible research and fair benchmarking\npurposes, we released our software implementation and data in the package\nOpen-L2O at https://github.com/VITA-Group/Open-L2O.",
    "published_date": "2021-03-23T00:00:00",
    "year": 2021,
    "categories": [
      "math.OC",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.12828v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.12715v2",
    "title": "Promoting Fairness through Hyperparameter Optimization",
    "authors": [
      "André F. Cruz",
      "Pedro Saleiro",
      "Catarina Belém",
      "Carlos Soares",
      "Pedro Bizarro"
    ],
    "author_ids": [],
    "abstract": "Considerable research effort has been guided towards algorithmic fairness but\nreal-world adoption of bias reduction techniques is still scarce. Existing\nmethods are either metric- or model-specific, require access to sensitive\nattributes at inference time, or carry high development or deployment costs.\nThis work explores the unfairness that emerges when optimizing ML models solely\nfor predictive performance, and how to mitigate it with a simple and easily\ndeployed intervention: fairness-aware hyperparameter optimization (HO). We\npropose and evaluate fairness-aware variants of three popular HO algorithms:\nFair Random Search, Fair TPE, and Fairband. We validate our approach on a\nreal-world bank account opening fraud case-study, as well as on three datasets\nfrom the fairness literature. Results show that, without extra training cost,\nit is feasible to find models with 111% mean fairness increase and just 6%\ndecrease in performance when compared with fairness-blind HO.",
    "published_date": "2021-03-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.12715v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.12679v4",
    "title": "The Road to a Successful HRI: AI, Trust and ethicS-TRAITS",
    "authors": [
      "Antonio Andriella",
      "Alessandra Rossi",
      "Silvia Rossi",
      "Anouk van Maris"
    ],
    "author_ids": [],
    "abstract": "The aim of this workshop is to give researchers from academia and industry\nthe possibility to discuss the inter-and multi-disciplinary nature of the\nrelationships between people and robots towards effective and long-lasting\ncollaborations. This workshop will provide a forum for the HRI and robotics\ncommunities to explore successful human-robot interaction (HRI) to analyse the\ndifferent aspects of HRI that impact its success. Particular focus are the AI\nalgorithms required to implement autonomous interactions, and the factors that\nenhance, undermine, or recover humans' trust in robots. Finally, potential\nethical and legal concerns, and how they can be addressed will be considered.\nWebsite: https://sites.google.com/view/traits-hri",
    "published_date": "2021-03-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.12679v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.13300v2",
    "title": "Automatic Cough Classification for Tuberculosis Screening in a Real-World Environment",
    "authors": [
      "Madhurananda Pahar",
      "Marisa Klopper",
      "Byron Reeve",
      "Grant Theron",
      "Rob Warren",
      "Thomas Niesler"
    ],
    "author_ids": [],
    "abstract": "Objective: The automatic discrimination between the coughing sounds produced\nby patients with tuberculosis (TB) and those produced by patients with other\nlung ailments.\n  Approach: We present experiments based on a dataset of 1358 forced cough\nrecordings obtained in a developing-world clinic from 16 patients with\nconfirmed active pulmonary TB and 35 patients suffering from respiratory\nconditions suggestive of TB but confirmed to be TB negative. Using nested\ncross-validation, we have trained and evaluated five machine learning\nclassifiers: logistic regression (LR), support vector machines (SVM), k-nearest\nneighbour (KNN), multilayer perceptrons (MLP) and convolutional neural networks\n(CNN).\n  Main Results: Although classification is possible in all cases, the best\nperformance is achieved using LR. In combination with feature selection by\nsequential forward selection (SFS), our best LR system achieves an area under\nthe ROC curve (AUC) of 0.94 using 23 features selected from a set of 78\nhigh-resolution mel-frequency cepstral coefficients (MFCCs). This system\nachieves a sensitivity of 93\\% at a specificity of 95\\% and thus exceeds the\n90\\% sensitivity at 70\\% specificity specification considered by the World\nHealth Organisation (WHO) as a minimal requirement for a community-based TB\ntriage test.\n  Significance: The automatic classification of cough audio sounds, when\napplied to symptomatic patients requiring investigation for TB, can meet the\nWHO triage specifications for the identification of patients who should undergo\nexpensive molecular downstream testing. This makes it a promising and viable\nmeans of low cost, easily deployable frontline screening for TB, which can\nbenefit especially developing countries with a heavy TB burden.",
    "published_date": "2021-03-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SD",
      "cs.LG",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.13300v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.12532v5",
    "title": "Balanced softmax cross-entropy for incremental learning with and without memory",
    "authors": [
      "Quentin Jodelet",
      "Xin Liu",
      "Tsuyoshi Murata"
    ],
    "author_ids": [],
    "abstract": "When incrementally trained on new classes, deep neural networks are subject\nto catastrophic forgetting which leads to an extreme deterioration of their\nperformance on the old classes while learning the new ones. Using a small\nmemory containing few samples from past classes has shown to be an effective\nmethod to mitigate catastrophic forgetting. However, due to the limited size of\nthe replay memory, there is a large imbalance between the number of samples for\nthe new and the old classes in the training dataset resulting in bias in the\nfinal model. To address this issue, we propose to use the Balanced Softmax\nCross-Entropy and show that it can be seamlessly combined with state-of-the-art\napproaches for class-incremental learning in order to improve their accuracy\nwhile also potentially decreasing the computational cost of the training\nprocedure. We further extend this approach to the more demanding\nclass-incremental learning without memory setting and achieve competitive\nresults with memory-based approaches. Experiments on the challenging ImageNet,\nImageNet-Subset and CIFAR100 benchmarks with various settings demonstrate the\nbenefits of our approach.",
    "published_date": "2021-03-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.12532v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.12491v1",
    "title": "Expanding Semantic Knowledge for Zero-shot Graph Embedding",
    "authors": [
      "Zheng Wang",
      "Ruihang Shao",
      "Changping Wang",
      "Changjun Hu",
      "Chaokun Wang",
      "Zhiguo Gong"
    ],
    "author_ids": [],
    "abstract": "Zero-shot graph embedding is a major challenge for supervised graph learning.\nAlthough a recent method RECT has shown promising performance, its working\nmechanisms are not clear and still needs lots of training data. In this paper,\nwe give deep insights into RECT, and address its fundamental limits. We show\nthat its core part is a GNN prototypical model in which a class prototype is\ndescribed by its mean feature vector. As such, RECT maps nodes from the\nraw-input feature space into an intermediate-level semantic space that connects\nthe raw-input features to both seen and unseen classes. This mechanism makes\nRECT work well on both seen and unseen classes, which however also reduces the\ndiscrimination. To realize its full potentials, we propose two label expansion\nstrategies. Specifically, besides expanding the labeled node set of seen\nclasses, we can also expand that of unseen classes. Experiments on real-world\ndatasets validate the superiority of our methods.",
    "published_date": "2021-03-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.12491v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.13885v3",
    "title": "Supervised Contrastive Replay: Revisiting the Nearest Class Mean Classifier in Online Class-Incremental Continual Learning",
    "authors": [
      "Zheda Mai",
      "Ruiwen Li",
      "Hyunwoo Kim",
      "Scott Sanner"
    ],
    "author_ids": [],
    "abstract": "Online class-incremental continual learning (CL) studies the problem of\nlearning new classes continually from an online non-stationary data stream,\nintending to adapt to new data while mitigating catastrophic forgetting. While\nmemory replay has shown promising results, the recency bias in online learning\ncaused by the commonly used Softmax classifier remains an unsolved challenge.\nAlthough the Nearest-Class-Mean (NCM) classifier is significantly undervalued\nin the CL community, we demonstrate that it is a simple yet effective\nsubstitute for the Softmax classifier. It addresses the recency bias and avoids\nstructural changes in the fully-connected layer for new classes. Moreover, we\nobserve considerable and consistent performance gains when replacing the\nSoftmax classifier with the NCM classifier for several state-of-the-art replay\nmethods. To leverage the NCM classifier more effectively, data embeddings\nbelonging to the same class should be clustered and well-separated from those\nwith a different class label. To this end, we contribute Supervised Contrastive\nReplay (SCR), which explicitly encourages samples from the same class to\ncluster tightly in embedding space while pushing those of different classes\nfurther apart during replay-based training. Overall, we observe that our\nproposed SCR substantially reduces catastrophic forgetting and outperforms\nstate-of-the-art CL methods by a significant margin on a variety of datasets.",
    "published_date": "2021-03-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.13885v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.12016v1",
    "title": "Fairness Perceptions of Algorithmic Decision-Making: A Systematic Review of the Empirical Literature",
    "authors": [
      "Christopher Starke",
      "Janine Baleis",
      "Birte Keller",
      "Frank Marcinkowski"
    ],
    "author_ids": [],
    "abstract": "Algorithmic decision-making (ADM) increasingly shapes people's daily lives.\nGiven that such autonomous systems can cause severe harm to individuals and\nsocial groups, fairness concerns have arisen. A human-centric approach demanded\nby scholars and policymakers requires taking people's fairness perceptions into\naccount when designing and implementing ADM. We provide a comprehensive,\nsystematic literature review synthesizing the existing empirical insights on\nperceptions of algorithmic fairness from 39 empirical studies spanning multiple\ndomains and scientific disciplines. Through thorough coding, we systemize the\ncurrent empirical literature along four dimensions: (a) algorithmic predictors,\n(b) human predictors, (c) comparative effects (human decision-making vs.\nalgorithmic decision-making), and (d) consequences of ADM. While we identify\nmuch heterogeneity around the theoretical concepts and empirical measurements\nof algorithmic fairness, the insights come almost exclusively from\nWestern-democratic contexts. By advocating for more interdisciplinary research\nadopting a society-in-the-loop framework, we hope our work will contribute to\nfairer and more responsible ADM.",
    "published_date": "2021-03-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.12016v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.13814v1",
    "title": "Dynamic Weighted Learning for Unsupervised Domain Adaptation",
    "authors": [
      "Ni Xiao",
      "Lei Zhang"
    ],
    "author_ids": [],
    "abstract": "Unsupervised domain adaptation (UDA) aims to improve the classification\nperformance on an unlabeled target domain by leveraging information from a\nfully labeled source domain. Recent approaches explore domain-invariant and\nclass-discriminant representations to tackle this task. These methods, however,\nignore the interaction between domain alignment learning and class\ndiscrimination learning. As a result, the missing or inadequate tradeoff\nbetween domain alignment and class discrimination are prone to the problem of\nnegative transfer. In this paper, we propose Dynamic Weighted Learning (DWL) to\navoid the discriminability vanishing problem caused by excessive alignment\nlearning and domain misalignment problem caused by excessive discriminant\nlearning. Technically, DWL dynamically weights the learning losses of alignment\nand discriminability by introducing the degree of alignment and\ndiscriminability. Besides, the problem of sample imbalance across domains is\nfirst considered in our work, and we solve the problem by weighing the samples\nto guarantee information balance across domains. Extensive experiments\ndemonstrate that DWL has an excellent performance in several benchmark\ndatasets.",
    "published_date": "2021-03-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.13814v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.11926v1",
    "title": "Differentiated nonblocking: a new progress condition and a matching queue algorithm",
    "authors": [
      "David Y. C. Chan",
      "Shucheng Chi",
      "Vassos Hadzilacos",
      "Sam Toueg"
    ],
    "author_ids": [],
    "abstract": "In this paper, we first propose a new liveness requirement for shared objects\nand data structures, we then give a shared queue algorithm that satisfies this\nrequirement and we prove its correctness. We also implement this algorithm and\ncompare it to a well-known shared queue algorithm that is used in practice. In\naddition to having a stronger worst-case progress guarantee, our experimental\nresults suggest that, at the cost of a marginal decrease in throughput, our\nalgorithm is significantly fairer, by a natural definition of fairness that we\nintroduce here.",
    "published_date": "2021-03-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.11926v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.11853v1",
    "title": "Studying Moral-based Differences in the Framing of Political Tweets",
    "authors": [
      "Markus Reiter-Haas",
      "Simone Kopeinik",
      "Elisabeth Lex"
    ],
    "author_ids": [],
    "abstract": "In this paper, we study the moral framing of political content on Twitter.\nSpecifically, we examine differences in moral framing in two datasets: (i)\ntweets from US-based politicians annotated with political affiliation and (ii)\nCOVID-19 related tweets in German from followers of the leaders of the five\nmajor Austrian political parties. Our research is based on recent work that\nintroduces an unsupervised approach to extract framing bias and intensity in\nnews using a dictionary of moral virtues and vices. In this paper, we use a\nmore extensive dictionary and adapt it to German-language tweets. Overall, in\nboth datasets, we observe a moral framing that is congruent with the public\nperception of the political parties. In the US dataset, democrats have a\ntendency to frame tweets in terms of care, while loyalty is a characteristic\nframe for republicans. In the Austrian dataset, we find that the followers of\nthe governing conservative party emphasize care, which is a key message and\nmoral frame in the party's COVID-19 campaign slogan. Our work complements\nexisting studies on moral framing in social media. Also, our empirical findings\nprovide novel insights into moral-based framing on COVID-19 in Austria.",
    "published_date": "2021-03-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.11853v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.11852v1",
    "title": "Detecting Racial Bias in Jury Selection",
    "authors": [
      "Jack Dunn",
      "Ying Daisy Zhuo"
    ],
    "author_ids": [],
    "abstract": "To support the 2019 U.S. Supreme Court case \"Flowers v. Mississippi\", APM\nReports collated historical court records to assess whether the State exhibited\na racial bias in striking potential jurors. This analysis used backward\nstepwise logistic regression to conclude that race was a significant factor,\nhowever this method for selecting relevant features is only a heuristic, and\nadditionally cannot consider interactions between features. We apply Optimal\nFeature Selection to identify the globally-optimal subset of features and\naffirm that there is significant evidence of racial bias in the strike\ndecisions. We also use Optimal Classification Trees to segment the juror\npopulation subgroups with similar characteristics and probability of being\nstruck, and find that three of these subgroups exhibit significant racial\ndisparity in strike rate, pinpointing specific areas of bias in the dataset.",
    "published_date": "2021-03-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.11852v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.11849v3",
    "title": "Almost (Weighted) Proportional Allocations for Indivisible Chores",
    "authors": [
      "Bo Li",
      "Yingkai Li",
      "Xiaowei Wu"
    ],
    "author_ids": [],
    "abstract": "In this paper, we study how to fairly allocate m indivisible chores to n\n(asymmetric) agents. We consider (weighted) proportionality up to any item\n(PROPX) and show that a (weighted) PROPX allocation always exists and can be\ncomputed efficiently. For chores, we argue that PROPX might be a more reliable\nrelaxation for proportionality by the facts that any PROPX allocation ensures\n2-approximation of maximin share (MMS) fairness [Budish, 2011] for symmetric\nagents and of anyprice share (APS) fairness [Babaioff et al, 2021] for\nasymmetric agents. APS allocations for chores have not been studied before the\ncurrent work, and our result implies a 2-approximation algorithm. Another\nby-product result is that an EFX and a weighted EF1 allocation for indivisible\nchores exist if all agents have the same ordinal preference, which might be of\nindependent interest. We then consider the partial information setting and\ndesign algorithms that only use agents' ordinal preferences to compute\napproximately PROPX allocations. Our algorithm achieves a 2-approximation for\nboth symmetric and asymmetric agents, and the approximation ratio is optimal.\nFinally, we study the price of fairness (PoF), i.e., the loss in social welfare\nby enforcing allocations to be (weighted) PROPX. We prove that the tight ratio\nfor PoF is Theta(n) for symmetric agents and unbounded for asymmetric agents.",
    "published_date": "2021-03-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.11849v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.11806v1",
    "title": "Tackling Racial Bias in Automated Online Hate Detection: Towards Fair and Accurate Classification of Hateful Online Users Using Geometric Deep Learning",
    "authors": [
      "Zo Ahmed",
      "Bertie Vidgen",
      "Scott A. Hale"
    ],
    "author_ids": [],
    "abstract": "Online hate is a growing concern on many social media platforms and other\nsites. To combat it, technology companies are increasingly identifying and\nsanctioning `hateful users' rather than simply moderating hateful content. Yet,\nmost research in online hate detection to date has focused on hateful content.\nThis paper examines how fairer and more accurate hateful user detection systems\ncan be developed by incorporating social network information through geometric\ndeep learning. Geometric deep learning dynamically learns information-rich\nnetwork representations and can generalise to unseen nodes. This is essential\nfor moving beyond manually engineered network features, which lack scalability\nand produce information-sparse network representations. This paper compares the\naccuracy of geometric deep learning with other techniques which either exclude\nnetwork information or incorporate it through manual feature engineering (e.g.,\nnode2vec). It also evaluates the fairness of these techniques using the\n`predictive equality' criteria, comparing the false positive rates on a subset\nof 136 African-American users with 4836 other users. Geometric deep learning\nproduces the most accurate and fairest classifier, with an AUC score of 90.8\\%\non the entire dataset and a false positive rate of zero among the\nAfrican-American subset for the best performing model. This highlights the\nbenefits of more effectively incorporating social network features in automated\nhateful user detection. Such an approach is also easily operationalized for\nreal-world content moderation as it has an efficient and scalable design.",
    "published_date": "2021-03-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.11806v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.11678v1",
    "title": "Feature Selection for Imbalanced Data with Deep Sparse Autoencoders Ensemble",
    "authors": [
      "Michela C. Massi",
      "Francesca Ieva",
      "Francesca Gasperoni",
      "Anna Maria Paganoni"
    ],
    "author_ids": [],
    "abstract": "Class imbalance is a common issue in many domain applications of learning\nalgorithms. Oftentimes, in the same domains it is much more relevant to\ncorrectly classify and profile minority class observations. This need can be\naddressed by Feature Selection (FS), that offers several further advantages,\ns.a. decreasing computational costs, aiding inference and interpretability.\nHowever, traditional FS techniques may become sub-optimal in the presence of\nstrongly imbalanced data. To achieve FS advantages in this setting, we propose\na filtering FS algorithm ranking feature importance on the basis of the\nReconstruction Error of a Deep Sparse AutoEncoders Ensemble (DSAEE). We use\neach DSAE trained only on majority class to reconstruct both classes. From the\nanalysis of the aggregated Reconstruction Error, we determine the features\nwhere the minority class presents a different distribution of values w.r.t. the\noverrepresented one, thus identifying the most relevant features to\ndiscriminate between the two. We empirically demonstrate the efficacy of our\nalgorithm in several experiments on high-dimensional datasets of varying sample\nsize, showcasing its capability to select relevant and generalizable features\nto profile and classify minority class, outperforming other benchmark FS\nmethods. We also briefly present a real application in radiogenomics, where the\nmethodology was applied successfully.",
    "published_date": "2021-03-22T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.11678v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.11661v2",
    "title": "Re-energizing Domain Discriminator with Sample Relabeling for Adversarial Domain Adaptation",
    "authors": [
      "Xin Jin",
      "Cuiling Lan",
      "Wenjun Zeng",
      "Zhibo Chen"
    ],
    "author_ids": [],
    "abstract": "Many unsupervised domain adaptation (UDA) methods exploit domain adversarial\ntraining to align the features to reduce domain gap, where a feature extractor\nis trained to fool a domain discriminator in order to have aligned feature\ndistributions. The discrimination capability of the domain classifier w.r.t the\nincreasingly aligned feature distributions deteriorates as training goes on,\nthus cannot effectively further drive the training of feature extractor. In\nthis work, we propose an efficient optimization strategy named Re-enforceable\nAdversarial Domain Adaptation (RADA) which aims to re-energize the domain\ndiscriminator during the training by using dynamic domain labels. Particularly,\nwe relabel the well aligned target domain samples as source domain samples on\nthe fly. Such relabeling makes the less separable distributions more separable,\nand thus leads to a more powerful domain classifier w.r.t. the new data\ndistributions, which in turn further drives feature alignment. Extensive\nexperiments on multiple UDA benchmarks demonstrate the effectiveness and\nsuperiority of our RADA.",
    "published_date": "2021-03-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.11661v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.11603v1",
    "title": "Alleviate Exposure Bias in Sequence Prediction \\\\ with Recurrent Neural Networks",
    "authors": [
      "Liping Yuan",
      "Jiangtao Feng",
      "Xiaoqing Zheng",
      "Xuanjing Huang"
    ],
    "author_ids": [],
    "abstract": "A popular strategy to train recurrent neural networks (RNNs), known as\n``teacher forcing'' takes the ground truth as input at each time step and makes\nthe later predictions partly conditioned on those inputs. Such training\nstrategy impairs their ability to learn rich distributions over entire\nsequences because the chosen inputs hinders the gradients back-propagating to\nall previous states in an end-to-end manner. We propose a fully differentiable\ntraining algorithm for RNNs to better capture long-term dependencies by\nrecovering the probability of the whole sequence. The key idea is that at each\ntime step, the network takes as input a ``bundle'' of similar words predicted\nat the previous step instead of a single ground truth. The representations of\nthese similar words forms a convex hull, which can be taken as a kind of\nregularization to the input. Smoothing the inputs by this way makes the whole\nprocess trainable and differentiable. This design makes it possible for the\nmodel to explore more feasible combinations (possibly unseen sequences), and\ncan be interpreted as a computationally efficient approximation to the beam\nsearch. Experiments on multiple sequence generation tasks yield performance\nimprovements, especially in sequence-level metrics, such as BLUE or ROUGE-2.",
    "published_date": "2021-03-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.11603v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.11469v2",
    "title": "Fairmandering: A column generation heuristic for fairness-optimized political districting",
    "authors": [
      "Wes Gurnee",
      "David B. Shmoys"
    ],
    "author_ids": [],
    "abstract": "The American winner-take-all congressional district system empowers\npoliticians to engineer electoral outcomes by manipulating district boundaries.\nExisting computational solutions mostly focus on drawing unbiased maps by\nignoring political and demographic input, and instead simply optimize for\ncompactness. We claim that this is a flawed approach because compactness and\nfairness are orthogonal qualities, and introduce a scalable two-stage method to\nexplicitly optimize for arbitrary piecewise-linear definitions of fairness. The\nfirst stage is a randomized divide-and-conquer column generation heuristic\nwhich produces an exponential number of distinct district plans by exploiting\nthe compositional structure of graph partitioning problems. This district\nensemble forms the input to a master selection problem to choose the districts\nto include in the final plan. Our decoupled design allows for unprecedented\nflexibility in defining fairness-aligned objective functions. The pipeline is\narbitrarily parallelizable, is flexible to support additional redistricting\nconstraints, and can be applied to a wide array of other regionalization\nproblems. In the largest ever ensemble study of congressional districts, we use\nour method to understand the range of possible expected outcomes and the\nimplications of this range on potential definitions of fairness.",
    "published_date": "2021-03-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.DM",
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.11469v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.11436v1",
    "title": "Responsible AI: Gender bias assessment in emotion recognition",
    "authors": [
      "Artem Domnich",
      "Gholamreza Anbarjafari"
    ],
    "author_ids": [],
    "abstract": "Rapid development of artificial intelligence (AI) systems amplify many\nconcerns in society. These AI algorithms inherit different biases from humans\ndue to mysterious operational flow and because of that it is becoming adverse\nin usage. As a result, researchers have started to address the issue by\ninvestigating deeper in the direction towards Responsible and Explainable AI.\nAmong variety of applications of AI, facial expression recognition might not be\nthe most important one, yet is considered as a valuable part of human-AI\ninteraction. Evolution of facial expression recognition from the feature based\nmethods to deep learning drastically improve quality of such algorithms. This\nresearch work aims to study a gender bias in deep learning methods for facial\nexpression recognition by investigating six distinct neural networks, training\nthem, and further analysed on the presence of bias, according to the three\ndefinition of fairness. The main outcomes show which models are gender biased,\nwhich are not and how gender of subject affects its emotion recognition. More\nbiased neural networks show bigger accuracy gap in emotion recognition between\nmale and female test sets. Furthermore, this trend keeps for true positive and\nfalse positive rates. In addition, due to the nature of the research, we can\nobserve which types of emotions are better classified for men and which for\nwomen. Since the topic of biases in facial expression recognition is not well\nstudied, a spectrum of continuation of this research is truly extensive, and\nmay comprise detail analysis of state-of-the-art methods, as well as targeting\nother biases.",
    "published_date": "2021-03-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.11436v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.11424v1",
    "title": "Deep Distribution-preserving Incomplete Clustering with Optimal Transport",
    "authors": [
      "Mingjie Luo",
      "Siwei Wang",
      "Xinwang Liu",
      "Wenxuan Tu",
      "Yi Zhang",
      "Xifeng Guo",
      "Sihang Zhou",
      "En Zhu"
    ],
    "author_ids": [],
    "abstract": "Clustering is a fundamental task in the computer vision and machine learning\ncommunity. Although various methods have been proposed, the performance of\nexisting approaches drops dramatically when handling incomplete\nhigh-dimensional data (which is common in real world applications). To solve\nthe problem, we propose a novel deep incomplete clustering method, named Deep\nDistribution-preserving Incomplete Clustering with Optimal Transport (DDIC-OT).\nTo avoid insufficient sample utilization in existing methods limited by few\nfully-observed samples, we propose to measure distribution distance with the\noptimal transport for reconstruction evaluation instead of traditional\npixel-wise loss function. Moreover, the clustering loss of the latent feature\nis introduced to regularize the embedding with more discrimination capability.\nAs a consequence, the network becomes more robust against missing features and\nthe unified framework which combines clustering and sample imputation enables\nthe two procedures to negotiate to better serve for each other. Extensive\nexperiments demonstrate that the proposed network achieves superior and stable\nclustering performance improvement against existing state-of-the-art incomplete\nclustering methods over different missing ratios.",
    "published_date": "2021-03-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.11424v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.11320v2",
    "title": "Lawyers are Dishonest? Quantifying Representational Harms in Commonsense Knowledge Resources",
    "authors": [
      "Ninareh Mehrabi",
      "Pei Zhou",
      "Fred Morstatter",
      "Jay Pujara",
      "Xiang Ren",
      "Aram Galstyan"
    ],
    "author_ids": [],
    "abstract": "Warning: this paper contains content that may be offensive or upsetting.\n  Numerous natural language processing models have tried injecting commonsense\nby using the ConceptNet knowledge base to improve performance on different\ntasks. ConceptNet, however, is mostly crowdsourced from humans and may reflect\nhuman biases such as \"lawyers are dishonest.\" It is important that these biases\nare not conflated with the notion of commonsense. We study this missing yet\nimportant problem by first defining and quantifying biases in ConceptNet as two\ntypes of representational harms: overgeneralization of polarized perceptions\nand representation disparity. We find that ConceptNet contains severe biases\nand disparities across four demographic categories. In addition, we analyze two\ndownstream models that use ConceptNet as a source for commonsense knowledge and\nfind the existence of biases in those models as well. We further propose a\nfiltered-based bias-mitigation approach and examine its effectiveness. We show\nthat our mitigation approach can reduce the issues in both resource and models\nbut leads to a performance drop, leaving room for future work to build fairer\nand stronger commonsense models.",
    "published_date": "2021-03-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.11320v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.02604v2",
    "title": "Using Molecular Embeddings in QSAR Modeling: Does it Make a Difference?",
    "authors": [
      "María Virginia Sabando",
      "Ignacio Ponzoni",
      "Evangelos E. Milios",
      "Axel J. Soto"
    ],
    "author_ids": [],
    "abstract": "With the consolidation of deep learning in drug discovery, several novel\nalgorithms for learning molecular representations have been proposed. Despite\nthe interest of the community in developing new methods for learning molecular\nembeddings and their theoretical benefits, comparing molecular embeddings with\neach other and with traditional representations is not straightforward, which\nin turn hinders the process of choosing a suitable representation for QSAR\nmodeling. A reason behind this issue is the difficulty of conducting a fair and\nthorough comparison of the different existing embedding approaches, which\nrequires numerous experiments on various datasets and training scenarios. To\nclose this gap, we reviewed the literature on methods for molecular embeddings\nand reproduced three unsupervised and two supervised molecular embedding\ntechniques recently proposed in the literature. We compared these five methods\nconcerning their performance in QSAR scenarios using different classification\nand regression datasets. We also compared these representations to traditional\nmolecular representations, namely molecular descriptors and fingerprints. As\nopposed to the expected outcome, our experimental setup consisting of over\n25,000 trained models and statistical tests revealed that the predictive\nperformance using molecular embeddings did not significantly surpass that of\ntraditional representations. While supervised embeddings yielded competitive\nresults compared to those using traditional molecular representations,\nunsupervised embeddings tended to perform worse than traditional\nrepresentations. Our results highlight the need for conducting a careful\ncomparison and analysis of the different embedding techniques prior to using\nthem in drug design tasks, and motivate a discussion about the potential of\nmolecular embeddings in computer-aided drug design.",
    "published_date": "2021-03-20T00:00:00",
    "year": 2021,
    "categories": [
      "q-bio.BM",
      "cs.LG",
      "q-bio.QM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.02604v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.13879v1",
    "title": "Examining mobility data justice during 2017 Hurricane Harvey",
    "authors": [
      "Hengfang Deng",
      "Qi Wang"
    ],
    "author_ids": [],
    "abstract": "Natural disasters can significantly disrupt human mobility in urban areas.\nStudies have attempted to understand and quantify such disruptions using\ncrowdsourced mobility data sets. However, limited research has studied the\njustice issues of mobility data in the context of natural disasters. The lack\nof research leaves us without an empirical foundation to quantify and control\nthe possible biases in the data. This study, using 2017 Hurricane Harvey as a\ncase study, explores three aspects of mobility data that could potentially\ncause injustice: representativeness, quality, and precision. We find\nrepresentativeness being a major factor contributing to mobility data\ninjustice. There is a persistent disparity of representativeness across\nneighborhoods of different socioeconomic characteristics before, during, and\nafter the hurricane's landfall. Additionally, we observed significant drops of\ndata precision during the hurricane, adding uncertainty to locate people and\nunderstand their movements during extreme weather events. The findings\nhighlight the necessity in understanding and controlling the possible bias of\nmobility data as well as developing practical tools through data justice lenses\nin collecting and analyzing data during disasters.",
    "published_date": "2021-03-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI",
      "physics.soc-ph",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.13879v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.11163v2",
    "title": "An Empirical Framework for Domain Generalization in Clinical Settings",
    "authors": [
      "Haoran Zhang",
      "Natalie Dullerud",
      "Laleh Seyyed-Kalantari",
      "Quaid Morris",
      "Shalmali Joshi",
      "Marzyeh Ghassemi"
    ],
    "author_ids": [],
    "abstract": "Clinical machine learning models experience significantly degraded\nperformance in datasets not seen during training, e.g., new hospitals or\npopulations. Recent developments in domain generalization offer a promising\nsolution to this problem by creating models that learn invariances across\nenvironments. In this work, we benchmark the performance of eight domain\ngeneralization methods on multi-site clinical time series and medical imaging\ndata. We introduce a framework to induce synthetic but realistic domain shifts\nand sampling bias to stress-test these methods over existing non-healthcare\nbenchmarks. We find that current domain generalization methods do not\nconsistently achieve significant gains in out-of-distribution performance over\nempirical risk minimization on real-world medical imaging data, in line with\nprior work on general imaging datasets. However, a subset of realistic\ninduced-shift scenarios in clinical time series data do exhibit limited\nperformance gains. We characterize these scenarios in detail, and recommend\nbest practices for domain generalization in the clinical setting.",
    "published_date": "2021-03-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.11163v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.11023v1",
    "title": "Individually Fair Ranking",
    "authors": [
      "Amanda Bower",
      "Hamid Eftekhari",
      "Mikhail Yurochkin",
      "Yuekai Sun"
    ],
    "author_ids": [],
    "abstract": "We develop an algorithm to train individually fair learning-to-rank (LTR)\nmodels. The proposed approach ensures items from minority groups appear\nalongside similar items from majority groups. This notion of fair ranking is\nbased on the definition of individual fairness from supervised learning and is\nmore nuanced than prior fair LTR approaches that simply ensure the ranking\nmodel provides underrepresented items with a basic level of exposure. The crux\nof our method is an optimal transport-based regularizer that enforces\nindividual fairness and an efficient algorithm for optimizing the regularizer.\nWe show that our approach leads to certifiably individually fair LTR models and\ndemonstrate the efficacy of our method on ranking tasks subject to demographic\nbiases.",
    "published_date": "2021-03-19T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.11023v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.11015v2",
    "title": "Video Class Agnostic Segmentation Benchmark for Autonomous Driving",
    "authors": [
      "Mennatullah Siam",
      "Alex Kendall",
      "Martin Jagersand"
    ],
    "author_ids": [],
    "abstract": "Semantic segmentation approaches are typically trained on large-scale data\nwith a closed finite set of known classes without considering unknown objects.\nIn certain safety-critical robotics applications, especially autonomous\ndriving, it is important to segment all objects, including those unknown at\ntraining time. We formalize the task of video class agnostic segmentation from\nmonocular video sequences in autonomous driving to account for unknown objects.\nVideo class agnostic segmentation can be formulated as an open-set or a motion\nsegmentation problem. We discuss both formulations and provide datasets and\nbenchmark different baseline approaches for both tracks. In the\nmotion-segmentation track we benchmark real-time joint panoptic and motion\ninstance segmentation, and evaluate the effect of ego-flow suppression. In the\nopen-set segmentation track we evaluate baseline methods that combine\nappearance, and geometry to learn prototypes per semantic class. We then\ncompare it to a model that uses an auxiliary contrastive loss to improve the\ndiscrimination between known and unknown objects. Datasets and models are\npublicly released at https://msiam.github.io/vca/.",
    "published_date": "2021-03-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.11015v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.10979v2",
    "title": "Social Media Polarization and Echo Chambers in the Context of COVID-19: Case Study",
    "authors": [
      "Julie Jiang",
      "Xiang Ren",
      "Emilio Ferrara"
    ],
    "author_ids": [],
    "abstract": "Background: Social media chatter in 2020 has been largely dominated by the\nCOVID-19 pandemic. Existing research shows that COVID-19 discourse is highly\npoliticized, with political preferences linked to beliefs and disbeliefs about\nthe virus. As it happens with topics that become politicized, people may fall\ninto echo chambers, which is the idea that one is only presented with\ninformation they already agree with, thereby reinforcing one's confirmation\nbias. Understanding the relationship between information dissemination and\npolitical preference is crucial for effective public health communication.\n  Objective: We aimed to study the extent of polarization and examine the\nstructure of echo chambers related to COVID-19 discourse on Twitter in the\nUnited States.\n  Methods: First, we presented Retweet-BERT, a scalable and highly accurate\nmodel for estimating user polarity by leveraging language features and network\nstructures. Then, by analyzing the user polarity predicted by Retweet-BERT, we\nprovided new insights into the characterization of partisan users.\n  Results: We observed that right-leaning users were noticeably more vocal and\nactive in the production and consumption of COVID-19 information. We also found\nthat most of the highly influential users were partisan, which may contribute\nto further polarization. Importantly, while echo chambers exist in both the\nright- and left-leaning communities, the right-leaning community was by far\nmore densely connected within their echo chamber and isolated from the rest.\n  Conclusions: We provided empirical evidence that political echo chambers are\nprevalent, especially in the right-leaning community, which can exacerbate the\nexposure to information in line with pre-existing users' views. Our findings\nhave broader implications in developing effective public health campaigns and\npromoting the circulation of factual information online.",
    "published_date": "2021-03-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.10979v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.10944v2",
    "title": "Emergence of Structural Inequalities in Scientific Citation Networks",
    "authors": [
      "Buddhika Nettasinghe",
      "Nazanin Alipourfard",
      "Vikram Krishnamurthy",
      "Kristina Lerman"
    ],
    "author_ids": [],
    "abstract": "Structural inequalities persist in society, conferring systematic advantages\nto some people at the expense of others, for example, by giving them\nsubstantially more influence and opportunities. Using bibliometric data about\nauthors of scientific publications, we identify two types of structural\ninequalities in scientific citations. First, female authors, who represent a\nminority of researchers, receive less recognition for their work (through\ncitations) relative to male authors; second, authors affiliated with top-ranked\ninstitutions, who are also a minority, receive substantially more recognition\ncompared to other authors. We present a model for the growth of directed\ncitation networks and show that citations disparities arise from individual\npreferences to cite authors from the same group (homophily), highly cited or\nactive authors (preferential attachment), as well as the size of the group and\nhow frequently new authors join. We analyze the model and show that its\npredictions align well with real-world observations. Our theoretical and\nempirical analysis also suggests potential strategies to mitigate structural\ninequalities in science. In particular, we find that merely increasing the\nminority group size does little to narrow the disparities. Instead, reducing\nthe homophily of each group, frequently adding new authors to a research field\nwhile providing them an accessible platform among existing, established\nauthors, together with balanced group sizes can have the largest impact on\nreducing inequality. Our work highlights additional complexities of mitigating\nstructural disparities stemming from asymmetric relations (e.g., directed\ncitations) compared to symmetric relations (e.g., collaborations).",
    "published_date": "2021-03-19T00:00:00",
    "year": 2021,
    "categories": [
      "physics.soc-ph",
      "cs.CY",
      "cs.DL",
      "cs.SI",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.10944v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.10769v2",
    "title": "Trustworthy Transparency by Design",
    "authors": [
      "Valentin Zieglmeier",
      "Alexander Pretschner"
    ],
    "author_ids": [],
    "abstract": "Individuals lack oversight over systems that process their data. This can\nlead to discrimination and hidden biases that are hard to uncover. Recent data\nprotection legislation tries to tackle these issues, but it is inadequate. It\ndoes not prevent data misusage while stifling sensible use cases for data. We\nthink the conflict between data protection and increasingly data-based systems\nshould be solved differently. When access to data is given, all usages should\nbe made transparent to the data subjects. This enables their data sovereignty,\nallowing individuals to benefit from sensible data usage while addressing\npotentially harmful consequences of data misusage. We contribute to this with a\ntechnical concept and an empirical evaluation. First, we conceptualize a\ntransparency framework for software design, incorporating research on user\ntrust and experience. Second, we instantiate and empirically evaluate the\nframework in a focus group study over three months, centering on the user\nperspective. Our transparency framework enables developing software that\nincorporates transparency in its design. The evaluation shows that it satisfies\nusability and trustworthiness requirements. The provided transparency is\nexperienced as beneficial and participants feel empowered by it. This shows\nthat our framework enables Trustworthy Transparency by Design.",
    "published_date": "2021-03-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SE",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.10769v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.10725v1",
    "title": "How Social Are Social Media The Dark Patterns In Facebook's Interface",
    "authors": [
      "Thomas Mildner",
      "Gian-Luca Savino"
    ],
    "author_ids": [],
    "abstract": "Many researchers have been concerned with social media and possible negative\nimpacts on the well-being of their audience. With the popularity of social\nnetworking sites (SNS) steadily increasing, psychological and social sciences\nhave shown great interest in their effects and consequences on humans.\nUnfortunately, it appears to be difficult to find correlations between SNS and\nthe results of their works. We, therefore, investigate Facebook using the tools\nof HCI to find connections between interface features and the concerns raised\nby these domains. With a nod towards Dark Patterns, we use an empirical design\nanalysis to identify interface interferences that impact users' online privacy.\nWe further discuss how HCI can help to work towards more ethical user\ninterfaces in the future.",
    "published_date": "2021-03-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.10725v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.10697v2",
    "title": "ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases",
    "authors": [
      "Stéphane d'Ascoli",
      "Hugo Touvron",
      "Matthew Leavitt",
      "Ari Morcos",
      "Giulio Biroli",
      "Levent Sagun"
    ],
    "author_ids": [],
    "abstract": "Convolutional architectures have proven extremely successful for vision\ntasks. Their hard inductive biases enable sample-efficient learning, but come\nat the cost of a potentially lower performance ceiling. Vision Transformers\n(ViTs) rely on more flexible self-attention layers, and have recently\noutperformed CNNs for image classification. However, they require costly\npre-training on large external datasets or distillation from pre-trained\nconvolutional networks. In this paper, we ask the following question: is it\npossible to combine the strengths of these two architectures while avoiding\ntheir respective limitations? To this end, we introduce gated positional\nself-attention (GPSA), a form of positional self-attention which can be\nequipped with a ``soft\" convolutional inductive bias. We initialise the GPSA\nlayers to mimic the locality of convolutional layers, then give each attention\nhead the freedom to escape locality by adjusting a gating parameter regulating\nthe attention paid to position versus content information. The resulting\nconvolutional-like ViT architecture, ConViT, outperforms the DeiT on ImageNet,\nwhile offering a much improved sample efficiency. We further investigate the\nrole of locality in learning by first quantifying how it is encouraged in\nvanilla self-attention layers, then analysing how it is escaped in GPSA layers.\nWe conclude by presenting various ablations to better understand the success of\nthe ConViT. Our code and models are released publicly at\nhttps://github.com/facebookresearch/convit.",
    "published_date": "2021-03-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.10697v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.10626v2",
    "title": "Cluster-to-Conquer: A Framework for End-to-End Multi-Instance Learning for Whole Slide Image Classification",
    "authors": [
      "Yash Sharma",
      "Aman Shrivastava",
      "Lubaina Ehsan",
      "Christopher A. Moskaluk",
      "Sana Syed",
      "Donald E. Brown"
    ],
    "author_ids": [],
    "abstract": "In recent years, the availability of digitized Whole Slide Images (WSIs) has\nenabled the use of deep learning-based computer vision techniques for automated\ndisease diagnosis. However, WSIs present unique computational and algorithmic\nchallenges. WSIs are gigapixel-sized ($\\sim$100K pixels), making them\ninfeasible to be used directly for training deep neural networks. Also, often\nonly slide-level labels are available for training as detailed annotations are\ntedious and can be time-consuming for experts. Approaches using\nmultiple-instance learning (MIL) frameworks have been shown to overcome these\nchallenges. Current state-of-the-art approaches divide the learning framework\ninto two decoupled parts: a convolutional neural network (CNN) for encoding the\npatches followed by an independent aggregation approach for slide-level\nprediction. In this approach, the aggregation step has no bearing on the\nrepresentations learned by the CNN encoder. We have proposed an end-to-end\nframework that clusters the patches from a WSI into ${k}$-groups, samples\n${k}'$ patches from each group for training, and uses an adaptive attention\nmechanism for slide level prediction; Cluster-to-Conquer (C2C). We have\ndemonstrated that dividing a WSI into clusters can improve the model training\nby exposing it to diverse discriminative features extracted from the patches.\nWe regularized the clustering mechanism by introducing a KL-divergence loss\nbetween the attention weights of patches in a cluster and the uniform\ndistribution. The framework is optimized end-to-end on slide-level\ncross-entropy, patch-level cross-entropy, and KL-divergence loss\n(Implementation: https://github.com/YashSharma/C2C).",
    "published_date": "2021-03-19T00:00:00",
    "year": 2021,
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.10626v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.10585v1",
    "title": "The evolving ecosystem of COVID-19 contact tracing applications",
    "authors": [
      "Benjamin Levy",
      "Matthew Stewart"
    ],
    "author_ids": [],
    "abstract": "Since the outbreak of the novel coronavirus, COVID-19, there has been\nincreased interest in the use of digital contact tracing as a means of stopping\nchains of viral transmission, provoking alarm from privacy advocates.\nConcerning the ethics of this technology, recent studies have predominantly\nfocused on (1) the formation of guidelines for ethical contact tracing, (2) the\nanalysis of specific implementations, or (3) the review of a select number of\ncontact tracing applications and their relevant privacy or ethical\nimplications. In this study, we provide a comprehensive survey of the evolving\necosystem of COVID-19 tracing applications, examining 152 contact tracing\napplications and assessing the extent to which they comply with existing\nguidelines for ethical contact tracing. The assessed criteria cover areas\nincluding data collection and storage, transparency and consent, and whether\nthe implementation is open source. We find that although many apps released\nearly in the pandemic fell short of best practices, apps released more\nrecently, following the publication of the Apple/Google exposure notification\nprotocol, have tended to be more closely aligned with ethical contact tracing\nprinciples. This dataset will be publicly available and may be updated as the\npandemic continues.",
    "published_date": "2021-03-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.CR",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.10585v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.10550v1",
    "title": "Gender and Racial Fairness in Depression Research using Social Media",
    "authors": [
      "Carlos Aguirre",
      "Keith Harrigian",
      "Mark Dredze"
    ],
    "author_ids": [],
    "abstract": "Multiple studies have demonstrated that behavior on internet-based social\nmedia platforms can be indicative of an individual's mental health status. The\nwidespread availability of such data has spurred interest in mental health\nresearch from a computational lens. While previous research has raised concerns\nabout possible biases in models produced from this data, no study has\nquantified how these biases actually manifest themselves with respect to\ndifferent demographic groups, such as gender and racial/ethnic groups. Here, we\nanalyze the fairness of depression classifiers trained on Twitter data with\nrespect to gender and racial demographic groups. We find that model performance\nsystematically differs for underrepresented groups and that these discrepancies\ncannot be fully explained by trivial data representation issues. Our study\nconcludes with recommendations on how to avoid these biases in future research.",
    "published_date": "2021-03-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.10550v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.10510v2",
    "title": "Hidden Technical Debts for Fair Machine Learning in Financial Services",
    "authors": [
      "Chong Huang",
      "Arash Nourian",
      "Kevin Griest"
    ],
    "author_ids": [],
    "abstract": "The recent advancements in machine learning (ML) have demonstrated the\npotential for providing a powerful solution to build complex prediction systems\nin a short time. However, in highly regulated industries, such as the financial\ntechnology (Fintech), people have raised concerns about the risk of ML systems\ndiscriminating against specific protected groups or individuals. To address\nthese concerns, researchers have introduced various mathematical fairness\nmetrics and bias mitigation algorithms. This paper discusses hidden technical\ndebts and challenges of building fair ML systems in a production environment\nfor Fintech. We explore various stages that require attention for fairness in\nthe ML system development and deployment life cycle. To identify hidden\ntechnical debts that exist in building fair ML system for Fintech, we focus on\nkey pipeline stages including data preparation, model development, system\nmonitoring and integration in production. Our analysis shows that enforcing\nfairness for production-ready ML systems in Fintech requires specific\nengineering commitments at different stages of ML system life cycle. We also\npropose several initial starting points to mitigate these technical debts for\ndeploying fair ML systems in production.",
    "published_date": "2021-03-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.10510v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.10427v4",
    "title": "The Low-Rank Simplicity Bias in Deep Networks",
    "authors": [
      "Minyoung Huh",
      "Hossein Mobahi",
      "Richard Zhang",
      "Brian Cheung",
      "Pulkit Agrawal",
      "Phillip Isola"
    ],
    "author_ids": [],
    "abstract": "Modern deep neural networks are highly over-parameterized compared to the\ndata on which they are trained, yet they often generalize remarkably well. A\nflurry of recent work has asked: why do deep networks not overfit to their\ntraining data? In this work, we make a series of empirical observations that\ninvestigate and extend the hypothesis that deeper networks are inductively\nbiased to find solutions with lower effective rank embeddings. We conjecture\nthat this bias exists because the volume of functions that maps to low\neffective rank embedding increases with depth. We show empirically that our\nclaim holds true on finite width linear and non-linear models on practical\nlearning paradigms and show that on natural data, these are often the solutions\nthat generalize well. We then show that the simplicity bias exists at both\ninitialization and after training and is resilient to hyper-parameters and\nlearning methods. We further demonstrate how linear over-parameterization of\ndeep non-linear models can be used to induce low-rank bias, improving\ngeneralization performance on CIFAR and ImageNet without changing the modeling\ncapacity.",
    "published_date": "2021-03-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.10427v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.10415v3",
    "title": "Refining Language Models with Compositional Explanations",
    "authors": [
      "Huihan Yao",
      "Ying Chen",
      "Qinyuan Ye",
      "Xisen Jin",
      "Xiang Ren"
    ],
    "author_ids": [],
    "abstract": "Pre-trained language models have been successful on text classification\ntasks, but are prone to learning spurious correlations from biased datasets,\nand are thus vulnerable when making inferences in a new domain. Prior work\nreveals such spurious patterns via post-hoc explanation algorithms which\ncompute the importance of input features. Further, the model is regularized to\nalign the importance scores with human knowledge, so that the unintended model\nbehaviors are eliminated. However, such a regularization technique lacks\nflexibility and coverage, since only importance scores towards a pre-defined\nlist of features are adjusted, while more complex human knowledge such as\nfeature interaction and pattern generalization can hardly be incorporated. In\nthis work, we propose to refine a learned language model for a target domain by\ncollecting human-provided compositional explanations regarding observed biases.\nBy parsing these explanations into executable logic rules, the human-specified\nrefinement advice from a small set of explanations can be generalized to more\ntraining examples. We additionally introduce a regularization term allowing\nadjustments for both importance and interaction of features to better rectify\nmodel behavior. We demonstrate the effectiveness of the proposed approach on\ntwo text classification tasks by showing improved performance in target domain\nas well as improved model fairness after refinement.",
    "published_date": "2021-03-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.10415v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.10230v2",
    "title": "Collective Decision of One-vs-Rest Networks for Open Set Recognition",
    "authors": [
      "Jaeyeon Jang",
      "Chang Ouk Kim"
    ],
    "author_ids": [],
    "abstract": "Unknown examples that are unseen during training often appear in real-world\nmachine learning tasks, and an intelligent self-learning system should be able\nto distinguish between known and unknown examples. Accordingly, open set\nrecognition (OSR), which addresses the problem of classifying knowns and\nidentifying unknowns, has recently been highlighted. However, conventional deep\nneural networks using a softmax layer are vulnerable to overgeneralization,\nproducing high confidence scores for unknowns. In this paper, we propose a\nsimple OSR method based on the intuition that OSR performance can be maximized\nby setting strict and sophisticated decision boundaries that reject unknowns\nwhile maintaining satisfactory classification performance on knowns. For this\npurpose, a novel network structure is proposed, in which multiple one-vs-rest\nnetworks (OVRNs) follow a convolutional neural network feature extractor. Here,\nthe OVRN is a simple feed-forward neural network that enhances the ability to\nreject nonmatches by learning class-specific discriminative features.\nFurthermore, the collective decision score is modeled by combining the multiple\ndecisions reached by the OVRNs to alleviate overgeneralization. Extensive\nexperiments were conducted on various datasets, and the experimental results\nshowed that the proposed method performed significantly better than the\nstate-of-the-art methods by effectively reducing overgeneralization.",
    "published_date": "2021-03-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.10230v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.09947v2",
    "title": "Understanding Generalization in Adversarial Training via the Bias-Variance Decomposition",
    "authors": [
      "Yaodong Yu",
      "Zitong Yang",
      "Edgar Dobriban",
      "Jacob Steinhardt",
      "Yi Ma"
    ],
    "author_ids": [],
    "abstract": "Adversarially trained models exhibit a large generalization gap: they can\ninterpolate the training set even for large perturbation radii, but at the cost\nof large test error on clean samples. To investigate this gap, we decompose the\ntest risk into its bias and variance components and study their behavior as a\nfunction of adversarial training perturbation radii ($\\varepsilon$). We find\nthat the bias increases monotonically with $\\varepsilon$ and is the dominant\nterm in the risk. Meanwhile, the variance is unimodal as a function of\n$\\varepsilon$, peaking near the interpolation threshold for the training set.\nThis characteristic behavior occurs robustly across different datasets and also\nfor other robust training procedures such as randomized smoothing. It thus\nprovides a test for proposed explanations of the generalization gap. We find\nthat some existing explanations fail this test--for instance, by predicting a\nmonotonically increasing variance curve. This underscores the power of\nbias-variance decompositions in modern settings-by providing two measurements\ninstead of one, they can rule out more explanations than test accuracy alone.\nWe also show that bias and variance can provide useful guidance for scalably\nreducing the generalization gap, highlighting pre-training and unlabeled data\nas promising routes.",
    "published_date": "2021-03-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.09947v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.09882v2",
    "title": "Hierarchical Attention-based Age Estimation and Bias Estimation",
    "authors": [
      "Shakediel Hiba",
      "Yosi Keller"
    ],
    "author_ids": [],
    "abstract": "In this work we propose a novel deep-learning approach for age estimation\nbased on face images. We first introduce a dual image augmentation-aggregation\napproach based on attention. This allows the network to jointly utilize\nmultiple face image augmentations whose embeddings are aggregated by a\nTransformer-Encoder. The resulting aggregated embedding is shown to better\nencode the face image attributes. We then propose a probabilistic hierarchical\nregression framework that combines a discrete probabilistic estimate of age\nlabels, with a corresponding ensemble of regressors. Each regressor is\nparticularly adapted and trained to refine the probabilistic estimate over a\nrange of ages. Our scheme is shown to outperform contemporary schemes and\nprovide a new state-of-the-art age estimation accuracy, when applied to the\nMORPH II dataset for age estimation. Last, we introduce a bias analysis of\nstate-of-the-art age estimation results.",
    "published_date": "2021-03-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.09882v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.09876v2",
    "title": "Bias-Free FedGAN: A Federated Approach to Generate Bias-Free Datasets",
    "authors": [
      "Vaikkunth Mugunthan",
      "Vignesh Gokul",
      "Lalana Kagal",
      "Shlomo Dubnov"
    ],
    "author_ids": [],
    "abstract": "Federated Generative Adversarial Network (FedGAN) is a\ncommunication-efficient approach to train a GAN across distributed clients\nwithout clients having to share their sensitive training data. In this paper,\nwe experimentally show that FedGAN generates biased data points under\nnon-independent-and-identically-distributed (non-iid) settings. Also, we\npropose Bias-Free FedGAN, an approach to generate bias-free synthetic datasets\nusing FedGAN. Our approach generates metadata at the aggregator using the\nmodels received from clients and retrains the federated model to achieve\nbias-free results for image synthesis. Bias-Free FedGAN has the same\ncommunication cost as that of FedGAN. Experimental results on image datasets\n(MNIST and FashionMNIST) validate our claims.",
    "published_date": "2021-03-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CV",
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.09876v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.09713v1",
    "title": "Cyber Intrusion Detection by Using Deep Neural Networks with Attack-sharing Loss",
    "authors": [
      "Boxiang Dong",
      "Hui",
      "Wang",
      "Aparna S. Varde",
      "Dawei Li",
      "Bharath K. Samanthula",
      "Weifeng Sun",
      "Liang Zhao"
    ],
    "author_ids": [],
    "abstract": "Cyber attacks pose crucial threats to computer system security, and put\ndigital treasuries at excessive risks. This leads to an urgent call for an\neffective intrusion detection system that can identify the intrusion attacks\nwith high accuracy. It is challenging to classify the intrusion events due to\nthe wide variety of attacks. Furthermore, in a normal network environment, a\nmajority of the connections are initiated by benign behaviors. The class\nimbalance issue in intrusion detection forces the classifier to be biased\ntoward the majority/benign class, thus leave many attack incidents undetected.\nSpurred by the success of deep neural networks in computer vision and natural\nlanguage processing, in this paper, we design a new system named DeepIDEA that\ntakes full advantage of deep learning to enable intrusion detection and\nclassification. To achieve high detection accuracy on imbalanced data, we\ndesign a novel attack-sharing loss function that can effectively move the\ndecision boundary towards the attack classes and eliminates the bias towards\nthe majority/benign class. By using this loss function, DeepIDEA respects the\nfact that the intrusion mis-classification should receive higher penalty than\nthe attack mis-classification. Extensive experimental results on three\nbenchmark datasets demonstrate the high detection accuracy of DeepIDEA. In\nparticular, compared with eight state-of-the-art approaches, DeepIDEA always\nprovides the best class-balanced accuracy.",
    "published_date": "2021-03-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.09713v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.09419v1",
    "title": "Fairness-aware Outlier Ensemble",
    "authors": [
      "Haoyu Liu",
      "Fenglong Ma",
      "Shibo He",
      "Jiming Chen",
      "Jing Gao"
    ],
    "author_ids": [],
    "abstract": "Outlier ensemble methods have shown outstanding performance on the discovery\nof instances that are significantly different from the majority of the data.\nHowever, without the awareness of fairness, their applicability in the ethical\nscenarios, such as fraud detection and judiciary judgement system, could be\ndegraded. In this paper, we propose to reduce the bias of the outlier ensemble\nresults through a fairness-aware ensemble framework. Due to the lack of ground\ntruth in the outlier detection task, the key challenge is how to mitigate the\ndegradation in the detection performance with the improvement of fairness. To\naddress this challenge, we define a distance measure based on the output of\nconventional outlier ensemble techniques to estimate the possible cost\nassociated with detection performance degradation. Meanwhile, we propose a\npost-processing framework to tune the original ensemble results through a\nstacking process so that we can achieve a trade off between fairness and\ndetection performance. Detection performance is measured by the area under ROC\ncurve (AUC) while fairness is measured at both group and individual level.\nExperiments on eight public datasets are conducted. Results demonstrate the\neffectiveness of the proposed framework in improving fairness of outlier\nensemble results. We also analyze the trade-off between AUC and fairness.",
    "published_date": "2021-03-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.09419v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.09396v3",
    "title": "Pros and Cons of GAN Evaluation Measures: New Developments",
    "authors": [
      "Ali Borji"
    ],
    "author_ids": [],
    "abstract": "This work is an update of a previous paper on the same topic published a few\nyears ago. With the dramatic progress in generative modeling, a suite of new\nquantitative and qualitative techniques to evaluate models has emerged.\nAlthough some measures such as Inception Score, Frechet Inception Distance,\nPrecision-Recall, and Perceptual Path Length are relatively more popular, GAN\nevaluation is not a settled issue and there is still room for improvement.\nHere, I describe new dimensions that are becoming important in assessing models\n(e.g. bias and fairness) and discuss the connection between GAN evaluation and\ndeepfakes. These are important areas of concern in the machine learning\ncommunity today and progress in GAN evaluation can help mitigate them.",
    "published_date": "2021-03-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.09396v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.09118v5",
    "title": "Balancing Biases and Preserving Privacy on Balanced Faces in the Wild",
    "authors": [
      "Joseph P Robinson",
      "Can Qin",
      "Yann Henon",
      "Samson Timoner",
      "Yun Fu"
    ],
    "author_ids": [],
    "abstract": "There are demographic biases present in current facial recognition (FR)\nmodels. To measure these biases across different ethnic and gender subgroups,\nwe introduce our Balanced Faces in the Wild (BFW) dataset. This dataset allows\nfor the characterization of FR performance per subgroup. We found that relying\non a single score threshold to differentiate between genuine and imposters\nsample pairs leads to suboptimal results. Additionally, performance within\nsubgroups often varies significantly from the global average. Therefore,\nspecific error rates only hold for populations that match the validation data.\nTo mitigate imbalanced performances, we propose a novel domain adaptation\nlearning scheme that uses facial features extracted from state-of-the-art\nneural networks. This scheme boosts the average performance and preserves\nidentity information while removing demographic knowledge. Removing demographic\nknowledge prevents potential biases from affecting decision-making and protects\nprivacy by eliminating demographic information. We explore the proposed method\nand demonstrate that subgroup classifiers can no longer learn from features\nprojected using our domain adaptation scheme. For access to the source code and\ndata, please visit https://github.com/visionjo/facerec-bias-bfw.",
    "published_date": "2021-03-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.09118v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.09068v1",
    "title": "Predicting Early Dropout: Calibration and Algorithmic Fairness Considerations",
    "authors": [
      "Marzieh Karimi-Haghighi",
      "Carlos Castillo",
      "Davinia Hernandez-Leo",
      "Veronica Moreno Oliver"
    ],
    "author_ids": [],
    "abstract": "In this work, the problem of predicting dropout risk in undergraduate studies\nis addressed from a perspective of algorithmic fairness. We develop a machine\nlearning method to predict the risks of university dropout and\nunderperformance. The objective is to understand if such a system can identify\nstudents at risk while avoiding potential discriminatory biases. When modeling\nboth risks, we obtain prediction models with an Area Under the ROC Curve (AUC)\nof 0.77-0.78 based on the data available at the enrollment time, before the\nfirst year of studies starts. This data includes the students' demographics,\nthe high school they attended, and their admission (average) grade. Our models\nare calibrated: they produce estimated probabilities for each risk, not mere\nscores. We analyze if this method leads to discriminatory outcomes for some\nsensitive groups in terms of prediction accuracy (AUC) and error rates\n(Generalized False Positive Rate, GFPR, or Generalized False Negative Rate,\nGFNR). The models exhibit some equity in terms of AUC and GFNR along groups.\nThe similar GFNR means a similar probability of failing to detect risk for\nstudents who drop out. The disparities in GFPR are addressed through a\nmitigation process that does not affect the calibration of the model.",
    "published_date": "2021-03-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.09068v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.12506v1",
    "title": "A Survey on Predicting the Factuality and the Bias of News Media",
    "authors": [
      "Preslav Nakov",
      "Husrev Taha Sencar",
      "Jisun An",
      "Haewoon Kwak"
    ],
    "author_ids": [],
    "abstract": "The present level of proliferation of fake, biased, and propagandistic\ncontent online has made it impossible to fact-check every single suspicious\nclaim or article, either manually or automatically. Thus, many researchers are\nshifting their attention to higher granularity, aiming to profile entire news\noutlets, which makes it possible to detect likely \"fake news\" the moment it is\npublished, by simply checking the reliability of its source. Source factuality\nis also an important element of systems for automatic fact-checking and \"fake\nnews\" detection, as they need to assess the reliability of the evidence they\nretrieve online. Political bias detection, which in the Western political\nlandscape is about predicting left-center-right bias, is an equally important\ntopic, which has experienced a similar shift towards profiling entire news\noutlets. Moreover, there is a clear connection between the two, as highly\nbiased media are less likely to be factual; yet, the two problems have been\naddressed separately. In this survey, we review the state of the art on media\nprofiling for factuality and bias, arguing for the need to model them jointly.\nWe further discuss interesting recent advances in using different information\nsources and modalities, which go beyond the text of the articles the target\nnews outlet has published. Finally, we discuss current challenges and outline\nfuture research directions.",
    "published_date": "2021-03-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI",
      "cs.CL",
      "68T50",
      "I.2.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.12506v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2104.03909v1",
    "title": "RAWLSNET: Altering Bayesian Networks to Encode Rawlsian Fair Equality of Opportunity",
    "authors": [
      "David Liu",
      "Zohair Shafi",
      "William Fleisher",
      "Tina Eliassi-Rad",
      "Scott Alfeld"
    ],
    "author_ids": [],
    "abstract": "We present RAWLSNET, a system for altering Bayesian Network (BN) models to\nsatisfy the Rawlsian principle of fair equality of opportunity (FEO).\nRAWLSNET's BN models generate aspirational data distributions: data generated\nto reflect an ideally fair, FEO-satisfying society. FEO states that everyone\nwith the same talent and willingness to use it should have the same chance of\nachieving advantageous social positions (e.g., employment), regardless of their\nbackground circumstances (e.g., socioeconomic status). Satisfying FEO requires\nalterations to social structures such as school assignments. Our paper\ndescribes RAWLSNET, a method which takes as input a BN representation of an FEO\napplication and alters the BN's parameters so as to satisfy FEO when possible,\nand minimize deviation from FEO otherwise. We also offer guidance for applying\nRAWLSNET, including on recognizing proper applications of FEO. We demonstrate\nthe use of our system with publicly available data sets. RAWLSNET's altered BNs\noffer the novel capability of generating aspirational data for FEO-relevant\ntasks. Aspirational data are free from the biases of real-world data, and thus\nare useful for recognizing and detecting sources of unfairness in machine\nlearning algorithms besides biased data.",
    "published_date": "2021-03-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.03909v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.08786v1",
    "title": "Fairness and Transparency in Recommendation: The Users' Perspective",
    "authors": [
      "Nasim Sonboli",
      "Jessie J. Smith",
      "Florencia Cabral Berenfus",
      "Robin Burke",
      "Casey Fiesler"
    ],
    "author_ids": [],
    "abstract": "Though recommender systems are defined by personalization, recent work has\nshown the importance of additional, beyond-accuracy objectives, such as\nfairness. Because users often expect their recommendations to be purely\npersonalized, these new algorithmic objectives must be communicated\ntransparently in a fairness-aware recommender system. While explanation has a\nlong history in recommender systems research, there has been little work that\nattempts to explain systems that use a fairness objective. Even though the\nprevious work in other branches of AI has explored the use of explanations as a\ntool to increase fairness, this work has not been focused on recommendation.\nHere, we consider user perspectives of fairness-aware recommender systems and\ntechniques for enhancing their transparency. We describe the results of an\nexploratory interview study that investigates user perceptions of fairness,\nrecommender systems, and fairness-aware objectives. We propose three features\n-- informed by the needs of our participants -- that could improve user\nunderstanding of and trust in fairness-aware recommender systems.",
    "published_date": "2021-03-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.08786v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.08763v1",
    "title": "Please Don't Go -- A Comprehensive Approach to Increase Women's Participation in Open Source Software",
    "authors": [
      "Bianca Trinkenreich"
    ],
    "author_ids": [],
    "abstract": "Women represent less than 24% of employees in the software development\nindustry and experience various types of prejudice and bias. Despite various\nefforts to increase diversity and multi-gendered participation, women are even\nmore underrepresented in Open Source Software (OSS) projects. In my PhD, I\ninvestigate the following question: How can OSS communities increase women's\nparticipation in their projects? I will identify different OSS career pathways\nand develop a holistic view of women's motivations to join or leave OSS, as\nwell as their definitions of success. Based on this empirical investigation, I\nwill work together with the Linux Foundation to design attraction and retention\nstrategies focused on women. Before and after implementing the strategies, I\nwill conduct empirical studies to evaluate the state of the practice and\nunderstand the implications of the strategies.",
    "published_date": "2021-03-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SE",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.08763v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.01109v1",
    "title": "AI Fairness via Domain Adaptation",
    "authors": [
      "Neil Joshi",
      "Phil Burlina"
    ],
    "author_ids": [],
    "abstract": "While deep learning (DL) approaches are reaching human-level performance for\nmany tasks, including for diagnostics AI, the focus is now on challenges\npossibly affecting DL deployment, including AI privacy, domain generalization,\nand fairness. This last challenge is addressed in this study. Here we look at a\nnovel method for ensuring AI fairness with respect to protected or sensitive\nfactors. This method uses domain adaptation via training set enhancement to\ntackle bias-causing training data imbalance. More specifically, it uses\ngenerative models that allow the generation of more synthetic training samples\nfor underrepresented populations. This paper applies this method to the use\ncase of detection of age related macular degeneration (AMD). Our experiments\nshow that starting with an originally biased AMD diagnostics model the method\nhas the ability to improve fairness.",
    "published_date": "2021-03-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.01109v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.09233v1",
    "title": "Towards Fair Affective Robotics: Continual Learning for Mitigating Bias in Facial Expression and Action Unit Recognition",
    "authors": [
      "Ozgur Kara",
      "Nikhil Churamani",
      "Hatice Gunes"
    ],
    "author_ids": [],
    "abstract": "As affective robots become integral in human life, these agents must be able\nto fairly evaluate human affective expressions without discriminating against\nspecific demographic groups. Identifying bias in Machine Learning (ML) systems\nas a critical problem, different approaches have been proposed to mitigate such\nbiases in the models both at data and algorithmic levels. In this work, we\npropose Continual Learning (CL) as an effective strategy to enhance fairness in\nFacial Expression Recognition (FER) systems, guarding against biases arising\nfrom imbalances in data distributions. We compare different state-of-the-art\nbias mitigation approaches with CL-based strategies for fairness on expression\nrecognition and Action Unit (AU) detection tasks using popular benchmarks for\neach; RAF-DB and BP4D. Our experiments show that CL-based methods, on average,\noutperform popular bias mitigation techniques, strengthening the need for\nfurther investigation into CL for the development of fairer FER algorithms.",
    "published_date": "2021-03-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.09233v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.08637v1",
    "title": "Domain-Incremental Continual Learning for Mitigating Bias in Facial Expression and Action Unit Recognition",
    "authors": [
      "Nikhil Churamani",
      "Ozgur Kara",
      "Hatice Gunes"
    ],
    "author_ids": [],
    "abstract": "As Facial Expression Recognition (FER) systems become integrated into our\ndaily lives, these systems need to prioritise making fair decisions instead of\naiming at higher individual accuracy scores. Ranging from surveillance systems\nto diagnosing mental and emotional health conditions of individuals, these\nsystems need to balance the accuracy vs fairness trade-off to make decisions\nthat do not unjustly discriminate against specific under-represented\ndemographic groups. Identifying bias as a critical problem in facial analysis\nsystems, different methods have been proposed that aim to mitigate bias both at\ndata and algorithmic levels. In this work, we propose the novel usage of\nContinual Learning (CL), in particular, using Domain-Incremental Learning\n(Domain-IL) settings, as a potent bias mitigation method to enhance the\nfairness of FER systems while guarding against biases arising from skewed data\ndistributions. We compare different non-CL-based and CL-based methods for their\nclassification accuracy and fairness scores on expression recognition and\nAction Unit (AU) detection tasks using two popular benchmarks, the RAF-DB and\nBP4D datasets, respectively. Our experimental results show that CL-based\nmethods, on average, outperform other popular bias mitigation techniques on\nboth accuracy and fairness metrics.",
    "published_date": "2021-03-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.08637v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.08535v1",
    "title": "Quantum projective measurements and the CHSH inequality in Isabelle/HOL",
    "authors": [
      "Mnacho Echenim",
      "Mehdi Mhalla"
    ],
    "author_ids": [],
    "abstract": "We present a formalization in Isabelle/HOL of quantum projective\nmeasurements, a class of measurements involving orthogonal projectors that is\nfrequently used in quantum computing. We also formalize the CHSH inequality, a\nresult that holds on arbitrary probability spaces, which can used to disprove\nthe existence of a local hidden-variable theory for quantum mechanics.",
    "published_date": "2021-03-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.08535v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.08508v2",
    "title": "Stack of discriminative autoencoders for multiclass anomaly detection in endoscopy images",
    "authors": [
      "Mohammad Reza Mohebbian",
      "Khan A. Wahid",
      "Paul Babyn"
    ],
    "author_ids": [],
    "abstract": "Wireless Capsule Endoscopy (WCE) helps physicians examine the\ngastrointestinal (GI) tract noninvasively. There are few studies that address\npathological assessment of endoscopy images in multiclass classification and\nmost of them are based on binary anomaly detection or aim to detect a specific\ntype of anomaly. Multiclass anomaly detection is challenging, especially when\nthe dataset is poorly sampled or imbalanced. Many available datasets in\nendoscopy field, such as KID2, suffer from an imbalance issue, which makes it\ndifficult to train a high-performance model. Additionally, increasing the\nnumber of classes makes classification more difficult. We proposed a multiclass\nclassification algorithm that is extensible to any number of classes and can\nhandle an imbalance issue. The proposed method uses multiple autoencoders where\neach one is trained on one class to extract features with the most\ndiscrimination from other classes. The loss function of autoencoders is set\nbased on reconstruction, compactness, distance from other classes, and\nKullback-Leibler (KL) divergence. The extracted features are clustered and then\nclassified using an ensemble of support vector data descriptors. A total of\n1,778 normal, 227 inflammation, 303 vascular, and 44 polyp images from the KID2\ndataset are used for evaluation. The entire algorithm ran 5 times and achieved\nF1-score of 96.3 +- 0.2% and 85.0 +- 0.4% on the test set for binary and\nmulticlass anomaly detection, respectively. The impact of each step of the\nalgorithm was investigated by various ablation studies and the results were\ncompared with published works. The suggested approach is a competitive option\nfor detecting multiclass anomalies in the GI field.",
    "published_date": "2021-03-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.08508v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.08454v2",
    "title": "Margin Preserving Self-paced Contrastive Learning Towards Domain Adaptation for Medical Image Segmentation",
    "authors": [
      "Zhizhe Liu",
      "Zhenfeng Zhu",
      "Shuai Zheng",
      "Yang Liu",
      "Jiayu Zhou",
      "Yao Zhao"
    ],
    "author_ids": [],
    "abstract": "To bridge the gap between the source and target domains in unsupervised\ndomain adaptation (UDA), the most common strategy puts focus on matching the\nmarginal distributions in the feature space through adversarial learning.\nHowever, such category-agnostic global alignment lacks of exploiting the\nclass-level joint distributions, causing the aligned distribution less\ndiscriminative. To address this issue, we propose in this paper a novel margin\npreserving self-paced contrastive Learning (MPSCL) model for cross-modal\nmedical image segmentation. Unlike the conventional construction of contrastive\npairs in contrastive learning, the domain-adaptive category prototypes are\nutilized to constitute the positive and negative sample pairs. With the\nguidance of progressively refined semantic prototypes, a novel margin\npreserving contrastive loss is proposed to boost the discriminability of\nembedded representation space. To enhance the supervision for contrastive\nlearning, more informative pseudo-labels are generated in target domain in a\nself-paced way, thus benefiting the category-aware distribution alignment for\nUDA. Furthermore, the domain-invariant representations are learned through\njoint contrastive learning between the two domains. Extensive experiments on\ncross-modal cardiac segmentation tasks demonstrate that MPSCL significantly\nimproves semantic segmentation performance, and outperforms a wide variety of\nstate-of-the-art methods by a large margin.",
    "published_date": "2021-03-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.08454v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.08391v1",
    "title": "Flexible FOND Planning with Explicit Fairness Assumptions",
    "authors": [
      "Ivan D. Rodriguez",
      "Blai Bonet",
      "Sebastian Sardina",
      "Hector Geffner"
    ],
    "author_ids": [],
    "abstract": "We consider the problem of reaching a propositional goal condition in\nfully-observable non-deterministic (FOND) planning under a general class of\nfairness assumptions that are given explicitly. The fairness assumptions are of\nthe form A/B and say that state trajectories that contain infinite occurrences\nof an action a from A in a state s and finite occurrence of actions from B,\nmust also contain infinite occurrences of action a in s followed by each one of\nits possible outcomes. The infinite trajectories that violate this condition\nare deemed as unfair, and the solutions are policies for which all the fair\ntrajectories reach a goal state. We show that strong and strong-cyclic FOND\nplanning, as well as QNP planning, a planning model introduced recently for\ngeneralized planning, are all special cases of FOND planning with fairness\nassumptions of this form which can also be combined. FOND+ planning, as this\nform of planning is called, combines the syntax of FOND planning with some of\nthe versatility of LTL for expressing fairness constraints. A new planner is\nimplemented by reducing FOND+ planning to answer set programs, and the\nperformance of the planner is evaluated in comparison with FOND and QNP\nplanners, and LTL synthesis tools.",
    "published_date": "2021-03-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.08391v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.08359v1",
    "title": "Explaining Credit Risk Scoring through Feature Contribution Alignment with Expert Risk Analysts",
    "authors": [
      "Ayoub El Qadi",
      "Natalia Diaz-Rodriguez",
      "Maria Trocan",
      "Thomas Frossard"
    ],
    "author_ids": [],
    "abstract": "Credit assessments activities are essential for financial institutions and\nallow the global economy to grow. Building robust, solid and accurate models\nthat estimate the probability of a default of a company is mandatory for credit\ninsurance companies, moreover when it comes to bridging the trade finance gap.\nAutomating the risk assessment process will allow credit risk experts to reduce\ntheir workload and focus on the critical and complex cases, as well as to\nimprove the loan approval process by reducing the time to process the\napplication. The recent developments in Artificial Intelligence are offering\nnew powerful opportunities. However, most AI techniques are labelled as\nblackbox models due to their lack of explainability. For both users and\nregulators, in order to deploy such technologies at scale, being able to\nunderstand the model logic is a must to grant accurate and ethical decision\nmaking. In this study, we focus on companies credit scoring and we benchmark\ndifferent machine learning models. The aim is to build a model to predict\nwhether a company will experience financial problems in a given time horizon.\nWe address the black box problem using eXplainable Artificial Techniques in\nparticular, post-hoc explanations using SHapley Additive exPlanations. We bring\nlight by providing an expert-aligned feature relevance score highlighting the\ndisagreement between a credit risk expert and a model feature attribution\nexplanation in order to better quantify the convergence towards a better\nhuman-aligned decision making.",
    "published_date": "2021-03-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.08359v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.09316v3",
    "title": "Are deep learning models superior for missing data imputation in large surveys? Evidence from an empirical comparison",
    "authors": [
      "Zhenhua Wang",
      "Olanrewaju Akande",
      "Jason Poulos",
      "Fan Li"
    ],
    "author_ids": [],
    "abstract": "Multiple imputation (MI) is a popular approach for dealing with missing data\narising from non-response in sample surveys. Multiple imputation by chained\nequations (MICE) is one of the most widely used MI algorithms for multivariate\ndata, but it lacks theoretical foundation and is computationally intensive.\nRecently, missing data imputation methods based on deep learning models have\nbeen developed with encouraging results in small studies. However, there has\nbeen limited research on evaluating their performance in realistic settings\ncompared to MICE, particularly in big surveys. We conduct extensive simulation\nstudies based on a subsample of the American Community Survey to compare the\nrepeated sampling properties of four machine learning based MI methods: MICE\nwith classification trees, MICE with random forests, generative adversarial\nimputation networks, and multiple imputation using denoising autoencoders. We\nfind the deep learning imputation methods are superior to MICE in terms of\ncomputational time. However, with the default choice of hyperparameters in the\ncommon software packages, MICE with classification trees consistently\noutperforms, often by a large margin, the deep learning imputation methods in\nterms of bias, mean squared error, and coverage under a range of realistic\nsettings.",
    "published_date": "2021-03-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.AP",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.09316v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.07849v1",
    "title": "Fairness-aware Personalized Ranking Recommendation via Adversarial Learning",
    "authors": [
      "Ziwei Zhu",
      "Jianling Wang",
      "James Caverlee"
    ],
    "author_ids": [],
    "abstract": "Recommendation algorithms typically build models based on historical\nuser-item interactions (e.g., clicks, likes, or ratings) to provide a\npersonalized ranked list of items. These interactions are often distributed\nunevenly over different groups of items due to varying user preferences.\nHowever, we show that recommendation algorithms can inherit or even amplify\nthis imbalanced distribution, leading to unfair recommendations to item groups.\nConcretely, we formalize the concepts of ranking-based statistical parity and\nequal opportunity as two measures of fairness in personalized ranking\nrecommendation for item groups. Then, we empirically show that one of the most\nwidely adopted algorithms -- Bayesian Personalized Ranking -- produces unfair\nrecommendations, which motivates our effort to propose the novel fairness-aware\npersonalized ranking model. The debiased model is able to improve the two\nproposed fairness metrics while preserving recommendation performance.\nExperiments on three public datasets show strong fairness improvement of the\nproposed model versus state-of-the-art alternatives.\n  This is paper is an extended and reorganized version of our SIGIR\n2020~\\cite{zhu2020measuring} paper. In this paper, we re-frame the studied\nproblem as `item recommendation fairness' in personalized ranking\nrecommendation systems, and provide more details about the training process of\nthe proposed model and details of experiment setup.",
    "published_date": "2021-03-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.07849v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.09086v1",
    "title": "Cooperation in lot-sizing problems with heterogeneous costs: the effect of consolidated periods",
    "authors": [
      "Luis Guardiola",
      "Ana Meca",
      "Justo Puerto"
    ],
    "author_ids": [],
    "abstract": "We consider a cooperative game defined by an economic lot-sizing problem with\nheterogeneous costs over a finite time horizon, in which each firm faces demand\nfor a single product in each period and coalitions can pool orders. The model\nof cooperation works as follows: ordering channels and holding and backlogging\ntechnologies are shared among the members of the coalitions. This implies that\neach firm uses the best ordering channel and holding technology provided by the\nparticipants in the consortium. That is, they purchase, hold inventory, pay\nbacklogged demand and make orders at the minimum cost of the coalition members.\nThus, firms aim at satisfying their demand over the planing horizon with\nminimal operation cost. Our contribution is to show that there exist fair\nallocations of the overall operation cost among the firms so that no group of\nagents profit from leaving the consortium. Then we propose a parametric family\nof cost allocations and provide sufficient conditions for this to be a stable\nfamily against coalitional defections of firms. Finally, we focus on those\nperiods of the time horizon that are consolidated and we analyze their effect\non the stability of cost allocations.",
    "published_date": "2021-03-14T00:00:00",
    "year": 2021,
    "categories": [
      "eess.SY",
      "cs.GT",
      "cs.SY",
      "91A12, 90B05"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.09086v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.07830v1",
    "title": "Sum-GDoF of Symmetric Multi-hop Interference Channel under Finite Precision CSIT using Aligned-Images Sumset Inequalities",
    "authors": [
      "Junge Wang",
      "Syed Ali Jafar"
    ],
    "author_ids": [],
    "abstract": "Aligned-Images Sumset Inequalities are used in this work to study the\nGeneralized Degrees of Freedom (GDoF) of the symmetric layered multi-hop\ninterference channel under the robust assumption that the channel state\ninformation at the transmitters (CSIT) is limited to finite precision. First,\nthe sum-GDoF value is characterized for the $2\\times 2\\times 2$ setting that is\ncomprised of $2$ sources, $2$ relays, and $2$ destinations. It is shown that\nthe sum-GDoF do not improve even if perfect CSIT is allowed in the first hop,\nas long as the CSIT in the second hop is limited to finite precision. The sum\nGDoF characterization is then generalized to the $2\\times 2\\times \\cdots \\times\n2$ setting that is comprised of $L$ hops. Remarkably, for large $L$, the GDoF\nvalue approaches that of the one hop broadcast channel that is obtained by full\ncooperation among the two transmitters of the last hop, with finite precision\nCSIT. Previous studies of multi-hop interference networks either identified\nsophisticated GDoF optimal schemes under perfect CSIT, such as aligned\ninterference neutralization and network diagonalization, that are powerful in\ntheory but too fragile to be practical, or studied robust achievable schemes\nlike classical amplify/decode/compress-and-forward without claims of\ninformation-theoretic optimality. In contrast, under finite precision CSIT, we\nshow that the benefits of fragile schemes are lost, while a combination of\nclassical random coding schemes that are simpler and much more robust, namely a\nrate-splitting between decode-and-forward and amplify-and-forward, is shown to\nbe GDoF optimal. As such, this work represents another step towards bridging\nthe gap between theory (optimality) and practice (robustness) with the aid of\nAligned-Images Sumset Inequalities.",
    "published_date": "2021-03-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.07830v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.07788v1",
    "title": "Treatment Effect Estimation using Invariant Risk Minimization",
    "authors": [
      "Abhin Shah",
      "Kartik Ahuja",
      "Karthikeyan Shanmugam",
      "Dennis Wei",
      "Kush Varshney",
      "Amit Dhurandhar"
    ],
    "author_ids": [],
    "abstract": "Inferring causal individual treatment effect (ITE) from observational data is\na challenging problem whose difficulty is exacerbated by the presence of\ntreatment assignment bias. In this work, we propose a new way to estimate the\nITE using the domain generalization framework of invariant risk minimization\n(IRM). IRM uses data from multiple domains, learns predictors that do not\nexploit spurious domain-dependent factors, and generalizes better to unseen\ndomains. We propose an IRM-based ITE estimator aimed at tackling treatment\nassignment bias when there is little support overlap between the control group\nand the treatment group. We accomplish this by creating diversity: given a\nsingle dataset, we split the data into multiple domains artificially. These\ndiverse domains are then exploited by IRM to more effectively generalize\nregression-based models to data regions that lack support overlap. We show\ngains over classical regression approaches to ITE estimation in settings when\nsupport mismatch is more pronounced.",
    "published_date": "2021-03-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.07788v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.07710v1",
    "title": "Delegation to autonomous agents promotes cooperation in collective-risk dilemmas",
    "authors": [
      "Elias Fernández Domingos",
      "Inês Terrucha",
      "Rémi Suchon",
      "Jelena Grujić",
      "Juan C. Burguillo",
      "Francisco C. Santos",
      "Tom Lenaerts"
    ],
    "author_ids": [],
    "abstract": "Home assistant chat-bots, self-driving cars, drones or automated negotiations\nare some of the several examples of autonomous (artificial) agents that have\npervaded our society. These agents enable the automation of multiple tasks,\nsaving time and (human) effort. However, their presence in social settings\nraises the need for a better understanding of their effect on social\ninteractions and how they may be used to enhance cooperation towards the public\ngood, instead of hindering it. To this end, we present an experimental study of\nhuman delegation to autonomous agents and hybrid human-agent interactions\ncentered on a public goods dilemma shaped by a collective risk. Our aim to\nunderstand experimentally whether the presence of autonomous agents has a\npositive or negative impact on social behaviour, fairness and cooperation in\nsuch a dilemma. Our results show that cooperation increases when participants\ndelegate their actions to an artificial agent that plays on their behalf. Yet,\nthis positive effect is reduced when humans interact in hybrid human-agent\ngroups. Finally, we show that humans are biased towards agent behaviour,\nassuming that they will contribute less to the collective effort.",
    "published_date": "2021-03-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC",
      "J.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.07710v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.07637v1",
    "title": "AIR4Children: Artificial Intelligence and Robotics for Children",
    "authors": [
      "Rocio Montenegro",
      "Elva Corona",
      "Donato Badillo-Perez",
      "Angel Mandujano",
      "Leticia Vazquez",
      "Dago Cruz",
      "Miguel Xochicale"
    ],
    "author_ids": [],
    "abstract": "We introduce AIR4Children, Artificial Intelligence for Children, as a way to\n(a) tackle aspects for inclusion, accessibility, transparency, equity, fairness\nand participation and (b) to create affordable child-centred materials in AI\nand Robotics (AIR). We present current challenges and opportunities for a\nchild-centred approaches for AIR. Similarly, we touch on open-sourced software\nand hardware technologies to make a more inclusive, affordable and fair\nparticipation of children in areas of AIR. Then, we describe the avenues that\nAIR4Children can take with the development of open-sourced software and\nhardware based on our initial pilots and experiences. Similarly, we propose to\nfollow the philosophy of Montessori education to help children to not only\ndevelop computational thinking but also to internalise new concepts and\nlearning skills through activities of movement and repetition. Finally, we\nconclude with the opportunities of our work and mainly we pose the future work\nof putting in practice what is proposed here to evaluate the potential impact\non AIR to children, instructors, parents and their community.",
    "published_date": "2021-03-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.07637v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.11795v1",
    "title": "Simpson's Bias in NLP Training",
    "authors": [
      "Fei Yuan",
      "Longtu Zhang",
      "Huang Bojun",
      "Yaobo Liang"
    ],
    "author_ids": [],
    "abstract": "In most machine learning tasks, we evaluate a model $M$ on a given data\npopulation $S$ by measuring a population-level metric $F(S;M)$. Examples of\nsuch evaluation metric $F$ include precision/recall for (binary) recognition,\nthe F1 score for multi-class classification, and the BLEU metric for language\ngeneration. On the other hand, the model $M$ is trained by optimizing a\nsample-level loss $G(S_t;M)$ at each learning step $t$, where $S_t$ is a subset\nof $S$ (a.k.a. the mini-batch). Popular choices of $G$ include cross-entropy\nloss, the Dice loss, and sentence-level BLEU scores. A fundamental assumption\nbehind this paradigm is that the mean value of the sample-level loss $G$, if\naveraged over all possible samples, should effectively represent the\npopulation-level metric $F$ of the task, such as, that $\\mathbb{E}[ G(S_t;M) ]\n\\approx F(S;M)$.\n  In this paper, we systematically investigate the above assumption in several\nNLP tasks. We show, both theoretically and experimentally, that some popular\ndesigns of the sample-level loss $G$ may be inconsistent with the true\npopulation-level metric $F$ of the task, so that models trained to optimize the\nformer can be substantially sub-optimal to the latter, a phenomenon we call it,\nSimpson's bias, due to its deep connections with the classic paradox known as\nSimpson's reversal paradox in statistics and social sciences.",
    "published_date": "2021-03-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.11795v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.09055v1",
    "title": "OmniFair: A Declarative System for Model-Agnostic Group Fairness in Machine Learning",
    "authors": [
      "Hantian Zhang",
      "Xu Chu",
      "Abolfazl Asudeh",
      "Shamkant B. Navathe"
    ],
    "author_ids": [],
    "abstract": "Machine learning (ML) is increasingly being used to make decisions in our\nsociety. ML models, however, can be unfair to certain demographic groups (e.g.,\nAfrican Americans or females) according to various fairness metrics. Existing\ntechniques for producing fair ML models either are limited to the type of\nfairness constraints they can handle (e.g., preprocessing) or require\nnontrivial modifications to downstream ML training algorithms (e.g.,\nin-processing).\n  We propose a declarative system OmniFair for supporting group fairness in ML.\nOmniFair features a declarative interface for users to specify desired group\nfairness constraints and supports all commonly used group fairness notions,\nincluding statistical parity, equalized odds, and predictive parity. OmniFair\nis also model-agnostic in the sense that it does not require modifications to a\nchosen ML algorithm. OmniFair also supports enforcing multiple user declared\nfairness constraints simultaneously while most previous techniques cannot. The\nalgorithms in OmniFair maximize model accuracy while meeting the specified\nfairness constraints, and their efficiency is optimized based on the\ntheoretically provable monotonicity property regarding the trade-off between\naccuracy and fairness that is unique to our system.\n  We conduct experiments on commonly used datasets that exhibit bias against\nminority groups in the fairness literature. We show that OmniFair is more\nversatile than existing algorithmic fairness approaches in terms of both\nsupported fairness constraints and downstream ML models. OmniFair reduces the\naccuracy loss by up to $94.8\\%$ compared with the second best method. OmniFair\nalso achieves similar running time to preprocessing methods, and is up to\n$270\\times$ faster than in-processing methods.",
    "published_date": "2021-03-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.DB",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.09055v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.07534v3",
    "title": "S2AND: A Benchmark and Evaluation System for Author Name Disambiguation",
    "authors": [
      "Shivashankar Subramanian",
      "Daniel King",
      "Doug Downey",
      "Sergey Feldman"
    ],
    "author_ids": [],
    "abstract": "Author Name Disambiguation (AND) is the task of resolving which author\nmentions in a bibliographic database refer to the same real-world person, and\nis a critical ingredient of digital library applications such as search and\ncitation analysis. While many AND algorithms have been proposed, comparing them\nis difficult because they often employ distinct features and are evaluated on\ndifferent datasets. In response to this challenge, we present S2AND, a unified\nbenchmark dataset for AND on scholarly papers, as well as an open-source\nreference model implementation. Our dataset harmonizes eight disparate AND\ndatasets into a uniform format, with a single rich feature set drawn from the\nSemantic Scholar (S2) database. Our evaluation suite for S2AND reports\nperformance split by facets like publication year and number of papers,\nallowing researchers to track both global performance and measures of fairness\nacross facet values. Our experiments show that because previous datasets tend\nto cover idiosyncratic and biased slices of the literature, algorithms trained\nto perform well on one on them may generalize poorly to others. By contrast, we\nshow how training on a union of datasets in S2AND results in more robust models\nthat perform well even on datasets unseen in training. The resulting AND model\nalso substantially improves over the production algorithm in S2, reducing error\nby over 50% in terms of $B^3$ F1. We release our unified dataset, model code,\ntrained models, and evaluation suite to the research community.\nhttps://github.com/allenai/S2AND/",
    "published_date": "2021-03-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.07534v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.01107v1",
    "title": "Geodesic B-Score for Improved Assessment of Knee Osteoarthritis",
    "authors": [
      "Felix Ambellan",
      "Stefan Zachow",
      "Christoph von Tycowicz"
    ],
    "author_ids": [],
    "abstract": "Three-dimensional medical imaging enables detailed understanding of\nosteoarthritis structural status. However, there remains a vast need for\nautomatic, thus, reader-independent measures that provide reliable assessment\nof subject-specific clinical outcomes. To this end, we derive a consistent\ngeneralization of the recently proposed B-score to Riemannian shape spaces. We\nfurther present an algorithmic treatment yielding simple, yet efficient\ncomputations allowing for analysis of large shape populations with several\nthousand samples. Our intrinsic formulation exhibits improved discrimination\nability over its Euclidean counterpart, which we demonstrate for predictive\nvalidity on assessing risks of total knee replacement. This result highlights\nthe potential of the geodesic B-score to enable improved personalized\nassessment and stratification for interventions.",
    "published_date": "2021-03-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "math.DG",
      "stat.AP",
      "stat.OT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.01107v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.06828v2",
    "title": "Wasserstein Robust Classification with Fairness Constraints",
    "authors": [
      "Yijie Wang",
      "Viet Anh Nguyen",
      "Grani A. Hanasusanto"
    ],
    "author_ids": [],
    "abstract": "We propose a distributionally robust classification model with a fairness\nconstraint that encourages the classifier to be fair in view of the equality of\nopportunity criterion. We use a type-$\\infty$ Wasserstein ambiguity set\ncentered at the empirical distribution to model distributional uncertainty and\nderive a conservative reformulation for the worst-case equal opportunity\nunfairness measure. We establish that the model is equivalent to a mixed binary\noptimization problem, which can be solved by standard off-the-shelf solvers. To\nimprove scalability, we further propose a convex, hinge-loss-based model for\nlarge problem instances whose reformulation does not incur any binary\nvariables. Moreover, we also consider the distributionally robust learning\nproblem with a generic ground transportation cost to hedge against the\nuncertainties in the label and sensitive attribute. Finally, we numerically\ndemonstrate that our proposed approaches improve fairness with negligible loss\nof predictive accuracy.",
    "published_date": "2021-03-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.06828v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.06650v1",
    "title": "A Generalization of the Concavity of Rényi Entropy Powe",
    "authors": [
      "Laigang Guo",
      "Chun-Ming Yuan",
      "Xiao-Shan Gao"
    ],
    "author_ids": [],
    "abstract": "Recently, Savar\\'{e}-Toscani proved that the R\\'{e}nyi entropy power of\ngeneral probability densities solving the $p$-nonlinear heat equation in\n$\\mathbb{R}^n$ is always a concave function of time, which extends Costa's\nconcavity inequality for Shannon's entropy power to R\\'{e}nyi entropies. In\nthis paper, we give a generalization of Savar\\'{e}-Toscani's result by giving a\nclass of sufficient conditions of the parameters under which the concavity of\nthe R\\'{e}nyi entropy power is still valid. These conditions are quite general\nand include the parameter range given by Savar\\'{e}-Toscani as special cases.\nAlso, the conditions are obtained with a systematical approach.",
    "published_date": "2021-03-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.06650v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.06598v1",
    "title": "DebIE: A Platform for Implicit and Explicit Debiasing of Word Embedding Spaces",
    "authors": [
      "Niklas Friedrich",
      "Anne Lauscher",
      "Simone Paolo Ponzetto",
      "Goran Glavaš"
    ],
    "author_ids": [],
    "abstract": "Recent research efforts in NLP have demonstrated that distributional word\nvector spaces often encode stereotypical human biases, such as racism and\nsexism. With word representations ubiquitously used in NLP models and\npipelines, this raises ethical issues and jeopardizes the fairness of language\ntechnologies. While there exists a large body of work on bias measures and\ndebiasing methods, to date, there is no platform that would unify these\nresearch efforts and make bias measuring and debiasing of representation spaces\nwidely accessible. In this work, we present DebIE, the first integrated\nplatform for (1) measuring and (2) mitigating bias in word embeddings. Given an\n(i) embedding space (users can choose between the predefined spaces or upload\ntheir own) and (ii) a bias specification (users can choose between existing\nbias specifications or create their own), DebIE can (1) compute several\nmeasures of implicit and explicit bias and modify the embedding space by\nexecuting two (mutually composable) debiasing models. DebIE's functionality can\nbe accessed through four different interfaces: (a) a web application, (b) a\ndesktop application, (c) a REST-ful API, and (d) as a command-line application.\nDebIE is available at: debie.informatik.uni-mannheim.de.",
    "published_date": "2021-03-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.06598v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.06587v1",
    "title": "Privacy-preserving Object Detection",
    "authors": [
      "Peiyang He",
      "Charlie Griffin",
      "Krzysztof Kacprzyk",
      "Artjom Joosen",
      "Michael Collyer",
      "Aleksandar Shtedritski",
      "Yuki M. Asano"
    ],
    "author_ids": [],
    "abstract": "Privacy considerations and bias in datasets are quickly becoming\nhigh-priority issues that the computer vision community needs to face. So far,\nlittle attention has been given to practical solutions that do not involve\ncollection of new datasets. In this work, we show that for object detection on\nCOCO, both anonymizing the dataset by blurring faces, as well as swapping faces\nin a balanced manner along the gender and skin tone dimension, can retain\nobject detection performances while preserving privacy and partially balancing\nbias.",
    "published_date": "2021-03-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.06587v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.06503v1",
    "title": "Fair Mixup: Fairness via Interpolation",
    "authors": [
      "Ching-Yao Chuang",
      "Youssef Mroueh"
    ],
    "author_ids": [],
    "abstract": "Training classifiers under fairness constraints such as group fairness,\nregularizes the disparities of predictions between the groups. Nevertheless,\neven though the constraints are satisfied during training, they might not\ngeneralize at evaluation time. To improve the generalizability of fair\nclassifiers, we propose fair mixup, a new data augmentation strategy for\nimposing the fairness constraint. In particular, we show that fairness can be\nachieved by regularizing the models on paths of interpolated samples between\nthe groups. We use mixup, a powerful data augmentation strategy to generate\nthese interpolates. We analyze fair mixup and empirically show that it ensures\na better generalization for both accuracy and fairness measurement in tabular,\nvision, and language benchmarks.",
    "published_date": "2021-03-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.06503v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.06484v2",
    "title": "Robust High-speed Running for Quadruped Robots via Deep Reinforcement Learning",
    "authors": [
      "Guillaume Bellegarda",
      "Yiyu Chen",
      "Zhuochen Liu",
      "Quan Nguyen"
    ],
    "author_ids": [],
    "abstract": "Deep reinforcement learning has emerged as a popular and powerful way to\ndevelop locomotion controllers for quadruped robots. Common approaches have\nlargely focused on learning actions directly in joint space, or learning to\nmodify and offset foot positions produced by trajectory generators. Both\napproaches typically require careful reward shaping and training for millions\nof time steps, and with trajectory generators introduce human bias into the\nresulting control policies. In this paper, we present a learning framework that\nleads to the natural emergence of fast and robust bounding policies for\nquadruped robots. The agent both selects and controls actions directly in task\nspace to track desired velocity commands subject to environmental noise\nincluding model uncertainty and rough terrain. We observe that this framework\nimproves sample efficiency, necessitates little reward shaping, leads to the\nemergence of natural gaits such as galloping and bounding, and eases the\nsim-to-real transfer at running speeds. Policies can be learned in only a few\nmillion time steps, even for challenging tasks of running over rough terrain\nwith loads of over 100% of the nominal quadruped mass. Training occurs in\nPyBullet, and we perform a sim-to-sim transfer to Gazebo and sim-to-real\ntransfer to the Unitree A1 hardware. For sim-to-sim, our results show the\nquadruped is able to run at over 4 m/s without a load, and 3.5 m/s with a 10 kg\nload, which is over 83% of the nominal quadruped mass. For sim-to-real, the\nUnitree A1 is able to bound at 2 m/s with a 5 kg load, representing 42% of the\nnominal quadruped mass.",
    "published_date": "2021-03-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.RO",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.06484v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.06435v1",
    "title": "Population-Based Evolution Optimizes a Meta-Learning Objective",
    "authors": [
      "Kevin Frans",
      "Olaf Witkowski"
    ],
    "author_ids": [],
    "abstract": "Meta-learning models, or models that learn to learn, have been a long-desired\ntarget for their ability to quickly solve new tasks. Traditional meta-learning\nmethods can require expensive inner and outer loops, thus there is demand for\nalgorithms that discover strong learners without explicitly searching for them.\nWe draw parallels to the study of evolvable genomes in evolutionary systems --\ngenomes with a strong capacity to adapt -- and propose that meta-learning and\nadaptive evolvability optimize for the same objective: high performance after a\nset of learning iterations. We argue that population-based evolutionary systems\nwith non-static fitness landscapes naturally bias towards high-evolvability\ngenomes, and therefore optimize for populations with strong learning ability.\nWe demonstrate this claim with a simple evolutionary algorithm,\nPopulation-Based Meta Learning (PBML), that consistently discovers genomes\nwhich display higher rates of improvement over generations, and can rapidly\nadapt to solve sparse fitness and robotic control tasks.",
    "published_date": "2021-03-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.NE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.06435v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.06413v1",
    "title": "FairFil: Contrastive Neural Debiasing Method for Pretrained Text Encoders",
    "authors": [
      "Pengyu Cheng",
      "Weituo Hao",
      "Siyang Yuan",
      "Shijing Si",
      "Lawrence Carin"
    ],
    "author_ids": [],
    "abstract": "Pretrained text encoders, such as BERT, have been applied increasingly in\nvarious natural language processing (NLP) tasks, and have recently demonstrated\nsignificant performance gains. However, recent studies have demonstrated the\nexistence of social bias in these pretrained NLP models. Although prior works\nhave made progress on word-level debiasing, improved sentence-level fairness of\npretrained encoders still lacks exploration. In this paper, we proposed the\nfirst neural debiasing method for a pretrained sentence encoder, which\ntransforms the pretrained encoder outputs into debiased representations via a\nfair filter (FairFil) network. To learn the FairFil, we introduce a contrastive\nlearning framework that not only minimizes the correlation between filtered\nembeddings and bias words but also preserves rich semantic information of the\noriginal sentences. On real-world datasets, our FairFil effectively reduces the\nbias degree of pretrained text encoders, while continuously showing desirable\nperformance on downstream tasks. Moreover, our post-hoc method does not require\nany retraining of the text encoders, further enlarging FairFil's application\nspace.",
    "published_date": "2021-03-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.06413v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.15552v5",
    "title": "Energy Decay Network (EDeN)",
    "authors": [
      "Jamie Nicholas Shelley",
      "Optishell Consultancy"
    ],
    "author_ids": [],
    "abstract": "This paper and accompanying Python and C++ Framework is the product of the\nauthors perceived problems with narrow (Discrimination based) AI. (Artificial\nIntelligence) The Framework attempts to develop a genetic transfer of\nexperience through potential structural expressions using a common\nregulation/exchange value (energy) to create a model whereby neural\narchitecture and all unit processes are co-dependently developed by genetic and\nreal time signal processing influences; successful routes are defined by\nstability of the spike distribution per epoch which is influenced by\ngenetically encoded morphological development biases.These principles are aimed\ntowards creating a diverse and robust network that is capable of adapting to\ngeneral tasks by training within a simulation designed for transfer learning to\nother mediums at scale.",
    "published_date": "2021-03-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.15552v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.06364v1",
    "title": "User-centered Evaluation of Popularity Bias in Recommender Systems",
    "authors": [
      "Himan Abdollahpouri",
      "Masoud Mansoury",
      "Robin Burke",
      "Bamshad Mobasher",
      "Edward Malthouse"
    ],
    "author_ids": [],
    "abstract": "Recommendation and ranking systems are known to suffer from popularity bias;\nthe tendency of the algorithm to favor a few popular items while\nunder-representing the majority of other items. Prior research has examined\nvarious approaches for mitigating popularity bias and enhancing the\nrecommendation of long-tail, less popular, items. The effectiveness of these\napproaches is often assessed using different metrics to evaluate the extent to\nwhich over-concentration on popular items is reduced. However, not much\nattention has been given to the user-centered evaluation of this bias; how\ndifferent users with different levels of interest towards popular items are\naffected by such algorithms. In this paper, we show the limitations of the\nexisting metrics to evaluate popularity bias mitigation when we want to assess\nthese algorithms from the users' perspective and we propose a new metric that\ncan address these limitations. In addition, we present an effective approach\nthat mitigates popularity bias from the user-centered point of view. Finally,\nwe investigate several state-of-the-art approaches proposed in recent years to\nmitigate popularity bias and evaluate their performances using the existing\nmetrics and also from the users' perspective. Our experimental results using\ntwo publicly-available datasets show that existing popularity bias mitigation\ntechniques ignore the users' tolerance towards popular items. Our proposed\nuser-centered method can tackle popularity bias effectively for different users\nwhile also improving the existing metrics.",
    "published_date": "2021-03-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.06364v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.06172v2",
    "title": "Fairness On The Ground: Applying Algorithmic Fairness Approaches to Production Systems",
    "authors": [
      "Chloé Bakalar",
      "Renata Barreto",
      "Stevie Bergman",
      "Miranda Bogen",
      "Bobbie Chern",
      "Sam Corbett-Davies",
      "Melissa Hall",
      "Isabel Kloumann",
      "Michelle Lam",
      "Joaquin Quiñonero Candela",
      "Manish Raghavan",
      "Joshua Simons",
      "Jonathan Tannen",
      "Edmund Tong",
      "Kate Vredenburgh",
      "Jiejing Zhao"
    ],
    "author_ids": [],
    "abstract": "Many technical approaches have been proposed for ensuring that decisions made\nby machine learning systems are fair, but few of these proposals have been\nstress-tested in real-world systems. This paper presents an example of one\nteam's approach to the challenge of applying algorithmic fairness approaches to\ncomplex production systems within the context of a large technology company. We\ndiscuss how we disentangle normative questions of product and policy design\n(like, \"how should the system trade off between different stakeholders'\ninterests and needs?\") from empirical questions of system implementation (like,\n\"is the system achieving the desired tradeoff in practice?\"). We also present\nan approach for answering questions of the latter sort, which allows us to\nmeasure how machine learning systems and human labelers are making these\ntradeoffs across different relevant groups. We hope our experience integrating\nfairness tools and approaches into large-scale and complex production systems\nwill be useful to other practitioners facing similar challenges, and\nilluminating to academics and researchers looking to better address the needs\nof practitioners.",
    "published_date": "2021-03-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.06172v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.09058v2",
    "title": "Understanding the Representation and Representativeness of Age in AI Data Sets",
    "authors": [
      "Joon Sung Park",
      "Michael S. Bernstein",
      "Robin N. Brewer",
      "Ece Kamar",
      "Meredith Ringel Morris"
    ],
    "author_ids": [],
    "abstract": "A diverse representation of different demographic groups in AI training data\nsets is important in ensuring that the models will work for a large range of\nusers. To this end, recent efforts in AI fairness and inclusion have advocated\nfor creating AI data sets that are well-balanced across race, gender,\nsocioeconomic status, and disability status. In this paper, we contribute to\nthis line of work by focusing on the representation of age by asking whether\nolder adults are represented proportionally to the population at large in AI\ndata sets. We examine publicly-available information about 92 face data sets to\nunderstand how they codify age as a case study to investigate how the subjects'\nages are recorded and whether older generations are represented. We find that\nolder adults are very under-represented; five data sets in the study that\nexplicitly documented the closed age intervals of their subjects included older\nadults (defined as older than 65 years), while only one included oldest-old\nadults (defined as older than 85 years). Additionally, we find that only 24 of\nthe data sets include any age-related information in their documentation or\nmetadata, and that there is no consistent method followed across these data\nsets to collect and record the subjects' ages. We recognize the unique\ndifficulties in creating representative data sets in terms of age, but raise it\nas an important dimension that researchers and engineers interested in\ninclusive AI should consider.",
    "published_date": "2021-03-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.09058v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.05866v3",
    "title": "An Incentive Mechanism for Sustainable Blockchain Storage",
    "authors": [
      "Yunshu Liu",
      "Zhixuan Fang",
      "Man Hon Cheung",
      "Wei Cai",
      "Jianwei Huang"
    ],
    "author_ids": [],
    "abstract": "Miners in a blockchain system are suffering from ever-increasing storage\ncosts, which in general have not been properly compensated by the users'\ntransaction fees. This reduces the incentives for the miners' participation and\nmay jeopardize the blockchain security. We propose to mitigate this blockchain\ninsufficient fee issue through a Fee and Waiting Tax (FWT) mechanism, which\nexplicitly considers the two types of negative externalities in the system.\nSpecifically, we model the interactions between the protocol designer, users,\nand miners as a three-stage Stackelberg game. By characterizing the equilibrium\nof the game, we find that miners neglecting the negative externality in\ntransaction selection cause they are willing to accept insufficient-fee\ntransactions. This leads to the insufficient storage fee issue in the existing\nprotocol. Moreover, our proposed optimal FWT mechanism can motivate users to\npay sufficient transaction fees to cover the storage costs and achieve the\nunconstrained social optimum. Numerical results show that the optimal FWT\nmechanism guarantees sufficient transaction fees and achieves an average social\nwelfare improvement of 33.73\\% or more over the existing protocol. Furthermore,\nthe optimal FWT mechanism achieves the maximum fairness index and performs well\neven under heterogeneous-storage-cost miners.",
    "published_date": "2021-03-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.05866v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.05841v1",
    "title": "Interpretable bias mitigation for textual data: Reducing gender bias in patient notes while maintaining classification performance",
    "authors": [
      "Joshua R. Minot",
      "Nicholas Cheney",
      "Marc Maier",
      "Danne C. Elbers",
      "Christopher M. Danforth",
      "Peter Sheridan Dodds"
    ],
    "author_ids": [],
    "abstract": "Medical systems in general, and patient treatment decisions and outcomes in\nparticular, are affected by bias based on gender and other demographic\nelements. As language models are increasingly applied to medicine, there is a\ngrowing interest in building algorithmic fairness into processes impacting\npatient care. Much of the work addressing this question has focused on biases\nencoded in language models -- statistical estimates of the relationships\nbetween concepts derived from distant reading of corpora. Building on this\nwork, we investigate how word choices made by healthcare practitioners and\nlanguage models interact with regards to bias. We identify and remove gendered\nlanguage from two clinical-note datasets and describe a new debiasing procedure\nusing BERT-based gender classifiers. We show minimal degradation in health\ncondition classification tasks for low- to medium-levels of bias removal via\ndata augmentation. Finally, we compare the bias semantically encoded in the\nlanguage models with the bias empirically observed in health records. This work\noutlines an interpretable approach for using data augmentation to identify and\nreduce the potential for bias in natural language processing pipelines.",
    "published_date": "2021-03-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.05841v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.05741v1",
    "title": "Non-asymptotic Confidence Intervals of Off-policy Evaluation: Primal and Dual Bounds",
    "authors": [
      "Yihao Feng",
      "Ziyang Tang",
      "Na Zhang",
      "Qiang Liu"
    ],
    "author_ids": [],
    "abstract": "Off-policy evaluation (OPE) is the task of estimating the expected reward of\na given policy based on offline data previously collected under different\npolicies. Therefore, OPE is a key step in applying reinforcement learning to\nreal-world domains such as medical treatment, where interactive data collection\nis expensive or even unsafe. As the observed data tends to be noisy and\nlimited, it is essential to provide rigorous uncertainty quantification, not\njust a point estimation, when applying OPE to make high stakes decisions. This\nwork considers the problem of constructing non-asymptotic confidence intervals\nin infinite-horizon off-policy evaluation, which remains a challenging open\nquestion. We develop a practical algorithm through a primal-dual\noptimization-based approach, which leverages the kernel Bellman loss (KBL) of\nFeng et al.(2019) and a new martingale concentration inequality of KBL\napplicable to time-dependent data with unknown mixing conditions. Our algorithm\nmakes minimum assumptions on the data and the function class of the Q-function,\nand works for the behavior-agnostic settings where the data is collected under\na mix of arbitrary unknown behavior policies. We present empirical results that\nclearly demonstrate the advantages of our approach over existing methods.",
    "published_date": "2021-03-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.05741v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.05434v2",
    "title": "When is it permissible for artificial intelligence to lie? A trust-based approach",
    "authors": [
      "Tae Wan Kim",
      "Tong",
      "Lu",
      "Kyusong Lee",
      "Zhaoqi Cheng",
      "Yanhan Tang",
      "John Hooker"
    ],
    "author_ids": [],
    "abstract": "Conversational Artificial Intelligence (AI) used in industry settings can be\ntrained to closely mimic human behaviors, including lying and deception.\nHowever, lying is often a necessary part of negotiation. To address this, we\ndevelop a normative framework for when it is ethical or unethical for a\nconversational AI to lie to humans, based on whether there is what we call\n\"invitation of trust\" in a particular scenario. Importantly, cultural norms\nplay an important role in determining whether there is invitation of trust\nacross negotiation settings, and thus an AI trained in one culture may not be\ngeneralizable to others. Moreover, individuals may have different expectations\nregarding the invitation of trust and propensity to lie for human vs. AI\nnegotiators, and these expectations may vary across cultures as well. Finally,\nwe outline how a conversational chatbot can be trained to negotiate ethically\nby applying autoregressive models to large dialog and negotiations datasets.",
    "published_date": "2021-03-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.05434v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.05271v2",
    "title": "Probabilistic Modeling of Semantic Ambiguity for Scene Graph Generation",
    "authors": [
      "Gengcong Yang",
      "Jingyi Zhang",
      "Yong Zhang",
      "Baoyuan Wu",
      "Yujiu Yang"
    ],
    "author_ids": [],
    "abstract": "To generate \"accurate\" scene graphs, almost all existing methods predict\npairwise relationships in a deterministic manner. However, we argue that visual\nrelationships are often semantically ambiguous. Specifically, inspired by\nlinguistic knowledge, we classify the ambiguity into three types: Synonymy\nAmbiguity, Hyponymy Ambiguity, and Multi-view Ambiguity. The ambiguity\nnaturally leads to the issue of \\emph{implicit multi-label}, motivating the\nneed for diverse predictions. In this work, we propose a novel plug-and-play\nProbabilistic Uncertainty Modeling (PUM) module. It models each union region as\na Gaussian distribution, whose variance measures the uncertainty of the\ncorresponding visual content. Compared to the conventional deterministic\nmethods, such uncertainty modeling brings stochasticity of feature\nrepresentation, which naturally enables diverse predictions. As a byproduct,\nPUM also manages to cover more fine-grained relationships and thus alleviates\nthe issue of bias towards frequent relationships. Extensive experiments on the\nlarge-scale Visual Genome benchmark show that combining PUM with newly proposed\nResCAGCN can achieve state-of-the-art performances, especially under the mean\nrecall metric. Furthermore, we prove the universal effectiveness of PUM by\nplugging it into some existing models and provide insightful analysis of its\nability to generate diverse yet plausible visual relationships.",
    "published_date": "2021-03-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.05271v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.05134v5",
    "title": "Constrained Learning with Non-Convex Losses",
    "authors": [
      "Luiz F. O. Chamon",
      "Santiago Paternain",
      "Miguel Calvo-Fullana",
      "Alejandro Ribeiro"
    ],
    "author_ids": [],
    "abstract": "Though learning has become a core component of modern information processing,\nthere is now ample evidence that it can lead to biased, unsafe, and prejudiced\nsystems. The need to impose requirements on learning is therefore paramount,\nespecially as it reaches critical applications in social, industrial, and\nmedical domains. However, the non-convexity of most modern statistical problems\nis only exacerbated by the introduction of constraints. Whereas good\nunconstrained solutions can often be learned using empirical risk minimization,\neven obtaining a model that satisfies statistical constraints can be\nchallenging. All the more so, a good one. In this paper, we overcome this issue\nby learning in the empirical dual domain, where constrained statistical\nlearning problems become unconstrained and deterministic. We analyze the\ngeneralization properties of this approach by bounding the empirical duality\ngap -- i.e., the difference between our approximate, tractable solution and the\nsolution of the original (non-convex) statistical problem -- and provide a\npractical constrained learning algorithm. These results establish a constrained\ncounterpart to classical learning theory, enabling the explicit use of\nconstraints in learning. We illustrate this theory and algorithm in\nrate-constrained learning applications arising in fairness and adversarial\nrobustness.",
    "published_date": "2021-03-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.05134v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.04963v1",
    "title": "Reducing quantum annealing biases for solving the graph partitioning problem",
    "authors": [
      "Elijah Pelofske",
      "Georg Hahn",
      "Hristo N. Djidjev"
    ],
    "author_ids": [],
    "abstract": "Quantum annealers offer an efficient way to compute high quality solutions of\nNP-hard problems when expressed in a QUBO (quadratic unconstrained binary\noptimization) or an Ising form. This is done by mapping a problem onto the\nphysical qubits and couplers of the quantum chip, from which a solution is read\nafter a process called quantum annealing. However, this process is subject to\nmultiple sources of biases, including poor calibration, leakage between\nadjacent qubits, control biases, etc., which might negatively influence the\nquality of the annealing results. In this work, we aim at mitigating the effect\nof such biases for solving constrained optimization problems, by offering a\ntwo-step method, and apply it to Graph Partitioning. In the first step, we\nmeasure and reduce any biases that result from implementing the constraints of\nthe problem. In the second, we add the objective function to the resulting\nbias-corrected implementation of the constraints, and send the problem to the\nquantum annealer. We apply this concept to Graph Partitioning, an important\nNP-hard problem, which asks to find a partition of the vertices of a graph that\nis balanced (the constraint) and minimizes the cut size (the objective). We\nfirst quantify the bias of the implementation of the constraint on the quantum\nannealer, that is, we require, in an unbiased implementation, that any two\nvertices have the same likelihood of being assigned to the same or to different\nparts of the partition. We then propose an iterative method to correct any such\nbiases. We demonstrate that, after adding the objective, solving the resulting\nbias-corrected Ising problem on the quantum annealer results in a higher\nsolution accuracy.",
    "published_date": "2021-03-08T00:00:00",
    "year": 2021,
    "categories": [
      "quant-ph",
      "cs.ET"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.04963v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.04951v1",
    "title": "A Comparative Approach to Explainable Artificial Intelligence Methods in Application to High-Dimensional Electronic Health Records: Examining the Usability of XAI",
    "authors": [
      "Jamie Andrew Duell"
    ],
    "author_ids": [],
    "abstract": "Explainable Artificial Intelligence (XAI) is a rising field in AI. It aims to\nproduce a demonstrative factor of trust, which for human subjects is achieved\nthrough communicative means, which Machine Learning (ML) algorithms cannot\nsolely produce, illustrating the necessity of an extra layer producing support\nto the model output. When approaching the medical field, we can see challenges\narise when dealing with the involvement of human-subjects, the ideology behind\ntrusting a machine to tend towards the livelihood of a human poses an ethical\nconundrum - leaving trust as the basis of the human-expert in acceptance to the\nmachines decision. The aim of this paper is to apply XAI methods to demonstrate\nthe usability of explainable architectures as a tertiary layer for the medical\ndomain supporting ML predictions and human-expert opinion, XAI methods produce\nvisualization of the feature contribution towards a given models output on both\na local and global level. The work in this paper uses XAI to determine feature\nimportance towards high-dimensional data-driven questions to inform\ndomain-experts of identifiable trends with a comparison of model-agnostic\nmethods in application to ML algorithms. The performance metrics for a\nglass-box method is also provided as a comparison against black-box capability\nfor tabular data. Future work will aim to produce a user-study using metrics to\nevaluate human-expert usability and opinion of the given models.",
    "published_date": "2021-03-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.04951v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.11790v3",
    "title": "Large Pre-trained Language Models Contain Human-like Biases of What is Right and Wrong to Do",
    "authors": [
      "Patrick Schramowski",
      "Cigdem Turan",
      "Nico Andersen",
      "Constantin A. Rothkopf",
      "Kristian Kersting"
    ],
    "author_ids": [],
    "abstract": "Artificial writing is permeating our lives due to recent advances in\nlarge-scale, transformer-based language models (LMs) such as BERT, its\nvariants, GPT-2/3, and others. Using them as pre-trained models and fine-tuning\nthem for specific tasks, researchers have extended state of the art for many\nNLP tasks and shown that they capture not only linguistic knowledge but also\nretain general knowledge implicitly present in the data. Unfortunately, LMs\ntrained on unfiltered text corpora suffer from degenerated and biased\nbehaviour. While this is well established, we show that recent LMs also contain\nhuman-like biases of what is right and wrong to do, some form of ethical and\nmoral norms of the society -- they bring a \"moral direction\" to surface. That\nis, we show that these norms can be captured geometrically by a direction,\nwhich can be computed, e.g., by a PCA, in the embedding space, reflecting well\nthe agreement of phrases to social norms implicitly expressed in the training\ntexts and providing a path for attenuating or even preventing toxic\ndegeneration in LMs. Being able to rate the (non-)normativity of arbitrary\nphrases without explicitly training the LM for this task, we demonstrate the\ncapabilities of the \"moral direction\" for guiding (even other) LMs towards\nproducing normative text and showcase it on RealToxicityPrompts testbed,\npreventing the neural toxic degeneration in GPT-2.",
    "published_date": "2021-03-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.11790v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.04850v6",
    "title": "Quantifying Ignorance in Individual-Level Causal-Effect Estimates under Hidden Confounding",
    "authors": [
      "Andrew Jesson",
      "Sören Mindermann",
      "Yarin Gal",
      "Uri Shalit"
    ],
    "author_ids": [],
    "abstract": "We study the problem of learning conditional average treatment effects (CATE)\nfrom high-dimensional, observational data with unobserved confounders.\nUnobserved confounders introduce ignorance -- a level of unidentifiability --\nabout an individual's response to treatment by inducing bias in CATE estimates.\nWe present a new parametric interval estimator suited for high-dimensional\ndata, that estimates a range of possible CATE values when given a predefined\nbound on the level of hidden confounding. Further, previous interval estimators\ndo not account for ignorance about the CATE associated with samples that may be\nunderrepresented in the original study, or samples that violate the overlap\nassumption. Our interval estimator also incorporates model uncertainty so that\npractitioners can be made aware of out-of-distribution data. We prove that our\nestimator converges to tight bounds on CATE when there may be unobserved\nconfounding, and assess it using semi-synthetic, high-dimensional datasets.",
    "published_date": "2021-03-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.04850v6",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.04757v1",
    "title": "A Study on Fairness and Trust Perceptions in Automated Decision Making",
    "authors": [
      "Jakob Schoeffer",
      "Yvette Machowski",
      "Niklas Kuehl"
    ],
    "author_ids": [],
    "abstract": "Automated decision systems are increasingly used for consequential decision\nmaking -- for a variety of reasons. These systems often rely on sophisticated\nyet opaque models, which do not (or hardly) allow for understanding how or why\na given decision was arrived at. This is not only problematic from a legal\nperspective, but non-transparent systems are also prone to yield undesirable\n(e.g., unfair) outcomes because their sanity is difficult to assess and\ncalibrate in the first place. In this work, we conduct a study to evaluate\ndifferent attempts of explaining such systems with respect to their effect on\npeople's perceptions of fairness and trustworthiness towards the underlying\nmechanisms. A pilot study revealed surprising qualitative insights as well as\npreliminary significant effects, which will have to be verified, extended and\nthoroughly discussed in the larger main study.",
    "published_date": "2021-03-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.04757v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.04333v2",
    "title": "Measuring Discrimination to Boost Comparative Testing for Multiple Deep Learning Models",
    "authors": [
      "Linghan Meng",
      "Yanhui Li",
      "Lin Chen",
      "Zhi Wang",
      "Di Wu",
      "Yuming Zhou",
      "Baowen Xu"
    ],
    "author_ids": [],
    "abstract": "The boom of DL technology leads to massive DL models built and shared, which\nfacilitates the acquisition and reuse of DL models. For a given task, we\nencounter multiple DL models available with the same functionality, which are\nconsidered as candidates to achieve this task. Testers are expected to compare\nmultiple DL models and select the more suitable ones w.r.t. the whole testing\ncontext. Due to the limitation of labeling effort, testers aim to select an\nefficient subset of samples to make an as precise rank estimation as possible\nfor these models. To tackle this problem, we propose Sample Discrimination\nbased Selection (SDS) to select efficient samples that could discriminate\nmultiple models, i.e., the prediction behaviors (right/wrong) of these samples\nwould be helpful to indicate the trend of model performance. To evaluate SDS,\nwe conduct an extensive empirical study with three widely-used image datasets\nand 80 real world DL models. The experimental results show that, compared with\nstate-of-the-art baseline methods, SDS is an effective and efficient sample\nselection method to rank multiple DL models.",
    "published_date": "2021-03-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SE",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.04333v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.04314v1",
    "title": "Expert System Gradient Descent Style Training: Development of a Defensible Artificial Intelligence Technique",
    "authors": [
      "Jeremy Straub"
    ],
    "author_ids": [],
    "abstract": "Artificial intelligence systems, which are designed with a capability to\nlearn from the data presented to them, are used throughout society. These\nsystems are used to screen loan applicants, make sentencing recommendations for\ncriminal defendants, scan social media posts for disallowed content and more.\nBecause these systems don't assign meaning to their complex learned correlation\nnetwork, they can learn associations that don't equate to causality, resulting\nin non-optimal and indefensible decisions being made. In addition to making\ndecisions that are sub-optimal, these systems may create legal liability for\ntheir designers and operators by learning correlations that violate\nanti-discrimination and other laws regarding what factors can be used in\ndifferent types of decision making. This paper presents the use of a machine\nlearning expert system, which is developed with meaning-assigned nodes (facts)\nand correlations (rules). Multiple potential implementations are considered and\nevaluated under different conditions, including different network error and\naugmentation levels and different training levels. The performance of these\nsystems is compared to random and fully connected networks.",
    "published_date": "2021-03-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.04314v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.04304v4",
    "title": "Fair-Share Allocations for Agents with Arbitrary Entitlements",
    "authors": [
      "Moshe Babaioff",
      "Tomer Ezra",
      "Uriel Feige"
    ],
    "author_ids": [],
    "abstract": "We consider the problem of fair allocation of indivisible goods to $n$\nagents, with no transfers. When agents have equal entitlements, the well\nestablished notion of the maximin share (MMS) serves as an attractive fairness\ncriterion, where to qualify as fair, an allocation needs to give every agent at\nleast a substantial fraction of her MMS.\n  In this paper we consider the case of arbitrary (unequal) entitlements. We\nexplain shortcomings in previous attempts that extend the MMS to unequal\nentitlements. Our conceptual contribution is the introduction of a new notion\nof a share, the AnyPrice share (APS), that is appropriate for settings with\narbitrary entitlements. Even for the equal entitlements case, this notion is\nnew, and satisfies $APS \\ge MMS$, where the inequality is sometimes strict. We\npresent two equivalent definitions for the APS (one as a minimization problem,\nthe other as a maximization problem), and provide comparisons between the APS\nand previous notions of fairness.\n  Our main result concerns additive valuations and arbitrary entitlements, for\nwhich we provide a polynomial-time algorithm that gives every agent at least a\n$\\frac{3}{5}$-fraction of her APS. This algorithm can also be viewed as\nproviding strategies in a certain natural bidding game, and these strategies\nsecure each agent at least a $\\frac{3}{5}$-fraction of her APS.",
    "published_date": "2021-03-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.04304v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.04278v1",
    "title": "Routing Towards Discriminative Power of Class Capsules",
    "authors": [
      "Haoyu Yang",
      "Shuhe Li",
      "Bei Yu"
    ],
    "author_ids": [],
    "abstract": "Capsule networks are recently proposed as an alternative to modern neural\nnetwork architectures. Neurons are replaced with capsule units that represent\nspecific features or entities with normalized vectors or matrices. The\nactivation of lower layer capsules affects the behavior of the following\ncapsules via routing links that are constructed during training via certain\nrouting algorithms. We discuss the routing-by-agreement scheme in dynamic\nrouting algorithm which, in certain cases, leads the networks away from\noptimality. To obtain better and faster convergence, we propose a routing\nalgorithm that incorporates a regularized quadratic programming problem which\ncan be solved efficiently. Particularly, the proposed routing algorithm targets\ndirectly on the discriminative power of class capsules making the correct\ndecision on input instances. We conduct experiments on MNIST, MNIST-Fashion,\nand CIFAR-10 and show competitive classification results compared to existing\ncapsule networks.",
    "published_date": "2021-03-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.04278v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.04266v5",
    "title": "Resource Distribution Under Spatiotemporal Uncertainty of Disease Spread: Stochastic versus Robust Approaches",
    "authors": [
      "Beste Basciftci",
      "Xian Yu",
      "Siqian Shen"
    ],
    "author_ids": [],
    "abstract": "We consider the problem of optimizing locations of distribution centers (DCs)\nand plans for distributing resources such as test kits and vaccines, under\nspatiotemporal uncertainties of disease spread and demand for the resources. We\naim to balance the operational cost (including costs of deploying facilities,\nshipping, and storage) and quality of service (reflected by demand coverage),\nwhile ensuring equity and fairness of resource distribution across multiple\npopulations. We compare a sample-based stochastic programming (SP) approach\nwith a distributionally robust optimization (DRO) approach using a moment-based\nambiguity set. Numerical studies are conducted on instances of distributing\nCOVID-19 vaccines in the United States and test kits, to compare SP and DRO\nmodels with a deterministic formulation using estimated demand and with the\ncurrent resource distribution plans implemented in the US. We demonstrate the\nresults over distinct phases of the pandemic to estimate the cost and speed of\nresource distribution depending on scale and coverage, and show the\n``demand-driven'' properties of the SP and DRO solutions. Our results further\nindicate that if the worst-case unmet demand is prioritized, then the DRO\napproach is preferred despite of its higher overall cost. Nevertheless, the SP\napproach can provide an intermediate plan under budgetary restrictions without\nsignificant compromises in demand coverage.",
    "published_date": "2021-03-07T00:00:00",
    "year": 2021,
    "categories": [
      "math.OC",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.04266v5",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.04243v2",
    "title": "Estimating and Improving Fairness with Adversarial Learning",
    "authors": [
      "Xiaoxiao Li",
      "Ziteng Cui",
      "Yifan Wu",
      "Lin Gu",
      "Tatsuya Harada"
    ],
    "author_ids": [],
    "abstract": "Fairness and accountability are two essential pillars for trustworthy\nArtificial Intelligence (AI) in healthcare. However, the existing AI model may\nbe biased in its decision marking. To tackle this issue, we propose an\nadversarial multi-task training strategy to simultaneously mitigate and detect\nbias in the deep learning-based medical image analysis system. Specifically, we\npropose to add a discrimination module against bias and a critical module that\npredicts unfairness within the base classification model. We further impose an\northogonality regularization to force the two modules to be independent during\ntraining. Hence, we can keep these deep learning tasks distinct from one\nanother, and avoid collapsing them into a singular point on the manifold.\nThrough this adversarial training method, the data from the underprivileged\ngroup, which is vulnerable to bias because of attributes such as sex and skin\ntone, are transferred into a domain that is neutral relative to these\nattributes. Furthermore, the critical module can predict fairness scores for\nthe data with unknown sensitive attributes. We evaluate our framework on a\nlarge-scale public-available skin lesion dataset under various fairness\nevaluation metrics. The experiments demonstrate the effectiveness of our\nproposed method for estimating and improving fairness in the deep\nlearning-based medical image analysis system.",
    "published_date": "2021-03-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.04243v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.04180v2",
    "title": "Neural networks can understand compositional functions that humans do not, in the context of emergent communication",
    "authors": [
      "Hugh Perkins"
    ],
    "author_ids": [],
    "abstract": "We show that it is possible to craft transformations that, applied to\ncompositional grammars, result in grammars that neural networks can learn\neasily, but humans do not. This could explain the disconnect between current\nmetrics of compositionality, that are arguably human-centric, and the ability\nof neural networks to generalize to unseen examples. We propose to use the\ntransformations as a benchmark, ICY, which could be used to measure aspects of\nthe compositional inductive bias of networks, and to search for networks with\nsimilar compositional inductive biases to humans. As an example of this\napproach, we propose a hierarchical model, HU-RNN, which shows an inductive\nbias towards position-independent, word-like groups of tokens.",
    "published_date": "2021-03-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.04180v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.04142v4",
    "title": "On an innovative architecture for digital immunity passports and vaccination certificates",
    "authors": [
      "John C. Polley",
      "Ilias Politis",
      "Christos Xenakis",
      "Adarbad Master",
      "Michał Kępkowski"
    ],
    "author_ids": [],
    "abstract": "With the COVID-19 pandemic entering a second phase and vaccination strategies\nbeing applied by countries and governments worldwide, there is an increasing\nexpectation by people to return to normal life. There is currently a debate\nabout immunity passports, privacy, and the enablement of individuals to safely\nenter everyday social life, workplace, and travel. Such digital immunity\npassports and vaccination certificates should meet people's expectations for\nprivacy while enabling them to present to 3rd party verifiers tamper-evident\ncredentials. This paper provides a comprehensive answer to the technological,\nethical and security challenges, by proposing an architecture that provides to\nindividuals, employers, and government agencies, a digital, decentralized,\nportable, immutable, and non-refutable health status cryptographic proof. It\ncan be used to evaluate the risk of allowing individuals to return to work,\ntravel, and public life activities.",
    "published_date": "2021-03-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CR",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.04142v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.04048v1",
    "title": "Fairness in TabNet Model by Disentangled Representation for the Prediction of Hospital No-Show",
    "authors": [
      "Sabri Boughorbel",
      "Fethi Jarray",
      "Abdou Kadri"
    ],
    "author_ids": [],
    "abstract": "Patient no-shows is a major burden for health centers leading to loss of\nrevenue, increased waiting time and deteriorated health outcome. Developing\nmachine learning (ML) models for the prediction of no -shows could help\naddressing this important issue. It is crucial to consider fair ML models for\nno-show prediction in order to ensure equality of opportunity in accessing\nhealthcare services. In this wo rk, we are interested in developing deep\nlearning models for no-show prediction based on tabular data while ensuring\nfairness properties. Our baseline model, TabNet, uses on attentive feature\ntransforme rs and has shown promising results for tabular data. We propose\nFair-TabNet based on representation learning that disentangles predictive from\nsensitive components. The model is trained to jointly min imize loss functions\non no-shows and sensitive variables while ensuring that the sensitive and\nprediction representations are orthogonal. In the experimental analysis, we\nused a hospital dataset of 210, 000 appointments collected in 2019. Our\npreliminary results show that the proposed Fair-TabNet improves the predictive,\nfairness performance and convergence speed over TabNet for the task of\nappointment no-show prediction. The comparison with the state-of-the art models\nfor tabular data shows promising results and could be further improved by a\nbetter tuning of hyper-parameters.",
    "published_date": "2021-03-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.04048v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.04021v3",
    "title": "Asymptotic Theory for IV-Based Reinforcement Learning with Potential Endogeneity",
    "authors": [
      "Jin Li",
      "Ye Luo",
      "Zigan Wang",
      "Xiaowei Zhang"
    ],
    "author_ids": [],
    "abstract": "In the standard data analysis framework, data is collected (once and for\nall), and then data analysis is carried out. However, with the advancement of\ndigital technology, decision-makers constantly analyze past data and generate\nnew data through their decisions. We model this as a Markov decision process\nand show that the dynamic interaction between data generation and data analysis\nleads to a new type of bias -- reinforcement bias -- that exacerbates the\nendogeneity problem in standard data analysis. We propose a class of instrument\nvariable (IV)-based reinforcement learning (RL) algorithms to correct for the\nbias and establish their theoretical properties by incorporating them into a\nstochastic approximation (SA) framework. Our analysis accommodates\niterate-dependent Markovian structures and, therefore, can be used to study RL\nalgorithms with policy improvement. We also provide formulas for inference on\noptimal policies of the IV-RL algorithms. These formulas highlight how\nintertemporal dependencies of the Markovian environment affect the inference.",
    "published_date": "2021-03-06T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG",
      "econ.EM",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.04021v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.03975v2",
    "title": "Global canopy height regression and uncertainty estimation from GEDI LIDAR waveforms with deep ensembles",
    "authors": [
      "Nico Lang",
      "Nikolai Kalischek",
      "John Armston",
      "Konrad Schindler",
      "Ralph Dubayah",
      "Jan Dirk Wegner"
    ],
    "author_ids": [],
    "abstract": "NASA's Global Ecosystem Dynamics Investigation (GEDI) is a key climate\nmission whose goal is to advance our understanding of the role of forests in\nthe global carbon cycle. While GEDI is the first space-based LIDAR explicitly\noptimized to measure vertical forest structure predictive of aboveground\nbiomass, the accurate interpretation of this vast amount of waveform data\nacross the broad range of observational and environmental conditions is\nchallenging. Here, we present a novel supervised machine learning approach to\ninterpret GEDI waveforms and regress canopy top height globally. We propose a\nprobabilistic deep learning approach based on an ensemble of deep convolutional\nneural networks(CNN) to avoid the explicit modelling of unknown effects, such\nas atmospheric noise. The model learns to extract robust features that\ngeneralize to unseen geographical regions and, in addition, yields reliable\nestimates of predictive uncertainty. Ultimately, the global canopy top height\nestimates produced by our model have an expected RMSE of 2.7 m with low bias.",
    "published_date": "2021-03-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.03975v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.03872v1",
    "title": "Rissanen Data Analysis: Examining Dataset Characteristics via Description Length",
    "authors": [
      "Ethan Perez",
      "Douwe Kiela",
      "Kyunghyun Cho"
    ],
    "author_ids": [],
    "abstract": "We introduce a method to determine if a certain capability helps to achieve\nan accurate model of given data. We view labels as being generated from the\ninputs by a program composed of subroutines with different capabilities, and we\nposit that a subroutine is useful if and only if the minimal program that\ninvokes it is shorter than the one that does not. Since minimum program length\nis uncomputable, we instead estimate the labels' minimum description length\n(MDL) as a proxy, giving us a theoretically-grounded method for analyzing\ndataset characteristics. We call the method Rissanen Data Analysis (RDA) after\nthe father of MDL, and we showcase its applicability on a wide variety of\nsettings in NLP, ranging from evaluating the utility of generating subquestions\nbefore answering a question, to analyzing the value of rationales and\nexplanations, to investigating the importance of different parts of speech, and\nuncovering dataset gender bias.",
    "published_date": "2021-03-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.03872v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.11762v3",
    "title": "Complexity-based permutation entropies: from deterministic time series to white noise",
    "authors": [
      "J. M. Amigó",
      "R. Dale",
      "P. Tempesta"
    ],
    "author_ids": [],
    "abstract": "This is a paper in the intersection of time series analysis and complexity\ntheory that presents new results on permutation complexity in general and\npermutation entropy in particular. In this context, permutation complexity\nrefers to the characterization of time series by means of ordinal patterns\n(permutations), entropic measures, decay rates of missing ordinal patterns, and\nmore. Since the inception of this \\textquotedblleft ordinal\\textquotedblright\\\nmethodology, its practical application to any type of scalar time series and\nreal-valued processes have proven to be simple and useful. However, the\ntheoretical aspects have remained limited to noiseless deterministic series and\ndynamical systems, the main obstacle being the super-exponential growth of\nvisible permutations with length when randomness (also in form of observational\nnoise) is present in the data. To overcome this difficulty, we take a new\napproach through complexity classes, which are precisely defined by the growth\nof visible permutations with length, regardless of the deterministic or noisy\nnature of the data. We consider three major classes: exponential, sub-factorial\nand factorial. The next step is to adapt the concept of Z-entropy to each of\nthose classes, which we call permutation entropy because it coincides with the\nconventional permutation entropy on the exponential class. Z-entropies are a\nfamily of group entropies, each of them extensive on a given complexity class.\nThe result is a unified approach to the ordinal analysis of deterministic and\nrandom processes, from dynamical systems to white noise, with new concepts and\ntools. Numerical simulations show that permutation entropy discriminates time\nseries from all complexity classes.",
    "published_date": "2021-03-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IT",
      "math.IT",
      "physics.data-an"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.11762v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.03598v2",
    "title": "WordBias: An Interactive Visual Tool for Discovering Intersectional Biases Encoded in Word Embeddings",
    "authors": [
      "Bhavya Ghai",
      "Md Naimul Hoque",
      "Klaus Mueller"
    ],
    "author_ids": [],
    "abstract": "Intersectional bias is a bias caused by an overlap of multiple social factors\nlike gender, sexuality, race, disability, religion, etc. A recent study has\nshown that word embedding models can be laden with biases against\nintersectional groups like African American females, etc. The first step\ntowards tackling such intersectional biases is to identify them. However,\ndiscovering biases against different intersectional groups remains a\nchallenging task. In this work, we present WordBias, an interactive visual tool\ndesigned to explore biases against intersectional groups encoded in static word\nembeddings. Given a pretrained static word embedding, WordBias computes the\nassociation of each word along different groups based on race, age, etc. and\nthen visualizes them using a novel interactive interface. Using a case study,\nwe demonstrate how WordBias can help uncover biases against intersectional\ngroups like Black Muslim Males, Poor Females, etc. encoded in word embedding.\nIn addition, we also evaluate our tool using qualitative feedback from expert\ninterviews. The source code for this tool can be publicly accessed for\nreproducibility at github.com/bhavyaghai/WordBias.",
    "published_date": "2021-03-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.03598v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.03544v2",
    "title": "Challenges of engineering safe and secure highly automated vehicles",
    "authors": [
      "Nadja Marko",
      "Eike Möhlmann",
      "Dejan Ničković",
      "Jürgen Niehaus",
      "Peter Priller",
      "Martijn Rooker"
    ],
    "author_ids": [],
    "abstract": "After more than a decade of intense focus on automated vehicles, we are still\nfacing huge challenges for the vision of fully autonomous driving to become a\nreality. The same \"disillusionment\" is true in many other domains, in which\nautonomous Cyber-Physical Systems (CPS) could considerably help to overcome\nsocietal challenges and be highly beneficial to society and individuals. Taking\nthe automotive domain, i.e. highly automated vehicles (HAV), as an example,\nthis paper sets out to summarize the major challenges that are still to\novercome for achieving safe, secure, reliable and trustworthy highly automated\nresp. autonomous CPS. We constrain ourselves to technical challenges,\nacknowledging the importance of (legal) regulations, certification,\nstandardization, ethics, and societal acceptance, to name but a few, without\ndelving deeper into them as this is beyond the scope of this paper. Four\nchallenges have been identified as being the main obstacles to realizing HAV:\nRealization of continuous, post-deployment systems improvement, handling of\nuncertainties and incomplete information, verification of HAV with machine\nlearning components, and prediction. Each of these challenges is described in\ndetail, including sub-challenges and, where appropriate, possible approaches to\novercome them. By working together in a common effort between industry and\nacademy and focusing on these challenges, the authors hope to contribute to\novercome the \"disillusionment\" for realizing HAV.",
    "published_date": "2021-03-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.03544v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.03417v3",
    "title": "Measuring Model Biases in the Absence of Ground Truth",
    "authors": [
      "Osman Aka",
      "Ken Burke",
      "Alex Bäuerle",
      "Christina Greer",
      "Margaret Mitchell"
    ],
    "author_ids": [],
    "abstract": "The measurement of bias in machine learning often focuses on model\nperformance across identity subgroups (such as man and woman) with respect to\ngroundtruth labels. However, these methods do not directly measure the\nassociations that a model may have learned, for example between labels and\nidentity subgroups. Further, measuring a model's bias requires a fully\nannotated evaluation dataset which may not be easily available in practice. We\npresent an elegant mathematical solution that tackles both issues\nsimultaneously, using image classification as a working example. By treating a\nclassification model's predictions for a given image as a set of labels\nanalogous to a bag of words, we rank the biases that a model has learned with\nrespect to different identity labels. We use (man, woman) as a concrete example\nof an identity label set (although this set need not be binary), and present\nrankings for the labels that are most biased towards one identity or the other.\nWe demonstrate how the statistical properties of different association metrics\ncan lead to different rankings of the most \"gender biased\" labels, and conclude\nthat normalized pointwise mutual information (nPMI) is most useful in practice.\nFinally, we announce an open-sourced nPMI visualization tool using TensorBoard.",
    "published_date": "2021-03-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.03417v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.03391v1",
    "title": "Gemini: Dynamic Bias Correction for Autonomous Experimentation and Molecular Simulation",
    "authors": [
      "Riley J. Hickman",
      "Florian Häse",
      "Loïc M. Roch",
      "Alán Aspuru-Guzik"
    ],
    "author_ids": [],
    "abstract": "Bayesian optimization has emerged as a powerful strategy to accelerate\nscientific discovery by means of autonomous experimentation. However, expensive\nmeasurements are required to accurately estimate materials properties, and can\nquickly become a hindrance to exhaustive materials discovery campaigns. Here,\nwe introduce Gemini: a data-driven model capable of using inexpensive\nmeasurements as proxies for expensive measurements by correcting systematic\nbiases between property evaluation methods. We recommend using Gemini for\nregression tasks with sparse data and in an autonomous workflow setting where\nits predictions of expensive to evaluate objectives can be used to construct a\nmore informative acquisition function, thus reducing the number of expensive\nevaluations an optimizer needs to achieve desired target values. In a\nregression setting, we showcase the ability of our method to make accurate\npredictions of DFT calculated bandgaps of hybrid organic-inorganic perovskite\nmaterials. We further demonstrate the benefits that Gemini provides to\nautonomous workflows by augmenting the Bayesian optimizer Phoenics to yeild a\nscalable optimization framework leveraging multiple sources of measurement.\nFinally, we simulate an autonomous materials discovery platform for optimizing\nthe activity of electrocatalysts for the oxygen evolution reaction. Realizing\nautonomous workflows with Gemini, we show that the number of measurements of a\ncomposition space comprising expensive and rare metals needed to achieve a\ntarget overpotential is significantly reduced when measurements from a proxy\ncomposition system with less expensive metals are available.",
    "published_date": "2021-03-05T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.03391v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.03389v1",
    "title": "An Analytical Solution to the IMU Initialization Problem for Visual-Inertial Systems",
    "authors": [
      "David Zuñiga-Noël",
      "Francisco-Angel Moreno",
      "Javier Gonzalez-Jimenez"
    ],
    "author_ids": [],
    "abstract": "The fusion of visual and inertial measurements is becoming more and more\npopular in the robotics community since both sources of information complement\nwell each other. However, in order to perform this fusion, the biases of the\nInertial Measurement Unit (IMU) as well as the direction of gravity must be\ninitialized first. Additionally, in case of a monocular camera, the metric\nscale is also needed. The most popular visual-inertial initialization\napproaches rely on accurate vision-only motion estimates to build a non-linear\noptimization problem that solves for these parameters in an iterative way. In\nthis paper, we rely on the previous work in [1] and propose an analytical\nsolution to estimate the accelerometer bias, the direction of gravity and the\nscale factor in a maximum-likelihood framework. This formulation results in a\nvery efficient estimation approach and, due to the non-iterative nature of the\nsolution, avoids the intrinsic issues of previous iterative solutions. We\npresent an extensive validation of the proposed IMU initialization approach and\na performance comparison against the state-of-the-art approach described in [2]\nwith real data from the publicly available EuRoC dataset, achieving comparable\naccuracy at a fraction of its computational cost and without requiring an\ninitial guess for the scale factor. We also provide a C++ open source reference\nimplementation.",
    "published_date": "2021-03-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.03389v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.09051v1",
    "title": "Exploring the Assessment List for Trustworthy AI in the Context of Advanced Driver-Assistance Systems",
    "authors": [
      "Markus Borg",
      "Joshua Bronson",
      "Linus Christensson",
      "Fredrik Olsson",
      "Olof Lennartsson",
      "Elias Sonnsjö",
      "Hamid Ebabi",
      "Martin Karsberg"
    ],
    "author_ids": [],
    "abstract": "Artificial Intelligence (AI) is increasingly used in critical applications.\nThus, the need for dependable AI systems is rapidly growing. In 2018, the\nEuropean Commission appointed experts to a High-Level Expert Group on AI\n(AI-HLEG). AI-HLEG defined Trustworthy AI as 1) lawful, 2) ethical, and 3)\nrobust and specified seven corresponding key requirements. To help development\norganizations, AI-HLEG recently published the Assessment List for Trustworthy\nAI (ALTAI). We present an illustrative case study from applying ALTAI to an\nongoing development project of an Advanced Driver-Assistance System (ADAS) that\nrelies on Machine Learning (ML). Our experience shows that ALTAI is largely\napplicable to ADAS development, but specific parts related to human agency and\ntransparency can be disregarded. Moreover, bigger questions related to societal\nand environmental impact cannot be tackled by an ADAS supplier in isolation. We\npresent how we plan to develop the ADAS to ensure ALTAI-compliance. Finally, we\nprovide three recommendations for the next revision of ALTAI, i.e., life-cycle\nvariants, domain-specific adaptations, and removed redundancy.",
    "published_date": "2021-03-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.09051v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.03337v2",
    "title": "Revisiting Priority $k$-Center: Fairness and Outliers",
    "authors": [
      "Tanvi Bajpai",
      "Deeparnab Chakrabarty",
      "Chandra Chekuri",
      "Maryam Negahbani"
    ],
    "author_ids": [],
    "abstract": "In the Priority $k$-Center problem, the input consists of a metric space\n$(X,d)$, an integer $k$, and for each point $v \\in X$ a priority radius $r(v)$.\nThe goal is to choose $k$-centers $S \\subseteq X$ to minimize $\\max_{v \\in X}\n\\frac{1}{r(v)} d(v,S)$. If all $r(v)$'s are uniform, one obtains the $k$-Center\nproblem. Plesn\\'ik [Plesn\\'ik, Disc. Appl. Math. 1987] introduced the Priority\n$k$-Center problem and gave a $2$-approximation algorithm matching the best\npossible algorithm for $k$-Center. We show how the problem is related to two\ndifferent notions of fair clustering [Harris et al., NeurIPS 2018; Jung et al.,\nFORC 2020]. Motivated by these developments we revisit the problem and, in our\nmain technical contribution, develop a framework that yields constant factor\napproximation algorithms for Priority $k$-Center with outliers. Our framework\nextends to generalizations of Priority $k$-Center to matroid and knapsack\nconstraints, and as a corollary, also yields algorithms with fairness\nguarantees in the lottery model of Harris et al [Harris et al, JMLR 2019].",
    "published_date": "2021-03-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DS",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.03337v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.03332v1",
    "title": "An Agent-based Model to Evaluate Interventions on Online Dating Platforms to Decrease Racial Homogamy",
    "authors": [
      "Stefania Ionescu",
      "Aniko Hannak",
      "Kenneth Joseph"
    ],
    "author_ids": [],
    "abstract": "Perhaps the most controversial questions in the study of online platforms\ntoday surround the extent to which platforms can intervene to reduce the\nsocietal ills perpetrated on them. Up for debate is whether there exist any\neffective and lasting interventions a platform can adopt to address, e.g.,\nonline bullying, or if other, more far-reaching change is necessary to address\nsuch problems. Empirical work is critical to addressing such questions. But it\nis also challenging, because it is time-consuming, expensive, and sometimes\nlimited to the questions companies are willing to ask. To help focus and inform\nthis empirical work, we here propose an agent-based modeling (ABM) approach. As\nan application, we analyze the impact of a set of interventions on a simulated\nonline dating platform on the lack of long-term interracial relationships in an\nartificial society. In the real world, a lack of interracial relationships are\na critical vehicle through which inequality is maintained. Our work shows that\nmany previously hypothesized interventions online dating platforms could take\nto increase the number of interracial relationships from their website have\nlimited effects, and that the effectiveness of any intervention is subject to\nassumptions about sociocultural structure. Further, interventions that are\neffective in increasing diversity in long-term relationships are at odds with\nplatforms' profit-oriented goals. At a general level, the present work shows\nthe value of using an ABM approach to help understand the potential effects and\nside effects of different interventions that a platform could take.",
    "published_date": "2021-03-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.03332v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.03304v1",
    "title": "A Hybrid Controller for DOS-Resilient String-Stable Vehicle Platoons",
    "authors": [
      "Roberto Merco",
      "Francesco Ferrante",
      "Pierluigi Pisu"
    ],
    "author_ids": [],
    "abstract": "This paper deals with the design of resilient Cooperative Adaptive Cruise\nControl (CACC) for homogeneous vehicle platoons in which communication is\nvulnerable to Denial-of-Service (DOS) attacks. We consider DOS attacks as\nconsecutive packet dropouts. We present a controller tuning procedure based on\nlinear matrix inequalities (LMI) that maximizes the resiliency to DOS attacks,\nwhile guaranteeing performance and string stability. The design procedure\nreturns controller gains and gives a lower bound on the maximum allowable\nnumber of successive packet dropouts. A numerical example is employed to\nillustrate the effectiveness of the proposed approach.",
    "published_date": "2021-03-04T00:00:00",
    "year": 2021,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.03304v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.02735v2",
    "title": "Fairness of Exposure in Stochastic Bandits",
    "authors": [
      "Lequn Wang",
      "Yiwei Bai",
      "Wen Sun",
      "Thorsten Joachims"
    ],
    "author_ids": [],
    "abstract": "Contextual bandit algorithms have become widely used for recommendation in\nonline systems (e.g. marketplaces, music streaming, news), where they now wield\nsubstantial influence on which items get exposed to the users. This raises\nquestions of fairness to the items -- and to the sellers, artists, and writers\nthat benefit from this exposure. We argue that the conventional bandit\nformulation can lead to an undesirable and unfair winner-takes-all allocation\nof exposure. To remedy this problem, we propose a new bandit objective that\nguarantees merit-based fairness of exposure to the items while optimizing\nutility to the users. We formulate fairness regret and reward regret in this\nsetting, and present algorithms for both stochastic multi-armed bandits and\nstochastic linear bandits. We prove that the algorithms achieve sub-linear\nfairness regret and reward regret. Beyond the theoretical analysis, we also\nprovide empirical evidence that these algorithms can fairly allocate exposure\nto different arms effectively.",
    "published_date": "2021-03-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.02735v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.02662v3",
    "title": "Deep Clustering by Semantic Contrastive Learning",
    "authors": [
      "Jiabo Huang",
      "Shaogang Gong"
    ],
    "author_ids": [],
    "abstract": "Whilst contrastive learning has recently brought notable benefits to deep\nclustering of unlabelled images by learning sample-specific discriminative\nvisual features, its potential for explicitly inferring class decision\nboundaries is less well understood. This is because its instance discrimination\nstrategy is not class sensitive, therefore, the clusters derived on the\nresulting sample-specific feature space are not optimised for corresponding to\nmeaningful class decision boundaries. In this work, we solve this problem by\nintroducing Semantic Contrastive Learning (SCL). SCL imposes explicitly\ndistance-based cluster structures on unlabelled training data by formulating a\nsemantic (cluster-aware) contrastive learning objective. Moreover, we introduce\na clustering consistency condition to be satisfied jointly by both instance\nvisual similarities and cluster decision boundaries, and concurrently\noptimising both to reason about the hypotheses of semantic ground-truth classes\n(unknown/unlabelled) on-the-fly by their consensus. This semantic contrastive\nlearning approach to discovering unknown class decision boundaries has\nconsiderable advantages to unsupervised learning of object recognition tasks.\nExtensive experiments show that SCL outperforms state-of-the-art contrastive\nlearning and deep clustering methods on six object recognition benchmarks,\nespecially on the more challenging finer-grained and larger datasets.",
    "published_date": "2021-03-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.02662v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.02590v2",
    "title": "Elliot: a Comprehensive and Rigorous Framework for Reproducible Recommender Systems Evaluation",
    "authors": [
      "Vito Walter Anelli",
      "Alejandro Bellogín",
      "Antonio Ferrara",
      "Daniele Malitesta",
      "Felice Antonio Merra",
      "Claudio Pomo",
      "Francesco Maria Donini",
      "Tommaso Di Noia"
    ],
    "author_ids": [],
    "abstract": "Recommender Systems have shown to be an effective way to alleviate the\nover-choice problem and provide accurate and tailored recommendations. However,\nthe impressive number of proposed recommendation algorithms, splitting\nstrategies, evaluation protocols, metrics, and tasks, has made rigorous\nexperimental evaluation particularly challenging. Puzzled and frustrated by the\ncontinuous recreation of appropriate evaluation benchmarks, experimental\npipelines, hyperparameter optimization, and evaluation procedures, we have\ndeveloped an exhaustive framework to address such needs. Elliot is a\ncomprehensive recommendation framework that aims to run and reproduce an entire\nexperimental pipeline by processing a simple configuration file. The framework\nloads, filters, and splits the data considering a vast set of strategies (13\nsplitting methods and 8 filtering approaches, from temporal training-test\nsplitting to nested K-folds Cross-Validation). Elliot optimizes hyperparameters\n(51 strategies) for several recommendation algorithms (50), selects the best\nmodels, compares them with the baselines providing intra-model statistics,\ncomputes metrics (36) spanning from accuracy to beyond-accuracy, bias, and\nfairness, and conducts statistical analysis (Wilcoxon and Paired t-test). The\naim is to provide the researchers with a tool to ease (and make them\nreproducible) all the experimental evaluation phases, from data reading to\nresults collection. Elliot is available on GitHub\n(https://github.com/sisinflab/elliot).",
    "published_date": "2021-03-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.02590v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.02512v2",
    "title": "Approximation Algorithms for Socially Fair Clustering",
    "authors": [
      "Yury Makarychev",
      "Ali Vakilian"
    ],
    "author_ids": [],
    "abstract": "We present an $(e^{O(p)} \\frac{\\log \\ell}{\\log\\log\\ell})$-approximation\nalgorithm for socially fair clustering with the $\\ell_p$-objective. In this\nproblem, we are given a set of points in a metric space. Each point belongs to\none (or several) of $\\ell$ groups. The goal is to find a $k$-medians,\n$k$-means, or, more generally, $\\ell_p$-clustering that is simultaneously good\nfor all of the groups. More precisely, we need to find a set of $k$ centers $C$\nso as to minimize the maximum over all groups $j$ of $\\sum_{u \\text{ in group\n}j} d(u,C)^p$. The socially fair clustering problem was independently proposed\nby Ghadiri, Samadi, and Vempala [2021] and Abbasi, Bhaskara, and\nVenkatasubramanian [2021]. Our algorithm improves and generalizes their\n$O(\\ell)$-approximation algorithms for the problem. The natural LP relaxation\nfor the problem has an integrality gap of $\\Omega(\\ell)$. In order to obtain\nour result, we introduce a strengthened LP relaxation and show that it has an\nintegrality gap of $\\Theta(\\frac{\\log \\ell}{\\log\\log\\ell})$ for a fixed $p$.\nAdditionally, we present a bicriteria approximation algorithm, which\ngeneralizes the bicriteria approximation of Abbasi et al. [2021].",
    "published_date": "2021-03-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DS",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.02512v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.02414v1",
    "title": "Facets of the cone of exact games",
    "authors": [
      "Milan Studený",
      "Václav Kratochvíl"
    ],
    "author_ids": [],
    "abstract": "The class of exact transferable utility coalitional games, introduced in 1972\nby Schmeidler, has been studied both in the context of game theory and in the\ncontext of imprecise probabilities. We characterize the cone of exact games by\ndescribing the minimal set of linear inequalities defining this cone; these\nfacet-defining inequalities for the exact cone appear to correspond to certain\nset systems (= systems of coalitions). We noticed that non-empty proper\ncoalitions having non-zero coefficients in these facet-defining inequalities\nform set systems with particular properties.\n  More specifically, we introduce the concept of a semi-balanced system of\ncoalitions, which generalizes the classic concept of a balanced coalitional\nsystem in cooperative game theory. The semi-balanced coalitional systems\nprovide valid inequalities for the exact cone and minimal semi-balanced systems\n(in the sense of inclusion of set systems) characterize this cone. We also\nintroduce basic classification of minimal semi-balanced systems, their\npictorial representatives and a substantial concept of an indecomposable\n(minimal) semi-balanced system of coalitions. The main result of the paper is\nthat indecomposable semi-balanced systems are in one-to-one correspondence with\nfacet-defining inequalities for the exact cone. The secondary relevant result\nis the rebuttal of a former conjecture claiming that a coalitional game is\nexact iff it is totally balanced and its anti-dual is also totally balanced. We\nadditionally characterize those inequalities which are facet-defining both for\nthe exact cone and the cone of totally balanced games.",
    "published_date": "2021-03-03T00:00:00",
    "year": 2021,
    "categories": [
      "math.CO",
      "cs.GT",
      "91A12, 52B05, 90C57"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.02414v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.02404v2",
    "title": "Quantum Network Discrimination",
    "authors": [
      "Christoph Hirche"
    ],
    "author_ids": [],
    "abstract": "Discrimination between objects, in particular quantum states, is one of the\nmost fundamental tasks in (quantum) information theory. Recent years have seen\nsignificant progress towards extending the framework to point-to-point quantum\nchannels. However, with technological progress the focus of the field is\nshifting to more complex structures: Quantum networks. In contrast to channels,\nnetworks allow for intermediate access points where information can be\nreceived, processed and reintroduced into the network. In this work we study\nthe discrimination of quantum networks and its fundamental limitations. In\nparticular when multiple uses of the network are at hand, the rooster of\navailable strategies becomes increasingly complex. The simplest quantum network\nthat capturers the structure of the problem is given by a quantum superchannel.\nWe discuss the available classes of strategies when considering $n$ copies of a\nsuperchannel and give fundamental bounds on the asymptotically achievable rates\nin an asymmetric discrimination setting. Furthermore, we discuss achievability,\nsymmetric network discrimination, the strong converse exponent, generalization\nto arbitrary quantum networks and finally an application to an active version\nof the quantum illumination problem.",
    "published_date": "2021-03-03T00:00:00",
    "year": 2021,
    "categories": [
      "quant-ph",
      "cs.IT",
      "math-ph",
      "math.IT",
      "math.MP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.02404v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.02381v3",
    "title": "Human-AI Interactions in Public Sector Decision-Making: \"Automation Bias\" and \"Selective Adherence\" to Algorithmic Advice",
    "authors": [
      "Saar Alon-Barkat",
      "Madalina Busuioc"
    ],
    "author_ids": [],
    "abstract": "Artificial intelligence algorithms are increasingly adopted as decisional\naides by public bodies, with the promise of overcoming biases of human\ndecision-makers. At the same time, they may introduce new biases in the\nhuman-algorithm interaction. Drawing on psychology and public administration\nliteratures, we investigate two key biases: overreliance on algorithmic advice\neven in the face of warning signals from other sources (automation bias), and\nselective adoption of algorithmic advice when this corresponds to stereotypes\n(selective adherence). We assess these via three experimental studies conducted\nin the NetherlandsWe discuss the implications of our findings for public sector\ndecision making in the age of automation. Overall, our study speaks to\npotential negative effects of automation of the administrative state for\nalready vulnerable and disadvantaged citizens.",
    "published_date": "2021-03-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.02381v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.02220v1",
    "title": "Unsupervised Domain Adaptation Network with Category-Centric Prototype Aligner for Biomedical Image Segmentation",
    "authors": [
      "Ping Gong",
      "Wenwen Yu",
      "Qiuwen Sun",
      "Ruohan Zhao",
      "Junfeng Hu"
    ],
    "author_ids": [],
    "abstract": "With the widespread success of deep learning in biomedical image\nsegmentation, domain shift becomes a critical and challenging problem, as the\ngap between two domains can severely affect model performance when deployed to\nunseen data with heterogeneous features. To alleviate this problem, we present\na novel unsupervised domain adaptation network, for generalizing models learned\nfrom the labeled source domain to the unlabeled target domain for\ncross-modality biomedical image segmentation. Specifically, our approach\nconsists of two key modules, a conditional domain discriminator~(CDD) and a\ncategory-centric prototype aligner~(CCPA). The CDD, extended from conditional\ndomain adversarial networks in classifier tasks, is effective and robust in\nhandling complex cross-modality biomedical images. The CCPA, improved from the\ngraph-induced prototype alignment mechanism in cross-domain object detection,\ncan exploit precise instance-level features through an elaborate prototype\nrepresentation. In addition, it can address the negative effect of class\nimbalance via entropy-based loss. Extensive experiments on a public benchmark\nfor the cardiac substructure segmentation task demonstrate that our method\nsignificantly improves performance on the target domain.",
    "published_date": "2021-03-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.02220v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.02208v1",
    "title": "On the solution of contact problems with Tresca friction by the semismooth* Newton method",
    "authors": [
      "Helmut Gfrerer",
      "Jiri V. Outrata",
      "Jan Valdman"
    ],
    "author_ids": [],
    "abstract": "An equilibrium of a linear elastic body subject to loading and satisfying the\nfriction and contact conditions can be described by a variational inequality of\nthe second kind and the respective discrete model attains the form of a\ngeneralized equation. To its numerical solution we apply the semismooth* Newton\nmethod by Gfrerer and Outrata (2019) in which, in contrast to most available\nNewton-type methods for inclusions, one approximates not only the single-valued\nbut also the multi-valued part. This is performed on the basis of limiting\n(Morduchovich) coderivative. In our case of the Tresca friction, the\nmulti-valued part amounts to the subdifferential of a convex function generated\nby the friction and contact conditions. The full 3D discrete problem is then\nreduced to the contact boundary. Implementation details of the semismooth*\nNewton method are provided and numerical tests demonstrate its superlinear\nconvergence and mesh independence.",
    "published_date": "2021-03-03T00:00:00",
    "year": 2021,
    "categories": [
      "math.OC",
      "cs.NA",
      "math.NA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.02208v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.02185v1",
    "title": "Task Aligned Generative Meta-learning for Zero-shot Learning",
    "authors": [
      "Zhe Liu",
      "Yun Li",
      "Lina Yao",
      "Xianzhi Wang",
      "Guodong Long"
    ],
    "author_ids": [],
    "abstract": "Zero-shot learning (ZSL) refers to the problem of learning to classify\ninstances from the novel classes (unseen) that are absent in the training set\n(seen). Most ZSL methods infer the correlation between visual features and\nattributes to train the classifier for unseen classes. However, such models may\nhave a strong bias towards seen classes during training. Meta-learning has been\nintroduced to mitigate the basis, but meta-ZSL methods are inapplicable when\ntasks used for training are sampled from diverse distributions. In this regard,\nwe propose a novel Task-aligned Generative Meta-learning model for Zero-shot\nlearning (TGMZ). TGMZ mitigates the potentially biased training and enables\nmeta-ZSL to accommodate real-world datasets containing diverse distributions.\nTGMZ incorporates an attribute-conditioned task-wise distribution alignment\nnetwork that projects tasks into a unified distribution to deliver an unbiased\nmodel. Our comparisons with state-of-the-art algorithms show the improvements\nof 2.1%, 3.0%, 2.5%, and 7.6% achieved by TGMZ on AWA1, AWA2, CUB, and aPY\ndatasets, respectively. TGMZ also outperforms competitors by 3.6% in\ngeneralized zero-shot learning (GZSL) setting and 7.9% in our proposed\nfusion-ZSL setting.",
    "published_date": "2021-03-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.02185v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.02155v1",
    "title": "Sensing population distribution from satellite imagery via deep learning: model selection, neighboring effect, and systematic biases",
    "authors": [
      "Xiao Huang",
      "Di Zhu",
      "Fan Zhang",
      "Tao Liu",
      "Xiao Li",
      "Lei Zou"
    ],
    "author_ids": [],
    "abstract": "The rapid development of remote sensing techniques provides rich,\nlarge-coverage, and high-temporal information of the ground, which can be\ncoupled with the emerging deep learning approaches that enable latent features\nand hidden geographical patterns to be extracted. This study marks the first\nattempt to cross-compare performances of popular state-of-the-art deep learning\nmodels in estimating population distribution from remote sensing images,\ninvestigate the contribution of neighboring effect, and explore the potential\nsystematic population estimation biases. We conduct an end-to-end training of\nfour popular deep learning architectures, i.e., VGG, ResNet, Xception, and\nDenseNet, by establishing a mapping between Sentinel-2 image patches and their\ncorresponding population count from the LandScan population grid. The results\nreveal that DenseNet outperforms the other three models, while VGG has the\nworst performances in all evaluating metrics under all selected neighboring\nscenarios. As for the neighboring effect, contradicting existing studies, our\nresults suggest that the increase of neighboring sizes leads to reduced\npopulation estimation performance, which is found universal for all four\nselected models in all evaluating metrics. In addition, there exists a notable,\nuniversal bias that all selected deep learning models tend to overestimate\nsparsely populated image patches and underestimate densely populated image\npatches, regardless of neighboring sizes. The methodological, experimental, and\ncontextual knowledge this study provides is expected to benefit a wide range of\nfuture studies that estimate population distribution via remote sensing\nimagery.",
    "published_date": "2021-03-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.02155v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.02152v3",
    "title": "Group-wise Inhibition based Feature Regularization for Robust Classification",
    "authors": [
      "Haozhe Liu",
      "Haoqian Wu",
      "Weicheng Xie",
      "Feng Liu",
      "Linlin Shen"
    ],
    "author_ids": [],
    "abstract": "The convolutional neural network (CNN) is vulnerable to degraded images with\neven very small variations (e.g. corrupted and adversarial samples). One of the\npossible reasons is that CNN pays more attention to the most discriminative\nregions, but ignores the auxiliary features when learning, leading to the lack\nof feature diversity for final judgment. In our method, we propose to\ndynamically suppress significant activation values of CNN by group-wise\ninhibition, but not fixedly or randomly handle them when training. The feature\nmaps with different activation distribution are then processed separately to\ntake the feature independence into account. CNN is finally guided to learn\nricher discriminative features hierarchically for robust classification\naccording to the proposed regularization. Our method is comprehensively\nevaluated under multiple settings, including classification against\ncorruptions, adversarial attacks and low data regime. Extensive experimental\nresults show that the proposed method can achieve significant improvements in\nterms of both robustness and generalization performances, when compared with\nthe state-of-the-art methods. Code is available at\nhttps://github.com/LinusWu/TENET_Training.",
    "published_date": "2021-03-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.02152v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.02140v1",
    "title": "PML: Progressive Margin Loss for Long-tailed Age Classification",
    "authors": [
      "Zongyong Deng",
      "Hao Liu",
      "Yaoxing Wang",
      "Chenyang Wang",
      "Zekuan Yu",
      "Xuehong Sun"
    ],
    "author_ids": [],
    "abstract": "In this paper, we propose a progressive margin loss (PML) approach for\nunconstrained facial age classification. Conventional methods make strong\nassumption on that each class owns adequate instances to outline its data\ndistribution, likely leading to bias prediction where the training samples are\nsparse across age classes. Instead, our PML aims to adaptively refine the age\nlabel pattern by enforcing a couple of margins, which fully takes in the\nin-between discrepancy of the intra-class variance, inter-class variance and\nclass center. Our PML typically incorporates with the ordinal margin and the\nvariational margin, simultaneously plugging in the globally-tuned deep neural\nnetwork paradigm. More specifically, the ordinal margin learns to exploit the\ncorrelated relationship of the real-world age labels. Accordingly, the\nvariational margin is leveraged to minimize the influence of head classes that\nmisleads the prediction of tailed samples. Moreover, our optimization carefully\nseeks a series of indicator curricula to achieve robust and efficient model\ntraining. Extensive experimental results on three face aging datasets\ndemonstrate that our PML achieves compelling performance compared to state of\nthe arts. Code will be made publicly.",
    "published_date": "2021-03-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.02140v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.02052v1",
    "title": "Convergence and Inequality in Research Globalization",
    "authors": [
      "Saurabh Mishra",
      "Kuansan Wang"
    ],
    "author_ids": [],
    "abstract": "The catch-up effect and the Matthew effect offer opposing characterizations\nof globalization: the former predicts an eventual convergence as the poor can\ngrow faster than the rich due to free exchanges of complementary resources,\nwhile the latter, a deepening inequality between the rich and the poor. To\nunderstand these effects on the globalization of research, we conduct an\nin-depth study based on scholarly and patent publications covering STEM\nresearch from 218 countries/regions over the past four decades, covering more\nthan 55 million scholarly articles and 1.7 billion citations. Unique to this\ninvestigation is the simultaneous examination of both the research output and\nits impact in the same data set, using a novel machine learning based measure,\ncalled saliency, to mitigate the intrinsic biases in quantifying the research\nimpact. The results show that the two effects are in fact co-occurring: there\nare clear indications of convergence among the high income and upper middle\nincome countries across the STEM fields, but a widening gap is developing that\nsegregates the lower middle and low income regions from the higher income\nregions. Furthermore, the rate of convergence varies notably among the STEM\nsub-fields, with the highly strategic area of Artificial Intelligence (AI)\nsandwiched between fields such as Medicine and Materials Science that occupy\nthe opposite ends of the spectrum. The data support the argument that a leading\nexplanation of the Matthew effect, namely, the preferential attachment theory,\ncan actually foster the catch-up effect when organizations from lower income\ncountries forge substantial research collaborations with those already\ndominant. The data resoundingly show such collaborations benefit all parties\ninvolved, and a case of role reversal can be seen in the Materials Science\nfield where the most advanced signs of convergence are observed.",
    "published_date": "2021-03-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.DL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.02052v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.02023v1",
    "title": "EnD: Entangling and Disentangling deep representations for bias correction",
    "authors": [
      "Enzo Tartaglione",
      "Carlo Alberto Barbano",
      "Marco Grangetto"
    ],
    "author_ids": [],
    "abstract": "Artificial neural networks perform state-of-the-art in an ever-growing number\nof tasks, and nowadays they are used to solve an incredibly large variety of\ntasks. There are problems, like the presence of biases in the training data,\nwhich question the generalization capability of these models. In this work we\npropose EnD, a regularization strategy whose aim is to prevent deep models from\nlearning unwanted biases. In particular, we insert an \"information bottleneck\"\nat a certain point of the deep neural network, where we disentangle the\ninformation about the bias, still letting the useful information for the\ntraining task forward-propagating in the rest of the model. One big advantage\nof EnD is that we do not require additional training complexity (like decoders\nor extra layers in the model), since it is a regularizer directly applied on\nthe trained model. Our experiments show that EnD effectively improves the\ngeneralization on unbiased test sets, and it can be effectively applied on\nreal-case scenarios, like removing hidden biases in the COVID-19 detection from\nradiographic images.",
    "published_date": "2021-03-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.02023v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.02013v1",
    "title": "Fairness, Semi-Supervised Learning, and More: A General Framework for Clustering with Stochastic Pairwise Constraints",
    "authors": [
      "Brian Brubach",
      "Darshan Chakrabarti",
      "John P. Dickerson",
      "Aravind Srinivasan",
      "Leonidas Tsepenekas"
    ],
    "author_ids": [],
    "abstract": "Metric clustering is fundamental in areas ranging from Combinatorial\nOptimization and Data Mining, to Machine Learning and Operations Research.\nHowever, in a variety of situations we may have additional requirements or\nknowledge, distinct from the underlying metric, regarding which pairs of points\nshould be clustered together. To capture and analyze such scenarios, we\nintroduce a novel family of \\emph{stochastic pairwise constraints}, which we\nincorporate into several essential clustering objectives (radius/median/means).\nMoreover, we demonstrate that these constraints can succinctly model an\nintriguing collection of applications, including among others \\emph{Individual\nFairness} in clustering and \\emph{Must-link} constraints in semi-supervised\nlearning. Our main result consists of a general framework that yields\napproximation algorithms with provable guarantees for important clustering\nobjectives, while at the same time producing solutions that respect the\nstochastic pairwise constraints. Furthermore, for certain objectives we devise\nimproved results in the case of Must-link constraints, which are also the best\npossible from a theoretical perspective. Finally, we present experimental\nevidence that validates the effectiveness of our algorithms.",
    "published_date": "2021-03-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.02013v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.01938v1",
    "title": "Medical Imaging and Machine Learning",
    "authors": [
      "Rohan Shad",
      "John P. Cunningham",
      "Euan A. Ashley",
      "Curtis P. Langlotz",
      "William Hiesinger"
    ],
    "author_ids": [],
    "abstract": "Advances in computing power, deep learning architectures, and expert labelled\ndatasets have spurred the development of medical imaging artificial\nintelligence systems that rival clinical experts in a variety of scenarios. The\nNational Institutes of Health in 2018 identified key focus areas for the future\nof artificial intelligence in medical imaging, creating a foundational roadmap\nfor research in image acquisition, algorithms, data standardization, and\ntranslatable clinical decision support systems. Among the key issues raised in\nthe report: data availability, need for novel computing architectures and\nexplainable AI algorithms, are still relevant despite the tremendous progress\nmade over the past few years alone. Furthermore, translational goals of data\nsharing, validation of performance for regulatory approval, generalizability\nand mitigation of unintended bias must be accounted for early in the\ndevelopment process. In this perspective paper we explore challenges unique to\nhigh dimensional clinical imaging data, in addition to highlighting some of the\ntechnical and ethical considerations in developing high-dimensional,\nmulti-modality, machine learning systems for clinical decision support.",
    "published_date": "2021-03-02T00:00:00",
    "year": 2021,
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.01938v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.01907v4",
    "title": "Fairness in Credit Scoring: Assessment, Implementation and Profit Implications",
    "authors": [
      "Nikita Kozodoi",
      "Johannes Jacob",
      "Stefan Lessmann"
    ],
    "author_ids": [],
    "abstract": "The rise of algorithmic decision-making has spawned much research on fair\nmachine learning (ML). Financial institutions use ML for building risk\nscorecards that support a range of credit-related decisions. Yet, the\nliterature on fair ML in credit scoring is scarce. The paper makes three\ncontributions. First, we revisit statistical fairness criteria and examine\ntheir adequacy for credit scoring. Second, we catalog algorithmic options for\nincorporating fairness goals in the ML model development pipeline. Last, we\nempirically compare different fairness processors in a profit-oriented credit\nscoring context using real-world data. The empirical results substantiate the\nevaluation of fairness measures, identify suitable options to implement fair\ncredit scoring, and clarify the profit-fairness trade-off in lending decisions.\nWe find that multiple fairness criteria can be approximately satisfied at once\nand recommend separation as a proper criterion for measuring the fairness of a\nscorecard. We also find fair in-processors to deliver a good balance between\nprofit and fairness and show that algorithmic discrimination can be reduced to\na reasonable level at a relatively low cost. The codes corresponding to the\npaper are available on GitHub.",
    "published_date": "2021-03-02T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG",
      "q-fin.RM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.01907v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.01885v1",
    "title": "Learning-based Bias Correction for Time Difference of Arrival Ultra-wideband Localization of Resource-constrained Mobile Robots",
    "authors": [
      "Wenda Zhao",
      "Jacopo Panerati",
      "Angela P. Schoellig"
    ],
    "author_ids": [],
    "abstract": "Accurate indoor localization is a crucial enabling technology for many\nrobotics applications, from warehouse management to monitoring tasks.\nUltra-wideband (UWB) time difference of arrival (TDOA)-based localization is a\npromising lightweight, low-cost solution that can scale to a large number of\ndevices -- making it especially suited for resource-constrained multi-robot\napplications. However, the localization accuracy of standard, commercially\navailable UWB radios is often insufficient due to significant measurement bias\nand outliers. In this letter, we address these issues by proposing a robust UWB\nTDOA localization framework comprising of (i) learning-based bias correction\nand (ii) M-estimation-based robust filtering to handle outliers. The key\nproperties of our approach are that (i) the learned biases generalize to\ndifferent UWB anchor setups and (ii) the approach is computationally efficient\nenough to run on resource-constrained hardware. We demonstrate our approach on\na Crazyflie nano-quadcopter. Experimental results show that the proposed\nlocalization framework, relying only on the onboard IMU and UWB, provides an\naverage of 42.08 percent localization error reduction (in three different\nanchor setups) compared to the baseline approach without bias compensation. {We\nalso show autonomous trajectory tracking on a quadcopter using our UWB TDOA\nlocalization approach.}",
    "published_date": "2021-03-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.RO",
      "cs.LG",
      "I.2.6; I.2.9; J.2; J.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.01885v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.01846v2",
    "title": "The KL-Divergence between a Graph Model and its Fair I-Projection as a Fairness Regularizer",
    "authors": [
      "Maarten Buyl",
      "Tijl De Bie"
    ],
    "author_ids": [],
    "abstract": "Learning and reasoning over graphs is increasingly done by means of\nprobabilistic models, e.g. exponential random graph models, graph embedding\nmodels, and graph neural networks. When graphs are modeling relations between\npeople, however, they will inevitably reflect biases, prejudices, and other\nforms of inequity and inequality. An important challenge is thus to design\naccurate graph modeling approaches while guaranteeing fairness according to the\nspecific notion of fairness that the problem requires. Yet, past work on the\ntopic remains scarce, is limited to debiasing specific graph modeling methods,\nand often aims to ensure fairness in an indirect manner.\n  We propose a generic approach applicable to most probabilistic graph modeling\napproaches. Specifically, we first define the class of fair graph models\ncorresponding to a chosen set of fairness criteria. Given this, we propose a\nfairness regularizer defined as the KL-divergence between the graph model and\nits I-projection onto the set of fair models. We demonstrate that using this\nfairness regularizer in combination with existing graph modeling approaches\nefficiently trades-off fairness with accuracy, whereas the state-of-the-art\nmodels can only make this trade-off for the fairness criterion that they were\nspecifically designed for.",
    "published_date": "2021-03-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.01846v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.01823v1",
    "title": "A Structurally Regularized Convolutional Neural Network for Image Classification using Wavelet-based SubBand Decomposition",
    "authors": [
      "Pavel Sinha",
      "Ioannis Psaromiligkos",
      "Zeljko Zilic"
    ],
    "author_ids": [],
    "abstract": "We propose a convolutional neural network (CNN) architecture for image\nclassification based on subband decomposition of the image using wavelets. The\nproposed architecture decomposes the input image spectra into multiple\ncritically sampled subbands, extracts features using a single CNN per subband,\nand finally, performs classification by combining the extracted features using\na fully connected layer. Processing each of the subbands by an individual CNN,\nthereby limiting the learning scope of each CNN to a single subband, imposes a\nform of structural regularization. This provides better generalization\ncapability as seen by the presented results. The proposed architecture achieves\nbest-in-class performance in terms of total multiply-add-accumulator operations\nand nearly best-in-class performance in terms of total parameters required, yet\nit maintains competitive classification performance. We also show the proposed\narchitecture is more robust than the regular full-band CNN to noise caused by\nweight-and-bias quantization and input quantization.",
    "published_date": "2021-03-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.01823v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.01592v1",
    "title": "A Comprehensive Study on Face Recognition Biases Beyond Demographics",
    "authors": [
      "Philipp Terhörst",
      "Jan Niklas Kolf",
      "Marco Huber",
      "Florian Kirchbuchner",
      "Naser Damer",
      "Aythami Morales",
      "Julian Fierrez",
      "Arjan Kuijper"
    ],
    "author_ids": [],
    "abstract": "Face recognition (FR) systems have a growing effect on critical\ndecision-making processes. Recent works have shown that FR solutions show\nstrong performance differences based on the user's demographics. However, to\nenable a trustworthy FR technology, it is essential to know the influence of an\nextended range of facial attributes on FR beyond demographics. Therefore, in\nthis work, we analyse FR bias over a wide range of attributes. We investigate\nthe influence of 47 attributes on the verification performance of two popular\nFR models. The experiments were performed on the publicly available MAADFace\nattribute database with over 120M high-quality attribute annotations. To\nprevent misleading statements about biased performances, we introduced control\ngroup based validity values to decide if unbalanced test data causes the\nperformance differences. The results demonstrate that also many non-demographic\nattributes strongly affect the recognition performance, such as accessories,\nhair-styles and colors, face shapes, or facial anomalies. The observations of\nthis work show the strong need for further advances in making FR system more\nrobust, explainable, and fair. Moreover, our findings might help to a better\nunderstanding of how FR networks work, to enhance the robustness of these\nnetworks, and to develop more generalized bias-mitigating face recognition\nsolutions.",
    "published_date": "2021-03-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.01592v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.01559v1",
    "title": "Inter-class Discrepancy Alignment for Face Recognition",
    "authors": [
      "Jiaheng Liu",
      "Yudong Wu",
      "Yichao Wu",
      "Zhenmao Li",
      "Chen Ken",
      "Ding Liang",
      "Junjie Yan"
    ],
    "author_ids": [],
    "abstract": "The field of face recognition (FR) has witnessed great progress with the\nsurge of deep learning. Existing methods mainly focus on extracting\ndiscriminative features, and directly compute the cosine or L2 distance by the\npoint-to-point way without considering the context information. In this study,\nwe make a key observation that the local con-text represented by the\nsimilarities between the instance and its inter-class neighbors1plays an\nimportant role forFR. Specifically, we attempt to incorporate the local\nin-formation in the feature space into the metric, and pro-pose a unified\nframework calledInter-class DiscrepancyAlignment(IDA), with two dedicated\nmodules, Discrepancy Alignment Operator(IDA-DAO) andSupport Set\nEstimation(IDA-SSE). IDA-DAO is used to align the similarity scores considering\nthe discrepancy between the images and its neighbors, which is defined by\nadaptive support sets on the hypersphere. For practical inference, it is\ndifficult to acquire support set during online inference. IDA-SSE can provide\nconvincing inter-class neighbors by introducing virtual candidate images\ngenerated with GAN. Further-more, we propose the learnable IDA-SSE, which can\nimplicitly give estimation without the need of any other images in the\nevaluation process. The proposed IDA can be incorporated into existing FR\nsystems seamlessly and efficiently. Extensive experiments demonstrate that this\nframe-work can 1) significantly improve the accuracy, and 2) make the model\nrobust to the face images of various distributions.Without bells and whistles,\nour method achieves state-of-the-art performance on multiple standard FR\nbenchmarks.",
    "published_date": "2021-03-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.01559v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.01548v2",
    "title": "PFA: Privacy-preserving Federated Adaptation for Effective Model Personalization",
    "authors": [
      "Bingyan Liu",
      "Yao Guo",
      "Xiangqun Chen"
    ],
    "author_ids": [],
    "abstract": "Federated learning (FL) has become a prevalent distributed machine learning\nparadigm with improved privacy. After learning, the resulting federated model\nshould be further personalized to each different client. While several methods\nhave been proposed to achieve personalization, they are typically limited to a\nsingle local device, which may incur bias or overfitting since data in a single\ndevice is extremely limited. In this paper, we attempt to realize\npersonalization beyond a single client. The motivation is that during FL, there\nmay exist many clients with similar data distribution, and thus the\npersonalization performance could be significantly boosted if these similar\nclients can cooperate with each other. Inspired by this, this paper introduces\na new concept called federated adaptation, targeting at adapting the trained\nmodel in a federated manner to achieve better personalization results. However,\nthe key challenge for federated adaptation is that we could not outsource any\nraw data from the client during adaptation, due to privacy concerns. In this\npaper, we propose PFA, a framework to accomplish Privacy-preserving Federated\nAdaptation. PFA leverages the sparsity property of neural networks to generate\nprivacy-preserving representations and uses them to efficiently identify\nclients with similar data distributions. Based on the grouping results, PFA\nconducts an FL process in a group-wise way on the federated model to accomplish\nthe adaptation. For evaluation, we manually construct several practical FL\ndatasets based on public datasets in order to simulate both the class-imbalance\nand background-difference conditions. Extensive experiments on these datasets\nand popular model architectures demonstrate the effectiveness of PFA,\noutperforming other state-of-the-art methods by a large margin while ensuring\nuser privacy. We will release our code at: https://github.com/lebyni/PFA.",
    "published_date": "2021-03-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.01548v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.01520v2",
    "title": "When Age-Invariant Face Recognition Meets Face Age Synthesis: A Multi-Task Learning Framework",
    "authors": [
      "Zhizhong Huang",
      "Junping Zhang",
      "Hongming Shan"
    ],
    "author_ids": [],
    "abstract": "To minimize the effects of age variation in face recognition, previous work\neither extracts identity-related discriminative features by minimizing the\ncorrelation between identity- and age-related features, called age-invariant\nface recognition (AIFR), or removes age variation by transforming the faces of\ndifferent age groups into the same age group, called face age synthesis (FAS);\nhowever, the former lacks visual results for model interpretation while the\nlatter suffers from artifacts compromising downstream recognition. Therefore,\nthis paper proposes a unified, multi-task framework to jointly handle these two\ntasks, termed MTLFace, which can learn age-invariant identity-related\nrepresentation while achieving pleasing face synthesis. Specifically, we first\ndecompose the mixed face feature into two uncorrelated components -- identity-\nand age-related feature -- through an attention mechanism, and then decorrelate\nthese two components using multi-task training and continuous domain adaption.\nIn contrast to the conventional one-hot encoding that achieves group-level FAS,\nwe propose a novel identity conditional module to achieve identity-level FAS,\nwith a weight-sharing strategy to improve the age smoothness of synthesized\nfaces. In addition, we collect and release a large cross-age face dataset with\nage and gender annotations to advance the development of the AIFR and FAS.\nExtensive experiments on five benchmark cross-age datasets demonstrate the\nsuperior performance of our proposed MTLFace over existing state-of-the-art\nmethods for AIFR and FAS. We further validate MTLFace on two popular general\nface recognition datasets, showing competitive performance for face recognition\nin the wild. The source code and dataset are available\nat~\\url{https://github.com/Hzzone/MTLFace}.",
    "published_date": "2021-03-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.01520v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.01373v1",
    "title": "DeepMerge II: Building Robust Deep Learning Algorithms for Merging Galaxy Identification Across Domains",
    "authors": [
      "A. Ćiprijanović",
      "D. Kafkes",
      "K. Downey",
      "S. Jenkins",
      "G. N. Perdue",
      "S. Madireddy",
      "T. Johnston",
      "G. F. Snyder",
      "B. Nord"
    ],
    "author_ids": [],
    "abstract": "In astronomy, neural networks are often trained on simulation data with the\nprospect of being used on telescope observations. Unfortunately, training a\nmodel on simulation data and then applying it to instrument data leads to a\nsubstantial and potentially even detrimental decrease in model accuracy on the\nnew target dataset. Simulated and instrument data represent different data\ndomains, and for an algorithm to work in both, domain-invariant learning is\nnecessary. Here we employ domain adaptation techniques$-$ Maximum Mean\nDiscrepancy (MMD) as an additional transfer loss and Domain Adversarial Neural\nNetworks (DANNs)$-$ and demonstrate their viability to extract domain-invariant\nfeatures within the astronomical context of classifying merging and non-merging\ngalaxies. Additionally, we explore the use of Fisher loss and entropy\nminimization to enforce better in-domain class discriminability. We show that\nthe addition of each domain adaptation technique improves the performance of a\nclassifier when compared to conventional deep learning algorithms. We\ndemonstrate this on two examples: between two Illustris-1 simulated datasets of\ndistant merging galaxies, and between Illustris-1 simulated data of nearby\nmerging galaxies and observed data from the Sloan Digital Sky Survey. The use\nof domain adaptation techniques in our experiments leads to an increase of\ntarget domain classification accuracy of up to ${\\sim}20\\%$. With further\ndevelopment, these techniques will allow astronomers to successfully implement\nneural network models trained on simulation data to efficiently detect and\nstudy astrophysical objects in current and future large-scale astronomical\nsurveys.",
    "published_date": "2021-03-02T00:00:00",
    "year": 2021,
    "categories": [
      "astro-ph.IM",
      "astro-ph.GA",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.01373v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.01335v1",
    "title": "How Fair is Fairness-aware Representative Ranking and Methods for Fair Ranking",
    "authors": [
      "Akrati Saxena",
      "George Fletcher",
      "Mykola Pechenizkiy"
    ],
    "author_ids": [],
    "abstract": "Rankings of people and items has been highly used in selection-making,\nmatch-making, and recommendation algorithms that have been deployed on ranging\nof platforms from employment websites to searching tools. The ranking position\nof a candidate affects the amount of opportunities received by the ranked\ncandidate. It has been observed in several works that the ranking of candidates\nbased on their score can be biased for candidates belonging to the minority\ncommunity. In recent works, the fairness-aware representative ranking was\nproposed for computing fairness-aware re-ranking of results. The proposed\nalgorithm achieves the desired distribution of top-ranked results with respect\nto one or more protected attributes. In this work, we highlight the bias in\nfairness-aware representative ranking for an individual as well as for a group\nif the group is sub-active on the platform. We define individual unfairness and\ngroup unfairness and propose methods to generate ideal individual and group\nfair representative ranking if the universal representation ratio is known or\nunknown. The simulation results show the quantified analysis of fairness in the\nproposed solutions. The paper is concluded with open challenges and further\ndirections.",
    "published_date": "2021-03-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.01335v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.01312v2",
    "title": "UCB Momentum Q-learning: Correcting the bias without forgetting",
    "authors": [
      "Pierre Menard",
      "Omar Darwiche Domingues",
      "Xuedong Shang",
      "Michal Valko"
    ],
    "author_ids": [],
    "abstract": "We propose UCBMQ, Upper Confidence Bound Momentum Q-learning, a new algorithm\nfor reinforcement learning in tabular and possibly stage-dependent, episodic\nMarkov decision process. UCBMQ is based on Q-learning where we add a momentum\nterm and rely on the principle of optimism in face of uncertainty to deal with\nexploration. Our new technical ingredient of UCBMQ is the use of momentum to\ncorrect the bias that Q-learning suffers while, at the same time, limiting the\nimpact it has on the second-order term of the regret. For UCBMQ, we are able to\nguarantee a regret of at most $O(\\sqrt{H^3SAT}+ H^4 S A )$ where $H$ is the\nlength of an episode, $S$ the number of states, $A$ the number of actions, $T$\nthe number of episodes and ignoring terms in poly-$\\log(SAHT)$. Notably, UCBMQ\nis the first algorithm that simultaneously matches the lower bound of\n$\\Omega(\\sqrt{H^3SAT})$ for large enough $T$ and has a second-order term (with\nrespect to the horizon $T$) that scales only linearly with the number of states\n$S$.",
    "published_date": "2021-03-01T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.01312v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.01168v1",
    "title": "Narratives and Counternarratives on Data Sharing in Africa",
    "authors": [
      "Rediet Abebe",
      "Kehinde Aruleba",
      "Abeba Birhane",
      "Sara Kingsley",
      "George Obaido",
      "Sekou L. Remy",
      "Swathi Sadagopan"
    ],
    "author_ids": [],
    "abstract": "As machine learning and data science applications grow ever more prevalent,\nthere is an increased focus on data sharing and open data initiatives,\nparticularly in the context of the African continent. Many argue that data\nsharing can support research and policy design to alleviate poverty,\ninequality, and derivative effects in Africa. Despite the fact that the\ndatasets in question are often extracted from African communities,\nconversations around the challenges of accessing and sharing African data are\ntoo often driven by nonAfrican stakeholders. These perspectives frequently\nemploy a deficit narratives, often focusing on lack of education, training, and\ntechnological resources in the continent as the leading causes of friction in\nthe data ecosystem. We argue that these narratives obfuscate and distort the\nfull complexity of the African data sharing landscape. In particular, we use\nstorytelling via fictional personas built from a series of interviews with\nAfrican data experts to complicate dominant narratives and to provide\ncounternarratives. Coupling these personas with research on data practices\nwithin the continent, we identify recurring barriers to data sharing as well as\ninequities in the distribution of data sharing benefits. In particular, we\ndiscuss issues arising from power imbalances resulting from the legacies of\ncolonialism, ethno-centrism, and slavery, disinvestment in building trust, lack\nof acknowledgement of historical and present-day extractive practices, and\nWestern-centric policies that are ill-suited to the African context. After\noutlining these problems, we discuss avenues for addressing them when sharing\ndata generated in the continent.",
    "published_date": "2021-03-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.01168v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.01093v3",
    "title": "Quantifying Indirect Gender Discrimination on Collaborative Platforms",
    "authors": [
      "Orsolya Vasarhelyi",
      "Balazs Vedres"
    ],
    "author_ids": [],
    "abstract": "Digital collaborative platforms have become crucial venues of career\nadvancement and individual success in many creative fields, from engineering to\nthe arts. Indirect gender discrimination is a key component to gendered\ndisadvantage on platforms. Such platforms carried the promise of opening\navenues of advancement to previously discriminated groups, such as women, as\nplatforms lack managerial gatekeepers with conventional prejudice. We analyzed\nthe extent of indirect gender discriminatory on two diverse platforms, GitHub\nand Behance, focused on software development and fine arts and design. We found\nthat the main cause of women's disadvantage in attention, success, and survival\nis largely due to indirect discrimination that varies between 60-90\\% of total\nfemale disadvantage. Men and women are penalized if they follow highly\nfemale-like behavior, while categorical gender's impact varies by outcome and\nfield. As platforms employ algorithmic tools and AI systems to manage users'\nactivity, visibility and recommend new projects to collaborate, stereotypes\nrooted in behavior can have long-lasting consequences.",
    "published_date": "2021-03-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.01093v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.00964v1",
    "title": "Practices for Engineering Trustworthy Machine Learning Applications",
    "authors": [
      "Alex Serban",
      "Koen van der Blom",
      "Holger Hoos",
      "Joost Visser"
    ],
    "author_ids": [],
    "abstract": "Following the recent surge in adoption of machine learning (ML), the negative\nimpact that improper use of ML can have on users and society is now also widely\nrecognised. To address this issue, policy makers and other stakeholders, such\nas the European Commission or NIST, have proposed high-level guidelines aiming\nto promote trustworthy ML (i.e., lawful, ethical and robust). However, these\nguidelines do not specify actions to be taken by those involved in building ML\nsystems. In this paper, we argue that guidelines related to the development of\ntrustworthy ML can be translated to operational practices, and should become\npart of the ML development life cycle. Towards this goal, we ran a multi-vocal\nliterature review, and mined operational practices from white and grey\nliterature. Moreover, we launched a global survey to measure practice adoption\nand the effects of these practices. In total, we identified 14 new practices,\nand used them to complement an existing catalogue of ML engineering practices.\nInitial analysis of the survey results reveals that so far, practice adoption\nfor trustworthy ML is relatively low. In particular, practices related to\nassuring security of ML components have very low adoption. Other practices\nenjoy slightly larger adoption, such as providing explanations to users. Our\nextended practice catalogue can be used by ML development teams to bridge the\ngap between high-level guidelines and actual development of trustworthy ML\nsystems; it is open for review and contribution",
    "published_date": "2021-03-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.00964v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.00950v2",
    "title": "On the Fairness of Generative Adversarial Networks (GANs)",
    "authors": [
      "Patrik Joslin Kenfack",
      "Daniil Dmitrievich Arapov",
      "Rasheed Hussain",
      "S. M. Ahsan Kazmi",
      "Adil Mehmood Khan"
    ],
    "author_ids": [],
    "abstract": "Generative adversarial networks (GANs) are one of the greatest advances in AI\nin recent years. With their ability to directly learn the probability\ndistribution of data, and then sample synthetic realistic data. Many\napplications have emerged, using GANs to solve classical problems in machine\nlearning, such as data augmentation, class unbalance problems, and fair\nrepresentation learning. In this paper, we analyze and highlight fairness\nconcerns of GANs model. In this regard, we show empirically that GANs models\nmay inherently prefer certain groups during the training process and therefore\nthey're not able to homogeneously generate data from different groups during\nthe testing phase. Furthermore, we propose solutions to solve this issue by\nconditioning the GAN model towards samples' group or using ensemble method\n(boosting) to allow the GAN model to leverage distributed structure of data\nduring the training phase and generate groups at equal rate during the testing\nphase.",
    "published_date": "2021-03-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.00950v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.00896v2",
    "title": "Bernoulli sums and Rényi entropy inequalities",
    "authors": [
      "Mokshay Madiman",
      "James Melbourne",
      "Cyril Roberto"
    ],
    "author_ids": [],
    "abstract": "We investigate the R\\'enyi entropy of independent sums of integer valued\nrandom variables through Fourier theoretic means, and give sharp comparisons\nbetween the variance and the R\\'enyi entropy, for Poisson-Bernoulli variables.\nAs applications we prove that a discrete ``min-entropy power'' is super\nadditive on independent variables up to a universal constant, and give new\nbounds on an entropic generalization of the Littlewood-Offord problem that are\nsharp in the ``Poisson regime''.",
    "published_date": "2021-03-01T00:00:00",
    "year": 2021,
    "categories": [
      "math.PR",
      "cs.IT",
      "math.CO",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.00896v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.00813v1",
    "title": "DST: Data Selection and joint Training for Learning with Noisy Labels",
    "authors": [
      "Yi Wei",
      "Xue Mei",
      "Xin Liu",
      "Pengxiang Xu"
    ],
    "author_ids": [],
    "abstract": "Training a deep neural network heavily relies on a large amount of training\ndata with accurate annotations. To alleviate this problem, various methods have\nbeen proposed to annotate the data automatically. However, automatically\ngenerating annotations will inevitably yields noisy labels. In this paper, we\npropose a Data Selection and joint Training (DST) method to automatically\nselect training samples with accurate annotations. Specifically, DST fits a\nmixture model according to the original annotation as well as the predicted\nlabel for each training sample, and the mixture model is utilized to\ndynamically divide the training dataset into a correctly labeled dataset, a\ncorrectly predicted set and a wrong dataset. Then, DST is trained with these\ndatasets in a supervised manner. Due to confirmation bias problem, we train the\ntwo networks alternately, and each network is tasked to establish the data\ndivision to teach another network. For each iteration, the correctly labeled\nand predicted labels are reweighted respectively by the probabilities from the\nmixture model, and a uniform distribution is used to generate the probabilities\nof the wrong samples. Experiments on CIFAR-10, CIFAR-100 and Clothing1M\ndemonstrate that DST is the comparable or superior to the state-of-the-art\nmethods.",
    "published_date": "2021-03-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.00813v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.00766v1",
    "title": "Computing Prices for Target Profits in Contracts",
    "authors": [
      "Ghurumuruhan Ganesan"
    ],
    "author_ids": [],
    "abstract": "Price discrimination for maximizing expected profit is a well-studied concept\nin economics and there are various methods that achieve the maximum given the\nuser type distribution and the budget constraints. In many applications,\nparticularly with regards to engineering and computing, it is often the case\nthan the user type distribution is unknown or not accurately known. In this\npaper, we therefore propose and study a mathematical framework for price\ndiscrimination with \\emph{target} profits under the contract-theoretic model.\nWe first consider service providers with a given user type profile and\ndetermine sufficient conditions for achieving a target profit. Our proof is\nconstructive in that it also provides a method to compute the quality-price tag\nmenu. Next we consider a dual scenario where the offered service qualities are\npredetermined and describe an iterative method to obtain nominal demand values\nthat best match the qualities offered by the service provider while achieving a\ntarget profit-user satisfaction margin. We also illustrate our methods with\ndesign examples in both cases.",
    "published_date": "2021-03-01T00:00:00",
    "year": 2021,
    "categories": [
      "econ.GN",
      "cs.IT",
      "math.IT",
      "q-fin.EC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.00766v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.00755v2",
    "title": "Adaptive Sampling for Minimax Fair Classification",
    "authors": [
      "Shubhanshu Shekhar",
      "Greg Fields",
      "Mohammad Ghavamzadeh",
      "Tara Javidi"
    ],
    "author_ids": [],
    "abstract": "Machine learning models trained on uncurated datasets can often end up\nadversely affecting inputs belonging to underrepresented groups. To address\nthis issue, we consider the problem of adaptively constructing training sets\nwhich allow us to learn classifiers that are fair in a minimax sense. We first\npropose an adaptive sampling algorithm based on the principle of optimism, and\nderive theoretical bounds on its performance. We also propose heuristic\nextensions of this algorithm suitable for application to large scale, practical\nproblems. Next, by deriving algorithm independent lower-bounds for a specific\nclass of problems, we show that the performance achieved by our adaptive scheme\ncannot be improved in general. We then validate the benefits of adaptively\nconstructing training sets via experiments on synthetic tasks with logistic\nregression classifiers, as well as on several real-world tasks using\nconvolutional neural networks (CNNs).",
    "published_date": "2021-03-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.00755v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.00752v1",
    "title": "Reasons, Values, Stakeholders: A Philosophical Framework for Explainable Artificial Intelligence",
    "authors": [
      "Atoosa Kasirzadeh"
    ],
    "author_ids": [],
    "abstract": "The societal and ethical implications of the use of opaque artificial\nintelligence systems for consequential decisions, such as welfare allocation\nand criminal justice, have generated a lively debate among multiple stakeholder\ngroups, including computer scientists, ethicists, social scientists, policy\nmakers, and end users. However, the lack of a common language or a\nmulti-dimensional framework to appropriately bridge the technical, epistemic,\nand normative aspects of this debate prevents the discussion from being as\nproductive as it could be. Drawing on the philosophical literature on the\nnature and value of explanations, this paper offers a multi-faceted framework\nthat brings more conceptual precision to the present debate by (1) identifying\nthe types of explanations that are most pertinent to artificial intelligence\npredictions, (2) recognizing the relevance and importance of social and ethical\nvalues for the evaluation of these explanations, and (3) demonstrating the\nimportance of these explanations for incorporating a diversified approach to\nimproving the design of truthful algorithmic ecosystems. The proposed\nphilosophical framework thus lays the groundwork for establishing a pertinent\nconnection between the technical and ethical aspects of artificial intelligence\nsystems.",
    "published_date": "2021-03-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.00752v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.01779v1",
    "title": "COVID-19 vs Social Media Apps: Does Privacy Really Matter?",
    "authors": [
      "Omar Haggag",
      "Sherif Haggag",
      "John Grundy",
      "Mohamed Abdelrazek"
    ],
    "author_ids": [],
    "abstract": "Many people around the world are worried about using or even downloading\nCOVID-19 contact tracing mobile apps. The main reported concerns are centered\naround privacy and ethical issues. At the same time, people are voluntarily\nusing Social Media apps at a significantly higher rate during the pandemic\nwithout similar privacy concerns compared with COVID-19 apps. To better\nunderstand these seemingly anomalous behaviours, we analysed the privacy\npolicies, terms & conditions and data use agreements of the most commonly used\nCOVID-19, Social Media & Productivity apps. We also developed a tool to extract\nand analyse nearly 2 million user reviews for these apps. Our results show that\nSocial Media & Productivity apps actually have substantially higher privacy and\nethical issues compared with the majority of COVID-19 apps. Surprisingly, lots\nof people indicated in their user reviews that they feel more secure as their\nprivacy are better handled in COVID-19 apps than in Social Media apps. On the\nother hand, most of the COVID-19 apps are less accessible and stable compared\nto most Social Media apps, which negatively impacted their store ratings and\nled users to uninstall COVID-19 apps more frequently. Our findings suggest that\nin order to effectively fight this pandemic, health officials and technologists\nwill need to better raise awareness among people about COVID-19 app behaviour\nand trustworthiness. This will allow people to better understand COVID-19 apps\nand encourage them to download and use these apps. Moreover, COVID-19 apps need\nmany accessibility enhancements to allow a wider range of users from different\nsocieties and cultures to access to these apps.",
    "published_date": "2021-03-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.01779v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.00453v2",
    "title": "Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP",
    "authors": [
      "Timo Schick",
      "Sahana Udupa",
      "Hinrich Schütze"
    ],
    "author_ids": [],
    "abstract": "When trained on large, unfiltered crawls from the internet, language models\npick up and reproduce all kinds of undesirable biases that can be found in the\ndata: they often generate racist, sexist, violent or otherwise toxic language.\nAs large models require millions of training examples to achieve good\nperformance, it is difficult to completely prevent them from being exposed to\nsuch content. In this paper, we first demonstrate a surprising finding:\npretrained language models recognize, to a considerable degree, their\nundesirable biases and the toxicity of the content they produce. We refer to\nthis capability as self-diagnosis. Based on this finding, we then propose a\ndecoding algorithm that, given only a textual description of the undesired\nbehavior, reduces the probability of a language model producing problematic\ntext. We refer to this approach as self-debiasing. Self-debiasing does not rely\non manually curated word lists, nor does it require any training data or\nchanges to the model's parameters. While we by no means eliminate the issue of\nlanguage models generating biased text, we believe our approach to be an\nimportant step in this direction.",
    "published_date": "2021-02-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.00453v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.00347v2",
    "title": "Better Together? How Externalities of Size Complicate Notions of Solidarity and Actuarial Fairness",
    "authors": [
      "Kate Donahue",
      "Solon Barocas"
    ],
    "author_ids": [],
    "abstract": "Consider a cost-sharing game with players of different contribution to the\ntotal cost: an example might be an insurance company calculating premiums for a\npopulation of mixed-risk individuals. Two natural and competing notions of\nfairness might be to a) charge each individual the same price or b) charge each\nindividual according to the cost that they bring to the pool. In the insurance\nliterature, these general approaches are referred to as \"solidarity\" and\n\"actuarial fairness\" and are commonly viewed as opposites. However, in\ninsurance (and many other natural settings), the cost-sharing game also\nexhibits \"externalities of size\": all else being equal, larger groups have\nlower average cost. In the insurance case, we analyze a model with\nexternalities of size due to a reduction in the variability of losses. We\nexplore how this complicates traditional understandings of fairness, drawing on\nliterature in cooperative game theory.\n  First, we explore solidarity: we show that it is possible for both groups\n(high and low risk) to strictly benefit by joining an insurance pool where\ncosts are evenly split, as opposed to being in separate risk pools. We build on\nthis by producing a pricing scheme that maximally subsidizes the high risk\ngroup, while maintaining an incentive for lower risk people to stay in the\ninsurance pool. Next, we demonstrate that with this new model, the price\ncharged to each individual has to depend on the risk of other participants,\nmaking naive actuarial fairness inefficient. Furthermore, we prove that stable\npricing schemes must be ones where players have the anti-social incentive of\ndesiring riskier partners, contradicting motivations for using actuarial\nfairness. Finally, we describe how these results relate to debates about\nfairness in machine learning and potential avenues for future research.",
    "published_date": "2021-02-27T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.00347v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.00263v2",
    "title": "A semismooth Newton method for implicitly constituted non-Newtonian fluids and its application to the numerical approximation of Bingham flow",
    "authors": [
      "P. A. Gazca-Orozco"
    ],
    "author_ids": [],
    "abstract": "We propose a semismooth Newton method for non-Newtonian models of\nincompressible flow where the constitutive relation between the shear stress\nand the symmetric velocity gradient is given implicitly; this class of\nconstitutive relations captures for instance the models of Bingham and\nHerschel-Bulkley. The proposed method avoids the use of variational\ninequalities and is based on a particularly simple regularisation for which the\n(weak) convergence of the approximate stresses is known to hold. The system is\nanalysed at the function space level and results in mesh-independent behaviour\nof the nonlinear iterations.",
    "published_date": "2021-02-27T00:00:00",
    "year": 2021,
    "categories": [
      "math.NA",
      "cs.NA",
      "math.AP",
      "65N30, 65J99, 76A05"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.00263v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.00110v1",
    "title": "MBNet: MOS Prediction for Synthesized Speech with Mean-Bias Network",
    "authors": [
      "Yichong Leng",
      "Xu Tan",
      "Sheng Zhao",
      "Frank Soong",
      "Xiang-Yang Li",
      "Tao Qin"
    ],
    "author_ids": [],
    "abstract": "Mean opinion score (MOS) is a popular subjective metric to assess the quality\nof synthesized speech, and usually involves multiple human judges to evaluate\neach speech utterance. To reduce the labor cost in MOS test, multiple methods\nhave been proposed to automatically predict MOS scores. To our knowledge, for a\nspeech utterance, all previous works only used the average of multiple scores\nfrom different judges as the training target and discarded the score of each\nindividual judge, which did not well exploit the precious MOS training data. In\nthis paper, we propose MBNet, a MOS predictor with a mean subnet and a bias\nsubnet to better utilize every judge score in MOS datasets, where the mean\nsubnet is used to predict the mean score of each utterance similar to that in\nprevious works, and the bias subnet to predict the bias score (the difference\nbetween the mean score and each individual judge score) and capture the\npersonal preference of individual judges. Experiments show that compared with\nMOSNet baseline that only leverages mean score for training, MBNet improves the\nsystem-level spearmans rank correlation co-efficient (SRCC) by 2.9% on VCC 2018\ndataset and 6.7% on VCC 2016 dataset.",
    "published_date": "2021-02-27T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SD",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.00110v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.13645v2",
    "title": "Convolution-Free Medical Image Segmentation using Transformers",
    "authors": [
      "Davood Karimi",
      "Serge Vasylechko",
      "Ali Gholipour"
    ],
    "author_ids": [],
    "abstract": "Like other applications in computer vision, medical image segmentation has\nbeen most successfully addressed using deep learning models that rely on the\nconvolution operation as their main building block. Convolutions enjoy\nimportant properties such as sparse interactions, weight sharing, and\ntranslation equivariance. These properties give convolutional neural networks\n(CNNs) a strong and useful inductive bias for vision tasks. In this work we\nshow that a different method, based entirely on self-attention between\nneighboring image patches and without any convolution operations, can achieve\ncompetitive or better results. Given a 3D image block, our network divides it\ninto $n^3$ 3D patches, where $n=3 \\text{ or } 5$ and computes a 1D embedding\nfor each patch. The network predicts the segmentation map for the center patch\nof the block based on the self-attention between these patch embeddings. We\nshow that the proposed model can achieve segmentation accuracies that are\nbetter than the state of the art CNNs on three datasets. We also propose\nmethods for pre-training this model on large corpora of unlabeled images. Our\nexperiments show that with pre-training the advantage of our proposed network\nover CNNs can be significant when labeled training data is small.",
    "published_date": "2021-02-26T00:00:00",
    "year": 2021,
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.13645v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.13597v1",
    "title": "Evolution of collective fairness in complex networks through degree-based role assignment",
    "authors": [
      "Andreia Sofia Teixeira",
      "Francisco C. Santos",
      "Alexandre P. Francisco",
      "Fernando P. Santos"
    ],
    "author_ids": [],
    "abstract": "From social contracts to climate agreements, individuals engage in groups\nthat must collectively reach decisions with varying levels of equality and\nfairness. These dilemmas also pervade Distributed Artificial Intelligence, in\ndomains such as automated negotiation, conflict resolution or resource\nallocation. As evidenced by the well-known Ultimatum Game -- where a Proposer\nhas to divide a resource with a Responder -- payoff-maximizing outcomes are\nfrequently at odds with fairness. Eliciting equality in populations of\nself-regarding agents requires judicious interventions. Here we use knowledge\nabout agents' social networks to implement fairness mechanisms, in the context\nof Multiplayer Ultimatum Games. We focus on network-based role assignment and\nshow that preferentially attributing the role of Proposer to low-connected\nnodes increases the fairness levels in a population. We evaluate the\neffectiveness of low-degree Proposer assignment considering networks with\ndifferent average connectivity, group sizes, and group voting rules when\naccepting proposals (e.g. majority or unanimity). We further show that\nlow-degree Proposer assignment is efficient, not only optimizing fairness, but\nalso the average payoff level in the population. Finally, we show that stricter\nvoting rules (i.e., imposing an accepting consensus as requirement for\ncollectives to accept a proposal) attenuates the unfairness that results from\nsituations where high-degree nodes (hubs) are the natural candidates to play as\nProposers. Our results suggest new routes to use role assignment and voting\nmechanisms to prevent unfair behaviors from spreading on complex networks.",
    "published_date": "2021-02-26T00:00:00",
    "year": 2021,
    "categories": [
      "physics.soc-ph",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.13597v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.13387v2",
    "title": "Ethical Issues in Empirical Studies using Student Subjects: Re-visiting Practices and Perceptions",
    "authors": [
      "Grischa Liebel",
      "Shalini Chakraborty"
    ],
    "author_ids": [],
    "abstract": "Context: Using student subjects in empirical studies has been discussed\nextensively from a methodological perspective in Software Engineering (SE), but\nthere is a lack of similar discussion surrounding ethical aspects of doing so.\nAs students are in a subordinate relationship to their instructors, such a\ndiscussion is needed. Objective: We aim to increase the understanding of\npractices and perceptions SE researchers have of ethical issues with student\nparticipation in empirical studies. Method: We conducted a systematic mapping\nstudy of 372 empirical SE studies involving students, following up with a\nsurvey answered by 100 SE researchers regarding their current practices and\nopinions regarding student participation. Results: The mapping study shows that\nthe majority of studies does not report conditions regarding recruitment,\nvoluntariness, compensation, and ethics approval. In contrast, the majority of\nsurvey participants supports reporting these conditions. The survey further\nreveals that less than half of the participants require ethics approval.\nAdditionally, the majority of participants recruit their own students on a\nvoluntary basis, and use informed consent with withdrawal options. There is\ndisagreement among the participants whether course instructors should be\ninvolved in research studies and if should know who participates in a study.\nConclusions: It is a positive sign that mandatory participation is rare, and\nthat informed consent and withdrawal options are standard. However, we see\nimmediate need for action, as study conditions are under-reported, and as\nopinions on ethical practices differ widely. In particular, there is little\nregard in SE on the power relationship between instructors and students.",
    "published_date": "2021-02-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.13387v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.13327v2",
    "title": "Mitigating Domain Mismatch in Face Recognition Using Style Matching",
    "authors": [
      "Chun-Hsien Lin",
      "Bing-Fei Wu"
    ],
    "author_ids": [],
    "abstract": "Despite outstanding performance on public benchmarks, face recognition still\nsuffers due to domain mismatch between training (source) and testing (target)\ndata. Furthermore, these domains are not shared classes, which complicates\ndomain adaptation. Since this is also a fine-grained classification problem\nwhich does not strictly follow the low-density separation principle,\nconventional domain adaptation approaches do not resolve these problems. In\nthis paper, we formulate domain mismatch in face recognition as a style\nmismatch problem for which we propose two methods. First, we design a domain\ndiscriminator with human-level judgment to mine target-like images in the\ntraining data to mitigate the domain gap. Second, we extract style\nrepresentations in low-level feature maps of the backbone model, and match the\nstyle distributions of the two domains to find a common style representation.\nEvaluations on verification and open-set and closed-set identification\nprotocols show that both methods yield good improvements, and that performance\nis more robust if they are combined. Our approach is competitive with related\nwork, and its effectiveness is verified in a practical application.",
    "published_date": "2021-02-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.13327v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.13304v1",
    "title": "Feasibility Enhancement of Constrained Receding Horizon Control Using Generalized Control Barrier Function",
    "authors": [
      "Haitong Ma",
      "Xiangteng Zhang",
      "Shengbo Eben Li",
      "Ziyu Lin",
      "Yao Lyu",
      "Sifa Zheng"
    ],
    "author_ids": [],
    "abstract": "Receding horizon control (RHC) is a popular procedure to deal with optimal\ncontrol problems. Due to the existence of state constraints, optimization-based\nRHC often suffers the notorious issue of infeasibility, which strongly shrinks\nthe region of controllable state. This paper proposes a generalized control\nbarrier function (CBF) to enlarge the feasible region of constrained RHC with\nonly a one-step constraint on the prediction horizon. This design can reduce\nthe constrained steps by penalizing the tendency to move towards the constraint\nboundary. Additionally, generalized CBF is able to handle high-order equality\nor inequality constraints through extending the constrained step to nonadjacent\nnodes. We apply this technique on an automated vehicle control task. The\nresults show that compared to multi-step pointwise constraints, generalized CBF\ncan effectively avoid the infeasibility issue in a larger partition of the\nstate space, and the computing efficiency is also improved by 14%-23%.",
    "published_date": "2021-02-26T00:00:00",
    "year": 2021,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.13304v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.13291v2",
    "title": "Algorithmic Correspondence for Hybrid Logic with Binder",
    "authors": [
      "Zhiguang Zhao"
    ],
    "author_ids": [],
    "abstract": "In the present paper, we develop the algorithmic correspondence theory for\nhybrid logic with binder. We define the class of Sahlqvist inequalities, each\ninequality of which is shown to have a first-order frame correspondent\neffectively computable by an algorithm ALBA.",
    "published_date": "2021-02-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LO",
      "math.LO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.13291v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.13190v1",
    "title": "Machine Biometrics -- Towards Identifying Machines in a Smart City Environment",
    "authors": [
      "G. K. Sidiropoulos",
      "G. A. Papakostas"
    ],
    "author_ids": [],
    "abstract": "This paper deals with the identification of machines in a smart city\nenvironment. The concept of machine biometrics is proposed in this work for the\nfirst time, as a way to authenticate machine identities interacting with humans\nin everyday life. This definition is imposed in modern years where autonomous\nvehicles, social robots, etc. are considered active members of contemporary\nsocieties. In this context, the case of car identification from the engine\nbehavioral biometrics is examined. For this purpose, 22 sound features were\nextracted and their discrimination capabilities were tested in combination with\n9 different machine learning classifiers, towards identifying 5 car\nmanufacturers. The experimental results revealed the ability of the proposed\nbiometrics to identify cars with high accuracy up to 98% for the case of the\nMultilayer Perceptron (MLP) neural network model.",
    "published_date": "2021-02-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CV",
      "I.5.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.13190v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.13186v3",
    "title": "Towards a Unified Framework for Fair and Stable Graph Representation Learning",
    "authors": [
      "Chirag Agarwal",
      "Himabindu Lakkaraju",
      "Marinka Zitnik"
    ],
    "author_ids": [],
    "abstract": "As the representations output by Graph Neural Networks (GNNs) are\nincreasingly employed in real-world applications, it becomes important to\nensure that these representations are fair and stable. In this work, we\nestablish a key connection between counterfactual fairness and stability and\nleverage it to propose a novel framework, NIFTY (uNIfying Fairness and\nstabiliTY), which can be used with any GNN to learn fair and stable\nrepresentations. We introduce a novel objective function that simultaneously\naccounts for fairness and stability and develop a layer-wise weight\nnormalization using the Lipschitz constant to enhance neural message passing in\nGNNs. In doing so, we enforce fairness and stability both in the objective\nfunction as well as in the GNN architecture. Further, we show theoretically\nthat our layer-wise weight normalization promotes counterfactual fairness and\nstability in the resulting representations. We introduce three new graph\ndatasets comprising of high-stakes decisions in criminal justice and financial\nlending domains. Extensive experimentation with the above datasets demonstrates\nthe efficacy of our framework.",
    "published_date": "2021-02-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.13186v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.13167v2",
    "title": "Images, Emotions, and Credibility: Effect of Emotional Facial Images on Perceptions of News Content Bias and Source Credibility in Social Media",
    "authors": [
      "Alireza Karduni",
      "Ryan Wesslen",
      "Douglas Markant",
      "Wenwen Dou"
    ],
    "author_ids": [],
    "abstract": "Images are an indispensable part of the news content we consume. Highly\nemotional images from sources of misinformation can greatly influence our\njudgements. We present two studies on the effects of emotional facial images on\nusers' perception of bias in news content and the credibility of sources. In\nstudy 1, we investigate the impact of happy and angry facial images on users'\ndecisions. In study 2, we focus on sources' systematic emotional treatment of\nspecific politicians. Our results show that depending on the political\norientation of the source, the cumulative effect of angry facial emotions\nimpacts users' perceived content bias and source credibility. When sources\nsystematically portray specific politicians as angry, users are more likely to\nfind those sources as less credible and their content as more biased. These\nresults highlight how implicit visual propositions manifested by emotions in\nfacial expressions might have a substantial effect on our trust of news content\nand sources.",
    "published_date": "2021-02-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.13167v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.13076v1",
    "title": "Benchmarking and Survey of Explanation Methods for Black Box Models",
    "authors": [
      "Francesco Bodria",
      "Fosca Giannotti",
      "Riccardo Guidotti",
      "Francesca Naretto",
      "Dino Pedreschi",
      "Salvatore Rinzivillo"
    ],
    "author_ids": [],
    "abstract": "The widespread adoption of black-box models in Artificial Intelligence has\nenhanced the need for explanation methods to reveal how these obscure models\nreach specific decisions. Retrieving explanations is fundamental to unveil\npossible biases and to resolve practical or ethical issues. Nowadays, the\nliterature is full of methods with different explanations. We provide a\ncategorization of explanation methods based on the type of explanation\nreturned. We present the most recent and widely used explainers, and we show a\nvisual comparison among explanations and a quantitative benchmarking.",
    "published_date": "2021-02-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.13076v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.13004v2",
    "title": "Towards Unbiased and Accurate Deferral to Multiple Experts",
    "authors": [
      "Vijay Keswani",
      "Matthew Lease",
      "Krishnaram Kenthapadi"
    ],
    "author_ids": [],
    "abstract": "Machine learning models are often implemented in cohort with humans in the\npipeline, with the model having an option to defer to a domain expert in cases\nwhere it has low confidence in its inference. Our goal is to design mechanisms\nfor ensuring accuracy and fairness in such prediction systems that combine\nmachine learning model inferences and domain expert predictions. Prior work on\n\"deferral systems\" in classification settings has focused on the setting of a\npipeline with a single expert and aimed to accommodate the inaccuracies and\nbiases of this expert to simultaneously learn an inference model and a deferral\nsystem. Our work extends this framework to settings where multiple experts are\navailable, with each expert having their own domain of expertise and biases. We\npropose a framework that simultaneously learns a classifier and a deferral\nsystem, with the deferral system choosing to defer to one or more human experts\nin cases of input where the classifier has low confidence. We test our\nframework on a synthetic dataset and a content moderation dataset with biased\nsynthetic experts, and show that it significantly improves the accuracy and\nfairness of the final predictions, compared to the baselines. We also collect\ncrowdsourced labels for the content moderation task to construct a real-world\ndataset for the evaluation of hybrid machine-human frameworks and show that our\nproposed learning framework outperforms baselines on this real-world dataset as\nwell.",
    "published_date": "2021-02-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.HC",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.13004v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.12962v3",
    "title": "Bias-reduced Multi-step Hindsight Experience Replay for Efficient Multi-goal Reinforcement Learning",
    "authors": [
      "Rui Yang",
      "Jiafei Lyu",
      "Yu Yang",
      "Jiangpeng Yan",
      "Feng Luo",
      "Dijun Luo",
      "Lanqing Li",
      "Xiu Li"
    ],
    "author_ids": [],
    "abstract": "Multi-goal reinforcement learning is widely applied in planning and robot\nmanipulation. Two main challenges in multi-goal reinforcement learning are\nsparse rewards and sample inefficiency. Hindsight Experience Replay (HER) aims\nto tackle the two challenges via goal relabeling. However, HER-related works\nstill need millions of samples and a huge computation. In this paper, we\npropose Multi-step Hindsight Experience Replay (MHER), incorporating multi-step\nrelabeled returns based on $n$-step relabeling to improve sample efficiency.\nDespite the advantages of $n$-step relabeling, we theoretically and\nexperimentally prove the off-policy $n$-step bias introduced by $n$-step\nrelabeling may lead to poor performance in many environments. To address the\nabove issue, two bias-reduced MHER algorithms, MHER($\\lambda$) and Model-based\nMHER (MMHER) are presented. MHER($\\lambda$) exploits the $\\lambda$ return while\nMMHER benefits from model-based value expansions. Experimental results on\nnumerous multi-goal robotic tasks show that our solutions can successfully\nalleviate off-policy $n$-step bias and achieve significantly higher sample\nefficiency than HER and Curriculum-guided HER with little additional\ncomputation beyond HER.",
    "published_date": "2021-02-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.12962v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.12953v1",
    "title": "Optimized Memoryless Fair-Share HPC Resources Scheduling using Transparent Checkpoint-Restart Preemption",
    "authors": [
      "Kfir Zvi",
      "Gal Oren"
    ],
    "author_ids": [],
    "abstract": "Common resource management methods in supercomputing systems usually include\nhard divisions, capping, and quota allotment. Those methods, despite their\n'advantages', have some known serious disadvantages including unoptimized\nutilization of an expensive facility, and occasionally there is still a need to\ndynamically reschedule and reallocate the resources. Consequently, those\nmethods involve bad supply-and-demand management rather than a free market\nplayground that will eventually increase system utilization and productivity.\nIn this work, we propose the newly Optimized Memoryless Fair-Share HPC\nResources Scheduling using Transparent Checkpoint-Restart Preemption, in which\nthe social welfare increases using a free-of-cost interchangeable proprietary\npossession scheme. Accordingly, we permanently keep the status-quo in regard to\nthe fairness of the resources distribution while maximizing the ability of all\nusers to achieve more CPUs and CPU hours for longer period without any\nnon-straightforward costs, penalties or additional human intervention.",
    "published_date": "2021-02-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.12953v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.12799v1",
    "title": "Cognitive network science for understanding online social cognitions: A brief review",
    "authors": [
      "Massimo Stella"
    ],
    "author_ids": [],
    "abstract": "Social media are digitalising massive amounts of users' cognitions in terms\nof timelines and emotional content. Such Big Data opens unprecedented\nopportunities for investigating cognitive phenomena like perception,\npersonality and information diffusion but requires suitable interpretable\nframeworks. Since social media data come from users' minds, worthy candidates\nfor this challenge are cognitive networks, models of cognition giving structure\nto mental conceptual associations. This work outlines how cognitive network\nscience can open new, quantitative ways for understanding cognition through\nonline media, like: (i) reconstructing how users semantically and emotionally\nframe events with contextual knowledge unavailable to machine learning, (ii)\ninvestigating conceptual salience/prominence through knowledge structure in\nsocial discourse; (iii) studying users' personality traits like\nopenness-to-experience, curiosity, and creativity through language in posts;\n(iv) bridging cognitive/emotional content and social dynamics via multilayer\nnetworks comparing the mindsets of influencers and followers. These\nadvancements combine cognitive-, network- and computer science to understand\ncognitive mechanisms in both digital and real-world settings but come with\nlimitations concerning representativeness, individual variability and data\nintegration. Such aspects are discussed along the ethical implications of\nmanipulating socio-cognitive data. In the future, reading cognitions through\nnetworks and social media can expose cognitive biases amplified by online\nplatforms and relevantly inform policy making, education and markets about\nmassive, complex cognitive trends.",
    "published_date": "2021-02-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.CL",
      "cs.SI",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.12799v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.01766v1",
    "title": "COVID-19 Digital Contact Tracing Applications and Techniques: A Review Post Initial Deployments",
    "authors": [
      "Muhammad Shahroz",
      "Farooq Ahmad",
      "Muhammad Shahzad Younis",
      "Nadeem Ahmad",
      "Maged N. Kamel Boulos",
      "Ricardo Vinuesa",
      "Junaid Qadir"
    ],
    "author_ids": [],
    "abstract": "The coronavirus disease 2019 (COVID-19) is a severe global pandemic that has\nclaimed millions of lives and continues to overwhelm public health systems in\nmany countries. The spread of COVID-19 pandemic has negatively impacted the\nhuman mobility patterns such as daily transportation-related behavior of the\npublic. There is a requirement to understand the disease spread patterns and\nits routes among neighboring individuals for the timely implementation of\ncorrective measures at the required placement. To increase the effectiveness of\ncontact tracing, countries across the globe are leveraging advancements in\nmobile technology and Internet of Things (IoT) to aid traditional manual\ncontact tracing to track individuals who have come in close contact with\nidentified COVID-19 patients. Even as the first administration of vaccines\nbegins in 2021, the COVID-19 management strategy will continue to be\nmulti-pronged for the foreseeable future with digital contact tracing being a\nvital component of the response along with the use of preventive measures such\nas social distancing and the use of face masks. After some months of deployment\nof digital contact tracing technology, deeper insights into the merits of\nvarious approaches and the usability, privacy, and ethical trade-offs involved\nare emerging. In this paper, we provide a comprehensive analysis of digital\ncontact tracing solutions in terms of their methodologies and technologies in\nthe light of the new data emerging about international experiences of\ndeployments of digital contact tracing technology. We also provide a discussion\non open challenges such as scalability, privacy, adaptability and highlight\npromising directions for future work.",
    "published_date": "2021-02-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.01766v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.12736v2",
    "title": "Time-Series Imputation with Wasserstein Interpolation for Optimal Look-Ahead-Bias and Variance Tradeoff",
    "authors": [
      "Jose Blanchet",
      "Fernando Hernandez",
      "Viet Anh Nguyen",
      "Markus Pelger",
      "Xuhui Zhang"
    ],
    "author_ids": [],
    "abstract": "Missing time-series data is a prevalent practical problem. Imputation methods\nin time-series data often are applied to the full panel data with the purpose\nof training a model for a downstream out-of-sample task. For example, in\nfinance, imputation of missing returns may be applied prior to training a\nportfolio optimization model. Unfortunately, this practice may result in a\nlook-ahead-bias in the future performance on the downstream task. There is an\ninherent trade-off between the look-ahead-bias of using the full data set for\nimputation and the larger variance in the imputation from using only the\ntraining data. By connecting layers of information revealed in time, we propose\na Bayesian posterior consensus distribution which optimally controls the\nvariance and look-ahead-bias trade-off in the imputation. We demonstrate the\nbenefit of our methodology both in synthetic and real financial data.",
    "published_date": "2021-02-25T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.12736v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.12675v2",
    "title": "Computing Accurate Probabilistic Estimates of One-D Entropy from Equiprobable Random Samples",
    "authors": [
      "Hoshin V Gupta",
      "Mohammed Reza Ehsani",
      "Tirthankar Roy",
      "Maria A Sans-Fuentes",
      "Uwe Ehret",
      "Ali Behrangi"
    ],
    "author_ids": [],
    "abstract": "We develop a simple Quantile Spacing (QS) method for accurate probabilistic\nestimation of one-dimensional entropy from equiprobable random samples, and\ncompare it with the popular Bin-Counting (BC) method. In contrast to BC, which\nuses equal-width bins with varying probability mass, the QS method uses\nestimates of the quantiles that divide the support of the data generating\nprobability density function (pdf) into equal-probability-mass intervals.\nWhereas BC requires optimal tuning of a bin-width hyper-parameter whose value\nvaries with sample size and shape of the pdf, QS requires specification of the\nnumber of quantiles to be used. Results indicate, for the class of\ndistributions tested, that the optimal number of quantile-spacings is a fixed\nfraction of the sample size (empirically determined to be ~0.25-0.35), and that\nthis value is relatively insensitive to distributional form or sample size,\nproviding a clear advantage over BC since hyperparameter tuning is not\nrequired. Bootstrapping is used to approximate the sampling variability\ndistribution of the resulting entropy estimate, and is shown to accurately\nreflect the true uncertainty. For the four distributional forms studied\n(Gaussian, Log-Normal, Exponential and Bimodal Gaussian Mixture), expected\nestimation bias is less than 1% and uncertainty is relatively low even for very\nsmall sample sizes. We speculate that estimating quantile locations, rather\nthan bin-probabilities, results in more efficient use of the information in the\ndata to approximate the underlying shape of an unknown data generating pdf.",
    "published_date": "2021-02-25T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ME",
      "cs.IT",
      "math.IT",
      "94A15"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.12675v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.12606v1",
    "title": "3D4ALL: Toward an Inclusive Pipeline to Classify 3D Contents",
    "authors": [
      "Nahyun Kwon",
      "Chen Liang",
      "Jeeeun Kim"
    ],
    "author_ids": [],
    "abstract": "Algorithmic content moderation manages an explosive number of user-created\ncontent shared online everyday. Despite a massive number of 3D designs that are\nfree to be downloaded, shared, and 3D printed by the users, detecting\nsensitivity with transparency and fairness has been controversial. Although\nsensitive 3D content might have a greater impact than other media due to its\npossible reproducibility and replicability without restriction, prevailed\nunawareness resulted in proliferation of sensitive 3D models online and a lack\nof discussion on transparent and fair 3D content moderation. As the 3D content\nexists as a document on the web mainly consisting of text and images, we first\nstudy the existing algorithmic efforts based on text and images and the prior\nendeavors to encompass transparency and fairness in moderation, which can also\nbe useful in a 3D printing domain. At the same time, we identify 3D specific\nfeatures that should be addressed to advance a 3D specialized algorithmic\nmoderation. As a potential solution, we suggest a human-in-the-loop pipeline\nusing augmented learning, powered by various stakeholders with different\nbackgrounds and perspectives in understanding the content. Our pipeline aims to\nminimize personal biases by enabling diverse stakeholders to be vocal in\nreflecting various factors to interpret the content. We add our initial\nproposal for redesigning metadata of open 3D repositories, to invoke users'\nresponsible actions of being granted consent from the subject upon sharing\ncontents for free in the public spaces.",
    "published_date": "2021-02-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.12606v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.12594v2",
    "title": "Directional Bias Amplification",
    "authors": [
      "Angelina Wang",
      "Olga Russakovsky"
    ],
    "author_ids": [],
    "abstract": "Mitigating bias in machine learning systems requires refining our\nunderstanding of bias propagation pathways: from societal structures to\nlarge-scale data to trained models to impact on society. In this work, we focus\non one aspect of the problem, namely bias amplification: the tendency of models\nto amplify the biases present in the data they are trained on. A metric for\nmeasuring bias amplification was introduced in the seminal work by Zhao et al.\n(2017); however, as we demonstrate, this metric suffers from a number of\nshortcomings including conflating different types of bias amplification and\nfailing to account for varying base rates of protected attributes. We introduce\nand analyze a new, decoupled metric for measuring bias amplification,\n$\\text{BiasAmp}_{\\rightarrow}$ (Directional Bias Amplification). We thoroughly\nanalyze and discuss both the technical assumptions and normative implications\nof this metric. We provide suggestions about its measurement by cautioning\nagainst predicting sensitive attributes, encouraging the use of confidence\nintervals due to fluctuations in the fairness of models across runs, and\ndiscussing the limitations of what this metric captures. Throughout this paper,\nwe work to provide an interrogative look at the technical measurement of bias\namplification, guided by our normative ideas of what we want it to encompass.\nCode is located at https://github.com/princetonvisualai/directional-bias-amp",
    "published_date": "2021-02-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.12594v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.12586v5",
    "title": "A Stochastic Optimization Framework for Fair Risk Minimization",
    "authors": [
      "Andrew Lowy",
      "Sina Baharlouei",
      "Rakesh Pavan",
      "Meisam Razaviyayn",
      "Ahmad Beirami"
    ],
    "author_ids": [],
    "abstract": "Despite the success of large-scale empirical risk minimization (ERM) at\nachieving high accuracy across a variety of machine learning tasks, fair ERM is\nhindered by the incompatibility of fairness constraints with stochastic\noptimization. We consider the problem of fair classification with discrete\nsensitive attributes and potentially large models and data sets, requiring\nstochastic solvers. Existing in-processing fairness algorithms are either\nimpractical in the large-scale setting because they require large batches of\ndata at each iteration or they are not guaranteed to converge. In this paper,\nwe develop the first stochastic in-processing fairness algorithm with\nguaranteed convergence. For demographic parity, equalized odds, and equal\nopportunity notions of fairness, we provide slight variations of our\nalgorithm--called FERMI--and prove that each of these variations converges in\nstochastic optimization with any batch size. Empirically, we show that FERMI is\namenable to stochastic solvers with multiple (non-binary) sensitive attributes\nand non-binary targets, performing well even with minibatch size as small as\none. Extensive experiments show that FERMI achieves the most favorable\ntradeoffs between fairness violation and test accuracy across all tested setups\ncompared with state-of-the-art baselines for demographic parity, equalized\nodds, equal opportunity. These benefits are especially significant with small\nbatch sizes and for non-binary classification with large number of sensitive\nattributes, making FERMI a practical, scalable fairness algorithm. The code for\nall of the experiments in this paper is available at:\nhttps://github.com/optimization-for-data-driven-science/FERMI.",
    "published_date": "2021-02-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.12586v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.12406v1",
    "title": "Actionable Principles for Artificial Intelligence Policy: Three Pathways",
    "authors": [
      "Charlotte Stix"
    ],
    "author_ids": [],
    "abstract": "In the development of governmental policy for artificial intelligence (AI)\nthat is informed by ethics, one avenue currently pursued is that of drawing on\nAI Ethics Principles. However, these AI Ethics Principles often fail to be\nactioned in governmental policy. This paper proposes a novel framework for the\ndevelopment of Actionable Principles for AI. The approach acknowledges the\nrelevance of AI Ethics Principles and homes in on methodological elements to\nincrease their practical implementability in policy processes. As a case study,\nelements are extracted from the development process of the Ethics Guidelines\nfor Trustworthy AI of the European Commissions High Level Expert Group on AI.\nSubsequently, these elements are expanded on and evaluated in light of their\nability to contribute to a prototype framework for the development of\nActionable Principles for AI. The paper proposes the following three\npropositions for the formation of such a prototype framework: (1) preliminary\nlandscape assessments; (2) multi-stakeholder participation and cross-sectoral\nfeedback; and, (3) mechanisms to support implementation and\noperationalizability.",
    "published_date": "2021-02-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.12406v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.12258v1",
    "title": "Classification with abstention but without disparities",
    "authors": [
      "Nicolas Schreuder",
      "Evgenii Chzhen"
    ],
    "author_ids": [],
    "abstract": "Classification with abstention has gained a lot of attention in recent years\nas it allows to incorporate human decision-makers in the process. Yet,\nabstention can potentially amplify disparities and lead to discriminatory\npredictions. The goal of this work is to build a general purpose classification\nalgorithm, which is able to abstain from prediction, while avoiding disparate\nimpact. We formalize this problem as risk minimization under fairness and\nabstention constraints for which we derive the form of the optimal classifier.\nBuilding on this result, we propose a post-processing classification algorithm,\nwhich is able to modify any off-the-shelf score-based classifier using only\nunlabeled sample. We establish finite sample risk, fairness, and abstention\nguarantees for the proposed algorithm. In particular, it is shown that fairness\nand abstention constraints can be achieved independently from the initial\nclassifier as long as sufficiently many unlabeled data is available. The risk\nguarantee is established in terms of the quality of the initial classifier. Our\npost-processing scheme reduces to a sparse linear program allowing for an\nefficient implementation, which we provide. Finally, we validate our method\nempirically showing that moderate abstention rates allow to bypass the\nrisk-fairness trade-off.",
    "published_date": "2021-02-24T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.12258v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.11957v1",
    "title": "Quantifying Confounding Bias in Generative Art: A Case Study",
    "authors": [
      "Ramya Srinivasan",
      "Kanji Uchino"
    ],
    "author_ids": [],
    "abstract": "In recent years, AI generated art has become very popular. From generating\nart works in the style of famous artists like Paul Cezanne and Claude Monet to\nsimulating styles of art movements like Ukiyo-e, a variety of creative\napplications have been explored using AI. Looking from an art historical\nperspective, these applications raise some ethical questions. Can AI model\nartists' styles without stereotyping them? Does AI do justice to the\nsocio-cultural nuances of art movements? In this work, we take a first step\ntowards analyzing these issues. Leveraging directed acyclic graphs to represent\npotential process of art creation, we propose a simple metric to quantify\nconfounding bias due to the lack of modeling the influence of art movements in\nlearning artists' styles. As a case study, we consider the popular cycleGAN\nmodel and analyze confounding bias across various genres. The proposed metric\nis more effective than state-of-the-art outlier detection method in\nunderstanding the influence of art movements in artworks. We hope our work will\nelucidate important shortcomings of computationally modeling artists' styles\nand trigger discussions related to accountability of AI generated art.",
    "published_date": "2021-02-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.11957v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.11932v3",
    "title": "On Meritocracy in Optimal Set Selection",
    "authors": [
      "Thomas Kleine Buening",
      "Meirav Segal",
      "Debabrota Basu",
      "Christos Dimitrakakis",
      "Anne-Marie George"
    ],
    "author_ids": [],
    "abstract": "Typically, merit is defined with respect to some intrinsic measure of worth.\nWe instead consider a setting where an individual's worth is \\emph{relative}:\nwhen a Decision Maker (DM) selects a set of individuals from a population to\nmaximise expected utility, it is natural to consider the \\emph{Expected\nMarginal Contribution} (EMC) of each person to the utility. We show that this\nnotion satisfies an axiomatic definition of fairness for this setting. We also\nshow that for certain policy structures, this notion of fairness is aligned\nwith maximising expected utility, while for linear utility functions it is\nidentical to the Shapley value. However, for certain natural policies, such as\nthose that select individuals with a specific set of attributes (e.g. high\nenough test scores for college admissions), there is a trade-off between\nmeritocracy and utility maximisation. We analyse the effect of constraints on\nthe policy on both utility and fairness in extensive experiments based on\ncollege admissions and outcomes in Norwegian universities.",
    "published_date": "2021-02-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.11932v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.11929v4",
    "title": "PolicySpace2: modeling markets and endogenous public policies",
    "authors": [
      "Bernardo Alves Furtado"
    ],
    "author_ids": [],
    "abstract": "Policymakers decide on alternative policies facing restricted budgets and\nuncertain, ever-changing future. Designing public policies is further difficult\ndue to the need to decide on priorities and handle effects across policies.\nHousing policies, specifically, involve heterogeneous characteristics of\nproperties themselves and the intricacy of housing markets and the spatial\ncontext of cities. We propose PolicySpace2 (PS2) as an adapted and extended\nversion of the open source PolicySpace agent-based model. PS2 is a computer\nsimulation that relies on empirically detailed spatial data to model real\nestate, along with labor, credit, and goods and services markets. Interaction\namong workers, firms, a bank, households and municipalities follow the\nliterature benchmarks to integrate economic, spatial and transport scholarship.\nPS2 is applied to a comparison among three competing public policies aimed at\nreducing inequality and alleviating poverty: (a) house acquisition by the\ngovernment and distribution to lower income households, (b) rental vouchers,\nand (c) monetary aid. Within the model context, the monetary aid, that is,\nsmaller amounts of help for a larger number of households, makes the economy\nperform better in terms of production, consumption, reduction of inequality,\nand maintenance of financial duties. PS2 as such is also a framework that may\nbe further adapted to a number of related research questions.",
    "published_date": "2021-02-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.MA",
      "econ.GN",
      "physics.soc-ph",
      "q-fin.EC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.11929v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.11925v3",
    "title": "Chasm in Hegemony: Explaining and Reproducing Disparities in Homophilous Networks",
    "authors": [
      "Yiguang Zhang",
      "Jessy Xinyi Han",
      "Ilica Mahajan",
      "Priyanjana Bengani",
      "Augustin Chaintreau"
    ],
    "author_ids": [],
    "abstract": "In networks with a minority and a majority community, it is well-studied that\nminorities are under-represented at the top of the social hierarchy. However,\nresearchers are less clear about the representation of minorities from the\nlower levels of the hierarchy, where other disadvantages or vulnerabilities may\nexist. We offer a more complete picture of social disparities at each social\nlevel with empirical evidence that the minority representation exhibits two\nopposite phases: at the higher rungs of the social ladder, the representation\nof the minority community decreases; but, lower in the ladder, which is more\npopulous, as you ascend, the representation of the minority community improves.\nWe refer to this opposing phenomenon between the upper-level and lower-level as\nthe \\emph{chasm effect}. Previous models of network growth with homophily fail\nto detect and explain the presence of this chasm effect. We analyze the\ninteractions among a few well-observed network-growing mechanisms with a simple\nmodel to reveal the sufficient and necessary conditions for both phases in the\nchasm effect to occur. By generalizing the simple model naturally, we present a\ncomplete bi-affiliation bipartite network-growth model that could successfully\ncapture disparities at all social levels and reproduce real social networks.\nFinally, we illustrate that addressing the chasm effect can create fairer\nsystems with two applications in advertisement and fact-checks, thereby\ndemonstrating the potential impact of the chasm effect on the future research\nof minority-majority disparities and fair algorithms.",
    "published_date": "2021-02-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.11925v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.11864v3",
    "title": "A Refined Complexity Analysis of Fair Districting over Graphs",
    "authors": [
      "Niclas Boehmer",
      "Tomohiro Koana",
      "Rolf Niedermeier"
    ],
    "author_ids": [],
    "abstract": "We study the NP-hard Fair Connected Districting problem recently proposed by\nStoica et al. [AAMAS 2020]: Partition a vertex-colored graph into k connected\ncomponents (subsequently referred to as districts) so that in every district\nthe most frequent color occurs at most a given number of times more often than\nthe second most frequent color. Fair Connected Districting is motivated by\nvarious real-world scenarios where agents of different types, which are\none-to-one represented by nodes in a network, have to be partitioned into\ndisjoint districts. Herein, one strives for \"fair districts\" without any type\nbeing in a dominating majority in any of the districts. This is to e.g. prevent\nsegregation or political domination of some political party. We conduct a\nfine-grained analysis of the (parameterized) computational complexity of Fair\nConnected Districting. In particular, we prove that it is polynomial-time\nsolvable on paths, cycles, stars, and caterpillars, but already becomes NP-hard\non trees. Motivated by the latter negative result, we perform a parameterized\ncomplexity analysis with respect to various graph parameters, including\ntreewidth, and problem-specific parameters, including, the numbers of colors\nand districts. We obtain a rich and diverse, close to complete picture of the\ncorresponding parameterized complexity landscape (that is, a classification\nalong the complexity classes FPT, XP, W[1]-hard, and para-NP-hard).",
    "published_date": "2021-02-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT",
      "cs.DM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.11864v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.11814v1",
    "title": "A Multi-Stage Stochastic Programming Approach to Epidemic Resource Allocation with Equity Considerations",
    "authors": [
      "Xuecheng Yin",
      "I. Esra Buyuktahtakin"
    ],
    "author_ids": [],
    "abstract": "Existing compartmental models in epidemiology are limited in terms of\noptimizing the resource allocation to control an epidemic outbreak under\ndisease growth uncertainty. In this study, we address this core limitation by\npresenting a multi-stage stochastic programming compartmental model, which\nintegrates the uncertain disease progression and resource allocation to control\nan infectious disease outbreak. The proposed multi-stage stochastic program\ninvolves various disease growth scenarios and optimizes the distribution of\ntreatment centers and resources while minimizing the total expected number of\nnew infections and funerals. We define two new equity metrics, namely infection\nand capacity equity, and explicitly consider equity for allocating treatment\nfunds and facilities over multiple time stages. We also study the multi-stage\nvalue of the stochastic solution (VSS), which demonstrates the superiority of\nthe proposed stochastic programming model over its deterministic counterpart.\nWe apply the proposed formulation to control the Ebola Virus Disease (EVD) in\nGuinea, Sierra Leone, and Liberia of West Africa to determine the optimal and\nfair resource-allocation strategies. Our model balances the proportion of\ninfections over all regions, even without including the infection equity or\nprevalence equity constraints. Model results also show that allocating\ntreatment resources proportional to population is sub-optimal, and enforcing\nsuch a resource allocation policy might adversely impact the total number of\ninfections and deaths, and thus resulting in a high cost that we have to pay\nfor the fairness. Our multi-stage stochastic epidemic-logistics model is\npractical and can be adapted to control other infectious diseases in\nmeta-populations and dynamically evolving situations.",
    "published_date": "2021-02-23T00:00:00",
    "year": 2021,
    "categories": [
      "stat.AP",
      "cs.OH",
      "math.OC",
      "q-bio.PE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.11814v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.11655v2",
    "title": "Learning to Fairly Classify the Quality of Wireless Links",
    "authors": [
      "Gregor Cerar",
      "Halil Yetgin",
      "Mihael Mohorčič",
      "Carolina Fortuna"
    ],
    "author_ids": [],
    "abstract": "Machine learning (ML) has been used to develop increasingly accurate link\nquality estimators for wireless networks. However, more in-depth questions\nregarding the most suitable class of models, most suitable metrics and model\nperformance on imbalanced datasets remain open. In this paper, we propose a new\ntree-based link quality classifier that meets high performance and fairly\nclassifies the minority class and, at the same time, incurs low training cost.\nWe compare the tree-based model, to a multilayer perceptron (MLP) non-linear\nmodel and two linear models, namely logistic regression (LR) and SVM, on a\nselected imbalanced dataset and evaluate their results using five different\nperformance metrics. Our study shows that 1) non-linear models perform slightly\nbetter than linear models in general, 2) the proposed non-linear tree-based\nmodel yields the best performance trade-off considering F1, training time and\nfairness, 3) single metric aggregated evaluations based only on accuracy can\nhide poor, unfair performance especially on minority classes, and 4) it is\npossible to improve the performance on minority classes, by over 40% through\nfeature selection and by over 20% through resampling, therefore leading to\nfairer classification results.",
    "published_date": "2021-02-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.11655v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.11535v4",
    "title": "Neural Architecture Search on ImageNet in Four GPU Hours: A Theoretically Inspired Perspective",
    "authors": [
      "Wuyang Chen",
      "Xinyu Gong",
      "Zhangyang Wang"
    ],
    "author_ids": [],
    "abstract": "Neural Architecture Search (NAS) has been explosively studied to automate the\ndiscovery of top-performer neural networks. Current works require heavy\ntraining of supernet or intensive architecture evaluations, thus suffering from\nheavy resource consumption and often incurring search bias due to truncated\ntraining or approximations. Can we select the best neural architectures without\ninvolving any training and eliminate a drastic portion of the search cost? We\nprovide an affirmative answer, by proposing a novel framework called\ntraining-free neural architecture search (TE-NAS). TE-NAS ranks architectures\nby analyzing the spectrum of the neural tangent kernel (NTK) and the number of\nlinear regions in the input space. Both are motivated by recent theory advances\nin deep networks and can be computed without any training and any label. We\nshow that: (1) these two measurements imply the trainability and expressivity\nof a neural network; (2) they strongly correlate with the network's test\naccuracy. Further on, we design a pruning-based NAS mechanism to achieve a more\nflexible and superior trade-off between the trainability and expressivity\nduring the search. In NAS-Bench-201 and DARTS search spaces, TE-NAS completes\nhigh-quality search but only costs 0.5 and 4 GPU hours with one 1080Ti on\nCIFAR-10 and ImageNet, respectively. We hope our work inspires more attempts in\nbridging the theoretical findings of deep networks and practical impacts in\nreal NAS applications. Code is available at:\nhttps://github.com/VITA-Group/TENAS.",
    "published_date": "2021-02-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.11535v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.11504v1",
    "title": "Equivariant neural networks for inverse problems",
    "authors": [
      "Elena Celledoni",
      "Matthias J. Ehrhardt",
      "Christian Etmann",
      "Brynjulf Owren",
      "Carola-Bibiane Schönlieb",
      "Ferdia Sherry"
    ],
    "author_ids": [],
    "abstract": "In recent years the use of convolutional layers to encode an inductive bias\n(translational equivariance) in neural networks has proven to be a very\nfruitful idea. The successes of this approach have motivated a line of research\ninto incorporating other symmetries into deep learning methods, in the form of\ngroup equivariant convolutional neural networks. Much of this work has been\nfocused on roto-translational symmetry of $\\mathbf R^d$, but other examples are\nthe scaling symmetry of $\\mathbf R^d$ and rotational symmetry of the sphere. In\nthis work, we demonstrate that group equivariant convolutional operations can\nnaturally be incorporated into learned reconstruction methods for inverse\nproblems that are motivated by the variational regularisation approach. Indeed,\nif the regularisation functional is invariant under a group symmetry, the\ncorresponding proximal operator will satisfy an equivariance property with\nrespect to the same group symmetry. As a result of this observation, we design\nlearned iterative methods in which the proximal operators are modelled as group\nequivariant convolutional neural networks. We use roto-translationally\nequivariant operations in the proposed methodology and apply it to the problems\nof low-dose computerised tomography reconstruction and subsampled magnetic\nresonance imaging reconstruction. The proposed methodology is demonstrated to\nimprove the reconstruction quality of a learned reconstruction method with a\nlittle extra computational cost at training time but without any extra cost at\ntest time.",
    "published_date": "2021-02-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.11504v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.01774v1",
    "title": "Conceptualising Contestability: Perspectives on Contesting Algorithmic Decisions",
    "authors": [
      "Henrietta Lyons",
      "Eduardo Velloso",
      "Tim Miller"
    ],
    "author_ids": [],
    "abstract": "As the use of algorithmic systems in high-stakes decision-making increases,\nthe ability to contest algorithmic decisions is being recognised as an\nimportant safeguard for individuals. Yet, there is little guidance on what\n`contestability'--the ability to contest decisions--in relation to algorithmic\ndecision-making requires. Recent research presents different conceptualisations\nof contestability in algorithmic decision-making. We contribute to this growing\nbody of work by describing and analysing the perspectives of people and\norganisations who made submissions in response to Australia's proposed `AI\nEthics Framework', the first framework of its kind to include `contestability'\nas a core ethical principle. Our findings reveal that while the nature of\ncontestability is disputed, it is seen as a way to protect individuals, and it\nresembles contestability in relation to human decision-making. We reflect on\nand discuss the implications of these findings.",
    "published_date": "2021-02-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.01774v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.11319v1",
    "title": "Stratified Experience Replay: Correcting Multiplicity Bias in Off-Policy Reinforcement Learning",
    "authors": [
      "Brett Daley",
      "Cameron Hickert",
      "Christopher Amato"
    ],
    "author_ids": [],
    "abstract": "Deep Reinforcement Learning (RL) methods rely on experience replay to\napproximate the minibatched supervised learning setting; however, unlike\nsupervised learning where access to lots of training data is crucial to\ngeneralization, replay-based deep RL appears to struggle in the presence of\nextraneous data. Recent works have shown that the performance of Deep Q-Network\n(DQN) degrades when its replay memory becomes too large.\n  This suggests that outdated experiences somehow impact the performance of\ndeep RL, which should not be the case for off-policy methods like DQN.\nConsequently, we re-examine the motivation for sampling uniformly over a replay\nmemory, and find that it may be flawed when using function approximation. We\nshow that -- despite conventional wisdom -- sampling from the uniform\ndistribution does not yield uncorrelated training samples and therefore biases\ngradients during training. Our theory prescribes a special non-uniform\ndistribution to cancel this effect, and we propose a stratified sampling scheme\nto efficiently implement it.",
    "published_date": "2021-02-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.11319v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.11149v1",
    "title": "Phase Space Reconstruction Network for Lane Intrusion Action Recognition",
    "authors": [
      "Ruiwen Zhang",
      "Zhidong Deng",
      "Hongsen Lin",
      "Hongchao Lu"
    ],
    "author_ids": [],
    "abstract": "In a complex road traffic scene, illegal lane intrusion of pedestrians or\ncyclists constitutes one of the main safety challenges in autonomous driving\napplication. In this paper, we propose a novel object-level phase space\nreconstruction network (PSRNet) for motion time series classification, aiming\nto recognize lane intrusion actions that occur 150m ahead through a monocular\ncamera fixed on moving vehicle. In the PSRNet, the movement of pedestrians and\ncyclists, specifically viewed as an observable object-level dynamic process,\ncan be reconstructed as trajectories of state vectors in a latent phase space\nand further characterized by a learnable Lyapunov exponent-like classifier that\nindicates discrimination in terms of average exponential divergence of state\ntrajectories. Additionally, in order to first transform video inputs into\none-dimensional motion time series of each object, a lane width normalization\nbased on visual object tracking-by-detection is presented. Extensive\nexperiments are conducted on the THU-IntrudBehavior dataset collected from real\nurban roads. The results show that our PSRNet could reach the best accuracy of\n98.0%, which remarkably exceeds existing action recognition approaches by more\nthan 30%.",
    "published_date": "2021-02-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.11149v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.10809v3",
    "title": "Local Calibration: Metrics and Recalibration",
    "authors": [
      "Rachel Luo",
      "Aadyot Bhatnagar",
      "Yu Bai",
      "Shengjia Zhao",
      "Huan Wang",
      "Caiming Xiong",
      "Silvio Savarese",
      "Stefano Ermon",
      "Edward Schmerling",
      "Marco Pavone"
    ],
    "author_ids": [],
    "abstract": "Probabilistic classifiers output confidence scores along with their\npredictions, and these confidence scores should be calibrated, i.e., they\nshould reflect the reliability of the prediction. Confidence scores that\nminimize standard metrics such as the expected calibration error (ECE)\naccurately measure the reliability on average across the entire population.\nHowever, it is in general impossible to measure the reliability of an\nindividual prediction. In this work, we propose the local calibration error\n(LCE) to span the gap between average and individual reliability. For each\nindividual prediction, the LCE measures the average reliability of a set of\nsimilar predictions, where similarity is quantified by a kernel function on a\npretrained feature space and by a binning scheme over predicted model\nconfidences. We show theoretically that the LCE can be estimated\nsample-efficiently from data, and empirically find that it reveals\nmiscalibration modes that are more fine-grained than the ECE can detect. Our\nkey result is a novel local recalibration method LoRe, to improve confidence\nscores for individual predictions and decrease the LCE. Experimentally, we show\nthat our recalibration method produces more accurate confidence scores, which\nimproves downstream fairness and decision making on classification tasks with\nboth image and tabular data.",
    "published_date": "2021-02-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.10809v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.10787v1",
    "title": "Fair and Responsible AI: A Focus on the Ability to Contest",
    "authors": [
      "Henrietta Lyons",
      "Eduardo Velloso",
      "Tim Miller"
    ],
    "author_ids": [],
    "abstract": "As the use of artificial intelligence (AI) in high-stakes decision-making\nincreases, the ability to contest such decisions is being recognised in AI\nethics guidelines as an important safeguard for individuals. Yet, there is\nlittle guidance on how AI systems can be designed to support contestation. In\nthis paper we explain that the design of a contestation process is important\ndue to its impact on perceptions of fairness and satisfaction. We also consider\ndesign challenges, including a lack of transparency as well as the numerous\ndesign options that decision-making entities will be faced with. We argue for a\nhuman-centred approach to designing for contestability to ensure that the needs\nof decision subjects, and the community, are met.",
    "published_date": "2021-02-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.10787v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.10750v1",
    "title": "Coping with Mistreatment in Fair Algorithms",
    "authors": [
      "Ankit Kulshrestha",
      "Ilya Safro"
    ],
    "author_ids": [],
    "abstract": "Machine learning actively impacts our everyday life in almost all endeavors\nand domains such as healthcare, finance, and energy. As our dependence on the\nmachine learning increases, it is inevitable that these algorithms will be used\nto make decisions that will have a direct impact on the society spanning all\nresolutions from personal choices to world-wide policies. Hence, it is crucial\nto ensure that (un)intentional bias does not affect the machine learning\nalgorithms especially when they are required to take decisions that may have\nunintended consequences. Algorithmic fairness techniques have found traction in\nthe machine learning community and many methods and metrics have been proposed\nto ensure and evaluate fairness in algorithms and data collection.\n  In this paper, we study the algorithmic fairness in a supervised learning\nsetting and examine the effect of optimizing a classifier for the Equal\nOpportunity metric. We demonstrate that such a classifier has an increased\nfalse positive rate across sensitive groups and propose a conceptually simple\nmethod to mitigate this bias. We rigorously analyze the proposed method and\nevaluate it on several real world datasets demonstrating its efficacy.",
    "published_date": "2021-02-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.10750v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.10687v1",
    "title": "An agile and distributed mechanism for inter-domain network slicing in next generation mobile networks",
    "authors": [
      "Jalal Khamse-Ashari",
      "Gamini Senarath",
      "Irem Bor-Yaliniz",
      "Halim Yanikomeroglu"
    ],
    "author_ids": [],
    "abstract": "Network slicing is emerging as a promising method to provide sought-after\nversatility and flexibility to cope with ever-increasing demands. To realize\nsuch potential advantages and to meet the challenging requirements of various\nnetwork slices in an on-demand fashion, we need to develop an agile and\ndistributed mechanism for resource provisioning to different network slices in\na heterogeneous multi-resource multi-domain mobile network environment. We\nformulate inter-domain resource provisioning to network slices in such an\nenvironment as an optimization problem which maximizes social welfare among\nnetwork slice tenants (so that maximizing tenants' satisfaction), while\nminimizing operational expenditures for infrastructure service providers at the\nsame time. To solve the envisioned problem, we implement an iterative auction\ngame among network slice tenants, on one hand, and a plurality of price-taking\nsubnet service providers, on the other hand. We show that the proposed solution\nmethod results in a distributed privacy-saving mechanism which converges to the\noptimal solution of the described optimization problem. In addition to\nproviding analytical results to characterize the performance of the proposed\nmechanism, we also employ numerical evaluations to validate the results,\ndemonstrate convergence of the presented algorithm, and show the enhanced\nperformance of the proposed approach (in terms of resource utilization,\nfairness and operational costs) against the existing solutions.",
    "published_date": "2021-02-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT",
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.10687v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.10646v2",
    "title": "A Game-Theoretic Approach for Hierarchical Epidemic Control",
    "authors": [
      "Feiran Jia",
      "Aditya Mate",
      "Zun Li",
      "Shahin Jabbari",
      "Mithun Chakraborty",
      "Milind Tambe",
      "Michael Wellman",
      "Yevgeniy Vorobeychik"
    ],
    "author_ids": [],
    "abstract": "We design and analyze a multi-level game-theoretic model of hierarchical\npolicy interventions for epidemic control, such as those in response to the\nCOVID-19 pandemic. Our model captures the potentially mismatched priorities\namong a hierarchy of policy-makers (e.g., federal, state, and local\ngovernments) with respect to two cost components that have opposite dependence\non the policy strength -- post-intervention infection rates and the\nsocio-economic cost of policy implementation. Additionally, our model includes\na crucial third factor in decisions: a cost of non-compliance with the\npolicy-maker immediately above in the hierarchy, such as non-compliance of\ncounties with state-level policies. We propose two novel algorithms for\napproximating solutions to such games. The first is based on best response\ndynamics (BRD), and exploits the tree structure of the game. The second\ncombines quadratic integer programming (QIP), which enables us to collapse the\ntwo lowest levels of the game, with best response dynamics. Through extensive\nexperiments, we show that our QIP-based approach significantly outperforms the\nBRD algorithm both in running time and the quality of equilibrium solutions.\nFinally, we apply the QIP-based algorithm to experiments based on both\nsynthetic and real-world data under various parameter configurations and\nanalyze the resulting (approximate) equilibria to gain insight into the impact\nof decentralization on overall welfare (measured as the negative sum of costs)\nas well as emergent properties like free-riding and fairness in cost\ndistribution among policy-makers.",
    "published_date": "2021-02-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.10646v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2104.10639v1",
    "title": "Exerting Control in Repeated Social Dilemmas with Thresholds",
    "authors": [
      "Kathinka Frieswijk",
      "Alain Govaert",
      "Ming Cao"
    ],
    "author_ids": [],
    "abstract": "Situations in which immediate self-interest and long-term collective interest\nconflict often require some form of influence to prevent them from leading to\nundesirable or unsustainable outcomes. Next to sanctioning, social influence\nand social structure, it is possible that strategic solutions can exist for\nthese social dilemmas. However, the existence of strategies that enable a\nplayer to exert control in the long-run outcomes can be difficult to show and\ndifferent situations allow for different levels of strategic influence. Here,\nwe investigate the effect of threshold nonlinearities on the possibilities of\nexerting unilateral control in finitely repeated n-player public goods games\nand snowdrift games. These models can describe situations in which a collective\neffort is necessary in order for a benefit to be created. We identify\nconditions in terms of a cooperator threshold for the existence of generous,\nextortionate and equalizing zero-determinant (ZD) strategies. Our results show\nthat, for both games, the thresholds prevent equalizing ZD strategies from\nexisting. In the snowdrift game, introducing a cooperator threshold has no\neffect on the region of feasible extortionate ZD strategies. For extortionate\nstrategies in the public goods game, the threshold only restricts the region of\nenforceable strategies for small values of the public goods multiplier.\nGenerous ZD strategies exist for both games, but introducing a cooperator\nthreshold forces the slope more towards the value of a fair strategy, where the\nplayer has approximately the same payoff as the average payoff of his\nopponents.",
    "published_date": "2021-02-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.MA",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.10639v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.10544v2",
    "title": "Rethinking Content and Style: Exploring Bias for Unsupervised Disentanglement",
    "authors": [
      "Xuanchi Ren",
      "Tao Yang",
      "Yuwang Wang",
      "Wenjun Zeng"
    ],
    "author_ids": [],
    "abstract": "Content and style (C-S) disentanglement intends to decompose the underlying\nexplanatory factors of objects into two independent subspaces. From the\nunsupervised disentanglement perspective, we rethink content and style and\npropose a formulation for unsupervised C-S disentanglement based on our\nassumption that different factors are of different importance and popularity\nfor image reconstruction, which serves as a data bias. The corresponding model\ninductive bias is introduced by our proposed C-S disentanglement Module (C-S\nDisMo), which assigns different and independent roles to content and style when\napproximating the real data distributions. Specifically, each content embedding\nfrom the dataset, which encodes the most dominant factors for image\nreconstruction, is assumed to be sampled from a shared distribution across the\ndataset. The style embedding for a particular image, encoding the remaining\nfactors, is used to customize the shared distribution through an affine\ntransformation. The experiments on several popular datasets demonstrate that\nour method achieves the state-of-the-art unsupervised C-S disentanglement,\nwhich is comparable or even better than supervised methods. We verify the\neffectiveness of our method by downstream tasks: domain translation and\nsingle-view 3D reconstruction. Project page at\nhttps://github.com/xrenaa/CS-DisMo.",
    "published_date": "2021-02-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.10544v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.11724v3",
    "title": "Causal Mediation Analysis with Hidden Confounders",
    "authors": [
      "Lu Cheng",
      "Ruocheng Guo",
      "Huan Liu"
    ],
    "author_ids": [],
    "abstract": "An important problem in causal inference is to break down the total effect of\na treatment on an outcome into different causal pathways and to quantify the\ncausal effect in each pathway. For instance, in causal fairness, the total\neffect of being a male employee (i.e., treatment) constitutes its direct effect\non annual income (i.e., outcome) and the indirect effect via the employee's\noccupation (i.e., mediator). Causal mediation analysis (CMA) is a formal\nstatistical framework commonly used to reveal such underlying causal\nmechanisms. One major challenge of CMA in observational studies is handling\nconfounders, variables that cause spurious causal relationships among\ntreatment, mediator, and outcome. Conventional methods assume sequential\nignorability that implies all confounders can be measured, which is often\nunverifiable in practice. This work aims to circumvent the stringent sequential\nignorability assumptions and consider hidden confounders. Drawing upon proxy\nstrategies and recent advances in deep learning, we propose to simultaneously\nuncover the latent variables that characterize hidden confounders and estimate\nthe causal effects. Empirical evaluations using both synthetic and\nsemi-synthetic datasets validate the effectiveness of the proposed method. We\nfurther show the potentials of our approach for causal fairness analysis.",
    "published_date": "2021-02-21T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.11724v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.10365v1",
    "title": "Analyzing Overfitting under Class Imbalance in Neural Networks for Image Segmentation",
    "authors": [
      "Zeju Li",
      "Konstantinos Kamnitsas",
      "Ben Glocker"
    ],
    "author_ids": [],
    "abstract": "Class imbalance poses a challenge for developing unbiased, accurate\npredictive models. In particular, in image segmentation neural networks may\noverfit to the foreground samples from small structures, which are often\nheavily under-represented in the training set, leading to poor generalization.\nIn this study, we provide new insights on the problem of overfitting under\nclass imbalance by inspecting the network behavior. We find empirically that\nwhen training with limited data and strong class imbalance, at test time the\ndistribution of logit activations may shift across the decision boundary, while\nsamples of the well-represented class seem unaffected. This bias leads to a\nsystematic under-segmentation of small structures. This phenomenon is\nconsistently observed for different databases, tasks and network architectures.\nTo tackle this problem, we introduce new asymmetric variants of popular loss\nfunctions and regularization techniques including a large margin loss, focal\nloss, adversarial training, mixup and data augmentation, which are explicitly\ndesigned to counter logit shift of the under-represented classes. Extensive\nexperiments are conducted on several challenging segmentation tasks. Our\nresults demonstrate that the proposed modifications to the objective function\ncan lead to significantly improved segmentation accuracy compared to baselines\nand alternative approaches.",
    "published_date": "2021-02-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.10365v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.10349v1",
    "title": "Everything is Relative: Understanding Fairness with Optimal Transport",
    "authors": [
      "Kweku Kwegyir-Aggrey",
      "Rebecca Santorella",
      "Sarah M. Brown"
    ],
    "author_ids": [],
    "abstract": "To study discrimination in automated decision-making systems, scholars have\nproposed several definitions of fairness, each expressing a different fair\nideal. These definitions require practitioners to make complex decisions\nregarding which notion to employ and are often difficult to use in practice\nsince they make a binary judgement a system is fair or unfair instead of\nexplaining the structure of the detected unfairness. We present an optimal\ntransport-based approach to fairness that offers an interpretable and\nquantifiable exploration of bias and its structure by comparing a pair of\noutcomes to one another. In this work, we use the optimal transport map to\nexamine individual, subgroup, and group fairness. Our framework is able to\nrecover well known examples of algorithmic discrimination, detect unfairness\nwhen other metrics fail, and explore recourse opportunities.",
    "published_date": "2021-02-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.10349v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.10228v1",
    "title": "$\\mathcal{PT}$-Symmetric Quantum State Discrimination for Attack on BB84 Quantum Key Distribution",
    "authors": [
      "Yaroslav Balytskyi",
      "Manohar Raavi",
      "Anatoliy Pinchuk",
      "Sang-Yoon Chang"
    ],
    "author_ids": [],
    "abstract": "Quantum Key Distribution or QKD provides symmetric key distribution using the\nquantum mechanics/channels with new security properties. The security of QKD\nrelies on the difficulty of the quantum state discrimination problem. We\ndiscover that the recent developments in $\\mathcal{PT}$ symmetry can be used to\nexpedite the quantum state discrimination problem and therefore to attack the\nBB84 QKD scheme. We analyze the security of the BB84 scheme and show that the\nattack significantly increases the eavesdropping success rate over the previous\nHermitian quantum state discrimination approach. We design and analyze the\napproaches to attack BB84 QKD protocol exploiting an extra degree of freedom\nprovided by the $\\mathcal{PT}$-symmetric quantum mechanics.",
    "published_date": "2021-02-20T00:00:00",
    "year": 2021,
    "categories": [
      "quant-ph",
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.10228v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.09895v2",
    "title": "Self-Taught Semi-Supervised Anomaly Detection on Upper Limb X-rays",
    "authors": [
      "Antoine Spahr",
      "Behzad Bozorgtabar",
      "Jean-Philippe Thiran"
    ],
    "author_ids": [],
    "abstract": "Detecting anomalies in musculoskeletal radiographs is of paramount importance\nfor large-scale screening in the radiology workflow. Supervised deep networks\ntake for granted a large number of annotations by radiologists, which is often\nprohibitively very time-consuming to acquire. Moreover, supervised systems are\ntailored to closed set scenarios, e.g., trained models suffer from overfitting\nto previously seen rare anomalies at training. Instead, our approach's\nrationale is to use task agnostic pretext tasks to leverage unlabeled data\nbased on a cross-sample similarity measure. Besides, we formulate a complex\ndistribution of data from normal class within our framework to avoid a\npotential bias on the side of anomalies. Through extensive experiments, we show\nthat our method outperforms baselines across unsupervised and self-supervised\nanomaly detection settings on a real-world medical dataset, the MURA dataset.\nWe also provide rich ablation studies to analyze each training stage's effect\nand loss terms on the final performance.",
    "published_date": "2021-02-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.09895v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.09769v1",
    "title": "On the Implicit Bias of Initialization Shape: Beyond Infinitesimal Mirror Descent",
    "authors": [
      "Shahar Azulay",
      "Edward Moroshko",
      "Mor Shpigel Nacson",
      "Blake Woodworth",
      "Nathan Srebro",
      "Amir Globerson",
      "Daniel Soudry"
    ],
    "author_ids": [],
    "abstract": "Recent work has highlighted the role of initialization scale in determining\nthe structure of the solutions that gradient methods converge to. In\nparticular, it was shown that large initialization leads to the neural tangent\nkernel regime solution, whereas small initialization leads to so called \"rich\nregimes\". However, the initialization structure is richer than the overall\nscale alone and involves relative magnitudes of different weights and layers in\nthe network. Here we show that these relative scales, which we refer to as\ninitialization shape, play an important role in determining the learned model.\nWe develop a novel technique for deriving the inductive bias of gradient-flow\nand use it to obtain closed-form implicit regularizers for multiple cases of\ninterest.",
    "published_date": "2021-02-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "68T07 (Primary)",
      "I.2.6; G.1.6"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.09769v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.09704v2",
    "title": "Fair Sparse Regression with Clustering: An Invex Relaxation for a Combinatorial Problem",
    "authors": [
      "Adarsh Barik",
      "Jean Honorio"
    ],
    "author_ids": [],
    "abstract": "In this paper, we study the problem of fair sparse regression on a biased\ndataset where bias depends upon a hidden binary attribute. The presence of a\nhidden attribute adds an extra layer of complexity to the problem by combining\nsparse regression and clustering with unknown binary labels. The corresponding\noptimization problem is combinatorial, but we propose a novel relaxation of it\nas an \\emph{invex} optimization problem. To the best of our knowledge, this is\nthe first invex relaxation for a combinatorial problem. We show that the\ninclusion of the debiasing/fairness constraint in our model has no adverse\neffect on the performance. Rather, it enables the recovery of the hidden\nattribute. The support of our recovered regression parameter vector matches\nexactly with the true parameter vector. Moreover, we simultaneously solve the\nclustering problem by recovering the exact value of the hidden attribute for\neach sample. Our method uses carefully constructed primal dual witnesses to\nprovide theoretical guarantees for the combinatorial problem. To that end, we\nshow that the sample complexity of our method is logarithmic in terms of the\ndimension of the regression parameter vector.",
    "published_date": "2021-02-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.09704v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.09691v1",
    "title": "SLIP Walking over Rough Terrain via H-LIP Stepping and Backstepping-Barrier Function Inspired Quadratic Program",
    "authors": [
      "Xiaobin Xiong",
      "Aaron Ames"
    ],
    "author_ids": [],
    "abstract": "We present an advanced and novel control method to enable actuated Spring\nLoaded Inverted Pendulum model to walk over rough and challenging terrains. The\nhigh-level philosophy is the decoupling of the controls of the vertical and\nhorizontal states. The vertical state is controlled via Backstepping-Barrier\nFunction (BBF) based quadratic programs: a combination of control Lyapunov\nbackstepping and control barrier function, both of which provide inequality\nconstraints on the inputs. The horizontal state is stabilized via Hybrid-Linear\nInverted Pendulum (H-LIP) based stepping, which has a closed-form formulation.\nTherefore, the implementation is computationally-efficient. We evaluate our\nmethod in simulation, which demonstrates the aSLIP walking over various\nterrains, including slopes, stairs, and general rough terrains with\nuncertainties.",
    "published_date": "2021-02-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.09691v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.09670v1",
    "title": "Maximizing Marginal Fairness for Dynamic Learning to Rank",
    "authors": [
      "Tao Yang",
      "Qingyao Ai"
    ],
    "author_ids": [],
    "abstract": "Rankings, especially those in search and recommendation systems, often\ndetermine how people access information and how information is exposed to\npeople. Therefore, how to balance the relevance and fairness of information\nexposure is considered as one of the key problems for modern IR systems. As\nconventional ranking frameworks that myopically sorts documents with their\nrelevance will inevitably introduce unfair result exposure, recent studies on\nranking fairness mostly focus on dynamic ranking paradigms where result\nrankings can be adapted in real-time to support fairness in groups (i.e.,\nraces, genders, etc.). Existing studies on fairness in dynamic learning to\nrank, however, often achieve the overall fairness of document exposure in\nranked lists by significantly sacrificing the performance of result relevance\nand fairness on the top results. To address this problem, we propose a fair and\nunbiased ranking method named Maximal Marginal Fairness (MMF). The algorithm\nintegrates unbiased estimators for both relevance and merit-based fairness\nwhile providing an explicit controller that balances the selection of documents\nto maximize the marginal relevance and fairness in top-k results. Theoretical\nand empirical analysis shows that, with small compromises on long list\nfairness, our method achieves superior efficiency and effectiveness comparing\nto the state-of-the-art algorithms in both relevance and fairness for top-k\nrankings.",
    "published_date": "2021-02-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "H.3.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.09670v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.09635v3",
    "title": "Random Walks with Erasure: Diversifying Personalized Recommendations on Social and Information Networks",
    "authors": [
      "Bibek Paudel",
      "Abraham Bernstein"
    ],
    "author_ids": [],
    "abstract": "Most existing personalization systems promote items that match a user's\nprevious choices or those that are popular among similar users. This results in\nrecommendations that are highly similar to the ones users are already exposed\nto, resulting in their isolation inside familiar but insulated information\nsilos. In this context, we develop a novel recommendation framework with a goal\nof improving information diversity using a modified random walk exploration of\nthe user-item graph. We focus on the problem of political content\nrecommendation, while addressing a general problem applicable to\npersonalization tasks in other social and information networks.\n  For recommending political content on social networks, we first propose a new\nmodel to estimate the ideological positions for both users and the content they\nshare, which is able to recover ideological positions with high accuracy. Based\non these estimated positions, we generate diversified personalized\nrecommendations using our new random-walk based recommendation algorithm. With\nexperimental evaluations on large datasets of Twitter discussions, we show that\nour method based on \\emph{random walks with erasure} is able to generate more\nideologically diverse recommendations. Our approach does not depend on the\navailability of labels regarding the bias of users or content producers. With\nexperiments on open benchmark datasets from other social and information\nnetworks, we also demonstrate the effectiveness of our method in recommending\ndiverse long-tail items.",
    "published_date": "2021-02-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.09635v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.09480v1",
    "title": "Unbiased Teacher for Semi-Supervised Object Detection",
    "authors": [
      "Yen-Cheng Liu",
      "Chih-Yao Ma",
      "Zijian He",
      "Chia-Wen Kuo",
      "Kan Chen",
      "Peizhao Zhang",
      "Bichen Wu",
      "Zsolt Kira",
      "Peter Vajda"
    ],
    "author_ids": [],
    "abstract": "Semi-supervised learning, i.e., training networks with both labeled and\nunlabeled data, has made significant progress recently. However, existing works\nhave primarily focused on image classification tasks and neglected object\ndetection which requires more annotation effort. In this work, we revisit the\nSemi-Supervised Object Detection (SS-OD) and identify the pseudo-labeling bias\nissue in SS-OD. To address this, we introduce Unbiased Teacher, a simple yet\neffective approach that jointly trains a student and a gradually progressing\nteacher in a mutually-beneficial manner. Together with a class-balance loss to\ndownweight overly confident pseudo-labels, Unbiased Teacher consistently\nimproved state-of-the-art methods by significant margins on COCO-standard,\nCOCO-additional, and VOC datasets. Specifically, Unbiased Teacher achieves 6.8\nabsolute mAP improvements against state-of-the-art method when using 1% of\nlabeled data on MS-COCO, achieves around 10 mAP improvements against the\nsupervised baseline when using only 0.5, 1, 2% of labeled data on MS-COCO.",
    "published_date": "2021-02-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.09480v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.09210v1",
    "title": "Data-driven formulation of natural laws by recursive-LASSO-based symbolic regression",
    "authors": [
      "Yuma Iwasaki",
      "Masahiko Ishida"
    ],
    "author_ids": [],
    "abstract": "Discovery of new natural laws has for a long time relied on the inspiration\nof some genius. Recently, however, machine learning technologies, which analyze\nbig data without human prejudice and bias, are expected to find novel natural\nlaws. Here we demonstrate that our proposed machine learning,\nrecursive-LASSO-based symbolic (RLS) regression, enables data-driven\nformulation of natural laws from noisy data. The RLS regression recurrently\nrepeats feature generation and feature selection, eventually constructing a\ndata-driven model with highly nonlinear features. This data-driven formulation\nmethod is quite general and thus can discover new laws in various scientific\nfields.",
    "published_date": "2021-02-18T00:00:00",
    "year": 2021,
    "categories": [
      "physics.data-an",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.09210v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.09150v1",
    "title": "An Enhanced Adversarial Network with Combined Latent Features for Spatio-Temporal Facial Affect Estimation in the Wild",
    "authors": [
      "Decky Aspandi",
      "Federico Sukno",
      "Björn Schuller",
      "Xavier Binefa"
    ],
    "author_ids": [],
    "abstract": "Affective Computing has recently attracted the attention of the research\ncommunity, due to its numerous applications in diverse areas. In this context,\nthe emergence of video-based data allows to enrich the widely used spatial\nfeatures with the inclusion of temporal information. However, such\nspatio-temporal modelling often results in very high-dimensional feature spaces\nand large volumes of data, making training difficult and time consuming. This\npaper addresses these shortcomings by proposing a novel model that efficiently\nextracts both spatial and temporal features of the data by means of its\nenhanced temporal modelling based on latent features. Our proposed model\nconsists of three major networks, coined Generator, Discriminator, and\nCombiner, which are trained in an adversarial setting combined with curriculum\nlearning to enable our adaptive attention modules. In our experiments, we show\nthe effectiveness of our approach by reporting our competitive results on both\nthe AFEW-VA and SEWA datasets, suggesting that temporal modelling improves the\naffect estimates both in qualitative and quantitative terms. Furthermore, we\nfind that the inclusion of attention mechanisms leads to the highest accuracy\nimprovements, as its weights seem to correlate well with the appearance of\nfacial movements, both in terms of temporal localisation and intensity.\nFinally, we observe the sequence length of around 160\\,ms to be the optimum one\nfor temporal modelling, which is consistent with other relevant findings\nutilising similar lengths.",
    "published_date": "2021-02-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.09150v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.09103v1",
    "title": "Gender Bias, Social Bias and Representation: 70 Years of B$^H$ollywood",
    "authors": [
      "Kunal Khadilkar",
      "Ashiqur R. KhudaBukhsh",
      "Tom M. Mitchell"
    ],
    "author_ids": [],
    "abstract": "With an outreach in more than 90 countries, a market share of 2.1 billion\ndollars and a target audience base of at least 1.2 billion people, Bollywood,\naka the Mumbai film industry, is a formidable entertainment force. While the\nnumber of lives Bollywood can potentially touch is massive, no comprehensive\nNLP study on the evolution of social and gender biases in Bollywood dialogues\nexists. Via a substantial corpus of movie dialogues spanning a time horizon of\n70 years, we seek to understand the portrayal of women, in a broader context\nstudying subtle social signals, and analyze the evolving trends in geographic\nand religious representation in India. Our argument is simple -- popular movie\ncontent reflects social norms and beliefs in some form or shape. In this\nproject, we propose to analyze such trends over 70 years of Bollywood movies\ncontrasting them with their Hollywood counterpart and critically acclaimed\nworld movies.",
    "published_date": "2021-02-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.09103v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.08991v2",
    "title": "Generalization in Quantum Machine Learning: a Quantum Information Perspective",
    "authors": [
      "Leonardo Banchi",
      "Jason Pereira",
      "Stefano Pirandola"
    ],
    "author_ids": [],
    "abstract": "Quantum classification and hypothesis testing are two tightly related\nsubjects, the main difference being that the former is data driven: how to\nassign to quantum states $\\rho(x)$ the corresponding class $c$ (or hypothesis)\nis learnt from examples during training, where $x$ can be either tunable\nexperimental parameters or classical data \"embedded\" into quantum states. Does\nthe model generalize? This is the main question in any data-driven strategy,\nnamely the ability to predict the correct class even of previously unseen\nstates. Here we establish a link between quantum machine learning\nclassification and quantum hypothesis testing (state and channel\ndiscrimination) and then show that the accuracy and generalization capability\nof quantum classifiers depend on the (R\\'enyi) mutual informations $I(C{:}Q)$\nand $I_2(X{:}Q)$ between the quantum state space $Q$ and the classical\nparameter space $X$ or class space $C$. Based on the above characterization, we\nthen show how different properties of $Q$ affect classification accuracy and\ngeneralization, such as the dimension of the Hilbert space, the amount of\nnoise, and the amount of neglected information from $X$ via, e.g., pooling\nlayers. Moreover, we introduce a quantum version of the Information Bottleneck\nprinciple that allows us to explore the various tradeoffs between accuracy and\ngeneralization. Finally, in order to check our theoretical predictions, we\nstudy the classification of the quantum phases of an Ising spin chain, and we\npropose the Variational Quantum Information Bottleneck (VQIB) method to\noptimize quantum embeddings of classical data to favor generalization.",
    "published_date": "2021-02-17T00:00:00",
    "year": 2021,
    "categories": [
      "quant-ph",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.08991v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.08909v2",
    "title": "Surveying the Landscape of Ethics-Focused Design Methods",
    "authors": [
      "Shruthi Sai Chivukula",
      "Ziqing Li",
      "Anne C. Pivonka",
      "Jingning Chen",
      "Colin M. Gray"
    ],
    "author_ids": [],
    "abstract": "Over the past decade, HCI researchers, design researchers, and practitioners\nhave increasingly addressed ethics-focused issues through a range of\ntheoretical, methodological and pragmatic contributions to the field. While\nmany forms of design knowledge have been proposed and described, we focus\nexplicitly on knowledge that has been codified as \"methods,\" which we define as\nany supports for everyday work practices of designers. In this paper, we\nidentify, analyze, and map a collection of 63 existing ethics-focused methods\nintentionally designed for ethical impact. We present a content analysis,\nproviding a descriptive record of how they operationalize ethics, their\nintended audience or context of use, their \"core\" or \"script,\" and the means by\nwhich these methods are formulated, articulated, and languaged. Building on\nthese results, we provide an initial definition of ethics-focused methods,\nidentifying potential opportunities for the development of future methods to\nsupport design practice and research.",
    "published_date": "2021-02-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.08909v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.08897v1",
    "title": "Robust Domain-Free Domain Generalization with Class-aware Alignment",
    "authors": [
      "Wenyu Zhang",
      "Mohamed Ragab",
      "Ramon Sagarna"
    ],
    "author_ids": [],
    "abstract": "While deep neural networks demonstrate state-of-the-art performance on a\nvariety of learning tasks, their performance relies on the assumption that\ntrain and test distributions are the same, which may not hold in real-world\napplications. Domain generalization addresses this issue by employing multiple\nsource domains to build robust models that can generalize to unseen target\ndomains subject to shifts in data distribution. In this paper, we propose\nDomain-Free Domain Generalization (DFDG), a model-agnostic method to achieve\nbetter generalization performance on the unseen test domain without the need\nfor source domain labels. DFDG uses novel strategies to learn domain-invariant\nclass-discriminative features. It aligns class relationships of samples through\nclass-conditional soft labels, and uses saliency maps, traditionally developed\nfor post-hoc analysis of image classification networks, to remove superficial\nobservations from training inputs. DFDG obtains competitive performance on both\ntime series sensor and image classification public datasets.",
    "published_date": "2021-02-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.08897v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.08814v1",
    "title": "Distributed Fair Scheduling for Information Exchange in Multi-Agent Systems",
    "authors": [
      "Majid Raeis",
      "S. Jamaloddin Golestani"
    ],
    "author_ids": [],
    "abstract": "Information exchange is a crucial component of many real-world multi-agent\nsystems. However, the communication between the agents involves two major\nchallenges: the limited bandwidth, and the shared communication medium between\nthe agents, which restricts the number of agents that can simultaneously\nexchange information. While both of these issues need to be addressed in\npractice, the impact of the latter problem on the performance of the\nmulti-agent systems has often been neglected. This becomes even more important\nwhen the agents' information or observations have different importance, in\nwhich case the agents require different priorities for accessing the medium and\nsharing their information. Representing the agents' priorities by fairness\nweights and normalizing each agent's share by the assigned fairness weight, the\ngoal can be expressed as equalizing the agents' normalized shares of the\ncommunication medium. To achieve this goal, we adopt a queueing theoretic\napproach and propose a distributed fair scheduling algorithm for providing\nweighted fairness in single-hop networks. Our proposed algorithm guarantees an\nupper-bound on the normalized share disparity among any pair of agents. This\ncan particularly improve the short-term fairness, which is important in\nreal-time applications. Moreover, our scheduling algorithm adjusts itself\ndynamically to achieve a high throughput at the same time. The simulation\nresults validate our claims and comparisons with the existing methods show our\nalgorithm's superiority in providing short-term fairness, while achieving a\nhigh throughput.",
    "published_date": "2021-02-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.MA",
      "cs.NI",
      "cs.PF"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.08814v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2103.05770v1",
    "title": "Notebook articles: towards a transformative publishing experience in nonlinear science",
    "authors": [
      "Cristel Chandre",
      "Jonathan Dubois"
    ],
    "author_ids": [],
    "abstract": "Open Science, Reproducible Research, Findable, Accessible, Interoperable and\nReusable (FAIR) data principles are long term goals for scientific\ndissemination. However, the implementation of these principles calls for a\nreinspection of our means of dissemination. In our viewpoint, we discuss and\nadvocate, in the context of nonlinear science, how a notebook article\nrepresents an essential step toward this objective by fully embracing cloud\ncomputing solutions. Notebook articles as scholar articles offer an\nalternative, efficient and more ethical way to disseminate research through\ntheir versatile environment. This format invites the readers to delve deeper\ninto the reported research. Through the interactivity of the notebook articles,\nresearch results such as for instance equations and figures are reproducible\neven for non-expert readers. The codes and methods are available, in a\ntransparent manner, to interested readers. The methods can be reused and\nadapted to answer additional questions in related topics. The codes run on\ncloud computing services, which provide easy access, even to low-income\ncountries and research groups. The versatility of this environment provides the\nstakeholders - from the researchers to the publishers - with opportunities to\ndisseminate the research results in innovative ways.",
    "published_date": "2021-02-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.05770v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.08537v5",
    "title": "Political Bias and Factualness in News Sharing across more than 100,000 Online Communities",
    "authors": [
      "Galen Weld",
      "Maria Glenski",
      "Tim Althoff"
    ],
    "author_ids": [],
    "abstract": "As civil discourse increasingly takes place online, misinformation and the\npolarization of news shared in online communities have become ever more\nrelevant concerns with real world harms across our society. Studying online\nnews sharing at scale is challenging due to the massive volume of content which\nis shared by millions of users across thousands of communities. Therefore,\nexisting research has largely focused on specific communities or specific\ninterventions, such as bans. However, understanding the prevalence and spread\nof misinformation and polarization more broadly, across thousands of online\ncommunities, is critical for the development of governance strategies,\ninterventions, and community design. Here, we conduct the largest study of news\nsharing on reddit to date, analyzing more than 550 million links spanning 4\nyears. We use non-partisan news source ratings from Media Bias/Fact Check to\nannotate links to news sources with their political bias and factualness. We\nfind that, compared to left-leaning communities, right-leaning communities have\n105% more variance in the political bias of their news sources, and more links\nto relatively-more biased sources, on average. We observe that reddit users'\nvoting and re-sharing behaviors generally decrease the visibility of extremely\nbiased and low factual content, which receives 20% fewer upvotes and 30% fewer\nexposures from crossposts than more neutral or more factual content. This\nsuggests that reddit is more resilient to low factual content than Twitter. We\nshow that extremely biased and low factual content is very concentrated, with\n99% of such content being shared in only 0.5% of communities, giving credence\nto the recent strategy of community-wide bans and quarantines.",
    "published_date": "2021-02-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.08537v5",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.08534v2",
    "title": "StatEcoNet: Statistical Ecology Neural Networks for Species Distribution Modeling",
    "authors": [
      "Eugene Seo",
      "Rebecca A. Hutchinson",
      "Xiao Fu",
      "Chelsea Li",
      "Tyler A. Hallman",
      "John Kilbride",
      "W. Douglas Robinson"
    ],
    "author_ids": [],
    "abstract": "This paper focuses on a core task in computational sustainability and\nstatistical ecology: species distribution modeling (SDM). In SDM, the\noccurrence pattern of a species on a landscape is predicted by environmental\nfeatures based on observations at a set of locations. At first, SDM may appear\nto be a binary classification problem, and one might be inclined to employ\nclassic tools (e.g., logistic regression, support vector machines, neural\nnetworks) to tackle it. However, wildlife surveys introduce structured noise\n(especially under-counting) in the species observations. If unaccounted for,\nthese observation errors systematically bias SDMs. To address the unique\nchallenges of SDM, this paper proposes a framework called StatEcoNet.\nSpecifically, this work employs a graphical generative model in statistical\necology to serve as the skeleton of the proposed computational framework and\ncarefully integrates neural networks under the framework. The advantages of\nStatEcoNet over related approaches are demonstrated on simulated datasets as\nwell as bird species data. Since SDMs are critical tools for ecological science\nand natural resource management, StatEcoNet may offer boosted computational and\nanalytical powers to a wide range of applications that have significant social\nimpacts, e.g., the study and conservation of threatened species.",
    "published_date": "2021-02-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "q-bio.PE",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.08534v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.08454v1",
    "title": "Lexicographically Fair Learning: Algorithms and Generalization",
    "authors": [
      "Emily Diana",
      "Wesley Gill",
      "Ira Globus-Harris",
      "Michael Kearns",
      "Aaron Roth",
      "Saeed Sharifi-Malvajerdi"
    ],
    "author_ids": [],
    "abstract": "We extend the notion of minimax fairness in supervised learning problems to\nits natural conclusion: lexicographic minimax fairness (or lexifairness for\nshort). Informally, given a collection of demographic groups of interest,\nminimax fairness asks that the error of the group with the highest error be\nminimized. Lexifairness goes further and asks that amongst all minimax fair\nsolutions, the error of the group with the second highest error should be\nminimized, and amongst all of those solutions, the error of the group with the\nthird highest error should be minimized, and so on. Despite its naturalness,\ncorrectly defining lexifairness is considerably more subtle than minimax\nfairness, because of inherent sensitivity to approximation error. We give a\nnotion of approximate lexifairness that avoids this issue, and then derive\noracle-efficient algorithms for finding approximately lexifair solutions in a\nvery general setting. When the underlying empirical risk minimization problem\nabsent fairness constraints is convex (as it is, for example, with linear and\nlogistic regression), our algorithms are provably efficient even in the worst\ncase. Finally, we show generalization bounds -- approximate lexifairness on the\ntraining sample implies approximate lexifairness on the true distribution with\nhigh probability. Our ability to prove generalization bounds depends on our\nchoosing definitions that avoid the instability of naive definitions.",
    "published_date": "2021-02-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.DS",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.08454v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.08453v7",
    "title": "Towards the Right Kind of Fairness in AI",
    "authors": [
      "Boris Ruf",
      "Marcin Detyniecki"
    ],
    "author_ids": [],
    "abstract": "Fairness is a concept of justice. Various definitions exist, some of them\nconflicting with each other. In the absence of an uniformly accepted notion of\nfairness, choosing the right kind for a specific situation has always been a\ncentral issue in human history. When it comes to implementing sustainable\nfairness in artificial intelligence systems, this old question plays a key role\nonce again: How to identify the most appropriate fairness metric for a\nparticular application? The answer is often a matter of context, and the best\nchoice depends on ethical standards and legal requirements. Since ethics\nguidelines on this topic are kept rather general for now, we aim to provide\nmore hands-on guidance with this document. Therefore, we first structure the\ncomplex landscape of existing fairness metrics and explain the different\noptions by example. Furthermore, we propose the \"Fairness Compass\", a tool\nwhich formalises the selection process and makes identifying the most\nappropriate fairness definition for a given system a simple, straightforward\nprocedure. Because this process also allows to document the reasoning behind\nthe respective decisions, we argue that this approach can help to build trust\nfrom the user through explaining and justifying the implemented fairness.",
    "published_date": "2021-02-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.08453v7",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.08410v1",
    "title": "Evaluating Fairness of Machine Learning Models Under Uncertain and Incomplete Information",
    "authors": [
      "Pranjal Awasthi",
      "Alex Beutel",
      "Matthaeus Kleindessner",
      "Jamie Morgenstern",
      "Xuezhi Wang"
    ],
    "author_ids": [],
    "abstract": "Training and evaluation of fair classifiers is a challenging problem. This is\npartly due to the fact that most fairness metrics of interest depend on both\nthe sensitive attribute information and label information of the data points.\nIn many scenarios it is not possible to collect large datasets with such\ninformation. An alternate approach that is commonly used is to separately train\nan attribute classifier on data with sensitive attribute information, and then\nuse it later in the ML pipeline to evaluate the bias of a given classifier.\nWhile such decoupling helps alleviate the problem of demographic scarcity, it\nraises several natural questions such as: how should the attribute classifier\nbe trained?, and how should one use a given attribute classifier for accurate\nbias estimation? In this work we study this question from both theoretical and\nempirical perspectives.\n  We first experimentally demonstrate that the test accuracy of the attribute\nclassifier is not always correlated with its effectiveness in bias estimation\nfor a downstream model. In order to further investigate this phenomenon, we\nanalyze an idealized theoretical model and characterize the structure of the\noptimal classifier. Our analysis has surprising and counter-intuitive\nimplications where in certain regimes one might want to distribute the error of\nthe attribute classifier as unevenly as possible among the different subgroups.\nBased on our analysis we develop heuristics for both training and using\nattribute classifiers for bias estimation in the data scarce regime. We\nempirically demonstrate the effectiveness of our approach on real and simulated\ndata.",
    "published_date": "2021-02-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.08410v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.08352v2",
    "title": "Stochastic Variance Reduction for Variational Inequality Methods",
    "authors": [
      "Ahmet Alacaoglu",
      "Yura Malitsky"
    ],
    "author_ids": [],
    "abstract": "We propose stochastic variance reduced algorithms for solving convex-concave\nsaddle point problems, monotone variational inequalities, and monotone\ninclusions. Our framework applies to extragradient, forward-backward-forward,\nand forward-reflected-backward methods both in Euclidean and Bregman setups.\nAll proposed methods converge in the same setting as their deterministic\ncounterparts and they either match or improve the best-known complexities for\nsolving structured min-max problems. Our results reinforce the correspondence\nbetween variance reduction in variational inequalities and minimization. We\nalso illustrate the improvements of our approach with numerical evaluations on\nmatrix games.",
    "published_date": "2021-02-16T00:00:00",
    "year": 2021,
    "categories": [
      "math.OC",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.08352v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.08314v1",
    "title": "Tighter Bounds on the Log Marginal Likelihood of Gaussian Process Regression Using Conjugate Gradients",
    "authors": [
      "Artem Artemev",
      "David R. Burt",
      "Mark van der Wilk"
    ],
    "author_ids": [],
    "abstract": "We propose a lower bound on the log marginal likelihood of Gaussian process\nregression models that can be computed without matrix factorisation of the full\nkernel matrix. We show that approximate maximum likelihood learning of model\nparameters by maximising our lower bound retains many of the sparse variational\napproach benefits while reducing the bias introduced into parameter learning.\nThe basis of our bound is a more careful analysis of the log-determinant term\nappearing in the log marginal likelihood, as well as using the method of\nconjugate gradients to derive tight lower bounds on the term involving a\nquadratic form. Our approach is a step forward in unifying methods relying on\nlower bound maximisation (e.g. variational methods) and iterative approaches\nbased on conjugate gradients for training Gaussian processes. In experiments,\nwe show improved predictive performance with our model for a comparable amount\nof training time compared to other conjugate gradient based approaches.",
    "published_date": "2021-02-16T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.08314v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.11211v1",
    "title": "Moral Decision-Making in Medical Hybrid Intelligent Systems: A Team Design Patterns Approach to the Bias Mitigation and Data Sharing Design Problems",
    "authors": [
      "Jip van Stijn"
    ],
    "author_ids": [],
    "abstract": "Increasing automation in the healthcare sector calls for a Hybrid\nIntelligence (HI) approach to closely study and design the collaboration of\nhumans and autonomous machines. Ensuring that medical HI systems'\ndecision-making is ethical is key. The use of Team Design Patterns (TDPs) can\nadvance this goal by describing successful and reusable configurations of\ndesign problems in which decisions have a moral component, as well as through\nfacilitating communication in multidisciplinary teams designing HI systems. For\nthis research, TDPs were developed to describe a set of solutions for two\ndesign problems in a medical HI system: (1) mitigating harmful biases in\nmachine learning algorithms and (2) sharing health and behavioral patient data\nwith healthcare professionals and system developers. The Socio-Cognitive\nEngineering methodology was employed, integrating operational demands, human\nfactors knowledge, and a technological analysis into a set of TDPs. A survey\nwas created to assess the usability of the patterns on their understandability,\neffectiveness, and generalizability. The results showed that TDPs are a useful\nmethod to unambiguously describe solutions for diverse HI design problems with\na moral component on varying abstraction levels, that are usable by a\nheterogeneous group of multidisciplinary researchers. Additionally, results\nindicated that the SCE approach and the developed questionnaire are suitable\nmethods for creating and assessing TDPs. The study concludes with a set of\nproposed improvements to TDPs, including their integration with Interaction\nDesign Patterns, the inclusion of several additional concepts, and a number of\nmethodological improvements. Finally, the thesis recommends directions for\nfuture research.",
    "published_date": "2021-02-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.11211v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.08175v1",
    "title": "Accurate and Clear Precipitation Nowcasting with Consecutive Attention and Rain-map Discrimination",
    "authors": [
      "Ashesh",
      "Buo-Fu Chen",
      "Treng-Shi Huang",
      "Boyo Chen",
      "Chia-Tung Chang",
      "Hsuan-Tien Lin"
    ],
    "author_ids": [],
    "abstract": "Precipitation nowcasting is an important task for weather forecasting. Many\nrecent works aim to predict the high rainfall events more accurately with the\nhelp of deep learning techniques, but such events are relatively rare. The\nrarity is often addressed by formulations that re-weight the rare events.\nSomehow such a formulation carries a side effect of making \"blurry\" predictions\nin low rainfall regions and cannot convince meteorologists to trust its\npractical usability. We fix the trust issue by introducing a discriminator that\nencourages the prediction model to generate realistic rain-maps without\nsacrificing predictive accuracy. Furthermore, we extend the nowcasting time\nframe from one hour to three hours to further address the needs from\nmeteorologists. The extension is based on consecutive attentions across\ndifferent hours. We propose a new deep learning model for precipitation\nnowcasting that includes both the discrimination and attention techniques. The\nmodel is examined on a newly-built benchmark dataset that contains both radar\ndata and actual rain data. The benchmark, which will be publicly released, not\nonly establishes the superiority of the proposed model, but also is expected to\nencourage future research on precipitation nowcasting.",
    "published_date": "2021-02-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.08175v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.07997v3",
    "title": "A2-FPN for Semantic Segmentation of Fine-Resolution Remotely Sensed Images",
    "authors": [
      "Rui Li",
      "Shunyi Zheng",
      "Ce Zhang",
      "Chenxi Duan",
      "Libo Wang"
    ],
    "author_ids": [],
    "abstract": "Semantic segmentation using fine-resolution remotely sensed images plays a\ncritical role in many practical applications, such as urban planning,\nenvironmental protection, natural and anthropogenic landscape monitoring, etc.\nHowever, the automation of semantic segmentation, i.e., automatic\ncategorization/labeling and segmentation is still a challenging task,\nparticularly for fine-resolution images with huge spatial and spectral\ncomplexity. Addressing such a problem represents an exciting research field,\nwhich paves the way for scene-level landscape pattern analysis and decision\nmaking. In this paper, we propose an approach for automatic land segmentation\nbased on the Feature Pyramid Network (FPN). As a classic architecture, FPN can\nbuild a feature pyramid with high-level semantics throughout. However,\nintrinsic defects in feature extraction and fusion hinder FPN from further\naggregating more discriminative features. Hence, we propose an Attention\nAggregation Module (AAM) to enhance multi-scale feature learning through\nattention-guided feature aggregation. Based on FPN and AAM, a novel framework\nnamed Attention Aggregation Feature Pyramid Network (A2-FPN) is developed for\nsemantic segmentation of fine-resolution remotely sensed images. Extensive\nexperiments conducted on three datasets demonstrate the effectiveness of our A2\n-FPN in segmentation accuracy. Code is available at\nhttps://github.com/lironui/A2-FPN.",
    "published_date": "2021-02-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.07997v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.07869v2",
    "title": "Technical Report -- Expected Exploitability: Predicting the Development of Functional Vulnerability Exploits",
    "authors": [
      "Octavian Suciu",
      "Connor Nelson",
      "Zhuoer Lyu",
      "Tiffany Bao",
      "Tudor Dumitras"
    ],
    "author_ids": [],
    "abstract": "Assessing the exploitability of software vulnerabilities at the time of\ndisclosure is difficult and error-prone, as features extracted via technical\nanalysis by existing metrics are poor predictors for exploit development.\nMoreover, exploitability assessments suffer from a class bias because \"not\nexploitable\" labels could be inaccurate. To overcome these challenges, we\npropose a new metric, called Expected Exploitability (EE), which reflects, over\ntime, the likelihood that functional exploits will be developed. Key to our\nsolution is a time-varying view of exploitability, a departure from existing\nmetrics. This allows us to learn EE using data-driven techniques from artifacts\npublished after disclosure, such as technical write-ups and proof-of-concept\nexploits, for which we design novel feature sets. This view also allows us to\ninvestigate the effect of the label biases on the classifiers. We characterize\nthe noise-generating process for exploit prediction, showing that our problem\nis subject to the most challenging type of label noise, and propose techniques\nto learn EE in the presence of noise. On a dataset of 103,137 vulnerabilities,\nwe show that EE increases precision from 49% to 86% over existing metrics,\nincluding two state-of-the-art exploit classifiers, while its precision\nsubstantially improves over time. We also highlight the practical utility of EE\nfor predicting imminent exploits and prioritizing critical vulnerabilities. We\ndevelop EE into an online platform which is publicly available at\nhttps://exploitability.app/.",
    "published_date": "2021-02-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.07869v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.07684v3",
    "title": "Fair and Optimal Cohort Selection for Linear Utilities",
    "authors": [
      "Konstantina Bairaktari",
      "Huy Le Nguyen",
      "Jonathan Ullman"
    ],
    "author_ids": [],
    "abstract": "The rise of algorithmic decision-making has created an explosion of research\naround the fairness of those algorithms. While there are many compelling\nnotions of individual fairness, beginning with the work of Dwork et al., these\nnotions typically do not satisfy desirable composition properties. To this end,\nDwork and Ilvento introduced the fair cohort selection problem, which captures\na specific application where a single fair classifier is composed with itself\nto pick a group of candidates of size exactly $k$. In this work we introduce a\nspecific instance of cohort selection where the goal is to choose a cohort\nmaximizing a linear utility function. We give approximately optimal\npolynomial-time algorithms for this problem in both an offline setting where\nthe entire fair classifier is given at once, or an online setting where\ncandidates arrive one at a time and are classified as they arrive.",
    "published_date": "2021-02-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DS",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.07684v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.07681v2",
    "title": "RPPLNS: Pay-per-last-N-shares with a Randomised Twist",
    "authors": [
      "Philip Lazos",
      "Francisco J. Marmolejo-Cossío",
      "Xinyu Zhou",
      "Jonathan Katz"
    ],
    "author_ids": [],
    "abstract": "\"Pay-per-last-$N$-shares\" (PPLNS) is one of the most common payout strategies\nused by mining pools in Proof-of-Work (PoW) cryptocurrencies. As with any\npayment scheme, it is imperative to study issues of incentive compatibility of\nminers within the pool. For PPLNS this question has only been partially\nanswered; we know that reasonably-sized miners within a PPLNS pool prefer\nfollowing the pool protocol over employing specific deviations. In this paper,\nwe present a novel modification to PPLNS where we randomise the protocol in a\nnatural way. We call our protocol \"Randomised pay-per-last-$N$-shares\"\n(RPPLNS), and note that the randomised structure of the protocol greatly\nsimplifies the study of its incentive compatibility. We show that RPPLNS\nmaintains the strengths of PPLNS (i.e., fairness, variance reduction, and\nresistance to pool hopping), while also being robust against a richer class of\nstrategic mining than what has been shown for PPLNS.",
    "published_date": "2021-02-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.07681v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.07680v1",
    "title": "Translational Equivariance in Kernelizable Attention",
    "authors": [
      "Max Horn",
      "Kumar Shridhar",
      "Elrich Groenewald",
      "Philipp F. M. Baumann"
    ],
    "author_ids": [],
    "abstract": "While Transformer architectures have show remarkable success, they are bound\nto the computation of all pairwise interactions of input element and thus\nsuffer from limited scalability. Recent work has been successful by avoiding\nthe computation of the complete attention matrix, yet leads to problems down\nthe line. The absence of an explicit attention matrix makes the inclusion of\ninductive biases relying on relative interactions between elements more\nchallenging. An extremely powerful inductive bias is translational\nequivariance, which has been conjectured to be responsible for much of the\nsuccess of Convolutional Neural Networks on image recognition tasks. In this\nwork we show how translational equivariance can be implemented in efficient\nTransformers based on kernelizable attention - Performers. Our experiments\nhighlight that the devised approach significantly improves robustness of\nPerformers to shifts of input images compared to their naive application. This\nrepresents an important step on the path of replacing Convolutional Neural\nNetworks with more expressive Transformer architectures and will help to\nimprove sample efficiency and robustness in this realm.",
    "published_date": "2021-02-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.07680v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.07454v3",
    "title": "Tight Revenue Gaps among Multi-Unit Mechanisms",
    "authors": [
      "Yaonan Jin",
      "Shunhua Jiang",
      "Pinyan Lu",
      "Hengjie Zhang"
    ],
    "author_ids": [],
    "abstract": "This paper considers Bayesian revenue maximization in the $k$-unit setting,\nwhere a monopolist seller has $k$ copies of an indivisible item and faces $n$\nunit-demand buyers (whose value distributions can be non-identical). Four basic\nmechanisms among others have been widely employed in practice and widely\nstudied in the literature: {\\sf Myerson Auction}, {\\sf Sequential\nPosted-Pricing}, {\\sf $(k + 1)$-th Price Auction with Anonymous Reserve}, and\n{\\sf Anonymous Pricing}. Regarding a pair of mechanisms, we investigate the\nlargest possible ratio between the two revenues (a.k.a.\\ the revenue gap), over\nall possible value distributions of the buyers.\n  Divide these four mechanisms into two groups: (i)~the discriminating\nmechanism group, {\\sf Myerson Auction} and {\\sf Sequential Posted-Pricing}, and\n(ii)~the anonymous mechanism group, {\\sf Anonymous Reserve} and {\\sf Anonymous\nPricing}. Within one group, the involved two mechanisms have an asymptotically\ntight revenue gap of $1 + \\Theta(1 / \\sqrt{k})$. In contrast, any two\nmechanisms from the different groups have an asymptotically tight revenue gap\nof $\\Theta(\\log k)$.",
    "published_date": "2021-02-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.07454v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.07382v1",
    "title": "Dominance inequalities for scheduling around an unrestrictive common due date",
    "authors": [
      "Anne-Elisabeth Falq",
      "Pierre Fouilhoux",
      "Safia Kedad-Sidhoum"
    ],
    "author_ids": [],
    "abstract": "The problem considered in this work consists in scheduling a set of tasks on\na single machine, around an unrestrictive common due date to minimize the\nweighted sum of earliness and tardiness. This problem can be formulated as a\ncompact mixed integer program (MIP). In this article, we focus on\nneighborhood-based dominance properties, where the neighborhood is associated\nto insert and swap operations. We derive from these properties a local search\nprocedure providing a very good heuristic solution. The main contribution of\nthis work stands in an exact solving context: we derive constraints eliminating\nthe non locally optimal solutions with respect to the insert and swap\noperations. We propose linear inequalities translating these constraints to\nstrengthen the MIP compact formulation. These inequalities, called dominance\ninequalities, are different from standard reinforcement inequalities. We\nprovide a numerical analysis which shows that adding these inequalities\nsignificantly reduces the computation time required for solving the scheduling\nproblem using a standard solver.",
    "published_date": "2021-02-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DM",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.07382v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.07333v1",
    "title": "AI Ethics Needs Good Data",
    "authors": [
      "Angela Daly",
      "S Kate Devitt",
      "Monique Mann"
    ],
    "author_ids": [],
    "abstract": "In this chapter we argue that discourses on AI must transcend the language of\n'ethics' and engage with power and political economy in order to constitute\n'Good Data'. In particular, we must move beyond the depoliticised language of\n'ethics' currently deployed (Wagner 2018) in determining whether AI is 'good'\ngiven the limitations of ethics as a frame through which AI issues can be\nviewed. In order to circumvent these limits, we use instead the language and\nconceptualisation of 'Good Data', as a more expansive term to elucidate the\nvalues, rights and interests at stake when it comes to AI's development and\ndeployment, as well as that of other digital technologies. Good Data\nconsiderations move beyond recurring themes of data protection/privacy and the\nFAT (fairness, transparency and accountability) movement to include explicit\npolitical economy critiques of power. Instead of yet more ethics principles\n(that tend to say the same or similar things anyway), we offer four 'pillars'\non which Good Data AI can be built: community, rights, usability and politics.\nOverall we view AI's 'goodness' as an explicly political (economy) question of\npower and one which is always related to the degree which AI is created and\nused to increase the wellbeing of society and especially to increase the power\nof the most marginalized and disenfranchised. We offer recommendations and\nremedies towards implementing 'better' approaches towards AI. Our strategies\nenable a different (but complementary) kind of evaluation of AI as part of the\nbroader socio-technical systems in which AI is built and deployed.",
    "published_date": "2021-02-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "K.4.1; K.4.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.07333v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.07213v1",
    "title": "Why Talking about ethics is not enough: a proposal for Fintech's AI ethics",
    "authors": [
      "Cristina Godoy Bernardo de Oliveira",
      "Evandro Eduardo Seron Ruiz"
    ],
    "author_ids": [],
    "abstract": "As the potential applications of Artificial Intelligence (AI) in the\nfinancial sector increases, ethical issues become gradually latent. The\ndistrust of individuals, social groups, and governments about the risks arising\nfrom Fintech's activities is growing. Due to this scenario, the preparation of\nrecommendations and Ethics Guidelines is increasing and the risks of being\nchosen the principles and ethical values most appropriate to companies are\nhigh. Thus, this exploratory research aims to analyze the benefits of the\napplication of the stakeholder theory and the idea of Social License to build\nan environment of trust and for the realization of ethical principles by\nFintech. The formation of a Fintech association for the creation of a Social\nLicense will allow early-stage Fintech to participate from the beginning of its\nactivities in the elaboration of a dynamic ethical code and with the\nparticipation of stakeholders.",
    "published_date": "2021-02-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.07213v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.07174v1",
    "title": "Max-Min Fair Hybrid Precoding for Multi-group Multicasting in Millimeter-Wave Channel",
    "authors": [
      "Fawwaz Alsubaie"
    ],
    "author_ids": [],
    "abstract": "The potential of using millimeter-wave (mmWave) to encounter the current\nbandwidth shortage has motivated packing more antenna elements in the same\nphysical size which permits the advent of massive\nmultiple-input-multiple-output (MIMO) for mmWave communication. However, with\nincreasing number of antenna elements, the ability of allocating a single\nRF-chain per antenna becomes infeasible and unaffordable. As a cost-effective\nalternative, the design of hybrid precoding has been considered where the\nlimited-scattering signals are captured by a high-dimensional RF precoder\nrealized by an analog phase-shifter network followed by a low-dimensional\ndigital precoder at baseband. In this paper, the max-min fair problem is\nconsidered to design a low-complexity hybrid precoder for multi-group\nmulticasting systems in mmWave channels. The problem is non-trivial due to two\nmain reasons: the original max-min problem for multi-group multicasting for a\nfully-digital precoder is non-convex, and the analog precoder places constant\nmodules constraint which restricts the feasible set of the precoders in the\ndesign problem. Therefore, we consider a low complexity hybrid precoder design\nto tackle and benefit from the mmWave channel structure. Each analog beamformer\nwas designed to maximize the minimum matching component for users within a\ngiven group. Once obtained, the digital precoder was attained by solving the\nmax-min problem of the equivalent channel.",
    "published_date": "2021-02-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.07174v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.07120v3",
    "title": "Long-Term Resource Allocation Fairness in Average Markov Decision Process (AMDP) Environment",
    "authors": [
      "Ganesh Ghalme",
      "Vineet Nair",
      "Vishakha Patil",
      "Yilun Zhou"
    ],
    "author_ids": [],
    "abstract": "Fairness has emerged as an important concern in automated decision-making in\nrecent years, especially when these decisions affect human welfare. In this\nwork, we study fairness in temporally extended decision-making settings,\nspecifically those formulated as Markov Decision Processes (MDPs). Our proposed\nnotion of fairness ensures that each state's long-term visitation frequency is\nat least a specified fraction. This quota-based notion of fairness is natural\nin many resource-allocation settings where the dynamics of a single resource\nbeing allocated is governed by an MDP and the distribution of the shared\nresource is captured by its state-visitation frequency. In an average-reward\nMDP (AMDP) setting, we formulate the problem as a bilinear saddle point program\nand, for a generative model, solve it using a Stochastic Mirror Descent (SMD)\nbased algorithm. The proposed solution guarantees a simultaneous approximation\non the expected average-reward and fairness requirement. We give sample\ncomplexity bounds for the proposed algorithm and validate our theoretical\nresults with experiments on simulated data.",
    "published_date": "2021-02-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.07120v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.07006v2",
    "title": "Asymmetric Heavy Tails and Implicit Bias in Gaussian Noise Injections",
    "authors": [
      "Alexander Camuto",
      "Xiaoyu Wang",
      "Lingjiong Zhu",
      "Chris Holmes",
      "Mert Gürbüzbalaban",
      "Umut Şimşekli"
    ],
    "author_ids": [],
    "abstract": "Gaussian noise injections (GNIs) are a family of simple and widely-used\nregularisation methods for training neural networks, where one injects additive\nor multiplicative Gaussian noise to the network activations at every iteration\nof the optimisation algorithm, which is typically chosen as stochastic gradient\ndescent (SGD). In this paper we focus on the so-called `implicit effect' of\nGNIs, which is the effect of the injected noise on the dynamics of SGD. We show\nthat this effect induces an asymmetric heavy-tailed noise on SGD gradient\nupdates. In order to model this modified dynamics, we first develop a\nLangevin-like stochastic differential equation that is driven by a general\nfamily of asymmetric heavy-tailed noise. Using this model we then formally\nprove that GNIs induce an `implicit bias', which varies depending on the\nheaviness of the tails and the level of asymmetry. Our empirical results\nconfirm that different types of neural networks trained with GNIs are\nwell-modelled by the proposed dynamics and that the implicit effect of these\ninjections induces a bias that degrades the performance of networks.",
    "published_date": "2021-02-13T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.07006v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.06884v1",
    "title": "GPSPiChain-Blockchain based Self-Contained Family Security System in Smart Home",
    "authors": [
      "Ali Raza",
      "Lachlan Hardy",
      "Erin Roehrer",
      "Soonja Yeom",
      "Byeong ho Kang"
    ],
    "author_ids": [],
    "abstract": "With advancements in technology, personal computing devices are better\nadapted for and further integrated into people's lives and homes. The\nintegration of technology into society also results in an increasing desire to\ncontrol who and what has access to sensitive information, especially for\nvulnerable people including children and the elderly. With blockchain coming in\nto the picture as a technology that can revolutionise the world, it is now\npossible to have an immutable audit trail of locational data over time. By\ncontrolling the process through inexpensive equipment in the home, it is\npossible to control whom has access to such personal data. This paper presents\na blockchain based family security system for tracking the location of\nconsenting family members' smart phones. The locations of the family members'\nsmart phones are logged and stored in a private blockchain which can be\naccessed through a node installed in the family home on a computer. The data\nfor the whereabouts of family members stays within the family unit and does not\ngo to any third party. The system is implemented in a small scale (one miner\nand two other nodes) and the technical feasibility is discussed along with the\nlimitations of the system. Further research will cover the integration of the\nsystem into a smart home environment, and ethical implementations of tracking,\nespecially of vulnerable people, using the immutability of blockchain.",
    "published_date": "2021-02-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.06884v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.06822v1",
    "title": "Demystifying Inductive Biases for $β$-VAE Based Architectures",
    "authors": [
      "Dominik Zietlow",
      "Michal Rolinek",
      "Georg Martius"
    ],
    "author_ids": [],
    "abstract": "The performance of $\\beta$-Variational-Autoencoders ($\\beta$-VAEs) and their\nvariants on learning semantically meaningful, disentangled representations is\nunparalleled. On the other hand, there are theoretical arguments suggesting the\nimpossibility of unsupervised disentanglement. In this work, we shed light on\nthe inductive bias responsible for the success of VAE-based architectures. We\nshow that in classical datasets the structure of variance, induced by the\ngenerating factors, is conveniently aligned with the latent directions fostered\nby the VAE objective. This builds the pivotal bias on which the disentangling\nabilities of VAEs rely. By small, elaborate perturbations of existing datasets,\nwe hide the convenient correlation structure that is easily exploited by a\nvariety of architectures. To demonstrate this, we construct modified versions\nof standard datasets in which (i) the generative factors are perfectly\npreserved; (ii) each image undergoes a mild transformation causing a small\nchange of variance; (iii) the leading \\textbf{VAE-based disentanglement\narchitectures fail to produce disentangled representations whilst the\nperformance of a non-variational method remains unchanged}. The construction of\nour modifications is nontrivial and relies on recent progress on mechanistic\nunderstanding of $\\beta$-VAEs and their connection to PCA. We strengthen that\nconnection by providing additional insights that are of stand-alone interest.",
    "published_date": "2021-02-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.06822v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.06794v3",
    "title": "Extending Lagrangian and Hamiltonian Neural Networks with Differentiable Contact Models",
    "authors": [
      "Yaofeng Desmond Zhong",
      "Biswadip Dey",
      "Amit Chakraborty"
    ],
    "author_ids": [],
    "abstract": "The incorporation of appropriate inductive bias plays a critical role in\nlearning dynamics from data. A growing body of work has been exploring ways to\nenforce energy conservation in the learned dynamics by encoding Lagrangian or\nHamiltonian dynamics into the neural network architecture. These existing\napproaches are based on differential equations, which do not allow\ndiscontinuity in the states and thereby limit the class of systems one can\nlearn. However, in reality, most physical systems, such as legged robots and\nrobotic manipulators, involve contacts and collisions, which introduce\ndiscontinuities in the states. In this paper, we introduce a differentiable\ncontact model, which can capture contact mechanics: frictionless/frictional, as\nwell as elastic/inelastic. This model can also accommodate inequality\nconstraints, such as limits on the joint angles. The proposed contact model\nextends the scope of Lagrangian and Hamiltonian neural networks by allowing\nsimultaneous learning of contact and system properties. We demonstrate this\nframework on a series of challenging 2D and 3D physical systems with different\ncoefficients of restitution and friction. The learned dynamics can be used as a\ndifferentiable physics simulator for downstream gradient-based optimization\ntasks, such as planning and control.",
    "published_date": "2021-02-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.06794v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.06764v1",
    "title": "Technical Challenges for Training Fair Neural Networks",
    "authors": [
      "Valeriia Cherepanova",
      "Vedant Nanda",
      "Micah Goldblum",
      "John P. Dickerson",
      "Tom Goldstein"
    ],
    "author_ids": [],
    "abstract": "As machine learning algorithms have been widely deployed across applications,\nmany concerns have been raised over the fairness of their predictions,\nespecially in high stakes settings (such as facial recognition and medical\nimaging). To respond to these concerns, the community has proposed and\nformalized various notions of fairness as well as methods for rectifying unfair\nbehavior. While fairness constraints have been studied extensively for\nclassical models, the effectiveness of methods for imposing fairness on deep\nneural networks is unclear. In this paper, we observe that these large models\noverfit to fairness objectives, and produce a range of unintended and\nundesirable consequences. We conduct our experiments on both facial recognition\nand automated medical diagnosis datasets using state-of-the-art architectures.",
    "published_date": "2021-02-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.06764v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.06761v1",
    "title": "MIMIC-IF: Interpretability and Fairness Evaluation of Deep Learning Models on MIMIC-IV Dataset",
    "authors": [
      "Chuizheng Meng",
      "Loc Trinh",
      "Nan Xu",
      "Yan Liu"
    ],
    "author_ids": [],
    "abstract": "The recent release of large-scale healthcare datasets has greatly propelled\nthe research of data-driven deep learning models for healthcare applications.\nHowever, due to the nature of such deep black-boxed models, concerns about\ninterpretability, fairness, and biases in healthcare scenarios where human\nlives are at stake call for a careful and thorough examinations of both\ndatasets and models. In this work, we focus on MIMIC-IV (Medical Information\nMart for Intensive Care, version IV), the largest publicly available healthcare\ndataset, and conduct comprehensive analyses of dataset representation bias as\nwell as interpretability and prediction fairness of deep learning models for\nin-hospital mortality prediction. In terms of interpretabilty, we observe that\n(1) the best performing interpretability method successfully identifies\ncritical features for mortality prediction on various prediction models; (2)\ndemographic features are important for prediction. In terms of fairness, we\nobserve that (1) there exists disparate treatment in prescribing mechanical\nventilation among patient groups across ethnicity, gender and age; (2) all of\nthe studied mortality predictors are generally fair while the IMV-LSTM\n(Interpretable Multi-Variable Long Short-Term Memory) model provides the most\naccurate and unbiased predictions across all protected groups. We further draw\nconcrete connections between interpretability methods and fairness metrics by\nshowing how feature importance from interpretability methods can be beneficial\nin quantifying potential disparities in mortality predictors.",
    "published_date": "2021-02-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.06761v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.06695v2",
    "title": "Bias-Free Scalable Gaussian Processes via Randomized Truncations",
    "authors": [
      "Andres Potapczynski",
      "Luhuan Wu",
      "Dan Biderman",
      "Geoff Pleiss",
      "John P. Cunningham"
    ],
    "author_ids": [],
    "abstract": "Scalable Gaussian Process methods are computationally attractive, yet\nintroduce modeling biases that require rigorous study. This paper analyzes two\ncommon techniques: early truncated conjugate gradients (CG) and random Fourier\nfeatures (RFF). We find that both methods introduce a systematic bias on the\nlearned hyperparameters: CG tends to underfit while RFF tends to overfit. We\naddress these issues using randomized truncation estimators that eliminate bias\nin exchange for increased variance. In the case of RFF, we show that the\nbias-to-variance conversion is indeed a trade-off: the additional variance\nproves detrimental to optimization. However, in the case of CG, our unbiased\nlearning procedure meaningfully outperforms its biased counterpart with minimal\nadditional computation.",
    "published_date": "2021-02-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.06695v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.06619v3",
    "title": "Differences in the spatial landscape of urban mobility: gender and socioeconomic perspectives",
    "authors": [
      "Mariana Macedo",
      "Laura Lotero",
      "Alessio Cardillo",
      "Ronaldo Menezes",
      "Hugo Barbosa"
    ],
    "author_ids": [],
    "abstract": "Many of our routines and activities are linked to our ability to move; be it\ncommuting to work, shopping for groceries, or meeting friends. Yet, factors\nthat limit the individuals' ability to fully realise their mobility needs will\nultimately affect the opportunities they can have access to (e.g., cultural\nactivities, professional interactions). One important aspect frequently\noverlooked in human mobility studies is how gender-centred issues can amplify\nother sources of mobility disadvantages (e.g., socioeconomic inequalities),\nunevenly affecting the pool of opportunities men and women have access to. In\nthis work, we leverage on a combination of computational, statistical, and\ninformation-theoretical approaches to investigate the existence of systematic\ndiscrepancies in the mobility diversity (i.e., the diversity of travel\ndestinations) of (1) men and women from different socioeconomic backgrounds,\nand (2) work and non-work travels. Our analysis is based on datasets containing\nmultiple instances of large-scale, official, travel surveys carried out in\nthree major metropolitan areas in South America: Medell\\'in and Bogot\\'a in\nColombia, and S\\~ao Paulo in Brazil. Our results indicate the presence of\ngeneral discrepancies in the urban mobility diversities related to the gender\nand socioeconomic characteristics of the individuals. Lastly, this paper sheds\nnew light on the possible origins of gender-level human mobility inequalities,\ncontributing to the general understanding of disaggregated patterns in human\nmobility.",
    "published_date": "2021-02-12T00:00:00",
    "year": 2021,
    "categories": [
      "physics.soc-ph",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.06619v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.06362v3",
    "title": "A Decentralized Approach towards Responsible AI in Social Ecosystems",
    "authors": [
      "Wenjing Chu"
    ],
    "author_ids": [],
    "abstract": "For AI technology to fulfill its full promises, we must have effective means\nto ensure Responsible AI behavior and curtail potential irresponsible use,\ne.g., in areas of privacy protection, human autonomy, robustness, and\nprevention of biases and discrimination in automated decision making. Recent\nliterature in the field has identified serious shortcomings of narrow\ntechnology focused and formalism-oriented research and has proposed an\ninterdisciplinary approach that brings the social context into the scope of\nstudy. In this paper, we take a sociotechnical approach to propose a more\nexpansive framework of thinking about the Responsible AI challenges in both\ntechnical and social context. Effective solutions need to bridge the gap\nbetween a technical system with the social system that it will be deployed to.\nTo this end, we propose human agency and regulation as main mechanisms of\nintervention and propose a decentralized computational infrastructure, or a set\nof public utilities, as the computational means to bridge this gap. A\ndecentralized infrastructure is uniquely suited for meeting this challenge and\nenable technical solutions and social institutions in a mutually reinforcing\ndynamic to achieve Responsible AI goals. Our approach is novel in its\nsociotechnical approach and its aim in tackling the structural issues that\ncannot be solved within the narrow confines of AI technical research. We then\nexplore possible features of the proposed infrastructure and discuss how it may\nhelp solve example problems recently studied in the field.",
    "published_date": "2021-02-12T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.06362v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.06311v2",
    "title": "Does Culture Matter? Impact of Individualism and Uncertainty Avoidance on App Reviews",
    "authors": [
      "Ricarda Anna-Lena Fischer",
      "Rita Walczuch",
      "Emitza Guzman"
    ],
    "author_ids": [],
    "abstract": "Mobile applications are often used by an international audience and therefore\nreceive a high daily amount of user reviews from various countries. Previous\nwork found evidence that app store reviews contain helpful information for\nsoftware evolution processes. However, the cultural diversity of the reviews\nand its consequences on specific user feedback characteristics has only been\nresearched to a limited extent so far. In this paper, we examine the influence\nof two cultural dimensions, Individualism and Uncertainty Avoidance on user\nfeedback in Apple app store reviews written in different languages. For this\npurpose, we collected 647,141 reviews from eight countries and written in five\nlanguages over a period of six months. We then used manual content analysis and\nautomated processing to examine a sample of 3,120 reviews. The results show\nthat there is a statistically significant influence of Individualism and\nUncertainty Avoidance on user feedback characteristics. The results of this\nstudy will help researchers and practitioners to reduce algorithm bias caused\nby less diversified training and test data and to raise awareness of the\nimportance of analyzing diversified user feedback.",
    "published_date": "2021-02-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.06311v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.09364v1",
    "title": "Ethics as a service: a pragmatic operationalisation of AI Ethics",
    "authors": [
      "Jessica Morley",
      "Anat Elhalal",
      "Francesca Garcia",
      "Libby Kinsey",
      "Jakob Mokander",
      "Luciano Floridi"
    ],
    "author_ids": [],
    "abstract": "As the range of potential uses for Artificial Intelligence (AI), in\nparticular machine learning (ML), has increased, so has awareness of the\nassociated ethical issues. This increased awareness has led to the realisation\nthat existing legislation and regulation provides insufficient protection to\nindividuals, groups, society, and the environment from AI harms. In response to\nthis realisation, there has been a proliferation of principle-based ethics\ncodes, guidelines and frameworks. However, it has become increasingly clear\nthat a significant gap exists between the theory of AI ethics principles and\nthe practical design of AI systems. In previous work, we analysed whether it is\npossible to close this gap between the what and the how of AI ethics through\nthe use of tools and methods designed to help AI developers, engineers, and\ndesigners translate principles into practice. We concluded that this method of\nclosure is currently ineffective as almost all existing translational tools and\nmethods are either too flexible (and thus vulnerable to ethics washing) or too\nstrict (unresponsive to context). This raised the question: if, even with\ntechnical guidance, AI ethics is challenging to embed in the process of\nalgorithmic design, is the entire pro-ethical design endeavour rendered futile?\nAnd, if no, then how can AI ethics be made useful for AI practitioners? This is\nthe question we seek to address here by exploring why principles and technical\ntranslational tools are still needed even if they are limited, and how these\nlimitations can be potentially overcome by providing theoretical grounding of a\nconcept that has been termed Ethics as a Service.",
    "published_date": "2021-02-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.09364v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.06246v2",
    "title": "Regret, stability & fairness in matching markets with bandit learners",
    "authors": [
      "Sarah H. Cen",
      "Devavrat Shah"
    ],
    "author_ids": [],
    "abstract": "Making an informed decision -- for example, when choosing a career or housing\n-- requires knowledge about the available options. Such knowledge is generally\nacquired through costly trial and error, but this learning process can be\ndisrupted by competition. In this work, we study how competition affects the\nlong-term outcomes of individuals as they learn. We build on a line of work\nthat models this setting as a two-sided matching market with bandit learners. A\nrecent result in this area states that it is impossible to simultaneously\nguarantee two natural desiderata: stability and low optimal regret for all\nagents. Resource-allocating platforms can point to this result as a\njustification for assigning good long-term outcomes to some agents and poor\nones to others. We show that this impossibility need not hold true. In\nparticular, by modeling two additional components of competition -- namely,\ncosts and transfers -- we prove that it is possible to simultaneously guarantee\nfour desiderata: stability, low optimal regret, fairness in the distribution of\nregret, and high social welfare.",
    "published_date": "2021-02-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.GT",
      "cs.MA",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.06246v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.06150v1",
    "title": "A Geometric Nonlinear Stochastic Filter for Simultaneous Localization and Mapping",
    "authors": [
      "Hashim A. Hashim"
    ],
    "author_ids": [],
    "abstract": "Simultaneous Localization and Mapping (SLAM) is one of the key robotics tasks\nas it tackles simultaneous mapping of the unknown environment defined by\nmultiple landmark positions and localization of the unknown pose (i.e.,\nattitude and position) of the robot in three-dimensional (3D) space. The true\nSLAM problem is modeled on the Lie group of $\\mathbb{SLAM}_{n}\\left(3\\right)$,\nand its true dynamics rely on angular and translational velocities. This paper\nproposes a novel geometric nonlinear stochastic estimator algorithm for SLAM on\n$\\mathbb{SLAM}_{n}\\left(3\\right)$ that precisely mimics the nonlinear motion\ndynamics of the true SLAM problem. Unlike existing solutions, the proposed\nstochastic filter takes into account unknown constant bias and noise attached\nto the velocity measurements. The proposed nonlinear stochastic estimator on\nmanifold is guaranteed to produce good results provided with the measurements\nof angular velocities, translational velocities, landmarks, and inertial\nmeasurement unit (IMU). Simulation and experimental results reflect the ability\nof the proposed filter to successfully estimate the six-degrees-of-freedom (6\nDoF) robot's pose and landmark positions. Keywords: Simultaneous Localization\nand Mapping, nonlinear stochastic observer for SLAM, stochastic differential\nequations, pose estimator, position, attitude, Brownian motion process,\ninertial measurement unit, landmarks, features, SDE, SO(3), SE(3), SLAM.",
    "published_date": "2021-02-11T00:00:00",
    "year": 2021,
    "categories": [
      "eess.SY",
      "cs.RO",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.06150v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.06115v1",
    "title": "District-Fair Participatory Budgeting",
    "authors": [
      "D Ellis Hershkowitz",
      "Anson Kahng",
      "Dominik Peters",
      "Ariel D. Procaccia"
    ],
    "author_ids": [],
    "abstract": "Participatory budgeting is a method used by city governments to select public\nprojects to fund based on residents' votes. Many cities use participatory\nbudgeting at a district level. Typically, a budget is divided among districts\nproportionally to their population, and each district holds an election over\nlocal projects and then uses its budget to fund the projects most preferred by\nits voters. However, district-level participatory budgeting can yield poor\nsocial welfare because it does not necessarily fund projects supported across\nmultiple districts. On the other hand, decision making that only takes global\nsocial welfare into account can be unfair to districts: A\nsocial-welfare-maximizing solution might not fund any of the projects preferred\nby a district, despite the fact that its constituents pay taxes to the city.\nThus, we study how to fairly maximize social welfare in a participatory\nbudgeting setting with a single city-wide election. We propose a notion of\nfairness that guarantees each district at least as much welfare as it would\nhave received in a district-level election. We show that, although optimizing\nsocial welfare subject to this notion of fairness is NP-hard, we can\nefficiently construct a lottery over welfare-optimal outcomes that is fair in\nexpectation. Moreover, we show that, when we are allowed to slightly relax\nfairness, we can efficiently compute a fair solution that is\nwelfare-maximizing, but which may overspend the budget.",
    "published_date": "2021-02-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT",
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.06115v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.06004v3",
    "title": "Fairness-Aware PAC Learning from Corrupted Data",
    "authors": [
      "Nikola Konstantinov",
      "Christoph H. Lampert"
    ],
    "author_ids": [],
    "abstract": "Addressing fairness concerns about machine learning models is a crucial step\ntowards their long-term adoption in real-world automated systems. While many\napproaches have been developed for training fair models from data, little is\nknown about the robustness of these methods to data corruption. In this work we\nconsider fairness-aware learning under worst-case data manipulations. We show\nthat an adversary can in some situations force any learner to return an overly\nbiased classifier, regardless of the sample size and with or without degrading\naccuracy, and that the strength of the excess bias increases for learning\nproblems with underrepresented protected groups in the data. We also prove that\nour hardness results are tight up to constant factors. To this end, we study\ntwo natural learning algorithms that optimize for both accuracy and fairness\nand show that these algorithms enjoy guarantees that are order-optimal in terms\nof the corruption ratio and the protected groups frequencies in the large data\nlimit.",
    "published_date": "2021-02-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.06004v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.05996v2",
    "title": "Fairness Through Regularization for Learning to Rank",
    "authors": [
      "Nikola Konstantinov",
      "Christoph H. Lampert"
    ],
    "author_ids": [],
    "abstract": "Given the abundance of applications of ranking in recent years, addressing\nfairness concerns around automated ranking systems becomes necessary for\nincreasing the trust among end-users. Previous work on fair ranking has mostly\nfocused on application-specific fairness notions, often tailored to online\nadvertising, and it rarely considers learning as part of the process. In this\nwork, we show how to transfer numerous fairness notions from binary\nclassification to a learning to rank setting. Our formalism allows us to design\nmethods for incorporating fairness objectives with provable generalization\nguarantees. An extensive experimental evaluation shows that our method can\nimprove ranking fairness substantially with no or only little loss of model\nquality.",
    "published_date": "2021-02-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.IR",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.05996v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.05975v1",
    "title": "Investigating Trade-offs in Utility, Fairness and Differential Privacy in Neural Networks",
    "authors": [
      "Marlotte Pannekoek",
      "Giacomo Spigler"
    ],
    "author_ids": [],
    "abstract": "To enable an ethical and legal use of machine learning algorithms, they must\nboth be fair and protect the privacy of those whose data are being used.\nHowever, implementing privacy and fairness constraints might come at the cost\nof utility (Jayaraman & Evans, 2019; Gong et al., 2020). This paper\ninvestigates the privacy-utility-fairness trade-off in neural networks by\ncomparing a Simple (S-NN), a Fair (F-NN), a Differentially Private (DP-NN), and\na Differentially Private and Fair Neural Network (DPF-NN) to evaluate\ndifferences in performance on metrics for privacy (epsilon, delta), fairness\n(risk difference), and utility (accuracy). In the scenario with the highest\nconsidered privacy guarantees (epsilon = 0.1, delta = 0.00001), the DPF-NN was\nfound to achieve better risk difference than all the other neural networks with\nonly a marginally lower accuracy than the S-NN and DP-NN. This model is\nconsidered fair as it achieved a risk difference below the strict (0.05) and\nlenient (0.1) thresholds. However, while the accuracy of the proposed model\nimproved on previous work from Xu, Yuan and Wu (2019), the risk difference was\nfound to be worse.",
    "published_date": "2021-02-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.05975v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.05347v3",
    "title": "From Sampling to Optimization on Discrete Domains with Applications to Determinant Maximization",
    "authors": [
      "Nima Anari",
      "Thuy-Duong Vuong"
    ],
    "author_ids": [],
    "abstract": "We show a connection between sampling and optimization on discrete domains.\nFor a family of distributions $\\mu$ defined on size $k$ subsets of a ground set\nof elements that is closed under external fields, we show that rapid mixing of\nnatural local random walks implies the existence of simple approximation\nalgorithms to find $\\max \\mu(\\cdot)$. More precisely we show that if\n(multi-step) down-up random walks have spectral gap at least inverse\npolynomially large in $k$, then (multi-step) local search can find $\\max\n\\mu(\\cdot)$ within a factor of $k^{O(k)}$. As the main application of our\nresult, we show a simple nearly-optimal $k^{O(k)}$-factor approximation\nalgorithm for MAP inference on nonsymmetric DPPs. This is the first nontrivial\nmultiplicative approximation for finding the largest size $k$ principal minor\nof a square (not-necessarily-symmetric) matrix $L$ with $L+L^\\intercal\\succeq\n0$.\n  We establish the connection between sampling and optimization by showing that\nan exchange inequality, a concept rooted in discrete convex analysis, can be\nderived from fast mixing of local random walks. We further connect exchange\ninequalities with composable core-sets for optimization, generalizing recent\nresults on composable core-sets for DPP maximization to arbitrary distributions\nthat satisfy either the strongly Rayleigh property or that have a log-concave\ngenerating polynomial.",
    "published_date": "2021-02-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.DS",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.05347v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.05310v2",
    "title": "Controlled Experimentation in Continuous Experimentation: Knowledge and Challenges",
    "authors": [
      "Florian Auer",
      "Rasmus Ros",
      "Lukas Kaltenbrunner",
      "Per Runeson",
      "Michael Felderer"
    ],
    "author_ids": [],
    "abstract": "Context: Continuous experimentation and A/B testing is an established\nindustry practice that has been researched for more than 10 years. Our aim is\nto synthesize the conducted research.\n  Objective: We wanted to find the core constituents of a framework for\ncontinuous experimentation and the solutions that are applied within the field.\nFinally, we were interested in the challenges and benefits reported of\ncontinuous experimentation.\n  Method: We applied forward snowballing on a known set of papers and\nidentified a total of 128 relevant papers. Based on this set of papers we\nperformed two qualitative narrative syntheses and a thematic synthesis to\nanswer the research questions.\n  Results: The framework constituents for continuous experimentation include\nexperimentation processes as well as supportive technical and organizational\ninfrastructure. The solutions found in the literature were synthesized to nine\nthemes, e.g. experiment design, automated experiments, or metric specification.\nConcerning the challenges of continuous experimentation, the analysis\nidentified cultural, organizational, business, technical, statistical, ethical,\nand domain-specific challenges. Further, the study concludes that the benefits\nof experimentation are mostly implicit in the studies.\n  Conclusions: The research on continuous experimentation has yielded a large\nbody of knowledge on experimentation. The synthesis of published research\npresented within include recommended infrastructure and experimentation process\nmodels, guidelines to mitigate the identified challenges, and what problems the\nvarious published solutions solve.",
    "published_date": "2021-02-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.05310v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.05210v1",
    "title": "D2A U-Net: Automatic Segmentation of COVID-19 Lesions from CT Slices with Dilated Convolution and Dual Attention Mechanism",
    "authors": [
      "Xiangyu Zhao",
      "Peng Zhang",
      "Fan Song",
      "Guangda Fan",
      "Yangyang Sun",
      "Yujia Wang",
      "Zheyuan Tian",
      "Luqi Zhang",
      "Guanglei Zhang"
    ],
    "author_ids": [],
    "abstract": "Coronavirus Disease 2019 (COVID-19) has caused great casualties and becomes\nalmost the most urgent public health events worldwide. Computed tomography (CT)\nis a significant screening tool for COVID-19 infection, and automated\nsegmentation of lung infection in COVID-19 CT images will greatly assist\ndiagnosis and health care of patients. However, accurate and automatic\nsegmentation of COVID-19 lung infections remains to be challenging. In this\npaper we propose a dilated dual attention U-Net (D2A U-Net) for COVID-19 lesion\nsegmentation in CT slices based on dilated convolution and a novel dual\nattention mechanism to address the issues above. We introduce a dilated\nconvolution module in model decoder to achieve large receptive field, which\nrefines decoding process and contributes to segmentation accuracy. Also, we\npresent a dual attention mechanism composed of two attention modules which are\ninserted to skip connection and model decoder respectively. The dual attention\nmechanism is utilized to refine feature maps and reduce semantic gap between\ndifferent levels of the model. The proposed method has been evaluated on\nopen-source dataset and outperforms cutting edges methods in semantic\nsegmentation. Our proposed D2A U-Net with pretrained encoder achieves a Dice\nscore of 0.7298 and recall score of 0.7071. Besides, we also build a simplified\nD2A U-Net without pretrained encoder to provide a fair comparison with other\nmodels trained from scratch, which still outperforms popular U-Net family\nmodels with a Dice score of 0.7047 and recall score of 0.6626. Our experiment\nresults have shown that by introducing dilated convolution and dual attention\nmechanism, the number of false positives is significantly reduced, which\nimproves sensitivity to COVID-19 lesions and subsequently brings significant\nincrease to Dice score.",
    "published_date": "2021-02-10T00:00:00",
    "year": 2021,
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.05210v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.05143v3",
    "title": "Classifier Calibration: with application to threat scores in cybersecurity",
    "authors": [
      "Waleed A. Yousef",
      "Issa Traore",
      "William Briguglio"
    ],
    "author_ids": [],
    "abstract": "This paper explores the calibration of a classifier output score in binary\nclassification problems. A calibrator is a function that maps the arbitrary\nclassifier score, of a testing observation, onto $[0,1]$ to provide an estimate\nfor the posterior probability of belonging to one of the two classes.\nCalibration is important for two reasons; first, it provides a meaningful\nscore, that is the posterior probability; second, it puts the scores of\ndifferent classifiers on the same scale for comparable interpretation. The\npaper presents three main contributions: (1) Introducing multi-score\ncalibration, when more than one classifier provides a score for a single\nobservation. (2) Introducing the idea that the classifier scores to a\ncalibration process are nothing but features to a classifier, hence proposing\nexpanding the classifier scores to higher dimensions to boost the calibrator's\nperformance. (3) Conducting a massive simulation study, in the order of 24,000\nexperiments, that incorporates different configurations, in addition to\nexperimenting on two real datasets from the cybersecurity domain. The results\nshow that there is no overall winner among the different calibrators and\ndifferent configurations. However, general advices for practitioners include\nthe following: the Platt's\ncalibrator~\\citep{Platt1999ProbabilisticOutputsForSupport}, a version of the\nlogistic regression that decreases bias for a small sample size, has a very\nstable and acceptable performance among all experiments; our suggested\nmulti-score calibration provides better performance than single score\ncalibration in the majority of experiments, including the two real datasets. In\naddition, expanding the scores can help in some experiments.",
    "published_date": "2021-02-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.05143v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.05085v1",
    "title": "The Use and Misuse of Counterfactuals in Ethical Machine Learning",
    "authors": [
      "Atoosa Kasirzadeh",
      "Andrew Smart"
    ],
    "author_ids": [],
    "abstract": "The use of counterfactuals for considerations of algorithmic fairness and\nexplainability is gaining prominence within the machine learning community and\nindustry. This paper argues for more caution with the use of counterfactuals\nwhen the facts to be considered are social categories such as race or gender.\nWe review a broad body of papers from philosophy and social sciences on social\nontology and the semantics of counterfactuals, and we conclude that the\ncounterfactual approach in machine learning fairness and social explainability\ncan require an incoherent theory of what social categories are. Our findings\nsuggest that most often the social categories may not admit counterfactual\nmanipulation, and hence may not appropriately satisfy the demands for\nevaluating the truth or falsity of counterfactuals. This is important because\nthe widespread use of counterfactuals in machine learning can lead to\nmisleading results when applied in high-stakes domains. Accordingly, we argue\nthat even though counterfactuals play an essential part in some causal\ninferences, their use for questions of algorithmic fairness and social\nexplanations can create more problems than they resolve. Our positive result is\na set of tenets about using counterfactuals for fairness and explanations in\nmachine learning.",
    "published_date": "2021-02-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.05085v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.05077v2",
    "title": "The Multiplicative Version of Azuma's Inequality, with an Application to Contention Analysis",
    "authors": [
      "William Kuszmaul",
      "Qi Qi"
    ],
    "author_ids": [],
    "abstract": "Azuma's inequality is a tool for proving concentration bounds on random\nvariables. The inequality can be thought of as a natural generalization of\nadditive Chernoff bounds. On the other hand, the analogous generalization of\nmultiplicative Chernoff bounds does not appear to be widely known. We formulate\na multiplicative-error version of Azuma's inequality.\n  We then show how to apply this new inequality in order to greatly simplify\n(and correct) the analysis of contention delays in multithreaded systems\nmanaged by randomized work stealing.",
    "published_date": "2021-02-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.05077v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.09369v1",
    "title": "Effect of Social Media Use on Mental Health during Lockdown in India",
    "authors": [
      "Sweta Swarnam"
    ],
    "author_ids": [],
    "abstract": "This research paper studies about the role of social media use and increase\nthe risk factor of mental health during covid 19 or lockdown. Although few\nstudies have been conducted on the role about the effect of social media use on\nmental health during lockdown and impact on human reactive nature during\nlockdown. As a rapidly spreading pandemic, a biomedical disease has serious\nphysical and tremendous mental health implications. An occupational community\nof internal migrant workers is one of the most vulnerable, but neglected, and\nis likely to develop psychological ill-effects due to COVID-19's double whammy\nimpact. Mental health is a crucial aspect that needs to be addressed during\nthis lock-down as all modes of communication revolve around the virus. There\nare many difficulties with the unprecedented changes that have occurred so\nquickly due to the pandemic and stay-at - home confinement to achieve social\ndistance and mitigate the risk of infection. These include impaired health,\nwell-being, and sleep as a result of daily routine disruption, anxiety, worry,\nisolation, greater stress on family and work, and excessive screen time. An\nessential part of our overall health and well-being is mental and emotional\nhealth. An important skill is managing emotions and maintaining emotional\nbalance. It helps you face challenges and stress when you manage your emotional\nhealth. Lack of skills in emotional regulation may lead to poor mental health\nand relationship difficulties. It is as important to look after our mental\nhealth as it is to look after our physical health. For mental health\nprofessionals, the pandemic has also brought many ethical challenges.",
    "published_date": "2021-02-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "F.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.09369v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.04848v1",
    "title": "Train a One-Million-Way Instance Classifier for Unsupervised Visual Representation Learning",
    "authors": [
      "Yu Liu",
      "Lianghua Huang",
      "Pan Pan",
      "Bin Wang",
      "Yinghui Xu",
      "Rong Jin"
    ],
    "author_ids": [],
    "abstract": "This paper presents a simple unsupervised visual representation learning\nmethod with a pretext task of discriminating all images in a dataset using a\nparametric, instance-level classifier. The overall framework is a replica of a\nsupervised classification model, where semantic classes (e.g., dog, bird, and\nship) are replaced by instance IDs. However, scaling up the classification task\nfrom thousands of semantic labels to millions of instance labels brings\nspecific challenges including 1) the large-scale softmax computation; 2) the\nslow convergence due to the infrequent visiting of instance samples; and 3) the\nmassive number of negative classes that can be noisy. This work presents\nseveral novel techniques to handle these difficulties. First, we introduce a\nhybrid parallel training framework to make large-scale training feasible.\nSecond, we present a raw-feature initialization mechanism for classification\nweights, which we assume offers a contrastive prior for instance discrimination\nand can clearly speed up converge in our experiments. Finally, we propose to\nsmooth the labels of a few hardest classes to avoid optimizing over very\nsimilar negative pairs. While being conceptually simple, our framework achieves\ncompetitive or superior performance compared to state-of-the-art unsupervised\napproaches, i.e., SimCLR, MoCoV2, and PIC under ImageNet linear evaluation\nprotocol and on several downstream visual tasks, verifying that full instance\nclassification is a strong pretraining technique for many semantic visual\ntasks.",
    "published_date": "2021-02-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.04848v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.04568v1",
    "title": "Tracking e-cigarette warning label compliance on Instagram with deep learning",
    "authors": [
      "Chris J. Kennedy",
      "Julia Vassey",
      "Ho-Chun Herbert Chang",
      "Jennifer B. Unger",
      "Emilio Ferrara"
    ],
    "author_ids": [],
    "abstract": "The U.S. Food & Drug Administration (FDA) requires that e-cigarette\nadvertisements include a prominent warning label that reminds consumers that\nnicotine is addictive. However, the high volume of vaping-related posts on\nsocial media makes compliance auditing expensive and time-consuming, suggesting\nthat an automated, scalable method is needed. We sought to develop and evaluate\na deep learning system designed to automatically determine if an Instagram post\npromotes vaping, and if so, if an FDA-compliant warning label was included or\nif a non-compliant warning label was visible in the image. We compiled and\nlabeled a dataset of 4,363 Instagram images, of which 44% were vaping-related,\n3% contained FDA-compliant warning labels, and 4% contained non-compliant\nlabels. Using a 20% test set for evaluation, we tested multiple neural network\nvariations: image processing backbone model (Inceptionv3, ResNet50,\nEfficientNet), data augmentation, progressive layer unfreezing, output bias\ninitialization designed for class imbalance, and multitask learning. Our final\nmodel achieved an area under the curve (AUC) and [accuracy] of 0.97 [92%] on\nvaping classification, 0.99 [99%] on FDA-compliant warning labels, and 0.94\n[97%] on non-compliant warning labels. We conclude that deep learning models\ncan effectively identify vaping posts on Instagram and track compliance with\nFDA warning label requirements.",
    "published_date": "2021-02-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.04568v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.04565v2",
    "title": "A Ranking Approach to Fair Classification",
    "authors": [
      "Jakob Schoeffer",
      "Niklas Kuehl",
      "Isabel Valera"
    ],
    "author_ids": [],
    "abstract": "Algorithmic decision systems are increasingly used in areas such as hiring,\nschool admission, or loan approval. Typically, these systems rely on labeled\ndata for training a classification model. However, in many scenarios,\nground-truth labels are unavailable, and instead we have only access to\nimperfect labels as the result of (potentially biased) human-made decisions.\nDespite being imperfect, historical decisions often contain some useful\ninformation on the unobserved true labels. In this paper, we focus on scenarios\nwhere only imperfect labels are available and propose a new fair ranking-based\ndecision system based on monotonic relationships between legitimate features\nand the outcome. Our approach is both intuitive and easy to implement, and thus\nparticularly suitable for adoption in real-world settings. More in detail, we\nintroduce a distance-based decision criterion, which incorporates useful\ninformation from historical decisions and accounts for unwanted correlation\nbetween protected and legitimate features. Through extensive experiments on\nsynthetic and real-world data, we show that our method is fair in the sense\nthat a) it assigns the desirable outcome to the most qualified individuals, and\nb) it removes the effect of stereotypes in decision-making, thereby\noutperforming traditional classification algorithms. Additionally, we are able\nto show theoretically that our method is consistent with a prominent concept of\nindividual fairness which states that \"similar individuals should be treated\nsimilarly.\"",
    "published_date": "2021-02-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.04565v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.04521v1",
    "title": "A study of text representations in Hate Speech Detection",
    "authors": [
      "Chrysoula Themeli",
      "George Giannakopoulos",
      "Nikiforos Pittaras"
    ],
    "author_ids": [],
    "abstract": "The pervasiveness of the Internet and social media have enabled the rapid and\nanonymous spread of Hate Speech content on microblogging platforms such as\nTwitter. Current EU and US legislation against hateful language, in conjunction\nwith the large amount of data produced in these platforms has led to automatic\ntools being a necessary component of the Hate Speech detection task and\npipeline. In this study, we examine the performance of several, diverse text\nrepresentation techniques paired with multiple classification algorithms, on\nthe automatic Hate Speech detection and abusive language discrimination task.\nWe perform an experimental evaluation on binary and multiclass datasets, paired\nwith significance testing. Our results show that simple hate-keyword frequency\nfeatures (BoW) work best, followed by pre-trained word embeddings (GLoVe) as\nwell as N-gram graphs (NGGs): a graph-based representation which proved to\nproduce efficient, very low-dimensional but rich features for this task. A\ncombination of these representations paired with Logistic Regression or 3-layer\nneural network classifiers achieved the best detection performance, in terms of\nmicro and macro F-measure.",
    "published_date": "2021-02-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.04521v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.04378v2",
    "title": "TransReID: Transformer-based Object Re-Identification",
    "authors": [
      "Shuting He",
      "Hao Luo",
      "Pichao Wang",
      "Fan Wang",
      "Hao Li",
      "Wei Jiang"
    ],
    "author_ids": [],
    "abstract": "Extracting robust feature representation is one of the key challenges in\nobject re-identification (ReID). Although convolution neural network\n(CNN)-based methods have achieved great success, they only process one local\nneighborhood at a time and suffer from information loss on details caused by\nconvolution and downsampling operators (e.g. pooling and strided convolution).\nTo overcome these limitations, we propose a pure transformer-based object ReID\nframework named TransReID. Specifically, we first encode an image as a sequence\nof patches and build a transformer-based strong baseline with a few critical\nimprovements, which achieves competitive results on several ReID benchmarks\nwith CNN-based methods. To further enhance the robust feature learning in the\ncontext of transformers, two novel modules are carefully designed. (i) The\njigsaw patch module (JPM) is proposed to rearrange the patch embeddings via\nshift and patch shuffle operations which generates robust features with\nimproved discrimination ability and more diversified coverage. (ii) The side\ninformation embeddings (SIE) is introduced to mitigate feature bias towards\ncamera/view variations by plugging in learnable embeddings to incorporate these\nnon-visual clues. To the best of our knowledge, this is the first work to adopt\na pure transformer for ReID research. Experimental results of TransReID are\nsuperior promising, which achieve state-of-the-art performance on both person\nand vehicle ReID benchmarks.",
    "published_date": "2021-02-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.04378v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.04342v1",
    "title": "The Limits of Computation in Solving Equity Trade-Offs in Machine Learning and Justice System Risk Assessment",
    "authors": [
      "Jesse Russell"
    ],
    "author_ids": [],
    "abstract": "This paper explores how different ideas of racial equity in machine learning,\nin justice settings in particular, can present trade-offs that are difficult to\nsolve computationally. Machine learning is often used in justice settings to\ncreate risk assessments, which are used to determine interventions, resources,\nand punitive actions. Overall aspects and performance of these machine\nlearning-based tools, such as distributions of scores, outcome rates by levels,\nand the frequency of false positives and true positives, can be problematic\nwhen examined by racial group. Models that produce different distributions of\nscores or produce a different relationship between level and outcome are\nproblematic when those scores and levels are directly linked to the restriction\nof individual liberty and to the broader context of racial inequity. While\ncomputation can help highlight these aspects, data and computation are unlikely\nto solve them. This paper explores where values and mission might have to fill\nthe spaces computation leaves.",
    "published_date": "2021-02-08T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.04342v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.04326v2",
    "title": "We might walk together, but I run faster: Network Fairness and Scalability in Blockchains",
    "authors": [
      "Anurag Jain",
      "Shoeb Siddiqui",
      "Sujit Gujar"
    ],
    "author_ids": [],
    "abstract": "Blockchain-based Distributed Ledgers (DLs) promise to transform the existing\nfinancial system by making it truly democratic. In the past decade, blockchain\ntechnology has seen many novel applications ranging from the banking industry\nto real estate. However, in order to be adopted universally, blockchain systems\nmust be scalable to support a high volume of transactions. As we increase the\nthroughput of the DL system, the underlying peer-to-peer network might face\nmultiple levels of challenges to keep up with the requirements. Due to varying\nnetwork capacities, the slower nodes would be at a relative disadvantage\ncompared to the faster ones, which could negatively impact their revenue. In\norder to quantify their relative advantage or disadvantage, we introduce two\nmeasures of network fairness, $p_f$, the probability of frontrunning and\n$\\alpha_f$, the publishing fairness. We show that as we scale the blockchain,\nboth these measures deteriorate, implying that the slower nodes face a\ndisadvantage at higher throughputs. It results in the faster nodes getting more\nthan their fair share of the reward while the slower nodes (slow in terms of\nnetwork quality) get less. Thus, fairness and scalability in blockchain systems\ndo not go hand in hand.\n  In a setting with rational miners, lack of fairness causes miners to deviate\nfrom the \"longest chain rule\" or undercut, which would reduce the blockchain's\nresilience against byzantine adversaries. Hence, fairness is not only a\ndesirable property for a blockchain system but also essential for the security\nof the blockchain and any scalable blockchain protocol proposed must ensure\nfairness.",
    "published_date": "2021-02-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DC",
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.04326v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.04130v3",
    "title": "Bias Out-of-the-Box: An Empirical Analysis of Intersectional Occupational Biases in Popular Generative Language Models",
    "authors": [
      "Hannah Kirk",
      "Yennie Jun",
      "Haider Iqbal",
      "Elias Benussi",
      "Filippo Volpin",
      "Frederic A. Dreyer",
      "Aleksandar Shtedritski",
      "Yuki M. Asano"
    ],
    "author_ids": [],
    "abstract": "The capabilities of natural language models trained on large-scale data have\nincreased immensely over the past few years. Open source libraries such as\nHuggingFace have made these models easily available and accessible. While prior\nresearch has identified biases in large language models, this paper considers\nbiases contained in the most popular versions of these models when applied\n`out-of-the-box' for downstream tasks. We focus on generative language models\nas they are well-suited for extracting biases inherited from training data.\nSpecifically, we conduct an in-depth analysis of GPT-2, which is the most\ndownloaded text generation model on HuggingFace, with over half a million\ndownloads per month. We assess biases related to occupational associations for\ndifferent protected categories by intersecting gender with religion, sexuality,\nethnicity, political affiliation, and continental name origin. Using a\ntemplate-based data collection pipeline, we collect 396K sentence completions\nmade by GPT-2 and find: (i) The machine-predicted jobs are less diverse and\nmore stereotypical for women than for men, especially for intersections; (ii)\nIntersectional interactions are highly relevant for occupational associations,\nwhich we quantify by fitting 262 logistic models; (iii) For most occupations,\nGPT-2 reflects the skewed gender and ethnicity distribution found in US Labor\nBureau data, and even pulls the societally-skewed distribution towards gender\nparity in cases where its predictions deviate from real labor market\nobservations. This raises the normative question of what language models should\nlearn - whether they should reflect or correct for existing inequalities.",
    "published_date": "2021-02-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.04130v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.04119v1",
    "title": "The FairCeptron: A Framework for Measuring Human Perceptions of Algorithmic Fairness",
    "authors": [
      "Georg Ahnert",
      "Ivan Smirnov",
      "Florian Lemmerich",
      "Claudia Wagner",
      "Markus Strohmaier"
    ],
    "author_ids": [],
    "abstract": "Measures of algorithmic fairness often do not account for human perceptions\nof fairness that can substantially vary between different sociodemographics and\nstakeholders. The FairCeptron framework is an approach for studying perceptions\nof fairness in algorithmic decision making such as in ranking or\nclassification. It supports (i) studying human perceptions of fairness and (ii)\ncomparing these human perceptions with measures of algorithmic fairness. The\nframework includes fairness scenario generation, fairness perception\nelicitation and fairness perception analysis. We demonstrate the FairCeptron\nframework by applying it to a hypothetical university admission context where\nwe collect human perceptions of fairness in the presence of minorities. An\nimplementation of the FairCeptron framework is openly available, and it can\neasily be adapted to study perceptions of algorithmic fairness in other\napplication contexts. We hope our work paves the way towards elevating the role\nof studies of human fairness perceptions in the process of designing\nalgorithmic decision making systems.",
    "published_date": "2021-02-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.04119v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.04016v1",
    "title": "An Efficient Framework for Zero-Shot Sketch-Based Image Retrieval",
    "authors": [
      "Osman Tursun",
      "Simon Denman",
      "Sridha Sridharan",
      "Ethan Goan",
      "Clinton Fookes"
    ],
    "author_ids": [],
    "abstract": "Recently, Zero-shot Sketch-based Image Retrieval (ZS-SBIR) has attracted the\nattention of the computer vision community due to it's real-world applications,\nand the more realistic and challenging setting than found in SBIR. ZS-SBIR\ninherits the main challenges of multiple computer vision problems including\ncontent-based Image Retrieval (CBIR), zero-shot learning and domain adaptation.\nThe majority of previous studies using deep neural networks have achieved\nimproved results through either projecting sketch and images into a common\nlow-dimensional space or transferring knowledge from seen to unseen classes.\nHowever, those approaches are trained with complex frameworks composed of\nmultiple deep convolutional neural networks (CNNs) and are dependent on\ncategory-level word labels. This increases the requirements on training\nresources and datasets. In comparison, we propose a simple and efficient\nframework that does not require high computational training resources, and can\nbe trained on datasets without semantic categorical labels. Furthermore, at\ntraining and inference stages our method only uses a single CNN. In this work,\na pre-trained ImageNet CNN (e.g., ResNet50) is fine-tuned with three proposed\nlearning objects: domain-aware quadruplet loss, semantic classification loss,\nand semantic knowledge preservation loss. The domain-aware quadruplet and\nsemantic classification losses are introduced to learn discriminative, semantic\nand domain invariant features through considering ZS-SBIR as object detection\nand verification problem. ...",
    "published_date": "2021-02-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.04016v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.03782v2",
    "title": "Using Gaussian Processes to Design Dynamic Experiments for Black-Box Model Discrimination under Uncertainty",
    "authors": [
      "Simon Olofsson",
      "Eduardo S. Schultz",
      "Adel Mhamdi",
      "Alexander Mitsos",
      "Marc Peter Deisenroth",
      "Ruth Misener"
    ],
    "author_ids": [],
    "abstract": "Diverse domains of science and engineering use parameterised mechanistic\nmodels. Engineers and scientists can often hypothesise several rival models to\nexplain a specific process or phenomenon. Consider a model discrimination\nsetting where we wish to find the best mechanistic, dynamic model candidate and\nthe best model parameter estimates. Typically, several rival mechanistic models\ncan explain the available data, so design of dynamic experiments for model\ndiscrimination helps optimally collect additional data by finding experimental\nsettings that maximise model prediction divergence. We argue there are two main\napproaches in the literature for solving the optimal design problem: (i) the\nanalytical approach, using linear and Gaussian approximations to find\nclosed-form expressions for the design objective, and (ii) the data-driven\napproach, which often relies on computationally intensive Monte Carlo\ntechniques. Olofsson et al. (ICML 35, 2018) introduced Gaussian process (GP)\nsurrogate models to hybridise the analytical and data-driven approaches, which\nallowed for computationally efficient design of experiments for discriminating\nbetween black-box models. In this study, we demonstrate that we can extend\nexisting methods for optimal design of dynamic experiments to incorporate a\nwider range of problem uncertainty. We also extend the Olofsson et al. (2018)\nmethod of using GP surrogate models for discriminating between dynamic\nblack-box models. We evaluate our approach on a well-known case study from\nliterature, and explore the consequences of using GP surrogates to approximate\ngradient-based methods.",
    "published_date": "2021-02-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.03782v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.03721v1",
    "title": "Fairness in ERC token markets: A Case Study of CryptoKitties",
    "authors": [
      "Kentaro Sako",
      "Shin'ichiro Matsuo",
      "Sachin Meier"
    ],
    "author_ids": [],
    "abstract": "Fairness is an important trait of open, free markets. Ethereum is a platform\nmeant to enable digital, decentralized markets. Though many researchers debate\nthe market's fairness, there are few discussions around the fairness of\nautomated markets, such as those hosted on Ethereum. In this paper, using pilot\nstudies, we consider unfair factors caused by adding the program. Because\nCryptoKitties is one of the major blockchain-based games and has been in\noperation for an extended period of time, we focus on its market to examine\nfairness. As a result, we concluded that a gene determination algorithm in this\ngame has little randomness, and a significant advantage to gain profit is given\nto players who know its bias over those who do not. We state incompleteness and\nimpact of the algorithm and other factors. Besides, we suppose countermeasures\nto reduce CryptoKitties' unfairness as a market.",
    "published_date": "2021-02-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.03721v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.03717v1",
    "title": "Assessing Fairness in Classification Parity of Machine Learning Models in Healthcare",
    "authors": [
      "Ming Yuan",
      "Vikas Kumar",
      "Muhammad Aurangzeb Ahmad",
      "Ankur Teredesai"
    ],
    "author_ids": [],
    "abstract": "Fairness in AI and machine learning systems has become a fundamental problem\nin the accountability of AI systems. While the need for accountability of AI\nmodels is near ubiquitous, healthcare in particular is a challenging field\nwhere accountability of such systems takes upon additional importance, as\ndecisions in healthcare can have life altering consequences. In this paper we\npresent preliminary results on fairness in the context of classification parity\nin healthcare. We also present some exploratory methods to improve fairness and\nchoosing appropriate classification algorithms in the context of healthcare.",
    "published_date": "2021-02-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.03717v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.03692v1",
    "title": "What's in a Name? -- Gender Classification of Names with Character Based Machine Learning Models",
    "authors": [
      "Yifan Hu",
      "Changwei Hu",
      "Thanh Tran",
      "Tejaswi Kasturi",
      "Elizabeth Joseph",
      "Matt Gillingham"
    ],
    "author_ids": [],
    "abstract": "Gender information is no longer a mandatory input when registering for an\naccount at many leading Internet companies. However, prediction of demographic\ninformation such as gender and age remains an important task, especially in\nintervention of unintentional gender/age bias in recommender systems. Therefore\nit is necessary to infer the gender of those users who did not to provide this\ninformation during registration. We consider the problem of predicting the\ngender of registered users based on their declared name. By analyzing the first\nnames of 100M+ users, we found that genders can be very effectively classified\nusing the composition of the name strings. We propose a number of character\nbased machine learning models, and demonstrate that our models are able to\ninfer the gender of users with much higher accuracy than baseline models.\nMoreover, we show that using the last names in addition to the first names\nimproves classification performance further.",
    "published_date": "2021-02-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.03692v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.03526v3",
    "title": "Open-World Semi-Supervised Learning",
    "authors": [
      "Kaidi Cao",
      "Maria Brbic",
      "Jure Leskovec"
    ],
    "author_ids": [],
    "abstract": "A fundamental limitation of applying semi-supervised learning in real-world\nsettings is the assumption that unlabeled test data contains only classes\npreviously encountered in the labeled training data. However, this assumption\nrarely holds for data in-the-wild, where instances belonging to novel classes\nmay appear at testing time. Here, we introduce a novel open-world\nsemi-supervised learning setting that formalizes the notion that novel classes\nmay appear in the unlabeled test data. In this novel setting, the goal is to\nsolve the class distribution mismatch between labeled and unlabeled data, where\nat the test time every input instance either needs to be classified into one of\nthe existing classes or a new unseen class needs to be initialized. To tackle\nthis challenging problem, we propose ORCA, an end-to-end deep learning approach\nthat introduces uncertainty adaptive margin mechanism to circumvent the bias\ntowards seen classes caused by learning discriminative features for seen\nclasses faster than for the novel classes. In this way, ORCA reduces the gap\nbetween intra-class variance of seen with respect to novel classes. Experiments\non image classification datasets and a single-cell annotation dataset\ndemonstrate that ORCA consistently outperforms alternative baselines, achieving\n25% improvement on seen and 96% improvement on novel classes of the ImageNet\ndataset.",
    "published_date": "2021-02-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.03526v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.03472v1",
    "title": "Overcoming Bias in Community Detection Evaluation",
    "authors": [
      "Jeancarlo Campos Leão",
      "Alberto H. F. Laender",
      "Pedro O. S. Vaz de Melo"
    ],
    "author_ids": [],
    "abstract": "Community detection is a key task to further understand the function and the\nstructure of complex networks. Therefore, a strategy used to assess this task\nmust be able to avoid biased and incorrect results that might invalidate\nfurther analyses or applications that rely on such communities. Two widely used\nstrategies to assess this task are generally known as structural and\nfunctional. The structural strategy basically consists in detecting and\nassessing such communities by using multiple methods and structural metrics. On\nthe other hand, the functional strategy might be used when ground truth data\nare available to assess the detected communities. However, the evaluation of\ncommunities based on such strategies is usually done in experimental\nconfigurations that are largely susceptible to biases, a situation that is\ninherent to algorithms, metrics and network data used in this task.\nFurthermore, such strategies are not systematically combined in a way that\nallows for the identification and mitigation of bias in the algorithms, metrics\nor network data to converge into more consistent results. In this context, the\nmain contribution of this article is an approach that supports a robust quality\nevaluation when detecting communities in real-world networks. In our approach,\nwe measure the quality of a community by applying the structural and functional\nstrategies, and the combination of both, to obtain different pieces of\nevidence. Then, we consider the divergences and the consensus among the pieces\nof evidence to identify and overcome possible sources of bias in community\ndetection algorithms, evaluation metrics, and network data. Experiments\nconducted with several real and synthetic networks provided results that show\nthe effectiveness of our approach to obtain more consistent conclusions about\nthe quality of the detected communities.",
    "published_date": "2021-02-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.03472v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.03461v1",
    "title": "Promoting Fair Proposers, Fair Responders or Both? Cost-Efficient Interference in the Spatial Ultimatum Game",
    "authors": [
      "Theodor Cimpeanu",
      "Cedric Perret",
      "The Anh Han"
    ],
    "author_ids": [],
    "abstract": "Institutions and investors face the constant challenge of making accurate\ndecisions and predictions regarding how best they should distribute their\nendowments. The problem of achieving an optimal outcome at minimal cost has\nbeen extensively studied and resolved using several heuristics. However, these\nworks usually fail to address how an external party can target different types\nof fair behaviour or do not take into account how limited information can shape\nthis complex interplay. Here, we consider the well-known Ultimatum game in a\nspatial setting and propose a hierarchy of interference mechanisms based on the\namount of information available to an external decision-maker and desired\nstandards of fairness. Our analysis reveals that monitoring the population at a\nmacroscopic level requires more strict information gathering in order to obtain\nan optimal outcome and that local observations can mediate this requirement.\nMoreover, we identify the conditions which must be met for an individual to be\neligible for investment in order to avoid unnecessary spending. We further\nexplore the effects of varying mutation or behavioural exploration rates on the\nchoice of investment strategy and total accumulated costs to the investor.\nOverall, our analysis provides new insights about efficient heuristics for\ncost-efficient promotion of fairness in societies. Finally, we discuss the\ndifferences between our findings and previous work done on the PD and present\nour suggestions for promoting fairness as an external decision-maker.",
    "published_date": "2021-02-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.MA",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.03461v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.03198v2",
    "title": "Bias-Variance Reduced Local SGD for Less Heterogeneous Federated Learning",
    "authors": [
      "Tomoya Murata",
      "Taiji Suzuki"
    ],
    "author_ids": [],
    "abstract": "Recently, local SGD has got much attention and been extensively studied in\nthe distributed learning community to overcome the communication bottleneck\nproblem. However, the superiority of local SGD to minibatch SGD only holds in\nquite limited situations. In this paper, we study a new local algorithm called\nBias-Variance Reduced Local SGD (BVR-L-SGD) for nonconvex distributed\noptimization. Algorithmically, our proposed bias and variance reduced local\ngradient estimator fully utilizes small second-order heterogeneity of local\nobjectives and suggests randomly picking up one of the local models instead of\ntaking the average of them when workers are synchronized. Theoretically, under\nsmall heterogeneity of local objectives, we show that BVR-L-SGD achieves better\ncommunication complexity than both the previous non-local and local methods\nunder mild conditions, and particularly BVR-L-SGD is the first method that\nbreaks the barrier of communication complexity $\\Theta(1/\\varepsilon)$ for\ngeneral nonconvex smooth objectives when the heterogeneity is small and the\nlocal computation budget is large. Numerical results are given to verify the\ntheoretical findings and give empirical evidence of the superiority of our\nmethod.",
    "published_date": "2021-02-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.03198v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.03176v2",
    "title": "Feature Representation in Deep Metric Embeddings",
    "authors": [
      "Ryan Furlong",
      "Vincent O'Brien",
      "James Garland",
      "Daniel Palacios-Alonso",
      "Francisco Dominguez-Mateos"
    ],
    "author_ids": [],
    "abstract": "In deep metric learning (DML), high-level input data are represented in a\nlower-level representation (embedding) space, such that samples from the same\nclass are mapped close together, while samples from disparate classes are\nmapped further apart. In this lower-level representation, only a single\ninference sample from each known class is required to discriminate between\nclasses accurately. The features a DML model uses to discriminate between\nclasses and the importance of each feature in the training process are unknown.\nTo investigate this, this study takes embeddings trained to discriminate faces\n(identities) and uses unsupervised clustering to identify the features involved\nin facial identity discrimination by examining their representation within the\nembedded space. This study is split into two cases; intra class\nsub-discrimination, where attributes that differ between a single identity are\nconsidered; such as beards and emotions; and extra class sub-discrimination,\nwhere attributes which differ between different identities/people, are\nconsidered; such as gender, skin tone and age. In the intra class scenario, the\ninference process distinguishes common attributes between single identities,\nachieving 90.0\\% and 76.0\\% accuracy for beards and glasses, respectively. The\nsystem can also perform extra class sub-discrimination with a high accuracy\nrate, notably 99.3\\%, 99.3\\% and 94.1\\% for gender, skin tone, and age,\nrespectively.",
    "published_date": "2021-02-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.03176v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.03129v3",
    "title": "Integer Programming for Causal Structure Learning in the Presence of Latent Variables",
    "authors": [
      "Rui Chen",
      "Sanjeeb Dash",
      "Tian Gao"
    ],
    "author_ids": [],
    "abstract": "The problem of finding an ancestral acyclic directed mixed graph (ADMG) that\nrepresents the causal relationships between a set of variables is an important\narea of research on causal inference. Most existing score-based structure\nlearning methods focus on learning directed acyclic graph (DAG) models without\nlatent variables. A number of score-based methods have recently been proposed\nfor the ADMG learning, yet they are heuristic in nature and do not guarantee an\noptimal solution. We propose a novel exact score-based method that solves an\ninteger programming (IP) formulation and returns a score-maximizing ancestral\nADMG for a set of continuous variables that follow a multivariate Gaussian\ndistribution. We generalize the state-of-the-art IP model for DAG learning\nproblems and derive new classes of valid inequalities to formulate an IP model\nfor ADMG learning. Empirically, our model can be solved efficiently for\nmedium-sized problems and achieves better accuracy than state-of-the-art\nscore-based methods as well as benchmark constraint-based methods.",
    "published_date": "2021-02-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.03129v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.03088v3",
    "title": "Boost AI Power: Data Augmentation Strategies with unlabelled Data and Conformal Prediction, a Case in Alternative Herbal Medicine Discrimination with Electronic Nose",
    "authors": [
      "Li Liu",
      "Xianghao Zhan",
      "Rumeng Wu",
      "Xiaoqing Guan",
      "Zhan Wang",
      "Wei Zhang",
      "Mert Pilanci",
      "You Wang",
      "Zhiyuan Luo",
      "Guang Li"
    ],
    "author_ids": [],
    "abstract": "Electronic nose has been proven to be effective in alternative herbal\nmedicine classification, but due to the nature of supervised learning, previous\nresearch heavily relies on the labelled training data, which are time-costly\nand labor-intensive to collect. To alleviate the critical dependency on the\ntraining data in real-world applications, this study aims to improve\nclassification accuracy via data augmentation strategies. The effectiveness of\nfive data augmentation strategies under different training data inadequacy are\ninvestigated in two scenarios: the noise-free scenario where different\navailabilities of unlabelled data were considered, and the noisy scenario where\ndifferent levels of Gaussian noises and translational shifts were added to\nrepresent sensor drifts. The five augmentation strategies, namely noise-adding\ndata augmentation, semi-supervised learning, classifier-based online learning,\nInductive Conformal Prediction (ICP) online learning and our novel ensemble ICP\nonline learning proposed in this study, are experimented and compared against\nsupervised learning baseline, with Linear Discriminant Analysis (LDA) and\nSupport Vector Machine (SVM) as the classifiers. Our novel strategy, ensemble\nICP online learning, outperforms the others by showing non-decreasing\nclassification accuracy on all tasks and a significant improvement on most\nsimulated tasks (25out of 36 tasks,p<=0.05). Furthermore, this study provides a\nsystematic analysis of different augmentation strategies. It shows at least one\nstrategy significantly improved the classification accuracy with LDA (p<=0.05)\nand non-decreasing classification accuracy with SVM in each task. In\nparticular, our proposed strategy demonstrated both effectiveness and\nrobustness in boosting the classification model generalizability, which can be\nemployed in other machine learning applications.",
    "published_date": "2021-02-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.03088v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.03054v1",
    "title": "Removing biased data to improve fairness and accuracy",
    "authors": [
      "Sahil Verma",
      "Michael Ernst",
      "Rene Just"
    ],
    "author_ids": [],
    "abstract": "Machine learning systems are often trained using data collected from\nhistorical decisions. If past decisions were biased, then automated systems\nthat learn from historical data will also be biased. We propose a black-box\napproach to identify and remove biased training data. Machine learning models\ntrained on such debiased data (a subset of the original training data) have low\nindividual discrimination, often 0%. These models also have greater accuracy\nand lower statistical disparity than models trained on the full historical\ndata. We evaluated our methodology in experiments using 6 real-world datasets.\nOur approach outperformed seven previous approaches in terms of individual\ndiscrimination and accuracy.",
    "published_date": "2021-02-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.03054v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.09343v1",
    "title": "AI Can Stop Mass Shootings, and More",
    "authors": [
      "Selmer Bringsjord",
      "Naveen Sundar Govindarajulu",
      "Michael Giancola"
    ],
    "author_ids": [],
    "abstract": "We propose to build directly upon our longstanding, prior r&d in AI/machine\nethics in order to attempt to make real the blue-sky idea of AI that can thwart\nmass shootings, by bringing to bear its ethical reasoning. The r&d in question\nis overtly and avowedly logicist in form, and since we are hardly the only ones\nwho have established a firm foundation in the attempt to imbue AI's with their\nown ethical sensibility, the pursuit of our proposal by those in different\nmethodological camps should, we believe, be considered as well. We seek herein\nto make our vision at least somewhat concrete by anchoring our exposition to\ntwo simulations, one in which the AI saves the lives of innocents by locking\nout a malevolent human's gun, and a second in which this malevolent agent is\nallowed by the AI to be neutralized by law enforcement. Along the way, some\nobjections are anticipated, and rebutted.",
    "published_date": "2021-02-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.09343v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.02981v2",
    "title": "Finite Sample Analysis of Minimax Offline Reinforcement Learning: Completeness, Fast Rates and First-Order Efficiency",
    "authors": [
      "Masatoshi Uehara",
      "Masaaki Imaizumi",
      "Nan Jiang",
      "Nathan Kallus",
      "Wen Sun",
      "Tengyang Xie"
    ],
    "author_ids": [],
    "abstract": "We offer a theoretical characterization of off-policy evaluation (OPE) in\nreinforcement learning using function approximation for marginal importance\nweights and $q$-functions when these are estimated using recent minimax\nmethods. Under various combinations of realizability and completeness\nassumptions, we show that the minimax approach enables us to achieve a fast\nrate of convergence for weights and quality functions, characterized by the\ncritical inequality \\citep{bartlett2005}. Based on this result, we analyze\nconvergence rates for OPE. In particular, we introduce novel alternative\ncompleteness conditions under which OPE is feasible and we present the first\nfinite-sample result with first-order efficiency in non-tabular environments,\ni.e., having the minimal coefficient in the leading term.",
    "published_date": "2021-02-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.02981v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.02928v1",
    "title": "Toward a Rational and Ethical Sociotechnical System of Autonomous Vehicles: A Novel Application of Multi-Criteria Decision Analysis",
    "authors": [
      "Veljko Dubljević",
      "George F. List",
      "Jovan Milojevich",
      "Nirav Ajmeri",
      "William Bauer",
      "Munindar P. Singh",
      "Eleni Bardaka",
      "Thomas Birkland",
      "Charles Edwards",
      "Roger Mayer",
      "Ioan Muntean",
      "Thomas Powers",
      "Hesham Rakha",
      "Vance Ricks",
      "M. Shoaib Samandar"
    ],
    "author_ids": [],
    "abstract": "The expansion of artificial intelligence (AI) and autonomous systems has\nshown the potential to generate enormous social good while also raising serious\nethical and safety concerns. AI technology is increasingly adopted in\ntransportation. A survey of various in-vehicle technologies found that\napproximately 64% of the respondents used a smartphone application to assist\nwith their travel. The top-used applications were navigation and real-time\ntraffic information systems. Among those who used smartphones during their\ncommutes, the top-used applications were navigation and entertainment. There is\na pressing need to address relevant social concerns to allow for the\ndevelopment of systems of intelligent agents that are informed and cognizant of\nethical standards. Doing so will facilitate the responsible integration of\nthese systems in society. To this end, we have applied Multi-Criteria Decision\nAnalysis (MCDA) to develop a formal Multi-Attribute Impact Assessment (MAIA)\nquestionnaire for examining the social and ethical issues associated with the\nuptake of AI. We have focused on the domain of autonomous vehicles (AVs)\nbecause of their imminent expansion. However, AVs could serve as a stand-in for\nany domain where intelligent, autonomous agents interact with humans, either on\nan individual level (e.g., pedestrians, passengers) or a societal level.",
    "published_date": "2021-02-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.02928v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.04293v1",
    "title": "High-level Approaches to Detect Malicious Political Activity on Twitter",
    "authors": [
      "Miguel Sozinho Ramalho"
    ],
    "author_ids": [],
    "abstract": "Our work represents another step into the detection and prevention of these\never-more present political manipulation efforts. We, therefore, start by\nfocusing on understanding what the state-of-the-art approaches lack -- since\nthe problem remains, this is a fair assumption. We find concerning issues\nwithin the current literature and follow a diverging path. Notably, by placing\nemphasis on using data features that are less susceptible to malicious\nmanipulation and also on looking for high-level approaches that avoid a\ngranularity level that is biased towards easy-to-spot and low impact cases.\n  We designed and implemented a framework -- Twitter Watch -- that performs\nstructured Twitter data collection, applying it to the Portuguese\nTwittersphere. We investigate a data snapshot taken on May 2020, with around 5\nmillion accounts and over 120 million tweets (this value has since increased to\nover 175 million). The analyzed time period stretches from August 2019 to May\n2020, with a focus on the Portuguese elections of October 6th, 2019. However,\nthe Covid-19 pandemic showed itself in our data, and we also delve into how it\naffected typical Twitter behavior.\n  We performed three main approaches: content-oriented, metadata-oriented, and\nnetwork interaction-oriented. We learn that Twitter's suspension patterns are\nnot adequate to the type of political trolling found in the Portuguese\nTwittersphere -- identified by this work and by an independent peer - nor to\nfake news posting accounts. We also surmised that the different types of\nmalicious accounts we independently gathered are very similar both in terms of\ncontent and interaction, through two distinct analysis, and are simultaneously\nvery distinct from regular accounts.",
    "published_date": "2021-02-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.04293v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.02841v1",
    "title": "Building Representative Corpora from Illiterate Communities: A Review of Challenges and Mitigation Strategies for Developing Countries",
    "authors": [
      "Stephanie Hirmer",
      "Alycia Leonard",
      "Josephine Tumwesige",
      "Costanza Conforti"
    ],
    "author_ids": [],
    "abstract": "Most well-established data collection methods currently adopted in NLP depend\non the assumption of speaker literacy. Consequently, the collected corpora\nlargely fail to represent swathes of the global population, which tend to be\nsome of the most vulnerable and marginalised people in society, and often live\nin rural developing areas. Such underrepresented groups are thus not only\nignored when making modeling and system design decisions, but also prevented\nfrom benefiting from development outcomes achieved through data-driven NLP.\nThis paper aims to address the under-representation of illiterate communities\nin NLP corpora: we identify potential biases and ethical issues that might\narise when collecting data from rural communities with high illiteracy rates in\nLow-Income Countries, and propose a set of practical mitigation strategies to\nhelp future work.",
    "published_date": "2021-02-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.02841v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.04255v1",
    "title": "AI Development for the Public Interest: From Abstraction Traps to Sociotechnical Risks",
    "authors": [
      "McKane Andrus",
      "Sarah Dean",
      "Thomas Krendl Gilbert",
      "Nathan Lambert",
      "Tom Zick"
    ],
    "author_ids": [],
    "abstract": "Despite interest in communicating ethical problems and social contexts within\nthe undergraduate curriculum to advance Public Interest Technology (PIT) goals,\ninterventions at the graduate level remain largely unexplored. This may be due\nto the conflicting ways through which distinct Artificial Intelligence (AI)\nresearch tracks conceive of their interface with social contexts. In this paper\nwe track the historical emergence of sociotechnical inquiry in three distinct\nsubfields of AI research: AI Safety, Fair Machine Learning (Fair ML) and\nHuman-in-the-Loop (HIL) Autonomy. We show that for each subfield, perceptions\nof PIT stem from the particular dangers faced by past integration of technical\nsystems within a normative social order. We further interrogate how these\nhistories dictate the response of each subfield to conceptual traps, as defined\nin the Science and Technology Studies literature. Finally, through a\ncomparative analysis of these currently siloed fields, we present a roadmap for\na unified approach to sociotechnical graduate pedagogy in AI.",
    "published_date": "2021-02-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.04255v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.02584v1",
    "title": "Human Values in Software Release Planning",
    "authors": [
      "Davoud Mougouei",
      "Aditya Ghose",
      "Hoa Dam",
      "David Powers"
    ],
    "author_ids": [],
    "abstract": "Software products have become an integral part of human lives, and therefore\nneed to account for human values such as privacy, fairness, and equality.\nIgnoring human values in software development leads to biases and violations of\nhuman values: racial biases in recidivism assessment and facial recognition\nsoftware are well-known examples of such issues. One of the most critical steps\nin software development is Software Release Planning (SRP), where decisions are\nmade about the presence or absence of the requirements (features) in the\nsoftware. Such decisions are primarily guided by the economic value of the\nrequirements, ignoring their impacts on a broader range of human values. That\nmay result in ignoring (selecting) requirements that positively (negatively)\nimpact human values, increasing the risk of value breaches in the software. To\naddress this, we have proposed an Integer Programming approach to considering\nhuman values in software release planning. In this regard, an Integer Linear\nProgramming (ILP) model has been proposed, that explicitly accounts for human\nvalues in finding an \"optimal\" subset of the requirements. The ILP model\nexploits the algebraic structure of fuzzy graphs to capture dependencies and\nconflicts among the values of the requirements.",
    "published_date": "2021-02-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.02584v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.02320v1",
    "title": "One Label, One Billion Faces: Usage and Consistency of Racial Categories in Computer Vision",
    "authors": [
      "Zaid Khan",
      "Yun Fu"
    ],
    "author_ids": [],
    "abstract": "Computer vision is widely deployed, has highly visible, society altering\napplications, and documented problems with bias and representation. Datasets\nare critical for benchmarking progress in fair computer vision, and often\nemploy broad racial categories as population groups for measuring group\nfairness. Similarly, diversity is often measured in computer vision datasets by\nascribing and counting categorical race labels. However, racial categories are\nill-defined, unstable temporally and geographically, and have a problematic\nhistory of scientific use. Although the racial categories used across datasets\nare superficially similar, the complexity of human race perception suggests the\nracial system encoded by one dataset may be substantially inconsistent with\nanother. Using the insight that a classifier can learn the racial system\nencoded by a dataset, we conduct an empirical study of computer vision datasets\nsupplying categorical race labels for face images to determine the\ncross-dataset consistency and generalization of racial categories. We find that\neach dataset encodes a substantially unique racial system, despite nominally\nequivalent racial categories, and some racial categories are systemically less\nconsistent than others across datasets. We find evidence that racial categories\nencode stereotypes, and exclude ethnic groups from categories on the basis of\nnonconformity to stereotypes. Representing a billion humans under one racial\ncategory may obscure disparities and create new ones by encoding stereotypes of\nracial systems. The difficulty of adequately converting the abstract concept of\nrace into a tool for measuring fairness underscores the need for a method more\nflexible and culturally aware than racial categories.",
    "published_date": "2021-02-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.02320v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.02279v1",
    "title": "Insiders and Outsiders in Research on Machine Learning and Society",
    "authors": [
      "Yu Tao",
      "Kush R. Varshney"
    ],
    "author_ids": [],
    "abstract": "A subset of machine learning research intersects with societal issues,\nincluding fairness, accountability and transparency, as well as the use of\nmachine learning for social good. In this work, we analyze the scholars\ncontributing to this research at the intersection of machine learning and\nsociety through the lens of the sociology of science. By analyzing the\nauthorship of all machine learning papers posted to arXiv, we show that\ncompared to researchers from overrepresented backgrounds (defined by gender and\nrace/ethnicity), researchers from underrepresented backgrounds are more likely\nto conduct research at this intersection than other kinds of machine learning\nresearch. This state of affairs leads to contention between two perspectives on\ninsiders and outsiders in the scientific enterprise: outsiders being those\noutside the group being studied, and outsiders being those who have not\nparticipated as researchers in an area historically. This contention manifests\nas an epistemic question on the validity of knowledge derived from lived\nexperience in machine learning research, and predicts boundary work that we see\nin a real-world example.",
    "published_date": "2021-02-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.02279v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.04256v1",
    "title": "Problematic Machine Behavior: A Systematic Literature Review of Algorithm Audits",
    "authors": [
      "Jack Bandy"
    ],
    "author_ids": [],
    "abstract": "While algorithm audits are growing rapidly in commonality and public\nimportance, relatively little scholarly work has gone toward synthesizing prior\nwork and strategizing future research in the area. This systematic literature\nreview aims to do just that, following PRISMA guidelines in a review of over\n500 English articles that yielded 62 algorithm audit studies. The studies are\nsynthesized and organized primarily by behavior (discrimination, distortion,\nexploitation, and misjudgement), with codes also provided for domain (e.g.\nsearch, vision, advertising, etc.), organization (e.g. Google, Facebook,\nAmazon, etc.), and audit method (e.g. sock puppet, direct scrape,\ncrowdsourcing, etc.). The review shows how previous audit studies have exposed\npublic-facing algorithms exhibiting problematic behavior, such as search\nalgorithms culpable of distortion and advertising algorithms culpable of\ndiscrimination. Based on the studies reviewed, it also suggests some behaviors\n(e.g. discrimination on the basis of intersectional identities), domains (e.g.\nadvertising algorithms), methods (e.g. code auditing), and organizations (e.g.\nTwitter, TikTok, LinkedIn) that call for future audit attention. The paper\nconcludes by offering the common ingredients of successful audits, and\ndiscussing algorithm auditing in the context of broader research working toward\nalgorithmic justice.",
    "published_date": "2021-02-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.04256v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.04257v3",
    "title": "Fairness for Unobserved Characteristics: Insights from Technological Impacts on Queer Communities",
    "authors": [
      "Nenad Tomasev",
      "Kevin R. McKee",
      "Jackie Kay",
      "Shakir Mohamed"
    ],
    "author_ids": [],
    "abstract": "Advances in algorithmic fairness have largely omitted sexual orientation and\ngender identity. We explore queer concerns in privacy, censorship, language,\nonline safety, health, and employment to study the positive and negative\neffects of artificial intelligence on queer communities. These issues\nunderscore the need for new directions in fairness research that take into\naccount a multiplicity of considerations, from privacy preservation, context\nsensitivity and process fairness, to an awareness of sociotechnical impact and\nthe increasingly important role of inclusive and participatory research\nprocesses. Most current approaches for algorithmic fairness assume that the\ntarget characteristics for fairness--frequently, race and legal gender--can be\nobserved or recorded. Sexual orientation and gender identity are prototypical\ninstances of unobserved characteristics, which are frequently missing, unknown\nor fundamentally unmeasurable. This paper highlights the importance of\ndeveloping new approaches for algorithmic fairness that break away from the\nprevailing assumption of observed characteristics.",
    "published_date": "2021-02-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.04257v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.02137v2",
    "title": "BeFair: Addressing Fairness in the Banking Sector",
    "authors": [
      "Alessandro Castelnovo",
      "Riccardo Crupi",
      "Giulia Del Gamba",
      "Greta Greco",
      "Aisha Naseer",
      "Daniele Regoli",
      "Beatriz San Miguel Gonzalez"
    ],
    "author_ids": [],
    "abstract": "Algorithmic bias mitigation has been one of the most difficult conundrums for\nthe data science community and Machine Learning (ML) experts. Over several\nyears, there have appeared enormous efforts in the field of fairness in ML.\nDespite the progress toward identifying biases and designing fair algorithms,\ntranslating them into the industry remains a major challenge. In this paper, we\npresent the initial results of an industrial open innovation project in the\nbanking sector: we propose a general roadmap for fairness in ML and the\nimplementation of a toolkit called BeFair that helps to identify and mitigate\nbias. Results show that training a model without explicit constraints may lead\nto bias exacerbation in the predictions.",
    "published_date": "2021-02-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.02137v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.02114v1",
    "title": "Detecting Bias in Transfer Learning Approaches for Text Classification",
    "authors": [
      "Irene Li"
    ],
    "author_ids": [],
    "abstract": "Classification is an essential and fundamental task in machine learning,\nplaying a cardinal role in the field of natural language processing (NLP) and\ncomputer vision (CV). In a supervised learning setting, labels are always\nneeded for the classification task. Especially for deep neural models, a large\namount of high-quality labeled data are required for training. However, when a\nnew domain comes out, it is usually hard or expensive to acquire the labels.\nTransfer learning could be an option to transfer the knowledge from a source\ndomain to a target domain. A challenge is that these two domains can be\ndifferent, either on the feature distribution, or the class distribution for\nthe nature of the samples. In this work, we evaluate some existing transfer\nlearning approaches on detecting the bias of imbalanced classes including\ntraditional and deep models. Besides, we propose an approach to bridge the gap\nof the domain class imbalance issue.",
    "published_date": "2021-02-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.02114v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.01995v3",
    "title": "Convergence Voting: From Pairwise Comparisons to Consensus",
    "authors": [
      "Gergei Bana",
      "Wojciech Jamroga",
      "David Naccache",
      "Peter Y. A. Ryan"
    ],
    "author_ids": [],
    "abstract": "An important aspect of AI design and ethics is to create systems that reflect\naggregate preferences of the society. To this end, the techniques of social\nchoice theory are often utilized. We propose a new social choice function\nmotivated by the PageRank algorithm. The function ranks voting options based on\nthe Condorcet graph of pairwise comparisons. To this end, we transform the\nCondorcet graph into a Markov chain whose stationary distribution provides the\nscores of the options. We show how the values in the stationary distribution\ncan be interpreted as quantified aggregate support for the voting options, to\nwhich the community of voters converges through an imaginary sequence of\nnegotiating steps. Because of that, we suggest the name \"convergence voting\"\nfor the new voting scheme, and \"negotiated community support\" for the resulting\nstationary allocation of scores.\n  Our social choice function can be viewed as a consensus voting method,\nsitting somewhere between Copeland and Borda. On the one hand, it does not\nnecessarily choose the Condorcet winner, as strong support from a part of the\nsociety can outweigh mediocre uniform support. On the other hand, the influence\nof unpopular candidates on the outcome is smaller than in the primary technique\nof consensus voting, i.e., the Borda count. We achieve that without having to\nintroduce an ad hoc weighting that some other methods do.",
    "published_date": "2021-02-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.01995v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.01979v1",
    "title": "Key Technology Considerations in Developing and Deploying Machine Learning Models in Clinical Radiology Practice",
    "authors": [
      "Viraj Kulkarni",
      "Manish Gawali",
      "Amit Kharat"
    ],
    "author_ids": [],
    "abstract": "The use of machine learning to develop intelligent software tools for\ninterpretation of radiology images has gained widespread attention in recent\nyears. The development, deployment, and eventual adoption of these models in\nclinical practice, however, remains fraught with challenges. In this paper, we\npropose a list of key considerations that machine learning researchers must\nrecognize and address to make their models accurate, robust, and usable in\npractice. Namely, we discuss: insufficient training data, decentralized\ndatasets, high cost of annotations, ambiguous ground truth, imbalance in class\nrepresentation, asymmetric misclassification costs, relevant performance\nmetrics, generalization of models to unseen datasets, model decay, adversarial\nattacks, explainability, fairness and bias, and clinical validation. We\ndescribe each consideration and identify techniques to address it. Although\nthese techniques have been discussed in prior research literature, by freshly\nexamining them in the context of medical imaging and compiling them in the form\nof a laundry list, we hope to make them more accessible to researchers,\nsoftware developers, radiologists, and other stakeholders.",
    "published_date": "2021-02-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.01979v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.01874v1",
    "title": "Learning to identify image manipulations in scientific publications",
    "authors": [
      "Ghazal Mazaheri",
      "Kevin Urrutia Avila",
      "Amit K. Roy-Chowdhury"
    ],
    "author_ids": [],
    "abstract": "Adherence to scientific community standards ensures objectivity, clarity,\nreproducibility, and helps prevent bias, fabrication, falsification, and\nplagiarism. To help scientific integrity officers and journal/publisher\nreviewers monitor if researchers stick with these standards, it is important to\nhave a solid procedure to detect duplication as one of the most frequent types\nof manipulation in scientific papers. Images in scientific papers are used to\nsupport the experimental description and the discussion of the findings.\nTherefore, in this work we focus on detecting the duplications in images as one\nof the most important parts of a scientific paper. We propose a framework that\ncombines image processing and deep learning methods to classify images in the\narticles as duplicated or unduplicated ones. We show that our method leads to a\n90% accuracy rate of detecting duplicated images, a ~ 13% improvement in\ndetection accuracy in comparison to other manipulation detection methods. We\nalso show how effective the pre-processing steps are by comparing our method to\nother state-of-art manipulation detectors which lack these steps.",
    "published_date": "2021-02-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.01874v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.01859v2",
    "title": "BiasFinder: Metamorphic Test Generation to Uncover Bias for Sentiment Analysis Systems",
    "authors": [
      "Muhammad Hilmi Asyrofi",
      "Zhou Yang",
      "Imam Nur Bani Yusuf",
      "Hong Jin Kang",
      "Ferdian Thung",
      "David Lo"
    ],
    "author_ids": [],
    "abstract": "Artificial Intelligence (AI) software systems, such as Sentiment Analysis\n(SA) systems, typically learn from large amounts of data that may reflect human\nbiases. Consequently, the machine learning model in such software systems may\nexhibit unintended demographic bias based on specific characteristics (e.g.,\ngender, occupation, country-of-origin, etc.). Such biases manifest in an SA\nsystem when it predicts a different sentiment for similar texts that differ\nonly in the characteristic of individuals described. Existing studies on\nrevealing bias in SA systems rely on the production of sentences from a small\nset of short, predefined templates.\n  To address this limitation, we present BisaFinder, an approach to discover\nbiased predictions in SA systems via metamorphic testing. A key feature of\nBisaFinder is the automatic curation of suitable templates based on the pieces\nof text from a large corpus, using various Natural Language Processing (NLP)\ntechniques to identify words that describe demographic characteristics. Next,\nBisaFinder instantiates new text from these templates by filling in\nplaceholders with words associated with a class of a characteristic (e.g.,\ngender-specific words such as female names, \"she\", \"her\"). These texts are used\nto tease out bias in an SA system. BisaFinder identifies a bias-uncovering test\ncase when it detects that the SA system exhibits demographic bias for a pair of\ntexts, i.e., it predicts a different sentiment for texts that differ only in\nwords associated with a different class (e.g., male vs. female) of a target\ncharacteristic (e.g., gender). Our empirical evaluation showed that BiasFinder\ncan effectively create a larger number of fluent and diverse test cases that\nuncover various biases in an SA system.",
    "published_date": "2021-02-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.01859v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.01811v1",
    "title": "The Ethical Implications of Shared Medical Decision Making without Providing Adequate Computational Support to the Care Provider and to the Patient",
    "authors": [
      "Yuval Shahar"
    ],
    "author_ids": [],
    "abstract": "There is a clear need to involve patients in medical decisions. However,\ncognitive psychological research has highlighted the cognitive limitations of\nhumans with respect to 1. Probabilistic assessment of the patient state and of\npotential outcomes of various decisions, 2. Elicitation of the patient utility\nfunction, and 3. Integration of the probabilistic knowledge and of patient\npreferences to determine the optimal strategy. Therefore, without adequate\ncomputational support, current shared decision models have severe ethical\ndeficiencies. An informed consent model unfairly transfers the responsibility\nto a patient who does not have the necessary knowledge, nor the integration\ncapability. A paternalistic model endows with exaggerated power a physician who\nmight not be aware of the patient preferences, is prone to multiple cognitive\nbiases, and whose computational integration capability is bounded. Recent\nprogress in Artificial Intelligence suggests adding a third agent: a computer,\nin all deliberative medical decisions: Non emergency medical decisions in which\nmore than one alternative exists, the patient preferences can be elicited, the\ntherapeutic alternatives might be influenced by these preferences, medical\nknowledge exists regarding the likelihood of the decision outcomes, and there\nis sufficient decision time. Ethical physicians should exploit computational\ndecision support technologies, neither making the decisions solely on their\nown, nor shirking their duty and shifting the responsibility to patients in the\nname of informed consent. The resulting three way (patient, care provider,\ncomputer) human machine model that we suggest emphasizes the patient\npreferences, the physician knowledge, and the computational integration of both\naspects, does not diminish the physician role, but rather brings out the best\nin human and machine.",
    "published_date": "2021-02-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.CY",
      "J.3; I.2.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.01811v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.01567v4",
    "title": "A Lyapunov Theory for Finite-Sample Guarantees of Asynchronous Q-Learning and TD-Learning Variants",
    "authors": [
      "Zaiwei Chen",
      "Siva Theja Maguluri",
      "Sanjay Shakkottai",
      "Karthikeyan Shanmugam"
    ],
    "author_ids": [],
    "abstract": "This paper develops an unified framework to study finite-sample convergence\nguarantees of a large class of value-based asynchronous reinforcement learning\n(RL) algorithms. We do this by first reformulating the RL algorithms as\n\\textit{Markovian Stochastic Approximation} (SA) algorithms to solve\nfixed-point equations. We then develop a Lyapunov analysis and derive\nmean-square error bounds on the convergence of the Markovian SA. Based on this\nresult, we establish finite-sample mean-square convergence bounds for\nasynchronous RL algorithms such as $Q$-learning, $n$-step TD, TD$(\\lambda)$,\nand off-policy TD algorithms including V-trace. As a by-product, by analyzing\nthe convergence bounds of $n$-step TD and TD$(\\lambda)$, we provide theoretical\ninsights into the bias-variance trade-off, i.e., efficiency of bootstrapping in\nRL. This was first posed as an open problem in (Sutton, 1999).",
    "published_date": "2021-02-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.01567v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.01404v1",
    "title": "Face Recognition Using $Sf_{3}CNN$ With Higher Feature Discrimination",
    "authors": [
      "Nayaneesh Kumar Mishra",
      "Satish Kumar Singh"
    ],
    "author_ids": [],
    "abstract": "With the advent of 2-dimensional Convolution Neural Networks (2D CNNs), the\nface recognition accuracy has reached above 99%. However, face recognition is\nstill a challenge in real world conditions. A video, instead of an image, as an\ninput can be more useful to solve the challenges of face recognition in real\nworld conditions. This is because a video provides more features than an image.\nHowever, 2D CNNs cannot take advantage of the temporal features present in the\nvideo. We therefore, propose a framework called $Sf_{3}CNN$ for face\nrecognition in videos. The $Sf_{3}CNN$ framework uses 3-dimensional Residual\nNetwork (3D Resnet) and A-Softmax loss for face recognition in videos. The use\nof 3D ResNet helps to capture both spatial and temporal features into one\ncompact feature map. However, the 3D CNN features must be highly discriminative\nfor efficient face recognition. The use of A-Softmax loss helps to extract\nhighly discriminative features from the video for face recognition. $Sf_{3}CNN$\nframework gives an increased accuracy of 99.10% on CVBL video database in\ncomparison to the previous 97% on the same database using 3D ResNets.",
    "published_date": "2021-02-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.01404v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.01287v1",
    "title": "Detection of Racial Bias from Physiological Responses",
    "authors": [
      "Fateme Nikseresht",
      "Runze Yan",
      "Rachel Lew",
      "Yingzheng Liu",
      "Rose M. Sebastian",
      "Afsaneh Doryab"
    ],
    "author_ids": [],
    "abstract": "Despite the evolution of norms and regulations to mitigate the harm from\nbiases, harmful discrimination linked to an individual's unconscious biases\npersists. Our goal is to better understand and detect the physiological and\nbehavioral indicators of implicit biases. This paper investigates whether we\ncan reliably detect racial bias from physiological responses, including heart\nrate, conductive skin response, skin temperature, and micro-body movements. We\nanalyzed data from 46 subjects whose physiological data was collected with\nEmpatica E4 wristband while taking an Implicit Association Test (IAT). Our\nmachine learning and statistical analysis show that implicit bias can be\npredicted from physiological signals with 76.1% accuracy. Our results also show\nthat the EDA signal associated with skin response has the strongest correlation\nwith racial bias and that there are significant differences between the values\nof EDA features for biased and unbiased participants.",
    "published_date": "2021-02-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.01287v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.01265v1",
    "title": "The Limits of Global Inclusion in AI Development",
    "authors": [
      "Alan Chan",
      "Chinasa T. Okolo",
      "Zachary Terner",
      "Angelina Wang"
    ],
    "author_ids": [],
    "abstract": "Those best-positioned to profit from the proliferation of artificial\nintelligence (AI) systems are those with the most economic power. Extant global\ninequality has motivated Western institutions to involve more diverse groups in\nthe development and application of AI systems, including hiring foreign labour\nand establishing extra-national data centers and laboratories. However, given\nboth the propensity of wealth to abet its own accumulation and the lack of\ncontextual knowledge in top-down AI solutions, we argue that more focus should\nbe placed on the redistribution of power, rather than just on including\nunderrepresented groups. Unless more is done to ensure that opportunities to\nlead AI development are distributed justly, the future may hold only AI systems\nwhich are unsuited to their conditions of application, and exacerbate\ninequality.",
    "published_date": "2021-02-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.01265v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.01203v3",
    "title": "Emergent Unfairness in Algorithmic Fairness-Accuracy Trade-Off Research",
    "authors": [
      "A. Feder Cooper",
      "Ellen Abrams"
    ],
    "author_ids": [],
    "abstract": "Across machine learning (ML) sub-disciplines, researchers make explicit\nmathematical assumptions in order to facilitate proof-writing. We note that,\nspecifically in the area of fairness-accuracy trade-off optimization\nscholarship, similar attention is not paid to the normative assumptions that\nground this approach. Such assumptions presume that 1) accuracy and fairness\nare in inherent opposition to one another, 2) strict notions of mathematical\nequality can adequately model fairness, 3) it is possible to measure the\naccuracy and fairness of decisions independent from historical context, and 4)\ncollecting more data on marginalized individuals is a reasonable solution to\nmitigate the effects of the trade-off. We argue that such assumptions, which\nare often left implicit and unexamined, lead to inconsistent conclusions: While\nthe intended goal of this work may be to improve the fairness of machine\nlearning models, these unexamined, implicit assumptions can in fact result in\nemergent unfairness. We conclude by suggesting a concrete path forward toward a\npotential resolution.",
    "published_date": "2021-02-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.01203v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.01196v1",
    "title": "Soliciting Stakeholders' Fairness Notions in Child Maltreatment Predictive Systems",
    "authors": [
      "Hao-Fei Cheng",
      "Logan Stapleton",
      "Ruiqi Wang",
      "Paige Bullock",
      "Alexandra Chouldechova",
      "Zhiwei Steven Wu",
      "Haiyi Zhu"
    ],
    "author_ids": [],
    "abstract": "Recent work in fair machine learning has proposed dozens of technical\ndefinitions of algorithmic fairness and methods for enforcing these\ndefinitions. However, we still lack an understanding of how to develop machine\nlearning systems with fairness criteria that reflect relevant stakeholders'\nnuanced viewpoints in real-world contexts. To address this gap, we propose a\nframework for eliciting stakeholders' subjective fairness notions. Combining a\nuser interface that allows stakeholders to examine the data and the algorithm's\npredictions with an interview protocol to probe stakeholders' thoughts while\nthey are interacting with the interface, we can identify stakeholders' fairness\nbeliefs and principles. We conduct a user study to evaluate our framework in\nthe setting of a child maltreatment predictive system. Our evaluations show\nthat the framework allows stakeholders to comprehensively convey their fairness\nviewpoints. We also discuss how our results can inform the design of predictive\nsystems.",
    "published_date": "2021-02-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.01196v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.01175v1",
    "title": "Understanding collective human movement dynamics during large-scale events using big geosocial data analytics",
    "authors": [
      "Junchuan Fan",
      "Kathleen Stewart"
    ],
    "author_ids": [],
    "abstract": "With the rapid advancement of information and communication technologies,\nmany researchers have adopted alternative data sources from private data\nvendors to study human movement dynamics in response to large-scale natural or\nsocietal events. Big geosocial data such as georeferenced tweets are publicly\navailable and dynamically evolving as real-world events are happening, making\nit more likely to capture the real-time sentiments and responses of\npopulations. However, precisely-geolocated geosocial data is scarce and biased\ntoward urban population centers. In this research, we developed a big geosocial\ndata analytical framework for extracting human movement dynamics in response to\nlarge-scale events from publicly available georeferenced tweets. The framework\nincludes a two-stage data collection module that collects data in a more\ntargeted fashion in order to mitigate the data scarcity issue of georeferenced\ntweets; in addition, a variable bandwidth kernel density estimation(VB-KDE)\napproach was adopted to fuse georeference information at different spatial\nscales, further augmenting the signals of human movement dynamics contained in\ngeoreferenced tweets. To correct for the sampling bias of georeferenced tweets,\nwe adjusted the number of tweets for different spatial units (e.g., county,\nstate) by population. To demonstrate the performance of the proposed analytic\nframework, we chose an astronomical event that occurred nationwide across the\nUnited States, i.e., the 2017 Great American Eclipse, as an example event and\nstudied the human movement dynamics in response to this event. However, this\nanalytic framework can easily be applied to other types of large-scale events\nsuch as hurricanes or earthquakes.",
    "published_date": "2021-02-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.SI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.01175v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.01020v1",
    "title": "Relational Consensus-Based Cooperative Task Allocation Management for IIoT-Health Networks",
    "authors": [
      "Carlos Pedroso",
      "Yan Uehara de Moraes",
      "Michele Nogueira",
      "Aldri Santos"
    ],
    "author_ids": [],
    "abstract": "IIoT services focused on industry-oriented services often require objects run\nmore than one task. IIoT objects poses the challenge of distributing and\nmanaging task allocation among them. The fairness of task allocation brings\nflexible network reconfiguration and maximizes the tasks to be performed.\nAlthough existing approaches optimize and manage the dynamics of objects, not\nall them consider both co-relationship between tasks and object capabilities\nand the distributed allocation over the cluster service. This paper introduces\nthe ACADIA mechanism for task allocation in IIoT networks in order to\ndistribute task among objects. It relies on relational consensus strategies to\nallocate tasks and similarity capabilities to determine which objects can play\nin accomplishing those tasks. Evaluation on NS-3 showed that ACADIA achieved\n98% of allocated tasks in an IIoT-Health considering all scenarios, average\nmore than 95% of clusters apt to performed tasks in a low response time, and\nachieved 50% more effectiveness in task allocation compared to the literature\nsolution CONTASKI.",
    "published_date": "2021-02-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.01020v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.00753v2",
    "title": "Quantum Fair Machine Learning",
    "authors": [
      "Elija Perrier"
    ],
    "author_ids": [],
    "abstract": "In this paper, we inaugurate the field of quantum fair machine learning. We\nundertake a comparative analysis of differences and similarities between\nclassical and quantum fair machine learning algorithms, specifying how the\nunique features of quantum computation alter measures, metrics and remediation\nstrategies when quantum algorithms are subject to fairness constraints. We\npresent the first results in quantum fair machine learning by demonstrating the\nuse of Grover's search algorithm to satisfy statistical parity constraints\nimposed on quantum algorithms. We provide lower-bounds on iterations needed to\nachieve such statistical parity within $\\epsilon$-tolerance. We extend\ncanonical Lipschitz-conditioned individual fairness criteria to the quantum\nsetting using quantum metrics. We examine the consequences for typical measures\nof fairness in machine learning context when quantum information processing and\nquantum data are involved. Finally, we propose open questions and research\nprogrammes for this new field of interest to researchers in computer science,\nethics and quantum computation.",
    "published_date": "2021-02-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "quant-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.00753v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.00650v1",
    "title": "Rethinking Soft Labels for Knowledge Distillation: A Bias-Variance Tradeoff Perspective",
    "authors": [
      "Helong Zhou",
      "Liangchen Song",
      "Jiajie Chen",
      "Ye Zhou",
      "Guoli Wang",
      "Junsong Yuan",
      "Qian Zhang"
    ],
    "author_ids": [],
    "abstract": "Knowledge distillation is an effective approach to leverage a well-trained\nnetwork or an ensemble of them, named as the teacher, to guide the training of\na student network. The outputs from the teacher network are used as soft labels\nfor supervising the training of a new network. Recent studies\n\\citep{muller2019does,yuan2020revisiting} revealed an intriguing property of\nthe soft labels that making labels soft serves as a good regularization to the\nstudent network. From the perspective of statistical learning, regularization\naims to reduce the variance, however how bias and variance change is not clear\nfor training with soft labels. In this paper, we investigate the bias-variance\ntradeoff brought by distillation with soft labels. Specifically, we observe\nthat during training the bias-variance tradeoff varies sample-wisely. Further,\nunder the same distillation temperature setting, we observe that the\ndistillation performance is negatively associated with the number of some\nspecific samples, which are named as regularization samples since these samples\nlead to bias increasing and variance decreasing. Nevertheless, we empirically\nfind that completely filtering out regularization samples also deteriorates\ndistillation performance. Our discoveries inspired us to propose the novel\nweighted soft labels to help the network adaptively handle the sample-wise\nbias-variance tradeoff. Experiments on standard evaluation benchmarks validate\nthe effectiveness of our method. Our code is available at\n\\url{https://github.com/bellymonster/Weighted-Soft-Label-Distillation}.",
    "published_date": "2021-02-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.00650v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.00464v1",
    "title": "Beyond the Command: Feminist STS Research and Critical Issues for the Design of Social Machines",
    "authors": [
      "Kelly B. Wagman",
      "Lisa Parks"
    ],
    "author_ids": [],
    "abstract": "Machines, from artificially intelligent digital assistants to embodied\nrobots, are becoming more pervasive in everyday life. Drawing on feminist\nscience and technology studies (STS) perspectives, we demonstrate how machine\ndesigners are not just crafting neutral objects, but relationships between\nmachines and humans that are entangled in human social issues such as gender\nand power dynamics. Thus, in order to create a more ethical and just future,\nthe dominant assumptions currently underpinning the design of these\nhuman-machine relations must be challenged and reoriented toward relations of\njustice and inclusivity. This paper contributes the \"social machine\" as a model\nfor technology designers who seek to recognize the importance, diversity and\ncomplexity of the social in their work, and to engage with the agential power\nof machines. In our model, the social machine is imagined as a potentially\nequitable relationship partner that has agency and as an \"other\" that is\ndistinct from, yet related to, humans, objects, and animals. We critically\nexamine and contrast our model with tendencies in robotics that consider robots\nas tools, human companions, animals or creatures, and/or slaves. In doing so,\nwe demonstrate ingrained dominant assumptions about human-machine relations and\nreveal the challenges of radical thinking in the social machine design space.\nFinally, we present two design challenges based on non-anthropomorphic\nfiguration and mutuality, and call for experimentation, unlearning dominant\ntendencies, and reimagining of sociotechnical futures.",
    "published_date": "2021-01-31T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC",
      "cs.CY",
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.00464v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.00417v1",
    "title": "Priority-based Post-Processing Bias Mitigation for Individual and Group Fairness",
    "authors": [
      "Pranay Lohia"
    ],
    "author_ids": [],
    "abstract": "Previous post-processing bias mitigation algorithms on both group and\nindividual fairness don't work on regression models and datasets with\nmulti-class numerical labels. We propose a priority-based post-processing bias\nmitigation on both group and individual fairness with the notion that similar\nindividuals should get similar outcomes irrespective of socio-economic factors\nand more the unfairness, more the injustice. We establish this proposition by a\ncase study on tariff allotment in a smart grid. Our novel framework establishes\nit by using a user segmentation algorithm to capture the consumption strategy\nbetter. This process ensures priority-based fair pricing for group and\nindividual facing the maximum injustice. It upholds the notion of fair tariff\nallotment to the entire population taken into consideration without modifying\nthe in-built process for tariff calculation. We also validate our method and\nshow superior performance to previous work on a real-world dataset in criminal\nsentencing.",
    "published_date": "2021-01-31T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.00417v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.00324v3",
    "title": "Video Reenactment as Inductive Bias for Content-Motion Disentanglement",
    "authors": [
      "Juan F. Hernández Albarracín",
      "Adín Ramírez Rivera"
    ],
    "author_ids": [],
    "abstract": "Independent components within low-dimensional representations are essential\ninputs in several downstream tasks, and provide explanations over the observed\ndata. Video-based disentangled factors of variation provide low-dimensional\nrepresentations that can be identified and used to feed task-specific models.\nWe introduce MTC-VAE, a self-supervised motion-transfer VAE model to\ndisentangle motion and content from videos. Unlike previous work on video\ncontent-motion disentanglement, we adopt a chunk-wise modeling approach and\ntake advantage of the motion information contained in spatiotemporal\nneighborhoods. Our model yields independent per-chunk representations that\npreserve temporal consistency. Hence, we reconstruct whole videos in a single\nforward-pass. We extend the ELBO's log-likelihood term and include a Blind\nReenactment Loss as an inductive bias to leverage motion disentanglement, under\nthe assumption that swapping motion features yields reenactment between two\nvideos. We evaluate our model with recently-proposed disentanglement metrics\nand show that it outperforms a variety of methods for video motion-content\ndisentanglement. Experiments on video reenactment show the effectiveness of our\ndisentanglement in the input space where our model outperforms the baselines in\nreconstruction quality and motion alignment.",
    "published_date": "2021-01-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.00324v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.00311v4",
    "title": "Fairness through Social Welfare Optimization",
    "authors": [
      "Violet Xinying Chen",
      "J. N. Hooker"
    ],
    "author_ids": [],
    "abstract": "We propose social welfare optimization as a general paradigm for formalizing\nfairness in AI systems. We argue that optimization models allow formulation of\na wide range of fairness criteria as social welfare functions, while enabling\nAI to take advantage of highly advanced solution technology. Rather than\nattempting to reduce bias between selected groups, one can achieve equity\nacross all groups by incorporating fairness into the social welfare function.\nThis also allows a fuller accounting of the welfare of the individuals\ninvolved. We show how to integrate social welfare optimization with both\nrule-based AI and machine learning, using either an in-processing or a\npost-processing approach. We present empirical results from a case study as a\npreliminary examination of the validity and potential of these integration\nstrategies.",
    "published_date": "2021-01-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.00311v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.00287v1",
    "title": "Machine Translationese: Effects of Algorithmic Bias on Linguistic Complexity in Machine Translation",
    "authors": [
      "Eva Vanmassenhove",
      "Dimitar Shterionov",
      "Matthew Gwilliam"
    ],
    "author_ids": [],
    "abstract": "Recent studies in the field of Machine Translation (MT) and Natural Language\nProcessing (NLP) have shown that existing models amplify biases observed in the\ntraining data. The amplification of biases in language technology has mainly\nbeen examined with respect to specific phenomena, such as gender bias. In this\nwork, we go beyond the study of gender in MT and investigate how bias\namplification might affect language in a broader sense. We hypothesize that the\n'algorithmic bias', i.e. an exacerbation of frequently observed patterns in\ncombination with a loss of less frequent ones, not only exacerbates societal\nbiases present in current datasets but could also lead to an artificially\nimpoverished language: 'machine translationese'. We assess the linguistic\nrichness (on a lexical and morphological level) of translations created by\ndifferent data-driven MT paradigms - phrase-based statistical (PB-SMT) and\nneural MT (NMT). Our experiments show that there is a loss of lexical and\nmorphological richness in the translations produced by all investigated MT\nparadigms for two language pairs (EN<=>FR and EN<=>ES).",
    "published_date": "2021-01-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.00287v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.04234v1",
    "title": "Computability, Complexity, Consistency and Controllability: A Four C's Framework for cross-disciplinary Ethical Algorithm Research",
    "authors": [
      "Elija Perrier"
    ],
    "author_ids": [],
    "abstract": "The ethical consequences of, constraints upon and regulation of algorithms\narguably represent the defining challenges of our age, asking us to reckon with\nthe rise of computational technologies whose potential to radically\ntransforming social and individual orders and identity in unforeseen ways is\nalready being realised. Yet despite the multidisciplinary impact of this\nalgorithmic turn, there remains some way to go in motivating the\ncrossdisciplinary collaboration that is crucial to advancing feasible proposals\nfor the ethical design, implementation and regulation of algorithmic and\nautomated systems. In this work, we provide a framework to assist\ncross-disciplinary collaboration by presenting a Four C's Framework covering\nkey computational considerations researchers across such diverse fields should\nconsider when approaching these questions: (i) computability, (ii) complexity,\n(iii) consistency and (iv) controllability. In addition, we provide examples of\nhow insights from ethics, philosophy and population ethics are relevant to and\ntranslatable within sciences concerned with the study and design of algorithms.\nOur aim is to set out a framework which we believe is useful for fostering\ncross-disciplinary understanding of pertinent issues in ethical algorithmic\nliterature which is relevant considering the feasibility of ethical algorithmic\ngovernance, especially the impact of computational constraints upon algorithmic\ngovernance.",
    "published_date": "2021-01-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.04234v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.00141v2",
    "title": "When the Umpire is also a Player: Bias in Private Label Product Recommendations on E-commerce Marketplaces",
    "authors": [
      "Abhisek Dash",
      "Abhijnan Chakraborty",
      "Saptarshi Ghosh",
      "Animesh Mukherjee",
      "Krishna P. Gummadi"
    ],
    "author_ids": [],
    "abstract": "Algorithmic recommendations mediate interactions between millions of\ncustomers and products (in turn, their producers and sellers) on large\ne-commerce marketplaces like Amazon. In recent years, the producers and sellers\nhave raised concerns about the fairness of black-box recommendation algorithms\ndeployed on these marketplaces. Many complaints are centered around\nmarketplaces biasing the algorithms to preferentially favor their own `private\nlabel' products over competitors. These concerns are exacerbated as\nmarketplaces increasingly de-emphasize or replace `organic' recommendations\nwith ad-driven `sponsored' recommendations, which include their own private\nlabels. While these concerns have been covered in popular press and have\nspawned regulatory investigations, to our knowledge, there has not been any\npublic audit of these marketplace algorithms. In this study, we bridge this gap\nby performing an end-to-end systematic audit of related item recommendations on\nAmazon. We propose a network-centric framework to quantify and compare the\nbiases across organic and sponsored related item recommendations. Along a\nnumber of our proposed bias measures, we find that the sponsored\nrecommendations are significantly more biased toward Amazon private label\nproducts compared to organic recommendations. While our findings are primarily\ninteresting to producers and sellers on Amazon, our proposed bias measures are\ngenerally useful for measuring link formation bias in any social or content\nnetworks.",
    "published_date": "2021-01-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.00141v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.00128v2",
    "title": "The effect of differential victim crime reporting on predictive policing systems",
    "authors": [
      "Nil-Jana Akpinar",
      "Maria De-Arteaga",
      "Alexandra Chouldechova"
    ],
    "author_ids": [],
    "abstract": "Police departments around the world have been experimenting with forms of\nplace-based data-driven proactive policing for over two decades. Modern\nincarnations of such systems are commonly known as hot spot predictive\npolicing. These systems predict where future crime is likely to concentrate\nsuch that police can allocate patrols to these areas and deter crime before it\noccurs. Previous research on fairness in predictive policing has concentrated\non the feedback loops which occur when models are trained on discovered crime\ndata, but has limited implications for models trained on victim crime reporting\ndata. We demonstrate how differential victim crime reporting rates across\ngeographical areas can lead to outcome disparities in common crime hot spot\nprediction models. Our analysis is based on a simulation patterned after\ndistrict-level victimization and crime reporting survey data for Bogot\\'a,\nColombia. Our results suggest that differential crime reporting rates can lead\nto a displacement of predicted hotspots from high crime but low reporting areas\nto high or medium crime and high reporting areas. This may lead to\nmisallocations both in the form of over-policing and under-policing.",
    "published_date": "2021-01-30T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.00128v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.00086v1",
    "title": "Challenges in Automated Debiasing for Toxic Language Detection",
    "authors": [
      "Xuhui Zhou",
      "Maarten Sap",
      "Swabha Swayamdipta",
      "Noah A. Smith",
      "Yejin Choi"
    ],
    "author_ids": [],
    "abstract": "Biased associations have been a challenge in the development of classifiers\nfor detecting toxic language, hindering both fairness and accuracy. As\npotential solutions, we investigate recently introduced debiasing methods for\ntext classification datasets and models, as applied to toxic language\ndetection. Our focus is on lexical (e.g., swear words, slurs, identity\nmentions) and dialectal markers (specifically African American English). Our\ncomprehensive experiments establish that existing methods are limited in their\nability to prevent biased behavior in current toxicity detectors. We then\npropose an automatic, dialect-aware data correction method, as a\nproof-of-concept. Despite the use of synthetic labels, this method reduces\ndialectal associations with toxicity. Overall, our findings show that debiasing\na model trained on biased toxic language data is not as effective as simply\nrelabeling the data to remove existing biases.",
    "published_date": "2021-01-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.00086v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.12715v3",
    "title": "Disparate Impact Diminishes Consumer Trust Even for Advantaged Users",
    "authors": [
      "Tim Draws",
      "Zoltán Szlávik",
      "Benjamin Timmermans",
      "Nava Tintarev",
      "Kush R. Varshney",
      "Michael Hind"
    ],
    "author_ids": [],
    "abstract": "Systems aiming to aid consumers in their decision-making (e.g., by\nimplementing persuasive techniques) are more likely to be effective when\nconsumers trust them. However, recent research has demonstrated that the\nmachine learning algorithms that often underlie such technology can act\nunfairly towards specific groups (e.g., by making more favorable predictions\nfor men than for women). An undesired disparate impact resulting from this kind\nof algorithmic unfairness could diminish consumer trust and thereby undermine\nthe purpose of the system. We studied this effect by conducting a\nbetween-subjects user study investigating how (gender-related) disparate impact\naffected consumer trust in an app designed to improve consumers' financial\ndecision-making. Our results show that disparate impact decreased consumers'\ntrust in the system and made them less likely to use it. Moreover, we find that\ntrust was affected to the same degree across consumer groups (i.e., advantaged\nand disadvantaged users) despite both of these consumer groups recognizing\ntheir respective levels of personal benefit. Our findings highlight the\nimportance of fairness in consumer-oriented artificial intelligence systems.",
    "published_date": "2021-01-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.12715v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.12701v1",
    "title": "Time for AI (Ethics) Maturity Model Is Now",
    "authors": [
      "Ville Vakkuri",
      "Marianna Jantunen",
      "Erika Halme",
      "Kai-Kristian Kemell",
      "Anh Nguyen-Duc",
      "Tommi Mikkonen",
      "Pekka Abrahamsson"
    ],
    "author_ids": [],
    "abstract": "There appears to be a common agreement that ethical concerns are of high\nimportance when it comes to systems equipped with some sort of Artificial\nIntelligence (AI). Demands for ethical AI are declared from all directions. As\na response, in recent years, public bodies, governments, and universities have\nrushed in to provide a set of principles to be considered when AI based systems\nare designed and used. We have learned, however, that high-level principles do\nnot turn easily into actionable advice for practitioners. Hence, also companies\nare publishing their own ethical guidelines to guide their AI development. This\npaper argues that AI software is still software and needs to be approached from\nthe software development perspective. The software engineering paradigm has\nintroduced maturity model thinking, which provides a roadmap for companies to\nimprove their performance from the selected viewpoints known as the key\ncapabilities. We want to voice out a call for action for the development of a\nmaturity model for AI software. We wish to discuss whether the focus should be\non AI ethics or, more broadly, the quality of an AI system, called a maturity\nmodel for the development of AI systems.",
    "published_date": "2021-01-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.12701v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.12677v2",
    "title": "Diminishing Domain Bias by Leveraging Domain Labels in Object Detection on UAVs",
    "authors": [
      "Benjamin Kiefer",
      "Martin Messmer",
      "Andreas Zell"
    ],
    "author_ids": [],
    "abstract": "Object detection from Unmanned Aerial Vehicles (UAVs) is of great importance\nin many aerial vision-based applications. Despite the great success of generic\nobject detection methods, a significant performance drop is observed when\napplied to images captured by UAVs. This is due to large variations in imaging\nconditions, such as varying altitudes, dynamically changing viewing angles, and\ndifferent capture times. These variations lead to domain imbalances and, thus,\ntrained models suffering from domain bias. We demonstrate that domain knowledge\nis a valuable source of information and thus propose domain-aware object\ndetectors by using freely accessible sensor data. By splitting the model into\ncross-domain and domain-specific parts, substantial performance improvements\nare achieved on multiple data sets across various models and metrics without\nchanging the architecture. In particular, we achieve a new state-of-the-art\nperformance on UAVDT for embedded real-time detectors. Furthermore, we create a\nnew airborne image data set by annotating 13,713 objects in 2,900 images\nfeaturing precise altitude and viewing angle annotations.",
    "published_date": "2021-01-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.12677v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.12476v1",
    "title": "Beyond traditional assumptions in fair machine learning",
    "authors": [
      "Niki Kilbertus"
    ],
    "author_ids": [],
    "abstract": "This thesis scrutinizes common assumptions underlying traditional machine\nlearning approaches to fairness in consequential decision making. After\nchallenging the validity of these assumptions in real-world applications, we\npropose ways to move forward when they are violated. First, we show that group\nfairness criteria purely based on statistical properties of observed data are\nfundamentally limited. Revisiting this limitation from a causal viewpoint we\ndevelop a more versatile conceptual framework, causal fairness criteria, and\nfirst algorithms to achieve them. We also provide tools to analyze how\nsensitive a believed-to-be causally fair algorithm is to misspecifications of\nthe causal graph. Second, we overcome the assumption that sensitive data is\nreadily available in practice. To this end we devise protocols based on secure\nmulti-party computation to train, validate, and contest fair decision\nalgorithms without requiring users to disclose their sensitive data or decision\nmakers to disclose their models. Finally, we also accommodate the fact that\noutcome labels are often only observed when a certain decision has been made.\nWe suggest a paradigm shift away from training predictive models towards\ndirectly learning decisions to relax the traditional assumption that labels can\nalways be recorded. The main contribution of this thesis is the development of\ntheoretically substantiated and practically feasible methods to move research\non fair machine learning closer to real-world applications.",
    "published_date": "2021-01-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.12476v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.12406v2",
    "title": "Fairness for Whom? Understanding the Reader's Perception of Fairness in Text Summarization",
    "authors": [
      "Anurag Shandilya",
      "Abhisek Dash",
      "Abhijnan Chakraborty",
      "Kripabandhu Ghosh",
      "Saptarshi Ghosh"
    ],
    "author_ids": [],
    "abstract": "With the surge in user-generated textual information, there has been a recent\nincrease in the use of summarization algorithms for providing an overview of\nthe extensive content. Traditional metrics for evaluation of these algorithms\n(e.g. ROUGE scores) rely on matching algorithmic summaries to human-generated\nones. However, it has been shown that when the textual contents are\nheterogeneous, e.g., when they come from different socially salient groups,\nmost existing summarization algorithms represent the social groups very\ndifferently compared to their distribution in the original data. To mitigate\nsuch adverse impacts, some fairness-preserving summarization algorithms have\nalso been proposed. All of these studies have considered normative notions of\nfairness from the perspective of writers of the contents, neglecting the\nreaders' perceptions of the underlying fairness notions. To bridge this gap, in\nthis work, we study the interplay between the fairness notions and how readers\nperceive them in textual summaries. Through our experiments, we show that\nreader's perception of fairness is often context-sensitive. Moreover, standard\nROUGE evaluation metrics are unable to quantify the perceived (un)fairness of\nthe summaries. To this end, we propose a human-in-the-loop metric and an\nautomated graph-based methodology to quantify the perceived bias in textual\nsummaries. We demonstrate their utility by quantifying the (un)fairness of\nseveral summaries of heterogeneous socio-political microblog datasets.",
    "published_date": "2021-01-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.12406v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.12403v1",
    "title": "Fair Resource Allocation for Demands with Sharp Lower Tail Inequalities",
    "authors": [
      "Vacharapat Mettanant",
      "Jittat Fakcharoenphol"
    ],
    "author_ids": [],
    "abstract": "We consider a fairness problem in resource allocation where multiple groups\ndemand resources from a common source with the total fixed amount. The general\nmodel was introduced by Elzayn et al. [FAT*'19]. We follow Donahue and\nKleinberg [FAT*'20] who considered the case when the demand distribution is\nknown. We show that for many common demand distributions that satisfy sharp\nlower tail inequalities, a natural allocation that provides resources\nproportional to each group's average demand performs very well. More\nspecifically, this natural allocation is approximately fair and efficient\n(i.e., it provides near maximum utilization). We also show that, when small\namount of unfairness is allowed, the Price of Fairness (PoF), in this case, is\nclose to 1.",
    "published_date": "2021-01-29T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.GT",
      "F.2.2; G.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.12403v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.12369v1",
    "title": "Information Theoretic Limits of Exact Recovery in Sub-hypergraph Models for Community Detection",
    "authors": [
      "Jiajun Liang",
      "Chuyang Ke",
      "Jean Honorio"
    ],
    "author_ids": [],
    "abstract": "In this paper, we study the information theoretic bounds for exact recovery\nin sub-hypergraph models for community detection. We define a general model\ncalled the $m-$uniform sub-hypergraph stochastic block model ($m-$ShSBM). Under\nthe $m-$ShSBM, we use Fano's inequality to identify the region of model\nparameters where any algorithm fails to exactly recover the planted communities\nwith a large probability. We also identify the region where a Maximum\nLikelihood Estimation (MLE) algorithm succeeds to exactly recover the\ncommunities with high probability. Our bounds are tight and pertain to the\ncommunity detection problems in various models such as the planted hypergraph\nstochastic block model, the planted densest sub-hypergraph model, and the\nplanted multipartite hypergraph model.",
    "published_date": "2021-01-29T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.12369v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.12190v2",
    "title": "Practical distributed quantum information processing with LOCCNet",
    "authors": [
      "Xuanqiang Zhao",
      "Benchi Zhao",
      "Zihe Wang",
      "Zhixin Song",
      "Xin Wang"
    ],
    "author_ids": [],
    "abstract": "Distributed quantum information processing is essential for building quantum\nnetworks and enabling more extensive quantum computations. In this regime,\nseveral spatially separated parties share a multipartite quantum system, and\nthe most natural set of operations is Local Operations and Classical\nCommunication (LOCC). As a pivotal part in quantum information theory and\npractice, LOCC has led to many vital protocols such as quantum teleportation.\nHowever, designing practical LOCC protocols is challenging due to LOCC's\nintractable structure and limitations set by near-term quantum devices. Here we\nintroduce LOCCNet, a machine learning framework facilitating protocol design\nand optimization for distributed quantum information processing tasks. As\napplications, we explore various quantum information tasks such as entanglement\ndistillation, quantum state discrimination, and quantum channel simulation. We\ndiscover protocols with evident improvements, in particular, for entanglement\ndistillation with quantum states of interest in quantum information. Our\napproach opens up new opportunities for exploring entanglement and its\napplications with machine learning, which will potentially sharpen our\nunderstanding of the power and limitations of LOCC. An implementation of\nLOCCNet is available in Paddle Quantum, a quantum machine learning Python\npackage based on PaddlePaddle deep learning platform.",
    "published_date": "2021-01-28T00:00:00",
    "year": 2021,
    "categories": [
      "quant-ph",
      "cond-mat.dis-nn",
      "cs.IT",
      "cs.LG",
      "hep-th",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.12190v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.11974v1",
    "title": "Disembodied Machine Learning: On the Illusion of Objectivity in NLP",
    "authors": [
      "Zeerak Waseem",
      "Smarika Lulz",
      "Joachim Bingel",
      "Isabelle Augenstein"
    ],
    "author_ids": [],
    "abstract": "Machine Learning seeks to identify and encode bodies of knowledge within\nprovided datasets. However, data encodes subjective content, which determines\nthe possible outcomes of the models trained on it. Because such subjectivity\nenables marginalisation of parts of society, it is termed (social) `bias' and\nsought to be removed. In this paper, we contextualise this discourse of bias in\nthe ML community against the subjective choices in the development process.\nThrough a consideration of how choices in data and model development construct\nsubjectivity, or biases that are represented in a model, we argue that\naddressing and mitigating biases is near-impossible. This is because both data\nand ML models are objects for which meaning is made in each step of the\ndevelopment pipeline, from data selection over annotation to model training and\nanalysis. Accordingly, we find the prevalent discourse of bias limiting in its\nability to address social marginalisation. We recommend to be conscientious of\nthis, and to accept that de-biasing methods only correct for a fraction of\nbiases.",
    "published_date": "2021-01-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.11974v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.11956v3",
    "title": "Us vs. Them: A Dataset of Populist Attitudes, News Bias and Emotions",
    "authors": [
      "Pere-Lluís Huguet-Cabot",
      "David Abadi",
      "Agneta Fischer",
      "Ekaterina Shutova"
    ],
    "author_ids": [],
    "abstract": "Computational modelling of political discourse tasks has become an\nincreasingly important area of research in natural language processing.\nPopulist rhetoric has risen across the political sphere in recent years;\nhowever, computational approaches to it have been scarce due to its complex\nnature. In this paper, we present the new $\\textit{Us vs. Them}$ dataset,\nconsisting of 6861 Reddit comments annotated for populist attitudes and the\nfirst large-scale computational models of this phenomenon. We investigate the\nrelationship between populist mindsets and social groups, as well as a range of\nemotions typically associated with these. We set a baseline for two tasks\nrelated to populist attitudes and present a set of multi-task learning models\nthat leverage and demonstrate the importance of emotion and group\nidentification as auxiliary tasks.",
    "published_date": "2021-01-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.11956v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.11915v1",
    "title": "Detecting Malicious Accounts showing Adversarial Behavior in Permissionless Blockchains",
    "authors": [
      "Rachit Agarwal",
      "Tanmay Thapliyal",
      "Sandeep K. Shukla"
    ],
    "author_ids": [],
    "abstract": "Different types of malicious activities have been flagged in multiple\npermissionless blockchains such as bitcoin, Ethereum etc. While some malicious\nactivities exploit vulnerabilities in the infrastructure of the blockchain,\nsome target its users through social engineering techniques. To address these\nproblems, we aim at automatically flagging blockchain accounts that originate\nsuch malicious exploitation of accounts of other participants. To that end, we\nidentify a robust supervised machine learning (ML) algorithm that is resistant\nto any bias induced by an over representation of certain malicious activity in\nthe available dataset, as well as is robust against adversarial attacks. We\nfind that most of the malicious activities reported thus far, for example, in\nEthereum blockchain ecosystem, behaves statistically similar. Further, the\npreviously used ML algorithms for identifying malicious accounts show bias\ntowards a particular malicious activity which is over-represented. In the\nsequel, we identify that Neural Networks (NN) holds up the best in the face of\nsuch bias inducing dataset at the same time being robust against certain\nadversarial attacks.",
    "published_date": "2021-01-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.11915v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.11775v2",
    "title": "Moral and Social Ramifications of Autonomous Vehicles",
    "authors": [
      "Veljko Dubljević",
      "Sean Douglas",
      "Jovan Milojevich",
      "Nirav Ajmeri",
      "William A. Bauer",
      "George F. List",
      "Munindar P. Singh"
    ],
    "author_ids": [],
    "abstract": "Autonomous Vehicles (AVs) raise important social and ethical concerns,\nespecially about accountability, dignity, and justice. We focus on the specific\nconcerns arising from how AV technology will affect the lives and livelihoods\nof professional and semi-professional drivers. Whereas previous studies of such\nconcerns have focused on the opinions of experts, we seek to understand these\nethical and societal challenges from the perspectives of the drivers\nthemselves.\n  To this end, we adopted a qualitative research methodology based on\nsemi-structured interviews. This is an established social science methodology\nthat helps understand the core concerns of stakeholders in depth by avoiding\nthe biases of superficial methods such as surveys.\n  We find that whereas drivers agree with the experts that AVs will\nsignificantly impact transportation systems, they are apprehensive about the\nprospects for their livelihoods and dismiss the suggestions that driving jobs\nare unsatisfying and their profession does not merit protection.\n  By showing how drivers differ from the experts, our study has ramifications\nbeyond AVs to AI and other advanced technologies. Our findings suggest that\nqualitative research applied to the relevant, especially disempowered,\nstakeholders is essential to ensuring that new technologies are introduced\nethically.",
    "published_date": "2021-01-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.11775v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.11753v1",
    "title": "ProtoDA: Efficient Transfer Learning for Few-Shot Intent Classification",
    "authors": [
      "Manoj Kumar",
      "Varun Kumar",
      "Hadrien Glaude",
      "Cyprien delichy",
      "Aman Alok",
      "Rahul Gupta"
    ],
    "author_ids": [],
    "abstract": "Practical sequence classification tasks in natural language processing often\nsuffer from low training data availability for target classes. Recent works\ntowards mitigating this problem have focused on transfer learning using\nembeddings pre-trained on often unrelated tasks, for instance, language\nmodeling. We adopt an alternative approach by transfer learning on an ensemble\nof related tasks using prototypical networks under the meta-learning paradigm.\nUsing intent classification as a case study, we demonstrate that increasing\nvariability in training tasks can significantly improve classification\nperformance. Further, we apply data augmentation in conjunction with\nmeta-learning to reduce sampling bias. We make use of a conditional generator\nfor data augmentation that is trained directly using the meta-learning\nobjective and simultaneously with prototypical networks, hence ensuring that\ndata augmentation is customized to the task. We explore augmentation in the\nsentence embedding space as well as prototypical embedding space. Combining\nmeta-learning with augmentation provides upto 6.49% and 8.53% relative F1-score\nimprovements over the best performing systems in the 5-shot and 10-shot\nlearning, respectively.",
    "published_date": "2021-01-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.11753v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.11750v2",
    "title": "Information contraction in noisy binary neural networks and its implications",
    "authors": [
      "Chuteng Zhou",
      "Quntao Zhuang",
      "Matthew Mattina",
      "Paul N. Whatmough"
    ],
    "author_ids": [],
    "abstract": "Neural networks have gained importance as the machine learning models that\nachieve state-of-the-art performance on large-scale image classification,\nobject detection and natural language processing tasks. In this paper, we\nconsider noisy binary neural networks, where each neuron has a non-zero\nprobability of producing an incorrect output. These noisy models may arise from\nbiological, physical and electronic contexts and constitute an important class\nof models that are relevant to the physical world. Intuitively, the number of\nneurons in such systems has to grow to compensate for the noise while\nmaintaining the same level of expressive power and computation reliability. Our\nkey finding is a lower bound for the required number of neurons in noisy neural\nnetworks, which is first of its kind. To prove this lower bound, we take an\ninformation theoretic approach and obtain a novel strong data processing\ninequality (SDPI), which not only generalizes the Evans-Schulman results for\nbinary symmetric channels to general channels, but also improves the tightness\ndrastically when applied to estimate end-to-end information contraction in\nnetworks. Our SDPI can be applied to various information processing systems,\nincluding neural networks and cellular automata. Applying the SDPI in noisy\nbinary neural networks, we obtain our key lower bound and investigate its\nimplications on network depth-width trade-offs, our results suggest a\ndepth-width trade-off for noisy neural networks that is very different from the\nestablished understanding regarding noiseless neural networks. Furthermore, we\napply the SDPI to study fault-tolerant cellular automata and obtain bounds on\nthe error correction overheads and the relaxation time. This paper offers new\nunderstanding of noisy information processing systems through the lens of\ninformation theory.",
    "published_date": "2021-01-28T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IT",
      "cs.AI",
      "cs.CC",
      "cs.LG",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.11750v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.11718v1",
    "title": "BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation",
    "authors": [
      "Jwala Dhamala",
      "Tony Sun",
      "Varun Kumar",
      "Satyapriya Krishna",
      "Yada Pruksachatkun",
      "Kai-Wei Chang",
      "Rahul Gupta"
    ],
    "author_ids": [],
    "abstract": "Recent advances in deep learning techniques have enabled machines to generate\ncohesive open-ended text when prompted with a sequence of words as context.\nWhile these models now empower many downstream applications from conversation\nbots to automatic storytelling, they have been shown to generate texts that\nexhibit social biases. To systematically study and benchmark social biases in\nopen-ended language generation, we introduce the Bias in Open-Ended Language\nGeneration Dataset (BOLD), a large-scale dataset that consists of 23,679\nEnglish text generation prompts for bias benchmarking across five domains:\nprofession, gender, race, religion, and political ideology. We also propose new\nautomated metrics for toxicity, psycholinguistic norms, and text gender\npolarity to measure social biases in open-ended text generation from multiple\nangles. An examination of text generated from three popular language models\nreveals that the majority of these models exhibit a larger social bias than\nhuman-written Wikipedia text across all domains. With these results we\nhighlight the need to benchmark biases in open-ended language generation and\ncaution users of language generation models on downstream tasks to be cognizant\nof these embedded prejudices.",
    "published_date": "2021-01-27T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.11718v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.11665v2",
    "title": "On Statistical Bias In Active Learning: How and When To Fix It",
    "authors": [
      "Sebastian Farquhar",
      "Yarin Gal",
      "Tom Rainforth"
    ],
    "author_ids": [],
    "abstract": "Active learning is a powerful tool when labelling data is expensive, but it\nintroduces a bias because the training data no longer follows the population\ndistribution. We formalize this bias and investigate the situations in which it\ncan be harmful and sometimes even helpful. We further introduce novel\ncorrective weights to remove bias when doing so is beneficial. Through this,\nour work not only provides a useful mechanism that can improve the active\nlearning approach, but also an explanation of the empirical successes of\nvarious existing approaches which ignore this bias. In particular, we show that\nthis bias can be actively helpful when training overparameterized models --\nlike neural networks -- with relatively little data.",
    "published_date": "2021-01-27T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.11665v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.11496v2",
    "title": "A Balance for Fairness: Fair Distribution Utilising Physics in Games of Characteristic Function Form",
    "authors": [
      "Song-Ju Kim",
      "Taiki Takahashi",
      "Kazuo Sano"
    ],
    "author_ids": [],
    "abstract": "In chaotic modern society, there is an increasing demand for the realization\nof true 'fairness'. In Greek mythology, Themis, the 'goddess of justice', has a\nsword in her right hand to protect society from vices, and a 'balance of\njudgment' in her left hand that measures good and evil. In this study, we\npropose a fair distribution method 'utilising physics' for the profit in games\nof characteristic function form. Specifically, we show that the linear\nprogramming problem for calculating 'nucleolus' can be efficiently solved by\nconsidering it as a physical system in which gravity works. In addition to\nbeing able to significantly reduce computational complexity thereby, we believe\nthat this system could have flexibility necessary to respond to real-time\nchanges in the parameter.",
    "published_date": "2021-01-27T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT",
      "cs.AI",
      "econ.GN",
      "physics.soc-ph",
      "q-fin.EC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.11496v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.11358v1",
    "title": "Detecting discriminatory risk through data annotation based on Bayesian inferences",
    "authors": [
      "Elena Beretta",
      "Antonio Vetrò",
      "Bruno Lepri",
      "Juan Carlos De Martin"
    ],
    "author_ids": [],
    "abstract": "Thanks to the increasing growth of computational power and data availability,\nthe research in machine learning has advanced with tremendous rapidity.\nNowadays, the majority of automatic decision making systems are based on data.\nHowever, it is well known that machine learning systems can present problematic\nresults if they are built on partial or incomplete data. In fact, in recent\nyears several studies have found a convergence of issues related to the ethics\nand transparency of these systems in the process of data collection and how\nthey are recorded. Although the process of rigorous data collection and\nanalysis is fundamental in the model design, this step is still largely\noverlooked by the machine learning community. For this reason, we propose a\nmethod of data annotation based on Bayesian statistical inference that aims to\nwarn about the risk of discriminatory results of a given data set. In\nparticular, our method aims to deepen knowledge and promote awareness about the\nsampling practices employed to create the training set, highlighting that the\nprobability of success or failure conditioned to a minority membership is given\nby the structure of the data available. We empirically test our system on three\ndatasets commonly accessed by the machine learning community and we investigate\nthe risk of racial discrimination.",
    "published_date": "2021-01-27T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.CY",
      "68P99 (Primary), 68T01 (Secondary)"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.11358v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.11269v1",
    "title": "On asymptotic fairness in voting with greedy sampling",
    "authors": [
      "Abraham Gutierrez",
      "Sebastian Müller",
      "Stjepan Šebek"
    ],
    "author_ids": [],
    "abstract": "The basic idea of voting protocols is that nodes query a sample of other\nnodes and adjust their own opinion throughout several rounds based on the\nproportion of the sampled opinions. In the classic model, it is assumed that\nall nodes have the same weight. We study voting protocols for heterogeneous\nweights with respect to fairness. A voting protocol is fair if the influence on\nthe eventual outcome of a given participant is linear in its weight. Previous\nwork used sampling with replacement to construct a fair voting scheme. However,\nit was shown that using greedy sampling, i.e., sampling with replacement until\na given number of distinct elements is chosen, turns out to be more robust and\nperformant.\n  In this paper, we study fairness of voting protocols with greedy sampling and\npropose a voting scheme that is asymptotically fair for a broad class of weight\ndistributions. We complement our theoretical findings with numerical results\nand present several open questions and conjectures.",
    "published_date": "2021-01-27T00:00:00",
    "year": 2021,
    "categories": [
      "math.PR",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.11269v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2101.11253v4",
    "title": "Puzzle-CAM: Improved localization via matching partial and full features",
    "authors": [
      "Sanghyun Jo",
      "In-Jae Yu"
    ],
    "author_ids": [],
    "abstract": "Weakly-supervised semantic segmentation (WSSS) is introduced to narrow the\ngap for semantic segmentation performance from pixel-level supervision to\nimage-level supervision. Most advanced approaches are based on class activation\nmaps (CAMs) to generate pseudo-labels to train the segmentation network. The\nmain limitation of WSSS is that the process of generating pseudo-labels from\nCAMs that use an image classifier is mainly focused on the most discriminative\nparts of the objects. To address this issue, we propose Puzzle-CAM, a process\nthat minimizes differences between the features from separate patches and the\nwhole image. Our method consists of a puzzle module and two regularization\nterms to discover the most integrated region in an object. Puzzle-CAM can\nactivate the overall region of an object using image-level supervision without\nrequiring extra parameters. % In experiments, Puzzle-CAM outperformed previous\nstate-of-the-art methods using the same labels for supervision on the PASCAL\nVOC 2012 test dataset. In experiments, Puzzle-CAM outperformed previous\nstate-of-the-art methods using the same labels for supervision on the PASCAL\nVOC 2012 dataset. Code associated with our experiments is available at\nhttps://github.com/OFRIN/PuzzleCAM.",
    "published_date": "2021-01-27T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.11253v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.11054v2",
    "title": "Remote Learners, Home Makers: How Digital Fabrication Was Taught Online During a Pandemic",
    "authors": [
      "Gabrielle Benabdallah",
      "Samuelle Bourgault",
      "Nadya Peek",
      "Jennifer Jacobs"
    ],
    "author_ids": [],
    "abstract": "Digital fabrication courses that relied on physical makerspaces were severely\ndisrupted by COVID-19. As universities shut down in Spring 2020, instructors\ndeveloped new models for digital fabrication at a distance. Through interviews\nwith faculty and students and examination of course materials, we recount the\nexperiences of eight remote digital fabrication courses. We found that learning\nwith hobbyist equipment and online social networks could emulate using\nindustrial equipment in shared workshops. Furthermore, at-home digital\nfabrication offered unique learning opportunities including more iteration,\nmachine tuning, and maintenance. These opportunities depended on new forms of\nlabor and varied based on student living situations. Our findings have\nimplications for remote and in-person digital fabrication instruction. They\nindicate how access to tools was important, but not as critical as providing\nopportunities for iteration; they show how remote fabrication exacerbated\nstudent inequities; and they suggest strategies for evaluating trade-offs in\nremote fabrication models with respect to learning objectives.",
    "published_date": "2021-01-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.11054v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2102.04215v1",
    "title": "The Moral Consideration of Artificial Entities: A Literature Review",
    "authors": [
      "Jamie Harris",
      "Jacy Reese Anthis"
    ],
    "author_ids": [],
    "abstract": "Ethicists, policy-makers, and the general public have questioned whether\nartificial entities such as robots warrant rights or other forms of moral\nconsideration. There is little synthesis of the research on this topic so far.\nWe identify 294 relevant research or discussion items in our literature review\nof this topic. There is widespread agreement among scholars that some\nartificial entities could warrant moral consideration in the future, if not\nalso the present. The reasoning varies, such as concern for the effects on\nartificial entities and concern for the effects on human society. Beyond the\nconventional consequentialist, deontological, and virtue ethicist ethical\nframeworks, some scholars encourage \"information ethics\" and\n\"social-relational\" approaches, though there are opportunities for more\nin-depth ethical research on the nuances of moral consideration of artificial\nentities. There is limited relevant empirical data collection, primarily in a\nfew psychological studies on current moral and social attitudes of humans\ntowards robots and other artificial entities. This suggests an important gap\nfor social science research on how artificial entities will be integrated into\nsociety and the factors that will determine how the interests of sentient\nartificial entities are considered.",
    "published_date": "2021-01-26T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "J.4; K.4.1; K.4.2; I.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.04215v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2101.10367v1",
    "title": "Modeling Assumptions Clash with the Real World: Transparency, Equity, and Community Challenges for Student Assignment Algorithms",
    "authors": [
      "Samantha Robertson",
      "Tonya Nguyen",
      "Niloufar Salehi"
    ],
    "author_ids": [],
    "abstract": "Across the United States, a growing number of school districts are turning to\nmatching algorithms to assign students to public schools. The designers of\nthese algorithms aimed to promote values such as transparency, equity, and\ncommunity in the process. However, school districts have encountered practical\nchallenges in their deployment. In fact, San Francisco Unified School District\nvoted to stop using and completely redesign their student assignment algorithm\nbecause it was not promoting educational equity in practice. We analyze this\nsystem using a Value Sensitive Design approach and find that one reason values\nare not met in practice is that the system relies on modeling assumptions about\nfamilies' priorities, constraints, and goals that clash with the real world.\nThese assumptions overlook the complex barriers to ideal participation that\nmany families face, particularly because of socioeconomic inequalities. We\nargue that direct, ongoing engagement with stakeholders is central to aligning\nalgorithmic values with real world conditions. In doing so we must broaden how\nwe evaluate algorithms while recognizing the limitations of purely algorithmic\nsolutions in addressing complex socio-political problems.",
    "published_date": "2021-01-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC",
      "cs.CY",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.10367v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2101.10253v1",
    "title": "The emergence of visual semantics through communication games",
    "authors": [
      "Daniela Mihai",
      "Jonathon Hare"
    ],
    "author_ids": [],
    "abstract": "The emergence of communication systems between agents which learn to play\nreferential signalling games with realistic images has attracted a lot of\nattention recently. The majority of work has focused on using fixed, pretrained\nimage feature extraction networks which potentially bias the information the\nagents learn to communicate. In this work, we consider a signalling game\nsetting in which a `sender' agent must communicate the information about an\nimage to a `receiver' who must select the correct image from many distractors.\nWe investigate the effect of the feature extractor's weights and of the task\nbeing solved on the visual semantics learned by the models. We first\ndemonstrate to what extent the use of pretrained feature extraction networks\ninductively bias the visual semantics conveyed by emergent communication\nchannel and quantify the visual semantics that are induced.\n  We then go on to explore ways in which inductive biases can be introduced to\nencourage the emergence of semantically meaningful communication without the\nneed for any form of supervised pretraining of the visual feature extractor. We\nimpose various augmentations to the input images and additional tasks in the\ngame with the aim to induce visual representations which capture conceptual\nproperties of images. Through our experiments, we demonstrate that\ncommunication systems which capture visual semantics can be learned in a\ncompletely self-supervised manner by playing the right types of game. Our work\nbridges a gap between emergent communication research and self-supervised\nfeature learning.",
    "published_date": "2021-01-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.10253v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.04211v4",
    "title": "Challenging Social Media Threats using Collective Well-being Aware Recommendation Algorithms and an Educational Virtual Companion",
    "authors": [
      "Dimitri Ognibene",
      "Davide Taibi",
      "Udo Kruschwitz",
      "Rodrigo Souza Wilkens",
      "Davinia Hernandez-Leo",
      "Emily Theophilou",
      "Lidia Scifo",
      "Rene Alejandro Lobo",
      "Francesco Lomonaco",
      "Sabrina Eimler",
      "H. Ulrich Hoppe",
      "Nils Malzahn"
    ],
    "author_ids": [],
    "abstract": "Social media have become an integral part of our lives, expanding our\ninterlinking capabilities to new levels. There is plenty to be said about their\npositive effects. On the other hand, however, some serious negative\nimplications of social media have been repeatedly highlighted in recent years,\npointing at various threats to society and its more vulnerable members, such as\nteenagers. We thus propose a theoretical framework based on an adaptive \"Social\nMedia Virtual Companion\" for educating and supporting an entire community,\nteenage students, to interact in social media environments in order to achieve\ndesirable conditions, defined in terms of a community-specific and\nparticipatory designed measure of Collective Well-Being (CWB). This Companion\ncombines automatic processing with expert intervention and guidance. The\nvirtual Companion will be powered by a Recommender System (CWB-RS) that will\noptimize a CWB metric instead of engagement or platform profit, which currently\nlargely drives recommender systems thereby disregarding any societal collateral\neffect.We put an emphasis on experts and educators in the educationally managed\nsocial media community of the Companion. They play five key roles: (a) use the\nCompanion in classroom-based educational activities; (b) guide the definition\nof the CWB; (c) provide a hierarchical structure of learning strategies,\nobjectives and activities that will support and contain the adaptive sequencing\nalgorithms of the CWB-RS based on hierarchical reinforcement learning; (d) act\nas moderators of direct conflicts between the members of the community; and,\nfinally, (e) monitor and address ethical and educational issues that are beyond\nthe intelligent agent's competence and control. Preliminary results on the\nperformance of the Companion's components and studies of the educational and\npsychological underlying principles are presented.",
    "published_date": "2021-01-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.04211v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.10157v1",
    "title": "Performance of Cell-Free MmWave Massive MIMO Systems with Fronthaul Compression and DAC Quantization",
    "authors": [
      "In-soo Kim",
      "Junil Choi"
    ],
    "author_ids": [],
    "abstract": "In this paper, the zero-forcing (ZF) precoder with max-min power allocation\nis proposed for cell-free millimeter wave (mmWave) massive multiple-input\nmultiple-output (MIMO) systems using low-resolution digital-to-analog\nconverters (DACs) with limited-capacity fronthaul links. The proposed power\nallocation aims to achieve max-min fairness on the achievable rate lower bounds\nof the users obtained by the additive quantization noise model (AQNM), which\nmimics the effect of low-resolution DACs. To solve the max-min power allocation\nproblem, an alternating optimization (AO) method is proposed, which is\nguaranteed to converge because the global optima of the subproblems that\nconstitute the original problem are attained at each AO iteration. The\nperformance of cell-free and small-cell systems is explored in the simulation\nresults, which suggest that not-too-small fronthaul capacity suffices for\ncell-free systems to outperform small-cell systems.",
    "published_date": "2021-01-25T00:00:00",
    "year": 2021,
    "categories": [
      "eess.SP",
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.10157v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2101.10151v1",
    "title": "Pricing Energy Storage in Real-time Market",
    "authors": [
      "Cong Chen",
      "Lang Tong",
      "Ye Guo"
    ],
    "author_ids": [],
    "abstract": "The problem of pricing utility-scale energy storage resources (ESRs) in the\nreal-time electricity market is considered. Under a rolling-window dispatch\nmodel where the operator centrally dispatches generation and consumption under\nforecasting uncertainty, it is shown that almost all uniform pricing schemes,\nincluding the standard locational marginal pricing (LMP), result in lost\nopportunity costs that require out-of-the-market settlements. It is also shown\nthat such settlements give rise to disincentives for generating firms and\nstorage participants to bid truthfully, even when these market participants are\nrational price-takers in a competitive market. Temporal locational marginal\npricing (TLMP) is proposed for ESRs as a generalization of LMP to an in-market\ndiscriminative form. TLMP is a sum of the system-wide energy price, LMP, and\nthe individual state-of-charge price. It is shown that, under arbitrary\nforecasting errors, the rolling-window implementation of TLMP eliminates the\nlost opportunity costs and provides incentives to price-taking firms to bid\ntruthfully with their marginal costs. Numerical examples show insights into the\neffects of uniform and non-uniform pricing mechanisms on dispatch following and\ntruthful bidding incentives.",
    "published_date": "2021-01-25T00:00:00",
    "year": 2021,
    "categories": [
      "eess.SY",
      "cs.SY",
      "econ.GN",
      "q-fin.EC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.10151v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2101.10075v1",
    "title": "Camera Invariant Feature Learning for Generalized Face Anti-spoofing",
    "authors": [
      "Baoliang Chen",
      "Wenhan Yang",
      "Haoliang Li",
      "Shiqi Wang",
      "Sam Kwong"
    ],
    "author_ids": [],
    "abstract": "There has been an increasing consensus in learning based face anti-spoofing\nthat the divergence in terms of camera models is causing a large domain gap in\nreal application scenarios. We describe a framework that eliminates the\ninfluence of inherent variance from acquisition cameras at the feature level,\nleading to the generalized face spoofing detection model that could be highly\nadaptive to different acquisition devices. In particular, the framework is\ncomposed of two branches. The first branch aims to learn the camera invariant\nspoofing features via feature level decomposition in the high frequency domain.\nMotivated by the fact that the spoofing features exist not only in the high\nfrequency domain, in the second branch the discrimination capability of\nextracted spoofing features is further boosted from the enhanced image based on\nthe recomposition of the high-frequency and low-frequency information. Finally,\nthe classification results of the two branches are fused together by a\nweighting strategy. Experiments show that the proposed method can achieve\nbetter performance in both intra-dataset and cross-dataset settings,\ndemonstrating the high generalization capability in various application\nscenarios.",
    "published_date": "2021-01-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.10075v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.10043v5",
    "title": "Deep One-Class Classification via Interpolated Gaussian Descriptor",
    "authors": [
      "Yuanhong Chen",
      "Yu Tian",
      "Guansong Pang",
      "Gustavo Carneiro"
    ],
    "author_ids": [],
    "abstract": "One-class classification (OCC) aims to learn an effective data description to\nenclose all normal training samples and detect anomalies based on the deviation\nfrom the data description. Current state-of-the-art OCC models learn a compact\nnormality description by hyper-sphere minimisation, but they often suffer from\noverfitting the training data, especially when the training set is small or\ncontaminated with anomalous samples. To address this issue, we introduce the\ninterpolated Gaussian descriptor (IGD) method, a novel OCC model that learns a\none-class Gaussian anomaly classifier trained with adversarially interpolated\ntraining samples. The Gaussian anomaly classifier differentiates the training\nsamples based on their distance to the Gaussian centre and the standard\ndeviation of these distances, offering the model a discriminability w.r.t. the\ngiven samples during training. The adversarial interpolation is enforced to\nconsistently learn a smooth Gaussian descriptor, even when the training data is\nsmall or contaminated with anomalous samples. This enables our model to learn\nthe data description based on the representative normal samples rather than\nfringe or anomalous samples, resulting in significantly improved normality\ndescription. In extensive experiments on diverse popular benchmarks, including\nMNIST, Fashion MNIST, CIFAR10, MVTec AD and two medical datasets, IGD achieves\nbetter detection accuracy than current state-of-the-art models. IGD also shows\nbetter robustness in problems with small or contaminated training sets. Code is\navailable at https://github.com/tianyu0207/IGD.",
    "published_date": "2021-01-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.10043v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.10001v1",
    "title": "Diverse Adversaries for Mitigating Bias in Training",
    "authors": [
      "Xudong Han",
      "Timothy Baldwin",
      "Trevor Cohn"
    ],
    "author_ids": [],
    "abstract": "Adversarial learning can learn fairer and less biased models of language than\nstandard methods. However, current adversarial techniques only partially\nmitigate model bias, added to which their training procedures are often\nunstable. In this paper, we propose a novel approach to adversarial learning\nbased on the use of multiple diverse discriminators, whereby discriminators are\nencouraged to learn orthogonal hidden representations from one another.\nExperimental results show that our method substantially improves over standard\nadversarial removal methods, in terms of reducing bias and the stability of\ntraining.",
    "published_date": "2021-01-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.10001v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.09995v2",
    "title": "Re-imagining Algorithmic Fairness in India and Beyond",
    "authors": [
      "Nithya Sambasivan",
      "Erin Arnesen",
      "Ben Hutchinson",
      "Tulsee Doshi",
      "Vinodkumar Prabhakaran"
    ],
    "author_ids": [],
    "abstract": "Conventional algorithmic fairness is West-centric, as seen in its sub-groups,\nvalues, and methods. In this paper, we de-center algorithmic fairness and\nanalyse AI power in India. Based on 36 qualitative interviews and a discourse\nanalysis of algorithmic deployments in India, we find that several assumptions\nof algorithmic fairness are challenged. We find that in India, data is not\nalways reliable due to socio-economic factors, ML makers appear to follow\ndouble standards, and AI evokes unquestioning aspiration. We contend that\nlocalising model fairness alone can be window dressing in India, where the\ndistance between models and oppressed communities is large. Instead, we\nre-imagine algorithmic fairness in India and provide a roadmap to\nre-contextualise data and models, empower oppressed communities, and enable\nFair-ML ecosystems.",
    "published_date": "2021-01-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.09995v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.09979v1",
    "title": "A Unified Joint Maximum Mean Discrepancy for Domain Adaptation",
    "authors": [
      "Wei Wang",
      "Baopu Li",
      "Shuhui Yang",
      "Jing Sun",
      "Zhengming Ding",
      "Junyang Chen",
      "Xiao Dong",
      "Zhihui Wang",
      "Haojie Li"
    ],
    "author_ids": [],
    "abstract": "Domain adaptation has received a lot of attention in recent years, and many\nalgorithms have been proposed with impressive progress. However, it is still\nnot fully explored concerning the joint probability distribution (P(X, Y))\ndistance for this problem, since its empirical estimation derived from the\nmaximum mean discrepancy (joint maximum mean discrepancy, JMMD) will involve\ncomplex tensor-product operator that is hard to manipulate. To solve this\nissue, this paper theoretically derives a unified form of JMMD that is easy to\noptimize, and proves that the marginal, class conditional and weighted class\nconditional probability distribution distances are our special cases with\ndifferent label kernels, among which the weighted class conditional one not\nonly can realize feature alignment across domains in the category level, but\nalso deal with imbalance dataset using the class prior probabilities. From the\nrevealed unified JMMD, we illustrate that JMMD degrades the feature-label\ndependence (discriminability) that benefits to classification, and it is\nsensitive to the label distribution shift when the label kernel is the weighted\nclass conditional one. Therefore, we leverage Hilbert Schmidt independence\ncriterion and propose a novel MMD matrix to promote the dependence, and devise\na novel label kernel that is robust to label distribution shift. Finally, we\nconduct extensive experiments on several cross-domain datasets to demonstrate\nthe validity and effectiveness of the revealed theoretical results.",
    "published_date": "2021-01-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.09979v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.10901v1",
    "title": "Population-Based Methods: PARTICLE SWARM OPTIMIZATION -- Development of a General-Purpose Optimizer and Applications",
    "authors": [
      "Mauro S. Innocente"
    ],
    "author_ids": [],
    "abstract": "This thesis is concerned with continuous, static, and single-objective\noptimization problems subject to inequality constraints. Nevertheless, some\nmethods to handle other kinds of problems are briefly reviewed. The particle\nswarm optimization paradigm was inspired by previous simulations of the\ncooperative behaviour observed in social beings. It is a bottom-up, randomly\nweighted, population-based method whose ability to optimize emerges from local,\nindividual-to-individual interactions. As opposed to traditional methods, it\ncan deal with different problems with few or no adaptation due to the fact that\nit does profit from problem-specific features of the problem at issue but\nperforms a parallel, cooperative exploration of the search-space by means of a\npopulation of individuals. The main goal of this thesis consists of developing\nan optimizer that can perform reasonably well on most problems. Hence, the\ninfluence of the settings of the algorithm's parameters on the behaviour of the\nsystem is studied, some general-purpose settings are sought, and some\nvariations to the canonical version are proposed aiming to turn it into a more\ngeneral-purpose optimizer. Since no termination condition is included in the\ncanonical version, this thesis is also concerned with the design of some\nstopping criteria which allow the iterative search to be terminated if further\nsignificant improvement is unlikely, or if a certain number of time-steps are\nreached. In addition, some constraint-handling techniques are incorporated into\nthe canonical algorithm to handle inequality constraints. Finally, the\ncapabilities of the proposed general-purpose optimizers are illustrated by\noptimizing a few benchmark problems.",
    "published_date": "2021-01-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.NE",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.10901v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.09869v2",
    "title": "Black Feminist Musings on Algorithmic Oppression",
    "authors": [
      "Lelia Marie Hampton"
    ],
    "author_ids": [],
    "abstract": "This paper unapologetically reflects on the critical role that Black feminism\ncan and should play in abolishing algorithmic oppression. Positioning\nalgorithmic oppression in the broader field of feminist science and technology\nstudies, I draw upon feminist philosophical critiques of science and technology\nand discuss histories and continuities of scientific oppression against\nhistorically marginalized people. Moreover, I examine the concepts of\ninvisibility and hypervisibility in oppressive technologies a l\\'a the\ncanonical double bind. Furthermore, I discuss what it means to call for\ndiversity as a solution to algorithmic violence, and I critique dialectics of\nthe fairness, accountability, and transparency community. I end by inviting you\nto envision and imagine the struggle to abolish algorithmic oppression by\nabolishing oppressive systems and shifting algorithmic development practices,\nincluding engaging our communities in scientific processes, centering\nmarginalized communities in design, and consensual data and algorithmic\npractices.",
    "published_date": "2021-01-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.09869v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.09858v4",
    "title": "Weakly Supervised Learning for Facial Behavior Analysis : A Review",
    "authors": [
      "R. Gnana Praveen",
      "Patrick Cardinal",
      "Eric Granger"
    ],
    "author_ids": [],
    "abstract": "In the recent years, there has been a shift in facial behavior analysis from\nthe laboratory-controlled conditions to the challenging in-the-wild conditions\ndue to the superior performance of deep learning based approaches for many real\nworld applications.However, the performance of deep learning approaches relies\non the amount of training data. One of the major problems with data acquisition\nis the requirement of annotations for large amount of training data. Labeling\nprocess of huge training data demands lot of human support with strong domain\nexpertise for facial expressions or action units, which is difficult to obtain\nin real-time environments.Moreover, labeling process is highly vulnerable to\nambiguity of expressions or action units, especially for intensities due to the\nbias induced by the domain experts. Therefore, there is an imperative need to\naddress the problem of facial behavior analysis with weak annotations. In this\npaper, we provide a comprehensive review of weakly supervised learning (WSL)\napproaches for facial behavior analysis with both categorical as well as\ndimensional labels along with the challenges and potential research directions\nassociated with it. First, we introduce various types of weak annotations in\nthe context of facial behavior analysis and the corresponding challenges\nassociated with it. We then systematically review the existing state-of-the-art\napproaches and provide a taxonomy of these approaches along with their insights\nand limitations. In addition, widely used data-sets in the reviewed literature\nand the performance of these approaches along with evaluation principles are\nsummarized. Finally, we discuss the remaining challenges and opportunities\nalong with the potential research directions in order to apply facial behavior\nanalysis with weak labels in real life situations.",
    "published_date": "2021-01-25T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.09858v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.09817v2",
    "title": "Population and Inequality Dynamics in Simple Economies",
    "authors": [
      "John C. Stevenson"
    ],
    "author_ids": [],
    "abstract": "While the use of spatial agent-based and individual-based models has\nflourished across many scientific disciplines, the complexities these models\ngenerate are often difficult to manage and quantify. This research reduces\npopulation-driven, spatial modeling of individuals to the simplest\nconfigurations and parameters: an equal resource opportunity landscape with\nequally capable individuals; and asks the question, \"Will valid complex\npopulation and inequality dynamics emerge from this simple economic model?\" Two\nforaging economies are modeled: subsistence and surplus. The resulting,\nemergent population dynamics are characterized by their sensitivities to agent\nand landscape parameters. The various steady and oscillating regimes of\nsingle-species population dynamics are generated by appropriate selection of\nmodel growth parameters. These emergent dynamics are shown to be consistent\nwith the equation-based, continuum modeling of single-species populations in\nbiology and ecology. The intrinsic growth rates, carry capacities, and delay\nparameters of these models are implied for these simple economies. Aggregate\nmeasures of individual distributions are used to understand the sensitivities\nto model parameters. New local measures are defined to describe complex\nbehaviors driven by spatial effects, especially extinctions. This simple\neconomic model is shown to generate significantly complex population and\ninequality dynamics. Model parameters generating the intrinsic growth rate have\nstrong effects on these dynamics, including large variations in inequality.\nSignificant inequality effects are shown to be caused by birth costs above and\nbeyond their contribution to the intrinsic growth rate. The highest levels of\ninequality are found during the initial non-equilibrium period and are driven\nby factors different than those driving steady state inequality.",
    "published_date": "2021-01-24T00:00:00",
    "year": 2021,
    "categories": [
      "physics.soc-ph",
      "cs.MA",
      "econ.GN",
      "q-fin.EC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.09817v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2101.09795v1",
    "title": "Comparing Broadband ISP Performance using Big Data from M-Lab",
    "authors": [
      "Xiaohong Deng",
      "Yun Feng",
      "Thanchanok Sutjarittham",
      "Hassan Habibi Gharakheili",
      "Blanca Gallego",
      "Vijay Sivaraman"
    ],
    "author_ids": [],
    "abstract": "Comparing ISPs on broadband speed is challenging, since measurements can vary\ndue to subscriber attributes such as operation system and test conditions such\nas access capacity, server distance, TCP window size, time-of-day, and network\nsegment size. In this paper, we draw inspiration from observational studies in\nmedicine, which face a similar challenge in comparing the effect of treatments\non patients with diverse characteristics, and have successfully tackled this\nusing \"causal inference\" techniques for {\\em post facto} analysis of medical\nrecords. Our first contribution is to develop a tool to pre-process and\nvisualize the millions of data points in M-Lab at various time- and\nspace-granularities to get preliminary insights on factors affecting broadband\nperformance. Next, we analyze 24 months of data pertaining to twelve ISPs\nacross three countries, and demonstrate that there is observational bias in the\ndata due to disparities amongst ISPs in their attribute distributions. For our\nthird contribution, we apply a multi-variate matching method to identify\nsuitable cohorts that can be compared without bias, which reveals that ISPs are\ncloser in performance than thought before. Our final contribution is to refine\nour model by developing a method for estimating speed-tier and re-apply\nmatching for comparison of ISP performance. Our results challenge conventional\nrankings of ISPs, and pave the way towards data-driven approaches for unbiased\ncomparisons of ISPs world-wide.",
    "published_date": "2021-01-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.PF"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.09795v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2101.09794v2",
    "title": "Equitable Division of a Path",
    "authors": [
      "Neeldhara Misra",
      "Chinmay Sonar",
      "P. R. Vaidyanathan",
      "Rohit Vaish"
    ],
    "author_ids": [],
    "abstract": "We study fair resource allocation under a connectedness constraint wherein a\nset of indivisible items are arranged on a path and only connected subsets of\nitems may be allocated to the agents. An allocation is deemed fair if it\nsatisfies equitability up to one good (EQ1), which requires that agents'\nutilities are approximately equal. We show that achieving EQ1 in conjunction\nwith well-studied measures of economic efficiency (such as Pareto optimality,\nnon-wastefulness, maximum egalitarian or utilitarian welfare) is\ncomputationally hard even for binary additive valuations. On the algorithmic\nside, we show that by relaxing the efficiency requirement, a connected EQ1\nallocation can be computed in polynomial time for any given ordering of agents,\neven for general monotone valuations. Interestingly, the allocation computed by\nour algorithm has the highest egalitarian welfare among all allocations\nconsistent with the given ordering. On the other hand, if efficiency is\nrequired, then tractability can still be achieved for binary additive\nvaluations with interval structure. On our way, we strengthen some of the\nexisting results in the literature for other fairness notions such as\nenvy-freeness up to one good (EF1), and also provide novel results for\nnegatively-valued items or chores.",
    "published_date": "2021-01-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.09794v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2101.09688v2",
    "title": "Stereotype and Skew: Quantifying Gender Bias in Pre-trained and Fine-tuned Language Models",
    "authors": [
      "Daniel de Vassimon Manela",
      "David Errington",
      "Thomas Fisher",
      "Boris van Breugel",
      "Pasquale Minervini"
    ],
    "author_ids": [],
    "abstract": "This paper proposes two intuitive metrics, skew and stereotype, that quantify\nand analyse the gender bias present in contextual language models when tackling\nthe WinoBias pronoun resolution task. We find evidence that gender stereotype\ncorrelates approximately negatively with gender skew in out-of-the-box models,\nsuggesting that there is a trade-off between these two forms of bias. We\ninvestigate two methods to mitigate bias. The first approach is an online\nmethod which is effective at removing skew at the expense of stereotype. The\nsecond, inspired by previous work on ELMo, involves the fine-tuning of BERT\nusing an augmented gender-balanced dataset. We show that this reduces both skew\nand stereotype relative to its unaugmented fine-tuned counterpart. However, we\nfind that existing gender bias benchmarks do not fully probe professional bias\nas pronoun resolution may be obfuscated by cross-correlations from other\nmanifestations of gender prejudice. Our code is available online, at\nhttps://github.com/12kleingordon34/NLP_masters_project.",
    "published_date": "2021-01-24T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.09688v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.10857v1",
    "title": "Indoor Group Activity Recognition using Multi-Layered HMMs",
    "authors": [
      "Vinayak Elangovan"
    ],
    "author_ids": [],
    "abstract": "Discovery and recognition of Group Activities (GA) based on imagery data\nprocessing have significant applications in persistent surveillance systems,\nwhich play an important role in some Internet services. The process is involved\nwith analysis of sequential imagery data with spatiotemporal associations.\nDiscretion of video imagery requires a proper inference system capable of\ndiscriminating and differentiating cohesive observations and interlinking them\nto known ontologies. We propose an Ontology based GAR with a proper inference\nmodel that is capable of identifying and classifying a sequence of events in\ngroup activities. A multi-layered Hidden Markov Model (HMM) is proposed to\nrecognize different levels of abstract GA. The multi-layered HMM consists of N\nlayers of HMMs where each layer comprises of M number of HMMs running in\nparallel. The number of layers depends on the order of information to be\nextracted. At each layer, by matching and correlating attributes of detected\ngroup events, the model attempts to associate sensory observations to known\nontology perceptions. This paper demonstrates and compares performance of three\ndifferent implementation of HMM, namely, concatenated N-HMM, cascaded C-HMM and\nhybrid H-HMM for building effective multi-layered HMM.",
    "published_date": "2021-01-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.10857v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.09551v1",
    "title": "Learning Competitive Equilibria in Noisy Combinatorial Markets",
    "authors": [
      "Enrique Areyan Viqueira",
      "Cyrus Cousins",
      "Amy Greenwald"
    ],
    "author_ids": [],
    "abstract": "We present a methodology to robustly estimate the competitive equilibria (CE)\nof combinatorial markets under the assumption that buyers do not know their\nprecise valuations for bundles of goods, but instead can only provide noisy\nestimates. We first show tight lower- and upper-bounds on the buyers' utility\nloss, and hence the set of CE, given a uniform approximation of one market by\nanother. We then develop a learning framework for our setup, and present two\nprobably-approximately-correct algorithms for learning CE, i.e., producing\nuniform approximations that preserve CE, with finite-sample guarantees. The\nfirst is a baseline that uses Hoeffding's inequality to produce a uniform\napproximation of buyers' valuations with high probability. The second leverages\na connection between the first welfare theorem of economics and uniform\napproximations to adaptively prune value queries when it determines that they\nare provably not part of a CE. We experiment with our algorithms and find that\nthe pruning algorithm achieves better estimates than the baseline with far\nfewer samples.",
    "published_date": "2021-01-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.09551v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2101.09523v1",
    "title": "Debiasing Pre-trained Contextualised Embeddings",
    "authors": [
      "Masahiro Kaneko",
      "Danushka Bollegala"
    ],
    "author_ids": [],
    "abstract": "In comparison to the numerous debiasing methods proposed for the static\nnon-contextualised word embeddings, the discriminative biases in contextualised\nembeddings have received relatively little attention. We propose a fine-tuning\nmethod that can be applied at token- or sentence-levels to debias pre-trained\ncontextualised embeddings. Our proposed method can be applied to any\npre-trained contextualised embedding model, without requiring to retrain those\nmodels. Using gender bias as an illustrative example, we then conduct a\nsystematic study using several state-of-the-art (SoTA) contextualised\nrepresentations on multiple benchmark datasets to evaluate the level of biases\nencoded in different contextualised embeddings before and after debiasing using\nthe proposed method. We find that applying token-level debiasing for all tokens\nand across all layers of a contextualised embedding model produces the best\nperformance. Interestingly, we observe that there is a trade-off between\ncreating an accurate vs. unbiased contextualised embedding model, and different\ncontextualised embedding models respond differently to this trade-off.",
    "published_date": "2021-01-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.09523v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.09458v2",
    "title": "Decoupled Exploration and Exploitation Policies for Sample-Efficient Reinforcement Learning",
    "authors": [
      "William F. Whitney",
      "Michael Bloesch",
      "Jost Tobias Springenberg",
      "Abbas Abdolmaleki",
      "Kyunghyun Cho",
      "Martin Riedmiller"
    ],
    "author_ids": [],
    "abstract": "Despite the close connection between exploration and sample efficiency, most\nstate of the art reinforcement learning algorithms include no considerations\nfor exploration beyond maximizing the entropy of the policy. In this work we\naddress this seeming missed opportunity. We observe that the most common\nformulation of directed exploration in deep RL, known as bonus-based\nexploration (BBE), suffers from bias and slow coverage in the few-sample\nregime. This causes BBE to be actively detrimental to policy learning in many\ncontrol tasks. We show that by decoupling the task policy from the exploration\npolicy, directed exploration can be highly effective for sample-efficient\ncontinuous control. Our method, Decoupled Exploration and Exploitation Policies\n(DEEP), can be combined with any off-policy RL algorithm without modification.\nWhen used in conjunction with soft actor-critic, DEEP incurs no performance\npenalty in densely-rewarding environments. On sparse environments, DEEP gives a\nseveral-fold improvement in data efficiency due to better exploration.",
    "published_date": "2021-01-23T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.09458v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.09378v1",
    "title": "Ants-Review: a Protocol for Incentivized Open Peer-Reviews on Ethereum",
    "authors": [
      "Bianca Trovò",
      "Nazzareno Massari"
    ],
    "author_ids": [],
    "abstract": "Peer-review is a necessary and essential quality control step for scientific\npublications but lacks proper incentives. Indeed, the process, which is very\ncostly in terms of time and intellectual investment, not only is not\nremunerated by the journals but is also not openly recognized by the academic\ncommunity as a relevant scientific output for a researcher. Therefore,\nscientific dissemination is affected in timeliness, quality, and fairness.\nHere, to solve this issue, we propose a blockchain-based incentive system that\nrewards scientists for peer-reviewing other scientists' work and that builds up\ntrust and reputation. We designed a privacy-oriented protocol of smart\ncontracts called Ants-Review that allows authors to issue a bounty for open\nanonymous peer-reviews on Ethereum. If requirements are met, peer-reviews will\nbe accepted and paid by the approver proportionally to their assessed quality.\nTo promote ethical behavior and inclusiveness the system implements a gamified\nmechanism that allows the whole community to evaluate the peer-reviews and vote\nfor the best ones.",
    "published_date": "2021-01-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DL",
      "cs.CR",
      "cs.CY",
      "cs.DC",
      "cs.GT",
      "68M14",
      "E.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.09378v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2101.09294v1",
    "title": "Censorship of Online Encyclopedias: Implications for NLP Models",
    "authors": [
      "Eddie Yang",
      "Margaret E. Roberts"
    ],
    "author_ids": [],
    "abstract": "While artificial intelligence provides the backbone for many tools people use\naround the world, recent work has brought to attention that the algorithms\npowering AI are not free of politics, stereotypes, and bias. While most work in\nthis area has focused on the ways in which AI can exacerbate existing\ninequalities and discrimination, very little work has studied how governments\nactively shape training data. We describe how censorship has affected the\ndevelopment of Wikipedia corpuses, text data which are regularly used for\npre-trained inputs into NLP algorithms. We show that word embeddings trained on\nBaidu Baike, an online Chinese encyclopedia, have very different associations\nbetween adjectives and a range of concepts about democracy, freedom, collective\naction, equality, and people and historical events in China than its regularly\nblocked but uncensored counterpart - Chinese language Wikipedia. We examine the\nimplications of these discrepancies by studying their use in downstream AI\napplications. Our paper shows how government repression, censorship, and\nself-censorship may impact training data and the applications that draw from\nthem.",
    "published_date": "2021-01-22T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.09294v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.08419v2",
    "title": "Auditing E-Commerce Platforms for Algorithmically Curated Vaccine Misinformation",
    "authors": [
      "Prerna Juneja",
      "Tanushree Mitra"
    ],
    "author_ids": [],
    "abstract": "There is a growing concern that e-commerce platforms are amplifying\nvaccine-misinformation. To investigate, we conduct two-sets of algorithmic\naudits for vaccine misinformation on the search and recommendation algorithms\nof Amazon -- world's leading e-retailer. First, we systematically audit\nsearch-results belonging to vaccine-related search-queries without logging into\nthe platform -- unpersonalized audits. We find 10.47% of search-results promote\nmisinformative health products. We also observe ranking-bias, with Amazon\nranking misinformative search-results higher than debunking search-results.\nNext, we analyze the effects of personalization due to account-history, where\nhistory is built progressively by performing various real-world user-actions,\nsuch as clicking a product. We find evidence of filter-bubble effect in\nAmazon's recommendations; accounts performing actions on misinformative\nproducts are presented with more misinformation compared to accounts performing\nactions on neutral and debunking products. Interestingly, once user clicks on a\nmisinformative product, homepage recommendations become more contaminated\ncompared to when user shows an intention to buy that product.",
    "published_date": "2021-01-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.08419v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2101.08390v3",
    "title": "An Information-Theoretic Analysis of the Impact of Task Similarity on Meta-Learning",
    "authors": [
      "Sharu Theresa Jose",
      "Osvaldo Simeone"
    ],
    "author_ids": [],
    "abstract": "Meta-learning aims at optimizing the hyperparameters of a model class or\ntraining algorithm from the observation of data from a number of related tasks.\nFollowing the setting of Baxter [1], the tasks are assumed to belong to the\nsame task environment, which is defined by a distribution over the space of\ntasks and by per-task data distributions. The statistical properties of the\ntask environment thus dictate the similarity of the tasks. The goal of the\nmeta-learner is to ensure that the hyperparameters obtain a small loss when\napplied for training of a new task sampled from the task environment. The\ndifference between the resulting average loss, known as meta-population loss,\nand the corresponding empirical loss measured on the available data from\nrelated tasks, known as meta-generalization gap, is a measure of the\ngeneralization capability of the meta-learner. In this paper, we present novel\ninformation-theoretic bounds on the average absolute value of the\nmeta-generalization gap. Unlike prior work [2], our bounds explicitly capture\nthe impact of task relatedness, the number of tasks, and the number of data\nsamples per task on the meta-generalization gap. Task similarity is gauged via\nthe Kullback-Leibler (KL) and Jensen-Shannon (JS) divergences. We illustrate\nthe proposed bounds on the example of ridge regression with meta-learned bias.",
    "published_date": "2021-01-21T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.IT",
      "eess.SP",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.08390v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.08035v1",
    "title": "Bias in ontologies -- a preliminary assessment",
    "authors": [
      "C. Maria Keet"
    ],
    "author_ids": [],
    "abstract": "Logical theories in the form of ontologies and similar artefacts in computing\nand IT are used for structuring, annotating, and querying data, among others,\nand therewith influence data analytics regarding what is fed into the\nalgorithms. Algorithmic bias is a well-known notion, but what does bias mean in\nthe context of ontologies that provide a structuring mechanism for an\nalgorithm's input? What are the sources of bias there and how would they\nmanifest themselves in ontologies? We examine and enumerate types of bias\nrelevant for ontologies, and whether they are explicit or implicit. These eight\ntypes are illustrated with examples from extant production-level ontologies and\nsamples from the literature. We then assessed three concurrently developed\nCOVID-19 ontologies on bias and detected different subsets of types of bias in\neach one, to a greater or lesser extent. This first characterisation aims\ncontribute to a sensitisation of ethical aspects of ontologies primarily\nregarding representation of information and knowledge.",
    "published_date": "2021-01-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI",
      "I.2.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.08035v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.09161v1",
    "title": "Hardhats and Bungaloos: Comparing Crowdsourced Design Feedback with Peer Design Feedback in the Classroom",
    "authors": [
      "Jonas Oppenlaender",
      "Elina Kuosmanen",
      "Andrés Lucero",
      "Simo Hosio"
    ],
    "author_ids": [],
    "abstract": "Feedback is an important aspect of design education, and crowdsourcing has\nemerged as a convenient way to obtain feedback at scale. In this paper, we\ninvestigate how crowdsourced design feedback compares to peer design feedback\nwithin a design-oriented HCI class and across two metrics: perceived quality\nand perceived fairness. We also examine the perceived monetary value of\ncrowdsourced feedback, which provides an interesting contrast to the typical\nrequester-centric view of the value of labor on crowdsourcing platforms. Our\nresults reveal that the students (N=106) perceived the crowdsourced design\nfeedback as inferior to peer design feedback in multiple ways. However, they\nalso identified various positive aspects of the online crowds that peers cannot\nprovide. We discuss the meaning of the findings and provide suggestions for\nteachers in HCI and other researchers interested in crowd feedback systems on\nusing crowds as a potential complement to peers.",
    "published_date": "2021-01-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC",
      "K.3; J.6; J.m"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.09161v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2101.07974v4",
    "title": "TCLR: Temporal Contrastive Learning for Video Representation",
    "authors": [
      "Ishan Dave",
      "Rohit Gupta",
      "Mamshad Nayeem Rizve",
      "Mubarak Shah"
    ],
    "author_ids": [],
    "abstract": "Contrastive learning has nearly closed the gap between supervised and\nself-supervised learning of image representations, and has also been explored\nfor videos. However, prior work on contrastive learning for video data has not\nexplored the effect of explicitly encouraging the features to be distinct\nacross the temporal dimension. We develop a new temporal contrastive learning\nframework consisting of two novel losses to improve upon existing contrastive\nself-supervised video representation learning methods. The local-local temporal\ncontrastive loss adds the task of discriminating between non-overlapping clips\nfrom the same video, whereas the global-local temporal contrastive aims to\ndiscriminate between timesteps of the feature map of an input clip in order to\nincrease the temporal diversity of the learned features. Our proposed temporal\ncontrastive learning framework achieves significant improvement over the\nstate-of-the-art results in various downstream video understanding tasks such\nas action recognition, limited-label action classification, and\nnearest-neighbor video retrieval on multiple video datasets and backbones. We\nalso demonstrate significant improvement in fine-grained action classification\nfor visually similar classes. With the commonly used 3D ResNet-18 architecture\nwith UCF101 pretraining, we achieve 82.4\\% (+5.1\\% increase over the previous\nbest) top-1 accuracy on UCF101 and 52.9\\% (+5.4\\% increase) on HMDB51 action\nclassification, and 56.2\\% (+11.7\\% increase) Top-1 Recall on UCF101 nearest\nneighbor video retrieval. Code released at github.com/DAVEISHAN/TCLR.",
    "published_date": "2021-01-20T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.07974v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.07833v1",
    "title": "Implicit Bias of Linear RNNs",
    "authors": [
      "Melikasadat Emami",
      "Mojtaba Sahraee-Ardakan",
      "Parthe Pandit",
      "Sundeep Rangan",
      "Alyson K. Fletcher"
    ],
    "author_ids": [],
    "abstract": "Contemporary wisdom based on empirical studies suggests that standard\nrecurrent neural networks (RNNs) do not perform well on tasks requiring\nlong-term memory. However, precise reasoning for this behavior is still\nunknown. This paper provides a rigorous explanation of this property in the\nspecial case of linear RNNs. Although this work is limited to linear RNNs, even\nthese systems have traditionally been difficult to analyze due to their\nnon-linear parameterization. Using recently-developed kernel regime analysis,\nour main result shows that linear RNNs learned from random initializations are\nfunctionally equivalent to a certain weighted 1D-convolutional network.\nImportantly, the weightings in the equivalent model cause an implicit bias to\nelements with smaller time lags in the convolution and hence, shorter memory.\nThe degree of this bias depends on the variance of the transition kernel matrix\nat initialization and is related to the classic exploding and vanishing\ngradients problem. The theory is validated in both synthetic and real data\nexperiments.",
    "published_date": "2021-01-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.NE",
      "cs.SY",
      "eess.SY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.07833v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.07641v2",
    "title": "Cooperative NOMA-Based User Pairing for URLLC: A Max-Min Fairness Approach",
    "authors": [
      "Fateme Salehi",
      "Naaser Neda",
      "Mohammad-Hassan Majidi",
      "Hamed Ahmadi"
    ],
    "author_ids": [],
    "abstract": "In this paper, cooperative non-orthogonal multiple access (C-NOMA) is\nconsidered in short packet communications with finite blocklength (FBL) codes.\nThe performance of a decode-and-forward (DF) relaying along with selection\ncombining (SC) and maximum ratio combining (MRC) strategies at the receiver\nside is examined. We explore joint user pairing and resource allocation to\nmaximize fair throughput in a downlink (DL) scenario. In each pair, the user\nwith a stronger channel (strong user) acts as a relay for the other one (weak\nuser), and optimal power and blocklength are allocated to achieve max-min\nthroughput. To this end, first, only one pair is considered, and optimal\nresource allocation is explored. Also, a suboptimal algorithm is suggested,\nwhich converges to a near-optimal solution. Finally, the problem is extended to\na general scenario, and a suboptimal C-NOMA-based user pairing is proposed.\nNumerical results show that the proposed C-NOMA scheme in both SC and MRC\nstrategies significantly improves the users' fair throughput compared to the\nNOMA and OMA. It is also investigated that the proposed pairing scheme based on\nC-NOMA outperforms the Hybrid NOMA/OMA scheme from the average throughput\nperspective, while the fairness index degrades slightly.",
    "published_date": "2021-01-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IT",
      "cs.SY",
      "eess.SY",
      "math.IT",
      "93-08"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.07641v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2101.07463v1",
    "title": "Computer Science Communities: Who is Speaking, and Who is Listening to the Women? Using an Ethics of Care to Promote Diverse Voices",
    "authors": [
      "Marc Cheong",
      "Kobi Leins",
      "Simon Coghlan"
    ],
    "author_ids": [],
    "abstract": "Those working on policy, digital ethics and governance often refer to issues\nin `computer science', that includes, but is not limited to, common subfields\nof Artificial Intelligence (AI), Computer Science (CS) Computer Security\n(InfoSec), Computer Vision (CV), Human Computer Interaction (HCI), Information\nSystems, (IS), Machine Learning (ML), Natural Language Processing (NLP) and\nSystems Architecture. Within this framework, this paper is a preliminary\nexploration of two hypotheses, namely 1) Each community has differing inclusion\nof minoritised groups (using women as our test case); and 2) Even where women\nexist in a community, they are not published representatively. Using data from\n20,000 research records, totalling 503,318 names, preliminary data supported\nour hypothesis. We argue that ACM has an ethical duty of care to its community\nto increase these ratios, and to hold individual computing communities to\naccount in order to do so, by providing incentives and a regular reporting\nsystem, in order to uphold its own Code.",
    "published_date": "2021-01-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.07463v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.07435v3",
    "title": "Fairness Criteria for Allocating Indivisible Chores: Connections and Efficiencies",
    "authors": [
      "Ankang Sun",
      "Bo Chen",
      "Xuan Vinh Doan"
    ],
    "author_ids": [],
    "abstract": "We study several fairness notions in allocating indivisible chores (i.e.,\nitems with non-positive values) to agents who have additive and submodular cost\nfunctions. The fairness criteria we are concern with are envy-free up to any\nitem (EFX), envy-free up to one item (EF1), maximin share (MMS), and pairwise\nmaximin share (PMMS), which are proposed as relaxations of envy-freeness in the\nsetting of additive cost functions. For allocations under each fairness\ncriterion, we establish their approximation guarantee for other fairness\ncriteria. Under the additive setting, our results show strong connections\nbetween these fairness criteria and, at the same time, reveal intrinsic\ndifferences between goods allocation and chores allocation. However, such\nstrong relationships cannot be inherited by the submodular setting, under which\nPMMS and MMS are no longer relaxations of envy-freeness and, even worse, few\nnon-trivial guarantees exist. We also investigate efficiency loss under these\nfairness constraints and establish their prices of fairness.",
    "published_date": "2021-01-19T00:00:00",
    "year": 2021,
    "categories": [
      "cs.GT",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.07435v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2101.07342v1",
    "title": "Feature Fusion of Raman Chemical Imaging and Digital Histopathology using Machine Learning for Prostate Cancer Detection",
    "authors": [
      "Trevor Doherty",
      "Susan McKeever",
      "Nebras Al-Attar",
      "Tiarnan Murphy",
      "Claudia Aura",
      "Arman Rahman",
      "Amanda O'Neill",
      "Stephen P Finn",
      "Elaine Kay",
      "William M. Gallagher",
      "R. William G. Watson",
      "Aoife Gowen",
      "Patrick Jackman"
    ],
    "author_ids": [],
    "abstract": "The diagnosis of prostate cancer is challenging due to the heterogeneity of\nits presentations, leading to the over diagnosis and treatment of\nnon-clinically important disease. Accurate diagnosis can directly benefit a\npatient's quality of life and prognosis. Towards addressing this issue, we\npresent a learning model for the automatic identification of prostate cancer.\nWhile many prostate cancer studies have adopted Raman spectroscopy approaches,\nnone have utilised the combination of Raman Chemical Imaging (RCI) and other\nimaging modalities. This study uses multimodal images formed from stained\nDigital Histopathology (DP) and unstained RCI. The approach was developed and\ntested on a set of 178 clinical samples from 32 patients, containing a range of\nnon-cancerous, Gleason grade 3 (G3) and grade 4 (G4) tissue microarray samples.\nFor each histological sample, there is a pathologist labelled DP - RCI image\npair. The hypothesis tested was whether multimodal image models can outperform\nsingle modality baseline models in terms of diagnostic accuracy. Binary\nnon-cancer/cancer models and the more challenging G3/G4 differentiation were\ninvestigated. Regarding G3/G4 classification, the multimodal approach achieved\na sensitivity of 73.8% and specificity of 88.1% while the baseline DP model\nshowed a sensitivity and specificity of 54.1% and 84.7% respectively. The\nmultimodal approach demonstrated a statistically significant 12.7% AUC\nadvantage over the baseline with a value of 85.8% compared to 73.1%, also\noutperforming models based solely on RCI and median Raman spectra. Feature\nfusion of DP and RCI does not improve the more trivial task of tumour\nidentification but does deliver an observed advantage in G3/G4 discrimination.\nBuilding on these promising findings, future work could include the acquisition\nof larger datasets for enhanced model generalization.",
    "published_date": "2021-01-18T00:00:00",
    "year": 2021,
    "categories": [
      "eess.IV",
      "cs.LG",
      "q-bio.QM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.07342v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.07296v2",
    "title": "Using Shape to Categorize: Low-Shot Learning with an Explicit Shape Bias",
    "authors": [
      "Stefan Stojanov",
      "Anh Thai",
      "James M. Rehg"
    ],
    "author_ids": [],
    "abstract": "It is widely accepted that reasoning about object shape is important for\nobject recognition. However, the most powerful object recognition methods today\ndo not explicitly make use of object shape during learning. In this work,\nmotivated by recent developments in low-shot learning, findings in\ndevelopmental psychology, and the increased use of synthetic data in computer\nvision research, we investigate how reasoning about 3D shape can be used to\nimprove low-shot learning methods' generalization performance. We propose a new\nway to improve existing low-shot learning approaches by learning a\ndiscriminative embedding space using 3D object shape, and using this embedding\nby learning how to map images into it. Our new approach improves the\nperformance of image-only low-shot learning approaches on multiple datasets. We\nalso introduce Toys4K, a 3D object dataset with the largest number of object\ncategories currently available, which supports low-shot learning.",
    "published_date": "2021-01-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.07296v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.07235v2",
    "title": "Reducing bias and increasing utility by federated generative modeling of medical images using a centralized adversary",
    "authors": [
      "Jean-Francois Rajotte",
      "Sumit Mukherjee",
      "Caleb Robinson",
      "Anthony Ortiz",
      "Christopher West",
      "Juan Lavista Ferres",
      "Raymond T Ng"
    ],
    "author_ids": [],
    "abstract": "We introduce FELICIA (FEderated LearnIng with a CentralIzed Adversary) a\ngenerative mechanism enabling collaborative learning. In particular, we show\nhow a data owner with limited and biased data could benefit from other data\nowners while keeping data from all the sources private. This is a common\nscenario in medical image analysis where privacy legislation prevents data from\nbeing shared outside local premises. FELICIA works for a large family of\nGenerative Adversarial Networks (GAN) architectures including vanilla and\nconditional GANs as demonstrated in this work. We show that by using the\nFELICIA mechanism, a data owner with limited image samples can generate\nhigh-quality synthetic images with high utility while neither data owners has\nto provide access to its data. The sharing happens solely through a central\ndiscriminator that has access limited to synthetic data. Here, utility is\ndefined as classification performance on a real test set. We demonstrate these\nbenefits on several realistic healthcare scenarions using benchmark image\ndatasets (MNIST, CIFAR-10) as well as on medical images for the task of skin\nlesion classification. With multiple experiments, we show that even in the\nworst cases, combining FELICIA with real data gracefully achieves performance\non par with real data while most results significantly improves the utility.",
    "published_date": "2021-01-18T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.CV",
      "cs.DC",
      "cs.LG",
      "68W15",
      "I.2.11"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.07235v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.06980v1",
    "title": "Mitigating the Position Bias of Transformer Models in Passage Re-Ranking",
    "authors": [
      "Sebastian Hofstätter",
      "Aldo Lipani",
      "Sophia Althammer",
      "Markus Zlabinger",
      "Allan Hanbury"
    ],
    "author_ids": [],
    "abstract": "Supervised machine learning models and their evaluation strongly depends on\nthe quality of the underlying dataset. When we search for a relevant piece of\ninformation it may appear anywhere in a given passage. However, we observe a\nbias in the position of the correct answer in the text in two popular Question\nAnswering datasets used for passage re-ranking. The excessive favoring of\nearlier positions inside passages is an unwanted artefact. This leads to three\ncommon Transformer-based re-ranking models to ignore relevant parts in unseen\npassages. More concerningly, as the evaluation set is taken from the same\nbiased distribution, the models overfitting to that bias overestimate their\ntrue effectiveness. In this work we analyze position bias on datasets, the\ncontextualized representations, and their effect on retrieval results. We\npropose a debiasing method for retrieval datasets. Our results show that a\nmodel trained on a position-biased dataset exhibits a significant decrease in\nre-ranking effectiveness when evaluated on a debiased dataset. We demonstrate\nthat by mitigating the position bias, Transformer-based re-ranking models are\nequally effective on a biased and debiased dataset, as well as more effective\nin a transfer-learning setting between two differently biased datasets.",
    "published_date": "2021-01-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.06980v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.06811v1",
    "title": "Optimal Pre-Processing to Achieve Fairness and Its Relationship with Total Variation Barycenter",
    "authors": [
      "Farhad Farokhi"
    ],
    "author_ids": [],
    "abstract": "We use disparate impact, i.e., the extent that the probability of observing\nan output depends on protected attributes such as race and gender, to measure\nfairness. We prove that disparate impact is upper bounded by the total\nvariation distance between the distribution of the inputs given the protected\nattributes. We then use pre-processing, also known as data repair, to enforce\nfairness. We show that utility degradation, i.e., the extent that the success\nof a forecasting model changes by pre-processing the data, is upper bounded by\nthe total variation distance between the distribution of the data before and\nafter pre-processing. Hence, the problem of finding the optimal pre-processing\nregiment for enforcing fairness can be cast as minimizing total variations\ndistance between the distribution of the data before and after pre-processing\nsubject to a constraint on the total variation distance between the\ndistribution of the inputs given protected attributes. This problem is a linear\nprogram that can be efficiently solved. We show that this problem is intimately\nrelated to finding the barycenter (i.e., center of mass) of two distributions\nwhen distances in the probability space are measured by total variation\ndistance. We also investigate the effect of differential privacy on fairness\nusing the proposed the total variation distances. We demonstrate the results\nusing numerical experimentation with a practice dataset.",
    "published_date": "2021-01-18T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IT",
      "cs.LG",
      "math.IT",
      "math.OC",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.06811v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.06774v1",
    "title": "Learning from pandemics: using extraordinary events can improve disease now-casting models",
    "authors": [
      "Sara Mesquita",
      "Cláudio Haupt Vieira",
      "Lília Perfeito",
      "Joana Gonçalves-Sá"
    ],
    "author_ids": [],
    "abstract": "Online searches have been used to study different health-related behaviours,\nincluding monitoring disease outbreaks. An obvious caveat is that several\nreasons can motivate individuals to seek online information and models that are\nblind to people's motivations are of limited use and can even mislead. This is\nparticularly true during extraordinary public health crisis, such as the\nongoing pandemic, when fear, curiosity and many other reasons can lead\nindividuals to search for health-related information, masking the\ndisease-driven searches. However, health crisis can also offer an opportunity\nto disentangle between different drivers and learn about human behavior. Here,\nwe focus on the two pandemics of the 21st century (2009-H1N1 flu and Covid-19)\nand propose a methodology to discriminate between search patterns linked to\ngeneral information seeking (media driven) and search patterns possibly more\nassociated with actual infection (disease driven). We show that by learning\nfrom such pandemic periods, with high anxiety and media hype, it is possible to\nselect online searches and improve model performance both in pandemic and\nseasonal settings. Moreover, and despite the common claim that more data is\nalways better, our results indicate that lower volume of the right data can be\nbetter than including large volumes of apparently similar data, especially in\nthe long run. Our work provides a general framework that can be applied beyond\nspecific events and diseases, and argues that algorithms can be improved simply\nby using less (better) data. This has important consequences, for example, to\nsolve the accuracy-explainability trade-off in machine-learning.",
    "published_date": "2021-01-17T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.06774v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.06536v6",
    "title": "Deep Cox Mixtures for Survival Regression",
    "authors": [
      "Chirag Nagpal",
      "Steve Yadlowsky",
      "Negar Rostamzadeh",
      "Katherine Heller"
    ],
    "author_ids": [],
    "abstract": "Survival analysis is a challenging variation of regression modeling because\nof the presence of censoring, where the outcome measurement is only partially\nknown, due to, for example, loss to follow up. Such problems come up frequently\nin medical applications, making survival analysis a key endeavor in\nbiostatistics and machine learning for healthcare, with Cox regression models\nbeing amongst the most commonly employed models. We describe a new approach for\nsurvival analysis regression models, based on learning mixtures of Cox\nregressions to model individual survival distributions. We propose an\napproximation to the Expectation Maximization algorithm for this model that\ndoes hard assignments to mixture groups to make optimization efficient. In each\ngroup assignment, we fit the hazard ratios within each group using deep neural\nnetworks, and the baseline hazard for each mixture component\nnon-parametrically.\n  We perform experiments on multiple real world datasets, and look at the\nmortality rates of patients across ethnicity and gender. We emphasize the\nimportance of calibration in healthcare settings and demonstrate that our\napproach outperforms classical and modern survival analysis baselines, both in\nterms of discriminative performance and calibration, with large gains in\nperformance on the minority demographics.",
    "published_date": "2021-01-16T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ME",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.06536v6",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.06388v1",
    "title": "Informative core identification in complex networks",
    "authors": [
      "Ruizhong Miao",
      "Tianxi Li"
    ],
    "author_ids": [],
    "abstract": "In network analysis, the core structure of modeling interest is usually\nhidden in a larger network in which most structures are not informative. The\nnoise and bias introduced by the non-informative component in networks can\nobscure the salient structure and limit many network modeling procedures'\neffectiveness. This paper introduces a novel core-periphery model for the\nnon-informative periphery structure of networks without imposing a specific\nform for the informative core structure. We propose spectral algorithms for\ncore identification as a data preprocessing step for general downstream network\nanalysis tasks based on the model. The algorithm enjoys a strong theoretical\nguarantee of accuracy and is scalable for large networks. We evaluate the\nproposed method by extensive simulation studies demonstrating various\nadvantages over many traditional core-periphery methods. The method is applied\nto extract the informative core structure from a citation network and give more\ninformative results in the downstream hierarchical community detection.",
    "published_date": "2021-01-16T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.ME"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.06388v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.06256v1",
    "title": "Internet of Robotic Things: Current Technologies, Applications, Challenges and Future Directions",
    "authors": [
      "Davide Villa",
      "Xinchao Song",
      "Matthew Heim",
      "Liangshe Li"
    ],
    "author_ids": [],
    "abstract": "Nowadays, the Internet of Things (IoT) concept is gaining more and more\nnotoriety bringing the number of connected devices to reach the order of\nbillion units. Its smart technology is influencing the research and\ndevelopments of advanced solutions in many areas. This paper focuses on the\nmerger between the IoT and robotics named the Internet of Robotic Things\n(IoRT). Allowing robotic systems to communicate over the internet at a minimal\ncost is an important technological opportunity. Robots can use the cloud to\nimprove the overall performance and for offloading demanding tasks. Since\ncommunicating to the cloud results in latency, data loss, and energy loss,\nfinding efficient techniques is a concern that can be addressed with current\nmachine learning methodologies. Moreover, the use of robotic generates ethical\nand regulation questions that should be answered for a proper coexistence\nbetween humans and robots. This paper aims at providing a better understanding\nof the new concept of IoRT with its benefits and limitations, as well as\nguidelines and directions for future research and studies.",
    "published_date": "2021-01-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.RO",
      "cs.CY",
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.06256v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.06223v2",
    "title": "LIME: Learning Inductive Bias for Primitives of Mathematical Reasoning",
    "authors": [
      "Yuhuai Wu",
      "Markus Rabe",
      "Wenda Li",
      "Jimmy Ba",
      "Roger Grosse",
      "Christian Szegedy"
    ],
    "author_ids": [],
    "abstract": "While designing inductive bias in neural architectures has been widely\nstudied, we hypothesize that transformer networks are flexible enough to learn\ninductive bias from suitable generic tasks. Here, we replace architecture\nengineering by encoding inductive bias in the form of datasets. Inspired by\nPeirce's view that deduction, induction, and abduction are the primitives of\nreasoning, we design three synthetic tasks that are intended to require the\nmodel to have these three abilities. We specifically design these tasks to be\nsynthetic and devoid of mathematical knowledge to ensure that only the\nfundamental reasoning biases can be learned from these tasks. This defines a\nnew pre-training methodology called \"LIME\" (Learning Inductive bias for\nMathematical rEasoning). Models trained with LIME significantly outperform\nvanilla transformers on four very different large mathematical reasoning\nbenchmarks. Unlike dominating the computation cost as traditional pre-training\napproaches, LIME requires only a small fraction of the computation cost of the\ntypical downstream task. The code for generating LIME tasks is available at\nhttps://github.com/tonywu95/LIME.",
    "published_date": "2021-01-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.LO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.06223v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.06067v2",
    "title": "Constraint Handling in Continuous-Time DDP-Based Model Predictive Control",
    "authors": [
      "Jean-Pierre Sleiman",
      "Farbod Farshidian",
      "Marco Hutter"
    ],
    "author_ids": [],
    "abstract": "The Sequential Linear Quadratic (SLQ) algorithm is a continuous-time variant\nof the well-known Differential Dynamic Programming (DDP) technique with a\nGauss-Newton Hessian approximation. This family of methods has gained\npopularity in the robotics community due to its efficiency in solving complex\ntrajectory optimization problems. However, one major drawback of DDP-based\nformulations is their inability to properly incorporate path constraints. In\nthis paper, we address this issue by devising a constrained SLQ algorithm that\nhandles a mixture of constraints with a previously implemented projection\ntechnique and a new augmented-Lagrangian approach. By providing an appropriate\nmultiplier update law, and by solving a single inner and outer loop iteration,\nwe are able to retrieve suboptimal solutions at rates suitable for real-time\nmodel-predictive control applications. We particularly focus on the\ninequality-constrained case, where three augmented-Lagrangian penalty functions\nare introduced, along with their corresponding multiplier update rules. These\nare then benchmarked against a relaxed log-barrier formulation in a cart-pole\nswing up example, an obstacle-avoidance task, and an object-pushing task with a\nquadrupedal mobile manipulator.",
    "published_date": "2021-01-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.RO",
      "cs.SY",
      "eess.SY",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.06067v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2101.06060v2",
    "title": "The Challenge of Value Alignment: from Fairer Algorithms to AI Safety",
    "authors": [
      "Iason Gabriel",
      "Vafa Ghazavi"
    ],
    "author_ids": [],
    "abstract": "This paper addresses the question of how to align AI systems with human\nvalues and situates it within a wider body of thought regarding technology and\nvalue. Far from existing in a vacuum, there has long been an interest in the\nability of technology to 'lock-in' different value systems. There has also been\nconsiderable thought about how to align technologies with specific social\nvalues, including through participatory design-processes. In this paper we look\nmore closely at the question of AI value alignment and suggest that the power\nand autonomy of AI systems gives rise to opportunities and challenges in the\ndomain of value that have not been encountered before. Drawing important\ncontinuities between the work of the fairness, accountability, transparency and\nethics community, and work being done by technical AI safety researchers, we\nsuggest that more attention needs to be paid to the question of 'social value\nalignment' - that is, how to align AI systems with the plurality of values\nendorsed by groups of people, especially on the global level.",
    "published_date": "2021-01-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.06060v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.05967v1",
    "title": "Responsible AI Challenges in End-to-end Machine Learning",
    "authors": [
      "Steven Euijong Whang",
      "Ki Hyun Tae",
      "Yuji Roh",
      "Geon Heo"
    ],
    "author_ids": [],
    "abstract": "Responsible AI is becoming critical as AI is widely used in our everyday\nlives. Many companies that deploy AI publicly state that when training a model,\nwe not only need to improve its accuracy, but also need to guarantee that the\nmodel does not discriminate against users (fairness), is resilient to noisy or\npoisoned data (robustness), is explainable, and more. In addition, these\nobjectives are not only relevant to model training, but to all steps of\nend-to-end machine learning, which include data collection, data cleaning and\nvalidation, model training, model evaluation, and model management and serving.\nFinally, responsible AI is conceptually challenging, and supporting all the\nobjectives must be as easy as possible. We thus propose three key research\ndirections towards this vision - depth, breadth, and usability - to measure\nprogress and introduce our ongoing research. First, responsible AI must be\ndeeply supported where multiple objectives like fairness and robust must be\nhandled together. To this end, we propose FR-Train, a holistic framework for\nfair and robust model training in the presence of data bias and poisoning.\nSecond, responsible AI must be broadly supported, preferably in all steps of\nmachine learning. Currently we focus on the data pre-processing steps and\npropose Slice Tuner, a selective data acquisition framework for training fair\nand accurate models, and MLClean, a data cleaning framework that also improves\nfairness and robustness. Finally, responsible AI must be usable where the\ntechniques must be easy to deploy and actionable. We propose FairBatch, a batch\nselection approach for fairness that is effective and simple to use, and Slice\nFinder, a model evaluation tool that automatically finds problematic slices. We\nbelieve we scratched the surface of responsible AI for end-to-end machine\nlearning and suggest research challenges moving forward.",
    "published_date": "2021-01-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.05967v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.05957v1",
    "title": "Descriptive AI Ethics: Collecting and Understanding the Public Opinion",
    "authors": [
      "Gabriel Lima",
      "Meeyoung Cha"
    ],
    "author_ids": [],
    "abstract": "There is a growing need for data-driven research efforts on how the public\nperceives the ethical, moral, and legal issues of autonomous AI systems. The\ncurrent debate on the responsibility gap posed by these systems is one such\nexample. This work proposes a mixed AI ethics model that allows normative and\ndescriptive research to complement each other, by aiding scholarly discussion\nwith data gathered from the public. We discuss its implications on bridging the\ngap between optimistic and pessimistic views towards AI systems' deployment.",
    "published_date": "2021-01-15T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.05957v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.05913v3",
    "title": "Supervised Transfer Learning at Scale for Medical Imaging",
    "authors": [
      "Basil Mustafa",
      "Aaron Loh",
      "Jan Freyberg",
      "Patricia MacWilliams",
      "Megan Wilson",
      "Scott Mayer McKinney",
      "Marcin Sieniek",
      "Jim Winkens",
      "Yuan Liu",
      "Peggy Bui",
      "Shruthi Prabhakara",
      "Umesh Telang",
      "Alan Karthikesalingam",
      "Neil Houlsby",
      "Vivek Natarajan"
    ],
    "author_ids": [],
    "abstract": "Transfer learning is a standard technique to improve performance on tasks\nwith limited data. However, for medical imaging, the value of transfer learning\nis less clear. This is likely due to the large domain mismatch between the\nusual natural-image pre-training (e.g. ImageNet) and medical images. However,\nrecent advances in transfer learning have shown substantial improvements from\nscale. We investigate whether modern methods can change the fortune of transfer\nlearning for medical imaging. For this, we study the class of large-scale\npre-trained networks presented by Kolesnikov et al. on three diverse imaging\ntasks: chest radiography, mammography, and dermatology. We study both transfer\nperformance and critical properties for the deployment in the medical domain,\nincluding: out-of-distribution generalization, data-efficiency, sub-group\nfairness, and uncertainty estimation. Interestingly, we find that for some of\nthese properties transfer from natural to medical images is indeed extremely\neffective, but only when performed at sufficient scale.",
    "published_date": "2021-01-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.05913v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.05783v2",
    "title": "Persistent Anti-Muslim Bias in Large Language Models",
    "authors": [
      "Abubakar Abid",
      "Maheen Farooqi",
      "James Zou"
    ],
    "author_ids": [],
    "abstract": "It has been observed that large-scale language models capture undesirable\nsocietal biases, e.g. relating to race and gender; yet religious bias has been\nrelatively unexplored. We demonstrate that GPT-3, a state-of-the-art contextual\nlanguage model, captures persistent Muslim-violence bias. We probe GPT-3 in\nvarious ways, including prompt completion, analogical reasoning, and story\ngeneration, to understand this anti-Muslim bias, demonstrating that it appears\nconsistently and creatively in different uses of the model and that it is\nsevere even compared to biases about other religious groups. For instance,\n\"Muslim\" is analogized to \"terrorist\" in 23% of test cases, while \"Jewish\" is\nmapped to \"money\" in 5% of test cases. We quantify the positive distraction\nneeded to overcome this bias with adversarial text prompts, and find that use\nof the most positive 6 adjectives reduces violent completions for \"Muslims\"\nfrom 66% to 20%, but which is still higher than for other religious groups.",
    "published_date": "2021-01-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.05783v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2103.12516v1",
    "title": "Edge-Cloud Collaboration Enabled Video Service Enhancement: A Hybrid Human-Artificial Intelligence Scheme",
    "authors": [
      "Dapeng Wu",
      "Ruili Bao",
      "Zhidu Li",
      "Honggang Wang",
      "Ruyan Wang"
    ],
    "author_ids": [],
    "abstract": "In this paper, a video service enhancement strategy is investigated under an\nedge-cloud collaboration framework, where video caching and delivery decisions\nare made in the cloud and edge respectively. We aim to guarantee the user\nfairness in terms of video coding rate under statistical delay constraint and\nedge caching capacity constraint. A hybrid human-artificial intelligence\napproach is developed to improve the user hit rate for video caching.\nSpecifically, individual user interest is first characterized by merging\nfactorization machine (FM) model and multi-layer perceptron (MLP) model, where\nboth low-order and high-order features can be well learned simultaneously.\nThereafter, a social aware similarity model is constructed to transferred\nindividual user interest to group interest, based on which, videos can be\nselected to cache. Furthermore, a double bisection exploration scheme is\nproposed to optimize wireless resource allocation and video coding rate. The\neffectiveness of the proposed video caching scheme and video delivery scheme is\nfinally validated by extensive experiments with a real-world data set.",
    "published_date": "2021-01-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.MM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.12516v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.05536v1",
    "title": "Scaling Equilibrium Propagation to Deep ConvNets by Drastically Reducing its Gradient Estimator Bias",
    "authors": [
      "Axel Laborieux",
      "Maxence Ernoult",
      "Benjamin Scellier",
      "Yoshua Bengio",
      "Julie Grollier",
      "Damien Querlioz"
    ],
    "author_ids": [],
    "abstract": "Equilibrium Propagation (EP) is a biologically-inspired counterpart of\nBackpropagation Through Time (BPTT) which, owing to its strong theoretical\nguarantees and the locality in space of its learning rule, fosters the design\nof energy-efficient hardware dedicated to learning. In practice, however, EP\ndoes not scale to visual tasks harder than MNIST. In this work, we show that a\nbias in the gradient estimate of EP, inherent in the use of finite nudging, is\nresponsible for this phenomenon and that cancelling it allows training deep\nConvNets by EP, including architectures with distinct forward and backward\nconnections. These results highlight EP as a scalable approach to compute error\ngradients in deep neural networks, thereby motivating its hardware\nimplementation.",
    "published_date": "2021-01-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.NE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.05536v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.05509v3",
    "title": "Transformer-based Language Model Fine-tuning Methods for COVID-19 Fake News Detection",
    "authors": [
      "Ben Chen",
      "Bin Chen",
      "Dehong Gao",
      "Qijin Chen",
      "Chengfu Huo",
      "Xiaonan Meng",
      "Weijun Ren",
      "Yang Zhou"
    ],
    "author_ids": [],
    "abstract": "With the pandemic of COVID-19, relevant fake news is spreading all over the\nsky throughout the social media. Believing in them without discrimination can\ncause great trouble to people's life. However, universal language models may\nperform weakly in these fake news detection for lack of large-scale annotated\ndata and sufficient semantic understanding of domain-specific knowledge. While\nthe model trained on corresponding corpora is also mediocre for insufficient\nlearning. In this paper, we propose a novel transformer-based language model\nfine-tuning approach for these fake news detection. First, the token vocabulary\nof individual model is expanded for the actual semantics of professional\nphrases. Second, we adapt the heated-up softmax loss to distinguish the\nhard-mining samples, which are common for fake news because of the\ndisambiguation of short text. Then, we involve adversarial training to improve\nthe model's robustness. Last, the predicted features extracted by universal\nlanguage model RoBERTa and domain-specific model CT-BERT are fused by one\nmultiple layer perception to integrate fine-grained and high-level specific\nrepresentations. Quantitative experimental results evaluated on existing\nCOVID-19 fake news dataset show its superior performances compared to the\nstate-of-the-art methods among various evaluation metrics. Furthermore, the\nbest weighted average F1 score achieves 99.02%.",
    "published_date": "2021-01-14T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.05509v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.05260v8",
    "title": "Hand-Based Person Identification using Global and Part-Aware Deep Feature Representation Learning",
    "authors": [
      "Nathanael L. Baisa",
      "Bryan Williams",
      "Hossein Rahmani",
      "Plamen Angelov",
      "Sue Black"
    ],
    "author_ids": [],
    "abstract": "In cases of serious crime, including sexual abuse, often the only available\ninformation with demonstrated potential for identification is images of the\nhands. Since this evidence is captured in uncontrolled situations, it is\ndifficult to analyse. As global approaches to feature comparison are limited in\nthis case, it is important to extend to consider local information. In this\nwork, we propose hand-based person identification by learning both global and\nlocal deep feature representations. Our proposed method, Global and Part-Aware\nNetwork (GPA-Net), creates global and local branches on the conv-layer for\nlearning robust discriminative global and part-level features. For learning the\nlocal (part-level) features, we perform uniform partitioning on the conv-layer\nin both horizontal and vertical directions. We retrieve the parts by conducting\na soft partition without explicitly partitioning the images or requiring\nexternal cues such as pose estimation. We make extensive evaluations on two\nlarge multi-ethnic and publicly available hand datasets, demonstrating that our\nproposed method significantly outperforms competing approaches.",
    "published_date": "2021-01-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.05260v8",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.04968v2",
    "title": "Learning with Gradient Descent and Weakly Convex Losses",
    "authors": [
      "Dominic Richards",
      "Mike Rabbat"
    ],
    "author_ids": [],
    "abstract": "We study the learning performance of gradient descent when the empirical risk\nis weakly convex, namely, the smallest negative eigenvalue of the empirical\nrisk's Hessian is bounded in magnitude. By showing that this eigenvalue can\ncontrol the stability of gradient descent, generalisation error bounds are\nproven that hold under a wider range of step sizes compared to previous work.\nOut of sample guarantees are then achieved by decomposing the test error into\ngeneralisation, optimisation and approximation errors, each of which can be\nbounded and traded off with respect to algorithmic parameters, sample size and\nmagnitude of this eigenvalue. In the case of a two layer neural network, we\ndemonstrate that the empirical risk can satisfy a notion of local weak\nconvexity, specifically, the Hessian's smallest eigenvalue during training can\nbe controlled by the normalisation of the layers, i.e., network scaling. This\nallows test error guarantees to then be achieved when the population risk\nminimiser satisfies a complexity assumption. By trading off the network\ncomplexity and scaling, insights are gained into the implicit bias of neural\nnetwork scaling, which are further supported by experimental findings.",
    "published_date": "2021-01-13T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.ST",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.04968v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.04843v1",
    "title": "What Makes a Dark Pattern... Dark? Design Attributes, Normative Considerations, and Measurement Methods",
    "authors": [
      "Arunesh Mathur",
      "Jonathan Mayer",
      "Mihir Kshirsagar"
    ],
    "author_ids": [],
    "abstract": "There is a rapidly growing literature on dark patterns, user interface\ndesigns -- typically related to shopping or privacy -- that researchers deem\nproblematic. Recent work has been predominantly descriptive, documenting and\ncategorizing objectionable user interfaces. These contributions have been\ninvaluable in highlighting specific designs for researchers and policymakers.\nBut the current literature lacks a conceptual foundation: What makes a user\ninterface a dark pattern? Why are certain designs problematic for users or\nsociety?\n  We review recent work on dark patterns and demonstrate that the literature\ndoes not reflect a singular concern or consistent definition, but rather, a set\nof thematically related considerations. Drawing from scholarship in psychology,\neconomics, ethics, philosophy, and law, we articulate a set of normative\nperspectives for analyzing dark patterns and their effects on individuals and\nsociety. We then show how future research on dark patterns can go beyond\nsubjective criticism of user interface designs and apply empirical methods\ngrounded in normative perspectives.",
    "published_date": "2021-01-13T00:00:00",
    "year": 2021,
    "categories": [
      "cs.HC",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.04843v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2101.04655v1",
    "title": "PolyAR: A Highly Parallelizable Solver For Polynomial Inequality Constraints Using Convex Abstraction Refinement",
    "authors": [
      "Wael Fatnassi",
      "Yasser Shoukry"
    ],
    "author_ids": [],
    "abstract": "Numerical tools for constraints solving are a cornerstone to control\nverification problems. This is evident by the plethora of research that uses\ntools like linear and convex programming for the design of control systems.\nNevertheless, the capability of linear and convex programming is limited and is\nnot adequate to reason about general nonlinear polynomials constraints that\narise naturally in the design of nonlinear systems. This limitation calls for\nnew solvers that are capable of utilizing the power of linear and convex\nprogramming to reason about general multivariate polynomials. In this paper, we\npropose PolyAR, a highly parallelizable solver for polynomial inequality\nconstraints. PolyAR provides several key contributions. First, it uses convex\nrelaxations of the problem to accelerate the process of finding a solution to\nthe set of the non-convex multivariate polynomials. Second, it utilizes an\niterative convex abstraction refinement process which aims to prune the search\nspace and identify regions for which the convex relaxation fails to solve the\nproblem. Third, it allows for a highly parallelizable usage of off-the-shelf\nsolvers to analyze the regions in which the convex relaxation failed to provide\nsolutions. We compared the scalability of PolyAR against Z3 8.9 and Yices 2.6\non control designing problems. Finally, we demonstrate the performance of\nPolyAR on designing switching signals for continuous-time linear switching\nsystems.",
    "published_date": "2021-01-12T00:00:00",
    "year": 2021,
    "categories": [
      "math.OC",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.04655v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2101.04199v1",
    "title": "Where you live matters: a spatial analysis of COVID-19 mortality",
    "authors": [
      "Behzad Javaheri"
    ],
    "author_ids": [],
    "abstract": "The COVID-19 pandemic has caused ~ 2 million fatalities. Significant progress\nhas been made in advancing our understanding of the disease process, one of the\nunanswered questions, however, is the anomaly in the case/mortality ratio with\nMexico as a clear example. Herein, this anomaly is explored by spatial analysis\nand whether mortality varies locally according to local factors. To address\nthis, hexagonal cartogram maps (hexbin) used to spatially map COVID-19\nmortality and visualise association with patient-level data on demographics and\npre-existing health conditions. This was further interrogated at local Mexico\nCity level by choropleth mapping. Our data show that the use of hexagonal\ncartograms is a better approach for spatial mapping of COVID-19 data in Mexico\nas it addresses bias in area size and population. We report sex/age-related\nspatial relationship with mortality amongst the Mexican states and a trend\nbetween health conditions and mortality at the state level. Within Mexico City,\nthere is a clear south, north divide with higher mortality in the northern\nmunicipalities. Deceased patients in these northern municipalities have the\nhighest pre-existing health conditions. Taken together, this study provides an\nimproved presentation of COVID-19 mapping in Mexico and demonstrates spatial\ndivergence of the mortality in Mexico.",
    "published_date": "2021-01-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.04199v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.04108v3",
    "title": "Controllable Guarantees for Fair Outcomes via Contrastive Information Estimation",
    "authors": [
      "Umang Gupta",
      "Aaron M Ferber",
      "Bistra Dilkina",
      "Greg Ver Steeg"
    ],
    "author_ids": [],
    "abstract": "Controlling bias in training datasets is vital for ensuring equal treatment,\nor parity, between different groups in downstream applications. A naive\nsolution is to transform the data so that it is statistically independent of\ngroup membership, but this may throw away too much information when a\nreasonable compromise between fairness and accuracy is desired. Another common\napproach is to limit the ability of a particular adversary who seeks to\nmaximize parity. Unfortunately, representations produced by adversarial\napproaches may still retain biases as their efficacy is tied to the complexity\nof the adversary used during training. To this end, we theoretically establish\nthat by limiting the mutual information between representations and protected\nattributes, we can assuredly control the parity of any downstream classifier.\nWe demonstrate an effective method for controlling parity through mutual\ninformation based on contrastive information estimators and show that they\noutperform approaches that rely on variational bounds based on complex\ngenerative models. We test our approach on UCI Adult and Heritage Health\ndatasets and demonstrate that our approach provides more informative\nrepresentations across a range of desired parity thresholds while providing\nstrong theoretical guarantees on the parity of any downstream algorithm.",
    "published_date": "2021-01-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.04108v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.03903v1",
    "title": "Adaptive modelling of variably saturated seepage problems",
    "authors": [
      "Ben Ashby",
      "Cassiano Bortolozo",
      "Alex Lukyanov",
      "Tristan Pryer"
    ],
    "author_ids": [],
    "abstract": "In this article we present a goal-oriented adaptive finite element method for\na class of subsurface flow problems in porous media, which exhibit seepage\nfaces. We focus on a representative case of the steady state flows governed by\na nonlinear Darcy-Buckingham law with physical constraints on\nsubsurface-atmosphere boundaries. This leads to the formulation of the problem\nas a variational inequality. The solutions to this problem are investigated\nusing an adaptive finite element method based on a dual-weighted a posteriori\nerror estimate, derived with the aim of reducing error in a specific target\nquantity. The quantity of interest is chosen as volumetric water flux across\nthe seepage face, and therefore depends on an a priori unknown free boundary.\nWe apply our method to challenging numerical examples as well as specific case\nstudies, from which this research originates, illustrating the major\ndifficulties that arise in practical situations. We summarise extensive\nnumerical results that clearly demonstrate the designed method produces rapid\nerror reduction measured against the number of degrees of freedom.",
    "published_date": "2021-01-11T00:00:00",
    "year": 2021,
    "categories": [
      "math.NA",
      "cs.NA",
      "65N30, 65N50, 65Z05, 76S05"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.03903v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2101.03827v3",
    "title": "Representativeness in Statistics, Politics, and Machine Learning",
    "authors": [
      "Kyla Chasalow",
      "Karen Levy"
    ],
    "author_ids": [],
    "abstract": "Representativeness is a foundational yet slippery concept. Though familiar at\nfirst blush, it lacks a single precise meaning. Instead, meanings range from\ntypical or characteristic, to a proportionate match between sample and\npopulation, to a more general sense of accuracy, generalizability, coverage, or\ninclusiveness. Moreover, the concept has long been contested. In statistics,\ndebates about the merits and methods of selecting a representative sample date\nback to the late 19th century; in politics, debates about the value of likeness\nas a logic of political representation are older still. Today, as the concept\ncrops up in the study of fairness and accountability in machine learning, we\nneed to carefully consider the term's meanings in order to communicate clearly\nand account for their normative implications. In this paper, we ask what\nrepresentativeness means, how it is mobilized socially, and what values and\nideals it communicates or confronts. We trace the concept's history in\nstatistics and discuss normative tensions concerning its relationship to\nlikeness, exclusion, authority, and aspiration. We draw on these analyses to\nthink through how representativeness is used in FAccT debates, with emphasis on\ndata, shift, participation, and power.",
    "published_date": "2021-01-11T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.03827v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2102.08941v1",
    "title": "Automatic Face Understanding: Recognizing Families in Photos",
    "authors": [
      "Joseph P Robinson"
    ],
    "author_ids": [],
    "abstract": "We built the largest database for kinship recognition. The data were labeled\nusing a novel clustering algorithm that used label proposals as side\ninformation to guide more accurate clusters. Great savings in time and human\ninput was had. Statistically, FIW shows enormous gains over its predecessors.\nWe have several benchmarks in kinship verification, family classification,\ntri-subject verification, and large-scale search and retrieval. We also trained\nCNNs on FIW and deployed the model on the renowned KinWild I and II to gain\nSOTA. Most recently, we further augmented FIW with MM. Now, video dynamics,\naudio, and text captions can be used in the decision making of kinship\nrecognition systems. We expect FIW will significantly impact research and\nreality. Additionally, we tackled the classic problem of facial landmark\nlocalization. A majority of these networks have objectives based on L1 or L2\nnorms, which inherit several disadvantages. The locations of landmarks are\ndetermined from generated heatmaps from which predicted landmark locations get\npenalized without accounting for the spread: a high scatter corresponds to low\nconfidence and vice-versa. To address this, we introduced an objective that\npenalizes for low confidence. Another issue is a dependency on labeled data,\nwhich is expensive to collect and susceptible to error. We addressed both\nissues by proposing an adversarial training framework that leverages unlabeled\ndata to improve model performance. Our method claims SOTA on renowned\nbenchmarks. Furthermore, our model is robust with a reduced size: 1/8 the\nnumber of channels is comparable to SOTA in real-time on a CPU. Finally, we\nbuilt BFW to serve as a proxy to measure bias across ethnicity and gender\nsubgroups, allowing us to characterize FR performances per subgroup. We show\nperformances are non-optimal when a single threshold is used to determine\nwhether sample pairs are genuine.",
    "published_date": "2021-01-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.08941v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.03584v1",
    "title": "Towards Long-term Fairness in Recommendation",
    "authors": [
      "Yingqiang Ge",
      "Shuchang Liu",
      "Ruoyuan Gao",
      "Yikun Xian",
      "Yunqi Li",
      "Xiangyu Zhao",
      "Changhua Pei",
      "Fei Sun",
      "Junfeng Ge",
      "Wenwu Ou",
      "Yongfeng Zhang"
    ],
    "author_ids": [],
    "abstract": "As Recommender Systems (RS) influence more and more people in their daily\nlife, the issue of fairness in recommendation is becoming more and more\nimportant. Most of the prior approaches to fairness-aware recommendation have\nbeen situated in a static or one-shot setting, where the protected groups of\nitems are fixed, and the model provides a one-time fairness solution based on\nfairness-constrained optimization. This fails to consider the dynamic nature of\nthe recommender systems, where attributes such as item popularity may change\nover time due to the recommendation policy and user engagement. For example,\nproducts that were once popular may become no longer popular, and vice versa.\nAs a result, the system that aims to maintain long-term fairness on the item\nexposure in different popularity groups must accommodate this change in a\ntimely fashion.\n  Novel to this work, we explore the problem of long-term fairness in\nrecommendation and accomplish the problem through dynamic fairness learning. We\nfocus on the fairness of exposure of items in different groups, while the\ndivision of the groups is based on item popularity, which dynamically changes\nover time in the recommendation process. We tackle this problem by proposing a\nfairness-constrained reinforcement learning algorithm for recommendation, which\nmodels the recommendation problem as a Constrained Markov Decision Process\n(CMDP), so that the model can dynamically adjust its recommendation policy to\nmake sure the fairness requirement is always satisfied when the environment\nchanges. Experiments on several real-world datasets verify our framework's\nsuperiority in terms of recommendation performance, short-term fairness, and\nlong-term fairness.",
    "published_date": "2021-01-10T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.03584v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.03366v1",
    "title": "The Future of Artificial Intelligence and its Social, Economic and Ethical Consequences",
    "authors": [
      "Burhan Rashid Hussein",
      "Chongomweru Halimu",
      "Muhammad Tariq Siddique"
    ],
    "author_ids": [],
    "abstract": "Recent development in AI has enabled the expansion of its application to\nmultiple domains. From medical treatment, gaming, manufacturing to daily\nbusiness processes. A huge amount of money has been poured into AI research due\nto its exciting discoveries. Technology giants like Google, Facebook, Amazon,\nand Baidu are the driving forces in the field today. But the rapid growth and\nexcitement that the technology offers obscure us from looking at the impact it\nbrings on our society. This short paper gives a brief history of AI and\nsummarizes various social, economic and ethical issues that are impacting our\nsociety today. We hope that this work will provide a useful starting point and\nperhaps reference for newcomers and stakeholders of the field.",
    "published_date": "2021-01-09T00:00:00",
    "year": 2021,
    "categories": [
      "cs.OH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.03366v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.03207v1",
    "title": "Leveraging Multilingual Transformers for Hate Speech Detection",
    "authors": [
      "Sayar Ghosh Roy",
      "Ujwal Narayan",
      "Tathagata Raha",
      "Zubair Abid",
      "Vasudeva Varma"
    ],
    "author_ids": [],
    "abstract": "Detecting and classifying instances of hate in social media text has been a\nproblem of interest in Natural Language Processing in the recent years. Our\nwork leverages state of the art Transformer language models to identify hate\nspeech in a multilingual setting. Capturing the intent of a post or a comment\non social media involves careful evaluation of the language style, semantic\ncontent and additional pointers such as hashtags and emojis. In this paper, we\nlook at the problem of identifying whether a Twitter post is hateful and\noffensive or not. We further discriminate the detected toxic content into one\nof the following three classes: (a) Hate Speech (HATE), (b) Offensive (OFFN)\nand (c) Profane (PRFN). With a pre-trained multilingual Transformer-based text\nencoder at the base, we are able to successfully identify and classify hate\nspeech from multiple languages. On the provided testing corpora, we achieve\nMacro F1 scores of 90.29, 81.87 and 75.40 for English, German and Hindi\nrespectively while performing hate speech detection and of 60.70, 53.28 and\n49.74 during fine-grained classification. In our experiments, we show the\nefficacy of Perspective API features for hate speech classification and the\neffects of exploiting a multilingual training scheme. A feature selection study\nis provided to illustrate impacts of specific features upon the architecture's\nclassification head.",
    "published_date": "2021-01-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.IR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.03207v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.03036v1",
    "title": "Contextual Non-Local Alignment over Full-Scale Representation for Text-Based Person Search",
    "authors": [
      "Chenyang Gao",
      "Guanyu Cai",
      "Xinyang Jiang",
      "Feng Zheng",
      "Jun Zhang",
      "Yifei Gong",
      "Pai Peng",
      "Xiaowei Guo",
      "Xing Sun"
    ],
    "author_ids": [],
    "abstract": "Text-based person search aims at retrieving target person in an image gallery\nusing a descriptive sentence of that person. It is very challenging since modal\ngap makes effectively extracting discriminative features more difficult.\nMoreover, the inter-class variance of both pedestrian images and descriptions\nis small. So comprehensive information is needed to align visual and textual\nclues across all scales. Most existing methods merely consider the local\nalignment between images and texts within a single scale (e.g. only global\nscale or only partial scale) then simply construct alignment at each scale\nseparately. To address this problem, we propose a method that is able to\nadaptively align image and textual features across all scales, called NAFS\n(i.e.Non-local Alignment over Full-Scale representations). Firstly, a novel\nstaircase network structure is proposed to extract full-scale image features\nwith better locality. Secondly, a BERT with locality-constrained attention is\nproposed to obtain representations of descriptions at different scales. Then,\ninstead of separately aligning features at each scale, a novel contextual\nnon-local attention mechanism is applied to simultaneously discover latent\nalignments across all scales. The experimental results show that our method\noutperforms the state-of-the-art methods by 5.53% in terms of top-1 and 5.35%\nin terms of top-5 on text-based person search dataset. The code is available at\nhttps://github.com/TencentYoutuResearch/PersonReID-NAFS",
    "published_date": "2021-01-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.03036v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.02968v1",
    "title": "Group Fairness: Independence Revisited",
    "authors": [
      "Tim Räz"
    ],
    "author_ids": [],
    "abstract": "This paper critically examines arguments against independence, a measure of\ngroup fairness also known as statistical parity and as demographic parity. In\nrecent discussions of fairness in computer science, some have maintained that\nindependence is not a suitable measure of group fairness. This position is at\nleast partially based on two influential papers (Dwork et al., 2012, Hardt et\nal., 2016) that provide arguments against independence. We revisit these\narguments, and we find that the case against independence is rather weak. We\nalso give arguments in favor of independence, showing that it plays a\ndistinctive role in considerations of fairness. Finally, we discuss how to\nbalance different fairness considerations.",
    "published_date": "2021-01-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.02968v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2101.02963v1",
    "title": "Hermes: Decentralized Dynamic Spectrum Access System for Massive Devices Deployment in 5G",
    "authors": [
      "Zhihui Gao",
      "Ang Li",
      "Yunfan Gao",
      "Yu Wang",
      "Yiran Chen"
    ],
    "author_ids": [],
    "abstract": "With the incoming 5G network, the ubiquitous Internet of Things (IoT) devices\ncan benefit our daily life, such as smart cameras, drones, etc. With the\nintroduction of the millimeter-wave band and the thriving number of IoT\ndevices, it is critical to design new dynamic spectrum access (DSA) system to\ncoordinate the spectrum allocation across massive devices in 5G. In this paper,\nwe present Hermes, the first decentralized DSA system for massive devices\ndeployment. Specifically, we propose an efficient multi-agent reinforcement\nlearning algorithm and introduce a novel shuffle mechanism, addressing the\ndrawbacks of collision and fairness in existing decentralized systems. We\nimplement Hermes in 5G network via simulations. Extensive evaluations show that\nHermes significantly reduces collisions and improves fairness compared to the\nstate-of-the-art decentralized methods. Furthermore, Hermes is able to adapt\nthe environmental changes within 0.5 seconds, showing its deployment\npracticability in dynamic environment of 5G.",
    "published_date": "2021-01-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.02963v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2101.02957v1",
    "title": "Observations on the Bias of Nonnegative Mechanisms for Differential Privacy",
    "authors": [
      "Aisling Mc Glinchey",
      "Oliver Mason"
    ],
    "author_ids": [],
    "abstract": "We study two methods for differentially private analysis of bounded data and\nextend these to nonnegative queries. We first recall that for the Laplace\nmechanism, boundary inflated truncation (BIT) applied to nonnegative queries\nand truncation both lead to strictly positive bias. We then consider a\ngeneralization of BIT using translated ramp functions. We explicitly\ncharacterise the optimal function in this class for worst case bias. We show\nthat applying any square-integrable post-processing function to a Laplace\nmechanism leads to a strictly positive maximal absolute bias. A corresponding\nresult is also shown for a generalisation of truncation, which we refer to as\nrestriction. We also briefly consider an alternative approach based on\nmultiplicative mechanisms for positive data and show that, without additional\nrestrictions, these mechanisms can lead to infinite bias.",
    "published_date": "2021-01-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CR",
      "math.ST",
      "stat.TH",
      "68P27"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.02957v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2101.02939v2",
    "title": "Application of Machine Learning to Performance Assessment for a class of PID-based Control Systems",
    "authors": [
      "Patryk Grelewicz",
      "Thanh Tung Khuat",
      "Jacek Czeczot",
      "Pawel Nowak",
      "Tomasz Klopot",
      "Bogdan Gabrys"
    ],
    "author_ids": [],
    "abstract": "In this paper, a novel machine learning derived control performance\nassessment (CPA) classification system is proposed. It is dedicated for a wide\nclass of PID-based control industrial loops with processes exhibiting dynamical\nproperties close to second order plus delay time (SOPDT). The proposed concept\nis very general and easy to configure to distinguish between acceptable and\npoor closed loop performance. This approach allows for determining the best\n(but also robust and practically achievable) closed loop performance based on\nvery popular and intuitive closed loop quality factors. Training set can be\nautomatically derived off-line using a number of different, diverse control\nperformance indices (CPIs) used as discriminative features of the assessed\ncontrol system. The proposed extended set of CPIs is discussed with\ncomprehensive performance assessment of different machine learning based\nclassification methods and practical application of the suggested solution. As\na result, a general-purpose CPA system is derived that can be immediately\napplied in practice without any preliminary or additional learning stage during\nnormal closed loop operation. It is verified by practical application to assess\nthe control system for a laboratory heat exchange and distribution setup.",
    "published_date": "2021-01-08T00:00:00",
    "year": 2021,
    "categories": [
      "eess.SY",
      "cs.SY",
      "93",
      "I.2; I.5"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.02939v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.02831v1",
    "title": "A Tale of Fairness Revisited: Beyond Adversarial Learning for Deep Neural Network Fairness",
    "authors": [
      "Becky Mashaido",
      "Winston Moh Tangongho"
    ],
    "author_ids": [],
    "abstract": "Motivated by the need for fair algorithmic decision making in the age of\nautomation and artificially-intelligent technology, this technical report\nprovides a theoretical insight into adversarial training for fairness in deep\nlearning. We build upon previous work in adversarial fairness, show the\npersistent tradeoff between fair predictions and model performance, and explore\nfurther mechanisms that help in offsetting this tradeoff.",
    "published_date": "2021-01-08T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.6"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.02831v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.02701v1",
    "title": "Does double-blind peer-review reduce bias? Evidence from a top computer science conference",
    "authors": [
      "Mengyi Sun",
      "Jainabou Barry Danfa",
      "Misha Teplitskiy"
    ],
    "author_ids": [],
    "abstract": "Peer review is widely regarded as essential for advancing scientific\nresearch. However, reviewers may be biased by authors' prestige or other\ncharacteristics. Double-blind peer review, in which the authors' identities are\nmasked from the reviewers, has been proposed as a way to reduce reviewer bias.\nAlthough intuitive, evidence for the effectiveness of double-blind peer review\nin reducing bias is limited and mixed. Here, we examine the effects of\ndouble-blind peer review on prestige bias by analyzing the peer review files of\n5027 papers submitted to the International Conference on Learning\nRepresentations (ICLR), a top computer science conference that changed its\nreviewing policy from single-blind peer review to double-blind peer review in\n2018. We find that after switching to double-blind review, the scores given to\nthe most prestigious authors significantly decreased. However, because many of\nthese papers were above the threshold for acceptance, the change did not affect\npaper acceptance decisions significantly. Nevertheless, we show that\ndouble-blind peer review may have improved the quality of the selections by\nlimiting other (non-author-prestige) biases. Specifically, papers rejected in\nthe single-blind format are cited more than those rejected under the\ndouble-blind format, suggesting that double-blind review better identifies\npoorer quality papers. Interestingly, an apparently unrelated change - the\nchange of rating scale from 10 to 4 points - likely reduced prestige bias\nsignificantly, to an extent that affected papers' acceptance. These results\nprovide some support for the effectiveness of double-blind review in reducing\nprestige bias, while opening new research directions on the impact of peer\nreview formats.",
    "published_date": "2021-01-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.DL",
      "econ.GN",
      "q-fin.EC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.02701v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2101.02647v2",
    "title": "From Learning to Relearning: A Framework for Diminishing Bias in Social Robot Navigation",
    "authors": [
      "Juana Valeria Hurtado",
      "Laura Londoño",
      "Abhinav Valada"
    ],
    "author_ids": [],
    "abstract": "The exponentially increasing advances in robotics and machine learning are\nfacilitating the transition of robots from being confined to controlled\nindustrial spaces to performing novel everyday tasks in domestic and urban\nenvironments. In order to make the presence of robots safe as well as\ncomfortable for humans, and to facilitate their acceptance in public\nenvironments, they are often equipped with social abilities for navigation and\ninteraction. Socially compliant robot navigation is increasingly being learned\nfrom human observations or demonstrations. We argue that these techniques that\ntypically aim to mimic human behavior do not guarantee fair behavior. As a\nconsequence, social navigation models can replicate, promote, and amplify\nsocietal unfairness such as discrimination and segregation. In this work, we\ninvestigate a framework for diminishing bias in social robot navigation models\nso that robots are equipped with the capability to plan as well as adapt their\npaths based on both physical and social demands. Our proposed framework\nconsists of two components: \\textit{learning} which incorporates social context\ninto the learning process to account for safety and comfort, and\n\\textit{relearning} to detect and correct potentially harmful outcomes before\nthe onset. We provide both technological and societal analysis using three\ndiverse case studies in different social scenarios of interaction. Moreover, we\npresent ethical implications of deploying robots in social environments and\npropose potential solutions. Through this study, we highlight the importance\nand advocate for fairness in human-robot interactions in order to promote more\nequitable social relationships, roles, and dynamics and consequently positively\ninfluence our society.",
    "published_date": "2021-01-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.02647v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.06110v1",
    "title": "Bridging the Gap: the case for an Incompletely Theorized Agreement on AI policy",
    "authors": [
      "Charlotte Stix",
      "Matthijs M. Maas"
    ],
    "author_ids": [],
    "abstract": "Recent progress in artificial intelligence (AI) raises a wide array of\nethical and societal concerns. Accordingly, an appropriate policy approach is\nneeded today. While there has been a wave of scholarship in this field, the\nresearch community at times appears divided amongst those who emphasize\nnear-term concerns, and those focusing on long-term concerns and corresponding\npolicy measures. In this paper, we seek to map and critically examine this\nalleged gulf, with a view to understanding the practical space for\ninter-community collaboration on AI policy. This culminates in a proposal to\nmake use of the legal notion of an incompletely theorized agreement. We propose\nthat on certain issue areas, scholars working with near-term and long-term\nperspectives can converge and cooperate on selected mutually beneficial AI\npolicy projects all the while maintaining divergent perspectives.",
    "published_date": "2021-01-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.06110v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.02459v1",
    "title": "Incorporating Vision Bias into Click Models for Image-oriented Search Engine",
    "authors": [
      "Ningxin Xu",
      "Cheng Yang",
      "Yixin Zhu",
      "Xiaowei Hu",
      "Changhu Wang"
    ],
    "author_ids": [],
    "abstract": "Most typical click models assume that the probability of a document to be\nexamined by users only depends on position, such as PBM and UBM. It works well\nin various kinds of search engines. However, in a search engine where massive\ncandidate documents display images as responses to the query, the examination\nprobability should not only depend on position. The visual appearance of an\nimage-oriented document also plays an important role in its opportunity to be\nexamined. In this paper, we assume that vision bias exists in an image-oriented\nsearch engine as another crucial factor affecting the examination probability\naside from position. Specifically, we apply this assumption to classical click\nmodels and propose an extended model, to better capture the examination\nprobabilities of documents. We use regression-based EM algorithm to predict the\nvision bias given the visual features extracted from candidate documents.\nEmpirically, we evaluate our model on a dataset developed from a real-world\nonline image-oriented search engine, and demonstrate that our proposed model\ncan achieve significant improvements over its baseline model in data fitness\nand sparsity handling.",
    "published_date": "2021-01-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.02459v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.02374v1",
    "title": "Efficient 3D Point Cloud Feature Learning for Large-Scale Place Recognition",
    "authors": [
      "Le Hui",
      "Mingmei Cheng",
      "Jin Xie",
      "Jian Yang"
    ],
    "author_ids": [],
    "abstract": "Point cloud based retrieval for place recognition is still a challenging\nproblem due to drastic appearance and illumination changes of scenes in\nchanging environments. Existing deep learning based global descriptors for the\nretrieval task usually consume a large amount of computation resources (e.g.,\nmemory), which may not be suitable for the cases of limited hardware resources.\nIn this paper, we develop an efficient point cloud learning network (EPC-Net)\nto form a global descriptor for visual place recognition, which can obtain good\nperformance and reduce computation memory and inference time. First, we propose\na lightweight but effective neural network module, called ProxyConv, to\naggregate the local geometric features of point clouds. We leverage the spatial\nadjacent matrix and proxy points to simplify the original edge convolution for\nlower memory consumption. Then, we design a lightweight grouped VLAD network\n(G-VLAD) to form global descriptors for retrieval. Compared with the original\nVLAD network, we propose a grouped fully connected (GFC) layer to decompose the\nhigh-dimensional vectors into a group of low-dimensional vectors, which can\nreduce the number of parameters of the network and maintain the discrimination\nof the feature vector. Finally, to further reduce the inference time, we\ndevelop a simple version of EPC-Net, called EPC-Net-L, which consists of two\nProxyConv modules and one max pooling layer to aggregate global descriptors. By\ndistilling the knowledge from EPC-Net, EPC-Net-L can obtain discriminative\nglobal descriptors for retrieval. Extensive experiments on the Oxford dataset\nand three in-house datasets demonstrate that our proposed method can achieve\nstate-of-the-art performance with lower parameters, FLOPs, and runtime per\nframe.",
    "published_date": "2021-01-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.02374v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.02334v2",
    "title": "Machine Learning on Cloud with Blockchain: A Secure, Verifiable and Fair Approach to Outsource the Linear Regression for Data Analysis",
    "authors": [
      "Hanlin Zhang",
      "Peng Gao",
      "Jia Yu",
      "Jie Lin",
      "Neal N. Xiong"
    ],
    "author_ids": [],
    "abstract": "Linear Regression (LR) is a classical machine learning algorithm which has\nmany applications in the cyber physical social systems (CPSS) to shape and\nsimplify the way we live, work and communicate. This paper focuses on the data\nanalysis for CPSS when the Linear Regression is applied. The training process\nof LR is time-consuming since it involves complex matrix operations, especially\nwhen it gets a large scale training dataset In the CPSS. Thus, how to enable\ndevices to efficiently perform the training process of the Linear Regression is\nof significant importance. To address this issue, in this paper, we present a\nsecure, verifiable and fair approach to outsource LR to an untrustworthy\ncloud-server. In the proposed scheme, computation inputs/outputs are obscured\nso that the privacy of sensitive information is protected against cloud-server.\nMeanwhile, computation result from cloud-server is verifiable. Also, fairness\nis guaranteed by the blockchain, which ensures that the cloud gets paid only if\nhe correctly performed the outsourced workload. Based on the presented\napproach, we exploited the fair, secure outsourcing system on the Ethereum\nblockchain. We analysed our presented scheme on theoretical and experimental,\nall of which indicate that the presented scheme is valid, secure and efficient.",
    "published_date": "2021-01-07T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.02334v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.02258v1",
    "title": "Can RNNs learn Recursive Nested Subject-Verb Agreements?",
    "authors": [
      "Yair Lakretz",
      "Théo Desbordes",
      "Jean-Rémi King",
      "Benoît Crabbé",
      "Maxime Oquab",
      "Stanislas Dehaene"
    ],
    "author_ids": [],
    "abstract": "One of the fundamental principles of contemporary linguistics states that\nlanguage processing requires the ability to extract recursively nested tree\nstructures. However, it remains unclear whether and how this code could be\nimplemented in neural circuits. Recent advances in Recurrent Neural Networks\n(RNNs), which achieve near-human performance in some language tasks, provide a\ncompelling model to address such questions. Here, we present a new framework to\nstudy recursive processing in RNNs, using subject-verb agreement as a probe\ninto the representations of the neural network. We trained six distinct types\nof RNNs on a simplified probabilistic context-free grammar designed to\nindependently manipulate the length of a sentence and the depth of its\nsyntactic tree. All RNNs generalized to subject-verb dependencies longer than\nthose seen during training. However, none systematically generalized to deeper\ntree structures, even those with a structural bias towards learning nested tree\n(i.e., stack-RNNs). In addition, our analyses revealed primacy and recency\neffects in the generalization patterns of LSTM-based models, showing that these\nmodels tend to perform well on the outer- and innermost parts of a\ncenter-embedded tree structure, but poorly on its middle levels. Finally,\nprobing the internal states of the model during the processing of sentences\nwith nested tree structures, we found a complex encoding of grammatical\nagreement information (e.g. grammatical number), in which all the information\nfor multiple words nouns was carried by a single unit. Taken together, these\nresults indicate how neural networks may extract bounded nested tree\nstructures, without learning a systematic recursive rule.",
    "published_date": "2021-01-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.02258v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.02256v2",
    "title": "Locally supported, quasi-interpolatory bases for the approximation of functions on graphs",
    "authors": [
      "Edward J. Fuselier",
      "John Paul Ward"
    ],
    "author_ids": [],
    "abstract": "Graph-based approximation methods are of growing interest in many areas,\nincluding transportation, biological and chemical networks, financial models,\nimage processing, network flows, and more. In these applications, often a basis\nfor the approximation space is not available analytically and must be computed.\nWe propose perturbations of Lagrange bases on graphs, where the Lagrange\nfunctions come from a class of functions analogous to classical splines. The\nbasis functions we consider have local support, with each basis function\nobtained by solving a small energy minimization problem related to a\ndifferential operator on the graph. We present $\\ell_\\infty$ error estimates\nbetween the local basis and the corresponding interpolatory Lagrange basis\nfunctions in cases where the underlying graph satisfies a mild assumption on\nthe connections of vertices where the function is not known, and the\ntheoretical bounds are examined further in numerical experiments. Included in\nour analysis is a mixed-norm inequality for positive definite matrices that is\ntighter than the general estimate $\\|A\\|_{\\infty} \\leq \\sqrt{n} \\|A\\|_{2}$.",
    "published_date": "2021-01-06T00:00:00",
    "year": 2021,
    "categories": [
      "math.NA",
      "cs.NA",
      "math.CA",
      "05C90, 41A05, 41A15, 65D07"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.02256v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2101.02084v1",
    "title": "Fairness with Continuous Optimal Transport",
    "authors": [
      "Silvia Chiappa",
      "Aldo Pacchiano"
    ],
    "author_ids": [],
    "abstract": "Whilst optimal transport (OT) is increasingly being recognized as a powerful\nand flexible approach for dealing with fairness issues, current OT fairness\nmethods are confined to the use of discrete OT. In this paper, we leverage\nrecent advances from the OT literature to introduce a stochastic-gradient\nfairness method based on a dual formulation of continuous OT. We show that this\nmethod gives superior performance to discrete OT methods when little data is\navailable to solve the OT problem, and similar performance otherwise. We also\nshow that both continuous and discrete OT methods are able to continually\nadjust the model parameters to adapt to different levels of unfairness that\nmight occur in real-world applications of ML systems.",
    "published_date": "2021-01-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.02084v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.01985v1",
    "title": "Statistical CSI Based Hybrid mmWave MIMO-NOMA with Max-Min Fairness",
    "authors": [
      "Jinle Zhu",
      "Qiang Li",
      "Hongyang Chen",
      "H. Vincent Poor"
    ],
    "author_ids": [],
    "abstract": "Non-orthogonal multiple access (NOMA) and millimeter wave (mmWave) are two\nkey enabling technologies for the fifth-generation (5G) mobile networks and\nbeyond. In this paper, we consider mmWave NOMA systems with max-min fairness\nconstraints. On the one hand, existing beamforming designs aiming at maximizing\nthe spectrum efficiency (SE) are unsuitable for the NOMA systems with fairness\nin this paper. On the other hand, previous work on about mmWave NOMA mostly\ndepends on full knowledge of channel state information (CSI) which is extremely\ndifficult to obtain accurately in mmWave communication systems. To address this\nproblem, we propose a heuristic hybrid beamforming design based on the\nstatistical CSI (SCSI) user grouping strategy. An analog beamforming scheme is\nfirst proposed to integrate the whole cluster users to mitigate the\ninter-cluster interference in the first stage. Then two digital beamforming\ndesigns are proposed to further suppress the interference based on SCSI. One is\nthe widely used zero forcing (ZF) approach and the other is derived from the\nsignal-to leakage-plus-noise ratio (SLNR) metric extended from orthogonal\nmultiple access (OMA) systems. The effective gains fed back from the users are\nused for the power allocation. We introduce the quadratic transform (QT) method\nand bisection approach to reformulate this complex problem so as to rend it\nsolvable. Simulation results show that our proposed algorithms outperform the\nprevious algorithms in term of user fairness.",
    "published_date": "2021-01-06T00:00:00",
    "year": 2021,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.01985v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2101.01765v2",
    "title": "A unifying approach on bias and variance analysis for classification",
    "authors": [
      "Cemre Zor",
      "Terry Windeatt"
    ],
    "author_ids": [],
    "abstract": "Standard bias and variance (B&V) terminologies were originally defined for\nthe regression setting and their extensions to classification have led to\nseveral different models / definitions in the literature. In this paper, we aim\nto provide the link between the commonly used frameworks of Tumer & Ghosh (T&G)\nand James. By unifying the two approaches, we relate the B&V defined for the\n0/1 loss to the standard B&V of the boundary distributions given for the\nsquared error loss. The closed form relationships provide a deeper\nunderstanding of classification performance, and their use is demonstrated in\ntwo case studies.",
    "published_date": "2021-01-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML",
      "62H30 (Primary), 68T01 (Secondary)",
      "I.5.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.01765v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.01703v3",
    "title": "Detecting Bias in the Presence of Spatial Autocorrelation",
    "authors": [
      "Subhabrata Majumdar",
      "Cheryl Flynn",
      "Ritwik Mitra"
    ],
    "author_ids": [],
    "abstract": "In spite of considerable practical importance, current algorithmic fairness\nliterature lacks technical methods to account for underlying geographic\ndependency while evaluating or mitigating bias issues for spatial data. We\ninitiate the study of bias in spatial applications in this paper, taking the\nfirst step towards formalizing this line of quantitative methods. Bias in\nspatial data applications often gets confounded by underlying spatial\nautocorrelation. We propose hypothesis testing methodology to detect the\npresence and strength of this effect, then account for it by using a spatial\nfiltering-based approach -- in order to enable application of existing bias\ndetection metrics. We evaluate our proposed methodology through numerical\nexperiments on real and synthetic datasets, demonstrating that in the presence\nof several types of confounding effects due to the underlying spatial structure\nour testing methods perform well in maintaining low type-II errors and nominal\ntype-I errors.",
    "published_date": "2021-01-05T00:00:00",
    "year": 2021,
    "categories": [
      "stat.AP",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.01703v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2101.01673v5",
    "title": "Characterizing Intersectional Group Fairness with Worst-Case Comparisons",
    "authors": [
      "Avijit Ghosh",
      "Lea Genuit",
      "Mary Reagan"
    ],
    "author_ids": [],
    "abstract": "Machine Learning or Artificial Intelligence algorithms have gained\nconsiderable scrutiny in recent times owing to their propensity towards\nimitating and amplifying existing prejudices in society. This has led to a\nniche but growing body of work that identifies and attempts to fix these\nbiases. A first step towards making these algorithms more fair is designing\nmetrics that measure unfairness. Most existing work in this field deals with\neither a binary view of fairness (protected vs. unprotected groups) or\npolitically defined categories (race or gender). Such categorization misses the\nimportant nuance of intersectionality - biases can often be amplified in\nsubgroups that combine membership from different categories, especially if such\na subgroup is particularly underrepresented in historical platforms of\nopportunity.\n  In this paper, we discuss why fairness metrics need to be looked at under the\nlens of intersectionality, identify existing work in intersectional fairness,\nsuggest a simple worst case comparison method to expand the definitions of\nexisting group fairness metrics to incorporate intersectionality, and finally\nconclude with the social, legal and political framework to handle\nintersectional fairness in the modern context.",
    "published_date": "2021-01-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.01673v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.01648v2",
    "title": "Nonlinear Filter for Simultaneous Localization and Mapping on a Matrix Lie Group using IMU and Feature Measurements",
    "authors": [
      "Hashim A. Hashim",
      "Abdelrahman E. E. Eltoukhy"
    ],
    "author_ids": [],
    "abstract": "Simultaneous Localization and Mapping (SLAM) is a process of concurrent\nestimation of the vehicle's pose and feature locations with respect to a frame\nof reference. This paper proposes a computationally cheap geometric nonlinear\nSLAM filter algorithm structured to mimic the nonlinear motion dynamics of the\ntrue SLAM problem posed on the matrix Lie group of\n$\\mathbb{SLAM}_{n}\\left(3\\right)$. The nonlinear filter on manifold is proposed\nin continuous form and it utilizes available measurements obtained from group\nvelocity vectors, feature measurements and an inertial measurement unit (IMU).\nThe unknown bias attached to velocity measurements is successfully handled by\nthe proposed estimator. Simulation results illustrate the robustness of the\nproposed filter in discrete form demonstrating its utility for the\nsix-degrees-of-freedom (6 DoF) pose estimation as well as feature estimation in\nthree-dimensional (3D) space. In addition, the quaternion representation of the\nnonlinear filter for SLAM is provided. Keywords: Simultaneous Localization and\nMapping, Nonlinear observer algorithm for SLAM, inertial measurement unit,\ninertial vision system, pose, position, attitude, landmark, estimation, IMU,\nSE(3), SO(3), unmanned aerial vehicle, rigid-body, noise, nonlinear observer\nfor SLAM, Gaussian filter, Kalman filtering, navigation.",
    "published_date": "2021-01-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.01648v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2101.01294v1",
    "title": "One vs Previous and Similar Classes Learning -- A Comparative Study",
    "authors": [
      "Daniel Cauchi",
      "Adrian Muscat"
    ],
    "author_ids": [],
    "abstract": "When dealing with multi-class classification problems, it is common practice\nto build a model consisting of a series of binary classifiers using a learning\nparadigm which dictates how the classifiers are built and combined to\ndiscriminate between the individual classes. As new data enters the system and\nthe model needs updating, these models would often need to be retrained from\nscratch. This work proposes three learning paradigms which allow trained models\nto be updated without the need of retraining from scratch. A comparative\nanalysis is performed to evaluate them against a baseline. Results show that\nthe proposed paradigms are faster than the baseline at updating, with two of\nthem being faster at training from scratch as well, especially on larger\ndatasets, while retaining a comparable classification performance.",
    "published_date": "2021-01-05T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.01294v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.01214v2",
    "title": "Reconstructing Patchy Reionization with Deep Learning",
    "authors": [
      "Eric Guzman",
      "Joel Meyers"
    ],
    "author_ids": [],
    "abstract": "The precision anticipated from next-generation cosmic microwave background\n(CMB) surveys will create opportunities for characteristically new insights\ninto cosmology. Secondary anisotropies of the CMB will have an increased\nimportance in forthcoming surveys, due both to the cosmological information\nthey encode and the role they play in obscuring our view of the primary\nfluctuations. Quadratic estimators have become the standard tools for\nreconstructing the fields that distort the primary CMB and produce secondary\nanisotropies. While successful for lensing reconstruction with current data,\nquadratic estimators will be sub-optimal for the reconstruction of lensing and\nother effects at the expected sensitivity of the upcoming CMB surveys. In this\npaper we describe a convolutional neural network, ResUNet-CMB, that is capable\nof the simultaneous reconstruction of two sources of secondary CMB\nanisotropies, gravitational lensing and patchy reionization. We show that the\nResUNet-CMB network significantly outperforms the quadratic estimator at low\nnoise levels and is not subject to the lensing-induced bias on the patchy\nreionization reconstruction that would be present with a straightforward\napplication of the quadratic estimator.",
    "published_date": "2021-01-04T00:00:00",
    "year": 2021,
    "categories": [
      "astro-ph.CO",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.01214v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.01016v4",
    "title": "A Second-Order Nonlocal Approximation for Manifold Poisson Model with Dirichlet Boundary",
    "authors": [
      "Yajie Zhang",
      "Zuoqiang Shi"
    ],
    "author_ids": [],
    "abstract": "Recently, we constructed a class of nonlocal Poisson model on manifold under\nDirichlet boundary with global $\\mathcal{O}(\\delta^2)$ truncation error to its\nlocal counterpart, where $\\delta$ denotes the nonlocal horizon parameter. In\nthis paper, the well-posedness of such manifold model is studied. We utilize\nPoincare inequality to control the lower order terms along the\n$2\\delta$-boundary layer in the weak formulation of model. The second order\nlocalization rate of model is attained by combining the well-posedness argument\nand the truncation error analysis. Such rate is currently optimal among all\nnonlocal models. Besides, we implement the point integral method(PIM) to our\nnonlocal model through 2 specific numerical examples to illustrate the\nquadratic rate of convergence on the other side.",
    "published_date": "2021-01-04T00:00:00",
    "year": 2021,
    "categories": [
      "math.NA",
      "cs.NA",
      "math.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.01016v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2101.00786v4",
    "title": "Do Abstractions Have Politics? Toward a More Critical Algorithm Analysis",
    "authors": [
      "Kevin Lin"
    ],
    "author_ids": [],
    "abstract": "The expansion of computer science (CS) education in K--12 and\nhigher-education in the United States has prompted deeper engagement with\nequity that moves beyond inclusion toward a more critical CS education. Rather\nthan frame computing as a value-neutral tool, a justice-centered approach to\nequitable CS education draws on critical pedagogy to ensure the rightful\npresence of political struggles by emphasizing the development of not only\nknowledge and skills but also CS disciplinary identities. While recent efforts\nhave integrated ethics into several areas of the undergraduate CS curriculum,\ncritical approaches for teaching data structures and algorithms in particular\nare undertheorized. Basic Data Structures remains focused on runtime-centered\nalgorithm analysis.\n  We argue for affordance analysis, a more critical algorithm analysis based on\nan affordance account of value embedding. Drawing on critical methods from\nscience and technology studies, philosophy of technology, and human-computer\ninteraction, affordance analysis examines how the design of computational\nabstractions such as data structures and algorithms embody affordances, which\nin turn embody values with political consequences. We illustrate 5 case studies\nof how affordance analysis refutes social determination of technology,\nforegrounds the limitations of data abstractions, and implicates the design of\nalgorithms in disproportionately distributing benefits and harms to particular\nsocial identities within the matrix of domination.",
    "published_date": "2021-01-04T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "K.3.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.00786v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.00598v1",
    "title": "Copula Flows for Synthetic Data Generation",
    "authors": [
      "Sanket Kamthe",
      "Samuel Assefa",
      "Marc Deisenroth"
    ],
    "author_ids": [],
    "abstract": "The ability to generate high-fidelity synthetic data is crucial when\navailable (real) data is limited or where privacy and data protection standards\nallow only for limited use of the given data, e.g., in medical and financial\ndata-sets. Current state-of-the-art methods for synthetic data generation are\nbased on generative models, such as Generative Adversarial Networks (GANs).\nEven though GANs have achieved remarkable results in synthetic data generation,\nthey are often challenging to interpret.Furthermore, GAN-based methods can\nsuffer when used with mixed real and categorical variables.Moreover, loss\nfunction (discriminator loss) design itself is problem specific, i.e., the\ngenerative model may not be useful for tasks it was not explicitly trained for.\nIn this paper, we propose to use a probabilistic model as a synthetic data\ngenerator. Learning the probabilistic model for the data is equivalent to\nestimating the density of the data. Based on the copula theory, we divide the\ndensity estimation task into two parts, i.e., estimating univariate marginals\nand estimating the multivariate copula density over the univariate marginals.\nWe use normalising flows to learn both the copula density and univariate\nmarginals. We benchmark our method on both simulated and real data-sets in\nterms of density estimation as well as the ability to generate high-fidelity\nsynthetic data",
    "published_date": "2021-01-03T00:00:00",
    "year": 2021,
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.00598v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.00579v3",
    "title": "A pessimist's approach to one-sided matching",
    "authors": [
      "Tom Demeulemeester",
      "Dries Goossens",
      "Ben Hermans",
      "Roel Leus"
    ],
    "author_ids": [],
    "abstract": "Inspired by real-world applications such as the assignment of pupils to\nschools or the allocation of social housing, the one-sided matching problem\nstudies how a set of agents can be assigned to a set of objects when the agents\nhave preferences over the objects, but not vice versa. For fairness reasons,\nmost mechanisms use randomness, and therefore result in a probabilistic\nassignment. We study the problem of decomposing these probabilistic assignments\ninto a weighted sum of ex-post (Pareto-)efficient matchings, while maximizing\nthe worst-case number of assigned agents. This decomposition preserves all the\nassignments' desirable properties, most notably strategy-proofness. For a\nspecific class of probabilistic assignments, including the assignment by the\nProbabilistic Serial mechanism, we propose a polynomial-time algorithm for this\nproblem that obtains a decomposition in which all matchings assign at least the\nexpected number of assigned agents by the probabilistic assignment, rounded\ndown, thus achieving the theoretically best possible guarantee. For general\nprobabilistic assignments, the problem becomes NP-hard. For the Random Serial\nDictatorship mechanism, we show that the worst-case number of assigned agents\nis at least half of the optimal, and that this bound is asymptotically tight.\nLastly, we propose a column generation framework for the introduced problem,\nwhich we evaluate both on randomly generated data, and on real-world school\nchoice data from the Belgian cities Antwerp and Ghent.",
    "published_date": "2021-01-03T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DS",
      "cs.DM",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.00579v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2101.00454v1",
    "title": "Mining the online infosphere: A survey",
    "authors": [
      "Sayantan Adak",
      "Souvic Chakraborty",
      "Paramtia Das",
      "Mithun Das",
      "Abhisek Dash",
      "Rima Hazra",
      "Binny Mathew",
      "Punyajoy Saha",
      "Soumya Sarkar",
      "Animesh Mukherjee"
    ],
    "author_ids": [],
    "abstract": "The evolution of AI-based system and applications had pervaded everyday life\nto make decisions that have momentous impact on individuals and society. With\nthe staggering growth of online data, often termed as the Online Infosphere it\nhas become paramount to monitor the infosphere to ensure social good as the\nAI-based decisions are severely dependent on it. The goal of this survey is to\nprovide a comprehensive review of some of the most important research areas\nrelated to infosphere, focusing on the technical challenges and potential\nsolutions. The survey also outlines some of the important future directions. We\nbegin by discussions focused on the collaborative systems that have emerged\nwithin the infosphere with a special thrust on Wikipedia. In the follow up we\ndemonstrate how the infosphere has been instrumental in the growth of\nscientific citations and collaborations thus fueling interdisciplinary\nresearch. Finally, we illustrate the issues related to the governance of the\ninfosphere such as the tackling of the (a) rising hateful and abusive behavior\nand (b) bias and discrimination in different online platforms and news\nreporting.",
    "published_date": "2021-01-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.DL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.00454v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.00421v3",
    "title": "The Highs and Lows of Simple Lexical Domain Adaptation Approaches for Neural Machine Translation",
    "authors": [
      "Nikolay Bogoychev",
      "Pinzhen Chen"
    ],
    "author_ids": [],
    "abstract": "Machine translation systems are vulnerable to domain mismatch, especially in\na low-resource scenario. Out-of-domain translations are often of poor quality\nand prone to hallucinations, due to exposure bias and the decoder acting as a\nlanguage model. We adopt two approaches to alleviate this problem: lexical\nshortlisting restricted by IBM statistical alignments, and hypothesis\nre-ranking based on similarity. The methods are computationally cheap, widely\nknown, but not extensively experimented on domain adaptation. We demonstrate\nsuccess on low-resource out-of-domain test sets, however, the methods are\nineffective when there is sufficient data or too great domain mismatch. This is\ndue to both the IBM model losing its advantage over the implicitly learned\nneural alignment, and issues with subword segmentation of out-of-domain words.",
    "published_date": "2021-01-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.00421v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.00360v1",
    "title": "New-Type Hoeffding's Inequalities and Application in Tail Bounds",
    "authors": [
      "Pingyi Fan"
    ],
    "author_ids": [],
    "abstract": "It is well known that Hoeffding's inequality has a lot of applications in the\nsignal and information processing fields. How to improve Hoeffding's inequality\nand find the refinements of its applications have always attracted much\nattentions. An improvement of Hoeffding inequality was recently given by Hertz\n\\cite{r1}. Eventhough such an improvement is not so big, it still can be used\nto update many known results with original Hoeffding's inequality, especially\nfor Hoeffding-Azuma inequality for martingales. However, the results in\noriginal Hoeffding's inequality and its refinement one by Hertz only considered\nthe first order moment of random variables. In this paper, we present a new\ntype of Hoeffding's inequalities, where the high order moments of random\nvariables are taken into account. It can get some considerable improvements in\nthe tail bounds evaluation compared with the known results. It is expected that\nthe developed new type Hoeffding's inequalities could get more interesting\napplications in some related fields that use Hoeffding's results.",
    "published_date": "2021-01-02T00:00:00",
    "year": 2021,
    "categories": [
      "math.ST",
      "cs.AI",
      "cs.IT",
      "math.IT",
      "math.PR",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.00360v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.00352v3",
    "title": "Characterizing Fairness Over the Set of Good Models Under Selective Labels",
    "authors": [
      "Amanda Coston",
      "Ashesh Rambachan",
      "Alexandra Chouldechova"
    ],
    "author_ids": [],
    "abstract": "Algorithmic risk assessments are used to inform decisions in a wide variety\nof high-stakes settings. Often multiple predictive models deliver similar\noverall performance but differ markedly in their predictions for individual\ncases, an empirical phenomenon known as the \"Rashomon Effect.\" These models may\nhave different properties over various groups, and therefore have different\npredictive fairness properties. We develop a framework for characterizing\npredictive fairness properties over the set of models that deliver similar\noverall performance, or \"the set of good models.\" Our framework addresses the\nempirically relevant challenge of selectively labelled data in the setting\nwhere the selection decision and outcome are unconfounded given the observed\ndata features. Our framework can be used to 1) replace an existing model with\none that has better fairness properties; or 2) audit for predictive bias. We\nillustrate these uses cases on a real-world credit-scoring task and a\nrecidivism prediction task.",
    "published_date": "2021-01-02T00:00:00",
    "year": 2021,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.00352v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.00330v1",
    "title": "e-PoS: Making Proof-of-Stake Decentralized and Fair",
    "authors": [
      "Muhammad Saad",
      "Zhan Qin",
      "Kui Ren",
      "DaeHun Nyang",
      "David Mohaisen"
    ],
    "author_ids": [],
    "abstract": "Blockchain applications that rely on the Proof-of-Work (PoW) have\nincreasingly become energy inefficient with a staggering carbon footprint. In\ncontrast, energy-efficient alternative consensus protocols such as\nProof-of-Stake (PoS) may cause centralization and unfairness in the blockchain\nsystem. To address these challenges, we propose a modular version of PoS-based\nblockchain systems called epos that resists the centralization of network\nresources by extending mining opportunities to a wider set of stakeholders.\nMoreover, epos leverages the in-built system operations to promote fair mining\npractices by penalizing malicious entities. We validate epos's achievable\nobjectives through theoretical analysis and simulations. Our results show that\nepos ensures fairness and decentralization, and can be applied to existing\nblockchain applications.",
    "published_date": "2021-01-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CR",
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.00330v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2101.00307v2",
    "title": "Quantifying Spatial Homogeneity of Urban Road Networks via Graph Neural Networks",
    "authors": [
      "Jiawei Xue",
      "Nan Jiang",
      "Senwei Liang",
      "Qiyuan Pang",
      "Takahiro Yabe",
      "Satish V. Ukkusuri",
      "Jianzhu Ma"
    ],
    "author_ids": [],
    "abstract": "Quantifying the topological similarities of different parts of urban road\nnetworks (URNs) enables us to understand the urban growth patterns. While\nconventional statistics provide useful information about characteristics of\neither a single node's direct neighbors or the entire network, such metrics\nfail to measure the similarities of subnetworks considering local indirect\nneighborhood relationships. In this study, we propose a graph-based\nmachine-learning method to quantify the spatial homogeneity of subnetworks. We\napply the method to 11,790 urban road networks across 30 cities worldwide to\nmeasure the spatial homogeneity of road networks within each city and across\ndifferent cities. We find that intra-city spatial homogeneity is highly\nassociated with socioeconomic statuses such as GDP and population growth.\nMoreover, inter-city spatial homogeneity obtained by transferring the model\nacross different cities, reveals the inter-city similarity of urban network\nstructures originating in Europe, passed on to cities in the US and Asia.\nSocioeconomic development and inter-city similarity revealed using our method\ncan be leveraged to understand and transfer insights across cities. It also\nenables us to address urban policy challenges including network planning in\nrapidly urbanizing areas and combating regional inequality.",
    "published_date": "2021-01-01T00:00:00",
    "year": 2021,
    "categories": [
      "physics.soc-ph",
      "cs.LG",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.00307v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.02032v5",
    "title": "Socially Responsible AI Algorithms: Issues, Purposes, and Challenges",
    "authors": [
      "Lu Cheng",
      "Kush R. Varshney",
      "Huan Liu"
    ],
    "author_ids": [],
    "abstract": "In the current era, people and society have grown increasingly reliant on\nartificial intelligence (AI) technologies. AI has the potential to drive us\ntowards a future in which all of humanity flourishes. It also comes with\nsubstantial risks for oppression and calamity. Discussions about whether we\nshould (re)trust AI have repeatedly emerged in recent years and in many\nquarters, including industry, academia, healthcare, services, and so on.\nTechnologists and AI researchers have a responsibility to develop trustworthy\nAI systems. They have responded with great effort to design more responsible AI\nalgorithms. However, existing technical solutions are narrow in scope and have\nbeen primarily directed towards algorithms for scoring or classification tasks,\nwith an emphasis on fairness and unwanted bias. To build long-lasting trust\nbetween AI and human beings, we argue that the key is to think beyond\nalgorithmic fairness and connect major aspects of AI that potentially cause\nAI's indifferent behavior. In this survey, we provide a systematic framework of\nSocially Responsible AI Algorithms that aims to examine the subjects of AI\nindifference and the need for socially responsible AI algorithms, define the\nobjectives, and introduce the means by which we may achieve these objectives.\nWe further discuss how to leverage this framework to improve societal\nwell-being through protection, information, and prevention/mitigation.",
    "published_date": "2021-01-01T00:00:00",
    "year": 2021,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.02032v5",
    "is_ai_related": true
  }
]